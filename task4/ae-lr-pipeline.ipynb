{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "#     x_train = scaler.transform(x_train)\n",
    "#     x_test_transed = scaler.transform(x_test)\n",
    "#     x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(256, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = AE()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 200\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x).detach().cpu().numpy()\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0926ced3b80143f598f84916504c2c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.045193626343595736, val loss: 0.03517645636200905\n",
      "Epoch 2: train loss: 0.03484088212677411, val loss: 0.034365701138973234\n",
      "Epoch 3: train loss: 0.034034438332124634, val loss: 0.033725160002708435\n",
      "Epoch 4: train loss: 0.033132624750234645, val loss: 0.032804823100566864\n",
      "Epoch 5: train loss: 0.03235218577117336, val loss: 0.032190517455339435\n",
      "Epoch 6: train loss: 0.03187025284645509, val loss: 0.03171134021878243\n",
      "Epoch 7: train loss: 0.03150641838567598, val loss: 0.03147904694080353\n",
      "Epoch 8: train loss: 0.03129563632181712, val loss: 0.03136818146705628\n",
      "Epoch 9: train loss: 0.031080616723213876, val loss: 0.03110436172783375\n",
      "Epoch 10: train loss: 0.03084573337131617, val loss: 0.03086446487903595\n",
      "Epoch 11: train loss: 0.030559434197691023, val loss: 0.030541934058070184\n",
      "Epoch 12: train loss: 0.030246505167715404, val loss: 0.030253270119428636\n",
      "Epoch 13: train loss: 0.029949373878994767, val loss: 0.029975097224116325\n",
      "Epoch 14: train loss: 0.029628474351094693, val loss: 0.029599492251873017\n",
      "Epoch 15: train loss: 0.0293633980717586, val loss: 0.029359995022416114\n",
      "Epoch 16: train loss: 0.02909147895294793, val loss: 0.029084672063589095\n",
      "Epoch 17: train loss: 0.028922724302021825, val loss: 0.02892848925292492\n",
      "Epoch 18: train loss: 0.02873733656777411, val loss: 0.029002387166023255\n",
      "Epoch 19: train loss: 0.028626938242997443, val loss: 0.028710072562098503\n",
      "Epoch 20: train loss: 0.028555667360826414, val loss: 0.028632057681679726\n",
      "Epoch 21: train loss: 0.028468784494363533, val loss: 0.02863636080920696\n",
      "Epoch 22: train loss: 0.02837989561807136, val loss: 0.028519421726465224\n",
      "Epoch 23: train loss: 0.02830143926824842, val loss: 0.02840889523923397\n",
      "Epoch 24: train loss: 0.028231649423132138, val loss: 0.028358582437038423\n",
      "Epoch 25: train loss: 0.028177345251854585, val loss: 0.028286413326859473\n",
      "Epoch 26: train loss: 0.02809773409397018, val loss: 0.028170660853385925\n",
      "Epoch 27: train loss: 0.028001807213741906, val loss: 0.028208643466234206\n",
      "Epoch 28: train loss: 0.027927472425966847, val loss: 0.028176859468221665\n",
      "Epoch 29: train loss: 0.027858099833130836, val loss: 0.02792442034184933\n",
      "Epoch 30: train loss: 0.027800301422269975, val loss: 0.02789760176837444\n",
      "Epoch 31: train loss: 0.027744947770116282, val loss: 0.02788667643070221\n",
      "Epoch 32: train loss: 0.027697057776001035, val loss: 0.027830621629953384\n",
      "Epoch 33: train loss: 0.027617314626367725, val loss: 0.027687202766537668\n",
      "Epoch 34: train loss: 0.027574799201318195, val loss: 0.027791815385222435\n",
      "Epoch 35: train loss: 0.02752655803427404, val loss: 0.027677647158503533\n",
      "Epoch 36: train loss: 0.027479393559451008, val loss: 0.027604670867323874\n",
      "Epoch 37: train loss: 0.027445955992353204, val loss: 0.027542741864919663\n",
      "Epoch 38: train loss: 0.02738562206650267, val loss: 0.02749456186592579\n",
      "Epoch 39: train loss: 0.02737765873786138, val loss: 0.027604667246341707\n",
      "Epoch 40: train loss: 0.027335614261274434, val loss: 0.027418861374258995\n",
      "Epoch 41: train loss: 0.02729290936187822, val loss: 0.02742264148592949\n",
      "Epoch 42: train loss: 0.02727612959456687, val loss: 0.02741711677610874\n",
      "Epoch 43: train loss: 0.027225641270681302, val loss: 0.027492988169193266\n",
      "Epoch 44: train loss: 0.027215783460711945, val loss: 0.027343490332365036\n",
      "Epoch 45: train loss: 0.027158002237884366, val loss: 0.02717079909145832\n",
      "Epoch 46: train loss: 0.027144466622751585, val loss: 0.027374033480882645\n",
      "Epoch 47: train loss: 0.02711934819695901, val loss: 0.027365207746624945\n",
      "Epoch 48: train loss: 0.027059833125496396, val loss: 0.027147718593478202\n",
      "Epoch 49: train loss: 0.027023161037235843, val loss: 0.027147246658802034\n",
      "Epoch 50: train loss: 0.027004139720785376, val loss: 0.027116958245635033\n",
      "Epoch 51: train loss: 0.026995479328291758, val loss: 0.027088551595807076\n",
      "Epoch 52: train loss: 0.026939451101179027, val loss: 0.027091296032071114\n",
      "Epoch 53: train loss: 0.02692208672299677, val loss: 0.027079581171274184\n",
      "Epoch 54: train loss: 0.026945641532236216, val loss: 0.02699258255958557\n",
      "Epoch 55: train loss: 0.026896045596928014, val loss: 0.027078322023153304\n",
      "Epoch 56: train loss: 0.026879669129544374, val loss: 0.026986076235771178\n",
      "Epoch 57: train loss: 0.026870312585818526, val loss: 0.027027053877711295\n",
      "Epoch 58: train loss: 0.02683041363498386, val loss: 0.02692687912285328\n",
      "Epoch 59: train loss: 0.0268069910729418, val loss: 0.02707475097477436\n",
      "Epoch 60: train loss: 0.02683187846322449, val loss: 0.02690811988711357\n",
      "Epoch 61: train loss: 0.02679778301898314, val loss: 0.026870271235704422\n",
      "Epoch 62: train loss: 0.02676897269761076, val loss: 0.026936888337135315\n",
      "Epoch 63: train loss: 0.026779106585042817, val loss: 0.026938477113842965\n",
      "Epoch 64: train loss: 0.02672880694330955, val loss: 0.026939738407731055\n",
      "Epoch 65: train loss: 0.02675160960366531, val loss: 0.026962164744734764\n",
      "Epoch 66: train loss: 0.026726207572282577, val loss: 0.02687136422097683\n",
      "Epoch 67: train loss: 0.02673572082787144, val loss: 0.026823567256331445\n",
      "Epoch 68: train loss: 0.026726505584862767, val loss: 0.02685301223397255\n",
      "Epoch 69: train loss: 0.026705224267378147, val loss: 0.027015678718686105\n",
      "Epoch 70: train loss: 0.02670171704036849, val loss: 0.026847443521022796\n",
      "Epoch 71: train loss: 0.026658448207743313, val loss: 0.026753592163324357\n",
      "Epoch 72: train loss: 0.026689294448616552, val loss: 0.02669556576013565\n",
      "Epoch 73: train loss: 0.026667320872447928, val loss: 0.026701453685760497\n",
      "Epoch 74: train loss: 0.026634641758337314, val loss: 0.026687640234827996\n",
      "Epoch 75: train loss: 0.02666280260803748, val loss: 0.026778827100992202\n",
      "Epoch 76: train loss: 0.02662213943746625, val loss: 0.02683404339849949\n",
      "Epoch 77: train loss: 0.026640879541027302, val loss: 0.026661384239792824\n",
      "Epoch 78: train loss: 0.02662455853819847, val loss: 0.026855032935738564\n",
      "Epoch 79: train loss: 0.02658966367798192, val loss: 0.02684132295846939\n",
      "Epoch 80: train loss: 0.02659939381480217, val loss: 0.026830357775092125\n",
      "Epoch 81: train loss: 0.026612446840320315, val loss: 0.026746831715106965\n",
      "Epoch 82: train loss: 0.02657603949460448, val loss: 0.026651121705770494\n",
      "Epoch 83: train loss: 0.026549962592976435, val loss: 0.026538353636860846\n",
      "Epoch 84: train loss: 0.026576763759158094, val loss: 0.026578135877847672\n",
      "Epoch 85: train loss: 0.026588778173132818, val loss: 0.026648531436920166\n",
      "Epoch 86: train loss: 0.026536329106712827, val loss: 0.0266743952780962\n",
      "Epoch 87: train loss: 0.026533214656370027, val loss: 0.02660758900642395\n",
      "Epoch 88: train loss: 0.026515071697076974, val loss: 0.026604442492127418\n",
      "Epoch 00089: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 89: train loss: 0.02651408237675015, val loss: 0.02658926123380661\n",
      "Epoch 90: train loss: 0.02638971011158155, val loss: 0.02651036864519119\n",
      "Epoch 91: train loss: 0.026344267711043357, val loss: 0.02634637349843979\n",
      "Epoch 92: train loss: 0.026326308193255443, val loss: 0.0265969400703907\n",
      "Epoch 93: train loss: 0.02630970395128338, val loss: 0.02644307179749012\n",
      "Epoch 94: train loss: 0.026309088152890302, val loss: 0.026534448966383933\n",
      "Epoch 95: train loss: 0.02628260545219694, val loss: 0.026372159883379938\n",
      "Epoch 96: train loss: 0.026298379668471764, val loss: 0.026460950806736945\n",
      "Epoch 00097: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 97: train loss: 0.026258206024461862, val loss: 0.026371121153235436\n",
      "Epoch 98: train loss: 0.026213312583614368, val loss: 0.026520290583372116\n",
      "Epoch 99: train loss: 0.02620840144279052, val loss: 0.026344866767525673\n",
      "Epoch 100: train loss: 0.026219476310270173, val loss: 0.02636850430071354\n",
      "Epoch 101: train loss: 0.02619344421369689, val loss: 0.026288128852844237\n",
      "Epoch 102: train loss: 0.026210760714144122, val loss: 0.026465858459472656\n",
      "Epoch 103: train loss: 0.026204970196497684, val loss: 0.026321477130055428\n",
      "Epoch 104: train loss: 0.0261805170439944, val loss: 0.026161667048931123\n",
      "Epoch 105: train loss: 0.02617200040665208, val loss: 0.02637407383322716\n",
      "Epoch 106: train loss: 0.026174227497103263, val loss: 0.02625924262404442\n",
      "Epoch 107: train loss: 0.02616981707209227, val loss: 0.02635269394516945\n",
      "Epoch 108: train loss: 0.026170238235775304, val loss: 0.026323349073529245\n",
      "Epoch 109: train loss: 0.02620162868104419, val loss: 0.026312242358922957\n",
      "Epoch 00110: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 110: train loss: 0.02617493128989424, val loss: 0.026266455441713334\n",
      "Epoch 111: train loss: 0.026169661584861423, val loss: 0.0262083123922348\n",
      "Epoch 112: train loss: 0.02614660457932219, val loss: 0.02629150830209255\n",
      "Epoch 113: train loss: 0.026185009688747173, val loss: 0.02633504994213581\n",
      "Epoch 114: train loss: 0.026155612004350643, val loss: 0.026282556504011154\n",
      "Epoch 115: train loss: 0.026151629055032925, val loss: 0.026281614840030672\n",
      "Epoch 00116: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 116: train loss: 0.026149715426929142, val loss: 0.026168408140540122\n",
      "Epoch 117: train loss: 0.02614521389044061, val loss: 0.026222674921154977\n",
      "Epoch 118: train loss: 0.026162509955617846, val loss: 0.026260276034474374\n",
      "Epoch 119: train loss: 0.026156937665781196, val loss: 0.026261621206998827\n",
      "Epoch 120: train loss: 0.026166780205101384, val loss: 0.026266502350568773\n",
      "Epoch 121: train loss: 0.02614661782736681, val loss: 0.026112679287791252\n",
      "Epoch 122: train loss: 0.026125778067476894, val loss: 0.02637464579939842\n",
      "Epoch 123: train loss: 0.026168125896429528, val loss: 0.026275945037603377\n",
      "Epoch 124: train loss: 0.02615429026010085, val loss: 0.026255074828863145\n",
      "Epoch 125: train loss: 0.026145252531280323, val loss: 0.026215814173221587\n",
      "Epoch 126: train loss: 0.026160243516977953, val loss: 0.026385818883776665\n",
      "Epoch 00127: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 127: train loss: 0.02615122123822874, val loss: 0.026289710611104966\n",
      "Epoch 128: train loss: 0.026140867993843797, val loss: 0.026309015214443208\n",
      "Epoch 129: train loss: 0.02613328915621553, val loss: 0.026414870783686636\n",
      "Epoch 130: train loss: 0.02613453993657414, val loss: 0.026274144977331163\n",
      "Epoch 131: train loss: 0.02613316845376881, val loss: 0.026228207796812057\n",
      "Epoch 132: train loss: 0.02613625291567676, val loss: 0.026252396002411842\n",
      "Epoch 00133: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 133: train loss: 0.026130169763248792, val loss: 0.026454240545630454\n",
      "Epoch 134: train loss: 0.02614865851128588, val loss: 0.026465165123343466\n",
      "Epoch 135: train loss: 0.026136587124698015, val loss: 0.02628248281776905\n",
      "Epoch 136: train loss: 0.026164576751845223, val loss: 0.026465508848428725\n",
      "Epoch 137: train loss: 0.026130751944926322, val loss: 0.026300541713833808\n",
      "Epoch 138: train loss: 0.026112091831102663, val loss: 0.02631674316525459\n",
      "Epoch 00139: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 139: train loss: 0.026140247108376757, val loss: 0.026392369717359542\n",
      "Epoch 140: train loss: 0.026159629053911386, val loss: 0.026236208215355872\n",
      "Epoch 141: train loss: 0.02614825138905827, val loss: 0.026330239698290826\n",
      "Epoch 142: train loss: 0.026140039477421315, val loss: 0.02613692955672741\n",
      "Epoch 143: train loss: 0.02613189284625102, val loss: 0.02635483780503273\n",
      "Epoch 144: train loss: 0.026154129773074266, val loss: 0.02609313549101353\n",
      "Epoch 145: train loss: 0.026150266287278155, val loss: 0.026340115562081337\n",
      "Epoch 146: train loss: 0.026173463846955982, val loss: 0.026190473437309264\n",
      "Epoch 147: train loss: 0.02613290886976281, val loss: 0.026284064903855323\n",
      "Epoch 148: train loss: 0.026135772472437546, val loss: 0.0262880080640316\n",
      "Epoch 149: train loss: 0.026137336789345256, val loss: 0.026440262362360953\n",
      "Epoch 00150: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 150: train loss: 0.02616585731810453, val loss: 0.02617670652270317\n",
      "Early stop at epoch 150\n"
     ]
    }
   ],
   "source": [
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "def get_regression_model():\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    model = ElasticNet(alpha=0.01, l1_ratio=0.01, max_iter=100000)\n",
    "    # model = LinearRegression()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae-en.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-ae-en.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"featureExtractor\", pretrainedfeatures),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", get_regression_model())\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = pipeline.predict(x_test.to_numpy())\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
