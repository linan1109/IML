{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 256)\n",
    "        self.fc6 = nn.Linear(256, 128)\n",
    "        self.fc7 = nn.Linear(128, 64)\n",
    "        self.fc8 = nn.Linear(64, 1)\n",
    "\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "        nn.init.xavier_normal_(self.fc6.weight)\n",
    "        nn.init.xavier_normal_(self.fc7.weight)\n",
    "        nn.init.xavier_normal_(self.fc8.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-std-2-128.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d896c71d7447c0b80080c7b70db2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.8039839331373877, val loss: 0.3068343231678009\n",
      "Epoch 2: train loss: 0.19747745149962756, val loss: 0.13597665691375732\n",
      "Epoch 3: train loss: 0.10337491994366353, val loss: 0.07787841010093689\n",
      "Epoch 4: train loss: 0.06557657432069584, val loss: 0.05148509618639946\n",
      "Epoch 5: train loss: 0.0443164117609968, val loss: 0.035646419316530224\n",
      "Epoch 6: train loss: 0.033742312747604995, val loss: 0.02958827018737793\n",
      "Epoch 7: train loss: 0.026814531113420213, val loss: 0.024851875826716423\n",
      "Epoch 8: train loss: 0.02234016000373023, val loss: 0.02237956130504608\n",
      "Epoch 9: train loss: 0.019565305419722383, val loss: 0.019625342935323715\n",
      "Epoch 10: train loss: 0.01763995278657091, val loss: 0.019552548229694367\n",
      "Epoch 11: train loss: 0.016176928615083498, val loss: 0.01511939961463213\n",
      "Epoch 12: train loss: 0.014500025208355213, val loss: 0.014376088686287402\n",
      "Epoch 13: train loss: 0.013395809617881872, val loss: 0.014220310010015964\n",
      "Epoch 14: train loss: 0.012454448915105693, val loss: 0.012741538353264331\n",
      "Epoch 15: train loss: 0.01160733860913588, val loss: 0.013526590168476104\n",
      "Epoch 16: train loss: 0.010841233273854061, val loss: 0.011849784769117832\n",
      "Epoch 17: train loss: 0.009950025805101103, val loss: 0.011912566162645816\n",
      "Epoch 18: train loss: 0.009600587159699324, val loss: 0.01142082865536213\n",
      "Epoch 19: train loss: 0.008864849064803245, val loss: 0.010169367007911205\n",
      "Epoch 20: train loss: 0.008535017442490373, val loss: 0.009513452619314194\n",
      "Epoch 21: train loss: 0.00783172756828824, val loss: 0.00869491720944643\n",
      "Epoch 22: train loss: 0.007495776630329842, val loss: 0.008242520354688167\n",
      "Epoch 23: train loss: 0.0071509682999885815, val loss: 0.007517048846930266\n",
      "Epoch 24: train loss: 0.0068631712737284144, val loss: 0.007631344176828861\n",
      "Epoch 25: train loss: 0.006861539335883394, val loss: 0.008165649361908436\n",
      "Epoch 26: train loss: 0.006776913191134833, val loss: 0.00748263868689537\n",
      "Epoch 27: train loss: 0.006562375002521641, val loss: 0.006919118329882621\n",
      "Epoch 28: train loss: 0.006650486586045246, val loss: 0.007305403381586075\n",
      "Epoch 29: train loss: 0.006331569958858344, val loss: 0.00798041868954897\n",
      "Epoch 30: train loss: 0.006739483874823366, val loss: 0.006757537677884102\n",
      "Epoch 31: train loss: 0.006414490110107831, val loss: 0.006338831081986428\n",
      "Epoch 32: train loss: 0.00650787217793416, val loss: 0.007194105483591557\n",
      "Epoch 33: train loss: 0.006251457295399539, val loss: 0.006977555122226477\n",
      "Epoch 34: train loss: 0.006449938781559467, val loss: 0.006494871087372303\n",
      "Epoch 35: train loss: 0.006503962007104134, val loss: 0.00751530746743083\n",
      "Epoch 36: train loss: 0.006289072061831854, val loss: 0.00800426747649908\n",
      "Epoch 00037: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 37: train loss: 0.006882376160016473, val loss: 0.007175510764122009\n",
      "Epoch 38: train loss: 0.004509683932044676, val loss: 0.004576357606798411\n",
      "Epoch 39: train loss: 0.003753501384462021, val loss: 0.004215204279869795\n",
      "Epoch 40: train loss: 0.0036964153441391427, val loss: 0.0047839867845177655\n",
      "Epoch 41: train loss: 0.0035193118797044974, val loss: 0.004321899503469467\n",
      "Epoch 42: train loss: 0.0035020766246075534, val loss: 0.004334201361984014\n",
      "Epoch 43: train loss: 0.0036408939772874724, val loss: 0.004237921491265297\n",
      "Epoch 44: train loss: 0.0037385419535697724, val loss: 0.004666143849492073\n",
      "Epoch 00045: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 45: train loss: 0.003916522286604254, val loss: 0.004730942994356156\n",
      "Epoch 46: train loss: 0.003009475943993549, val loss: 0.003895233113318682\n",
      "Epoch 47: train loss: 0.002607182795067831, val loss: 0.003386798419058323\n",
      "Epoch 48: train loss: 0.0024992841475700235, val loss: 0.0036482286285609007\n",
      "Epoch 49: train loss: 0.0024833660901961278, val loss: 0.003450973076745868\n",
      "Epoch 50: train loss: 0.002420414518070768, val loss: 0.003659576810896397\n",
      "Epoch 51: train loss: 0.0024045565080809956, val loss: 0.00366450946778059\n",
      "Epoch 52: train loss: 0.0023903267981142413, val loss: 0.003613787356764078\n",
      "Epoch 00053: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 53: train loss: 0.0023624121438027643, val loss: 0.0034453029111027717\n",
      "Epoch 54: train loss: 0.0021738254474666045, val loss: 0.0031625122260302307\n",
      "Epoch 55: train loss: 0.002038408311121926, val loss: 0.003184852538630366\n",
      "Epoch 56: train loss: 0.0019577158983720808, val loss: 0.003098826479166746\n",
      "Epoch 57: train loss: 0.0019230926603040828, val loss: 0.003246345652267337\n",
      "Epoch 58: train loss: 0.0019450898778301722, val loss: 0.0031626994628459215\n",
      "Epoch 59: train loss: 0.0019379287349554348, val loss: 0.003207726178690791\n",
      "Epoch 60: train loss: 0.0018861752457308527, val loss: 0.0030462189968675376\n",
      "Epoch 61: train loss: 0.001883990248551174, val loss: 0.003222878770902753\n",
      "Epoch 62: train loss: 0.0018642761936905432, val loss: 0.003062744837254286\n",
      "Epoch 63: train loss: 0.0018984341222260679, val loss: 0.003156452260911465\n",
      "Epoch 64: train loss: 0.0018810742540779162, val loss: 0.003451747849583626\n",
      "Epoch 65: train loss: 0.001833443691801964, val loss: 0.0030424760933965445\n",
      "Epoch 66: train loss: 0.0018600279044131843, val loss: 0.003206650534644723\n",
      "Epoch 67: train loss: 0.00183667931690508, val loss: 0.0030471950769424437\n",
      "Epoch 68: train loss: 0.0017947226121299426, val loss: 0.003197500145062804\n",
      "Epoch 69: train loss: 0.0018274579237082176, val loss: 0.003312691254541278\n",
      "Epoch 70: train loss: 0.001803649679833681, val loss: 0.0032725699953734875\n",
      "Epoch 00071: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 71: train loss: 0.0017611047125858615, val loss: 0.0031981203351169824\n",
      "Epoch 72: train loss: 0.0017140651248699547, val loss: 0.0029380991254001855\n",
      "Epoch 73: train loss: 0.0016739334587615971, val loss: 0.0030676491297781465\n",
      "Epoch 74: train loss: 0.0016703061237931251, val loss: 0.003186390407383442\n",
      "Epoch 75: train loss: 0.0016544014668966435, val loss: 0.0032013258337974548\n",
      "Epoch 76: train loss: 0.0016478355516280447, val loss: 0.0029284038115292787\n",
      "Epoch 77: train loss: 0.001632631230833275, val loss: 0.0033081141095608474\n",
      "Epoch 78: train loss: 0.001632367731604193, val loss: 0.002995798518881202\n",
      "Epoch 79: train loss: 0.0016243810576519796, val loss: 0.0031569658722728493\n",
      "Epoch 80: train loss: 0.0016121857754353966, val loss: 0.003288696235045791\n",
      "Epoch 81: train loss: 0.0016082310675624377, val loss: 0.002835476817563176\n",
      "Epoch 82: train loss: 0.001640901074744761, val loss: 0.0031401841677725314\n",
      "Epoch 83: train loss: 0.0016147510134139839, val loss: 0.0030291998330503702\n",
      "Epoch 84: train loss: 0.0015973435622658962, val loss: 0.0031921730525791645\n",
      "Epoch 85: train loss: 0.0016075055448870573, val loss: 0.0030966442041099073\n",
      "Epoch 86: train loss: 0.0015911905080156058, val loss: 0.0029161822963505984\n",
      "Epoch 00087: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 87: train loss: 0.0015891576598645472, val loss: 0.003124222645536065\n",
      "Epoch 88: train loss: 0.0015732446976424177, val loss: 0.002936929412186146\n",
      "Epoch 89: train loss: 0.0015476128299321448, val loss: 0.002888640379533172\n",
      "Epoch 90: train loss: 0.001561180251669519, val loss: 0.002939178040251136\n",
      "Epoch 91: train loss: 0.0015531280588320627, val loss: 0.003060587737709284\n",
      "Epoch 92: train loss: 0.0015640624896148029, val loss: 0.0032472817320376634\n",
      "Epoch 93: train loss: 0.001550671436576819, val loss: 0.0028302960395812987\n",
      "Epoch 94: train loss: 0.0015470001808728794, val loss: 0.0032224882673472168\n",
      "Epoch 95: train loss: 0.00154362126394194, val loss: 0.002951901335269213\n",
      "Epoch 96: train loss: 0.0015560170519450793, val loss: 0.0028708232417702676\n",
      "Epoch 97: train loss: 0.0015460891874545084, val loss: 0.0029249652530997992\n",
      "Epoch 98: train loss: 0.0015313095700718005, val loss: 0.0029602994322776794\n",
      "Epoch 00099: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 99: train loss: 0.0015263952754856068, val loss: 0.003021961959078908\n",
      "Early stop at epoch 99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4920fead5a490ebad6105ed221c1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 2.869431734085083\n",
      "Epoch 20: train loss: 2.493277072906494\n",
      "Epoch 30: train loss: 2.105720043182373\n",
      "Epoch 40: train loss: 1.762355923652649\n",
      "Epoch 50: train loss: 1.4373857975006104\n",
      "Epoch 60: train loss: 1.1410131454467773\n",
      "Epoch 70: train loss: 0.9056570529937744\n",
      "Epoch 80: train loss: 0.7290709018707275\n",
      "Epoch 90: train loss: 0.5848274230957031\n",
      "Epoch 100: train loss: 0.4588254690170288\n",
      "Epoch 110: train loss: 0.3505600690841675\n",
      "Epoch 120: train loss: 0.2640278935432434\n",
      "Epoch 130: train loss: 0.1976080983877182\n",
      "Epoch 140: train loss: 0.150174081325531\n",
      "Epoch 150: train loss: 0.11877354234457016\n",
      "Epoch 160: train loss: 0.09855983406305313\n",
      "Epoch 170: train loss: 0.0857013687491417\n",
      "Epoch 180: train loss: 0.07759332656860352\n",
      "Epoch 190: train loss: 0.07232049852609634\n",
      "Epoch 200: train loss: 0.0687674805521965\n",
      "Epoch 210: train loss: 0.06615366041660309\n",
      "Epoch 220: train loss: 0.06438715755939484\n",
      "Epoch 230: train loss: 0.06319604068994522\n",
      "Epoch 240: train loss: 0.062235526740550995\n",
      "Epoch 250: train loss: 0.061542533338069916\n",
      "Epoch 260: train loss: 0.061091888695955276\n",
      "Epoch 270: train loss: 0.060780417174100876\n",
      "Epoch 280: train loss: 0.060463473200798035\n",
      "Epoch 290: train loss: 0.060173701494932175\n",
      "Epoch 300: train loss: 0.05994294956326485\n",
      "Epoch 310: train loss: 0.05963823199272156\n",
      "Epoch 320: train loss: 0.05937521904706955\n",
      "Epoch 330: train loss: 0.05917348712682724\n",
      "Epoch 340: train loss: 0.05895782262086868\n",
      "Epoch 350: train loss: 0.05881531909108162\n",
      "Epoch 360: train loss: 0.05868213623762131\n",
      "Epoch 370: train loss: 0.05854722484946251\n",
      "Epoch 380: train loss: 0.05843400955200195\n",
      "Epoch 390: train loss: 0.05833413824439049\n",
      "Epoch 400: train loss: 0.05825432762503624\n",
      "Epoch 410: train loss: 0.05818163603544235\n",
      "Epoch 420: train loss: 0.058096062391996384\n",
      "Epoch 430: train loss: 0.05801519379019737\n",
      "Epoch 440: train loss: 0.0579313263297081\n",
      "Epoch 450: train loss: 0.05785519629716873\n",
      "Epoch 460: train loss: 0.05779778212308884\n",
      "Epoch 470: train loss: 0.05774901434779167\n",
      "Epoch 480: train loss: 0.057701949030160904\n",
      "Epoch 490: train loss: 0.0576573945581913\n",
      "Epoch 500: train loss: 0.057614993304014206\n",
      "Epoch 510: train loss: 0.05757053196430206\n",
      "Epoch 520: train loss: 0.057523999363183975\n",
      "Epoch 530: train loss: 0.05748343467712402\n",
      "Epoch 540: train loss: 0.05743836238980293\n",
      "Epoch 550: train loss: 0.05739854648709297\n",
      "Epoch 560: train loss: 0.057363223284482956\n",
      "Epoch 570: train loss: 0.05732812359929085\n",
      "Epoch 580: train loss: 0.05729202181100845\n",
      "Epoch 590: train loss: 0.05725880339741707\n",
      "Epoch 600: train loss: 0.05722561851143837\n",
      "Epoch 610: train loss: 0.05719280242919922\n",
      "Epoch 620: train loss: 0.05715935677289963\n",
      "Epoch 630: train loss: 0.05712718516588211\n",
      "Epoch 640: train loss: 0.05709940940141678\n",
      "Epoch 650: train loss: 0.057064902037382126\n",
      "Epoch 660: train loss: 0.05703512579202652\n",
      "Epoch 670: train loss: 0.05700525641441345\n",
      "Epoch 680: train loss: 0.05697345733642578\n",
      "Epoch 690: train loss: 0.05694635212421417\n",
      "Epoch 700: train loss: 0.05691869184374809\n",
      "Epoch 710: train loss: 0.056888777762651443\n",
      "Epoch 720: train loss: 0.05686039850115776\n",
      "Epoch 730: train loss: 0.0568326897919178\n",
      "Epoch 740: train loss: 0.056807391345500946\n",
      "Epoch 750: train loss: 0.05677880719304085\n",
      "Epoch 760: train loss: 0.056752510368824005\n",
      "Epoch 770: train loss: 0.056724146008491516\n",
      "Epoch 780: train loss: 0.056696198880672455\n",
      "Epoch 790: train loss: 0.05666545778512955\n",
      "Epoch 800: train loss: 0.056637104600667953\n",
      "Epoch 810: train loss: 0.056606143712997437\n",
      "Epoch 820: train loss: 0.056567173451185226\n",
      "Epoch 830: train loss: 0.05652396380901337\n",
      "Epoch 840: train loss: 0.056482408195734024\n",
      "Epoch 850: train loss: 0.056434549391269684\n",
      "Epoch 860: train loss: 0.05638054758310318\n",
      "Epoch 870: train loss: 0.05631295591592789\n",
      "Epoch 880: train loss: 0.05624140799045563\n",
      "Epoch 890: train loss: 0.056164130568504333\n",
      "Epoch 900: train loss: 0.05614480748772621\n",
      "Epoch 910: train loss: 0.05612209811806679\n",
      "Epoch 920: train loss: 0.05609987676143646\n",
      "Epoch 930: train loss: 0.0560746006667614\n",
      "Epoch 940: train loss: 0.05605192855000496\n",
      "Epoch 950: train loss: 0.05603006109595299\n",
      "Epoch 960: train loss: 0.05600367486476898\n",
      "Epoch 970: train loss: 0.05598518252372742\n",
      "Epoch 980: train loss: 0.055964238941669464\n",
      "Epoch 990: train loss: 0.055941734462976456\n",
      "Epoch 1000: train loss: 0.055921971797943115\n",
      "Epoch 1010: train loss: 0.055899959057569504\n",
      "Epoch 1020: train loss: 0.055882617831230164\n",
      "Epoch 1030: train loss: 0.055862635374069214\n",
      "Epoch 1040: train loss: 0.05584122613072395\n",
      "Epoch 1050: train loss: 0.05582352355122566\n",
      "Epoch 1060: train loss: 0.05580376088619232\n",
      "Epoch 1070: train loss: 0.05577542632818222\n",
      "Epoch 1080: train loss: 0.055756356567144394\n",
      "Epoch 1090: train loss: 0.055737778544425964\n",
      "Epoch 1100: train loss: 0.055720776319503784\n",
      "Epoch 1110: train loss: 0.05570394545793533\n",
      "Epoch 1120: train loss: 0.055688124150037766\n",
      "Epoch 1130: train loss: 0.055668413639068604\n",
      "Epoch 1140: train loss: 0.05565354600548744\n",
      "Epoch 1150: train loss: 0.05563719570636749\n",
      "Epoch 1160: train loss: 0.0556148961186409\n",
      "Epoch 1170: train loss: 0.0555989108979702\n",
      "Epoch 01176: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 1180: train loss: 0.05558934435248375\n",
      "Epoch 01188: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 1190: train loss: 0.05558128282427788\n",
      "Epoch 01198: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 1200: train loss: 0.05557902529835701\n",
      "Epoch 01204: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 01210: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 1210: train loss: 0.05557840317487717\n",
      "Epoch 01216: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 1220: train loss: 0.05557820200920105\n",
      "Epoch 01222: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 01228: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 1228, loss: 0.05557817220687866\n",
      "Predictions saved to results-std-2-128.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
