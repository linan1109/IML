{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(10, 1),\n",
    "            )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        return self.seq[:-4](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = NN()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 200\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x).detach().cpu().numpy()\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2e3fb31773438eba6c727b6092c12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 3.257568154335022, val loss: 1.1797654657363892\n",
      "Epoch 2: train loss: 0.8103077400655163, val loss: 0.5257570695877075\n",
      "Epoch 3: train loss: 0.3518899952002934, val loss: 0.21353434884548186\n",
      "Epoch 4: train loss: 0.1593256001350831, val loss: 0.11068434673547745\n",
      "Epoch 5: train loss: 0.0979965468085542, val loss: 0.08329905050992965\n",
      "Epoch 6: train loss: 0.08066516159140334, val loss: 0.0714854655265808\n",
      "Epoch 7: train loss: 0.07427448689329381, val loss: 0.06618212538957596\n",
      "Epoch 8: train loss: 0.07174217458768767, val loss: 0.06543376058340072\n",
      "Epoch 9: train loss: 0.0691433426713457, val loss: 0.06455696338415146\n",
      "Epoch 10: train loss: 0.06604744790403211, val loss: 0.05974799287319183\n",
      "Epoch 11: train loss: 0.0631228680628903, val loss: 0.06537766492366791\n",
      "Epoch 12: train loss: 0.061080086947703845, val loss: 0.056331239283084866\n",
      "Epoch 13: train loss: 0.05836829812368568, val loss: 0.057910795271396635\n",
      "Epoch 14: train loss: 0.05708951846799072, val loss: 0.052676523923873904\n",
      "Epoch 15: train loss: 0.05531549252478444, val loss: 0.05557257106900215\n",
      "Epoch 16: train loss: 0.05451072663798624, val loss: 0.0518603158891201\n",
      "Epoch 17: train loss: 0.053439082936364776, val loss: 0.04980936270952225\n",
      "Epoch 18: train loss: 0.051998186780481924, val loss: 0.050933252334594725\n",
      "Epoch 19: train loss: 0.051020706609195596, val loss: 0.05322671654820442\n",
      "Epoch 20: train loss: 0.05099913661820548, val loss: 0.050650230437517166\n",
      "Epoch 21: train loss: 0.049779456835012045, val loss: 0.05306261059641838\n",
      "Epoch 22: train loss: 0.048454741519324634, val loss: 0.041705947637557986\n",
      "Epoch 23: train loss: 0.04757955585633005, val loss: 0.04786762204766273\n",
      "Epoch 24: train loss: 0.0473644890310813, val loss: 0.04741105610132217\n",
      "Epoch 25: train loss: 0.04662814277288865, val loss: 0.04918598398566246\n",
      "Epoch 26: train loss: 0.04645674696260569, val loss: 0.04627176287770271\n",
      "Epoch 27: train loss: 0.04649588501879147, val loss: 0.04452201101183891\n",
      "Epoch 00028: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 28: train loss: 0.04624516433720686, val loss: 0.0496290500164032\n",
      "Epoch 29: train loss: 0.04543233033710597, val loss: 0.04826192513108254\n",
      "Epoch 30: train loss: 0.04474513174623859, val loss: 0.04431232291460037\n",
      "Epoch 31: train loss: 0.044794392271309484, val loss: 0.04476595282554627\n",
      "Epoch 32: train loss: 0.04487216763167965, val loss: 0.04172440528869629\n",
      "Epoch 33: train loss: 0.04450243191208158, val loss: 0.04516201737523079\n",
      "Epoch 00034: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 34: train loss: 0.04372955357724306, val loss: 0.04730128276348114\n",
      "Epoch 35: train loss: 0.04376276523239758, val loss: 0.04534618312120438\n",
      "Epoch 36: train loss: 0.04373323508671352, val loss: 0.04452493935823441\n",
      "Epoch 37: train loss: 0.04334549863119515, val loss: 0.04114091077446937\n",
      "Epoch 38: train loss: 0.043469074642779874, val loss: 0.040086851716041566\n",
      "Epoch 39: train loss: 0.043942297858243086, val loss: 0.04274589902162552\n",
      "Epoch 40: train loss: 0.04306608919282349, val loss: 0.040692197561264035\n",
      "Epoch 41: train loss: 0.042913326237274674, val loss: 0.04345190033316612\n",
      "Epoch 42: train loss: 0.04333086852273162, val loss: 0.04643535703420639\n",
      "Epoch 43: train loss: 0.043948387307780126, val loss: 0.04065920805931091\n",
      "Epoch 00044: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 44: train loss: 0.04350576952890474, val loss: 0.04187485685944557\n",
      "Epoch 45: train loss: 0.04284737691830615, val loss: 0.04259203720092773\n",
      "Epoch 46: train loss: 0.04300998883466332, val loss: 0.04156916245818138\n",
      "Epoch 47: train loss: 0.042966233751603536, val loss: 0.04271007600426674\n",
      "Epoch 48: train loss: 0.042550568252193686, val loss: 0.040234786301851275\n",
      "Epoch 49: train loss: 0.04316241043623613, val loss: 0.04147401371598244\n",
      "Epoch 00050: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 50: train loss: 0.04287095862687851, val loss: 0.04629239764809608\n",
      "Epoch 51: train loss: 0.042402822625272125, val loss: 0.042109454870224\n",
      "Epoch 52: train loss: 0.04296437546854116, val loss: 0.04334590312838554\n",
      "Epoch 53: train loss: 0.042935399832165974, val loss: 0.03814089971780777\n",
      "Epoch 54: train loss: 0.042729685642889564, val loss: 0.04548328977823257\n",
      "Epoch 55: train loss: 0.0423716945228528, val loss: 0.04337413230538368\n",
      "Epoch 56: train loss: 0.042593687506962796, val loss: 0.04440732863545418\n",
      "Epoch 57: train loss: 0.04234170943620254, val loss: 0.042828355014324186\n",
      "Epoch 58: train loss: 0.04260512939217139, val loss: 0.04566440850496292\n",
      "Epoch 00059: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 59: train loss: 0.04284644370784565, val loss: 0.04228685694932938\n",
      "Epoch 60: train loss: 0.04276795066376122, val loss: 0.04228744566440582\n",
      "Epoch 61: train loss: 0.04193717784601815, val loss: 0.03999054443836212\n",
      "Epoch 62: train loss: 0.04225110485541577, val loss: 0.04106574127078056\n",
      "Epoch 63: train loss: 0.04223685750669363, val loss: 0.045287586808204654\n",
      "Epoch 64: train loss: 0.042196937432702704, val loss: 0.04427773967385292\n",
      "Epoch 00065: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 65: train loss: 0.04264871274512641, val loss: 0.039916795551776886\n",
      "Epoch 66: train loss: 0.04252664359491699, val loss: 0.04728981214761734\n",
      "Epoch 67: train loss: 0.04274288648914318, val loss: 0.043991871923208234\n",
      "Epoch 68: train loss: 0.042470371152065235, val loss: 0.04333378100395203\n",
      "Epoch 69: train loss: 0.04217716064562603, val loss: 0.043337232649326325\n",
      "Epoch 70: train loss: 0.042280813558977476, val loss: 0.044589636445045475\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 71: train loss: 0.042503961335639565, val loss: 0.04482479652762413\n",
      "Epoch 72: train loss: 0.04239190623711567, val loss: 0.04118951386213303\n",
      "Epoch 73: train loss: 0.04259867694913125, val loss: 0.04773313897848129\n",
      "Epoch 74: train loss: 0.04305064396529781, val loss: 0.04321135067939758\n",
      "Epoch 75: train loss: 0.042419151095711455, val loss: 0.04433861249685288\n",
      "Epoch 76: train loss: 0.04251171116865411, val loss: 0.04281331461668014\n",
      "Epoch 00077: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 77: train loss: 0.042312294274568556, val loss: 0.04215810111165047\n",
      "Early stop at epoch 77\n"
     ]
    }
   ],
   "source": [
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "def get_regression_model():\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    model = ElasticNet(alpha=0.01, l1_ratio=0.01, max_iter=100000)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-nn-lr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Predictions saved to results-nn-lr.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "regression_model = get_regression_model()\n",
    "pipeline = Pipeline([\n",
    "    (\"featureExtractor\", pretrainedfeatures),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", regression_model)\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "print(regression_model.coef_.shape)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = pipeline.predict(x_test.to_numpy())\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
