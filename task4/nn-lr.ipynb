{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "#     x_train = scaler.transform(x_train)\n",
    "#     x_test_transed = scaler.transform(x_test)\n",
    "#     x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(10, 1),\n",
    "            )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        return self.seq[:-4](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = NN()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 200\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x).detach().cpu().numpy()\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7264a08e8a1e47d4899d00dc2e858a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 4.0283463814793805, val loss: 1.5693160696029662\n",
      "Epoch 2: train loss: 0.9947594458229688, val loss: 0.5442820992469788\n",
      "Epoch 3: train loss: 0.3369163450416254, val loss: 0.2153931427001953\n",
      "Epoch 4: train loss: 0.15638039969059886, val loss: 0.122977448284626\n",
      "Epoch 5: train loss: 0.10005794372728893, val loss: 0.0792028387784958\n",
      "Epoch 6: train loss: 0.0832228323525312, val loss: 0.07874281734228133\n",
      "Epoch 7: train loss: 0.07749177219308152, val loss: 0.06954656332731247\n",
      "Epoch 8: train loss: 0.07360066453048161, val loss: 0.07211814796924591\n",
      "Epoch 9: train loss: 0.0709004591131697, val loss: 0.06663256853818894\n",
      "Epoch 10: train loss: 0.06988233342584299, val loss: 0.06745617401599883\n",
      "Epoch 11: train loss: 0.068205594691695, val loss: 0.0673437911272049\n",
      "Epoch 12: train loss: 0.06719756874016353, val loss: 0.06493486413359642\n",
      "Epoch 13: train loss: 0.06721595986521973, val loss: 0.0623188002705574\n",
      "Epoch 14: train loss: 0.06653058039533849, val loss: 0.06535713568329811\n",
      "Epoch 15: train loss: 0.06450197117425957, val loss: 0.06281825584173202\n",
      "Epoch 16: train loss: 0.06059524672615285, val loss: 0.058030773788690565\n",
      "Epoch 17: train loss: 0.05899227511152929, val loss: 0.054950512558221816\n",
      "Epoch 18: train loss: 0.05657565007282763, val loss: 0.04831928551197052\n",
      "Epoch 19: train loss: 0.054297599459789236, val loss: 0.052420559763908386\n",
      "Epoch 20: train loss: 0.053383047574028675, val loss: 0.05034477952122688\n",
      "Epoch 21: train loss: 0.05262689557428263, val loss: 0.05379458004236221\n",
      "Epoch 22: train loss: 0.05230491700525187, val loss: 0.05098906430602074\n",
      "Epoch 23: train loss: 0.052044284476309405, val loss: 0.053463070780038834\n",
      "Epoch 00024: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 24: train loss: 0.051764203525319394, val loss: 0.0493325420320034\n",
      "Epoch 25: train loss: 0.050700682497146175, val loss: 0.05132555091381073\n",
      "Epoch 26: train loss: 0.049876996588950256, val loss: 0.04425506964325905\n",
      "Epoch 27: train loss: 0.04840057998104971, val loss: 0.043349046289920805\n",
      "Epoch 28: train loss: 0.04860886092392766, val loss: 0.04746718809008598\n",
      "Epoch 29: train loss: 0.0480651129733543, val loss: 0.04807822540402412\n",
      "Epoch 30: train loss: 0.0481717215691294, val loss: 0.0477856502532959\n",
      "Epoch 31: train loss: 0.048269932012168726, val loss: 0.045145527839660644\n",
      "Epoch 32: train loss: 0.04770582942938318, val loss: 0.04361614394187927\n",
      "Epoch 00033: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 33: train loss: 0.04702578894337829, val loss: 0.047253078281879426\n",
      "Epoch 34: train loss: 0.047178920587106626, val loss: 0.04577086767554283\n",
      "Epoch 35: train loss: 0.047496540091475664, val loss: 0.05008600202202797\n",
      "Epoch 36: train loss: 0.04724262803184743, val loss: 0.04364112460613251\n",
      "Epoch 37: train loss: 0.04752400843403777, val loss: 0.04284859621524811\n",
      "Epoch 38: train loss: 0.047384528958675814, val loss: 0.044437340289354324\n",
      "Epoch 39: train loss: 0.046975653419689255, val loss: 0.042110546678304675\n",
      "Epoch 40: train loss: 0.04700863538956156, val loss: 0.04715960389375687\n",
      "Epoch 41: train loss: 0.04670014744449635, val loss: 0.04654401218891144\n",
      "Epoch 42: train loss: 0.04688292230057473, val loss: 0.04770288527011871\n",
      "Epoch 43: train loss: 0.047024710365095916, val loss: 0.0424782917201519\n",
      "Epoch 44: train loss: 0.0464727866953733, val loss: 0.046318323075771335\n",
      "Epoch 00045: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 45: train loss: 0.04621792091398823, val loss: 0.04726593309640884\n",
      "Epoch 46: train loss: 0.04702565483353576, val loss: 0.04664377954602242\n",
      "Epoch 47: train loss: 0.046322424698240904, val loss: 0.047635300010442735\n",
      "Epoch 48: train loss: 0.046142480815551716, val loss: 0.04521978756785393\n",
      "Epoch 49: train loss: 0.04642656044388304, val loss: 0.044246007055044174\n",
      "Epoch 50: train loss: 0.046679272983755384, val loss: 0.041837537586689\n",
      "Epoch 51: train loss: 0.04610821973061075, val loss: 0.044685215085744856\n",
      "Epoch 52: train loss: 0.04580017300102175, val loss: 0.04530830338597298\n",
      "Epoch 53: train loss: 0.045562719454570695, val loss: 0.04304601815342903\n",
      "Epoch 54: train loss: 0.04573356061079064, val loss: 0.045626361936330795\n",
      "Epoch 55: train loss: 0.04560559973546437, val loss: 0.04624975311756134\n",
      "Epoch 00056: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 56: train loss: 0.04609837326224969, val loss: 0.045495761781930925\n",
      "Epoch 57: train loss: 0.04546353564882765, val loss: 0.045826207727193834\n",
      "Epoch 58: train loss: 0.04638161176078173, val loss: 0.04286327105760574\n",
      "Epoch 59: train loss: 0.04563541180381969, val loss: 0.044728651881217954\n",
      "Epoch 60: train loss: 0.04568106300611885, val loss: 0.04874403101205826\n",
      "Epoch 61: train loss: 0.04585667693250033, val loss: 0.04066508412361145\n",
      "Epoch 62: train loss: 0.04592307487860018, val loss: 0.04184206771850586\n",
      "Epoch 63: train loss: 0.0455818148699342, val loss: 0.04187601393461227\n",
      "Epoch 64: train loss: 0.045494562117420895, val loss: 0.044661547392606735\n",
      "Epoch 65: train loss: 0.04573956497287264, val loss: 0.04342611438035965\n",
      "Epoch 66: train loss: 0.04587233023254239, val loss: 0.04442897161841392\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 67: train loss: 0.04592013210605602, val loss: 0.04403097468614578\n",
      "Epoch 68: train loss: 0.0458611170679939, val loss: 0.047963044792413714\n",
      "Epoch 69: train loss: 0.046034505765048826, val loss: 0.040965577244758605\n",
      "Epoch 70: train loss: 0.04553188808657685, val loss: 0.04170100620388985\n",
      "Epoch 71: train loss: 0.04553632334726197, val loss: 0.0419266614317894\n",
      "Epoch 72: train loss: 0.045566186431719335, val loss: 0.04560792663693428\n",
      "Epoch 00073: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 73: train loss: 0.04540691730015132, val loss: 0.04870921394228935\n",
      "Epoch 74: train loss: 0.045585518505500286, val loss: 0.04453904616832733\n",
      "Epoch 75: train loss: 0.04540846838513199, val loss: 0.047208643585443494\n",
      "Epoch 76: train loss: 0.04579849652246553, val loss: 0.04533928647637367\n",
      "Epoch 77: train loss: 0.04581002685488487, val loss: 0.042159029394388196\n",
      "Epoch 78: train loss: 0.0457595074328841, val loss: 0.04424660289287567\n",
      "Epoch 00079: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 79: train loss: 0.04599460887300725, val loss: 0.04500240460038185\n",
      "Epoch 80: train loss: 0.044800980690790686, val loss: 0.0421922242641449\n",
      "Epoch 81: train loss: 0.04531630779772389, val loss: 0.044865138977766034\n",
      "Epoch 82: train loss: 0.04518419834606501, val loss: 0.044224307715892794\n",
      "Epoch 83: train loss: 0.04530797151278476, val loss: 0.04132340177893638\n",
      "Epoch 84: train loss: 0.04627124537862077, val loss: 0.04270368435978889\n",
      "Epoch 85: train loss: 0.0454416810036922, val loss: 0.040053128510713576\n",
      "Epoch 86: train loss: 0.04510060680764062, val loss: 0.04649006450176239\n",
      "Epoch 87: train loss: 0.044776887786631685, val loss: 0.04166830509901047\n",
      "Epoch 88: train loss: 0.045344239935582994, val loss: 0.041618626594543455\n",
      "Epoch 89: train loss: 0.045170183015112976, val loss: 0.04407963076233864\n",
      "Epoch 90: train loss: 0.04603979002821202, val loss: 0.043900540441274644\n",
      "Epoch 00091: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 91: train loss: 0.0454876922466317, val loss: 0.04406942105293274\n",
      "Early stop at epoch 91\n"
     ]
    }
   ],
   "source": [
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "def get_regression_model():\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    model = ElasticNet(alpha=0.01, l1_ratio=0.01, max_iter=100000)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-nn-lr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-nn-lr.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"featureExtractor\", pretrainedfeatures),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", get_regression_model())\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = pipeline.predict(x_test.to_numpy())\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
