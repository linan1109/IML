{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406e7d150a96431e98f47bdb41a12fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.247229183393925, val loss: 1.3342293798923492\n",
      "Epoch 2: train loss: 1.2365530314144844, val loss: 1.32391357421875\n",
      "Epoch 3: train loss: 1.2263977725846205, val loss: 1.3141462206840515\n",
      "Epoch 4: train loss: 1.2167847823644924, val loss: 1.3049388825893402\n",
      "Epoch 5: train loss: 1.2075902427656515, val loss: 1.296008139848709\n",
      "Epoch 6: train loss: 1.1986966206333443, val loss: 1.2871589660644531\n",
      "Epoch 7: train loss: 1.1899612926260703, val loss: 1.2786207497119904\n",
      "Epoch 8: train loss: 1.1813151351924973, val loss: 1.2697889804840088\n",
      "Epoch 9: train loss: 1.1725392781687265, val loss: 1.2610580027103424\n",
      "Epoch 10: train loss: 1.163497551949497, val loss: 1.251717209815979\n",
      "Epoch 11: train loss: 1.1539541384395686, val loss: 1.2420997321605682\n",
      "Epoch 12: train loss: 1.143664943981077, val loss: 1.2311311066150665\n",
      "Epoch 13: train loss: 1.1324066995834454, val loss: 1.2193787395954132\n",
      "Epoch 14: train loss: 1.1199405128680893, val loss: 1.2061858475208282\n",
      "Epoch 15: train loss: 1.1060350235674514, val loss: 1.191615492105484\n",
      "Epoch 16: train loss: 1.0905721002020738, val loss: 1.1754183769226074\n",
      "Epoch 17: train loss: 1.073544425946441, val loss: 1.1576622426509857\n",
      "Epoch 18: train loss: 1.055142755915004, val loss: 1.1389180719852448\n",
      "Epoch 19: train loss: 1.0358644309522123, val loss: 1.119746372103691\n",
      "Epoch 20: train loss: 1.0163640945185948, val loss: 1.1005121022462845\n",
      "Epoch 21: train loss: 0.9974261854879609, val loss: 1.0822727680206299\n",
      "Epoch 22: train loss: 0.9798135810021441, val loss: 1.065844178199768\n",
      "Epoch 23: train loss: 0.963978680684963, val loss: 1.0511894524097443\n",
      "Epoch 24: train loss: 0.9502189117138466, val loss: 1.0385852605104446\n",
      "Epoch 25: train loss: 0.9385060469958565, val loss: 1.0280149281024933\n",
      "Epoch 26: train loss: 0.928640110766876, val loss: 1.0190137922763824\n",
      "Epoch 27: train loss: 0.920363107758852, val loss: 1.011607676744461\n",
      "Epoch 28: train loss: 0.9133605553565638, val loss: 1.0051139742136002\n",
      "Epoch 29: train loss: 0.907373601920618, val loss: 0.999743327498436\n",
      "Epoch 30: train loss: 0.9022206090432061, val loss: 0.9950645267963409\n",
      "Epoch 31: train loss: 0.897678953376092, val loss: 0.99074287712574\n",
      "Epoch 32: train loss: 0.893646405398242, val loss: 0.986958771944046\n",
      "Epoch 33: train loss: 0.8900039716118966, val loss: 0.9835962802171707\n",
      "Epoch 34: train loss: 0.8866829469249904, val loss: 0.9803723841905594\n",
      "Epoch 35: train loss: 0.8836065065973687, val loss: 0.9775420874357224\n",
      "Epoch 36: train loss: 0.8807443713137076, val loss: 0.974784329533577\n",
      "Epoch 37: train loss: 0.878082430136823, val loss: 0.9721368700265884\n",
      "Epoch 38: train loss: 0.8755392956990737, val loss: 0.96978560090065\n",
      "Epoch 39: train loss: 0.8731615930863823, val loss: 0.9673846215009689\n",
      "Epoch 40: train loss: 0.8708831360043361, val loss: 0.9652030616998672\n",
      "Epoch 41: train loss: 0.8687150324142591, val loss: 0.9630365073680878\n",
      "Epoch 42: train loss: 0.8666103023479516, val loss: 0.9609703123569489\n",
      "Epoch 43: train loss: 0.8646219695210262, val loss: 0.9591455459594727\n",
      "Epoch 44: train loss: 0.862696916887239, val loss: 0.9572107940912247\n",
      "Epoch 45: train loss: 0.8608476982440718, val loss: 0.9552833437919617\n",
      "Epoch 46: train loss: 0.8590547097776887, val loss: 0.9535877406597137\n",
      "Epoch 47: train loss: 0.8573180763248679, val loss: 0.9518966227769852\n",
      "Epoch 48: train loss: 0.8556531598388039, val loss: 0.9501773118972778\n",
      "Epoch 49: train loss: 0.8540519329505355, val loss: 0.9486804306507111\n",
      "Epoch 50: train loss: 0.8524821875376547, val loss: 0.9471075385808945\n",
      "Epoch 51: train loss: 0.8509626989853768, val loss: 0.9455837905406952\n",
      "Epoch 52: train loss: 0.84949449422273, val loss: 0.9440534114837646\n",
      "Epoch 53: train loss: 0.84805246401908, val loss: 0.9428022056818008\n",
      "Epoch 54: train loss: 0.8466767937636227, val loss: 0.9413861483335495\n",
      "Epoch 55: train loss: 0.8453236780926824, val loss: 0.9399339705705643\n",
      "Epoch 56: train loss: 0.8440067864484703, val loss: 0.9386422783136368\n",
      "Epoch 57: train loss: 0.8427413340216172, val loss: 0.9373615980148315\n",
      "Epoch 58: train loss: 0.84149219854639, val loss: 0.9361963421106339\n",
      "Epoch 59: train loss: 0.8402780865929712, val loss: 0.934943363070488\n",
      "Epoch 60: train loss: 0.8390921165023275, val loss: 0.9337128698825836\n",
      "Epoch 61: train loss: 0.8379300431035306, val loss: 0.9326373040676117\n",
      "Epoch 62: train loss: 0.836813619618944, val loss: 0.9313990771770477\n",
      "Epoch 63: train loss: 0.8357144215456687, val loss: 0.9303796887397766\n",
      "Epoch 64: train loss: 0.834660345921521, val loss: 0.9293526262044907\n",
      "Epoch 65: train loss: 0.8336032069185052, val loss: 0.9282741844654083\n",
      "Epoch 66: train loss: 0.8325512266088957, val loss: 0.9272384941577911\n",
      "Epoch 67: train loss: 0.831570435138903, val loss: 0.9262217730283737\n",
      "Epoch 68: train loss: 0.830581686322263, val loss: 0.92522794008255\n",
      "Epoch 69: train loss: 0.8296239480439529, val loss: 0.9243694990873337\n",
      "Epoch 70: train loss: 0.8286871502540424, val loss: 0.9233247637748718\n",
      "Epoch 71: train loss: 0.8277578602542833, val loss: 0.9224606454372406\n",
      "Epoch 72: train loss: 0.8268623813543784, val loss: 0.921524316072464\n",
      "Epoch 73: train loss: 0.8259628193902642, val loss: 0.9207779616117477\n",
      "Epoch 74: train loss: 0.8250944809133186, val loss: 0.9196791648864746\n",
      "Epoch 75: train loss: 0.8242406193355609, val loss: 0.9190087169408798\n",
      "Epoch 76: train loss: 0.8234198412759676, val loss: 0.9181094318628311\n",
      "Epoch 77: train loss: 0.8225942656169021, val loss: 0.9172815829515457\n",
      "Epoch 78: train loss: 0.8217883139060081, val loss: 0.9165239185094833\n",
      "Epoch 79: train loss: 0.8209947382885815, val loss: 0.9156645387411118\n",
      "Epoch 80: train loss: 0.8202210521043729, val loss: 0.914956197142601\n",
      "Epoch 81: train loss: 0.8194652661238181, val loss: 0.9141440391540527\n",
      "Epoch 82: train loss: 0.8187169707199203, val loss: 0.9133587926626205\n",
      "Epoch 83: train loss: 0.8179802070914901, val loss: 0.9126818925142288\n",
      "Epoch 84: train loss: 0.8172536574715455, val loss: 0.9119305908679962\n",
      "Epoch 85: train loss: 0.816530855603173, val loss: 0.9111896008253098\n",
      "Epoch 86: train loss: 0.8158494802286173, val loss: 0.9104399234056473\n",
      "Epoch 87: train loss: 0.8151629475626592, val loss: 0.9098632633686066\n",
      "Epoch 88: train loss: 0.8144834087355314, val loss: 0.9091725200414658\n",
      "Epoch 89: train loss: 0.8138158118868918, val loss: 0.908505991101265\n",
      "Epoch 90: train loss: 0.8131598756423147, val loss: 0.9077422916889191\n",
      "Epoch 91: train loss: 0.8125159848519145, val loss: 0.907169759273529\n",
      "Epoch 92: train loss: 0.8118968063108051, val loss: 0.9065508544445038\n",
      "Epoch 93: train loss: 0.8112640543063455, val loss: 0.9058881998062134\n",
      "Epoch 94: train loss: 0.8106579499204187, val loss: 0.9052907377481461\n",
      "Epoch 95: train loss: 0.8100685598179626, val loss: 0.9046923816204071\n",
      "Epoch 96: train loss: 0.8094642168359404, val loss: 0.9040767550468445\n",
      "Epoch 97: train loss: 0.8088666513557334, val loss: 0.9035852551460266\n",
      "Epoch 98: train loss: 0.808289251289349, val loss: 0.9029112160205841\n",
      "Epoch 99: train loss: 0.8077386031544936, val loss: 0.9022650867700577\n",
      "Epoch 100: train loss: 0.8071769159122917, val loss: 0.9017549455165863\n",
      "Epoch 101: train loss: 0.8066387435494977, val loss: 0.9013146907091141\n",
      "Epoch 102: train loss: 0.8060963850557239, val loss: 0.9006667584180832\n",
      "Epoch 103: train loss: 0.8055718970898513, val loss: 0.900184765458107\n",
      "Epoch 104: train loss: 0.8050596896864471, val loss: 0.8997320085763931\n",
      "Epoch 105: train loss: 0.8045515090139502, val loss: 0.8991502672433853\n",
      "Epoch 106: train loss: 0.8040541653070759, val loss: 0.8987344652414322\n",
      "Epoch 107: train loss: 0.8035621548625855, val loss: 0.8982535153627396\n",
      "Epoch 108: train loss: 0.8030856963817842, val loss: 0.8976825922727585\n",
      "Epoch 109: train loss: 0.8026164390027465, val loss: 0.8972085118293762\n",
      "Epoch 110: train loss: 0.802148728696243, val loss: 0.8968473970890045\n",
      "Epoch 111: train loss: 0.801703433435908, val loss: 0.8962037414312363\n",
      "Epoch 112: train loss: 0.8012571367941741, val loss: 0.8958396762609482\n",
      "Epoch 113: train loss: 0.8008094707839203, val loss: 0.8954548984766006\n",
      "Epoch 114: train loss: 0.800380592971641, val loss: 0.8950372338294983\n",
      "Epoch 115: train loss: 0.7999464575662523, val loss: 0.8945982307195663\n",
      "Epoch 116: train loss: 0.7995263490946376, val loss: 0.8941732943058014\n",
      "Epoch 117: train loss: 0.7991246334363806, val loss: 0.8937157988548279\n",
      "Epoch 118: train loss: 0.7987301254809895, val loss: 0.8932938724756241\n",
      "Epoch 119: train loss: 0.7983388681304256, val loss: 0.8929579854011536\n",
      "Epoch 120: train loss: 0.797955407713878, val loss: 0.8925700038671494\n",
      "Epoch 121: train loss: 0.7975644552018354, val loss: 0.892156794667244\n",
      "Epoch 122: train loss: 0.797194037551157, val loss: 0.8917957097291946\n",
      "Epoch 123: train loss: 0.7968376044625747, val loss: 0.8914405256509781\n",
      "Epoch 124: train loss: 0.7964672331169749, val loss: 0.8910344839096069\n",
      "Epoch 125: train loss: 0.7961307329626809, val loss: 0.8907280564308167\n",
      "Epoch 126: train loss: 0.7957727955626881, val loss: 0.8903483897447586\n",
      "Epoch 127: train loss: 0.7954323021113892, val loss: 0.8900825828313828\n",
      "Epoch 128: train loss: 0.7951086100235908, val loss: 0.8897371143102646\n",
      "Epoch 129: train loss: 0.7947664269626783, val loss: 0.8894083946943283\n",
      "Epoch 130: train loss: 0.7944516670037935, val loss: 0.889007106423378\n",
      "Epoch 131: train loss: 0.79413320367066, val loss: 0.8887256681919098\n",
      "Epoch 132: train loss: 0.7938202529395962, val loss: 0.8883855789899826\n",
      "Epoch 133: train loss: 0.7935217904990033, val loss: 0.8881033062934875\n",
      "Epoch 134: train loss: 0.7932205164561043, val loss: 0.8877258449792862\n",
      "Epoch 135: train loss: 0.7929258685375264, val loss: 0.8874402791261673\n",
      "Epoch 136: train loss: 0.7926285161920639, val loss: 0.8871732801198959\n",
      "Epoch 137: train loss: 0.7923529571039698, val loss: 0.886913001537323\n",
      "Epoch 138: train loss: 0.7920679967802983, val loss: 0.8866755068302155\n",
      "Epoch 139: train loss: 0.7917871911788212, val loss: 0.8862903267145157\n",
      "Epoch 140: train loss: 0.791515540955932, val loss: 0.8860786706209183\n",
      "Epoch 141: train loss: 0.7912455682698436, val loss: 0.885788694024086\n",
      "Epoch 142: train loss: 0.7909835621407904, val loss: 0.8855629563331604\n",
      "Epoch 143: train loss: 0.790713866525754, val loss: 0.8852571398019791\n",
      "Epoch 144: train loss: 0.7904643350588423, val loss: 0.8850604295730591\n",
      "Epoch 145: train loss: 0.7902070302583158, val loss: 0.8848392069339752\n",
      "Epoch 146: train loss: 0.7899594650242072, val loss: 0.884538471698761\n",
      "Epoch 147: train loss: 0.7897169629525064, val loss: 0.8842659294605255\n",
      "Epoch 148: train loss: 0.7894719943467483, val loss: 0.8840463906526566\n",
      "Epoch 149: train loss: 0.7892418835374196, val loss: 0.8838200718164444\n",
      "Epoch 150: train loss: 0.7890021150620768, val loss: 0.8836438804864883\n",
      "Epoch 151: train loss: 0.7887726624547091, val loss: 0.8833096772432327\n",
      "Epoch 152: train loss: 0.7885481061323216, val loss: 0.8831849545240402\n",
      "Epoch 153: train loss: 0.788315535584761, val loss: 0.8828789293766022\n",
      "Epoch 154: train loss: 0.7880882640089019, val loss: 0.8826608806848526\n",
      "Epoch 155: train loss: 0.7878778231217947, val loss: 0.8824065625667572\n",
      "Epoch 156: train loss: 0.7876610263661207, val loss: 0.8822616934776306\n",
      "Epoch 157: train loss: 0.7874541341070644, val loss: 0.882103830575943\n",
      "Epoch 158: train loss: 0.7872418888796782, val loss: 0.8818709254264832\n",
      "Epoch 159: train loss: 0.7870435917233377, val loss: 0.8816868513822556\n",
      "Epoch 160: train loss: 0.7868345372624663, val loss: 0.8814312815666199\n",
      "Epoch 161: train loss: 0.7866296203297675, val loss: 0.8811818659305573\n",
      "Epoch 162: train loss: 0.7864376934320075, val loss: 0.8810947239398956\n",
      "Epoch 163: train loss: 0.7862409889133333, val loss: 0.8808461427688599\n",
      "Epoch 164: train loss: 0.7860539741120437, val loss: 0.8806828111410141\n",
      "Epoch 165: train loss: 0.7858608248030504, val loss: 0.8805027306079865\n",
      "Epoch 166: train loss: 0.7856765862813536, val loss: 0.8802803009748459\n",
      "Epoch 167: train loss: 0.7854868922113633, val loss: 0.8800943195819855\n",
      "Epoch 168: train loss: 0.7852970666645478, val loss: 0.8799684941768646\n",
      "Epoch 169: train loss: 0.78512257617971, val loss: 0.879711702466011\n",
      "Epoch 170: train loss: 0.7849585233814224, val loss: 0.8795024454593658\n",
      "Epoch 171: train loss: 0.7847795279265151, val loss: 0.8793578445911407\n",
      "Epoch 172: train loss: 0.7845938967863978, val loss: 0.879194900393486\n",
      "Epoch 173: train loss: 0.7844233985275897, val loss: 0.8789598345756531\n",
      "Epoch 174: train loss: 0.7842532384438439, val loss: 0.87881900370121\n",
      "Epoch 175: train loss: 0.7840807147565054, val loss: 0.8786396831274033\n",
      "Epoch 176: train loss: 0.7839252270930969, val loss: 0.8785278797149658\n",
      "Epoch 177: train loss: 0.7837581184906571, val loss: 0.8783885836601257\n",
      "Epoch 178: train loss: 0.78359418371385, val loss: 0.8782110065221786\n",
      "Epoch 179: train loss: 0.7834298401360261, val loss: 0.8779651522636414\n",
      "Epoch 180: train loss: 0.7832848877931724, val loss: 0.877891406416893\n",
      "Epoch 181: train loss: 0.7831243240377631, val loss: 0.8777020573616028\n",
      "Epoch 182: train loss: 0.7829861499171987, val loss: 0.8775397837162018\n",
      "Epoch 183: train loss: 0.7828250642852509, val loss: 0.8774036914110184\n",
      "Epoch 184: train loss: 0.7826781928130359, val loss: 0.8772222697734833\n",
      "Epoch 185: train loss: 0.7825273267586454, val loss: 0.8771026879549026\n",
      "Epoch 186: train loss: 0.7823756991434393, val loss: 0.8770297914743423\n",
      "Epoch 187: train loss: 0.7822316262076314, val loss: 0.8768695890903473\n",
      "Epoch 188: train loss: 0.7820795792958504, val loss: 0.8766916990280151\n",
      "Epoch 189: train loss: 0.7819456510861611, val loss: 0.8765603452920914\n",
      "Epoch 190: train loss: 0.7818018010495262, val loss: 0.8764302134513855\n",
      "Epoch 191: train loss: 0.7816696307776761, val loss: 0.8762054741382599\n",
      "Epoch 192: train loss: 0.7815330540265417, val loss: 0.8761985898017883\n",
      "Epoch 193: train loss: 0.7813965245540997, val loss: 0.8760092109441757\n",
      "Epoch 194: train loss: 0.7812672957841262, val loss: 0.8760576546192169\n",
      "Epoch 195: train loss: 0.7811255805518105, val loss: 0.8757961839437485\n",
      "Epoch 196: train loss: 0.7810106160632149, val loss: 0.8756052255630493\n",
      "Epoch 197: train loss: 0.7808750298863647, val loss: 0.8754462599754333\n",
      "Epoch 198: train loss: 0.7807477984074789, val loss: 0.8753447085618973\n",
      "Epoch 199: train loss: 0.7806246170651792, val loss: 0.8751842528581619\n",
      "Epoch 200: train loss: 0.7805023601887779, val loss: 0.875062569975853\n",
      "Epoch 201: train loss: 0.7803906790768543, val loss: 0.8750057965517044\n",
      "Epoch 202: train loss: 0.7802534971467734, val loss: 0.8748300820589066\n",
      "Epoch 203: train loss: 0.7801345566136119, val loss: 0.8747004866600037\n",
      "Epoch 204: train loss: 0.7800198079362799, val loss: 0.8745916485786438\n",
      "Epoch 205: train loss: 0.7798996170686374, val loss: 0.8745082765817642\n",
      "Epoch 206: train loss: 0.779779339451994, val loss: 0.8744005858898163\n",
      "Epoch 207: train loss: 0.7796664711736613, val loss: 0.8742825239896774\n",
      "Epoch 208: train loss: 0.7795581151480302, val loss: 0.8740772306919098\n",
      "Epoch 209: train loss: 0.7794363227828349, val loss: 0.8740205764770508\n",
      "Epoch 210: train loss: 0.7793295681768677, val loss: 0.8739472031593323\n",
      "Epoch 211: train loss: 0.7792186926915263, val loss: 0.8738012611865997\n",
      "Epoch 212: train loss: 0.7790975389197075, val loss: 0.8737020492553711\n",
      "Epoch 213: train loss: 0.7790003991407415, val loss: 0.8736283183097839\n",
      "Epoch 214: train loss: 0.7788915158252785, val loss: 0.8734719306230545\n",
      "Epoch 215: train loss: 0.7787887741249008, val loss: 0.8733399510383606\n",
      "Epoch 216: train loss: 0.7786809173490206, val loss: 0.873306393623352\n",
      "Epoch 217: train loss: 0.7785832599190005, val loss: 0.8731373697519302\n",
      "Epoch 218: train loss: 0.7784764962584237, val loss: 0.8730339109897614\n",
      "Epoch 219: train loss: 0.7783752460177527, val loss: 0.8729954808950424\n",
      "Epoch 220: train loss: 0.7782616220592323, val loss: 0.8728870153427124\n",
      "Epoch 221: train loss: 0.7781786963426377, val loss: 0.8728127181529999\n",
      "Epoch 222: train loss: 0.7780672579916114, val loss: 0.8726654499769211\n",
      "Epoch 223: train loss: 0.7779886085821187, val loss: 0.8726336061954498\n",
      "Epoch 224: train loss: 0.7778824447456119, val loss: 0.8724898248910904\n",
      "Epoch 225: train loss: 0.7777877545987653, val loss: 0.87237848341465\n",
      "Epoch 226: train loss: 0.7776966590150748, val loss: 0.8723223656415939\n",
      "Epoch 227: train loss: 0.7775947092327952, val loss: 0.8722400218248367\n",
      "Epoch 228: train loss: 0.777504223150897, val loss: 0.8720397353172302\n",
      "Epoch 229: train loss: 0.7774090710242407, val loss: 0.8719533383846283\n",
      "Epoch 230: train loss: 0.7773106541753555, val loss: 0.8719118982553482\n",
      "Epoch 231: train loss: 0.7772285978765279, val loss: 0.8718412071466446\n",
      "Epoch 232: train loss: 0.7771379351226461, val loss: 0.871695876121521\n",
      "Epoch 233: train loss: 0.7770512379761889, val loss: 0.8716150224208832\n",
      "Epoch 234: train loss: 0.7769661549453212, val loss: 0.8715740144252777\n",
      "Epoch 235: train loss: 0.7768814184236511, val loss: 0.8714353442192078\n",
      "Epoch 236: train loss: 0.7767968341167828, val loss: 0.871365025639534\n",
      "Epoch 237: train loss: 0.7767045522039913, val loss: 0.8713284283876419\n",
      "Epoch 238: train loss: 0.7766295059449296, val loss: 0.8711820840835571\n",
      "Epoch 239: train loss: 0.776553170543136, val loss: 0.871145635843277\n",
      "Epoch 240: train loss: 0.7764570298088956, val loss: 0.8709783852100372\n",
      "Epoch 241: train loss: 0.7763744878324953, val loss: 0.8709432780742645\n",
      "Epoch 242: train loss: 0.77629766095039, val loss: 0.8708391934633255\n",
      "Epoch 243: train loss: 0.7762255652867591, val loss: 0.8708157539367676\n",
      "Epoch 244: train loss: 0.77614461923884, val loss: 0.8707013726234436\n",
      "Epoch 245: train loss: 0.7760674122383843, val loss: 0.8707052916288376\n",
      "Epoch 246: train loss: 0.775987609855804, val loss: 0.8705549985170364\n",
      "Epoch 247: train loss: 0.7759106113839795, val loss: 0.8705121427774429\n",
      "Epoch 248: train loss: 0.7758314705136475, val loss: 0.8704128861427307\n",
      "Epoch 249: train loss: 0.7757497421864084, val loss: 0.8702924847602844\n",
      "Epoch 250: train loss: 0.7756812397034017, val loss: 0.8702405244112015\n",
      "Epoch 251: train loss: 0.7756002935775936, val loss: 0.8701462894678116\n",
      "Epoch 252: train loss: 0.7755222599594392, val loss: 0.8700977265834808\n",
      "Epoch 253: train loss: 0.77546152749976, val loss: 0.8700219243764877\n",
      "Epoch 254: train loss: 0.7753790255583645, val loss: 0.8699863106012344\n",
      "Epoch 255: train loss: 0.7753134112816081, val loss: 0.8698289692401886\n",
      "Epoch 256: train loss: 0.7752347505003049, val loss: 0.8698215782642365\n",
      "Epoch 257: train loss: 0.7751730045864139, val loss: 0.8697005063295364\n",
      "Epoch 258: train loss: 0.7750955883563246, val loss: 0.8696884661912918\n",
      "Epoch 259: train loss: 0.7750359218481202, val loss: 0.8696205615997314\n",
      "Epoch 260: train loss: 0.7749595384861043, val loss: 0.8695431053638458\n",
      "Epoch 261: train loss: 0.7748827293821894, val loss: 0.8694412410259247\n",
      "Epoch 262: train loss: 0.7748216171351884, val loss: 0.8693787604570389\n",
      "Epoch 263: train loss: 0.7747536937577473, val loss: 0.8692989647388458\n",
      "Epoch 264: train loss: 0.7746934180203624, val loss: 0.8692778497934341\n",
      "Epoch 265: train loss: 0.7746281148522012, val loss: 0.8691974580287933\n",
      "Epoch 266: train loss: 0.7745603380815456, val loss: 0.8691233247518539\n",
      "Epoch 267: train loss: 0.7745025892422815, val loss: 0.869071289896965\n",
      "Epoch 268: train loss: 0.7744385148722609, val loss: 0.8689857572317123\n",
      "Epoch 269: train loss: 0.7743677731551052, val loss: 0.8690388351678848\n",
      "Epoch 270: train loss: 0.7743017991474741, val loss: 0.8688726425170898\n",
      "Epoch 271: train loss: 0.7742468957143919, val loss: 0.8688476532697678\n",
      "Epoch 272: train loss: 0.7741878773020359, val loss: 0.8687925040721893\n",
      "Epoch 273: train loss: 0.7741176479784191, val loss: 0.8686759322881699\n",
      "Epoch 274: train loss: 0.7740576830380729, val loss: 0.8686896860599518\n",
      "Epoch 275: train loss: 0.774001227060746, val loss: 0.8685937970876694\n",
      "Epoch 276: train loss: 0.7739467371993889, val loss: 0.8684941381216049\n",
      "Epoch 277: train loss: 0.7738754501805435, val loss: 0.8684149235486984\n",
      "Epoch 278: train loss: 0.7738290176365119, val loss: 0.8684618920087814\n",
      "Epoch 279: train loss: 0.7737681698814711, val loss: 0.8683070242404938\n",
      "Epoch 280: train loss: 0.7737097163988617, val loss: 0.8682558238506317\n",
      "Epoch 281: train loss: 0.7736500207211838, val loss: 0.8682096004486084\n",
      "Epoch 282: train loss: 0.7735917662452616, val loss: 0.868216872215271\n",
      "Epoch 283: train loss: 0.7735324355754459, val loss: 0.8681157976388931\n",
      "Epoch 284: train loss: 0.7734830481326102, val loss: 0.8680108934640884\n",
      "Epoch 285: train loss: 0.7734240415009827, val loss: 0.8680528402328491\n",
      "Epoch 286: train loss: 0.7733711325744522, val loss: 0.8679562211036682\n",
      "Epoch 287: train loss: 0.773317857627501, val loss: 0.8679477423429489\n",
      "Epoch 288: train loss: 0.7732665115460505, val loss: 0.8678426444530487\n",
      "Epoch 289: train loss: 0.7732216176430566, val loss: 0.8678229004144669\n",
      "Epoch 290: train loss: 0.7731597338264886, val loss: 0.8677160143852234\n",
      "Epoch 291: train loss: 0.7730977565429834, val loss: 0.8676817566156387\n",
      "Epoch 292: train loss: 0.7730542260499921, val loss: 0.867626890540123\n",
      "Epoch 293: train loss: 0.7730020920617329, val loss: 0.8675230741500854\n",
      "Epoch 294: train loss: 0.7729381348564126, val loss: 0.8675397783517838\n",
      "Epoch 295: train loss: 0.772898376299563, val loss: 0.8675028681755066\n",
      "Epoch 296: train loss: 0.7728493658400407, val loss: 0.867477610707283\n",
      "Epoch 297: train loss: 0.7728071698991351, val loss: 0.8674761354923248\n",
      "Epoch 298: train loss: 0.7727448323200591, val loss: 0.8673045337200165\n",
      "Epoch 299: train loss: 0.7727011304515439, val loss: 0.8672640472650528\n",
      "Epoch 300: train loss: 0.7726457867891872, val loss: 0.8671782612800598\n",
      "Epoch 301: train loss: 0.7725924477005504, val loss: 0.8672293126583099\n",
      "Epoch 302: train loss: 0.7725564875621416, val loss: 0.8670577108860016\n",
      "Epoch 303: train loss: 0.7725118435468988, val loss: 0.867160975933075\n",
      "Epoch 304: train loss: 0.772463154633113, val loss: 0.8669871091842651\n",
      "Epoch 305: train loss: 0.7724068092778009, val loss: 0.8669446110725403\n",
      "Epoch 306: train loss: 0.7723577360007078, val loss: 0.8669495731592178\n",
      "Epoch 307: train loss: 0.7723186287136882, val loss: 0.8669300526380539\n",
      "Epoch 308: train loss: 0.7722623890733765, val loss: 0.8668576776981354\n",
      "Epoch 309: train loss: 0.7722120660070265, val loss: 0.8668400049209595\n",
      "Epoch 310: train loss: 0.7721738353230446, val loss: 0.8667701482772827\n",
      "Epoch 311: train loss: 0.7721348040947451, val loss: 0.8667062669992447\n",
      "Epoch 312: train loss: 0.7720845482856454, val loss: 0.8666571080684662\n",
      "Epoch 313: train loss: 0.772042548138501, val loss: 0.8666229695081711\n",
      "Epoch 314: train loss: 0.7720070365635019, val loss: 0.8666367679834366\n",
      "Epoch 315: train loss: 0.7719613983326588, val loss: 0.8665025979280472\n",
      "Epoch 316: train loss: 0.7719143310118173, val loss: 0.8665091842412949\n",
      "Epoch 317: train loss: 0.7718687068010147, val loss: 0.8664572536945343\n",
      "Epoch 318: train loss: 0.7718246809916728, val loss: 0.866481602191925\n",
      "Epoch 319: train loss: 0.7717840205372957, val loss: 0.8663272708654404\n",
      "Epoch 320: train loss: 0.7717586989225184, val loss: 0.8662953674793243\n",
      "Epoch 321: train loss: 0.7716925125696892, val loss: 0.8662982881069183\n",
      "Epoch 322: train loss: 0.7716497549314353, val loss: 0.8662354201078415\n",
      "Epoch 323: train loss: 0.7716177414733963, val loss: 0.8662313222885132\n",
      "Epoch 324: train loss: 0.7715769466992415, val loss: 0.866157278418541\n",
      "Epoch 325: train loss: 0.771536885426349, val loss: 0.8660925030708313\n",
      "Epoch 326: train loss: 0.7714907675348672, val loss: 0.866108700633049\n",
      "Epoch 327: train loss: 0.7714604277970627, val loss: 0.8660017549991608\n",
      "Epoch 328: train loss: 0.7714062191154084, val loss: 0.8660387247800827\n",
      "Epoch 329: train loss: 0.7713748730113015, val loss: 0.8659428209066391\n",
      "Epoch 330: train loss: 0.7713407602274351, val loss: 0.8659217655658722\n",
      "Epoch 331: train loss: 0.771292198642879, val loss: 0.8658993542194366\n",
      "Epoch 332: train loss: 0.7712537119373781, val loss: 0.8658399879932404\n",
      "Epoch 333: train loss: 0.7712198236299395, val loss: 0.8657614290714264\n",
      "Epoch 334: train loss: 0.7711797401975939, val loss: 0.865742638707161\n",
      "Epoch 335: train loss: 0.7711520535772199, val loss: 0.8656984120607376\n",
      "Epoch 336: train loss: 0.7711029906204345, val loss: 0.8656367063522339\n",
      "Epoch 337: train loss: 0.7710750812956104, val loss: 0.8656937628984451\n",
      "Epoch 338: train loss: 0.771026421843382, val loss: 0.8655998259782791\n",
      "Epoch 339: train loss: 0.7709914120300577, val loss: 0.8655479550361633\n",
      "Epoch 340: train loss: 0.7709626648130856, val loss: 0.8655223846435547\n",
      "Epoch 341: train loss: 0.7709172322873935, val loss: 0.8655020594596863\n",
      "Epoch 342: train loss: 0.7708872874863434, val loss: 0.8654881566762924\n",
      "Epoch 343: train loss: 0.7708522833394521, val loss: 0.8653677403926849\n",
      "Epoch 344: train loss: 0.7708100992376918, val loss: 0.8653871864080429\n",
      "Epoch 345: train loss: 0.7707860318713233, val loss: 0.8653874844312668\n",
      "Epoch 346: train loss: 0.7707428989827886, val loss: 0.865338146686554\n",
      "Epoch 347: train loss: 0.7707100707851411, val loss: 0.8652709424495697\n",
      "Epoch 348: train loss: 0.7706825525415602, val loss: 0.8653015941381454\n",
      "Epoch 349: train loss: 0.7706443799823298, val loss: 0.8651725500822067\n",
      "Epoch 350: train loss: 0.7706116685015203, val loss: 0.8651809245347977\n",
      "Epoch 351: train loss: 0.7705659903914506, val loss: 0.8651142120361328\n",
      "Epoch 352: train loss: 0.7705399144011282, val loss: 0.8650952577590942\n",
      "Epoch 353: train loss: 0.7705004543543096, val loss: 0.8650935590267181\n",
      "Epoch 354: train loss: 0.7704801211673347, val loss: 0.8650304824113846\n",
      "Epoch 355: train loss: 0.7704410872323885, val loss: 0.8650595545768738\n",
      "Epoch 356: train loss: 0.7704148532245253, val loss: 0.8650058805942535\n",
      "Epoch 357: train loss: 0.7703787008752545, val loss: 0.8648461848497391\n",
      "Epoch 358: train loss: 0.7703399300497368, val loss: 0.8649011701345444\n",
      "Epoch 359: train loss: 0.7703190277464753, val loss: 0.8648678809404373\n",
      "Epoch 360: train loss: 0.7702838141557555, val loss: 0.8648473024368286\n",
      "Epoch 361: train loss: 0.770248369721173, val loss: 0.8647399991750717\n",
      "Epoch 362: train loss: 0.7702199194759218, val loss: 0.8647638112306595\n",
      "Epoch 363: train loss: 0.7701821310658036, val loss: 0.8647140562534332\n",
      "Epoch 364: train loss: 0.7701605075677599, val loss: 0.864685907959938\n",
      "Epoch 365: train loss: 0.770122940674283, val loss: 0.8646531999111176\n",
      "Epoch 366: train loss: 0.7700860354098881, val loss: 0.8646300733089447\n",
      "Epoch 367: train loss: 0.7700571878327922, val loss: 0.864568367600441\n",
      "Epoch 368: train loss: 0.7700280832021464, val loss: 0.8645728230476379\n",
      "Epoch 369: train loss: 0.7699995065233903, val loss: 0.8645892888307571\n",
      "Epoch 370: train loss: 0.7699759082247408, val loss: 0.8645173013210297\n",
      "Epoch 371: train loss: 0.7699425105603058, val loss: 0.864489808678627\n",
      "Epoch 372: train loss: 0.769914361804344, val loss: 0.8645078539848328\n",
      "Epoch 373: train loss: 0.7698909422027952, val loss: 0.8644132167100906\n",
      "Epoch 374: train loss: 0.7698550602066404, val loss: 0.8643901199102402\n",
      "Epoch 375: train loss: 0.7698144140059712, val loss: 0.8644152283668518\n",
      "Epoch 376: train loss: 0.7698003287916673, val loss: 0.864405170083046\n",
      "Epoch 377: train loss: 0.7697668139041152, val loss: 0.864323616027832\n",
      "Epoch 378: train loss: 0.7697425427408788, val loss: 0.8643182069063187\n",
      "Epoch 379: train loss: 0.7697081377170554, val loss: 0.8642735034227371\n",
      "Epoch 380: train loss: 0.769676078927552, val loss: 0.8642044961452484\n",
      "Epoch 381: train loss: 0.7696559401323343, val loss: 0.8641835302114487\n",
      "Epoch 382: train loss: 0.7696226817336358, val loss: 0.8641682863235474\n",
      "Epoch 383: train loss: 0.7695963806827175, val loss: 0.8640550076961517\n",
      "Epoch 384: train loss: 0.7695780666541991, val loss: 0.8640592843294144\n",
      "Epoch 385: train loss: 0.7695368400043773, val loss: 0.8641048520803452\n",
      "Epoch 386: train loss: 0.7695187013042554, val loss: 0.864014133810997\n",
      "Epoch 387: train loss: 0.7694862580735556, val loss: 0.8640510886907578\n",
      "Epoch 388: train loss: 0.7694634818166193, val loss: 0.8640050292015076\n",
      "Epoch 389: train loss: 0.769434273632396, val loss: 0.8639366030693054\n",
      "Epoch 390: train loss: 0.7693963782999961, val loss: 0.8639317005872726\n",
      "Epoch 391: train loss: 0.7693764059343434, val loss: 0.8638811558485031\n",
      "Epoch 392: train loss: 0.7693606986987049, val loss: 0.8638717532157898\n",
      "Epoch 393: train loss: 0.7693281483510007, val loss: 0.8638774007558823\n",
      "Epoch 394: train loss: 0.7693068873488954, val loss: 0.8637917041778564\n",
      "Epoch 395: train loss: 0.7692880683263039, val loss: 0.8638483583927155\n",
      "Epoch 396: train loss: 0.7692550896663597, val loss: 0.8637620508670807\n",
      "Epoch 397: train loss: 0.7692363533230632, val loss: 0.863782599568367\n",
      "Epoch 398: train loss: 0.7692068660231985, val loss: 0.8636834919452667\n",
      "Epoch 399: train loss: 0.769174073323515, val loss: 0.863737016916275\n",
      "Epoch 400: train loss: 0.7691455151239511, val loss: 0.8636442273855209\n",
      "Epoch 401: train loss: 0.769119337936197, val loss: 0.863659679889679\n",
      "Epoch 402: train loss: 0.769100934335197, val loss: 0.8636452108621597\n",
      "Epoch 403: train loss: 0.7690780857192734, val loss: 0.8635355085134506\n",
      "Epoch 404: train loss: 0.7690541024536761, val loss: 0.863614484667778\n",
      "Epoch 405: train loss: 0.769022423081521, val loss: 0.8635128438472748\n",
      "Epoch 406: train loss: 0.7690009603271373, val loss: 0.8635065257549286\n",
      "Epoch 407: train loss: 0.7689787222060921, val loss: 0.8634328991174698\n",
      "Epoch 408: train loss: 0.7689596201149866, val loss: 0.8634805679321289\n",
      "Epoch 409: train loss: 0.7689376174332619, val loss: 0.8634590208530426\n",
      "Epoch 410: train loss: 0.7689107496416752, val loss: 0.8634550124406815\n",
      "Epoch 411: train loss: 0.7688883637455077, val loss: 0.8633800894021988\n",
      "Epoch 412: train loss: 0.7688651253140389, val loss: 0.8633013516664505\n",
      "Epoch 413: train loss: 0.7688389054041055, val loss: 0.8633908033370972\n",
      "Epoch 414: train loss: 0.7688107360659453, val loss: 0.8632473796606064\n",
      "Epoch 415: train loss: 0.7687898893755115, val loss: 0.863300621509552\n",
      "Epoch 416: train loss: 0.7687741744093785, val loss: 0.8632775694131851\n",
      "Epoch 417: train loss: 0.7687488472255294, val loss: 0.8632549494504929\n",
      "Epoch 00418: reducing learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 418: train loss: 0.768726080821566, val loss: 0.863261491060257\n",
      "Epoch 419: train loss: 0.7686988797814034, val loss: 0.8632481694221497\n",
      "Epoch 420: train loss: 0.7687051878844706, val loss: 0.8632186353206635\n",
      "Epoch 421: train loss: 0.7686826543300146, val loss: 0.8631758689880371\n",
      "Epoch 422: train loss: 0.7686843756210255, val loss: 0.8631828129291534\n",
      "Epoch 423: train loss: 0.7686747559064824, val loss: 0.8631892204284668\n",
      "Epoch 424: train loss: 0.7686730970241243, val loss: 0.8631970584392548\n",
      "Epoch 425: train loss: 0.7686622859955768, val loss: 0.8631633818149567\n",
      "Epoch 426: train loss: 0.7686492721849857, val loss: 0.8632264137268066\n",
      "Epoch 00427: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 427: train loss: 0.7686530246683212, val loss: 0.8631550818681717\n",
      "Epoch 428: train loss: 0.768649611898358, val loss: 0.8632305562496185\n",
      "Epoch 429: train loss: 0.7686370344420579, val loss: 0.8631483763456345\n",
      "Epoch 430: train loss: 0.7686403986676306, val loss: 0.8631530404090881\n",
      "Epoch 431: train loss: 0.7686344073590637, val loss: 0.8632193356752396\n",
      "Epoch 432: train loss: 0.7686432958116566, val loss: 0.8631868064403534\n",
      "Epoch 00433: reducing learning rate of group 0 to 2.7000e-03.\n",
      "Epoch 433: train loss: 0.7686260982067591, val loss: 0.8631735444068909\n",
      "Epoch 434: train loss: 0.7686314692258913, val loss: 0.8631748557090759\n",
      "Epoch 435: train loss: 0.7686267409282586, val loss: 0.8631777763366699\n",
      "Epoch 436: train loss: 0.7686261867277375, val loss: 0.8631331920623779\n",
      "Epoch 437: train loss: 0.7686253082358888, val loss: 0.8631173074245453\n",
      "Epoch 438: train loss: 0.7686271079417655, val loss: 0.8631812036037445\n",
      "Epoch 00439: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 439: train loss: 0.7686364299835858, val loss: 0.8631284683942795\n",
      "Epoch 440: train loss: 0.7686171804873315, val loss: 0.8630854934453964\n",
      "Epoch 441: train loss: 0.7686341275424203, val loss: 0.8631286770105362\n",
      "Epoch 442: train loss: 0.768624043297199, val loss: 0.8631663024425507\n",
      "Epoch 443: train loss: 0.7686247460712058, val loss: 0.8631666749715805\n",
      "Epoch 444: train loss: 0.7686242554087298, val loss: 0.8631762862205505\n",
      "Epoch 445: train loss: 0.7686259650942626, val loss: 0.8632718026638031\n",
      "Epoch 00446: reducing learning rate of group 0 to 2.4300e-04.\n",
      "Epoch 446: train loss: 0.7686224051528178, val loss: 0.8631350249052048\n",
      "Epoch 447: train loss: 0.7686175062001355, val loss: 0.863177016377449\n",
      "Epoch 448: train loss: 0.7686281017873927, val loss: 0.8631310313940048\n",
      "Epoch 449: train loss: 0.7686291581195618, val loss: 0.8632056266069412\n",
      "Epoch 450: train loss: 0.7686275136077455, val loss: 0.8631782680749893\n",
      "Epoch 451: train loss: 0.7686216568136791, val loss: 0.8631154298782349\n",
      "Epoch 00452: reducing learning rate of group 0 to 7.2900e-05.\n",
      "Epoch 452: train loss: 0.768622081835104, val loss: 0.8632252067327499\n",
      "Epoch 453: train loss: 0.7686169862708248, val loss: 0.863199844956398\n",
      "Epoch 454: train loss: 0.7686266871263528, val loss: 0.8631704598665237\n",
      "Epoch 455: train loss: 0.7686260534399906, val loss: 0.8631840944290161\n",
      "Epoch 456: train loss: 0.7686266295857695, val loss: 0.8631852120161057\n",
      "Epoch 457: train loss: 0.7686228791470233, val loss: 0.8632333278656006\n",
      "Epoch 00458: reducing learning rate of group 0 to 2.1870e-05.\n",
      "Epoch 458: train loss: 0.7686248020929513, val loss: 0.8631916791200638\n",
      "Epoch 459: train loss: 0.7686299137218604, val loss: 0.8631201237440109\n",
      "Epoch 460: train loss: 0.7686229369018016, val loss: 0.8631278723478317\n",
      "Epoch 461: train loss: 0.7686237140014958, val loss: 0.8631194978952408\n",
      "Epoch 462: train loss: 0.7686253463236657, val loss: 0.8631919026374817\n",
      "Epoch 463: train loss: 0.7686245267938765, val loss: 0.8631588369607925\n",
      "Epoch 00464: reducing learning rate of group 0 to 6.5610e-06.\n",
      "Epoch 464: train loss: 0.7686216095349867, val loss: 0.8631858974695206\n",
      "Epoch 465: train loss: 0.7686256814805249, val loss: 0.8631498217582703\n",
      "Epoch 466: train loss: 0.768636919399836, val loss: 0.8631735593080521\n",
      "Epoch 467: train loss: 0.7686245698470845, val loss: 0.8631920516490936\n",
      "Epoch 468: train loss: 0.768624573449456, val loss: 0.8631322085857391\n",
      "Epoch 469: train loss: 0.7686298566486118, val loss: 0.8632067143917084\n",
      "Epoch 00470: reducing learning rate of group 0 to 1.9683e-06.\n",
      "Epoch 470: train loss: 0.7686174538197061, val loss: 0.8631057888269424\n",
      "Epoch 471: train loss: 0.7686181500899718, val loss: 0.8631775826215744\n",
      "Epoch 472: train loss: 0.7686125955640861, val loss: 0.8631163388490677\n",
      "Epoch 473: train loss: 0.7686199508084071, val loss: 0.8631889969110489\n",
      "Epoch 474: train loss: 0.768624833384903, val loss: 0.863119438290596\n",
      "Epoch 475: train loss: 0.7686210008120856, val loss: 0.8631267547607422\n",
      "Epoch 00476: reducing learning rate of group 0 to 5.9049e-07.\n",
      "Epoch 476: train loss: 0.7686200392709687, val loss: 0.8631508946418762\n",
      "Early stop at epoch 476\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_x_train = ae_model.encoder(torch.tensor(x_train, dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class onelayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16, 1)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec41267916f487bbcfb48b53bc70b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 0.5754769444465637\n",
      "Epoch 00016: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 20: train loss: 0.5161610841751099\n",
      "Epoch 00029: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 30: train loss: 0.5125399231910706\n",
      "Epoch 00037: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 40: train loss: 0.540248692035675\n",
      "Epoch 00045: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 50: train loss: 0.6533942818641663\n",
      "Epoch 00057: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 60: train loss: 0.47424057126045227\n",
      "Epoch 00070: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 70: train loss: 0.48940008878707886\n",
      "Epoch 00076: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 80: train loss: 0.5579150915145874\n",
      "Epoch 00082: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 82, loss: 0.5151228904724121\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "one_model = get_regression_model(featured_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-i.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-i.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = one_model(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
