{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.BatchNorm1d(500))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 1000)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674e7541785647a5bb082f90913212ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.455342502852586, val loss: 1.2164575457572937\n",
      "Epoch 2: train loss: 1.0247395555593732, val loss: 1.0697510242462158\n",
      "Epoch 3: train loss: 0.9500808850179034, val loss: 1.0341199189424515\n",
      "Epoch 4: train loss: 0.9242395267343256, val loss: 1.0125617235898972\n",
      "Epoch 5: train loss: 0.9070757083431719, val loss: 0.999319851398468\n",
      "Epoch 6: train loss: 0.8919417210281071, val loss: 0.9863397181034088\n",
      "Epoch 7: train loss: 0.8771767250582581, val loss: 0.9808003157377243\n",
      "Epoch 8: train loss: 0.8635093486803648, val loss: 0.959432914853096\n",
      "Epoch 9: train loss: 0.8498581347805112, val loss: 0.9475092887878418\n",
      "Epoch 10: train loss: 0.8367392537797133, val loss: 0.934653103351593\n",
      "Epoch 11: train loss: 0.8240863383108399, val loss: 0.9230950772762299\n",
      "Epoch 12: train loss: 0.8120601441259674, val loss: 0.9116584062576294\n",
      "Epoch 13: train loss: 0.8006893337766005, val loss: 0.9005149304866791\n",
      "Epoch 14: train loss: 0.7889910020379606, val loss: 0.8898219466209412\n",
      "Epoch 15: train loss: 0.7783780134374587, val loss: 0.8805101066827774\n",
      "Epoch 16: train loss: 0.7674594073854837, val loss: 0.8695701062679291\n",
      "Epoch 17: train loss: 0.7577143178310164, val loss: 0.8603126853704453\n",
      "Epoch 18: train loss: 0.747933761370957, val loss: 0.8528096377849579\n",
      "Epoch 19: train loss: 0.7383863473008638, val loss: 0.8445662260055542\n",
      "Epoch 20: train loss: 0.7290416082421458, val loss: 0.8354068994522095\n",
      "Epoch 21: train loss: 0.7204553657830521, val loss: 0.8227729201316833\n",
      "Epoch 22: train loss: 0.7120485961612476, val loss: 0.8176140785217285\n",
      "Epoch 23: train loss: 0.7038790816109541, val loss: 0.808663085103035\n",
      "Epoch 24: train loss: 0.6956646323242985, val loss: 0.8004866689443588\n",
      "Epoch 25: train loss: 0.688120362884038, val loss: 0.7957654595375061\n",
      "Epoch 26: train loss: 0.6808755895508072, val loss: 0.7874354869127274\n",
      "Epoch 27: train loss: 0.6732520459524676, val loss: 0.7798775732517242\n",
      "Epoch 28: train loss: 0.6665026792627813, val loss: 0.7744903415441513\n",
      "Epoch 29: train loss: 0.6597820352160514, val loss: 0.7685702592134476\n",
      "Epoch 30: train loss: 0.6528955201275479, val loss: 0.7625279128551483\n",
      "Epoch 31: train loss: 0.6464733934916534, val loss: 0.7543688416481018\n",
      "Epoch 32: train loss: 0.6405361509720354, val loss: 0.7481595575809479\n",
      "Epoch 33: train loss: 0.6340185914971166, val loss: 0.744976133108139\n",
      "Epoch 34: train loss: 0.6282642382260671, val loss: 0.7372077703475952\n",
      "Epoch 35: train loss: 0.6226086603664409, val loss: 0.7317538112401962\n",
      "Epoch 36: train loss: 0.617053185849749, val loss: 0.7268547415733337\n",
      "Epoch 37: train loss: 0.6114715594203205, val loss: 0.7206896841526031\n",
      "Epoch 38: train loss: 0.6061843983958025, val loss: 0.7172644734382629\n",
      "Epoch 39: train loss: 0.6011212032552405, val loss: 0.71168552339077\n",
      "Epoch 40: train loss: 0.5960425110433429, val loss: 0.7058965265750885\n",
      "Epoch 41: train loss: 0.5909190104701713, val loss: 0.701828807592392\n",
      "Epoch 42: train loss: 0.5860961426593477, val loss: 0.6970592439174652\n",
      "Epoch 43: train loss: 0.581863454240321, val loss: 0.692697212100029\n",
      "Epoch 44: train loss: 0.5770267378228507, val loss: 0.688277393579483\n",
      "Epoch 45: train loss: 0.5727632003685728, val loss: 0.6835312694311142\n",
      "Epoch 46: train loss: 0.5678408219315031, val loss: 0.6794687807559967\n",
      "Epoch 47: train loss: 0.5636275194315146, val loss: 0.6762909144163132\n",
      "Epoch 48: train loss: 0.5590076571155786, val loss: 0.6731694340705872\n",
      "Epoch 49: train loss: 0.5551657996042146, val loss: 0.6694545596837997\n",
      "Epoch 50: train loss: 0.5509862256416189, val loss: 0.6670246720314026\n",
      "Epoch 51: train loss: 0.5471726318700217, val loss: 0.6600845009088516\n",
      "Epoch 52: train loss: 0.5436553934048064, val loss: 0.6564675271511078\n",
      "Epoch 53: train loss: 0.5392398572910537, val loss: 0.653942883014679\n",
      "Epoch 54: train loss: 0.5353844341247233, val loss: 0.6513612270355225\n",
      "Epoch 55: train loss: 0.5319466460027635, val loss: 0.6461187452077866\n",
      "Epoch 56: train loss: 0.5280586320642785, val loss: 0.6411210745573044\n",
      "Epoch 57: train loss: 0.5248235801161212, val loss: 0.6374224275350571\n",
      "Epoch 58: train loss: 0.5211789113795279, val loss: 0.6345000118017197\n",
      "Epoch 59: train loss: 0.5174478358476234, val loss: 0.6315182000398636\n",
      "Epoch 60: train loss: 0.5140172311076538, val loss: 0.627674862742424\n",
      "Epoch 61: train loss: 0.5107339538417194, val loss: 0.6234178394079208\n",
      "Epoch 62: train loss: 0.5077330423059592, val loss: 0.6217546910047531\n",
      "Epoch 63: train loss: 0.5039574509935026, val loss: 0.6200330257415771\n",
      "Epoch 64: train loss: 0.5009345362600023, val loss: 0.6189642399549484\n",
      "Epoch 65: train loss: 0.497568875288348, val loss: 0.6135656908154488\n",
      "Epoch 66: train loss: 0.49469568189239627, val loss: 0.6087001860141754\n",
      "Epoch 67: train loss: 0.4912601108061882, val loss: 0.6067635864019394\n",
      "Epoch 68: train loss: 0.48851506038648324, val loss: 0.6041409075260162\n",
      "Epoch 69: train loss: 0.48550213531161085, val loss: 0.6015660092234612\n",
      "Epoch 70: train loss: 0.48227379601284476, val loss: 0.5979725569486618\n",
      "Epoch 71: train loss: 0.4793017984643866, val loss: 0.5940975397825241\n",
      "Epoch 72: train loss: 0.47690261464732725, val loss: 0.5934851691126823\n",
      "Epoch 73: train loss: 0.47380339925057824, val loss: 0.5888746380805969\n",
      "Epoch 74: train loss: 0.47082150817813145, val loss: 0.5881238058209419\n",
      "Epoch 75: train loss: 0.4680896100266546, val loss: 0.583025649189949\n",
      "Epoch 76: train loss: 0.46546355932610206, val loss: 0.5807590037584305\n",
      "Epoch 77: train loss: 0.46254957845809996, val loss: 0.578208439052105\n",
      "Epoch 78: train loss: 0.4603928851364719, val loss: 0.5754583403468132\n",
      "Epoch 79: train loss: 0.4569796766478499, val loss: 0.573411613702774\n",
      "Epoch 80: train loss: 0.45442900322535895, val loss: 0.571320928633213\n",
      "Epoch 81: train loss: 0.4524220084002967, val loss: 0.5674359574913979\n",
      "Epoch 82: train loss: 0.4493951308049629, val loss: 0.5650731846690178\n",
      "Epoch 83: train loss: 0.4473090639927542, val loss: 0.562349297106266\n",
      "Epoch 84: train loss: 0.44443525519490984, val loss: 0.5607252344489098\n",
      "Epoch 85: train loss: 0.44223440443133655, val loss: 0.5567282661795616\n",
      "Epoch 86: train loss: 0.43998983508229683, val loss: 0.5551456362009048\n",
      "Epoch 87: train loss: 0.43725432294406597, val loss: 0.5533462911844254\n",
      "Epoch 88: train loss: 0.43460462154496227, val loss: 0.5494051426649094\n",
      "Epoch 89: train loss: 0.43227443882781125, val loss: 0.5477251634001732\n",
      "Epoch 90: train loss: 0.4297422928110675, val loss: 0.5465412065386772\n",
      "Epoch 91: train loss: 0.4278523170531322, val loss: 0.5506538301706314\n",
      "Epoch 92: train loss: 0.42538399052401843, val loss: 0.5427723452448845\n",
      "Epoch 93: train loss: 0.4234519597850022, val loss: 0.538114421069622\n",
      "Epoch 94: train loss: 0.42082007879215455, val loss: 0.5371528267860413\n",
      "Epoch 95: train loss: 0.4186159791275786, val loss: 0.5356961861252785\n",
      "Epoch 96: train loss: 0.4163187000144739, val loss: 0.5316174179315567\n",
      "Epoch 97: train loss: 0.4141180582566029, val loss: 0.5297654494643211\n",
      "Epoch 98: train loss: 0.41198236291527707, val loss: 0.5276818424463272\n",
      "Epoch 99: train loss: 0.4102555397716623, val loss: 0.5294270813465118\n",
      "Epoch 100: train loss: 0.4077837475079876, val loss: 0.5249864459037781\n",
      "Epoch 101: train loss: 0.4056836180401098, val loss: 0.520519345998764\n",
      "Epoch 102: train loss: 0.40342075079533446, val loss: 0.52225212007761\n",
      "Epoch 103: train loss: 0.40159740593261245, val loss: 0.5176282599568367\n",
      "Epoch 104: train loss: 0.39915433328395494, val loss: 0.5161361321806908\n",
      "Epoch 105: train loss: 0.3974748961250208, val loss: 0.5124257281422615\n",
      "Epoch 106: train loss: 0.39538557155192267, val loss: 0.5117009207606316\n",
      "Epoch 107: train loss: 0.3935196115296092, val loss: 0.5093398094177246\n",
      "Epoch 108: train loss: 0.391379355381455, val loss: 0.5068845227360725\n",
      "Epoch 109: train loss: 0.3893550674691772, val loss: 0.5049745887517929\n",
      "Epoch 110: train loss: 0.3876812502435125, val loss: 0.5050170421600342\n",
      "Epoch 111: train loss: 0.38564477015343884, val loss: 0.5031119361519814\n",
      "Epoch 112: train loss: 0.38376234121783737, val loss: 0.5000842437148094\n",
      "Epoch 113: train loss: 0.382506809371704, val loss: 0.5001293197274208\n",
      "Epoch 114: train loss: 0.3799773708779373, val loss: 0.49723081290721893\n",
      "Epoch 115: train loss: 0.3780877998610487, val loss: 0.49308472126722336\n",
      "Epoch 116: train loss: 0.3761306764623068, val loss: 0.4927358999848366\n",
      "Epoch 117: train loss: 0.37444155283785846, val loss: 0.4893501102924347\n",
      "Epoch 118: train loss: 0.3726403998689923, val loss: 0.4902597740292549\n",
      "Epoch 119: train loss: 0.37072987068287494, val loss: 0.484464555978775\n",
      "Epoch 120: train loss: 0.3690224099163135, val loss: 0.4842061698436737\n",
      "Epoch 121: train loss: 0.3671064238869961, val loss: 0.4826824590563774\n",
      "Epoch 122: train loss: 0.36574205126259224, val loss: 0.4801281616091728\n",
      "Epoch 123: train loss: 0.3639444329017364, val loss: 0.48021848499774933\n",
      "Epoch 124: train loss: 0.3618441528060474, val loss: 0.4776829406619072\n",
      "Epoch 125: train loss: 0.35989141738792524, val loss: 0.4751278832554817\n",
      "Epoch 126: train loss: 0.35850583103037237, val loss: 0.47490983456373215\n",
      "Epoch 127: train loss: 0.3568637281101305, val loss: 0.47161223739385605\n",
      "Epoch 128: train loss: 0.35509691705543284, val loss: 0.4747426211833954\n",
      "Epoch 129: train loss: 0.3537781895161142, val loss: 0.4675225764513016\n",
      "Epoch 130: train loss: 0.3517970016523733, val loss: 0.47016143053770065\n",
      "Epoch 131: train loss: 0.3502223581338544, val loss: 0.46495432406663895\n",
      "Epoch 132: train loss: 0.3487211027222496, val loss: 0.46172089129686356\n",
      "Epoch 133: train loss: 0.3470278427282294, val loss: 0.4594708979129791\n",
      "Epoch 134: train loss: 0.3452613193667118, val loss: 0.45922382175922394\n",
      "Epoch 135: train loss: 0.34390347819241074, val loss: 0.457537405192852\n",
      "Epoch 136: train loss: 0.34224588014806107, val loss: 0.45575132220983505\n",
      "Epoch 137: train loss: 0.3406093969371575, val loss: 0.4529702514410019\n",
      "Epoch 138: train loss: 0.33915107493032937, val loss: 0.4542020112276077\n",
      "Epoch 139: train loss: 0.3375805551244012, val loss: 0.4524144381284714\n",
      "Epoch 140: train loss: 0.3360025060309261, val loss: 0.4498189091682434\n",
      "Epoch 141: train loss: 0.3342808356611451, val loss: 0.44817377626895905\n",
      "Epoch 142: train loss: 0.33278362241222825, val loss: 0.44488731771707535\n",
      "Epoch 143: train loss: 0.33115796549613863, val loss: 0.44514189660549164\n",
      "Epoch 144: train loss: 0.3301312069279896, val loss: 0.44345560669898987\n",
      "Epoch 145: train loss: 0.3282937701330275, val loss: 0.4418269470334053\n",
      "Epoch 146: train loss: 0.326961056142802, val loss: 0.4401855170726776\n",
      "Epoch 147: train loss: 0.32552794178924077, val loss: 0.4383334666490555\n",
      "Epoch 148: train loss: 0.3242139494504776, val loss: 0.4381575956940651\n",
      "Epoch 149: train loss: 0.32271574942437076, val loss: 0.43471961468458176\n",
      "Epoch 150: train loss: 0.32099812115886095, val loss: 0.4354749247431755\n",
      "Epoch 151: train loss: 0.3197250071303278, val loss: 0.4339935630559921\n",
      "Epoch 152: train loss: 0.318518850683426, val loss: 0.4332674816250801\n",
      "Epoch 153: train loss: 0.31662505054076645, val loss: 0.4292328208684921\n",
      "Epoch 154: train loss: 0.31563490690533846, val loss: 0.4277159944176674\n",
      "Epoch 155: train loss: 0.31420956409394213, val loss: 0.42732328921556473\n",
      "Epoch 156: train loss: 0.31250768360535486, val loss: 0.42449402809143066\n",
      "Epoch 157: train loss: 0.3114263350493921, val loss: 0.4228670671582222\n",
      "Epoch 158: train loss: 0.3101315250956751, val loss: 0.4255847781896591\n",
      "Epoch 159: train loss: 0.30876831714719855, val loss: 0.4189041182398796\n",
      "Epoch 160: train loss: 0.3072395792041718, val loss: 0.4235793873667717\n",
      "Epoch 161: train loss: 0.3059066894182151, val loss: 0.41844388097524643\n",
      "Epoch 162: train loss: 0.3043153301460686, val loss: 0.4156046509742737\n",
      "Epoch 163: train loss: 0.30323372062262816, val loss: 0.4147775322198868\n",
      "Epoch 164: train loss: 0.30187621416705995, val loss: 0.41330405324697495\n",
      "Epoch 165: train loss: 0.30043494730593917, val loss: 0.4127873629331589\n",
      "Epoch 166: train loss: 0.2994962897673809, val loss: 0.4106852635741234\n",
      "Epoch 167: train loss: 0.2982338736918735, val loss: 0.4085085615515709\n",
      "Epoch 168: train loss: 0.2968488621267763, val loss: 0.40960872918367386\n",
      "Epoch 169: train loss: 0.2962492731555308, val loss: 0.41096098721027374\n",
      "Epoch 170: train loss: 0.29429691361080235, val loss: 0.4057222008705139\n",
      "Epoch 171: train loss: 0.29316404047101124, val loss: 0.4060237854719162\n",
      "Epoch 172: train loss: 0.2922863856284146, val loss: 0.4029453247785568\n",
      "Epoch 173: train loss: 0.2907792500949537, val loss: 0.40155958384275436\n",
      "Epoch 174: train loss: 0.2895351712686569, val loss: 0.400200791656971\n",
      "Epoch 175: train loss: 0.28818114013774687, val loss: 0.3991471379995346\n",
      "Epoch 176: train loss: 0.28730263467786826, val loss: 0.3978281468153\n",
      "Epoch 177: train loss: 0.2858798775416657, val loss: 0.39786496758461\n",
      "Epoch 178: train loss: 0.2845012461952196, val loss: 0.39579325914382935\n",
      "Epoch 179: train loss: 0.28306670377395465, val loss: 0.3953465074300766\n",
      "Epoch 180: train loss: 0.28227678684189755, val loss: 0.39496662467718124\n",
      "Epoch 181: train loss: 0.2811067466718704, val loss: 0.3914867416024208\n",
      "Epoch 182: train loss: 0.2797122625539755, val loss: 0.39059875905513763\n",
      "Epoch 183: train loss: 0.278923755930904, val loss: 0.38957013189792633\n",
      "Epoch 184: train loss: 0.2778964026501583, val loss: 0.38668690621852875\n",
      "Epoch 185: train loss: 0.2766491644271874, val loss: 0.3873898610472679\n",
      "Epoch 186: train loss: 0.27562350183676676, val loss: 0.3865528404712677\n",
      "Epoch 187: train loss: 0.2743343477037287, val loss: 0.3847104534506798\n",
      "Epoch 188: train loss: 0.272832567738147, val loss: 0.38582826405763626\n",
      "Epoch 189: train loss: 0.2724320111770094, val loss: 0.3809511512517929\n",
      "Epoch 190: train loss: 0.2710636194645677, val loss: 0.38226747512817383\n",
      "Epoch 191: train loss: 0.26998500465100517, val loss: 0.3785446733236313\n",
      "Epoch 192: train loss: 0.2686032938026439, val loss: 0.3794372081756592\n",
      "Epoch 193: train loss: 0.267711130437879, val loss: 0.3766845688223839\n",
      "Epoch 194: train loss: 0.26634254425579096, val loss: 0.3764960542321205\n",
      "Epoch 195: train loss: 0.2653440249410185, val loss: 0.374808594584465\n",
      "Epoch 196: train loss: 0.26465609014443187, val loss: 0.37456268072128296\n",
      "Epoch 197: train loss: 0.2636878442448831, val loss: 0.37376219034194946\n",
      "Epoch 198: train loss: 0.26218865065549724, val loss: 0.37255577743053436\n",
      "Epoch 199: train loss: 0.2612501315745759, val loss: 0.37079302221536636\n",
      "Epoch 200: train loss: 0.26022433737102424, val loss: 0.3688522055745125\n",
      "Epoch 201: train loss: 0.25948333629242387, val loss: 0.3695529103279114\n",
      "Epoch 202: train loss: 0.25791369045103973, val loss: 0.3674590289592743\n",
      "Epoch 203: train loss: 0.25690831567718486, val loss: 0.3661305233836174\n",
      "Epoch 204: train loss: 0.255946049558614, val loss: 0.3644583821296692\n",
      "Epoch 205: train loss: 0.2550389930649701, val loss: 0.36260392516851425\n",
      "Epoch 206: train loss: 0.25396355003245086, val loss: 0.3660098686814308\n",
      "Epoch 207: train loss: 0.2531592108988365, val loss: 0.36132843792438507\n",
      "Epoch 208: train loss: 0.25220766855353277, val loss: 0.36137886345386505\n",
      "Epoch 209: train loss: 0.2515200066025138, val loss: 0.359101265668869\n",
      "Epoch 210: train loss: 0.25045206887743043, val loss: 0.35958240926265717\n",
      "Epoch 211: train loss: 0.24913568255832017, val loss: 0.35729174315929413\n",
      "Epoch 212: train loss: 0.24792542790751254, val loss: 0.3569522872567177\n",
      "Epoch 213: train loss: 0.2471618865370478, val loss: 0.3550547882914543\n",
      "Epoch 214: train loss: 0.24581050542747612, val loss: 0.3545030951499939\n",
      "Epoch 215: train loss: 0.2453200673889545, val loss: 0.3566637933254242\n",
      "Epoch 216: train loss: 0.2445705441118494, val loss: 0.3520856499671936\n",
      "Epoch 217: train loss: 0.24314556255016556, val loss: 0.35407087951898575\n",
      "Epoch 218: train loss: 0.2420616754516594, val loss: 0.3488682582974434\n",
      "Epoch 219: train loss: 0.24109794258370348, val loss: 0.35014116764068604\n",
      "Epoch 220: train loss: 0.2403550059119302, val loss: 0.3478333353996277\n",
      "Epoch 221: train loss: 0.23919703522487157, val loss: 0.3479761630296707\n",
      "Epoch 222: train loss: 0.2384837692428827, val loss: 0.34608593583106995\n",
      "Epoch 223: train loss: 0.2379106304217927, val loss: 0.3468366526067257\n",
      "Epoch 224: train loss: 0.23708114393082214, val loss: 0.34694261848926544\n",
      "Epoch 225: train loss: 0.23574392950756803, val loss: 0.34256894886493683\n",
      "Epoch 226: train loss: 0.23481784739532488, val loss: 0.3421625420451164\n",
      "Epoch 227: train loss: 0.23437708975520255, val loss: 0.34146954491734505\n",
      "Epoch 228: train loss: 0.2332366133624142, val loss: 0.3430575281381607\n",
      "Epoch 229: train loss: 0.23256160662050382, val loss: 0.3395102433860302\n",
      "Epoch 230: train loss: 0.23154107744847197, val loss: 0.33838706836104393\n",
      "Epoch 231: train loss: 0.23081751901392297, val loss: 0.3372443690896034\n",
      "Epoch 232: train loss: 0.2297108673515837, val loss: 0.33622706681489944\n",
      "Epoch 233: train loss: 0.22840687695845635, val loss: 0.3369106389582157\n",
      "Epoch 234: train loss: 0.22807058889135276, val loss: 0.33595021441578865\n",
      "Epoch 235: train loss: 0.22717861867698724, val loss: 0.33319178968667984\n",
      "Epoch 236: train loss: 0.22595050531795158, val loss: 0.33269793540239334\n",
      "Epoch 237: train loss: 0.2253228515552096, val loss: 0.3323115296661854\n",
      "Epoch 238: train loss: 0.22457987061321094, val loss: 0.33175814896821976\n",
      "Epoch 239: train loss: 0.2235080424339621, val loss: 0.33030320331454277\n",
      "Epoch 240: train loss: 0.22251467573724515, val loss: 0.3296508714556694\n",
      "Epoch 241: train loss: 0.22156868336131394, val loss: 0.32835160568356514\n",
      "Epoch 242: train loss: 0.22097877565161617, val loss: 0.32787279412150383\n",
      "Epoch 243: train loss: 0.22021325697348967, val loss: 0.32771943137049675\n",
      "Epoch 244: train loss: 0.21915140844411765, val loss: 0.3277036026120186\n",
      "Epoch 245: train loss: 0.21867972709742375, val loss: 0.3253895342350006\n",
      "Epoch 246: train loss: 0.2181477408536078, val loss: 0.32527032494544983\n",
      "Epoch 247: train loss: 0.21656724126636495, val loss: 0.32448598742485046\n",
      "Epoch 248: train loss: 0.21602135217100216, val loss: 0.32289480045437813\n",
      "Epoch 249: train loss: 0.21569721762922145, val loss: 0.32220934331417084\n",
      "Epoch 250: train loss: 0.2150247053348796, val loss: 0.3203750550746918\n",
      "Epoch 251: train loss: 0.21390376314253093, val loss: 0.3192274235188961\n",
      "Epoch 252: train loss: 0.2129736154765251, val loss: 0.31975438073277473\n",
      "Epoch 253: train loss: 0.21205859794136903, val loss: 0.31865235418081284\n",
      "Epoch 254: train loss: 0.21169721890375998, val loss: 0.317219365388155\n",
      "Epoch 255: train loss: 0.21064091686309067, val loss: 0.3164635896682739\n",
      "Epoch 256: train loss: 0.20992540691032086, val loss: 0.3162776716053486\n",
      "Epoch 257: train loss: 0.20933196122877662, val loss: 0.31758761778473854\n",
      "Epoch 258: train loss: 0.20858438180109054, val loss: 0.3141155540943146\n",
      "Epoch 259: train loss: 0.20795830582353422, val loss: 0.3156941644847393\n",
      "Epoch 260: train loss: 0.20729526741584117, val loss: 0.31306730955839157\n",
      "Epoch 261: train loss: 0.20605828072443697, val loss: 0.3107019141316414\n",
      "Epoch 262: train loss: 0.20521987970262445, val loss: 0.310825876891613\n",
      "Epoch 263: train loss: 0.20490636171487686, val loss: 0.3104006312787533\n",
      "Epoch 264: train loss: 0.20439746539232895, val loss: 0.3104552701115608\n",
      "Epoch 265: train loss: 0.2034631916040379, val loss: 0.3082125000655651\n",
      "Epoch 266: train loss: 0.20222395862951967, val loss: 0.3074101321399212\n",
      "Epoch 267: train loss: 0.20201682874537963, val loss: 0.30700676515698433\n",
      "Epoch 268: train loss: 0.20089265267975928, val loss: 0.3057025484740734\n",
      "Epoch 269: train loss: 0.20055261705093858, val loss: 0.3058747202157974\n",
      "Epoch 270: train loss: 0.19968988693099285, val loss: 0.3041207939386368\n",
      "Epoch 271: train loss: 0.19915876654151685, val loss: 0.3040076047182083\n",
      "Epoch 272: train loss: 0.1982028700292052, val loss: 0.30352867394685745\n",
      "Epoch 273: train loss: 0.19750523571094164, val loss: 0.30268367379903793\n",
      "Epoch 274: train loss: 0.1970171404212022, val loss: 0.30452581122517586\n",
      "Epoch 275: train loss: 0.19665275116404535, val loss: 0.30185990408062935\n",
      "Epoch 276: train loss: 0.19535457614744772, val loss: 0.3016170859336853\n",
      "Epoch 277: train loss: 0.1943679742371479, val loss: 0.2986908592283726\n",
      "Epoch 278: train loss: 0.19410537976780112, val loss: 0.2973324619233608\n",
      "Epoch 279: train loss: 0.193586247880955, val loss: 0.2991426959633827\n",
      "Epoch 280: train loss: 0.19294429357945236, val loss: 0.2975507117807865\n",
      "Epoch 281: train loss: 0.19189482483841241, val loss: 0.2971295192837715\n",
      "Epoch 282: train loss: 0.1915368427933528, val loss: 0.2954676151275635\n",
      "Epoch 283: train loss: 0.19062824885134835, val loss: 0.2952282913029194\n",
      "Epoch 284: train loss: 0.190132223992167, val loss: 0.295969620347023\n",
      "Epoch 285: train loss: 0.18918967399379857, val loss: 0.29372991248965263\n",
      "Epoch 286: train loss: 0.18869447157064398, val loss: 0.29097456857562065\n",
      "Epoch 287: train loss: 0.18809973589445556, val loss: 0.29075275361537933\n",
      "Epoch 288: train loss: 0.18740368617180708, val loss: 0.291046891361475\n",
      "Epoch 289: train loss: 0.18660091374851995, val loss: 0.29167671129107475\n",
      "Epoch 290: train loss: 0.1860359977201791, val loss: 0.2899743840098381\n",
      "Epoch 291: train loss: 0.18557550470177078, val loss: 0.29227815568447113\n",
      "Epoch 292: train loss: 0.18483433503218236, val loss: 0.2886333279311657\n",
      "Epoch 293: train loss: 0.1836805008071889, val loss: 0.28758786991238594\n",
      "Epoch 294: train loss: 0.1835173028467995, val loss: 0.28775155916810036\n",
      "Epoch 295: train loss: 0.18303411582722767, val loss: 0.28530966117978096\n",
      "Epoch 296: train loss: 0.18203367602490242, val loss: 0.2848437838256359\n",
      "Epoch 297: train loss: 0.1815645770917956, val loss: 0.2839529737830162\n",
      "Epoch 298: train loss: 0.1810617061007533, val loss: 0.28387831151485443\n",
      "Epoch 299: train loss: 0.1801452175615193, val loss: 0.2857922725379467\n",
      "Epoch 300: train loss: 0.18004259049444563, val loss: 0.28289126977324486\n",
      "Epoch 301: train loss: 0.1792515097627138, val loss: 0.2842409871518612\n",
      "Epoch 302: train loss: 0.17812273838051937, val loss: 0.28084540739655495\n",
      "Epoch 303: train loss: 0.1774167935663639, val loss: 0.28133900836110115\n",
      "Epoch 304: train loss: 0.17726876932065186, val loss: 0.280202928930521\n",
      "Epoch 305: train loss: 0.1765914305786571, val loss: 0.2802271284162998\n",
      "Epoch 306: train loss: 0.17582564323663477, val loss: 0.27921106293797493\n",
      "Epoch 307: train loss: 0.17510242181854752, val loss: 0.27790822833776474\n",
      "Epoch 308: train loss: 0.17448593828947628, val loss: 0.27760226652026176\n",
      "Epoch 309: train loss: 0.1741701412467698, val loss: 0.27774904295802116\n",
      "Epoch 310: train loss: 0.1736990692314857, val loss: 0.27780650556087494\n",
      "Epoch 311: train loss: 0.17273803924236839, val loss: 0.2741600014269352\n",
      "Epoch 312: train loss: 0.17275520123733643, val loss: 0.27707045525312424\n",
      "Epoch 313: train loss: 0.1719450989080309, val loss: 0.2744591571390629\n",
      "Epoch 314: train loss: 0.17104064022081034, val loss: 0.2730029411613941\n",
      "Epoch 315: train loss: 0.17054450474448238, val loss: 0.2716974653303623\n",
      "Epoch 316: train loss: 0.17018072599194012, val loss: 0.2722140997648239\n",
      "Epoch 317: train loss: 0.16964136773817137, val loss: 0.2722080498933792\n",
      "Epoch 318: train loss: 0.16861921445640307, val loss: 0.27118905261158943\n",
      "Epoch 319: train loss: 0.168420514871349, val loss: 0.2696741744875908\n",
      "Epoch 320: train loss: 0.16777461026763263, val loss: 0.2681148052215576\n",
      "Epoch 321: train loss: 0.16710601484647025, val loss: 0.2697744593024254\n",
      "Epoch 322: train loss: 0.166549020257895, val loss: 0.2684456445276737\n",
      "Epoch 323: train loss: 0.16573268607896513, val loss: 0.26765644922852516\n",
      "Epoch 324: train loss: 0.16519936808220353, val loss: 0.26746298745274544\n",
      "Epoch 325: train loss: 0.16474906402119466, val loss: 0.26569999754428864\n",
      "Epoch 326: train loss: 0.1646000763163462, val loss: 0.26401060819625854\n",
      "Epoch 327: train loss: 0.1639495484679402, val loss: 0.2657696530222893\n",
      "Epoch 328: train loss: 0.16322543839347475, val loss: 0.2644902132451534\n",
      "Epoch 329: train loss: 0.16246552800419517, val loss: 0.2633075825870037\n",
      "Epoch 330: train loss: 0.16234774802857385, val loss: 0.2630236893892288\n",
      "Epoch 331: train loss: 0.16158830610824076, val loss: 0.26224879175424576\n",
      "Epoch 332: train loss: 0.16085896095061997, val loss: 0.2618960328400135\n",
      "Epoch 333: train loss: 0.16047134240422908, val loss: 0.26134300604462624\n",
      "Epoch 334: train loss: 0.15998919086201602, val loss: 0.2607966735959053\n",
      "Epoch 335: train loss: 0.15937297794406846, val loss: 0.26020826399326324\n",
      "Epoch 336: train loss: 0.15878406128085779, val loss: 0.2585503049194813\n",
      "Epoch 337: train loss: 0.15841951010196825, val loss: 0.25929810106754303\n",
      "Epoch 338: train loss: 0.15800641101779833, val loss: 0.25865473225712776\n",
      "Epoch 339: train loss: 0.15759368819646763, val loss: 0.25827542692422867\n",
      "Epoch 340: train loss: 0.15689949557423707, val loss: 0.25748618319630623\n",
      "Epoch 341: train loss: 0.15633213442647742, val loss: 0.2559417113661766\n",
      "Epoch 342: train loss: 0.15566271622365535, val loss: 0.2555352784693241\n",
      "Epoch 343: train loss: 0.15517205754474503, val loss: 0.2557535767555237\n",
      "Epoch 344: train loss: 0.15473758963559509, val loss: 0.25449860468506813\n",
      "Epoch 345: train loss: 0.15441642285892676, val loss: 0.2538783475756645\n",
      "Epoch 346: train loss: 0.15371423213943786, val loss: 0.25384974107146263\n",
      "Epoch 347: train loss: 0.15311093883006568, val loss: 0.25339512899518013\n",
      "Epoch 348: train loss: 0.15249861198988898, val loss: 0.25286300107836723\n",
      "Epoch 349: train loss: 0.15205477564134695, val loss: 0.2533646672964096\n",
      "Epoch 350: train loss: 0.1515060109967151, val loss: 0.25101280957460403\n",
      "Epoch 351: train loss: 0.15131356573229482, val loss: 0.25388676673173904\n",
      "Epoch 352: train loss: 0.1507518990236106, val loss: 0.24848756566643715\n",
      "Epoch 353: train loss: 0.1503140916917029, val loss: 0.2502090521156788\n",
      "Epoch 354: train loss: 0.1497207661979302, val loss: 0.2503036819398403\n",
      "Epoch 355: train loss: 0.14889305028717567, val loss: 0.24923591315746307\n",
      "Epoch 356: train loss: 0.14858004166502767, val loss: 0.24717019498348236\n",
      "Epoch 357: train loss: 0.14824099506731322, val loss: 0.2490788772702217\n",
      "Epoch 358: train loss: 0.14744322439573512, val loss: 0.2508709952235222\n",
      "Epoch 359: train loss: 0.14707238926427188, val loss: 0.24698616564273834\n",
      "Epoch 360: train loss: 0.14637001236545927, val loss: 0.2482295148074627\n",
      "Epoch 361: train loss: 0.14608081238151416, val loss: 0.24424627050757408\n",
      "Epoch 362: train loss: 0.14561446810403472, val loss: 0.24370934441685677\n",
      "Epoch 363: train loss: 0.14512911017367125, val loss: 0.24391688406467438\n",
      "Epoch 364: train loss: 0.1446162562808816, val loss: 0.24302415549755096\n",
      "Epoch 365: train loss: 0.1440076387393946, val loss: 0.2424287386238575\n",
      "Epoch 366: train loss: 0.1435880767718829, val loss: 0.24334930256009102\n",
      "Epoch 367: train loss: 0.1431780741548507, val loss: 0.24077510461211205\n",
      "Epoch 368: train loss: 0.14278037420619122, val loss: 0.24288902431726456\n",
      "Epoch 369: train loss: 0.1421171867868862, val loss: 0.23997887596488\n",
      "Epoch 370: train loss: 0.1418118280684469, val loss: 0.24019867926836014\n",
      "Epoch 371: train loss: 0.1410955109897294, val loss: 0.24177538231015205\n",
      "Epoch 372: train loss: 0.14097931372578024, val loss: 0.23994354903697968\n",
      "Epoch 373: train loss: 0.140284317263549, val loss: 0.2395157255232334\n",
      "Epoch 374: train loss: 0.13988770036789297, val loss: 0.23962151631712914\n",
      "Epoch 375: train loss: 0.1392072562307207, val loss: 0.23839463666081429\n",
      "Epoch 376: train loss: 0.13874668379462415, val loss: 0.23675593733787537\n",
      "Epoch 377: train loss: 0.13849481644290837, val loss: 0.23576776310801506\n",
      "Epoch 378: train loss: 0.13802697012253892, val loss: 0.23726918920874596\n",
      "Epoch 379: train loss: 0.13724190655369184, val loss: 0.2347058281302452\n",
      "Epoch 380: train loss: 0.13726433103904737, val loss: 0.2377551756799221\n",
      "Epoch 381: train loss: 0.13658116417104846, val loss: 0.23318208754062653\n",
      "Epoch 382: train loss: 0.1364222254941215, val loss: 0.2342713214457035\n",
      "Epoch 383: train loss: 0.1357651588691679, val loss: 0.2351965792477131\n",
      "Epoch 384: train loss: 0.1354819454961024, val loss: 0.2325316220521927\n",
      "Epoch 385: train loss: 0.13502339255922724, val loss: 0.23251160979270935\n",
      "Epoch 386: train loss: 0.13441925852449063, val loss: 0.23131222650408745\n",
      "Epoch 387: train loss: 0.1339899665076372, val loss: 0.23194744810461998\n",
      "Epoch 388: train loss: 0.13344695578989194, val loss: 0.23115983232855797\n",
      "Epoch 389: train loss: 0.13344827042183663, val loss: 0.23210351169109344\n",
      "Epoch 390: train loss: 0.13276863697890712, val loss: 0.23060749471187592\n",
      "Epoch 391: train loss: 0.13238684742503679, val loss: 0.2286716364324093\n",
      "Epoch 392: train loss: 0.13171010179891185, val loss: 0.2283552661538124\n",
      "Epoch 393: train loss: 0.13106445058795682, val loss: 0.23001615703105927\n",
      "Epoch 394: train loss: 0.1308456302049385, val loss: 0.22852932289242744\n",
      "Epoch 395: train loss: 0.13055531421602648, val loss: 0.22838407382369041\n",
      "Epoch 396: train loss: 0.13011434567165467, val loss: 0.2259320616722107\n",
      "Epoch 397: train loss: 0.13004615755884993, val loss: 0.22544589266180992\n",
      "Epoch 398: train loss: 0.12935206643100347, val loss: 0.22514844685792923\n",
      "Epoch 399: train loss: 0.1289553111702038, val loss: 0.22398168221116066\n",
      "Epoch 400: train loss: 0.12856918158133177, val loss: 0.22691471129655838\n",
      "Epoch 401: train loss: 0.12809903478649698, val loss: 0.22377772256731987\n",
      "Epoch 402: train loss: 0.1274955245802816, val loss: 0.22612088546156883\n",
      "Epoch 403: train loss: 0.12704523944145946, val loss: 0.2229985073208809\n",
      "Epoch 404: train loss: 0.12693386954036034, val loss: 0.22443528100848198\n",
      "Epoch 405: train loss: 0.12634389700744714, val loss: 0.2212320677936077\n",
      "Epoch 406: train loss: 0.12565904681938064, val loss: 0.22165581956505775\n",
      "Epoch 407: train loss: 0.1257249014570759, val loss: 0.221050176769495\n",
      "Epoch 408: train loss: 0.1252590351850914, val loss: 0.2207360528409481\n",
      "Epoch 409: train loss: 0.12453357751698169, val loss: 0.22097821906208992\n",
      "Epoch 410: train loss: 0.12433370950271865, val loss: 0.22122680395841599\n",
      "Epoch 411: train loss: 0.12394423184052904, val loss: 0.2191244252026081\n",
      "Epoch 412: train loss: 0.1235368353835277, val loss: 0.2226734682917595\n",
      "Epoch 413: train loss: 0.12328698803244444, val loss: 0.2171287164092064\n",
      "Epoch 414: train loss: 0.12299173948656535, val loss: 0.21772191673517227\n",
      "Epoch 415: train loss: 0.12209335813157039, val loss: 0.2184397578239441\n",
      "Epoch 416: train loss: 0.12205662508000739, val loss: 0.21791361644864082\n",
      "Epoch 417: train loss: 0.12179460686353716, val loss: 0.21705368161201477\n",
      "Epoch 418: train loss: 0.12098666064122346, val loss: 0.21564232930541039\n",
      "Epoch 419: train loss: 0.12089344508622216, val loss: 0.2175385095179081\n",
      "Epoch 420: train loss: 0.12063620067409969, val loss: 0.2167106308043003\n",
      "Epoch 421: train loss: 0.12025380090283086, val loss: 0.21571878530085087\n",
      "Epoch 422: train loss: 0.11972318873495807, val loss: 0.21352779306471348\n",
      "Epoch 423: train loss: 0.11925917363895072, val loss: 0.2137303240597248\n",
      "Epoch 424: train loss: 0.11886049794901511, val loss: 0.21641323156654835\n",
      "Epoch 425: train loss: 0.11842184480947826, val loss: 0.2128618061542511\n",
      "Epoch 426: train loss: 0.11828156185087835, val loss: 0.21247458085417747\n",
      "Epoch 427: train loss: 0.11801426882293636, val loss: 0.21267877146601677\n",
      "Epoch 428: train loss: 0.11766968825719436, val loss: 0.21098856627941132\n",
      "Epoch 429: train loss: 0.11728430211018286, val loss: 0.21373966708779335\n",
      "Epoch 430: train loss: 0.1169345971271128, val loss: 0.2116057612001896\n",
      "Epoch 431: train loss: 0.11638199409718843, val loss: 0.21086768805980682\n",
      "Epoch 432: train loss: 0.11598322554882427, val loss: 0.20949142053723335\n",
      "Epoch 433: train loss: 0.11560900344329891, val loss: 0.21010442450642586\n",
      "Epoch 434: train loss: 0.11524243424021313, val loss: 0.21145927533507347\n",
      "Epoch 435: train loss: 0.11479822001750077, val loss: 0.20768137276172638\n",
      "Epoch 436: train loss: 0.11467847702687023, val loss: 0.2081974409520626\n",
      "Epoch 437: train loss: 0.11409194092760674, val loss: 0.20767061039805412\n",
      "Epoch 438: train loss: 0.11386119014723789, val loss: 0.207846038043499\n",
      "Epoch 439: train loss: 0.11345995639866764, val loss: 0.20574861764907837\n",
      "Epoch 440: train loss: 0.11300457431303446, val loss: 0.2068621814250946\n",
      "Epoch 441: train loss: 0.11296242702361689, val loss: 0.20579009875655174\n",
      "Epoch 442: train loss: 0.11249529313550437, val loss: 0.20563095808029175\n",
      "Epoch 443: train loss: 0.11231702775960653, val loss: 0.20540845021605492\n",
      "Epoch 444: train loss: 0.1117412599861836, val loss: 0.20420831441879272\n",
      "Epoch 445: train loss: 0.11122768569697511, val loss: 0.20438309758901596\n",
      "Epoch 446: train loss: 0.11106343261656594, val loss: 0.20287284441292286\n",
      "Epoch 447: train loss: 0.11059136910206428, val loss: 0.20347913540899754\n",
      "Epoch 448: train loss: 0.11040793342728196, val loss: 0.2030596025288105\n",
      "Epoch 449: train loss: 0.11038046295893655, val loss: 0.20358136296272278\n",
      "Epoch 450: train loss: 0.10986739177012202, val loss: 0.20360946096479893\n",
      "Epoch 451: train loss: 0.10953977539020596, val loss: 0.20100752636790276\n",
      "Epoch 452: train loss: 0.10891310094190322, val loss: 0.20279352739453316\n",
      "Epoch 453: train loss: 0.10878460449025118, val loss: 0.20181470178067684\n",
      "Epoch 454: train loss: 0.10849432784507805, val loss: 0.2021037396043539\n",
      "Epoch 455: train loss: 0.10790614512125599, val loss: 0.20000344142317772\n",
      "Epoch 456: train loss: 0.10760739909126571, val loss: 0.20165716111660004\n",
      "Epoch 457: train loss: 0.10734417788229203, val loss: 0.20096161030232906\n",
      "Epoch 458: train loss: 0.10709039037892025, val loss: 0.20253161154687405\n",
      "Epoch 459: train loss: 0.1070545626520059, val loss: 0.1995166502892971\n",
      "Epoch 460: train loss: 0.1066368379621325, val loss: 0.1989018227905035\n",
      "Epoch 461: train loss: 0.10611370351921301, val loss: 0.19994926638901234\n",
      "Epoch 462: train loss: 0.1058025856163719, val loss: 0.19768065586686134\n",
      "Epoch 463: train loss: 0.10557158337586536, val loss: 0.1972950380295515\n",
      "Epoch 464: train loss: 0.10553096990809648, val loss: 0.20032844506204128\n",
      "Epoch 465: train loss: 0.1050153874281924, val loss: 0.19651223719120026\n",
      "Epoch 466: train loss: 0.10454635239823587, val loss: 0.19614937715232372\n",
      "Epoch 467: train loss: 0.10404628648141832, val loss: 0.1960988100618124\n",
      "Epoch 468: train loss: 0.10413003316875066, val loss: 0.19462723471224308\n",
      "Epoch 469: train loss: 0.10394177426897751, val loss: 0.19611010141670704\n",
      "Epoch 470: train loss: 0.10359360725028341, val loss: 0.19557719118893147\n",
      "Epoch 471: train loss: 0.10326681977605866, val loss: 0.1940453816205263\n",
      "Epoch 472: train loss: 0.10269983356215524, val loss: 0.19475209526717663\n",
      "Epoch 473: train loss: 0.10242793037976287, val loss: 0.19458011537790298\n",
      "Epoch 474: train loss: 0.10242033521506336, val loss: 0.19476419687271118\n",
      "Epoch 475: train loss: 0.1018462068103412, val loss: 0.194759925827384\n",
      "Epoch 476: train loss: 0.10166247238212456, val loss: 0.1929016187787056\n",
      "Epoch 477: train loss: 0.10135519070315306, val loss: 0.19339074939489365\n",
      "Epoch 478: train loss: 0.10104825951059206, val loss: 0.1919544693082571\n",
      "Epoch 479: train loss: 0.10099736901230767, val loss: 0.19219297543168068\n",
      "Epoch 480: train loss: 0.10038337496537393, val loss: 0.19141361489892006\n",
      "Epoch 481: train loss: 0.09993619024169402, val loss: 0.19194076023995876\n",
      "Epoch 482: train loss: 0.09984864838292341, val loss: 0.19127742387354374\n",
      "Epoch 483: train loss: 0.09961931730204647, val loss: 0.19444945454597473\n",
      "Epoch 484: train loss: 0.09901379469182124, val loss: 0.18920356035232544\n",
      "Epoch 485: train loss: 0.0990043096791993, val loss: 0.18966379761695862\n",
      "Epoch 486: train loss: 0.09877860198288484, val loss: 0.19038904830813408\n",
      "Epoch 487: train loss: 0.0984918452855303, val loss: 0.18892711214721203\n",
      "Epoch 488: train loss: 0.09824388443313756, val loss: 0.19148978404700756\n",
      "Epoch 489: train loss: 0.09802165296694221, val loss: 0.19056863710284233\n",
      "Epoch 490: train loss: 0.09771954774252926, val loss: 0.18889504484832287\n",
      "Epoch 491: train loss: 0.0972379416869926, val loss: 0.18849759362637997\n",
      "Epoch 492: train loss: 0.0975895766041046, val loss: 0.18784938380122185\n",
      "Epoch 493: train loss: 0.09703994529459921, val loss: 0.1883850060403347\n",
      "Epoch 494: train loss: 0.09668310045008174, val loss: 0.18703237362205982\n",
      "Epoch 495: train loss: 0.09654957826660958, val loss: 0.1866383720189333\n",
      "Epoch 496: train loss: 0.09624672722471808, val loss: 0.18684501387178898\n",
      "Epoch 497: train loss: 0.09582126480385744, val loss: 0.18743527494370937\n",
      "Epoch 498: train loss: 0.09553464923427604, val loss: 0.18531980738043785\n",
      "Epoch 499: train loss: 0.09531215446636358, val loss: 0.18479451537132263\n",
      "Epoch 500: train loss: 0.09490744284226668, val loss: 0.18511429242789745\n",
      "Epoch 501: train loss: 0.09493218421235033, val loss: 0.18429560586810112\n",
      "Epoch 502: train loss: 0.09466855557758953, val loss: 0.18488330021500587\n",
      "Epoch 503: train loss: 0.09450903086540006, val loss: 0.1836159396916628\n",
      "Epoch 504: train loss: 0.09406656060916697, val loss: 0.183807585388422\n",
      "Epoch 505: train loss: 0.09374079268884519, val loss: 0.18501200526952744\n",
      "Epoch 506: train loss: 0.09352038586034529, val loss: 0.1843925677239895\n",
      "Epoch 507: train loss: 0.09330017344307487, val loss: 0.18499580770730972\n",
      "Epoch 508: train loss: 0.0931195300735706, val loss: 0.18204711377620697\n",
      "Epoch 509: train loss: 0.0932395310913648, val loss: 0.18308918178081512\n",
      "Epoch 510: train loss: 0.09281582983830754, val loss: 0.18176517263054848\n",
      "Epoch 511: train loss: 0.09242025321019082, val loss: 0.18213588558137417\n",
      "Epoch 512: train loss: 0.09234041634220502, val loss: 0.18169152364134789\n",
      "Epoch 513: train loss: 0.09209100302821162, val loss: 0.1812050100415945\n",
      "Epoch 514: train loss: 0.09198663320680803, val loss: 0.18122181668877602\n",
      "Epoch 515: train loss: 0.09161320353052811, val loss: 0.1799155157059431\n",
      "Epoch 516: train loss: 0.09153810773885783, val loss: 0.18070733174681664\n",
      "Epoch 517: train loss: 0.0912575684159225, val loss: 0.17976838536560535\n",
      "Epoch 518: train loss: 0.09076038428923928, val loss: 0.18077150732278824\n",
      "Epoch 519: train loss: 0.09073766876147175, val loss: 0.17977527529001236\n",
      "Epoch 520: train loss: 0.09048570648239782, val loss: 0.17885597422719002\n",
      "Epoch 521: train loss: 0.09019420892292282, val loss: 0.17932110652327538\n",
      "Epoch 522: train loss: 0.08988257600995538, val loss: 0.18090081959962845\n",
      "Epoch 523: train loss: 0.08975173973523103, val loss: 0.18113800697028637\n",
      "Epoch 524: train loss: 0.08959565045630297, val loss: 0.1802121326327324\n",
      "Epoch 525: train loss: 0.08928709057315196, val loss: 0.17896281741559505\n",
      "Epoch 526: train loss: 0.0892301967275645, val loss: 0.17829393967986107\n",
      "Epoch 527: train loss: 0.08895887789364852, val loss: 0.17841449193656445\n",
      "Epoch 528: train loss: 0.08867976749276305, val loss: 0.1781107559800148\n",
      "Epoch 529: train loss: 0.08862517955197263, val loss: 0.17749410681426525\n",
      "Epoch 530: train loss: 0.08814973326455297, val loss: 0.17566470429301262\n",
      "Epoch 531: train loss: 0.08812268952632488, val loss: 0.17722276598215103\n",
      "Epoch 532: train loss: 0.08782049145808761, val loss: 0.17553481832146645\n",
      "Epoch 533: train loss: 0.08802423955509375, val loss: 0.1770230121910572\n",
      "Epoch 534: train loss: 0.08732678987054177, val loss: 0.1763144377619028\n",
      "Epoch 535: train loss: 0.08747453801638158, val loss: 0.17665228620171547\n",
      "Epoch 536: train loss: 0.08734016657138487, val loss: 0.17572122812271118\n",
      "Epoch 537: train loss: 0.08708744214002466, val loss: 0.17610509134829044\n",
      "Epoch 538: train loss: 0.08673883144789683, val loss: 0.1746987532824278\n",
      "Epoch 539: train loss: 0.08680729148863811, val loss: 0.17465905658900738\n",
      "Epoch 540: train loss: 0.08653757526219495, val loss: 0.17450941912829876\n",
      "Epoch 541: train loss: 0.08611813806817642, val loss: 0.17366067320108414\n",
      "Epoch 542: train loss: 0.0857141392317055, val loss: 0.173703420907259\n",
      "Epoch 543: train loss: 0.08547271275999342, val loss: 0.17378772981464863\n",
      "Epoch 544: train loss: 0.08559555702984858, val loss: 0.17318573966622353\n",
      "Epoch 545: train loss: 0.08562360884550543, val loss: 0.17344357073307037\n",
      "Epoch 546: train loss: 0.08508165712763148, val loss: 0.175099553540349\n",
      "Epoch 547: train loss: 0.0851941630658041, val loss: 0.17290813848376274\n",
      "Epoch 548: train loss: 0.08464213927775105, val loss: 0.17395645193755627\n",
      "Epoch 549: train loss: 0.08474806745587436, val loss: 0.1742697823792696\n",
      "Epoch 550: train loss: 0.08474321437257834, val loss: 0.17307015135884285\n",
      "Epoch 551: train loss: 0.08448343441177528, val loss: 0.17265204712748528\n",
      "Epoch 552: train loss: 0.08409401436318996, val loss: 0.17135832831263542\n",
      "Epoch 553: train loss: 0.08383484825866747, val loss: 0.1706988289952278\n",
      "Epoch 554: train loss: 0.0838053722989711, val loss: 0.17294827662408352\n",
      "Epoch 555: train loss: 0.08353799040441769, val loss: 0.17315406166017056\n",
      "Epoch 556: train loss: 0.08334299759152511, val loss: 0.17133096605539322\n",
      "Epoch 557: train loss: 0.08328465231393374, val loss: 0.17183114401996136\n",
      "Epoch 558: train loss: 0.08308793434965109, val loss: 0.1706542819738388\n",
      "Epoch 559: train loss: 0.08318005297765044, val loss: 0.1710196640342474\n",
      "Epoch 560: train loss: 0.08269677805777899, val loss: 0.17017957754433155\n",
      "Epoch 561: train loss: 0.08247538438637013, val loss: 0.1707280147820711\n",
      "Epoch 562: train loss: 0.08246983622791954, val loss: 0.1717897579073906\n",
      "Epoch 563: train loss: 0.08244238607402878, val loss: 0.16979473270475864\n",
      "Epoch 564: train loss: 0.08229114615188943, val loss: 0.1717632245272398\n",
      "Epoch 565: train loss: 0.08200344722018216, val loss: 0.1690420638769865\n",
      "Epoch 566: train loss: 0.08146202787927302, val loss: 0.16901288740336895\n",
      "Epoch 567: train loss: 0.08143270048819426, val loss: 0.16931793838739395\n",
      "Epoch 568: train loss: 0.08141387804578387, val loss: 0.1679935697466135\n",
      "Epoch 569: train loss: 0.08120197487087742, val loss: 0.1704333182424307\n",
      "Epoch 570: train loss: 0.0808265366241925, val loss: 0.16878144815564156\n",
      "Epoch 571: train loss: 0.08077417030730928, val loss: 0.16819165460765362\n",
      "Epoch 572: train loss: 0.08083479600535308, val loss: 0.1677126046270132\n",
      "Epoch 573: train loss: 0.08069355318025218, val loss: 0.16843287833034992\n",
      "Epoch 574: train loss: 0.08046842198751986, val loss: 0.16834517009556293\n",
      "Epoch 575: train loss: 0.07994962160891468, val loss: 0.16644527949392796\n",
      "Epoch 576: train loss: 0.080227606202975, val loss: 0.16807869262993336\n",
      "Epoch 577: train loss: 0.07978517093112444, val loss: 0.1673819199204445\n",
      "Epoch 578: train loss: 0.07968976674555797, val loss: 0.16672932915389538\n",
      "Epoch 579: train loss: 0.07958823394265217, val loss: 0.16771387122571468\n",
      "Epoch 580: train loss: 0.0794320935976735, val loss: 0.16751170717179775\n",
      "Epoch 00581: reducing learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 581: train loss: 0.07957477712068477, val loss: 0.16740017011761665\n",
      "Epoch 582: train loss: 0.07807652122130856, val loss: 0.16567043773829937\n",
      "Epoch 583: train loss: 0.07781963986008435, val loss: 0.16582716442644596\n",
      "Epoch 584: train loss: 0.0779529722156146, val loss: 0.16434027068316936\n",
      "Epoch 585: train loss: 0.07815766671890918, val loss: 0.16509811766445637\n",
      "Epoch 586: train loss: 0.07774534635306651, val loss: 0.1645160112529993\n",
      "Epoch 587: train loss: 0.07772428560630826, val loss: 0.1647748053073883\n",
      "Epoch 588: train loss: 0.07752045022724735, val loss: 0.16355260275304317\n",
      "Epoch 589: train loss: 0.07752757958826852, val loss: 0.16493472084403038\n",
      "Epoch 590: train loss: 0.07756760347682018, val loss: 0.16681769117712975\n",
      "Epoch 591: train loss: 0.07763813493474564, val loss: 0.16407650895416737\n",
      "Epoch 592: train loss: 0.07762859497135349, val loss: 0.16472644731402397\n",
      "Epoch 593: train loss: 0.0773376945658121, val loss: 0.1632712334394455\n",
      "Epoch 594: train loss: 0.07732299449970736, val loss: 0.16414519399404526\n",
      "Epoch 595: train loss: 0.07727673936273802, val loss: 0.16410859301686287\n",
      "Epoch 596: train loss: 0.07713426412738876, val loss: 0.16342519596219063\n",
      "Epoch 597: train loss: 0.07706001648488912, val loss: 0.1637722235172987\n",
      "Epoch 598: train loss: 0.07719856224606061, val loss: 0.16331923380494118\n",
      "Epoch 00599: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 599: train loss: 0.07708763645895553, val loss: 0.16473196260631084\n",
      "Epoch 600: train loss: 0.07684309660843174, val loss: 0.16381313651800156\n",
      "Epoch 601: train loss: 0.07670519019275349, val loss: 0.16358190216124058\n",
      "Epoch 602: train loss: 0.0767389169119223, val loss: 0.164157472550869\n",
      "Epoch 603: train loss: 0.07664832391261042, val loss: 0.16333321668207645\n",
      "Epoch 604: train loss: 0.07673778362530814, val loss: 0.16311032511293888\n",
      "Epoch 605: train loss: 0.07647853685930561, val loss: 0.1631421521306038\n",
      "Epoch 606: train loss: 0.07640596851097996, val loss: 0.16294979490339756\n",
      "Epoch 607: train loss: 0.07672597651182145, val loss: 0.16462480835616589\n",
      "Epoch 608: train loss: 0.07643372890208135, val loss: 0.16354948841035366\n",
      "Epoch 609: train loss: 0.0767641871661308, val loss: 0.16278665512800217\n",
      "Epoch 610: train loss: 0.07658170334987491, val loss: 0.1625266820192337\n",
      "Epoch 611: train loss: 0.07679269284652519, val loss: 0.16264602728188038\n",
      "Epoch 612: train loss: 0.07670830210871028, val loss: 0.16438054107129574\n",
      "Epoch 613: train loss: 0.0765283928222027, val loss: 0.16477477923035622\n",
      "Epoch 614: train loss: 0.0767041178592394, val loss: 0.16341105662286282\n",
      "Epoch 615: train loss: 0.07646616629596553, val loss: 0.16319389641284943\n",
      "Epoch 00616: reducing learning rate of group 0 to 2.7000e-03.\n",
      "Epoch 616: train loss: 0.07631179050209864, val loss: 0.16325289197266102\n",
      "Epoch 617: train loss: 0.07627433627035757, val loss: 0.16596447303891182\n",
      "Epoch 618: train loss: 0.07618549513596796, val loss: 0.16453221812844276\n",
      "Epoch 619: train loss: 0.07630659108972362, val loss: 0.16492736898362637\n",
      "Epoch 620: train loss: 0.07629528637562885, val loss: 0.16434258222579956\n",
      "Epoch 621: train loss: 0.07634371387164057, val loss: 0.1646948605775833\n",
      "Epoch 00622: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 622: train loss: 0.076326175874275, val loss: 0.16326035931706429\n",
      "Epoch 623: train loss: 0.0762896551289655, val loss: 0.16534405760467052\n",
      "Epoch 624: train loss: 0.07622643658506134, val loss: 0.16487989760935307\n",
      "Epoch 625: train loss: 0.0762581705477494, val loss: 0.16295307129621506\n",
      "Epoch 626: train loss: 0.07615413260212602, val loss: 0.16304260306060314\n",
      "Epoch 627: train loss: 0.07640330963705692, val loss: 0.16358504630625248\n",
      "Epoch 628: train loss: 0.07621381715869872, val loss: 0.16228086687624454\n",
      "Epoch 629: train loss: 0.0764531939832341, val loss: 0.16291229613125324\n",
      "Epoch 630: train loss: 0.07606989259921972, val loss: 0.16379108652472496\n",
      "Epoch 631: train loss: 0.07642836535300199, val loss: 0.16317584365606308\n",
      "Epoch 632: train loss: 0.0762314767726143, val loss: 0.16432481445372105\n",
      "Epoch 633: train loss: 0.07636005107930423, val loss: 0.16279293783009052\n",
      "Epoch 00634: reducing learning rate of group 0 to 2.4300e-04.\n",
      "Epoch 634: train loss: 0.07652681891686852, val loss: 0.1643651258200407\n",
      "Epoch 635: train loss: 0.07618693400835219, val loss: 0.16237540543079376\n",
      "Epoch 636: train loss: 0.07623718618830604, val loss: 0.1644875966012478\n",
      "Epoch 637: train loss: 0.07617991799303069, val loss: 0.1632883157581091\n",
      "Epoch 638: train loss: 0.07608972620054695, val loss: 0.16368249617516994\n",
      "Epoch 639: train loss: 0.07627201791303838, val loss: 0.164131049066782\n",
      "Epoch 00640: reducing learning rate of group 0 to 7.2900e-05.\n",
      "Epoch 640: train loss: 0.0762099498449023, val loss: 0.16661440208554268\n",
      "Epoch 641: train loss: 0.0761322136463312, val loss: 0.16537456586956978\n",
      "Epoch 642: train loss: 0.07615809975272883, val loss: 0.16428858414292336\n",
      "Epoch 643: train loss: 0.07618601151888996, val loss: 0.16281445138156414\n",
      "Epoch 644: train loss: 0.07605635142334144, val loss: 0.16341394931077957\n",
      "Epoch 645: train loss: 0.07610456245090011, val loss: 0.1638287603855133\n",
      "Epoch 00646: reducing learning rate of group 0 to 2.1870e-05.\n",
      "Epoch 646: train loss: 0.07624310715326645, val loss: 0.16334253922104836\n",
      "Epoch 647: train loss: 0.07623566505771492, val loss: 0.16298839449882507\n",
      "Epoch 648: train loss: 0.07617493848596589, val loss: 0.16370165161788464\n",
      "Epoch 649: train loss: 0.07603924621440585, val loss: 0.16288036480545998\n",
      "Epoch 650: train loss: 0.07637716506050132, val loss: 0.162669088691473\n",
      "Epoch 651: train loss: 0.07618154850427873, val loss: 0.16252447851002216\n",
      "Epoch 00652: reducing learning rate of group 0 to 6.5610e-06.\n",
      "Epoch 652: train loss: 0.0762994795482714, val loss: 0.16409280709922314\n",
      "Epoch 653: train loss: 0.0763026217923021, val loss: 0.16416925564408302\n",
      "Epoch 654: train loss: 0.07617855360698014, val loss: 0.1624050922691822\n",
      "Epoch 655: train loss: 0.07610728060817532, val loss: 0.16393336281180382\n",
      "Epoch 656: train loss: 0.07618890461511606, val loss: 0.16334687359631062\n",
      "Epoch 657: train loss: 0.07630787909819542, val loss: 0.16378719732165337\n",
      "Epoch 00658: reducing learning rate of group 0 to 1.9683e-06.\n",
      "Epoch 658: train loss: 0.07635377219439098, val loss: 0.1646126713603735\n",
      "Epoch 659: train loss: 0.07650053197361605, val loss: 0.16280156187713146\n",
      "Epoch 660: train loss: 0.07639482513444948, val loss: 0.16298966482281685\n",
      "Epoch 661: train loss: 0.07608859294070718, val loss: 0.16444604843854904\n",
      "Epoch 662: train loss: 0.07617006164892402, val loss: 0.16217420063912868\n",
      "Epoch 663: train loss: 0.0762098795718829, val loss: 0.16411627642810345\n",
      "Epoch 664: train loss: 0.07594663343109052, val loss: 0.1627728957682848\n",
      "Epoch 665: train loss: 0.07638605972718507, val loss: 0.1631537526845932\n",
      "Epoch 666: train loss: 0.0762254231478879, val loss: 0.1621874552220106\n",
      "Epoch 667: train loss: 0.07645960817857725, val loss: 0.16353517398238182\n",
      "Epoch 00668: reducing learning rate of group 0 to 5.9049e-07.\n",
      "Epoch 668: train loss: 0.07616108538609087, val loss: 0.16361607983708382\n",
      "Early stop at epoch 668\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_x_train = ae_model.encoder(torch.tensor(x_train, dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linan\\AppData\\Local\\Temp\\ipykernel_18016\\974611454.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dade3bc754174f259b10fbf054cb4024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 5.800898551940918, val loss: 4.748525619506836\n",
      "Epoch 2: train loss: 12.902047157287598, val loss: 4.234756946563721\n",
      "Epoch 3: train loss: 5.999337673187256, val loss: 3.890300750732422\n",
      "Epoch 4: train loss: 2.9360525608062744, val loss: 3.5686683654785156\n",
      "Epoch 5: train loss: 2.4128479957580566, val loss: 3.164647340774536\n",
      "Epoch 6: train loss: 2.3314661979675293, val loss: 2.8555142879486084\n",
      "Epoch 7: train loss: 2.156174659729004, val loss: 2.6738851070404053\n",
      "Epoch 8: train loss: 1.8882337808609009, val loss: 2.508727550506592\n",
      "Epoch 9: train loss: 1.5964884757995605, val loss: 2.3289260864257812\n",
      "Epoch 10: train loss: 1.3251292705535889, val loss: 2.132870674133301\n",
      "Epoch 11: train loss: 1.0800857543945312, val loss: 1.9432125091552734\n",
      "Epoch 12: train loss: 0.8710078001022339, val loss: 1.7651007175445557\n",
      "Epoch 13: train loss: 0.6975438594818115, val loss: 1.5909343957901\n",
      "Epoch 14: train loss: 0.5547029972076416, val loss: 1.4414156675338745\n",
      "Epoch 15: train loss: 0.4358433187007904, val loss: 1.3397632837295532\n",
      "Epoch 16: train loss: 0.33643287420272827, val loss: 1.2084486484527588\n",
      "Epoch 17: train loss: 0.2535609006881714, val loss: 1.0841288566589355\n",
      "Epoch 18: train loss: 0.18510447442531586, val loss: 0.9732980728149414\n",
      "Epoch 19: train loss: 0.129990816116333, val loss: 0.8770979046821594\n",
      "Epoch 20: train loss: 0.08782581239938736, val loss: 0.797723114490509\n",
      "Epoch 21: train loss: 0.058448676019907, val loss: 0.7374292612075806\n",
      "Epoch 22: train loss: 0.04153267294168472, val loss: 0.6939994692802429\n",
      "Epoch 23: train loss: 0.036639876663684845, val loss: 0.6487008929252625\n",
      "Epoch 24: train loss: 0.04231276363134384, val loss: 0.6322523951530457\n",
      "Epoch 25: train loss: 0.055779505521059036, val loss: 0.6307053565979004\n",
      "Epoch 26: train loss: 0.07321431487798691, val loss: 0.6405043601989746\n",
      "Epoch 27: train loss: 0.09053463488817215, val loss: 0.6666103601455688\n",
      "Epoch 28: train loss: 0.10409466922283173, val loss: 0.6719374656677246\n",
      "Epoch 29: train loss: 0.1114935576915741, val loss: 0.6745335459709167\n",
      "Epoch 30: train loss: 0.11172445118427277, val loss: 0.6728916764259338\n",
      "Epoch 31: train loss: 0.10498172044754028, val loss: 0.6661415100097656\n",
      "Epoch 32: train loss: 0.09251207113265991, val loss: 0.6544826626777649\n",
      "Epoch 33: train loss: 0.07628870010375977, val loss: 0.6396317481994629\n",
      "Epoch 34: train loss: 0.058626964688301086, val loss: 0.6230980753898621\n",
      "Epoch 35: train loss: 0.04171924293041229, val loss: 0.6064280271530151\n",
      "Epoch 36: train loss: 0.027300110086798668, val loss: 0.5907790064811707\n",
      "Epoch 37: train loss: 0.016416260972619057, val loss: 0.5769156813621521\n",
      "Epoch 38: train loss: 0.009408310987055302, val loss: 0.565394937992096\n",
      "Epoch 39: train loss: 0.006028741132467985, val loss: 0.5561894774436951\n",
      "Epoch 40: train loss: 0.005607703235000372, val loss: 0.5491254925727844\n",
      "Epoch 41: train loss: 0.007252448238432407, val loss: 0.5438613295555115\n",
      "Epoch 42: train loss: 0.01002147514373064, val loss: 0.5399193167686462\n",
      "Epoch 43: train loss: 0.013050166890025139, val loss: 0.5367744565010071\n",
      "Epoch 44: train loss: 0.01565983146429062, val loss: 0.5340122580528259\n",
      "Epoch 45: train loss: 0.017415031790733337, val loss: 0.5313272476196289\n",
      "Epoch 46: train loss: 0.01813593879342079, val loss: 0.5286237001419067\n",
      "Epoch 47: train loss: 0.01786031946539879, val loss: 0.5259529948234558\n",
      "Epoch 48: train loss: 0.01676325500011444, val loss: 0.5234911441802979\n",
      "Epoch 49: train loss: 0.015079421922564507, val loss: 0.5214499831199646\n",
      "Epoch 50: train loss: 0.013044694438576698, val loss: 0.5200291275978088\n",
      "Epoch 51: train loss: 0.010864888317883015, val loss: 0.5193702578544617\n",
      "Epoch 52: train loss: 0.00870910007506609, val loss: 0.5195319056510925\n",
      "Epoch 53: train loss: 0.0067162648774683475, val loss: 0.5204755663871765\n",
      "Epoch 54: train loss: 0.004995347931981087, val loss: 0.5220541954040527\n",
      "Epoch 55: train loss: 0.0036221311893314123, val loss: 0.524036169052124\n",
      "Epoch 56: train loss: 0.002632753225043416, val loss: 0.5261099934577942\n",
      "Epoch 57: train loss: 0.0020215518306940794, val loss: 0.5279698371887207\n",
      "Epoch 58: train loss: 0.0017493389314040542, val loss: 0.5293514132499695\n",
      "Epoch 59: train loss: 0.0017512365011498332, val loss: 0.5300547480583191\n",
      "Epoch 60: train loss: 0.0019462334457784891, val loss: 0.5299591422080994\n",
      "Epoch 61: train loss: 0.0022428410593420267, val loss: 0.5290188193321228\n",
      "Epoch 62: train loss: 0.002547760494053364, val loss: 0.5272471308708191\n",
      "Epoch 63: train loss: 0.002778115216642618, val loss: 0.5247197151184082\n",
      "Epoch 64: train loss: 0.0028753108344972134, val loss: 0.5215780138969421\n",
      "Epoch 65: train loss: 0.0028132949955761433, val loss: 0.5180179476737976\n",
      "Epoch 66: train loss: 0.002599399769678712, val loss: 0.5142671465873718\n",
      "Epoch 67: train loss: 0.0022666705772280693, val loss: 0.5105471014976501\n",
      "Epoch 68: train loss: 0.0018628374673426151, val loss: 0.5070754885673523\n",
      "Epoch 69: train loss: 0.0014404826797544956, val loss: 0.5040515661239624\n",
      "Epoch 70: train loss: 0.0010490388376638293, val loss: 0.501640796661377\n",
      "Epoch 71: train loss: 0.0007287714979611337, val loss: 0.49996286630630493\n",
      "Epoch 72: train loss: 0.0005046000587753952, val loss: 0.49909573793411255\n",
      "Epoch 73: train loss: 0.00038207409670576453, val loss: 0.4990632236003876\n",
      "Epoch 74: train loss: 0.0003487371723167598, val loss: 0.49983522295951843\n",
      "Epoch 75: train loss: 0.000379344419343397, val loss: 0.5013296008110046\n",
      "Epoch 76: train loss: 0.0004437500028871, val loss: 0.5034176111221313\n",
      "Epoch 77: train loss: 0.0005139111890457571, val loss: 0.5059327483177185\n",
      "Epoch 78: train loss: 0.0005681193433701992, val loss: 0.5086850523948669\n",
      "Epoch 79: train loss: 0.0005927032325416803, val loss: 0.5114775896072388\n",
      "Epoch 80: train loss: 0.0005822026869282126, val loss: 0.5141276121139526\n",
      "Epoch 81: train loss: 0.0005388700519688427, val loss: 0.5164794325828552\n",
      "Epoch 82: train loss: 0.00047112221363931894, val loss: 0.5184101462364197\n",
      "Epoch 83: train loss: 0.0003907315549440682, val loss: 0.5198401808738708\n",
      "Epoch 84: train loss: 0.0003095073625445366, val loss: 0.5207259058952332\n",
      "Epoch 85: train loss: 0.00023663546016905457, val loss: 0.521059513092041\n",
      "Epoch 86: train loss: 0.0001776689023245126, val loss: 0.5208629965782166\n",
      "Epoch 87: train loss: 0.00013493248843587935, val loss: 0.5201882719993591\n",
      "Epoch 88: train loss: 0.00010840981849469244, val loss: 0.5191113352775574\n",
      "Epoch 89: train loss: 9.633634181227535e-05, val loss: 0.517727792263031\n",
      "Epoch 90: train loss: 9.549247624818236e-05, val loss: 0.5161463618278503\n",
      "Epoch 91: train loss: 0.00010160917736357078, val loss: 0.5144813656806946\n",
      "Epoch 92: train loss: 0.00011011906462954357, val loss: 0.5128438472747803\n",
      "Epoch 93: train loss: 0.00011701085168169811, val loss: 0.5113343000411987\n",
      "Epoch 94: train loss: 0.00011943519348278642, val loss: 0.5100342631340027\n",
      "Epoch 95: train loss: 0.00011596550757531077, val loss: 0.5089990496635437\n",
      "Epoch 96: train loss: 0.00010659086547093466, val loss: 0.5082579851150513\n",
      "Epoch 97: train loss: 9.261879313271493e-05, val loss: 0.5078111290931702\n",
      "Epoch 98: train loss: 7.629075844306499e-05, val loss: 0.5076338648796082\n",
      "Epoch 99: train loss: 6.0161619330756366e-05, val loss: 0.5076815485954285\n",
      "Epoch 100: train loss: 4.633136268239468e-05, val loss: 0.5079001188278198\n",
      "Epoch 101: train loss: 3.597929026000202e-05, val loss: 0.508233904838562\n",
      "Epoch 102: train loss: 2.9319096938706934e-05, val loss: 0.5086291432380676\n",
      "Epoch 103: train loss: 2.5926587113644928e-05, val loss: 0.5090435147285461\n",
      "Epoch 104: train loss: 2.5030340111698024e-05, val loss: 0.5094451308250427\n",
      "Epoch 105: train loss: 2.5705652660690248e-05, val loss: 0.5098123550415039\n",
      "Epoch 106: train loss: 2.6963882191921584e-05, val loss: 0.510131299495697\n",
      "Epoch 107: train loss: 2.788508209050633e-05, val loss: 0.5103937387466431\n",
      "Epoch 108: train loss: 2.7812060579890385e-05, val loss: 0.5105963349342346\n",
      "Epoch 109: train loss: 2.6473186153452843e-05, val loss: 0.5107391476631165\n",
      "Epoch 110: train loss: 2.3980188416317105e-05, val loss: 0.5108256936073303\n",
      "Epoch 111: train loss: 2.0704432245111093e-05, val loss: 0.5108636021614075\n",
      "Epoch 112: train loss: 1.713292658678256e-05, val loss: 0.5108633637428284\n",
      "Epoch 113: train loss: 1.3749145182373468e-05, val loss: 0.5108360648155212\n",
      "Epoch 114: train loss: 1.0935156751656905e-05, val loss: 0.5107927322387695\n",
      "Epoch 115: train loss: 8.892615369404666e-06, val loss: 0.5107485055923462\n",
      "Epoch 116: train loss: 7.615547929162858e-06, val loss: 0.5107079744338989\n",
      "Epoch 117: train loss: 6.952591775188921e-06, val loss: 0.5106706023216248\n",
      "Epoch 118: train loss: 6.7015844251727685e-06, val loss: 0.5106303691864014\n",
      "Epoch 119: train loss: 6.67795166009455e-06, val loss: 0.5105776190757751\n",
      "Epoch 120: train loss: 6.727544132445473e-06, val loss: 0.5105008482933044\n",
      "Epoch 121: train loss: 6.7030787249677815e-06, val loss: 0.5103914141654968\n",
      "Epoch 122: train loss: 6.4761093199194875e-06, val loss: 0.5102443099021912\n",
      "Epoch 123: train loss: 5.97942334934487e-06, val loss: 0.5100601315498352\n",
      "Epoch 124: train loss: 5.241493454377633e-06, val loss: 0.5098446011543274\n",
      "Epoch 125: train loss: 4.372613148007076e-06, val loss: 0.5096068978309631\n",
      "Epoch 126: train loss: 3.5153016142430715e-06, val loss: 0.5093585848808289\n",
      "Epoch 127: train loss: 2.7907624371437123e-06, val loss: 0.5091114640235901\n",
      "Epoch 128: train loss: 2.272246774737141e-06, val loss: 0.5088763236999512\n",
      "Epoch 129: train loss: 1.9760382201639004e-06, val loss: 0.5086617469787598\n",
      "Epoch 130: train loss: 1.867032779045985e-06, val loss: 0.5084757804870605\n",
      "Epoch 131: train loss: 1.87200237178331e-06, val loss: 0.5083243250846863\n",
      "Epoch 132: train loss: 1.906603301904397e-06, val loss: 0.508212685585022\n",
      "Epoch 133: train loss: 1.905759177134314e-06, val loss: 0.5081435441970825\n",
      "Epoch 134: train loss: 1.839921196733485e-06, val loss: 0.5081179738044739\n",
      "Epoch 135: train loss: 1.7100823015425703e-06, val loss: 0.5081331133842468\n",
      "Epoch 136: train loss: 1.5335817806771956e-06, val loss: 0.5081825256347656\n",
      "Epoch 137: train loss: 1.3342591955733951e-06, val loss: 0.5082558393478394\n",
      "Epoch 138: train loss: 1.1376132533769123e-06, val loss: 0.5083401799201965\n",
      "Epoch 139: train loss: 9.652309245211654e-07, val loss: 0.5084217190742493\n",
      "Epoch 140: train loss: 8.28163592814235e-07, val loss: 0.5084865093231201\n",
      "Epoch 141: train loss: 7.239244155243796e-07, val loss: 0.5085237622261047\n",
      "Epoch 142: train loss: 6.426834033845807e-07, val loss: 0.5085268020629883\n",
      "Epoch 143: train loss: 5.756834866588179e-07, val loss: 0.5084924101829529\n",
      "Epoch 144: train loss: 5.189145326767175e-07, val loss: 0.5084220170974731\n",
      "Epoch 145: train loss: 4.7173955408652546e-07, val loss: 0.5083193182945251\n",
      "Epoch 146: train loss: 4.3334415522622294e-07, val loss: 0.5081909894943237\n",
      "Epoch 147: train loss: 4.018644119696546e-07, val loss: 0.5080440044403076\n",
      "Epoch 148: train loss: 3.7403876262942504e-07, val loss: 0.5078855156898499\n",
      "Epoch 149: train loss: 3.461961455286655e-07, val loss: 0.5077230930328369\n",
      "Epoch 150: train loss: 3.1454064242097957e-07, val loss: 0.5075632929801941\n",
      "Epoch 151: train loss: 2.77094756029328e-07, val loss: 0.5074111223220825\n",
      "Epoch 152: train loss: 2.3631466206097684e-07, val loss: 0.5072718858718872\n",
      "Epoch 153: train loss: 1.980987178740179e-07, val loss: 0.5071477293968201\n",
      "Epoch 154: train loss: 1.6901731214602478e-07, val loss: 0.5070404410362244\n",
      "Epoch 155: train loss: 1.5272493669726828e-07, val loss: 0.5069481134414673\n",
      "Epoch 156: train loss: 1.486269667339002e-07, val loss: 0.5068687796592712\n",
      "Epoch 157: train loss: 1.5273164422069385e-07, val loss: 0.5067986845970154\n",
      "Epoch 158: train loss: 1.5909293438198802e-07, val loss: 0.5067334771156311\n",
      "Epoch 159: train loss: 1.6176223027741798e-07, val loss: 0.506669819355011\n",
      "Epoch 160: train loss: 1.5661562713376043e-07, val loss: 0.5066053867340088\n",
      "Epoch 161: train loss: 1.4313300766843895e-07, val loss: 0.5065392255783081\n",
      "Epoch 162: train loss: 1.2395430815104191e-07, val loss: 0.5064709186553955\n",
      "Epoch 163: train loss: 1.0306810338533978e-07, val loss: 0.5064010620117188\n",
      "Epoch 164: train loss: 8.41289917730137e-08, val loss: 0.5063303112983704\n",
      "Epoch 165: train loss: 6.90603201292106e-08, val loss: 0.5062583088874817\n",
      "Epoch 166: train loss: 5.854784745906727e-08, val loss: 0.5061852931976318\n",
      "Epoch 167: train loss: 5.22692786830703e-08, val loss: 0.5061106085777283\n",
      "Epoch 168: train loss: 4.902582873000938e-08, val loss: 0.5060329437255859\n",
      "Epoch 169: train loss: 4.7213084997110855e-08, val loss: 0.505952775478363\n",
      "Epoch 170: train loss: 4.5470901710586986e-08, val loss: 0.5058795213699341\n",
      "Epoch 171: train loss: 4.304416378886344e-08, val loss: 0.5058070421218872\n",
      "Epoch 172: train loss: 3.9923868655478145e-08, val loss: 0.5057328343391418\n",
      "Epoch 173: train loss: 3.657087432884509e-08, val loss: 0.5056571960449219\n",
      "Epoch 174: train loss: 3.338368159688798e-08, val loss: 0.5055808424949646\n",
      "Epoch 175: train loss: 3.084105060224829e-08, val loss: 0.5055055022239685\n",
      "Epoch 176: train loss: 2.9065654771898153e-08, val loss: 0.5054318308830261\n",
      "Epoch 177: train loss: 2.7849011985381367e-08, val loss: 0.5053591132164001\n",
      "Epoch 178: train loss: 2.6837703615001374e-08, val loss: 0.505287230014801\n",
      "Epoch 179: train loss: 2.5714571805224296e-08, val loss: 0.5052171945571899\n",
      "Epoch 180: train loss: 2.4536570109034983e-08, val loss: 0.5051488280296326\n",
      "Epoch 181: train loss: 2.346140881570591e-08, val loss: 0.5050822496414185\n",
      "Epoch 182: train loss: 2.2632780982689837e-08, val loss: 0.5050171613693237\n",
      "Epoch 183: train loss: 2.1955596452016835e-08, val loss: 0.504953145980835\n",
      "Epoch 184: train loss: 2.1379578996061355e-08, val loss: 0.5048918128013611\n",
      "Epoch 185: train loss: 2.0770736242070598e-08, val loss: 0.5048350691795349\n",
      "Epoch 186: train loss: 2.00438368125333e-08, val loss: 0.5047774910926819\n",
      "Epoch 187: train loss: 1.9172905041386912e-08, val loss: 0.5047182440757751\n",
      "Epoch 188: train loss: 1.8075388297233985e-08, val loss: 0.5046581625938416\n",
      "Epoch 189: train loss: 1.687669204386566e-08, val loss: 0.5045974254608154\n",
      "Epoch 190: train loss: 1.5692949162371406e-08, val loss: 0.5045363306999207\n",
      "Epoch 191: train loss: 1.4647980606241617e-08, val loss: 0.504476010799408\n",
      "Epoch 192: train loss: 1.3827740730221194e-08, val loss: 0.5044164061546326\n",
      "Epoch 193: train loss: 1.3324394920743998e-08, val loss: 0.5043582320213318\n",
      "Epoch 194: train loss: 1.3126636666527247e-08, val loss: 0.5043014883995056\n",
      "Epoch 195: train loss: 1.311934294534467e-08, val loss: 0.5042466521263123\n",
      "Epoch 196: train loss: 1.3163409917638091e-08, val loss: 0.504193902015686\n",
      "Epoch 197: train loss: 1.3145419863747065e-08, val loss: 0.5041434168815613\n",
      "Epoch 198: train loss: 1.2988701669769398e-08, val loss: 0.5040950179100037\n",
      "Epoch 199: train loss: 1.274019734154308e-08, val loss: 0.5040482878684998\n",
      "Epoch 200: train loss: 1.2425572570862187e-08, val loss: 0.5040028691291809\n",
      "Epoch 201: train loss: 1.2093950729763492e-08, val loss: 0.5039577484130859\n",
      "Epoch 202: train loss: 1.1804656807612446e-08, val loss: 0.503912627696991\n",
      "Epoch 203: train loss: 1.1620582718308015e-08, val loss: 0.5038663744926453\n",
      "Epoch 204: train loss: 1.1507371944219358e-08, val loss: 0.5038188695907593\n",
      "Epoch 205: train loss: 1.1528057619614174e-08, val loss: 0.5037698149681091\n",
      "Epoch 206: train loss: 1.1598189963990535e-08, val loss: 0.5037193298339844\n",
      "Epoch 207: train loss: 1.1684576861625828e-08, val loss: 0.5036675333976746\n",
      "Epoch 208: train loss: 1.1821885692597789e-08, val loss: 0.5036153793334961\n",
      "Epoch 209: train loss: 1.1949897071872329e-08, val loss: 0.5035635828971863\n",
      "Epoch 210: train loss: 1.204881794336643e-08, val loss: 0.503512978553772\n",
      "Epoch 211: train loss: 1.2090487722105081e-08, val loss: 0.5034639239311218\n",
      "Epoch 212: train loss: 1.2062300491777478e-08, val loss: 0.5034173130989075\n",
      "Epoch 213: train loss: 1.1964839785605363e-08, val loss: 0.5033733248710632\n",
      "Epoch 214: train loss: 1.1811828848351524e-08, val loss: 0.5033320188522339\n",
      "Epoch 215: train loss: 1.1634981866848193e-08, val loss: 0.5032930970191956\n",
      "Epoch 216: train loss: 1.1443974656799583e-08, val loss: 0.5032560229301453\n",
      "Epoch 217: train loss: 1.1240094188735839e-08, val loss: 0.503220796585083\n",
      "Epoch 218: train loss: 1.1080008022190668e-08, val loss: 0.5031859874725342\n",
      "Epoch 219: train loss: 1.0964352092912577e-08, val loss: 0.503151535987854\n",
      "Epoch 220: train loss: 1.0854594556519714e-08, val loss: 0.5031172633171082\n",
      "Epoch 221: train loss: 1.0727096544371761e-08, val loss: 0.5030824542045593\n",
      "Epoch 222: train loss: 1.0595516464206867e-08, val loss: 0.5030476450920105\n",
      "Epoch 223: train loss: 1.0507232417467094e-08, val loss: 0.5030125975608826\n",
      "Epoch 224: train loss: 1.0509214831699865e-08, val loss: 0.5029777884483337\n",
      "Epoch 225: train loss: 1.0629404023632105e-08, val loss: 0.5029429793357849\n",
      "Epoch 226: train loss: 1.0819683815554981e-08, val loss: 0.5029088854789734\n",
      "Epoch 227: train loss: 1.1043182368553062e-08, val loss: 0.5028759837150574\n",
      "Epoch 228: train loss: 1.1268705080169639e-08, val loss: 0.5028438568115234\n",
      "Epoch 229: train loss: 1.1463546556456095e-08, val loss: 0.5028131604194641\n",
      "Epoch 230: train loss: 1.1633173535585684e-08, val loss: 0.5027838349342346\n",
      "Epoch 231: train loss: 1.1762937290882292e-08, val loss: 0.5027556419372559\n",
      "Epoch 232: train loss: 1.1825123991116016e-08, val loss: 0.5027293562889099\n",
      "Epoch 233: train loss: 1.1825652457275737e-08, val loss: 0.5027040839195251\n",
      "Epoch 234: train loss: 1.1748348960338717e-08, val loss: 0.5026805996894836\n",
      "Epoch 235: train loss: 1.1609391670219793e-08, val loss: 0.5026583075523376\n",
      "Epoch 236: train loss: 1.142576522283889e-08, val loss: 0.5026378035545349\n",
      "Epoch 237: train loss: 1.1245139042159735e-08, val loss: 0.5026187300682068\n",
      "Epoch 238: train loss: 1.111560177236015e-08, val loss: 0.5026013255119324\n",
      "Epoch 239: train loss: 1.1060905968918178e-08, val loss: 0.5025854110717773\n",
      "Epoch 240: train loss: 1.1018978618437814e-08, val loss: 0.502571165561676\n",
      "Epoch 241: train loss: 1.1022590840070734e-08, val loss: 0.502558708190918\n",
      "Epoch 242: train loss: 1.100086333138961e-08, val loss: 0.5025476217269897\n",
      "Epoch 243: train loss: 1.0984125609070361e-08, val loss: 0.5025374293327332\n",
      "Epoch 244: train loss: 1.1013853828956144e-08, val loss: 0.5025283098220825\n",
      "Epoch 245: train loss: 1.1023630897000203e-08, val loss: 0.502518892288208\n",
      "Epoch 246: train loss: 1.101170266082363e-08, val loss: 0.5025098323822021\n",
      "Epoch 247: train loss: 1.101270896697315e-08, val loss: 0.5025013089179993\n",
      "Epoch 248: train loss: 1.0997591282091435e-08, val loss: 0.5024933218955994\n",
      "Epoch 249: train loss: 1.0993166377204489e-08, val loss: 0.5024860501289368\n",
      "Epoch 250: train loss: 1.097133495164826e-08, val loss: 0.5024794936180115\n",
      "Epoch 251: train loss: 1.0959150920086813e-08, val loss: 0.502473771572113\n",
      "Epoch 252: train loss: 1.0915922388221588e-08, val loss: 0.5024690628051758\n",
      "Epoch 253: train loss: 1.087871659422035e-08, val loss: 0.5024650692939758\n",
      "Epoch 254: train loss: 1.0881259449035952e-08, val loss: 0.502461850643158\n",
      "Epoch 255: train loss: 1.0931473504172118e-08, val loss: 0.5024592280387878\n",
      "Epoch 256: train loss: 1.0983849385581834e-08, val loss: 0.5024574995040894\n",
      "Epoch 257: train loss: 1.1071880301471992e-08, val loss: 0.5024564862251282\n",
      "Epoch 258: train loss: 1.115076297963924e-08, val loss: 0.5024685263633728\n",
      "Epoch 259: train loss: 1.1214720707641845e-08, val loss: 0.5024871230125427\n",
      "Epoch 260: train loss: 1.1224408069665515e-08, val loss: 0.5025057792663574\n",
      "Epoch 261: train loss: 1.1244156716827547e-08, val loss: 0.5025251507759094\n",
      "Epoch 262: train loss: 1.1221588103182967e-08, val loss: 0.5025454163551331\n",
      "Epoch 263: train loss: 1.1173377778561644e-08, val loss: 0.5025670528411865\n",
      "Epoch 264: train loss: 1.1115646181281136e-08, val loss: 0.5025896430015564\n",
      "Epoch 265: train loss: 1.1028388868794536e-08, val loss: 0.5026128888130188\n",
      "Epoch 266: train loss: 1.0956777707349374e-08, val loss: 0.5026373267173767\n",
      "Epoch 267: train loss: 1.0891929136391809e-08, val loss: 0.5026625990867615\n",
      "Epoch 268: train loss: 1.08404192289413e-08, val loss: 0.5026891231536865\n",
      "Epoch 269: train loss: 1.07828999063031e-08, val loss: 0.5027167201042175\n",
      "Epoch 270: train loss: 1.0741723954765803e-08, val loss: 0.5027453899383545\n",
      "Epoch 271: train loss: 1.0713604225998097e-08, val loss: 0.5027755498886108\n",
      "Epoch 272: train loss: 1.0685891282946614e-08, val loss: 0.502807080745697\n",
      "Epoch 273: train loss: 1.06573203595417e-08, val loss: 0.5028406977653503\n",
      "Epoch 274: train loss: 1.0614783718665421e-08, val loss: 0.5028759241104126\n",
      "Epoch 275: train loss: 1.0588453669413411e-08, val loss: 0.5029126405715942\n",
      "Epoch 276: train loss: 1.0573226738586072e-08, val loss: 0.5029506087303162\n",
      "Epoch 277: train loss: 1.054829645852351e-08, val loss: 0.5029892325401306\n",
      "Epoch 278: train loss: 1.05203072919835e-08, val loss: 0.5030285716056824\n",
      "Epoch 279: train loss: 1.0500291303117137e-08, val loss: 0.5030695796012878\n",
      "Epoch 280: train loss: 1.0453424792444821e-08, val loss: 0.5031108856201172\n",
      "Epoch 281: train loss: 1.0442671616317512e-08, val loss: 0.5031527280807495\n",
      "Epoch 282: train loss: 1.0422494867157184e-08, val loss: 0.5031951069831848\n",
      "Epoch 283: train loss: 1.0372664505098328e-08, val loss: 0.5032379031181335\n",
      "Epoch 284: train loss: 1.0340921008378245e-08, val loss: 0.50328129529953\n",
      "Epoch 285: train loss: 1.0262174221509213e-08, val loss: 0.5033260583877563\n",
      "Epoch 286: train loss: 1.0221166135693238e-08, val loss: 0.5033718943595886\n",
      "Epoch 287: train loss: 1.0221465451820677e-08, val loss: 0.5034183859825134\n",
      "Epoch 288: train loss: 1.027071405701463e-08, val loss: 0.5034661293029785\n",
      "Epoch 289: train loss: 1.0316790977071832e-08, val loss: 0.5035147070884705\n",
      "Epoch 290: train loss: 1.0372743552977681e-08, val loss: 0.5035646557807922\n",
      "Epoch 291: train loss: 1.0431445929270922e-08, val loss: 0.5036152005195618\n",
      "Epoch 292: train loss: 1.0468268918373269e-08, val loss: 0.5036669969558716\n",
      "Epoch 293: train loss: 1.0472965605856643e-08, val loss: 0.5037192702293396\n",
      "Epoch 294: train loss: 1.0449835663450813e-08, val loss: 0.5037728548049927\n",
      "Epoch 295: train loss: 1.0425464047614241e-08, val loss: 0.5038269758224487\n",
      "Epoch 296: train loss: 1.0398945704537255e-08, val loss: 0.5038821697235107\n",
      "Epoch 297: train loss: 1.0345310386128403e-08, val loss: 0.5039383769035339\n",
      "Epoch 298: train loss: 1.0308472297992921e-08, val loss: 0.5039951205253601\n",
      "Epoch 299: train loss: 1.0275112316548984e-08, val loss: 0.504052996635437\n",
      "Epoch 300: train loss: 1.0269181061062227e-08, val loss: 0.5041118264198303\n",
      "Epoch 301: train loss: 1.0258147220554292e-08, val loss: 0.5041713118553162\n",
      "Epoch 302: train loss: 1.0255631899269702e-08, val loss: 0.5042312741279602\n",
      "Epoch 303: train loss: 1.0259341820528789e-08, val loss: 0.5042920112609863\n",
      "Epoch 304: train loss: 1.0260512439685954e-08, val loss: 0.5043537020683289\n",
      "Epoch 305: train loss: 1.0256953508758215e-08, val loss: 0.504416286945343\n",
      "Epoch 306: train loss: 1.0245821968624114e-08, val loss: 0.5044801831245422\n",
      "Epoch 307: train loss: 1.0257908300559393e-08, val loss: 0.5045452117919922\n",
      "Epoch 308: train loss: 1.0256115956508438e-08, val loss: 0.5046116709709167\n",
      "Epoch 309: train loss: 1.0243184966896024e-08, val loss: 0.5046794414520264\n",
      "Epoch 310: train loss: 1.023747842054945e-08, val loss: 0.5047487616539001\n",
      "Epoch 311: train loss: 1.0241635095553647e-08, val loss: 0.5048187971115112\n",
      "Epoch 312: train loss: 1.0241719472503519e-08, val loss: 0.5048898458480835\n",
      "Epoch 313: train loss: 1.0239926240274144e-08, val loss: 0.5049619078636169\n",
      "Epoch 314: train loss: 1.0258277782781988e-08, val loss: 0.5050345659255981\n",
      "Epoch 315: train loss: 1.0259266325363114e-08, val loss: 0.505107581615448\n",
      "Epoch 316: train loss: 1.0238326630940264e-08, val loss: 0.5051811933517456\n",
      "Epoch 317: train loss: 1.0213946133319496e-08, val loss: 0.5052546858787537\n",
      "Epoch 318: train loss: 1.0197896749275515e-08, val loss: 0.5053289532661438\n",
      "Epoch 319: train loss: 1.0153073937146928e-08, val loss: 0.505403459072113\n",
      "Epoch 320: train loss: 1.0107090275823793e-08, val loss: 0.5054787993431091\n",
      "Epoch 321: train loss: 1.0143495821068882e-08, val loss: 0.5055548548698425\n",
      "Epoch 322: train loss: 1.0309745945846771e-08, val loss: 0.5056315064430237\n",
      "Epoch 323: train loss: 1.0475616818439448e-08, val loss: 0.5057092905044556\n",
      "Epoch 324: train loss: 1.0629721991506358e-08, val loss: 0.5057874917984009\n",
      "Epoch 325: train loss: 1.0735845989984227e-08, val loss: 0.5058664679527283\n",
      "Epoch 326: train loss: 1.0803288930105737e-08, val loss: 0.5059458613395691\n",
      "Epoch 327: train loss: 1.0811617379147265e-08, val loss: 0.5060259103775024\n",
      "Epoch 328: train loss: 1.076811972922087e-08, val loss: 0.5061060190200806\n",
      "Epoch 329: train loss: 1.0662951410722599e-08, val loss: 0.5061864256858826\n",
      "Epoch 330: train loss: 1.0545031514652692e-08, val loss: 0.5062679648399353\n",
      "Epoch 331: train loss: 1.0375808656704066e-08, val loss: 0.506361722946167\n",
      "Epoch 332: train loss: 1.023218043627594e-08, val loss: 0.506456196308136\n",
      "Epoch 333: train loss: 1.0096983693586026e-08, val loss: 0.5065516829490662\n",
      "Epoch 334: train loss: 9.988875504518546e-09, val loss: 0.5066479444503784\n",
      "Epoch 335: train loss: 9.937782152746877e-09, val loss: 0.5067450404167175\n",
      "Epoch 336: train loss: 9.899589592521352e-09, val loss: 0.5068432688713074\n",
      "Epoch 337: train loss: 9.885575025236903e-09, val loss: 0.506942093372345\n",
      "Epoch 338: train loss: 9.888585950079687e-09, val loss: 0.5070416331291199\n",
      "Epoch 339: train loss: 9.876271356290545e-09, val loss: 0.5071418881416321\n",
      "Epoch 340: train loss: 9.887905605410197e-09, val loss: 0.5072427988052368\n",
      "Epoch 341: train loss: 9.89440973597766e-09, val loss: 0.5073438286781311\n",
      "Epoch 342: train loss: 9.927089372752107e-09, val loss: 0.5074456334114075\n",
      "Epoch 343: train loss: 9.95332349873479e-09, val loss: 0.5075477957725525\n",
      "Epoch 344: train loss: 9.971672376707374e-09, val loss: 0.5076505541801453\n",
      "Epoch 345: train loss: 1.0000477779215089e-08, val loss: 0.5077496767044067\n",
      "Epoch 346: train loss: 1.0030996477894405e-08, val loss: 0.5078522562980652\n",
      "Epoch 347: train loss: 1.0056735888497315e-08, val loss: 0.5079556703567505\n",
      "Epoch 348: train loss: 1.007644545580888e-08, val loss: 0.5080597400665283\n",
      "Epoch 349: train loss: 1.007819339093885e-08, val loss: 0.508164644241333\n",
      "Epoch 350: train loss: 1.0068052169742714e-08, val loss: 0.5082700252532959\n",
      "Epoch 351: train loss: 1.0046024456755731e-08, val loss: 0.5083762407302856\n",
      "Epoch 352: train loss: 1.0054034937923007e-08, val loss: 0.5084831118583679\n",
      "Epoch 353: train loss: 1.0053569532431084e-08, val loss: 0.5085906386375427\n",
      "Epoch 354: train loss: 1.004025040884926e-08, val loss: 0.5086986422538757\n",
      "Epoch 355: train loss: 1.0038313291715895e-08, val loss: 0.5088068842887878\n",
      "Epoch 356: train loss: 1.0030488439838336e-08, val loss: 0.5089156031608582\n",
      "Epoch 357: train loss: 1.0008809780970296e-08, val loss: 0.5090241432189941\n",
      "Epoch 358: train loss: 9.982144888454059e-09, val loss: 0.5091330409049988\n",
      "Epoch 359: train loss: 9.941953038605789e-09, val loss: 0.5092424154281616\n",
      "Epoch 360: train loss: 9.895936514681125e-09, val loss: 0.5093521475791931\n",
      "Epoch 361: train loss: 9.847386017725057e-09, val loss: 0.5094624161720276\n",
      "Epoch 362: train loss: 9.788947430422468e-09, val loss: 0.5095729231834412\n",
      "Epoch 363: train loss: 9.742008089119736e-09, val loss: 0.5096842050552368\n",
      "Epoch 364: train loss: 9.693518876474627e-09, val loss: 0.509796142578125\n",
      "Epoch 365: train loss: 9.652053378772507e-09, val loss: 0.5099087953567505\n",
      "Epoch 366: train loss: 9.632588948704779e-09, val loss: 0.5100224018096924\n",
      "Epoch 367: train loss: 9.607956208412816e-09, val loss: 0.5101369619369507\n",
      "Epoch 368: train loss: 9.59032142588967e-09, val loss: 0.5102521777153015\n",
      "Epoch 369: train loss: 9.586569760244856e-09, val loss: 0.5103681683540344\n",
      "Epoch 370: train loss: 9.600871209158868e-09, val loss: 0.5104845762252808\n",
      "Epoch 371: train loss: 9.594166350268551e-09, val loss: 0.5106006264686584\n",
      "Epoch 372: train loss: 9.605885864516495e-09, val loss: 0.5107169151306152\n",
      "Epoch 373: train loss: 9.617665774896977e-09, val loss: 0.5108332633972168\n",
      "Epoch 374: train loss: 9.625248154065957e-09, val loss: 0.5109497308731079\n",
      "Epoch 375: train loss: 9.619268936944536e-09, val loss: 0.5110660195350647\n",
      "Epoch 376: train loss: 9.629746777761738e-09, val loss: 0.5111835598945618\n",
      "Epoch 377: train loss: 9.591142102749473e-09, val loss: 0.5113017559051514\n",
      "Epoch 378: train loss: 9.554026902947044e-09, val loss: 0.5114204287528992\n",
      "Epoch 379: train loss: 9.519528276769051e-09, val loss: 0.5115394592285156\n",
      "Epoch 380: train loss: 9.526312183538721e-09, val loss: 0.5116588473320007\n",
      "Epoch 381: train loss: 9.510484844099665e-09, val loss: 0.5117794871330261\n",
      "Epoch 382: train loss: 9.502366893343606e-09, val loss: 0.511900782585144\n",
      "Epoch 383: train loss: 9.481432527991274e-09, val loss: 0.5120229721069336\n",
      "Epoch 384: train loss: 9.470195294625228e-09, val loss: 0.5121456384658813\n",
      "Epoch 385: train loss: 9.466599948382282e-09, val loss: 0.5122685432434082\n",
      "Epoch 386: train loss: 9.448678284229572e-09, val loss: 0.5123922228813171\n",
      "Epoch 387: train loss: 9.407799872462874e-09, val loss: 0.5125159621238708\n",
      "Epoch 388: train loss: 9.508321241469275e-09, val loss: 0.5126399397850037\n",
      "Epoch 389: train loss: 9.619773422286926e-09, val loss: 0.512763500213623\n",
      "Epoch 390: train loss: 9.622236341044754e-09, val loss: 0.5128869414329529\n",
      "Epoch 391: train loss: 9.489836472198476e-09, val loss: 0.5130099654197693\n",
      "Epoch 392: train loss: 9.329363059862317e-09, val loss: 0.5131327509880066\n",
      "Epoch 393: train loss: 9.236215348096266e-09, val loss: 0.5132552981376648\n",
      "Epoch 394: train loss: 9.195050054700005e-09, val loss: 0.5133780837059021\n",
      "Epoch 395: train loss: 9.195177064214022e-09, val loss: 0.5135012865066528\n",
      "Epoch 396: train loss: 9.237588471933122e-09, val loss: 0.5136250853538513\n",
      "Epoch 397: train loss: 9.406642575982005e-09, val loss: 0.5137494206428528\n",
      "Epoch 398: train loss: 9.58623136426695e-09, val loss: 0.5138744115829468\n",
      "Epoch 399: train loss: 9.708413628572998e-09, val loss: 0.5140000581741333\n",
      "Epoch 400: train loss: 9.720193538953481e-09, val loss: 0.5141264796257019\n",
      "Epoch 401: train loss: 9.652358023970464e-09, val loss: 0.5142537951469421\n",
      "Epoch 402: train loss: 9.500314313015679e-09, val loss: 0.5143815279006958\n",
      "Epoch 403: train loss: 9.34078592251808e-09, val loss: 0.5145099759101868\n",
      "Epoch 404: train loss: 9.16215991964009e-09, val loss: 0.514638364315033\n",
      "Epoch 405: train loss: 9.011793089541698e-09, val loss: 0.5147672891616821\n",
      "Epoch 406: train loss: 8.9331786412572e-09, val loss: 0.5148960947990417\n",
      "Epoch 407: train loss: 8.896351211262754e-09, val loss: 0.5150254964828491\n",
      "Epoch 408: train loss: 8.898947356783538e-09, val loss: 0.5151540637016296\n",
      "Epoch 409: train loss: 8.889044167403881e-09, val loss: 0.5152825117111206\n",
      "Epoch 410: train loss: 8.870926215820418e-09, val loss: 0.5154104232788086\n",
      "Epoch 411: train loss: 8.879169399733655e-09, val loss: 0.515537440776825\n",
      "Epoch 412: train loss: 8.93134455282052e-09, val loss: 0.5156645178794861\n",
      "Epoch 413: train loss: 8.966875242322203e-09, val loss: 0.5157915353775024\n",
      "Epoch 414: train loss: 9.016853930177149e-09, val loss: 0.5159188508987427\n",
      "Epoch 415: train loss: 9.052948612975342e-09, val loss: 0.5160468816757202\n",
      "Epoch 416: train loss: 9.089788477467664e-09, val loss: 0.5161751508712769\n",
      "Epoch 417: train loss: 9.104984322050313e-09, val loss: 0.5163044333457947\n",
      "Epoch 418: train loss: 9.089515806692816e-09, val loss: 0.5164340138435364\n",
      "Epoch 419: train loss: 9.05085251190485e-09, val loss: 0.5165635347366333\n",
      "Epoch 420: train loss: 9.018178204200922e-09, val loss: 0.5166929364204407\n",
      "Epoch 421: train loss: 8.981996479917598e-09, val loss: 0.516822338104248\n",
      "Epoch 422: train loss: 8.960889807951844e-09, val loss: 0.5169480443000793\n",
      "Epoch 423: train loss: 8.949579743955383e-09, val loss: 0.517072856426239\n",
      "Epoch 424: train loss: 8.968972231571115e-09, val loss: 0.5172185897827148\n",
      "Epoch 425: train loss: 8.986424049339803e-09, val loss: 0.5173649191856384\n",
      "Epoch 426: train loss: 9.000763689925861e-09, val loss: 0.5175120234489441\n",
      "Epoch 427: train loss: 8.993290556702505e-09, val loss: 0.5176602005958557\n",
      "Epoch 428: train loss: 8.916506644141009e-09, val loss: 0.5178090333938599\n",
      "Epoch 429: train loss: 8.823481500996877e-09, val loss: 0.517957866191864\n",
      "Epoch 430: train loss: 8.721581679083101e-09, val loss: 0.518106997013092\n",
      "Epoch 431: train loss: 8.627424108453852e-09, val loss: 0.5182555913925171\n",
      "Epoch 432: train loss: 8.565194775655982e-09, val loss: 0.5184032320976257\n",
      "Epoch 433: train loss: 8.527551109693832e-09, val loss: 0.5185505151748657\n",
      "Epoch 434: train loss: 8.533532991350512e-09, val loss: 0.518697202205658\n",
      "Epoch 435: train loss: 8.572038190379772e-09, val loss: 0.5188437700271606\n",
      "Epoch 436: train loss: 8.608381563135481e-09, val loss: 0.5189899206161499\n",
      "Epoch 437: train loss: 8.654465588620042e-09, val loss: 0.5191356539726257\n",
      "Epoch 438: train loss: 8.713628929513106e-09, val loss: 0.5192817449569702\n",
      "Epoch 439: train loss: 8.753530345018135e-09, val loss: 0.5194279551506042\n",
      "Epoch 440: train loss: 8.787526262210577e-09, val loss: 0.5195743441581726\n",
      "Epoch 441: train loss: 8.89556517336132e-09, val loss: 0.5197210311889648\n",
      "Epoch 442: train loss: 9.120587840527605e-09, val loss: 0.5198678970336914\n",
      "Epoch 443: train loss: 9.138150680598756e-09, val loss: 0.5200153589248657\n",
      "Epoch 444: train loss: 8.922286021117998e-09, val loss: 0.5201646089553833\n",
      "Epoch 445: train loss: 8.643232796146094e-09, val loss: 0.5203145146369934\n",
      "Epoch 446: train loss: 8.35091373829755e-09, val loss: 0.5204646587371826\n",
      "Epoch 447: train loss: 8.186503919205279e-09, val loss: 0.520614743232727\n",
      "Epoch 448: train loss: 8.118867356188275e-09, val loss: 0.5207648873329163\n",
      "Epoch 449: train loss: 8.184894539908782e-09, val loss: 0.5209145545959473\n",
      "Epoch 450: train loss: 8.326592748630901e-09, val loss: 0.5210632681846619\n",
      "Epoch 451: train loss: 8.489988267967874e-09, val loss: 0.5212108492851257\n",
      "Epoch 452: train loss: 8.549728924833744e-09, val loss: 0.521357536315918\n",
      "Epoch 453: train loss: 8.484199121028269e-09, val loss: 0.5215029716491699\n",
      "Epoch 454: train loss: 8.355225844525194e-09, val loss: 0.5216479897499084\n",
      "Epoch 455: train loss: 8.234226633874187e-09, val loss: 0.5217926502227783\n",
      "Epoch 456: train loss: 8.159378062089218e-09, val loss: 0.5219384431838989\n",
      "Epoch 457: train loss: 8.145364382983189e-09, val loss: 0.5220853686332703\n",
      "Epoch 458: train loss: 8.205024215612866e-09, val loss: 0.5222330093383789\n",
      "Epoch 459: train loss: 8.350210300989147e-09, val loss: 0.5223814249038696\n",
      "Epoch 460: train loss: 8.486058078460701e-09, val loss: 0.5225300788879395\n",
      "Epoch 461: train loss: 8.55542836575296e-09, val loss: 0.522678554058075\n",
      "Epoch 462: train loss: 8.529559281100774e-09, val loss: 0.522826611995697\n",
      "Epoch 463: train loss: 8.469253742759975e-09, val loss: 0.5229743719100952\n",
      "Epoch 464: train loss: 8.378236771022785e-09, val loss: 0.5231220722198486\n",
      "Epoch 465: train loss: 8.318111532901185e-09, val loss: 0.5232697129249573\n",
      "Epoch 466: train loss: 8.2661157918551e-09, val loss: 0.5234173536300659\n",
      "Epoch 467: train loss: 8.193425493630002e-09, val loss: 0.5235653519630432\n",
      "Epoch 468: train loss: 8.126596284796506e-09, val loss: 0.5237135291099548\n",
      "Epoch 469: train loss: 8.067817525159171e-09, val loss: 0.5238620638847351\n",
      "Epoch 470: train loss: 8.026322717569201e-09, val loss: 0.524010419845581\n",
      "Epoch 471: train loss: 8.017242869584607e-09, val loss: 0.5241581797599792\n",
      "Epoch 472: train loss: 7.998059103897504e-09, val loss: 0.5243052840232849\n",
      "Epoch 473: train loss: 7.980645477800863e-09, val loss: 0.5244521498680115\n",
      "Epoch 474: train loss: 7.89597454087243e-09, val loss: 0.5245992541313171\n",
      "Epoch 475: train loss: 7.817671843213247e-09, val loss: 0.5247455835342407\n",
      "Epoch 476: train loss: 7.840188942509485e-09, val loss: 0.5248920321464539\n",
      "Epoch 477: train loss: 7.889825681672846e-09, val loss: 0.5250383019447327\n",
      "Epoch 478: train loss: 7.909564558872262e-09, val loss: 0.525185227394104\n",
      "Epoch 479: train loss: 7.946558966409611e-09, val loss: 0.5253318548202515\n",
      "Epoch 480: train loss: 7.984672478755783e-09, val loss: 0.5254777073860168\n",
      "Epoch 481: train loss: 7.988257166857693e-09, val loss: 0.525622546672821\n",
      "Epoch 482: train loss: 7.927971168442127e-09, val loss: 0.5257658958435059\n",
      "Epoch 483: train loss: 7.830590398327786e-09, val loss: 0.5259079933166504\n",
      "Epoch 484: train loss: 7.778665711555277e-09, val loss: 0.5260491967201233\n",
      "Epoch 485: train loss: 7.739108909277093e-09, val loss: 0.5261897444725037\n",
      "Epoch 486: train loss: 7.784423772250193e-09, val loss: 0.5263299345970154\n",
      "Epoch 487: train loss: 7.869937590498921e-09, val loss: 0.5264701247215271\n",
      "Epoch 488: train loss: 7.950101021947376e-09, val loss: 0.526611328125\n",
      "Epoch 489: train loss: 7.997729589703795e-09, val loss: 0.5267531275749207\n",
      "Epoch 490: train loss: 7.976705518331073e-09, val loss: 0.5268957614898682\n",
      "Epoch 491: train loss: 7.876061580702753e-09, val loss: 0.5270385146141052\n",
      "Epoch 492: train loss: 7.768655052586837e-09, val loss: 0.5271807312965393\n",
      "Epoch 493: train loss: 7.669359369799622e-09, val loss: 0.5273219347000122\n",
      "Epoch 494: train loss: 7.578189631374244e-09, val loss: 0.5274614691734314\n",
      "Epoch 495: train loss: 7.503929921881536e-09, val loss: 0.527599573135376\n",
      "Epoch 496: train loss: 7.474947771868301e-09, val loss: 0.5277366638183594\n",
      "Epoch 497: train loss: 7.517762412589946e-09, val loss: 0.5278730392456055\n",
      "Epoch 498: train loss: 7.58826956825942e-09, val loss: 0.528009295463562\n",
      "Epoch 499: train loss: 7.7201329773402e-09, val loss: 0.5281409621238708\n",
      "Epoch 500: train loss: 7.911534538607157e-09, val loss: 0.5282658934593201\n",
      "Epoch 501: train loss: 8.01030175523465e-09, val loss: 0.5283846259117126\n",
      "Epoch 502: train loss: 7.988863792718348e-09, val loss: 0.5284981727600098\n",
      "Epoch 503: train loss: 7.868749207773362e-09, val loss: 0.5286085605621338\n",
      "Epoch 504: train loss: 7.747879671171631e-09, val loss: 0.5287176370620728\n",
      "Epoch 505: train loss: 7.635801324568092e-09, val loss: 0.5288252234458923\n",
      "Epoch 506: train loss: 7.520138289862643e-09, val loss: 0.5289304852485657\n",
      "Epoch 507: train loss: 7.442178873162675e-09, val loss: 0.5290364623069763\n",
      "Epoch 508: train loss: 7.373987642722568e-09, val loss: 0.5291422009468079\n",
      "Epoch 509: train loss: 7.399757695480957e-09, val loss: 0.5292506217956543\n",
      "Epoch 510: train loss: 7.458562656381673e-09, val loss: 0.5293605327606201\n",
      "Epoch 511: train loss: 7.540652546822457e-09, val loss: 0.5294713973999023\n",
      "Epoch 512: train loss: 7.639854970875604e-09, val loss: 0.5295818448066711\n",
      "Epoch 513: train loss: 7.741520313686578e-09, val loss: 0.5296840071678162\n",
      "Epoch 514: train loss: 7.768977461353188e-09, val loss: 0.5297852158546448\n",
      "Epoch 515: train loss: 7.73505970386168e-09, val loss: 0.5298859477043152\n",
      "Epoch 516: train loss: 7.652548816849958e-09, val loss: 0.5299859642982483\n",
      "Epoch 517: train loss: 7.602499962899856e-09, val loss: 0.5300816893577576\n",
      "Epoch 518: train loss: 7.525939871300125e-09, val loss: 0.5301738977432251\n",
      "Epoch 519: train loss: 7.464767470821698e-09, val loss: 0.530265212059021\n",
      "Epoch 520: train loss: 7.422360059905486e-09, val loss: 0.5303582549095154\n",
      "Epoch 521: train loss: 7.375672961273949e-09, val loss: 0.5304528474807739\n",
      "Epoch 522: train loss: 7.376774746603587e-09, val loss: 0.5305467844009399\n",
      "Epoch 523: train loss: 7.415433156410245e-09, val loss: 0.5306398868560791\n",
      "Epoch 524: train loss: 7.429522330681948e-09, val loss: 0.5307315587997437\n",
      "Epoch 525: train loss: 7.435628557317386e-09, val loss: 0.5308163166046143\n",
      "Epoch 526: train loss: 7.430235982042177e-09, val loss: 0.5308955311775208\n",
      "Epoch 527: train loss: 7.4194268506744265e-09, val loss: 0.5309750437736511\n",
      "Epoch 528: train loss: 7.382230826635805e-09, val loss: 0.5310553908348083\n",
      "Epoch 529: train loss: 7.339574281672867e-09, val loss: 0.5311350226402283\n",
      "Epoch 530: train loss: 7.319894024249152e-09, val loss: 0.5312129855155945\n",
      "Epoch 531: train loss: 7.334841178874285e-09, val loss: 0.5312896966934204\n",
      "Epoch 532: train loss: 7.350936748196091e-09, val loss: 0.531359851360321\n",
      "Epoch 533: train loss: 7.361480758305561e-09, val loss: 0.5314245820045471\n",
      "Epoch 534: train loss: 7.337600749224293e-09, val loss: 0.5314897894859314\n",
      "Epoch 535: train loss: 7.339318042198784e-09, val loss: 0.5315548777580261\n",
      "Epoch 536: train loss: 7.365196896813586e-09, val loss: 0.5316192507743835\n",
      "Epoch 537: train loss: 7.341910635005888e-09, val loss: 0.5316759943962097\n",
      "Epoch 538: train loss: 7.310773764146461e-09, val loss: 0.5317302346229553\n",
      "Epoch 539: train loss: 7.295045900690411e-09, val loss: 0.5317820906639099\n",
      "Epoch 540: train loss: 7.272566104887801e-09, val loss: 0.5318256616592407\n",
      "Epoch 541: train loss: 7.25127380363233e-09, val loss: 0.5318678617477417\n",
      "Epoch 542: train loss: 7.223297959768615e-09, val loss: 0.5319085717201233\n",
      "Epoch 543: train loss: 7.193296625018775e-09, val loss: 0.5319469571113586\n",
      "Epoch 544: train loss: 7.185903871942401e-09, val loss: 0.5319778323173523\n",
      "Epoch 545: train loss: 7.1807297885584376e-09, val loss: 0.5320062637329102\n",
      "Epoch 546: train loss: 7.227307197155142e-09, val loss: 0.5320326685905457\n",
      "Epoch 547: train loss: 7.2759083202811325e-09, val loss: 0.5320567488670349\n",
      "Epoch 548: train loss: 7.2745724999379036e-09, val loss: 0.5320731997489929\n",
      "Epoch 549: train loss: 7.270934077041602e-09, val loss: 0.5320879220962524\n",
      "Epoch 550: train loss: 7.669849644287297e-09, val loss: 0.532102644443512\n",
      "Epoch 551: train loss: 8.136153084592479e-09, val loss: 0.5321164131164551\n",
      "Epoch 552: train loss: 8.250183647362519e-09, val loss: 0.5321279764175415\n",
      "Epoch 553: train loss: 8.024706232845347e-09, val loss: 0.5321354269981384\n",
      "Epoch 554: train loss: 7.731751239248297e-09, val loss: 0.5321318507194519\n",
      "Epoch 555: train loss: 7.508162092051407e-09, val loss: 0.5321251153945923\n",
      "Epoch 556: train loss: 7.370616561530596e-09, val loss: 0.5321146845817566\n",
      "Epoch 557: train loss: 7.238550203680916e-09, val loss: 0.5320987105369568\n",
      "Epoch 558: train loss: 7.147553660047379e-09, val loss: 0.5320701599121094\n",
      "Epoch 559: train loss: 7.129316248466466e-09, val loss: 0.5320357084274292\n",
      "Epoch 560: train loss: 7.246866662313778e-09, val loss: 0.5319973230361938\n",
      "Epoch 561: train loss: 7.418599512476476e-09, val loss: 0.5319560766220093\n",
      "Epoch 562: train loss: 7.514496580540708e-09, val loss: 0.5319121479988098\n",
      "Epoch 563: train loss: 7.50024398143978e-09, val loss: 0.5318900942802429\n",
      "Epoch 564: train loss: 7.478313968078965e-09, val loss: 0.532025933265686\n",
      "Epoch 565: train loss: 7.477553687351701e-09, val loss: 0.5321645140647888\n",
      "Epoch 566: train loss: 7.385755118605175e-09, val loss: 0.5323056578636169\n",
      "Epoch 567: train loss: 7.2344783497158005e-09, val loss: 0.5324479937553406\n",
      "Epoch 568: train loss: 7.051886630193849e-09, val loss: 0.532589316368103\n",
      "Epoch 569: train loss: 6.946095254534157e-09, val loss: 0.5327224731445312\n",
      "Epoch 570: train loss: 6.8660206409276725e-09, val loss: 0.5328546762466431\n",
      "Epoch 571: train loss: 6.91368340355325e-09, val loss: 0.532986581325531\n",
      "Epoch 572: train loss: 7.00242086537628e-09, val loss: 0.5331183075904846\n",
      "Epoch 573: train loss: 7.005462876463753e-09, val loss: 0.5332518815994263\n",
      "Epoch 574: train loss: 6.9258820900586215e-09, val loss: 0.5333809852600098\n",
      "Epoch 575: train loss: 6.844212308010356e-09, val loss: 0.5335144996643066\n",
      "Epoch 576: train loss: 6.814650177489057e-09, val loss: 0.5336471796035767\n",
      "Epoch 577: train loss: 6.833416055229691e-09, val loss: 0.5337784886360168\n",
      "Epoch 578: train loss: 6.909409488997653e-09, val loss: 0.5339081883430481\n",
      "Epoch 579: train loss: 7.003345015021978e-09, val loss: 0.5340357422828674\n",
      "Epoch 580: train loss: 7.099414833788842e-09, val loss: 0.5341548919677734\n",
      "Epoch 581: train loss: 7.094833165410819e-09, val loss: 0.5342766642570496\n",
      "Epoch 582: train loss: 7.00626978655805e-09, val loss: 0.5344017148017883\n",
      "Epoch 583: train loss: 6.90802570701976e-09, val loss: 0.5345287322998047\n",
      "Epoch 584: train loss: 6.844681266215957e-09, val loss: 0.534655749797821\n",
      "Epoch 585: train loss: 6.817714393037022e-09, val loss: 0.5347756147384644\n",
      "Epoch 586: train loss: 6.856167189539519e-09, val loss: 0.5348960757255554\n",
      "Epoch 587: train loss: 6.936486496300631e-09, val loss: 0.5350175499916077\n",
      "Epoch 588: train loss: 6.9700876181855165e-09, val loss: 0.5351404547691345\n",
      "Epoch 589: train loss: 6.938154495372828e-09, val loss: 0.5352649092674255\n",
      "Epoch 590: train loss: 6.853131839790194e-09, val loss: 0.5353878140449524\n",
      "Epoch 591: train loss: 6.809588004585976e-09, val loss: 0.5355002284049988\n",
      "Epoch 592: train loss: 6.738690050411833e-09, val loss: 0.5356154441833496\n",
      "Epoch 593: train loss: 6.6669261222784826e-09, val loss: 0.5357314348220825\n",
      "Epoch 594: train loss: 6.65173871539082e-09, val loss: 0.5358479619026184\n",
      "Epoch 595: train loss: 6.69388677820848e-09, val loss: 0.535963773727417\n",
      "Epoch 596: train loss: 6.780819905571889e-09, val loss: 0.5360795259475708\n",
      "Epoch 597: train loss: 6.858768664130821e-09, val loss: 0.5361951589584351\n",
      "Epoch 598: train loss: 6.899507631885626e-09, val loss: 0.5363116264343262\n",
      "Epoch 599: train loss: 6.947879160890125e-09, val loss: 0.5364207625389099\n",
      "Epoch 600: train loss: 6.909671501631465e-09, val loss: 0.5365326404571533\n",
      "Epoch 601: train loss: 6.817730380248577e-09, val loss: 0.5366466641426086\n",
      "Epoch 602: train loss: 6.700838106610263e-09, val loss: 0.5367599725723267\n",
      "Epoch 603: train loss: 6.6178902358160485e-09, val loss: 0.5368725061416626\n",
      "Epoch 604: train loss: 6.557459464318072e-09, val loss: 0.5369834303855896\n",
      "Epoch 605: train loss: 6.553116715934948e-09, val loss: 0.537086009979248\n",
      "Epoch 606: train loss: 6.540429087209532e-09, val loss: 0.5371922850608826\n",
      "Epoch 607: train loss: 6.570774147007796e-09, val loss: 0.5373023152351379\n",
      "Epoch 608: train loss: 6.582867140281223e-09, val loss: 0.5374155640602112\n",
      "Epoch 609: train loss: 6.602748570117001e-09, val loss: 0.5375292301177979\n",
      "Epoch 610: train loss: 6.614718550679299e-09, val loss: 0.5376423001289368\n",
      "Epoch 611: train loss: 6.6269625342840754e-09, val loss: 0.5377540588378906\n",
      "Epoch 612: train loss: 6.635668015064766e-09, val loss: 0.5378655791282654\n",
      "Epoch 613: train loss: 6.63989263571807e-09, val loss: 0.5379685163497925\n",
      "Epoch 614: train loss: 6.634761628987462e-09, val loss: 0.5380730032920837\n",
      "Epoch 615: train loss: 6.631933668899137e-09, val loss: 0.5381787419319153\n",
      "Epoch 616: train loss: 6.620311854277361e-09, val loss: 0.5382855534553528\n",
      "Epoch 617: train loss: 6.577052236167447e-09, val loss: 0.5383927226066589\n",
      "Epoch 618: train loss: 6.521228890221664e-09, val loss: 0.5384992957115173\n",
      "Epoch 619: train loss: 6.503779292899026e-09, val loss: 0.5386067032814026\n",
      "Epoch 620: train loss: 6.504473404334021e-09, val loss: 0.5387163758277893\n",
      "Epoch 621: train loss: 6.578134037482641e-09, val loss: 0.5388180613517761\n",
      "Epoch 622: train loss: 6.608459557355673e-09, val loss: 0.5389215350151062\n",
      "Epoch 623: train loss: 6.624742976413245e-09, val loss: 0.539027214050293\n",
      "Epoch 624: train loss: 6.600068047646346e-09, val loss: 0.5391340255737305\n",
      "Epoch 625: train loss: 6.563344534527005e-09, val loss: 0.5392405986785889\n",
      "Epoch 626: train loss: 6.517764550295624e-09, val loss: 0.53934645652771\n",
      "Epoch 627: train loss: 6.484529802008865e-09, val loss: 0.539451539516449\n",
      "Epoch 628: train loss: 6.4666356713871664e-09, val loss: 0.5395564436912537\n",
      "Epoch 629: train loss: 6.454523582277716e-09, val loss: 0.5396527647972107\n",
      "Epoch 630: train loss: 6.486426062934925e-09, val loss: 0.5397524237632751\n",
      "Epoch 631: train loss: 6.504065286350169e-09, val loss: 0.5398523211479187\n",
      "Epoch 632: train loss: 6.484029313469364e-09, val loss: 0.5399510264396667\n",
      "Epoch 633: train loss: 6.458264145692283e-09, val loss: 0.5400487184524536\n",
      "Epoch 634: train loss: 6.4109384467769814e-09, val loss: 0.5401478409767151\n",
      "Epoch 635: train loss: 6.427663290509145e-09, val loss: 0.5402482151985168\n",
      "Epoch 636: train loss: 6.4274061628566415e-09, val loss: 0.5403411388397217\n",
      "Epoch 637: train loss: 6.422040232934023e-09, val loss: 0.5404301285743713\n",
      "Epoch 638: train loss: 6.451653877803665e-09, val loss: 0.5405184626579285\n",
      "Epoch 639: train loss: 6.473605207446553e-09, val loss: 0.5405960083007812\n",
      "Epoch 640: train loss: 6.460741719394036e-09, val loss: 0.5406746864318848\n",
      "Epoch 641: train loss: 6.4134764166112745e-09, val loss: 0.5407538414001465\n",
      "Epoch 642: train loss: 6.384982764728875e-09, val loss: 0.5408334136009216\n",
      "Epoch 643: train loss: 6.380507677761216e-09, val loss: 0.5409138798713684\n",
      "Epoch 644: train loss: 6.383009676369511e-09, val loss: 0.5409948229789734\n",
      "Epoch 645: train loss: 6.408368502519579e-09, val loss: 0.5410767197608948\n",
      "Epoch 646: train loss: 6.416054354474454e-09, val loss: 0.5411572456359863\n",
      "Epoch 647: train loss: 6.347587344635031e-09, val loss: 0.5412371754646301\n",
      "Epoch 648: train loss: 6.277416364497412e-09, val loss: 0.5413175821304321\n",
      "Epoch 649: train loss: 6.25772811346792e-09, val loss: 0.5413872599601746\n",
      "Epoch 650: train loss: 6.186992695944582e-09, val loss: 0.541461169719696\n",
      "Epoch 651: train loss: 6.2589191607287376e-09, val loss: 0.5415363311767578\n",
      "Epoch 652: train loss: 6.10759176566944e-09, val loss: 0.5416081547737122\n",
      "Epoch 653: train loss: 6.182069522964184e-09, val loss: 0.541676938533783\n",
      "Epoch 654: train loss: 6.150860265563551e-09, val loss: 0.541749119758606\n",
      "Epoch 655: train loss: 6.22810869543855e-09, val loss: 0.5418270826339722\n",
      "Epoch 656: train loss: 6.129595053749881e-09, val loss: 0.5419058203697205\n",
      "Epoch 657: train loss: 6.075701275420897e-09, val loss: 0.5419819951057434\n",
      "Epoch 658: train loss: 6.063391566613063e-09, val loss: 0.5420538783073425\n",
      "Epoch 659: train loss: 6.084798886973886e-09, val loss: 0.5421240329742432\n",
      "Epoch 660: train loss: 6.112291561777283e-09, val loss: 0.5421989560127258\n",
      "Epoch 661: train loss: 6.0640750199070226e-09, val loss: 0.5422661900520325\n",
      "Epoch 662: train loss: 6.0773177601447514e-09, val loss: 0.5423362851142883\n",
      "Epoch 663: train loss: 6.092340854024769e-09, val loss: 0.5424062609672546\n",
      "Epoch 664: train loss: 6.130927765468641e-09, val loss: 0.542471706867218\n",
      "Epoch 665: train loss: 6.052511380971737e-09, val loss: 0.5425328612327576\n",
      "Epoch 666: train loss: 6.024983179031551e-09, val loss: 0.5425931811332703\n",
      "Epoch 667: train loss: 6.015208775522751e-09, val loss: 0.542660117149353\n",
      "Epoch 668: train loss: 6.088620718713855e-09, val loss: 0.5427317023277283\n",
      "Epoch 669: train loss: 6.0716338623478805e-09, val loss: 0.5428014397621155\n",
      "Epoch 670: train loss: 6.0518043909496555e-09, val loss: 0.5428666472434998\n",
      "Epoch 671: train loss: 6.054266865618274e-09, val loss: 0.5429282784461975\n",
      "Epoch 672: train loss: 6.121467777120415e-09, val loss: 0.5429922342300415\n",
      "Epoch 673: train loss: 6.103063387996599e-09, val loss: 0.5430488586425781\n",
      "Epoch 674: train loss: 6.077185421560216e-09, val loss: 0.5431090593338013\n",
      "Epoch 675: train loss: 6.050068002139142e-09, val loss: 0.5431680679321289\n",
      "Epoch 676: train loss: 6.028352039777474e-09, val loss: 0.5432209372520447\n",
      "Epoch 677: train loss: 5.9985625355807315e-09, val loss: 0.543268620967865\n",
      "Epoch 678: train loss: 5.976249273231815e-09, val loss: 0.5433168411254883\n",
      "Epoch 679: train loss: 5.936340752299429e-09, val loss: 0.5433697700500488\n",
      "Epoch 680: train loss: 5.941526826092058e-09, val loss: 0.5434291958808899\n",
      "Epoch 681: train loss: 5.87405057927981e-09, val loss: 0.5434907078742981\n",
      "Epoch 682: train loss: 5.812631265200707e-09, val loss: 0.5435476303100586\n",
      "Epoch 683: train loss: 5.782622380934299e-09, val loss: 0.5436006188392639\n",
      "Epoch 684: train loss: 5.808048264555055e-09, val loss: 0.5436511039733887\n",
      "Epoch 685: train loss: 5.834638550084037e-09, val loss: 0.5436941981315613\n",
      "Epoch 686: train loss: 5.898144195271016e-09, val loss: 0.5437150001525879\n",
      "Epoch 687: train loss: 5.901261701524163e-09, val loss: 0.5437402725219727\n",
      "Epoch 688: train loss: 5.918733947396504e-09, val loss: 0.5437667965888977\n",
      "Epoch 689: train loss: 5.9175149225154655e-09, val loss: 0.5437853336334229\n",
      "Epoch 690: train loss: 5.936409586126956e-09, val loss: 0.5438017845153809\n",
      "Epoch 691: train loss: 5.905570255038128e-09, val loss: 0.5438300371170044\n",
      "Epoch 692: train loss: 5.836171546036439e-09, val loss: 0.543867826461792\n",
      "Epoch 693: train loss: 5.801640501346128e-09, val loss: 0.5439046025276184\n",
      "Epoch 694: train loss: 5.810107950310339e-09, val loss: 0.5439310073852539\n",
      "Epoch 695: train loss: 5.834584371200435e-09, val loss: 0.5439500212669373\n",
      "Epoch 696: train loss: 5.868917796192363e-09, val loss: 0.543975293636322\n",
      "Epoch 697: train loss: 5.845809614157815e-09, val loss: 0.5440086722373962\n",
      "Epoch 698: train loss: 5.76911451943829e-09, val loss: 0.5440410375595093\n",
      "Epoch 699: train loss: 5.743542530467494e-09, val loss: 0.5440674424171448\n",
      "Epoch 700: train loss: 5.706747074896157e-09, val loss: 0.5440878868103027\n",
      "Epoch 701: train loss: 5.704154037999842e-09, val loss: 0.5441066026687622\n",
      "Epoch 702: train loss: 5.733584718115026e-09, val loss: 0.5441247224807739\n",
      "Epoch 703: train loss: 5.797477609092994e-09, val loss: 0.5441516637802124\n",
      "Epoch 704: train loss: 5.781680023630997e-09, val loss: 0.5441784262657166\n",
      "Epoch 705: train loss: 5.807800906865168e-09, val loss: 0.5441846251487732\n",
      "Epoch 706: train loss: 5.762069932302438e-09, val loss: 0.5441882014274597\n",
      "Epoch 707: train loss: 5.776701783588578e-09, val loss: 0.5441851615905762\n",
      "Epoch 708: train loss: 5.785102619171312e-09, val loss: 0.5441795587539673\n",
      "Epoch 709: train loss: 5.7511577722380025e-09, val loss: 0.5441806316375732\n",
      "Epoch 710: train loss: 5.7168034750532115e-09, val loss: 0.544186532497406\n",
      "Epoch 711: train loss: 5.699603011777299e-09, val loss: 0.5441937446594238\n",
      "Epoch 712: train loss: 5.651941137330141e-09, val loss: 0.5441927313804626\n",
      "Epoch 713: train loss: 5.60473223387703e-09, val loss: 0.5441901087760925\n",
      "Epoch 714: train loss: 5.599360530794684e-09, val loss: 0.5441882014274597\n",
      "Epoch 715: train loss: 5.623990162462178e-09, val loss: 0.5441915988922119\n",
      "Epoch 716: train loss: 5.624958276939651e-09, val loss: 0.5441932082176208\n",
      "Epoch 717: train loss: 5.644297029760992e-09, val loss: 0.5441938638687134\n",
      "Epoch 718: train loss: 5.661291435643534e-09, val loss: 0.5441898703575134\n",
      "Epoch 719: train loss: 5.648000733771141e-09, val loss: 0.5441806316375732\n",
      "Epoch 720: train loss: 5.6365734302232795e-09, val loss: 0.5441773533821106\n",
      "Epoch 721: train loss: 5.617015297332273e-09, val loss: 0.5441751480102539\n",
      "Epoch 722: train loss: 5.633203237209727e-09, val loss: 0.5441707968711853\n",
      "Epoch 723: train loss: 5.651354495483929e-09, val loss: 0.5441595315933228\n",
      "Epoch 724: train loss: 5.655570234353036e-09, val loss: 0.5441434979438782\n",
      "Epoch 725: train loss: 5.617839082816545e-09, val loss: 0.544126570224762\n",
      "Epoch 726: train loss: 5.659339663566243e-09, val loss: 0.5440990328788757\n",
      "Epoch 727: train loss: 5.553514537126603e-09, val loss: 0.5440743565559387\n",
      "Epoch 728: train loss: 5.49179679509848e-09, val loss: 0.5440406203269958\n",
      "Epoch 729: train loss: 5.512701850562962e-09, val loss: 0.5439950227737427\n",
      "Epoch 730: train loss: 5.5435629420230725e-09, val loss: 0.5439557433128357\n",
      "Epoch 731: train loss: 5.5377902263842316e-09, val loss: 0.5439277291297913\n",
      "Epoch 732: train loss: 5.565594651812944e-09, val loss: 0.5438910722732544\n",
      "Epoch 733: train loss: 5.5846021140837365e-09, val loss: 0.5438383221626282\n",
      "Epoch 734: train loss: 5.565594207723734e-09, val loss: 0.5437881946563721\n",
      "Epoch 735: train loss: 5.525462309918794e-09, val loss: 0.5437460541725159\n",
      "Epoch 736: train loss: 5.489208643183474e-09, val loss: 0.5437093377113342\n",
      "Epoch 737: train loss: 5.447989170903611e-09, val loss: 0.5436672568321228\n",
      "Epoch 738: train loss: 5.463787200454817e-09, val loss: 0.5436200499534607\n",
      "Epoch 739: train loss: 5.489156684745922e-09, val loss: 0.543565571308136\n",
      "Epoch 740: train loss: 5.5346807137368614e-09, val loss: 0.5435105562210083\n",
      "Epoch 741: train loss: 5.56433210618934e-09, val loss: 0.5434653162956238\n",
      "Epoch 742: train loss: 5.588343565676723e-09, val loss: 0.543411910533905\n",
      "Epoch 743: train loss: 5.534452895972208e-09, val loss: 0.5433499217033386\n",
      "Epoch 744: train loss: 5.5003321897117985e-09, val loss: 0.5432841777801514\n",
      "Epoch 745: train loss: 5.441512573867158e-09, val loss: 0.543218731880188\n",
      "Epoch 746: train loss: 5.37738298334034e-09, val loss: 0.5431504845619202\n",
      "Epoch 747: train loss: 5.3480291306584604e-09, val loss: 0.5430735945701599\n",
      "Epoch 748: train loss: 5.499510624673576e-09, val loss: 0.5430073142051697\n",
      "Epoch 749: train loss: 5.765937061141813e-09, val loss: 0.5429390072822571\n",
      "Epoch 750: train loss: 5.861759966308e-09, val loss: 0.5428628325462341\n",
      "Epoch 751: train loss: 5.761310539753595e-09, val loss: 0.5427823066711426\n",
      "Epoch 752: train loss: 5.6242432933117925e-09, val loss: 0.5426989793777466\n",
      "Epoch 753: train loss: 5.5339159921175e-09, val loss: 0.542608916759491\n",
      "Epoch 754: train loss: 5.405427661031581e-09, val loss: 0.5425161719322205\n",
      "Epoch 755: train loss: 5.291285631869869e-09, val loss: 0.5424200892448425\n",
      "Epoch 756: train loss: 5.319563900485491e-09, val loss: 0.5423044562339783\n",
      "Epoch 757: train loss: 5.385249579603624e-09, val loss: 0.5421934723854065\n",
      "Epoch 758: train loss: 5.436690209137396e-09, val loss: 0.5420648455619812\n",
      "Epoch 759: train loss: 5.523389301487214e-09, val loss: 0.5419101119041443\n",
      "Epoch 760: train loss: 5.458525187407304e-09, val loss: 0.5417837500572205\n",
      "Epoch 761: train loss: 5.406863401447026e-09, val loss: 0.541652262210846\n",
      "Epoch 762: train loss: 5.419435122888672e-09, val loss: 0.5414910316467285\n",
      "Epoch 763: train loss: 5.446981088397251e-09, val loss: 0.5413362383842468\n",
      "Epoch 764: train loss: 5.482160503333944e-09, val loss: 0.5411879420280457\n",
      "Epoch 765: train loss: 5.50488943318328e-09, val loss: 0.5410390496253967\n",
      "Epoch 766: train loss: 5.571963779260614e-09, val loss: 0.540883481502533\n",
      "Epoch 767: train loss: 5.543030479060462e-09, val loss: 0.5407231450080872\n",
      "Epoch 768: train loss: 5.4504014634915166e-09, val loss: 0.5405640006065369\n",
      "Epoch 769: train loss: 5.475947695288141e-09, val loss: 0.5404018759727478\n",
      "Epoch 770: train loss: 5.718380435837389e-09, val loss: 0.5402300357818604\n",
      "Epoch 771: train loss: 7.023085224489023e-09, val loss: 0.5400416851043701\n",
      "Epoch 772: train loss: 1.0554880525148747e-08, val loss: 0.5398870706558228\n",
      "Epoch 773: train loss: 2.353761452411618e-08, val loss: 0.5396793484687805\n",
      "Epoch 774: train loss: 6.838855881596828e-08, val loss: 0.5395337343215942\n",
      "Epoch 775: train loss: 2.2647186881386006e-07, val loss: 0.539280354976654\n",
      "Epoch 776: train loss: 8.624419933767058e-07, val loss: 0.5391716361045837\n",
      "Epoch 777: train loss: 2.5289662062277785e-06, val loss: 0.5387777090072632\n",
      "Epoch 778: train loss: 8.213328328565694e-06, val loss: 0.5388439297676086\n",
      "Epoch 779: train loss: 2.5383333195350133e-06, val loss: 0.5379883050918579\n",
      "Epoch 780: train loss: 3.737536360404192e-07, val loss: 0.5380730628967285\n",
      "Epoch 781: train loss: 1.9580579646572005e-06, val loss: 0.5379694104194641\n",
      "Epoch 782: train loss: 1.6983702266770706e-07, val loss: 0.5370244383811951\n",
      "Epoch 783: train loss: 1.440174173694686e-06, val loss: 0.5373944044113159\n",
      "Epoch 784: train loss: 2.8837450827268185e-07, val loss: 0.5370232462882996\n",
      "Epoch 785: train loss: 8.762511356508185e-07, val loss: 0.5369579195976257\n",
      "Epoch 786: train loss: 8.025745046325028e-07, val loss: 0.5355406999588013\n",
      "Epoch 787: train loss: 2.1737066617788514e-06, val loss: 0.5375491380691528\n",
      "Epoch 788: train loss: 8.337094186572358e-06, val loss: 0.5343920588493347\n",
      "Epoch 789: train loss: 3.5309796658111736e-05, val loss: 0.5378581881523132\n",
      "Epoch 790: train loss: 6.696524360449985e-05, val loss: 0.539516270160675\n",
      "Epoch 791: train loss: 1.1274330063315574e-05, val loss: 0.5354806780815125\n",
      "Epoch 792: train loss: 6.178298644954339e-05, val loss: 0.5344504714012146\n",
      "Epoch 793: train loss: 8.760090167925227e-06, val loss: 0.539535403251648\n",
      "Epoch 794: train loss: 1.779761805664748e-05, val loss: 0.5423738360404968\n",
      "Epoch 795: train loss: 1.3861440493201371e-05, val loss: 0.5399918556213379\n",
      "Epoch 796: train loss: 7.35677122065681e-06, val loss: 0.5361927151679993\n",
      "Epoch 797: train loss: 1.4140848179522436e-05, val loss: 0.53569096326828\n",
      "Epoch 798: train loss: 8.383609383599833e-06, val loss: 0.5374109745025635\n",
      "Epoch 799: train loss: 5.259561476123054e-06, val loss: 0.5376861691474915\n",
      "Epoch 800: train loss: 7.728809578111395e-06, val loss: 0.5364848971366882\n",
      "Epoch 801: train loss: 9.081745702133048e-06, val loss: 0.5359017848968506\n",
      "Epoch 802: train loss: 3.047794507438084e-06, val loss: 0.5357912182807922\n",
      "Epoch 803: train loss: 5.999704171699705e-06, val loss: 0.5345922708511353\n",
      "Epoch 804: train loss: 5.0991889111173805e-06, val loss: 0.5333376526832581\n",
      "Epoch 805: train loss: 3.7225274809316033e-06, val loss: 0.5337578058242798\n",
      "Epoch 806: train loss: 2.9046925646980526e-06, val loss: 0.5342726707458496\n",
      "Epoch 807: train loss: 4.601981345331296e-06, val loss: 0.5330490469932556\n",
      "Epoch 808: train loss: 2.7028120257455157e-06, val loss: 0.5310854315757751\n",
      "Epoch 809: train loss: 2.414699565633782e-06, val loss: 0.5305140018463135\n",
      "Epoch 810: train loss: 2.619010047055781e-06, val loss: 0.5305798649787903\n",
      "Epoch 811: train loss: 2.5922081476892345e-06, val loss: 0.5296161770820618\n",
      "Epoch 812: train loss: 1.1877637007273734e-06, val loss: 0.5282862782478333\n",
      "Epoch 813: train loss: 2.278249212395167e-06, val loss: 0.528178870677948\n",
      "Epoch 814: train loss: 1.6927541537370416e-06, val loss: 0.5282570123672485\n",
      "Epoch 815: train loss: 1.324572394878487e-06, val loss: 0.5273639559745789\n",
      "Epoch 816: train loss: 1.0707203728088643e-06, val loss: 0.5260550379753113\n",
      "Epoch 817: train loss: 1.6875701476237737e-06, val loss: 0.5253955125808716\n",
      "Epoch 818: train loss: 6.7015878357779e-07, val loss: 0.5251461863517761\n",
      "Epoch 819: train loss: 9.245238743460504e-07, val loss: 0.5245401859283447\n",
      "Epoch 820: train loss: 1.1410430715841358e-06, val loss: 0.5237479209899902\n",
      "Epoch 821: train loss: 7.706429414611193e-07, val loss: 0.5232248306274414\n",
      "Epoch 822: train loss: 3.9471490254072705e-07, val loss: 0.5226442813873291\n",
      "Epoch 823: train loss: 9.875137720882776e-07, val loss: 0.521636962890625\n",
      "Epoch 824: train loss: 4.421806067966827e-07, val loss: 0.5206809043884277\n",
      "Epoch 825: train loss: 4.2752756712616247e-07, val loss: 0.5202121138572693\n",
      "Epoch 826: train loss: 5.308791628522158e-07, val loss: 0.5197831988334656\n",
      "Epoch 827: train loss: 5.682924779648602e-07, val loss: 0.5189155340194702\n",
      "Epoch 828: train loss: 1.5653229468171048e-07, val loss: 0.5178845524787903\n",
      "Epoch 829: train loss: 4.753709959004482e-07, val loss: 0.5170261263847351\n",
      "Epoch 830: train loss: 3.3193504123119055e-07, val loss: 0.5161300897598267\n",
      "Epoch 831: train loss: 2.1671711181170394e-07, val loss: 0.5150846838951111\n",
      "Epoch 832: train loss: 2.3699476514593698e-07, val loss: 0.5141915082931519\n",
      "Epoch 833: train loss: 3.1550757739751134e-07, val loss: 0.5134342312812805\n",
      "Epoch 834: train loss: 1.5103276496120088e-07, val loss: 0.5123682022094727\n",
      "Epoch 835: train loss: 1.882114304407878e-07, val loss: 0.5109894871711731\n",
      "Epoch 836: train loss: 2.0745893891671585e-07, val loss: 0.5098258852958679\n",
      "Epoch 837: train loss: 1.420426087861415e-07, val loss: 0.5089890956878662\n",
      "Epoch 838: train loss: 1.1175212222269693e-07, val loss: 0.508040726184845\n",
      "Epoch 839: train loss: 1.6406720249051432e-07, val loss: 0.5068208575248718\n",
      "Epoch 840: train loss: 1.0871022482206172e-07, val loss: 0.5055667757987976\n",
      "Epoch 841: train loss: 7.871103946399671e-08, val loss: 0.5042933821678162\n",
      "Epoch 842: train loss: 1.2246211156252684e-07, val loss: 0.5028502345085144\n",
      "Epoch 843: train loss: 7.628275966453657e-08, val loss: 0.5013932585716248\n",
      "Epoch 844: train loss: 6.803528407317572e-08, val loss: 0.500117301940918\n",
      "Epoch 845: train loss: 8.85720154997216e-08, val loss: 0.49881693720817566\n",
      "Epoch 846: train loss: 7.449794026115342e-08, val loss: 0.49725618958473206\n",
      "Epoch 847: train loss: 3.406931625704601e-08, val loss: 0.4955933690071106\n",
      "Epoch 848: train loss: 7.164839388451583e-08, val loss: 0.49402952194213867\n",
      "Epoch 849: train loss: 4.095777228485531e-08, val loss: 0.4924758970737457\n",
      "Epoch 850: train loss: 3.231760103972192e-08, val loss: 0.49083781242370605\n",
      "Epoch 851: train loss: 5.069511388455794e-08, val loss: 0.48915836215019226\n",
      "Epoch 852: train loss: 4.0151959979084495e-08, val loss: 0.48737746477127075\n",
      "Epoch 853: train loss: 2.4362712736092362e-08, val loss: 0.4853922426700592\n",
      "Epoch 854: train loss: 3.787782176800647e-08, val loss: 0.48334309458732605\n",
      "Epoch 855: train loss: 3.299261663869402e-08, val loss: 0.4814114570617676\n",
      "Epoch 856: train loss: 1.883070233077433e-08, val loss: 0.4794834554195404\n",
      "Epoch 857: train loss: 3.280753091416955e-08, val loss: 0.47737881541252136\n",
      "Epoch 858: train loss: 2.55325236508952e-08, val loss: 0.4751541316509247\n",
      "Epoch 859: train loss: 1.412988748228372e-08, val loss: 0.4729008376598358\n",
      "Epoch 860: train loss: 2.0100664244182553e-08, val loss: 0.47056636214256287\n",
      "Epoch 861: train loss: 1.7805476204557635e-08, val loss: 0.4681467115879059\n",
      "Epoch 862: train loss: 1.1142212485992786e-08, val loss: 0.46572351455688477\n",
      "Epoch 863: train loss: 2.0391016875009882e-08, val loss: 0.4632583558559418\n",
      "Epoch 864: train loss: 1.5251822915729463e-08, val loss: 0.46064862608909607\n",
      "Epoch 865: train loss: 1.1706159597224541e-08, val loss: 0.4579351842403412\n",
      "Epoch 866: train loss: 1.5651099971591975e-08, val loss: 0.455186665058136\n",
      "Epoch 867: train loss: 1.0936129335448186e-08, val loss: 0.45236697793006897\n",
      "Epoch 868: train loss: 9.942239920235352e-09, val loss: 0.44944992661476135\n",
      "Epoch 869: train loss: 1.356906498273247e-08, val loss: 0.44647812843322754\n",
      "Epoch 870: train loss: 9.133097833569082e-09, val loss: 0.4434548318386078\n",
      "Epoch 871: train loss: 8.606966694912899e-09, val loss: 0.4403485953807831\n",
      "Epoch 872: train loss: 1.1033956859307636e-08, val loss: 0.4371747672557831\n",
      "Epoch 873: train loss: 6.566980292888047e-09, val loss: 0.4339420795440674\n",
      "Epoch 874: train loss: 8.187453381935939e-09, val loss: 0.4306304156780243\n",
      "Epoch 875: train loss: 8.253315364470382e-09, val loss: 0.42725077271461487\n",
      "Epoch 876: train loss: 5.509822820215504e-09, val loss: 0.4238506853580475\n",
      "Epoch 877: train loss: 7.599902929200653e-09, val loss: 0.4204254746437073\n",
      "Epoch 878: train loss: 7.815635250096875e-09, val loss: 0.41806697845458984\n",
      "Epoch 879: train loss: 6.625848314456562e-09, val loss: 0.41803765296936035\n",
      "Epoch 880: train loss: 7.963302017799379e-09, val loss: 0.41801270842552185\n",
      "Epoch 881: train loss: 6.661022400322736e-09, val loss: 0.4179946482181549\n",
      "Epoch 882: train loss: 6.004358343858485e-09, val loss: 0.4179774224758148\n",
      "Epoch 883: train loss: 6.1493521386069006e-09, val loss: 0.4179691970348358\n",
      "Epoch 884: train loss: 5.418944404311787e-09, val loss: 0.4179655611515045\n",
      "Epoch 885: train loss: 5.415782933226865e-09, val loss: 0.4179462492465973\n",
      "Epoch 886: train loss: 5.63541702192083e-09, val loss: 0.4179113507270813\n",
      "Epoch 887: train loss: 5.126437052638266e-09, val loss: 0.4178941249847412\n",
      "Epoch 888: train loss: 5.340057285252442e-09, val loss: 0.4178926944732666\n",
      "Epoch 889: train loss: 5.321704854566178e-09, val loss: 0.41787979006767273\n",
      "Epoch 890: train loss: 5.280606618640604e-09, val loss: 0.4178615212440491\n",
      "Epoch 891: train loss: 5.4292539353184566e-09, val loss: 0.41784659028053284\n",
      "Epoch 892: train loss: 5.28728438808912e-09, val loss: 0.4178241789340973\n",
      "Epoch 893: train loss: 5.07548225670007e-09, val loss: 0.4178110659122467\n",
      "Epoch 894: train loss: 5.463883123724145e-09, val loss: 0.41780850291252136\n",
      "Epoch 895: train loss: 5.263483870976415e-09, val loss: 0.41778364777565\n",
      "Epoch 896: train loss: 5.231671540428806e-09, val loss: 0.4177466034889221\n",
      "Epoch 897: train loss: 5.314530149291841e-09, val loss: 0.4177250564098358\n",
      "Epoch 898: train loss: 4.656616869880281e-09, val loss: 0.41770386695861816\n",
      "Epoch 899: train loss: 4.5360781797398886e-09, val loss: 0.4176812171936035\n",
      "Epoch 900: train loss: 4.650590135213406e-09, val loss: 0.4176718294620514\n",
      "Epoch 901: train loss: 4.588256441451222e-09, val loss: 0.41765204071998596\n",
      "Epoch 902: train loss: 4.681947718410129e-09, val loss: 0.4176204204559326\n",
      "Epoch 903: train loss: 4.6807877573940004e-09, val loss: 0.4176008403301239\n",
      "Epoch 904: train loss: 4.689372445909612e-09, val loss: 0.4175897538661957\n",
      "Epoch 905: train loss: 4.834274758280799e-09, val loss: 0.4175718426704407\n",
      "Epoch 906: train loss: 4.869460834555639e-09, val loss: 0.4175485670566559\n",
      "Epoch 907: train loss: 4.869582070909928e-09, val loss: 0.41752567887306213\n",
      "Epoch 908: train loss: 4.7600492436572495e-09, val loss: 0.4175148606300354\n",
      "Epoch 909: train loss: 4.617100479720193e-09, val loss: 0.4175202548503876\n",
      "Epoch 910: train loss: 4.568473599420031e-09, val loss: 0.4175274968147278\n",
      "Epoch 911: train loss: 4.592034308359416e-09, val loss: 0.41751885414123535\n",
      "Epoch 912: train loss: 4.591318436553138e-09, val loss: 0.41749581694602966\n",
      "Epoch 913: train loss: 4.521278906821635e-09, val loss: 0.4174683690071106\n",
      "Epoch 914: train loss: 4.399700159751774e-09, val loss: 0.41744738817214966\n",
      "Epoch 915: train loss: 4.301159872710514e-09, val loss: 0.4174273610115051\n",
      "Epoch 916: train loss: 4.235647388384223e-09, val loss: 0.4174066185951233\n",
      "Epoch 917: train loss: 4.450610102679775e-09, val loss: 0.41736292839050293\n",
      "Epoch 918: train loss: 4.5028198947250075e-09, val loss: 0.4173142611980438\n",
      "Epoch 919: train loss: 4.655511975926174e-09, val loss: 0.4172707498073578\n",
      "Epoch 920: train loss: 4.789713070607604e-09, val loss: 0.4172237515449524\n",
      "Epoch 921: train loss: 4.683664123206199e-09, val loss: 0.41717758774757385\n",
      "Epoch 922: train loss: 4.520245067141104e-09, val loss: 0.41713881492614746\n",
      "Epoch 923: train loss: 4.429838273978248e-09, val loss: 0.41710349917411804\n",
      "Epoch 924: train loss: 4.392653796259083e-09, val loss: 0.4170612394809723\n",
      "Epoch 925: train loss: 4.281901944125366e-09, val loss: 0.4170166552066803\n",
      "Epoch 926: train loss: 4.242535212028997e-09, val loss: 0.4169633388519287\n",
      "Epoch 927: train loss: 4.286318411317325e-09, val loss: 0.4169180989265442\n",
      "Epoch 928: train loss: 4.355765970132097e-09, val loss: 0.4168848991394043\n",
      "Epoch 929: train loss: 4.437430867199055e-09, val loss: 0.4168507754802704\n",
      "Epoch 930: train loss: 4.385675822504709e-09, val loss: 0.4168139100074768\n",
      "Epoch 931: train loss: 4.254126384495294e-09, val loss: 0.41675081849098206\n",
      "Epoch 932: train loss: 4.184243174165658e-09, val loss: 0.4166496694087982\n",
      "Epoch 933: train loss: 4.259420816055126e-09, val loss: 0.4165720045566559\n",
      "Epoch 934: train loss: 4.562092481563695e-09, val loss: 0.4164906442165375\n",
      "Epoch 935: train loss: 4.79721817825407e-09, val loss: 0.41639479994773865\n",
      "Epoch 936: train loss: 4.880944093343942e-09, val loss: 0.4162920117378235\n",
      "Epoch 937: train loss: 4.599344904931968e-09, val loss: 0.4161859154701233\n",
      "Epoch 938: train loss: 4.469651759819726e-09, val loss: 0.4160892963409424\n",
      "Epoch 939: train loss: 4.2926573406987245e-09, val loss: 0.4159841537475586\n",
      "Epoch 940: train loss: 4.175624290780888e-09, val loss: 0.4158753454685211\n",
      "Epoch 941: train loss: 4.043562817912516e-09, val loss: 0.4157772958278656\n",
      "Epoch 942: train loss: 4.158621003114149e-09, val loss: 0.415682852268219\n",
      "Epoch 943: train loss: 4.379567375423221e-09, val loss: 0.41558000445365906\n",
      "Epoch 944: train loss: 4.506698569883838e-09, val loss: 0.41547027230262756\n",
      "Epoch 945: train loss: 4.440351197843029e-09, val loss: 0.4153578281402588\n",
      "Epoch 946: train loss: 4.236312189931368e-09, val loss: 0.415261834859848\n",
      "Epoch 947: train loss: 4.076640802708198e-09, val loss: 0.41515737771987915\n",
      "Epoch 948: train loss: 4.028824829305222e-09, val loss: 0.4150474965572357\n",
      "Epoch 949: train loss: 3.9955057040685915e-09, val loss: 0.4149414598941803\n",
      "Epoch 950: train loss: 3.9610328350647706e-09, val loss: 0.4148224890232086\n",
      "Epoch 951: train loss: 4.08934042184228e-09, val loss: 0.41471409797668457\n",
      "Epoch 952: train loss: 4.262176389602246e-09, val loss: 0.41459178924560547\n",
      "Epoch 953: train loss: 4.365451555798927e-09, val loss: 0.41449424624443054\n",
      "Epoch 954: train loss: 4.651824703216789e-09, val loss: 0.41439005732536316\n",
      "Epoch 955: train loss: 5.660778512606157e-09, val loss: 0.41429924964904785\n",
      "Epoch 956: train loss: 9.106498666255902e-09, val loss: 0.41414180397987366\n",
      "Epoch 957: train loss: 2.5512038703823237e-08, val loss: 0.41411352157592773\n",
      "Epoch 958: train loss: 1.3378236474181904e-07, val loss: 0.41388818621635437\n",
      "Epoch 959: train loss: 8.396858675041585e-07, val loss: 0.4141046106815338\n",
      "Epoch 960: train loss: 4.996832103643101e-06, val loss: 0.41307148337364197\n",
      "Epoch 961: train loss: 1.2837466783821583e-05, val loss: 0.4151090681552887\n",
      "Epoch 962: train loss: 8.12615326140076e-06, val loss: 0.4102953374385834\n",
      "Epoch 963: train loss: 1.3006970220885705e-05, val loss: 0.4138210415840149\n",
      "Epoch 964: train loss: 5.875346687389538e-06, val loss: 0.41223642230033875\n",
      "Epoch 965: train loss: 2.7899036467715632e-06, val loss: 0.4116802215576172\n",
      "Epoch 966: train loss: 6.018186468281783e-06, val loss: 0.4128805100917816\n",
      "Epoch 967: train loss: 7.591002031404059e-06, val loss: 0.4114362895488739\n",
      "Epoch 968: train loss: 5.558905286306981e-06, val loss: 0.41303664445877075\n",
      "Epoch 969: train loss: 4.497519967117114e-06, val loss: 0.41352686285972595\n",
      "Epoch 970: train loss: 1.4475788248091703e-06, val loss: 0.4122207760810852\n",
      "Epoch 971: train loss: 3.5472060062602395e-06, val loss: 0.41227540373802185\n",
      "Epoch 972: train loss: 5.175717433303362e-06, val loss: 0.4107160270214081\n",
      "Epoch 973: train loss: 2.453252136547235e-06, val loss: 0.41111573576927185\n",
      "Epoch 974: train loss: 1.8313734244657098e-06, val loss: 0.41079673171043396\n",
      "Epoch 975: train loss: 1.71393594428082e-06, val loss: 0.4095185697078705\n",
      "Epoch 976: train loss: 2.6346654067310737e-06, val loss: 0.41048240661621094\n",
      "Epoch 977: train loss: 1.7696547729428858e-06, val loss: 0.4089435040950775\n",
      "Epoch 978: train loss: 1.1692603720803163e-06, val loss: 0.40756580233573914\n",
      "Epoch 979: train loss: 1.1797735623986227e-06, val loss: 0.40787479281425476\n",
      "Epoch 980: train loss: 1.566445689604734e-06, val loss: 0.40723440051078796\n",
      "Epoch 981: train loss: 1.0946971542580286e-06, val loss: 0.40750208497047424\n",
      "Epoch 982: train loss: 9.413545285497094e-07, val loss: 0.40703797340393066\n",
      "Epoch 983: train loss: 6.579609248547058e-07, val loss: 0.40581750869750977\n",
      "Epoch 984: train loss: 8.916976526052167e-07, val loss: 0.405923455953598\n",
      "Epoch 985: train loss: 1.048879425979976e-06, val loss: 0.40576496720314026\n",
      "Epoch 986: train loss: 3.897931435403734e-07, val loss: 0.40594449639320374\n",
      "Epoch 987: train loss: 4.7361251631627965e-07, val loss: 0.4060945510864258\n",
      "Epoch 988: train loss: 7.081489457050338e-07, val loss: 0.4050283432006836\n",
      "Epoch 989: train loss: 5.705405783373863e-07, val loss: 0.40539082884788513\n",
      "Epoch 990: train loss: 3.018574545876618e-07, val loss: 0.40558648109436035\n",
      "Epoch 991: train loss: 3.7006904562986165e-07, val loss: 0.4055752754211426\n",
      "Epoch 992: train loss: 3.8724675732737524e-07, val loss: 0.4061395227909088\n",
      "Epoch 993: train loss: 4.5003736204307643e-07, val loss: 0.4054269790649414\n",
      "Epoch 994: train loss: 2.3311801555792044e-07, val loss: 0.4055400490760803\n",
      "Epoch 995: train loss: 1.8446860394760733e-07, val loss: 0.40598487854003906\n",
      "Epoch 996: train loss: 3.043893741505599e-07, val loss: 0.40570661425590515\n",
      "Epoch 997: train loss: 2.8329742463029106e-07, val loss: 0.40632936358451843\n",
      "Epoch 998: train loss: 2.2541001953868545e-07, val loss: 0.40601274371147156\n",
      "Epoch 999: train loss: 1.3608288895738951e-07, val loss: 0.40626826882362366\n",
      "Epoch 1000: train loss: 1.0949247553071473e-07, val loss: 0.4068396985530853\n",
      "Epoch 1001: train loss: 2.5269994807786134e-07, val loss: 0.40592727065086365\n",
      "Epoch 1002: train loss: 1.9600582845669123e-07, val loss: 0.4064973294734955\n",
      "Epoch 1003: train loss: 8.296917997085984e-08, val loss: 0.4069889187812805\n",
      "Epoch 1004: train loss: 1.1058495630322795e-07, val loss: 0.40690523386001587\n",
      "Epoch 1005: train loss: 9.776356080237747e-08, val loss: 0.40716081857681274\n",
      "Epoch 1006: train loss: 1.2526541581792117e-07, val loss: 0.40675145387649536\n",
      "Epoch 1007: train loss: 1.410698473591765e-07, val loss: 0.40727558732032776\n",
      "Epoch 1008: train loss: 6.93208122015676e-08, val loss: 0.4075891077518463\n",
      "Epoch 1009: train loss: 5.971274674720917e-08, val loss: 0.407485693693161\n",
      "Epoch 1010: train loss: 5.5141843091632836e-08, val loss: 0.4074958860874176\n",
      "Epoch 1011: train loss: 6.683935538376318e-08, val loss: 0.4074569642543793\n",
      "Epoch 1012: train loss: 9.916004728438566e-08, val loss: 0.40800848603248596\n",
      "Epoch 1013: train loss: 7.489055775522502e-08, val loss: 0.4078768789768219\n",
      "Epoch 1014: train loss: 4.380284579497129e-08, val loss: 0.4081422984600067\n",
      "Epoch 1015: train loss: 3.983086216408083e-08, val loss: 0.4082898795604706\n",
      "Epoch 1016: train loss: 3.9591579792386256e-08, val loss: 0.4084048271179199\n",
      "Epoch 1017: train loss: 5.2252282500830916e-08, val loss: 0.4084993302822113\n",
      "Epoch 1018: train loss: 5.7627797644954626e-08, val loss: 0.40845784544944763\n",
      "Epoch 1019: train loss: 6.42833484221228e-08, val loss: 0.40888386964797974\n",
      "Epoch 1020: train loss: 8.030868059449858e-08, val loss: 0.4089524447917938\n",
      "Epoch 1021: train loss: 1.094306085747121e-07, val loss: 0.4090251922607422\n",
      "Epoch 1022: train loss: 2.0311470905198803e-07, val loss: 0.40944215655326843\n",
      "Epoch 1023: train loss: 1.0151549645343039e-07, val loss: 0.4090309739112854\n",
      "Epoch 1024: train loss: 2.021552063524723e-07, val loss: 0.4101821482181549\n",
      "Epoch 1025: train loss: 2.9316629479581024e-07, val loss: 0.40956249833106995\n",
      "Epoch 1026: train loss: 3.130932100248174e-07, val loss: 0.41053247451782227\n",
      "Epoch 1027: train loss: 4.882610937784193e-07, val loss: 0.4098886549472809\n",
      "Epoch 1028: train loss: 6.664387797172822e-07, val loss: 0.4116390645503998\n",
      "Epoch 1029: train loss: 1.0251064850308467e-06, val loss: 0.40995922684669495\n",
      "Epoch 1030: train loss: 1.6577273527218495e-06, val loss: 0.4126501679420471\n",
      "Epoch 1031: train loss: 2.845891231118003e-06, val loss: 0.4099000096321106\n",
      "Epoch 1032: train loss: 5.118760782352183e-06, val loss: 0.4142499566078186\n",
      "Epoch 1033: train loss: 1.0174594535783399e-05, val loss: 0.40867048501968384\n",
      "Epoch 1034: train loss: 1.901237919810228e-05, val loss: 0.4166993200778961\n",
      "Epoch 1035: train loss: 3.669723082566634e-05, val loss: 0.40781694650650024\n",
      "Epoch 1036: train loss: 5.5978784075705335e-05, val loss: 0.4201663136482239\n",
      "Epoch 1037: train loss: 7.608662417624146e-05, val loss: 0.41184115409851074\n",
      "Epoch 1038: train loss: 4.926744077238254e-05, val loss: 0.4154561460018158\n",
      "Epoch 1039: train loss: 1.690643694018945e-05, val loss: 0.4169035851955414\n",
      "Epoch 1040: train loss: 3.1747033517603995e-06, val loss: 0.41553759574890137\n",
      "Epoch 1041: train loss: 2.209450576629024e-05, val loss: 0.4170001149177551\n",
      "Epoch 1042: train loss: 2.0508538000285625e-05, val loss: 0.4155435264110565\n",
      "Epoch 1043: train loss: 1.5412725815622252e-06, val loss: 0.4151547849178314\n",
      "Epoch 1044: train loss: 1.4171827388054226e-05, val loss: 0.415761798620224\n",
      "Epoch 1045: train loss: 1.1036071555281524e-05, val loss: 0.4137822687625885\n",
      "Epoch 1046: train loss: 5.735990725952433e-06, val loss: 0.41251254081726074\n",
      "Epoch 1047: train loss: 1.590665488038212e-05, val loss: 0.41359877586364746\n",
      "Epoch 1048: train loss: 5.719447472074535e-06, val loss: 0.4139379560947418\n",
      "Epoch 1049: train loss: 6.114029474701965e-06, val loss: 0.41195201873779297\n",
      "Epoch 1050: train loss: 6.669608410447836e-06, val loss: 0.411753386259079\n",
      "Epoch 1051: train loss: 1.7019970073306467e-06, val loss: 0.41280537843704224\n",
      "Epoch 1052: train loss: 7.63074331189273e-06, val loss: 0.4121592044830322\n",
      "Epoch 1053: train loss: 3.939978796552168e-06, val loss: 0.4123568534851074\n",
      "Epoch 1054: train loss: 2.1645421384164365e-06, val loss: 0.4128669798374176\n",
      "Epoch 1055: train loss: 4.560180514090462e-06, val loss: 0.4123147428035736\n",
      "Epoch 1056: train loss: 1.2709552947853808e-06, val loss: 0.41263362765312195\n",
      "Epoch 1057: train loss: 4.353219992481172e-06, val loss: 0.4137042164802551\n",
      "Epoch 1058: train loss: 1.994294734686264e-06, val loss: 0.4139873683452606\n",
      "Epoch 1059: train loss: 9.517779062662157e-07, val loss: 0.4137589633464813\n",
      "Epoch 1060: train loss: 3.0060423341637943e-06, val loss: 0.4144471287727356\n",
      "Epoch 1061: train loss: 9.24832363580208e-07, val loss: 0.415238618850708\n",
      "Epoch 1062: train loss: 2.632353016451816e-06, val loss: 0.41484832763671875\n",
      "Epoch 1063: train loss: 1.6488743312947918e-06, val loss: 0.4154298007488251\n",
      "Epoch 1064: train loss: 6.217398436092481e-07, val loss: 0.4163246154785156\n",
      "Epoch 1065: train loss: 1.6500277979503153e-06, val loss: 0.4161469638347626\n",
      "Epoch 1066: train loss: 3.454644854627986e-07, val loss: 0.4162178039550781\n",
      "Epoch 1067: train loss: 1.3090475476928987e-06, val loss: 0.4163847863674164\n",
      "Epoch 1068: train loss: 1.3223032055975636e-06, val loss: 0.41621875762939453\n",
      "Epoch 1069: train loss: 6.555329719049041e-07, val loss: 0.41576114296913147\n",
      "Epoch 1070: train loss: 1.288503767682414e-06, val loss: 0.41619449853897095\n",
      "Epoch 1071: train loss: 3.9375450455736427e-07, val loss: 0.41628342866897583\n",
      "Epoch 1072: train loss: 4.642396618237399e-07, val loss: 0.4160008132457733\n",
      "Epoch 1073: train loss: 6.342938831949141e-07, val loss: 0.41604724526405334\n",
      "Epoch 1074: train loss: 1.1257049692403598e-07, val loss: 0.4160851538181305\n",
      "Epoch 1075: train loss: 6.620386443501047e-07, val loss: 0.4160339832305908\n",
      "Epoch 1076: train loss: 5.051426228419587e-07, val loss: 0.4161873757839203\n",
      "Epoch 1077: train loss: 5.486331815518497e-07, val loss: 0.4166901707649231\n",
      "Epoch 1078: train loss: 1.0798136145240278e-06, val loss: 0.4163617193698883\n",
      "Epoch 1079: train loss: 1.0918383850366808e-06, val loss: 0.41637134552001953\n",
      "Epoch 1080: train loss: 2.0754414435941726e-06, val loss: 0.41620609164237976\n",
      "Epoch 1081: train loss: 3.6376547996042063e-06, val loss: 0.41660675406455994\n",
      "Epoch 1082: train loss: 7.000467576290248e-06, val loss: 0.41609668731689453\n",
      "Epoch 1083: train loss: 1.4951930097595323e-05, val loss: 0.41723236441612244\n",
      "Epoch 1084: train loss: 3.304212441435084e-05, val loss: 0.41661903262138367\n",
      "Epoch 1085: train loss: 6.613362347707152e-05, val loss: 0.4181078374385834\n",
      "Epoch 1086: train loss: 0.00011705850920407102, val loss: 0.41774415969848633\n",
      "Epoch 1087: train loss: 0.00011652274406515062, val loss: 0.4193737208843231\n",
      "Epoch 1088: train loss: 4.408722816151567e-05, val loss: 0.41952618956565857\n",
      "Epoch 1089: train loss: 6.236353328858968e-06, val loss: 0.4225177764892578\n",
      "Epoch 1090: train loss: 4.480196730582975e-05, val loss: 0.42104530334472656\n",
      "Epoch 1091: train loss: 2.1447618564707227e-05, val loss: 0.41830798983573914\n",
      "Epoch 1092: train loss: 1.9814739061985165e-05, val loss: 0.41197583079338074\n",
      "Epoch 1093: train loss: 1.778216210368555e-05, val loss: 0.410266250371933\n",
      "Epoch 1094: train loss: 1.5447832993231714e-05, val loss: 0.40793225169181824\n",
      "Epoch 1095: train loss: 1.8612408894114196e-05, val loss: 0.4067542552947998\n",
      "Epoch 1096: train loss: 3.0397366117540514e-06, val loss: 0.4065048396587372\n",
      "Epoch 1097: train loss: 2.1208667021710426e-05, val loss: 0.4056432247161865\n",
      "Epoch 1098: train loss: 1.6775170479377266e-06, val loss: 0.4046962857246399\n",
      "Epoch 1099: train loss: 1.2819678886444308e-05, val loss: 0.4055357575416565\n",
      "Epoch 1100: train loss: 6.299777851381805e-06, val loss: 0.4062926471233368\n",
      "Epoch 1101: train loss: 8.15756447991589e-06, val loss: 0.40573954582214355\n",
      "Epoch 1102: train loss: 3.5791513255389873e-06, val loss: 0.4053201377391815\n",
      "Epoch 1103: train loss: 9.548170964990277e-06, val loss: 0.406074196100235\n",
      "Epoch 1104: train loss: 1.5218982980513829e-06, val loss: 0.40622517466545105\n",
      "Epoch 1105: train loss: 6.4982705225702375e-06, val loss: 0.40576624870300293\n",
      "Epoch 1106: train loss: 3.5335203847353114e-06, val loss: 0.40591534972190857\n",
      "Epoch 1107: train loss: 4.058202193846228e-06, val loss: 0.40636202692985535\n",
      "Epoch 1108: train loss: 2.215415634054807e-06, val loss: 0.40612003207206726\n",
      "Epoch 1109: train loss: 4.758812337968266e-06, val loss: 0.4054820239543915\n",
      "Epoch 1110: train loss: 1.0101578027388314e-06, val loss: 0.40533527731895447\n",
      "Epoch 1111: train loss: 3.373750359969563e-06, val loss: 0.4057851731777191\n",
      "Epoch 1112: train loss: 1.8035600533039542e-06, val loss: 0.4060973823070526\n",
      "Epoch 1113: train loss: 2.184678351113689e-06, val loss: 0.4060019552707672\n",
      "Epoch 1114: train loss: 1.1460429050202947e-06, val loss: 0.4059097468852997\n",
      "Epoch 1115: train loss: 2.5263516363338567e-06, val loss: 0.4060080945491791\n",
      "Epoch 1116: train loss: 5.784813197351468e-07, val loss: 0.4059367775917053\n",
      "Epoch 1117: train loss: 1.7382996020387509e-06, val loss: 0.4057081341743469\n",
      "Epoch 1118: train loss: 1.0545974191700225e-06, val loss: 0.40571603178977966\n",
      "Epoch 1119: train loss: 1.16561363938672e-06, val loss: 0.4059135913848877\n",
      "Epoch 1120: train loss: 6.140145956123888e-07, val loss: 0.40595340728759766\n",
      "Epoch 1121: train loss: 1.3408172208073665e-06, val loss: 0.4057905375957489\n",
      "Epoch 1122: train loss: 4.1461674982201657e-07, val loss: 0.4057132303714752\n",
      "Epoch 1123: train loss: 7.933062420306669e-07, val loss: 0.40580520033836365\n",
      "Epoch 1124: train loss: 7.065127078931255e-07, val loss: 0.40583133697509766\n",
      "Epoch 1125: train loss: 5.471945883073204e-07, val loss: 0.40570497512817383\n",
      "Epoch 1126: train loss: 3.9138569718488725e-07, val loss: 0.40566378831863403\n",
      "Epoch 1127: train loss: 6.339946025946119e-07, val loss: 0.4057803153991699\n",
      "Epoch 1128: train loss: 4.14065453924195e-07, val loss: 0.4057620167732239\n",
      "Epoch 1129: train loss: 2.3910467916721245e-07, val loss: 0.4055941700935364\n",
      "Epoch 1130: train loss: 5.138608685228974e-07, val loss: 0.4055784344673157\n",
      "Epoch 1131: train loss: 2.671926893071941e-07, val loss: 0.40565067529678345\n",
      "Epoch 1132: train loss: 2.8403769647411536e-07, val loss: 0.4055708944797516\n",
      "Epoch 1133: train loss: 2.020367730892758e-07, val loss: 0.4054374396800995\n",
      "Epoch 1134: train loss: 3.963682218000031e-07, val loss: 0.40547099709510803\n",
      "Epoch 1135: train loss: 5.063427366280848e-08, val loss: 0.4056340157985687\n",
      "Epoch 1136: train loss: 2.784661319310544e-07, val loss: 0.4056694209575653\n",
      "Epoch 1137: train loss: 1.4603455156247946e-07, val loss: 0.40559178590774536\n",
      "Epoch 1138: train loss: 2.2895133611200436e-07, val loss: 0.40562930703163147\n",
      "Epoch 1139: train loss: 4.457529101387081e-08, val loss: 0.40564796328544617\n",
      "Epoch 1140: train loss: 1.9495425362947572e-07, val loss: 0.40555277466773987\n",
      "Epoch 1141: train loss: 1.39323091730148e-07, val loss: 0.4055548310279846\n",
      "Epoch 1142: train loss: 8.516254013102298e-08, val loss: 0.40565547347068787\n",
      "Epoch 1143: train loss: 9.751899199272884e-08, val loss: 0.4056731164455414\n",
      "Epoch 1144: train loss: 8.26801525022347e-08, val loss: 0.4056435525417328\n",
      "Epoch 1145: train loss: 1.42522154078506e-07, val loss: 0.40566593408584595\n",
      "Epoch 1146: train loss: 1.7133293894744384e-08, val loss: 0.4056973159313202\n",
      "Epoch 1147: train loss: 9.121190913674582e-08, val loss: 0.4056784212589264\n",
      "Epoch 1148: train loss: 4.83013060659232e-08, val loss: 0.4056723713874817\n",
      "Epoch 1149: train loss: 9.17658624643991e-08, val loss: 0.40572020411491394\n",
      "Epoch 1150: train loss: 3.96891230991514e-08, val loss: 0.4057144224643707\n",
      "Epoch 1151: train loss: 2.6173132994244952e-08, val loss: 0.4056810438632965\n",
      "Epoch 1152: train loss: 6.72999078688008e-08, val loss: 0.40572816133499146\n",
      "Epoch 1153: train loss: 3.366340095567466e-08, val loss: 0.4057534337043762\n",
      "Epoch 1154: train loss: 6.047951472964996e-08, val loss: 0.40575066208839417\n",
      "Epoch 1155: train loss: 1.8242145571889523e-08, val loss: 0.40575525164604187\n",
      "Epoch 1156: train loss: 3.182002217272384e-08, val loss: 0.4057296812534332\n",
      "Epoch 1157: train loss: 3.3081207106988586e-08, val loss: 0.4056943953037262\n",
      "Epoch 1158: train loss: 2.470279802935238e-08, val loss: 0.40563079714775085\n",
      "Epoch 1159: train loss: 4.966528877048404e-08, val loss: 0.4056253433227539\n",
      "Epoch 1160: train loss: 4.395254649125491e-08, val loss: 0.4056347906589508\n",
      "Epoch 1161: train loss: 1.7136889596258698e-07, val loss: 0.40555888414382935\n",
      "Epoch 1162: train loss: 1.274400744932791e-07, val loss: 0.4056014120578766\n",
      "Epoch 1163: train loss: 1.3901821205308806e-07, val loss: 0.4055301249027252\n",
      "Epoch 1164: train loss: 6.596736312758367e-08, val loss: 0.405414342880249\n",
      "Epoch 1165: train loss: 6.322224521682074e-08, val loss: 0.40538904070854187\n",
      "Epoch 1166: train loss: 7.4501429025986e-08, val loss: 0.4053988456726074\n",
      "Epoch 1167: train loss: 2.7617840459015497e-08, val loss: 0.40534648299217224\n",
      "Epoch 1168: train loss: 5.496645627545149e-08, val loss: 0.40526747703552246\n",
      "Epoch 1169: train loss: 4.0555239166906176e-08, val loss: 0.4051145613193512\n",
      "Epoch 1170: train loss: 3.31638574380122e-08, val loss: 0.4049012362957001\n",
      "Epoch 1171: train loss: 2.866187465144776e-08, val loss: 0.4048507809638977\n",
      "Epoch 1172: train loss: 3.334686482503457e-08, val loss: 0.40488219261169434\n",
      "Epoch 1173: train loss: 2.8798927687034848e-08, val loss: 0.40482303500175476\n",
      "Epoch 1174: train loss: 2.6190829061079057e-08, val loss: 0.40479207038879395\n",
      "Epoch 1175: train loss: 4.5367006151764144e-08, val loss: 0.4047006070613861\n",
      "Epoch 1176: train loss: 6.507314509462958e-08, val loss: 0.40460845828056335\n",
      "Epoch 1177: train loss: 1.032262844091747e-07, val loss: 0.4044129550457001\n",
      "Epoch 1178: train loss: 2.2950408151700685e-07, val loss: 0.4044615924358368\n",
      "Epoch 1179: train loss: 5.606382842415769e-07, val loss: 0.4043090045452118\n",
      "Epoch 1180: train loss: 1.471277300879592e-06, val loss: 0.4045078456401825\n",
      "Epoch 1181: train loss: 4.072423962497851e-06, val loss: 0.4040687680244446\n",
      "Epoch 1182: train loss: 1.1993577572866343e-05, val loss: 0.4044146239757538\n",
      "Epoch 1183: train loss: 3.065124110435136e-05, val loss: 0.4040036201477051\n",
      "Epoch 1184: train loss: 4.727387931779958e-05, val loss: 0.4068097770214081\n",
      "Epoch 1185: train loss: 4.910899224341847e-05, val loss: 0.40132027864456177\n",
      "Epoch 1186: train loss: 6.287142605287954e-05, val loss: 0.4032956063747406\n",
      "Epoch 1187: train loss: 7.669589831493795e-05, val loss: 0.39801427721977234\n",
      "Epoch 1188: train loss: 7.473569712601602e-05, val loss: 0.3986777365207672\n",
      "Epoch 1189: train loss: 4.147001163801178e-05, val loss: 0.3958483934402466\n",
      "Epoch 1190: train loss: 1.1546996574907098e-05, val loss: 0.39554327726364136\n",
      "Epoch 1191: train loss: 7.149792054406134e-06, val loss: 0.39440950751304626\n",
      "Epoch 1192: train loss: 2.5642886612331495e-05, val loss: 0.3955096900463104\n",
      "Epoch 1193: train loss: 2.9099152015987784e-05, val loss: 0.3943859338760376\n",
      "Epoch 1194: train loss: 1.160762803920079e-05, val loss: 0.3956466317176819\n",
      "Epoch 1195: train loss: 4.921264007862192e-06, val loss: 0.39880257844924927\n",
      "Epoch 1196: train loss: 1.1245278983551543e-05, val loss: 0.39462995529174805\n",
      "Epoch 1197: train loss: 1.555947346787434e-05, val loss: 0.3988492488861084\n",
      "Epoch 1198: train loss: 5.527505891222972e-06, val loss: 0.39983150362968445\n",
      "Epoch 1199: train loss: 4.325991540099494e-06, val loss: 0.39660510420799255\n",
      "Epoch 1200: train loss: 7.821146027708892e-06, val loss: 0.402524471282959\n",
      "Epoch 1201: train loss: 7.846812877687626e-06, val loss: 0.4017622172832489\n",
      "Epoch 1202: train loss: 3.3070741665142123e-06, val loss: 0.40208250284194946\n",
      "Epoch 1203: train loss: 3.096517730227788e-06, val loss: 0.405997097492218\n",
      "Epoch 1204: train loss: 5.835363936057547e-06, val loss: 0.40474629402160645\n",
      "Epoch 1205: train loss: 4.4365951907821e-06, val loss: 0.40563908219337463\n",
      "Epoch 1206: train loss: 9.96582571133331e-07, val loss: 0.40566158294677734\n",
      "Epoch 1207: train loss: 3.2812627068778966e-06, val loss: 0.4047073423862457\n",
      "Epoch 1208: train loss: 4.567588803183753e-06, val loss: 0.40476909279823303\n",
      "Epoch 1209: train loss: 8.383750014218094e-07, val loss: 0.4049348533153534\n",
      "Epoch 1210: train loss: 1.4813374491495779e-06, val loss: 0.4048020541667938\n",
      "Epoch 1211: train loss: 3.3712512959027663e-06, val loss: 0.40523767471313477\n",
      "Epoch 1212: train loss: 1.6205093515964109e-06, val loss: 0.40503057837486267\n",
      "Epoch 1213: train loss: 5.019276727580291e-07, val loss: 0.4048033356666565\n",
      "Epoch 1214: train loss: 1.9880853869835846e-06, val loss: 0.40515685081481934\n",
      "Epoch 1215: train loss: 1.781136688805418e-06, val loss: 0.40483275055885315\n",
      "Epoch 1216: train loss: 7.6092726430943e-07, val loss: 0.40477249026298523\n",
      "Epoch 1217: train loss: 6.563080319210712e-07, val loss: 0.40507784485816956\n",
      "Epoch 1218: train loss: 1.4534529100274085e-06, val loss: 0.4040592312812805\n",
      "Epoch 1219: train loss: 1.0627708206811803e-06, val loss: 0.40485796332359314\n",
      "Epoch 1220: train loss: 2.4419253463747737e-07, val loss: 0.40498921275138855\n",
      "Epoch 1221: train loss: 7.942000479488343e-07, val loss: 0.40366002917289734\n",
      "Epoch 1222: train loss: 1.0940298125206027e-06, val loss: 0.404936283826828\n",
      "Epoch 1223: train loss: 3.137047599466314e-07, val loss: 0.4039026200771332\n",
      "Epoch 1224: train loss: 3.14281351165846e-07, val loss: 0.4017927348613739\n",
      "Epoch 1225: train loss: 7.612932222400559e-07, val loss: 0.40340062975883484\n",
      "Epoch 1226: train loss: 4.969983251612575e-07, val loss: 0.40219736099243164\n",
      "Epoch 1227: train loss: 2.5738489739524084e-07, val loss: 0.4015178680419922\n",
      "Epoch 1228: train loss: 3.0040877163628465e-07, val loss: 0.40213823318481445\n",
      "Epoch 1229: train loss: 4.39311406807974e-07, val loss: 0.4001379609107971\n",
      "Epoch 1230: train loss: 3.999723787728726e-07, val loss: 0.4001036584377289\n",
      "Epoch 1231: train loss: 1.5760086569116538e-07, val loss: 0.39995336532592773\n",
      "Epoch 1232: train loss: 1.583991178222277e-07, val loss: 0.3987136781215668\n",
      "Epoch 1233: train loss: 3.542579634085996e-07, val loss: 0.39905911684036255\n",
      "Epoch 1234: train loss: 2.8810069352402934e-07, val loss: 0.39821305871009827\n",
      "Epoch 1235: train loss: 5.642019118567987e-08, val loss: 0.39793160557746887\n",
      "Epoch 1236: train loss: 1.3349327332434768e-07, val loss: 0.3979201316833496\n",
      "Epoch 1237: train loss: 2.5619695520617825e-07, val loss: 0.3967248797416687\n",
      "Epoch 1238: train loss: 1.7050558653863845e-07, val loss: 0.39684534072875977\n",
      "Epoch 1239: train loss: 7.236942423105575e-08, val loss: 0.3962208032608032\n",
      "Epoch 1240: train loss: 9.928840682960072e-08, val loss: 0.3956725597381592\n",
      "Epoch 1241: train loss: 1.4206759146873082e-07, val loss: 0.3958309590816498\n",
      "Epoch 1242: train loss: 1.1795796694968885e-07, val loss: 0.39500972628593445\n",
      "Epoch 1243: train loss: 9.873524930981148e-08, val loss: 0.39503151178359985\n",
      "Epoch 1244: train loss: 8.479484137069448e-08, val loss: 0.39467647671699524\n",
      "Epoch 1245: train loss: 6.365763027815774e-08, val loss: 0.3942125737667084\n",
      "Epoch 1246: train loss: 5.744204045754486e-08, val loss: 0.39422184228897095\n",
      "Epoch 1247: train loss: 8.469793755239152e-08, val loss: 0.3935260772705078\n",
      "Epoch 1248: train loss: 1.0436870212515714e-07, val loss: 0.39366409182548523\n",
      "Epoch 1249: train loss: 8.24290111722803e-08, val loss: 0.3933436870574951\n",
      "Epoch 1250: train loss: 3.902722056636776e-08, val loss: 0.39307618141174316\n",
      "Epoch 1251: train loss: 1.5972988265389176e-08, val loss: 0.3929661214351654\n",
      "Epoch 1252: train loss: 3.262998404807149e-08, val loss: 0.39331984519958496\n",
      "Epoch 1253: train loss: 7.111173516705094e-08, val loss: 0.3938731253147125\n",
      "Epoch 1254: train loss: 1.0176844966736098e-07, val loss: 0.3950711190700531\n",
      "Epoch 1255: train loss: 1.171672110444888e-07, val loss: 0.3948720395565033\n",
      "Epoch 1256: train loss: 1.2854019360020175e-07, val loss: 0.3962286412715912\n",
      "Epoch 1257: train loss: 1.341577444691211e-07, val loss: 0.39654964208602905\n",
      "Epoch 1258: train loss: 1.5905209238553653e-07, val loss: 0.3976007401943207\n",
      "Epoch 1259: train loss: 1.9235332615608058e-07, val loss: 0.39743509888648987\n",
      "Epoch 1260: train loss: 2.8800030804632115e-07, val loss: 0.3992442786693573\n",
      "Epoch 1261: train loss: 4.7332505914710055e-07, val loss: 0.3985782563686371\n",
      "Epoch 1262: train loss: 9.395063784722879e-07, val loss: 0.40059635043144226\n",
      "Epoch 1263: train loss: 2.0620466330001364e-06, val loss: 0.3994300961494446\n",
      "Epoch 1264: train loss: 5.161089120520046e-06, val loss: 0.4027997553348541\n",
      "Epoch 1265: train loss: 1.3389280866249464e-05, val loss: 0.39826950430870056\n",
      "Epoch 1266: train loss: 3.7513735151151195e-05, val loss: 0.4064891040325165\n",
      "Epoch 1267: train loss: 9.133451385423541e-05, val loss: 0.39298760890960693\n",
      "Epoch 1268: train loss: 0.00019449363753665239, val loss: 0.41318342089653015\n",
      "Epoch 1269: train loss: 0.00022710634220857173, val loss: 0.4029228389263153\n",
      "Epoch 1270: train loss: 0.00018936285050585866, val loss: 0.4076874256134033\n",
      "Epoch 1271: train loss: 0.00017374017625115812, val loss: 0.40780821442604065\n",
      "Epoch 1272: train loss: 8.764343510847539e-05, val loss: 0.4065578877925873\n",
      "Epoch 1273: train loss: 1.3731995750276837e-05, val loss: 0.4045332968235016\n",
      "Epoch 1274: train loss: 7.26673606550321e-05, val loss: 0.403496652841568\n",
      "Epoch 1275: train loss: 8.026939758565277e-05, val loss: 0.4048963487148285\n",
      "Epoch 1276: train loss: 5.89051705901511e-05, val loss: 0.40378084778785706\n",
      "Epoch 1277: train loss: 1.570679887663573e-05, val loss: 0.4025619626045227\n",
      "Epoch 1278: train loss: 4.15293725382071e-05, val loss: 0.4031600058078766\n",
      "Epoch 1279: train loss: 4.045479727210477e-05, val loss: 0.4037106931209564\n",
      "Epoch 1280: train loss: 2.970149762404617e-05, val loss: 0.4026736915111542\n",
      "Epoch 1281: train loss: 1.619681279407814e-05, val loss: 0.40104952454566956\n",
      "Epoch 1282: train loss: 2.2905915102455765e-05, val loss: 0.40069127082824707\n",
      "Epoch 1283: train loss: 2.9717013603658415e-05, val loss: 0.3991102874279022\n",
      "Epoch 1284: train loss: 1.5847841495997272e-05, val loss: 0.39880141615867615\n",
      "Epoch 1285: train loss: 7.0198634603002574e-06, val loss: 0.3991059958934784\n",
      "Epoch 1286: train loss: 2.2415708372136578e-05, val loss: 0.3975605070590973\n",
      "Epoch 1287: train loss: 1.7473415937274694e-05, val loss: 0.3973093330860138\n",
      "Epoch 1288: train loss: 3.108968940068735e-06, val loss: 0.39734381437301636\n",
      "Epoch 1289: train loss: 1.3701147508982103e-05, val loss: 0.3969535827636719\n",
      "Epoch 1290: train loss: 1.1664696103252936e-05, val loss: 0.39713814854621887\n",
      "Epoch 1291: train loss: 9.537530786474235e-06, val loss: 0.39588603377342224\n",
      "Epoch 1292: train loss: 4.077847279404523e-06, val loss: 0.39545542001724243\n",
      "Epoch 1293: train loss: 8.156594958563801e-06, val loss: 0.3966740071773529\n",
      "Epoch 1294: train loss: 9.270941518479958e-06, val loss: 0.3963092267513275\n",
      "Epoch 1295: train loss: 4.562149115372449e-06, val loss: 0.3960627317428589\n",
      "Epoch 1296: train loss: 2.190736267948523e-06, val loss: 0.3966835141181946\n",
      "Epoch 1297: train loss: 8.058585990511347e-06, val loss: 0.3966529071331024\n",
      "Epoch 1298: train loss: 4.5950873754918575e-06, val loss: 0.39705225825309753\n",
      "Epoch 1299: train loss: 1.757034056026896e-06, val loss: 0.3970169723033905\n",
      "Epoch 1300: train loss: 3.836569703707937e-06, val loss: 0.3967619240283966\n",
      "Epoch 1301: train loss: 4.037681264890125e-06, val loss: 0.39778873324394226\n",
      "Epoch 1302: train loss: 3.164817144352128e-06, val loss: 0.3981797695159912\n",
      "Epoch 1303: train loss: 1.471296513955167e-06, val loss: 0.3980519771575928\n",
      "Epoch 1304: train loss: 2.0557881725835614e-06, val loss: 0.398509681224823\n",
      "Epoch 1305: train loss: 3.391796326468466e-06, val loss: 0.3987475037574768\n",
      "Epoch 1306: train loss: 1.8548689695307985e-06, val loss: 0.39930927753448486\n",
      "Epoch 1307: train loss: 4.070140562362212e-07, val loss: 0.3996792435646057\n",
      "Epoch 1308: train loss: 2.2956737666390836e-06, val loss: 0.39967137575149536\n",
      "Epoch 1309: train loss: 1.8346712522543385e-06, val loss: 0.4004227817058563\n",
      "Epoch 1310: train loss: 1.063472154783085e-06, val loss: 0.40088072419166565\n",
      "Epoch 1311: train loss: 7.290605026355479e-07, val loss: 0.40109410881996155\n",
      "Epoch 1312: train loss: 1.1403750477256835e-06, val loss: 0.4015057682991028\n",
      "Epoch 1313: train loss: 1.3352915857467451e-06, val loss: 0.40163689851760864\n",
      "Epoch 1314: train loss: 9.701359431346646e-07, val loss: 0.40251514315605164\n",
      "Epoch 1315: train loss: 1.8229593479190953e-07, val loss: 0.40327349305152893\n",
      "Epoch 1316: train loss: 8.231378387790755e-07, val loss: 0.40323716402053833\n",
      "Epoch 1317: train loss: 1.0472408575878944e-06, val loss: 0.40369105339050293\n",
      "Epoch 1318: train loss: 5.012124688619224e-07, val loss: 0.40384578704833984\n",
      "Epoch 1319: train loss: 2.810484147630632e-07, val loss: 0.4041520059108734\n",
      "Epoch 1320: train loss: 4.76918444292096e-07, val loss: 0.4043436646461487\n",
      "Epoch 1321: train loss: 5.509161269401375e-07, val loss: 0.40436607599258423\n",
      "Epoch 1322: train loss: 5.679268042513286e-07, val loss: 0.40464287996292114\n",
      "Epoch 1323: train loss: 2.4372371854042285e-07, val loss: 0.4049158990383148\n",
      "Epoch 1324: train loss: 1.1139830036199783e-07, val loss: 0.4050368368625641\n",
      "Epoch 1325: train loss: 4.268084410341544e-07, val loss: 0.40518704056739807\n",
      "Epoch 1326: train loss: 4.144247611748142e-07, val loss: 0.40532469749450684\n",
      "Epoch 1327: train loss: 2.1250581028198212e-07, val loss: 0.40548115968704224\n",
      "Epoch 1328: train loss: 1.3567189682817116e-07, val loss: 0.4055912494659424\n",
      "Epoch 1329: train loss: 1.5468101821625169e-07, val loss: 0.40568867325782776\n",
      "Epoch 1330: train loss: 2.2189860260368732e-07, val loss: 0.4059280455112457\n",
      "Epoch 1331: train loss: 2.9318229621821956e-07, val loss: 0.40617313981056213\n",
      "Epoch 1332: train loss: 1.7415052866454062e-07, val loss: 0.4063126742839813\n",
      "Epoch 1333: train loss: 3.859307895481834e-08, val loss: 0.4063818156719208\n",
      "Epoch 1334: train loss: 1.012972816738511e-07, val loss: 0.4065394103527069\n",
      "Epoch 1335: train loss: 1.4196760389495466e-07, val loss: 0.40678414702415466\n",
      "Epoch 1336: train loss: 1.649678722515091e-07, val loss: 0.4069460928440094\n",
      "Epoch 1337: train loss: 1.409696466225796e-07, val loss: 0.40705689787864685\n",
      "Epoch 1338: train loss: 9.781969367850252e-08, val loss: 0.4071993827819824\n",
      "Epoch 1339: train loss: 1.7158084730795053e-08, val loss: 0.4074349105358124\n",
      "Epoch 1340: train loss: 5.504720235194327e-08, val loss: 0.40761977434158325\n",
      "Epoch 1341: train loss: 1.0144374584797333e-07, val loss: 0.4077015519142151\n",
      "Epoch 1342: train loss: 1.1115197651179187e-07, val loss: 0.4078521728515625\n",
      "Epoch 1343: train loss: 7.388901934746173e-08, val loss: 0.40804168581962585\n",
      "Epoch 1344: train loss: 7.167438553778993e-08, val loss: 0.4082150161266327\n",
      "Epoch 1345: train loss: 2.8527137985179252e-08, val loss: 0.40837985277175903\n",
      "Epoch 1346: train loss: 1.8468771401103368e-08, val loss: 0.4085787236690521\n",
      "Epoch 1347: train loss: 2.76886193972814e-08, val loss: 0.40875765681266785\n",
      "Epoch 1348: train loss: 5.943030600974453e-08, val loss: 0.40887972712516785\n",
      "Epoch 1349: train loss: 6.302731492269231e-08, val loss: 0.409039169549942\n",
      "Epoch 1350: train loss: 6.374735761482953e-08, val loss: 0.409201443195343\n",
      "Epoch 1351: train loss: 6.566820331954659e-08, val loss: 0.4093538224697113\n",
      "Epoch 1352: train loss: 5.808565717302372e-08, val loss: 0.4094807803630829\n",
      "Epoch 1353: train loss: 3.9588325506656474e-08, val loss: 0.40961843729019165\n",
      "Epoch 1354: train loss: 2.845457380828975e-08, val loss: 0.4097864329814911\n",
      "Epoch 1355: train loss: 2.6673884434558204e-08, val loss: 0.4099390208721161\n",
      "Epoch 1356: train loss: 1.8527250844613263e-08, val loss: 0.41008099913597107\n",
      "Epoch 1357: train loss: 1.1174215330811421e-08, val loss: 0.4102049767971039\n",
      "Epoch 1358: train loss: 8.406896512269668e-09, val loss: 0.4103589653968811\n",
      "Epoch 1359: train loss: 1.1348219253193292e-08, val loss: 0.410504549741745\n",
      "Epoch 1360: train loss: 8.60539373093161e-09, val loss: 0.41062313318252563\n",
      "Epoch 1361: train loss: 5.162208438491689e-09, val loss: 0.41076189279556274\n",
      "Epoch 1362: train loss: 5.767910593590386e-09, val loss: 0.4109155833721161\n",
      "Epoch 1363: train loss: 7.377503052907741e-09, val loss: 0.4110473692417145\n",
      "Epoch 1364: train loss: 7.566987036966566e-09, val loss: 0.41119059920310974\n",
      "Epoch 1365: train loss: 5.667055269498178e-09, val loss: 0.4113178253173828\n",
      "Epoch 1366: train loss: 6.889405046450747e-09, val loss: 0.4114413261413574\n",
      "Epoch 1367: train loss: 1.081091838273096e-08, val loss: 0.41156527400016785\n",
      "Epoch 1368: train loss: 1.9910268989065116e-08, val loss: 0.4116457998752594\n",
      "Epoch 1369: train loss: 4.3347714751007516e-08, val loss: 0.41179361939430237\n",
      "Epoch 1370: train loss: 1.031968892561963e-07, val loss: 0.4118494987487793\n",
      "Epoch 1371: train loss: 2.9127622269697895e-07, val loss: 0.41193994879722595\n",
      "Epoch 1372: train loss: 8.494515100210265e-07, val loss: 0.41196438670158386\n",
      "Epoch 1373: train loss: 2.6578170491120545e-06, val loss: 0.4118638038635254\n",
      "Epoch 1374: train loss: 8.4993234850117e-06, val loss: 0.41225335001945496\n",
      "Epoch 1375: train loss: 2.867754847102333e-05, val loss: 0.4120952785015106\n",
      "Epoch 1376: train loss: 9.051996312336996e-05, val loss: 0.41046634316444397\n",
      "Epoch 1377: train loss: 0.0002784112002700567, val loss: 0.41163668036460876\n",
      "Epoch 1378: train loss: 0.0006769181927666068, val loss: 0.41311463713645935\n",
      "Epoch 1379: train loss: 0.0015587926609441638, val loss: 0.4009425640106201\n",
      "Epoch 1380: train loss: 0.00274300342425704, val loss: 0.40893879532814026\n",
      "Epoch 1381: train loss: 0.0027220880147069693, val loss: 0.39955005049705505\n",
      "Epoch 1382: train loss: 0.0011260228930041194, val loss: 0.40012502670288086\n",
      "Epoch 1383: train loss: 0.00013859578757546842, val loss: 0.4034958779811859\n",
      "Epoch 1384: train loss: 0.0008136741816997528, val loss: 0.3977147936820984\n",
      "Epoch 1385: train loss: 0.0013529795687645674, val loss: 0.4000367224216461\n",
      "Epoch 1386: train loss: 0.0003589217085391283, val loss: 0.3970199227333069\n",
      "Epoch 1387: train loss: 0.00020022218814119697, val loss: 0.3905283808708191\n",
      "Epoch 1388: train loss: 0.0008070154581218958, val loss: 0.39800748229026794\n",
      "Epoch 1389: train loss: 0.000481727096484974, val loss: 0.3932860493659973\n",
      "Epoch 1390: train loss: 6.939914601389319e-05, val loss: 0.3881496489048004\n",
      "Epoch 1391: train loss: 0.0004938333295285702, val loss: 0.38735246658325195\n",
      "Epoch 1392: train loss: 0.0004012098361272365, val loss: 0.38661834597587585\n",
      "Epoch 1393: train loss: 4.142587204114534e-05, val loss: 0.387763112783432\n",
      "Epoch 1394: train loss: 0.00034829729702323675, val loss: 0.3855225145816803\n",
      "Epoch 1395: train loss: 0.0002717873430810869, val loss: 0.38643255829811096\n",
      "Epoch 1396: train loss: 4.984960105502978e-05, val loss: 0.38651689887046814\n",
      "Epoch 1397: train loss: 0.00024045728787314147, val loss: 0.38361793756484985\n",
      "Epoch 1398: train loss: 0.0001956199703272432, val loss: 0.38300180435180664\n",
      "Epoch 1399: train loss: 3.0459101253654808e-05, val loss: 0.3841036856174469\n",
      "Epoch 1400: train loss: 0.00018826551968231797, val loss: 0.38426467776298523\n",
      "Epoch 1401: train loss: 0.00012837242684327066, val loss: 0.3857094645500183\n",
      "Epoch 1402: train loss: 2.3173879526439123e-05, val loss: 0.38473254442214966\n",
      "Epoch 1403: train loss: 0.0001477828627685085, val loss: 0.38102594017982483\n",
      "Epoch 1404: train loss: 8.065135625656694e-05, val loss: 0.3787417411804199\n",
      "Epoch 1405: train loss: 2.0535961084533483e-05, val loss: 0.3817611336708069\n",
      "Epoch 1406: train loss: 0.00011304561485303566, val loss: 0.3825962245464325\n",
      "Epoch 1407: train loss: 5.14622479386162e-05, val loss: 0.3835911154747009\n",
      "Epoch 1408: train loss: 1.7540365661261603e-05, val loss: 0.38333696126937866\n",
      "Epoch 1409: train loss: 8.529640035703778e-05, val loss: 0.38131600618362427\n",
      "Epoch 1410: train loss: 3.428073614486493e-05, val loss: 0.3816509246826172\n",
      "Epoch 1411: train loss: 1.2862023140769452e-05, val loss: 0.3829091191291809\n",
      "Epoch 1412: train loss: 6.574353756150231e-05, val loss: 0.3818286657333374\n",
      "Epoch 1413: train loss: 2.075260090350639e-05, val loss: 0.3817158043384552\n",
      "Epoch 1414: train loss: 1.2483740647439845e-05, val loss: 0.38321372866630554\n",
      "Epoch 1415: train loss: 4.701221769209951e-05, val loss: 0.38371506333351135\n",
      "Epoch 1416: train loss: 1.53128585225204e-05, val loss: 0.3837086856365204\n",
      "Epoch 1417: train loss: 9.129414138442371e-06, val loss: 0.3827275335788727\n",
      "Epoch 1418: train loss: 3.490054950816557e-05, val loss: 0.38098999857902527\n",
      "Epoch 1419: train loss: 1.046103352564387e-05, val loss: 0.3815152645111084\n",
      "Epoch 1420: train loss: 7.46552996133687e-06, val loss: 0.3830152153968811\n",
      "Epoch 1421: train loss: 2.5433988412260078e-05, val loss: 0.38279709219932556\n",
      "Epoch 1422: train loss: 7.569109584437683e-06, val loss: 0.38270407915115356\n",
      "Epoch 1423: train loss: 5.953936124569736e-06, val loss: 0.38362768292427063\n",
      "Epoch 1424: train loss: 1.809178138501011e-05, val loss: 0.38427698612213135\n",
      "Epoch 1425: train loss: 5.85961424803827e-06, val loss: 0.3843477666378021\n",
      "Epoch 1426: train loss: 4.5266565393831115e-06, val loss: 0.3835853040218353\n",
      "Epoch 1427: train loss: 1.2942455214215443e-05, val loss: 0.3826063275337219\n",
      "Epoch 1428: train loss: 4.56041061625001e-06, val loss: 0.38310644030570984\n",
      "Epoch 1429: train loss: 3.2349221328331623e-06, val loss: 0.38405361771583557\n",
      "Epoch 1430: train loss: 9.567919732944574e-06, val loss: 0.3839495778083801\n",
      "Epoch 1431: train loss: 3.3615281154197874e-06, val loss: 0.38370946049690247\n",
      "Epoch 1432: train loss: 2.342829020562931e-06, val loss: 0.38417479395866394\n",
      "Epoch 1433: train loss: 7.0235641942417715e-06, val loss: 0.38483649492263794\n",
      "Epoch 1434: train loss: 2.6616271497914568e-06, val loss: 0.3851780891418457\n",
      "Epoch 1435: train loss: 1.4750587524758885e-06, val loss: 0.3849100172519684\n",
      "Epoch 1436: train loss: 5.152345238457201e-06, val loss: 0.38461676239967346\n",
      "Epoch 1437: train loss: 2.342202151339734e-06, val loss: 0.3850977122783661\n",
      "Epoch 1438: train loss: 7.062557756398746e-07, val loss: 0.3857637345790863\n",
      "Epoch 1439: train loss: 3.773743173951516e-06, val loss: 0.38592728972435\n",
      "Epoch 1440: train loss: 2.085553660435835e-06, val loss: 0.3857541084289551\n",
      "Epoch 1441: train loss: 2.86719284758874e-07, val loss: 0.3857744634151459\n",
      "Epoch 1442: train loss: 2.552451178416959e-06, val loss: 0.3860786259174347\n",
      "Epoch 1443: train loss: 2.005384658332332e-06, val loss: 0.386326402425766\n",
      "Epoch 1444: train loss: 1.1679416189736003e-07, val loss: 0.38637468218803406\n",
      "Epoch 1445: train loss: 1.511605432824581e-06, val loss: 0.38661202788352966\n",
      "Epoch 1446: train loss: 1.7916933074957342e-06, val loss: 0.3872222900390625\n",
      "Epoch 1447: train loss: 2.0575616588303092e-07, val loss: 0.38768529891967773\n",
      "Epoch 1448: train loss: 7.683673288738646e-07, val loss: 0.38777923583984375\n",
      "Epoch 1449: train loss: 1.4417350939766038e-06, val loss: 0.3875935971736908\n",
      "Epoch 1450: train loss: 3.538991961704596e-07, val loss: 0.3876819312572479\n",
      "Epoch 1451: train loss: 3.4946461369145254e-07, val loss: 0.38806530833244324\n",
      "Epoch 1452: train loss: 1.0339839491280145e-06, val loss: 0.3882409632205963\n",
      "Epoch 1453: train loss: 4.667411985792569e-07, val loss: 0.3883349597454071\n",
      "Epoch 1454: train loss: 1.383615426675533e-07, val loss: 0.3885861933231354\n",
      "Epoch 1455: train loss: 6.876084057694243e-07, val loss: 0.38881024718284607\n",
      "Epoch 1456: train loss: 4.885578732682916e-07, val loss: 0.38909295201301575\n",
      "Epoch 1457: train loss: 8.675671381297434e-08, val loss: 0.38927653431892395\n",
      "Epoch 1458: train loss: 3.8740108720958233e-07, val loss: 0.3892573416233063\n",
      "Epoch 1459: train loss: 4.701271336671198e-07, val loss: 0.389475017786026\n",
      "Epoch 1460: train loss: 8.979036181244737e-08, val loss: 0.38970112800598145\n",
      "Epoch 1461: train loss: 1.7686397768557072e-07, val loss: 0.3897622227668762\n",
      "Epoch 1462: train loss: 3.8336648344738933e-07, val loss: 0.38998472690582275\n",
      "Epoch 1463: train loss: 1.5934777763959573e-07, val loss: 0.3901435136795044\n",
      "Epoch 1464: train loss: 3.7940985464501864e-08, val loss: 0.3902686536312103\n",
      "Epoch 1465: train loss: 2.526431899241288e-07, val loss: 0.390604704618454\n",
      "Epoch 1466: train loss: 2.1420107998437743e-07, val loss: 0.39069804549217224\n",
      "Epoch 1467: train loss: 2.612112481870099e-08, val loss: 0.39080968499183655\n",
      "Epoch 1468: train loss: 9.886335305964167e-08, val loss: 0.3910699486732483\n",
      "Epoch 1469: train loss: 2.0313629534030042e-07, val loss: 0.3911355137825012\n",
      "Epoch 1470: train loss: 7.818414360372117e-08, val loss: 0.39132997393608093\n",
      "Epoch 1471: train loss: 1.881864264419164e-08, val loss: 0.3915322721004486\n",
      "Epoch 1472: train loss: 1.1563891888499711e-07, val loss: 0.3915807604789734\n",
      "Epoch 1473: train loss: 1.1697053992065776e-07, val loss: 0.39182183146476746\n",
      "Epoch 1474: train loss: 2.4303137280412557e-08, val loss: 0.3919900357723236\n",
      "Epoch 1475: train loss: 4.087315375045364e-08, val loss: 0.39207157492637634\n",
      "Epoch 1476: train loss: 9.215954577257435e-08, val loss: 0.3922867476940155\n",
      "Epoch 1477: train loss: 5.401529534765359e-08, val loss: 0.39237138628959656\n",
      "Epoch 1478: train loss: 1.3597608372606373e-08, val loss: 0.39247971773147583\n",
      "Epoch 1479: train loss: 5.0075925628334517e-08, val loss: 0.39264819025993347\n",
      "Epoch 1480: train loss: 6.365127802610004e-08, val loss: 0.39269065856933594\n",
      "Epoch 1481: train loss: 2.1369539027205064e-08, val loss: 0.39283570647239685\n",
      "Epoch 1482: train loss: 1.1831604140866148e-08, val loss: 0.39300063252449036\n",
      "Epoch 1483: train loss: 4.5077044319441484e-08, val loss: 0.3930651843547821\n",
      "Epoch 1484: train loss: 4.0546776602923273e-08, val loss: 0.39319702982902527\n",
      "Epoch 1485: train loss: 8.970029163890558e-09, val loss: 0.3932980000972748\n",
      "Epoch 1486: train loss: 1.0864527055787221e-08, val loss: 0.3932889401912689\n",
      "Epoch 1487: train loss: 3.353198252398215e-08, val loss: 0.39326316118240356\n",
      "Epoch 1488: train loss: 2.8456549117095165e-08, val loss: 0.3931301534175873\n",
      "Epoch 1489: train loss: 9.460554117879383e-09, val loss: 0.39301347732543945\n",
      "Epoch 1490: train loss: 1.3038524926400896e-08, val loss: 0.39296257495880127\n",
      "Epoch 1491: train loss: 2.3991177044990764e-08, val loss: 0.3927973210811615\n",
      "Epoch 1492: train loss: 1.6855226547818347e-08, val loss: 0.3927348554134369\n",
      "Epoch 1493: train loss: 4.300082068198208e-09, val loss: 0.3926326632499695\n",
      "Epoch 1494: train loss: 8.005907936592394e-09, val loss: 0.39245548844337463\n",
      "Epoch 1495: train loss: 1.672131588748016e-08, val loss: 0.3923843502998352\n",
      "Epoch 1496: train loss: 1.4349967436544375e-08, val loss: 0.39221784472465515\n",
      "Epoch 1497: train loss: 6.8151306820141144e-09, val loss: 0.39209213852882385\n",
      "Epoch 1498: train loss: 7.620795550167259e-09, val loss: 0.39198213815689087\n",
      "Epoch 1499: train loss: 1.361651502662653e-08, val loss: 0.3918024003505707\n",
      "Epoch 1500: train loss: 1.2672746407815794e-08, val loss: 0.3916895091533661\n",
      "Epoch 1501: train loss: 6.61359189635391e-09, val loss: 0.3915400505065918\n",
      "Epoch 1502: train loss: 4.918329743475169e-09, val loss: 0.3913569152355194\n",
      "Epoch 1503: train loss: 8.274134266628153e-09, val loss: 0.39124590158462524\n",
      "Epoch 1504: train loss: 9.798935884930415e-09, val loss: 0.39106419682502747\n",
      "Epoch 1505: train loss: 6.091794180207444e-09, val loss: 0.3909428119659424\n",
      "Epoch 1506: train loss: 3.649191837595822e-09, val loss: 0.390804260969162\n",
      "Epoch 1507: train loss: 4.958323973625056e-09, val loss: 0.39063140749931335\n",
      "Epoch 1508: train loss: 6.914282479897338e-09, val loss: 0.3905109465122223\n",
      "Epoch 1509: train loss: 6.768786420252582e-09, val loss: 0.3903495967388153\n",
      "Epoch 1510: train loss: 5.142307024641468e-09, val loss: 0.3901929259300232\n",
      "Epoch 1511: train loss: 5.183099727190665e-09, val loss: 0.39005032181739807\n",
      "Epoch 1512: train loss: 7.953231850876818e-09, val loss: 0.38987284898757935\n",
      "Epoch 1513: train loss: 1.2799650228600967e-08, val loss: 0.3897674083709717\n",
      "Epoch 1514: train loss: 1.9351912072806954e-08, val loss: 0.3895861506462097\n",
      "Epoch 1515: train loss: 3.327904707361995e-08, val loss: 0.3894760012626648\n",
      "Epoch 1516: train loss: 6.571134036903459e-08, val loss: 0.3892765939235687\n",
      "Epoch 1517: train loss: 1.4313381768715772e-07, val loss: 0.38917309045791626\n",
      "Epoch 1518: train loss: 3.262018708483083e-07, val loss: 0.3889191150665283\n",
      "Epoch 1519: train loss: 7.765899567857559e-07, val loss: 0.38891804218292236\n",
      "Epoch 1520: train loss: 1.9151668766426155e-06, val loss: 0.38851621747016907\n",
      "Epoch 1521: train loss: 4.4176517803862225e-06, val loss: 0.3888110816478729\n",
      "Epoch 1522: train loss: 7.637194357812405e-06, val loss: 0.3880431354045868\n",
      "Epoch 1523: train loss: 9.983657037082594e-06, val loss: 0.3886943757534027\n",
      "Epoch 1524: train loss: 1.1020738384104334e-05, val loss: 0.3877858817577362\n",
      "Epoch 1525: train loss: 1.0654377547325566e-05, val loss: 0.38849127292633057\n",
      "Epoch 1526: train loss: 9.325451173936017e-06, val loss: 0.38771483302116394\n",
      "Epoch 1527: train loss: 7.430737241520546e-06, val loss: 0.38827720284461975\n",
      "Epoch 1528: train loss: 5.4669653763994575e-06, val loss: 0.387653112411499\n",
      "Epoch 1529: train loss: 3.6841909150098218e-06, val loss: 0.3879867196083069\n",
      "Epoch 1530: train loss: 2.2549863842868945e-06, val loss: 0.38755032420158386\n",
      "Epoch 1531: train loss: 1.218279749082285e-06, val loss: 0.3876766562461853\n",
      "Epoch 1532: train loss: 5.549147203964822e-07, val loss: 0.3873318135738373\n",
      "Epoch 1533: train loss: 1.8432406534429902e-07, val loss: 0.3873100280761719\n",
      "Epoch 1534: train loss: 2.8105036520287285e-08, val loss: 0.38716796040534973\n",
      "Epoch 1535: train loss: 2.2054166493035154e-08, val loss: 0.3869428038597107\n",
      "Epoch 1536: train loss: 1.3367255746743467e-07, val loss: 0.3869461715221405\n",
      "Epoch 1537: train loss: 3.502590573134512e-07, val loss: 0.38658490777015686\n",
      "Epoch 1538: train loss: 6.795627314204467e-07, val loss: 0.3867126405239105\n",
      "Epoch 1539: train loss: 1.173725308944995e-06, val loss: 0.38615307211875916\n",
      "Epoch 1540: train loss: 1.9266578874521656e-06, val loss: 0.3864958882331848\n",
      "Epoch 1541: train loss: 3.156032789775054e-06, val loss: 0.3856598734855652\n",
      "Epoch 1542: train loss: 5.1911911214119755e-06, val loss: 0.38632798194885254\n",
      "Epoch 1543: train loss: 9.01864405022934e-06, val loss: 0.3849917948246002\n",
      "Epoch 1544: train loss: 1.6231462723226286e-05, val loss: 0.38637733459472656\n",
      "Epoch 1545: train loss: 3.0375229471246712e-05, val loss: 0.38343295454978943\n",
      "Epoch 1546: train loss: 5.7658642617752776e-05, val loss: 0.38678398728370667\n",
      "Epoch 1547: train loss: 0.00011230372911086306, val loss: 0.38159099221229553\n",
      "Epoch 1548: train loss: 0.00020263431360945106, val loss: 0.3891707956790924\n",
      "Epoch 1549: train loss: 0.00035108672454953194, val loss: 0.3796115815639496\n",
      "Epoch 1550: train loss: 0.0005530361086130142, val loss: 0.38708338141441345\n",
      "Epoch 1551: train loss: 0.0007382950279861689, val loss: 0.3761013448238373\n",
      "Epoch 1552: train loss: 0.0007167187286540866, val loss: 0.3775710165500641\n",
      "Epoch 1553: train loss: 0.00045149135985411704, val loss: 0.3725953698158264\n",
      "Epoch 1554: train loss: 0.00010331402882002294, val loss: 0.36871787905693054\n",
      "Epoch 1555: train loss: 5.152848461875692e-05, val loss: 0.37033408880233765\n",
      "Epoch 1556: train loss: 0.0002594641409814358, val loss: 0.36241766810417175\n",
      "Epoch 1557: train loss: 0.00035601636045612395, val loss: 0.3689218759536743\n",
      "Epoch 1558: train loss: 0.00019711606728378683, val loss: 0.36352524161338806\n",
      "Epoch 1559: train loss: 2.547250005591195e-05, val loss: 0.3638809621334076\n",
      "Epoch 1560: train loss: 7.75923253968358e-05, val loss: 0.36416345834732056\n",
      "Epoch 1561: train loss: 0.0001851526030804962, val loss: 0.361274778842926\n",
      "Epoch 1562: train loss: 0.00012443697778508067, val loss: 0.36119771003723145\n",
      "Epoch 1563: train loss: 1.3572940588346682e-05, val loss: 0.3604823052883148\n",
      "Epoch 1564: train loss: 3.738704981515184e-05, val loss: 0.3580821454524994\n",
      "Epoch 1565: train loss: 0.00011365290993126109, val loss: 0.3578663170337677\n",
      "Epoch 1566: train loss: 7.984822877915576e-05, val loss: 0.355914831161499\n",
      "Epoch 1567: train loss: 1.1911954970855732e-05, val loss: 0.35390543937683105\n",
      "Epoch 1568: train loss: 3.73318835045211e-05, val loss: 0.3567032516002655\n",
      "Epoch 1569: train loss: 7.873873983044177e-05, val loss: 0.3532026708126068\n",
      "Epoch 1570: train loss: 4.279136555851437e-05, val loss: 0.3533407151699066\n",
      "Epoch 1571: train loss: 2.67130121756054e-06, val loss: 0.35300326347351074\n",
      "Epoch 1572: train loss: 2.9367540264502168e-05, val loss: 0.35161539912223816\n",
      "Epoch 1573: train loss: 4.775014167535119e-05, val loss: 0.35292574763298035\n",
      "Epoch 1574: train loss: 1.713339952402748e-05, val loss: 0.35093745589256287\n",
      "Epoch 1575: train loss: 3.3078272281272803e-06, val loss: 0.3492729663848877\n",
      "Epoch 1576: train loss: 2.762620169960428e-05, val loss: 0.34950119256973267\n",
      "Epoch 1577: train loss: 3.1530213163932785e-05, val loss: 0.3490685522556305\n",
      "Epoch 1578: train loss: 8.649615665490273e-06, val loss: 0.34795624017715454\n",
      "Epoch 1579: train loss: 7.74656655266881e-06, val loss: 0.34892770648002625\n",
      "Epoch 1580: train loss: 2.4111255697789602e-05, val loss: 0.3467511832714081\n",
      "Epoch 1581: train loss: 1.896235698950477e-05, val loss: 0.34765493869781494\n",
      "Epoch 1582: train loss: 4.36862910646596e-06, val loss: 0.34656891226768494\n",
      "Epoch 1583: train loss: 8.188211722881533e-06, val loss: 0.3460073173046112\n",
      "Epoch 1584: train loss: 1.7033033145708032e-05, val loss: 0.3460979163646698\n",
      "Epoch 1585: train loss: 1.0317184205632657e-05, val loss: 0.3459130823612213\n",
      "Epoch 1586: train loss: 2.1492335235961946e-06, val loss: 0.3445427715778351\n",
      "Epoch 1587: train loss: 7.289490440598456e-06, val loss: 0.34524139761924744\n",
      "Epoch 1588: train loss: 1.165725461760303e-05, val loss: 0.3437056541442871\n",
      "Epoch 1589: train loss: 5.9242361203359906e-06, val loss: 0.34417611360549927\n",
      "Epoch 1590: train loss: 2.2035249003238278e-06, val loss: 0.34367474913597107\n",
      "Epoch 1591: train loss: 6.802285952289822e-06, val loss: 0.3435194492340088\n",
      "Epoch 1592: train loss: 9.529862836643588e-06, val loss: 0.3430565595626831\n",
      "Epoch 1593: train loss: 6.029541054886067e-06, val loss: 0.3435990810394287\n",
      "Epoch 1594: train loss: 5.760148269473575e-06, val loss: 0.34214699268341064\n",
      "Epoch 1595: train loss: 1.164608693215996e-05, val loss: 0.3438619077205658\n",
      "Epoch 1596: train loss: 1.7298372767982073e-05, val loss: 0.3413768708705902\n",
      "Epoch 1597: train loss: 2.1529942387132905e-05, val loss: 0.343252032995224\n",
      "Epoch 1598: train loss: 3.1932228012010455e-05, val loss: 0.3407224416732788\n",
      "Epoch 1599: train loss: 5.19644127052743e-05, val loss: 0.3435489535331726\n",
      "Epoch 1600: train loss: 8.307796815643087e-05, val loss: 0.3395903706550598\n",
      "Epoch 1601: train loss: 0.00012889542267657816, val loss: 0.34381529688835144\n",
      "Epoch 1602: train loss: 0.00020728092931676656, val loss: 0.33767980337142944\n",
      "Epoch 1603: train loss: 0.00032967873266898096, val loss: 0.3445234000682831\n",
      "Epoch 1604: train loss: 0.0005099882837384939, val loss: 0.335274875164032\n",
      "Epoch 1605: train loss: 0.0007466505048796535, val loss: 0.34459373354911804\n",
      "Epoch 1606: train loss: 0.0010474377777427435, val loss: 0.3346996307373047\n",
      "Epoch 1607: train loss: 0.0013503835070878267, val loss: 0.35042616724967957\n",
      "Epoch 1608: train loss: 0.001572751090861857, val loss: 0.335545152425766\n",
      "Epoch 1609: train loss: 0.0016022418858483434, val loss: 0.35149893164634705\n",
      "Epoch 1610: train loss: 0.0013286337489262223, val loss: 0.33490195870399475\n",
      "Epoch 1611: train loss: 0.000793756393250078, val loss: 0.34321722388267517\n",
      "Epoch 1612: train loss: 0.00025914516299963, val loss: 0.33736470341682434\n",
      "Epoch 1613: train loss: 5.759334726462839e-06, val loss: 0.3344326913356781\n",
      "Epoch 1614: train loss: 0.00012080380838597193, val loss: 0.3415999710559845\n",
      "Epoch 1615: train loss: 0.0004001244669780135, val loss: 0.33212384581565857\n",
      "Epoch 1616: train loss: 0.0005346182733774185, val loss: 0.34165194630622864\n",
      "Epoch 1617: train loss: 0.00038004410453140736, val loss: 0.3318977952003479\n",
      "Epoch 1618: train loss: 0.00010942289372906089, val loss: 0.33347445726394653\n",
      "Epoch 1619: train loss: 2.278984766235226e-06, val loss: 0.33679908514022827\n",
      "Epoch 1620: train loss: 0.0001185110304504633, val loss: 0.3296028673648834\n",
      "Epoch 1621: train loss: 0.00025803380412980914, val loss: 0.3363456428050995\n",
      "Epoch 1622: train loss: 0.00023077362857293338, val loss: 0.3292514383792877\n",
      "Epoch 1623: train loss: 8.174809045158327e-05, val loss: 0.33086615800857544\n",
      "Epoch 1624: train loss: 9.168343808596546e-07, val loss: 0.3325677812099457\n",
      "Epoch 1625: train loss: 6.160249904496595e-05, val loss: 0.3276505768299103\n",
      "Epoch 1626: train loss: 0.00014632282545790076, val loss: 0.3325711488723755\n",
      "Epoch 1627: train loss: 0.00013048574328422546, val loss: 0.3276907503604889\n",
      "Epoch 1628: train loss: 4.159777381573804e-05, val loss: 0.32848256826400757\n",
      "Epoch 1629: train loss: 6.640461833740119e-07, val loss: 0.32970768213272095\n",
      "Epoch 1630: train loss: 4.466553218662739e-05, val loss: 0.3253474533557892\n",
      "Epoch 1631: train loss: 9.037090057972819e-05, val loss: 0.32927629351615906\n",
      "Epoch 1632: train loss: 6.954988930374384e-05, val loss: 0.32595351338386536\n",
      "Epoch 1633: train loss: 1.6105168469948694e-05, val loss: 0.32603830099105835\n",
      "Epoch 1634: train loss: 2.4870817014743807e-06, val loss: 0.3273427486419678\n",
      "Epoch 1635: train loss: 3.432947414694354e-05, val loss: 0.32380029559135437\n",
      "Epoch 1636: train loss: 5.5674216127954423e-05, val loss: 0.3268517255783081\n",
      "Epoch 1637: train loss: 3.617989568738267e-05, val loss: 0.3246636390686035\n",
      "Epoch 1638: train loss: 5.710768164135516e-06, val loss: 0.3247377574443817\n",
      "Epoch 1639: train loss: 3.4845431855501374e-06, val loss: 0.3262307345867157\n",
      "Epoch 1640: train loss: 2.413959737168625e-05, val loss: 0.32322436571121216\n",
      "Epoch 1641: train loss: 3.3514217648189515e-05, val loss: 0.32534655928611755\n",
      "Epoch 1642: train loss: 1.9387407519388944e-05, val loss: 0.3237893283367157\n",
      "Epoch 1643: train loss: 2.2769736460759304e-06, val loss: 0.3237321972846985\n",
      "Epoch 1644: train loss: 2.9778670977975708e-06, val loss: 0.3250294625759125\n",
      "Epoch 1645: train loss: 1.5520481611019932e-05, val loss: 0.3226654529571533\n",
      "Epoch 1646: train loss: 2.0449861040106043e-05, val loss: 0.3245489299297333\n",
      "Epoch 1647: train loss: 1.1774538506870158e-05, val loss: 0.32332196831703186\n",
      "Epoch 1648: train loss: 1.5757344726807787e-06, val loss: 0.3232891857624054\n",
      "Epoch 1649: train loss: 1.4311187896964839e-06, val loss: 0.3242458999156952\n",
      "Epoch 1650: train loss: 8.73958197189495e-06, val loss: 0.32249459624290466\n",
      "Epoch 1651: train loss: 1.2641553439607378e-05, val loss: 0.32403793931007385\n",
      "Epoch 1652: train loss: 8.569145393266808e-06, val loss: 0.32282328605651855\n",
      "Epoch 1653: train loss: 1.990862301681773e-06, val loss: 0.3230000436306\n",
      "Epoch 1654: train loss: 2.3483279676383972e-07, val loss: 0.3235216438770294\n",
      "Epoch 1655: train loss: 3.7904781038378133e-06, val loss: 0.32224175333976746\n",
      "Epoch 1656: train loss: 7.275632469827542e-06, val loss: 0.32346633076667786\n",
      "Epoch 1657: train loss: 6.552979812113335e-06, val loss: 0.3222447335720062\n",
      "Epoch 1658: train loss: 2.816878577505122e-06, val loss: 0.3226993978023529\n",
      "Epoch 1659: train loss: 1.7600788737581752e-07, val loss: 0.3227269649505615\n",
      "Epoch 1660: train loss: 8.082830618150183e-07, val loss: 0.3219330310821533\n",
      "Epoch 1661: train loss: 3.185317382303765e-06, val loss: 0.3228432834148407\n",
      "Epoch 1662: train loss: 4.433782578416867e-06, val loss: 0.321763277053833\n",
      "Epoch 1663: train loss: 3.3627511584199965e-06, val loss: 0.32247301936149597\n",
      "Epoch 1664: train loss: 1.241784502781229e-06, val loss: 0.32204461097717285\n",
      "Epoch 1665: train loss: 5.199816754952735e-08, val loss: 0.32177501916885376\n",
      "Epoch 1666: train loss: 5.32591002411209e-07, val loss: 0.3221977651119232\n",
      "Epoch 1667: train loss: 1.8143297211281606e-06, val loss: 0.3211762607097626\n",
      "Epoch 1668: train loss: 2.5400222511962056e-06, val loss: 0.32208582758903503\n",
      "Epoch 1669: train loss: 2.125740365954698e-06, val loss: 0.32118427753448486\n",
      "Epoch 1670: train loss: 1.0292080787621671e-06, val loss: 0.3215140998363495\n",
      "Epoch 1671: train loss: 1.6378089640056714e-07, val loss: 0.3213104009628296\n",
      "Epoch 1672: train loss: 6.655923101561712e-08, val loss: 0.32086893916130066\n",
      "Epoch 1673: train loss: 6.031227712810505e-07, val loss: 0.3213745057582855\n",
      "Epoch 1674: train loss: 1.2263151347724488e-06, val loss: 0.32051438093185425\n",
      "Epoch 1675: train loss: 1.4605059277528198e-06, val loss: 0.3211258351802826\n",
      "Epoch 1676: train loss: 1.1972250604230794e-06, val loss: 0.3203687369823456\n",
      "Epoch 1677: train loss: 6.586329845958971e-07, val loss: 0.32066115736961365\n",
      "Epoch 1678: train loss: 1.8767207166092703e-07, val loss: 0.3204098641872406\n",
      "Epoch 1679: train loss: 6.9488184095689576e-09, val loss: 0.32019734382629395\n",
      "Epoch 1680: train loss: 1.2831209517116804e-07, val loss: 0.3203905522823334\n",
      "Epoch 1681: train loss: 4.1492248215035943e-07, val loss: 0.31976133584976196\n",
      "Epoch 1682: train loss: 6.82821053032967e-07, val loss: 0.3202141225337982\n",
      "Epoch 1683: train loss: 8.176372716661717e-07, val loss: 0.3195103406906128\n",
      "Epoch 1684: train loss: 7.795371175234322e-07, val loss: 0.3199731111526489\n",
      "Epoch 1685: train loss: 6.210727860889165e-07, val loss: 0.3193395435810089\n",
      "Epoch 1686: train loss: 4.1092539504461456e-07, val loss: 0.31957361102104187\n",
      "Epoch 1687: train loss: 2.3237338098169857e-07, val loss: 0.3191544711589813\n",
      "Epoch 1688: train loss: 1.171632391105959e-07, val loss: 0.3191923201084137\n",
      "Epoch 1689: train loss: 9.679003198925784e-08, val loss: 0.31900718808174133\n",
      "Epoch 1690: train loss: 1.5885868265286263e-07, val loss: 0.3188525140285492\n",
      "Epoch 1691: train loss: 3.3159298595819564e-07, val loss: 0.318772554397583\n",
      "Epoch 1692: train loss: 6.416395876840397e-07, val loss: 0.3185906410217285\n",
      "Epoch 1693: train loss: 1.3144070862836088e-06, val loss: 0.318432480096817\n",
      "Epoch 1694: train loss: 2.6236468784190947e-06, val loss: 0.31842315196990967\n",
      "Epoch 1695: train loss: 5.30425631950493e-06, val loss: 0.3184552788734436\n",
      "Epoch 1696: train loss: 8.158402124536224e-06, val loss: 0.3190547525882721\n",
      "Epoch 1697: train loss: 9.166573363472708e-06, val loss: 0.31885790824890137\n",
      "Epoch 1698: train loss: 5.462169610837009e-06, val loss: 0.31896132230758667\n",
      "Epoch 1699: train loss: 3.549208940967219e-06, val loss: 0.318965345621109\n",
      "Epoch 1700: train loss: 8.284557225124445e-06, val loss: 0.31820032000541687\n",
      "Epoch 1701: train loss: 1.4658032341685612e-05, val loss: 0.31796860694885254\n",
      "Epoch 1702: train loss: 2.016499638557434e-05, val loss: 0.31809359788894653\n",
      "Epoch 1703: train loss: 2.991381006722804e-05, val loss: 0.3170372247695923\n",
      "Epoch 1704: train loss: 4.5389042497845367e-05, val loss: 0.31783849000930786\n",
      "Epoch 1705: train loss: 6.359515100484714e-05, val loss: 0.31686684489250183\n",
      "Epoch 1706: train loss: 8.5419014794752e-05, val loss: 0.31777670979499817\n",
      "Epoch 1707: train loss: 0.00011599293065955862, val loss: 0.3162263035774231\n",
      "Epoch 1708: train loss: 0.00015427422476932406, val loss: 0.3192184567451477\n",
      "Epoch 1709: train loss: 0.00020624714670702815, val loss: 0.3140659034252167\n",
      "Epoch 1710: train loss: 0.00027441978454589844, val loss: 0.32170388102531433\n",
      "Epoch 1711: train loss: 0.0003788041649386287, val loss: 0.3108068108558655\n",
      "Epoch 1712: train loss: 0.0005268098902888596, val loss: 0.3235367238521576\n",
      "Epoch 1713: train loss: 0.0008205116027966142, val loss: 0.30474478006362915\n",
      "Epoch 1714: train loss: 0.0012921936577185988, val loss: 0.32520273327827454\n",
      "Epoch 1715: train loss: 0.0020186719484627247, val loss: 0.29886800050735474\n",
      "Epoch 1716: train loss: 0.0030098522547632456, val loss: 0.3171357810497284\n",
      "Epoch 1717: train loss: 0.004119938239455223, val loss: 0.29765745997428894\n",
      "Epoch 1718: train loss: 0.004761460702866316, val loss: 0.3209894895553589\n",
      "Epoch 1719: train loss: 0.004694461822509766, val loss: 0.28204017877578735\n",
      "Epoch 1720: train loss: 0.00345595576800406, val loss: 0.3007539212703705\n",
      "Epoch 1721: train loss: 0.00178189214784652, val loss: 0.2876112461090088\n",
      "Epoch 1722: train loss: 0.0007210669573396444, val loss: 0.28507161140441895\n",
      "Epoch 1723: train loss: 0.0009091205429285765, val loss: 0.3004210889339447\n",
      "Epoch 1724: train loss: 0.0015203701332211494, val loss: 0.281138151884079\n",
      "Epoch 1725: train loss: 0.0013590374728664756, val loss: 0.29665085673332214\n",
      "Epoch 1726: train loss: 0.0006559831672348082, val loss: 0.2929549813270569\n",
      "Epoch 1727: train loss: 0.0004838105814997107, val loss: 0.2833814322948456\n",
      "Epoch 1728: train loss: 0.0008245458593592048, val loss: 0.29832276701927185\n",
      "Epoch 1729: train loss: 0.0006895383121445775, val loss: 0.28830981254577637\n",
      "Epoch 1730: train loss: 0.0002922324638348073, val loss: 0.28927063941955566\n",
      "Epoch 1731: train loss: 0.00038771965773776174, val loss: 0.29663607478141785\n",
      "Epoch 1732: train loss: 0.0005650520324707031, val loss: 0.285081684589386\n",
      "Epoch 1733: train loss: 0.00028630514862015843, val loss: 0.29040247201919556\n",
      "Epoch 1734: train loss: 0.00015204072406049818, val loss: 0.29414451122283936\n",
      "Epoch 1735: train loss: 0.0003782445564866066, val loss: 0.28563761711120605\n",
      "Epoch 1736: train loss: 0.0003023026220034808, val loss: 0.2912966310977936\n",
      "Epoch 1737: train loss: 7.67418896430172e-05, val loss: 0.2914220094680786\n",
      "Epoch 1738: train loss: 0.00019199715461581945, val loss: 0.2864006757736206\n",
      "Epoch 1739: train loss: 0.0002781942021101713, val loss: 0.29138562083244324\n",
      "Epoch 1740: train loss: 7.712321530561894e-05, val loss: 0.2889750897884369\n",
      "Epoch 1741: train loss: 7.495951285818592e-05, val loss: 0.28634122014045715\n",
      "Epoch 1742: train loss: 0.00020847006817348301, val loss: 0.29130062460899353\n",
      "Epoch 1743: train loss: 0.00010307378397556022, val loss: 0.2873801589012146\n",
      "Epoch 1744: train loss: 1.8197424651589245e-05, val loss: 0.2845064699649811\n",
      "Epoch 1745: train loss: 0.00013244833098724484, val loss: 0.2886612117290497\n",
      "Epoch 1746: train loss: 0.00011296226875856519, val loss: 0.28649041056632996\n",
      "Epoch 1747: train loss: 7.967025339894462e-06, val loss: 0.28612634539604187\n",
      "Epoch 1748: train loss: 6.730105815222487e-05, val loss: 0.2888526916503906\n",
      "Epoch 1749: train loss: 0.00010330161603633314, val loss: 0.2857903838157654\n",
      "Epoch 1750: train loss: 1.8305157936993055e-05, val loss: 0.286042720079422\n",
      "Epoch 1751: train loss: 2.4573420887463726e-05, val loss: 0.2880535423755646\n",
      "Epoch 1752: train loss: 7.932100561447442e-05, val loss: 0.2846819758415222\n",
      "Epoch 1753: train loss: 3.101239417446777e-05, val loss: 0.2850959897041321\n",
      "Epoch 1754: train loss: 5.033941761212191e-06, val loss: 0.28684234619140625\n",
      "Epoch 1755: train loss: 5.037433220422827e-05, val loss: 0.28449007868766785\n",
      "Epoch 1756: train loss: 3.793808718910441e-05, val loss: 0.28543636202812195\n",
      "Epoch 1757: train loss: 8.6930475617919e-07, val loss: 0.2863557040691376\n",
      "Epoch 1758: train loss: 2.5921290216501802e-05, val loss: 0.2844768166542053\n",
      "Epoch 1759: train loss: 3.6006524169351906e-05, val loss: 0.2859339416027069\n",
      "Epoch 1760: train loss: 5.69073790757102e-06, val loss: 0.2861076295375824\n",
      "Epoch 1761: train loss: 9.259919352189172e-06, val loss: 0.2841120660305023\n",
      "Epoch 1762: train loss: 2.7742909878725186e-05, val loss: 0.28537803888320923\n",
      "Epoch 1763: train loss: 1.1496460501803085e-05, val loss: 0.2851668894290924\n",
      "Epoch 1764: train loss: 2.0067570858373074e-06, val loss: 0.28395745158195496\n",
      "Epoch 1765: train loss: 1.6940035493462346e-05, val loss: 0.2854178845882416\n",
      "Epoch 1766: train loss: 1.4251936590881087e-05, val loss: 0.2847878932952881\n",
      "Epoch 1767: train loss: 1.468849063712696e-06, val loss: 0.2840259373188019\n",
      "Epoch 1768: train loss: 7.583195838378742e-06, val loss: 0.28541064262390137\n",
      "Epoch 1769: train loss: 1.3025804037170019e-05, val loss: 0.2845912575721741\n",
      "Epoch 1770: train loss: 3.780075076065259e-06, val loss: 0.28432828187942505\n",
      "Epoch 1771: train loss: 2.3616303224116564e-06, val loss: 0.2852400243282318\n",
      "Epoch 1772: train loss: 8.861778042046353e-06, val loss: 0.28399255871772766\n",
      "Epoch 1773: train loss: 5.835895535710733e-06, val loss: 0.2841281592845917\n",
      "Epoch 1774: train loss: 9.494668802290107e-07, val loss: 0.2848476767539978\n",
      "Epoch 1775: train loss: 4.437414645508397e-06, val loss: 0.28383922576904297\n",
      "Epoch 1776: train loss: 5.9229428188700695e-06, val loss: 0.2843751609325409\n",
      "Epoch 1777: train loss: 1.7023046439135214e-06, val loss: 0.28458094596862793\n",
      "Epoch 1778: train loss: 1.5238656487781554e-06, val loss: 0.2838471829891205\n",
      "Epoch 1779: train loss: 4.252117378200637e-06, val loss: 0.2846468389034271\n",
      "Epoch 1780: train loss: 2.763040811259998e-06, val loss: 0.28420859575271606\n",
      "Epoch 1781: train loss: 6.07353001669253e-07, val loss: 0.28360962867736816\n",
      "Epoch 1782: train loss: 2.109909928549314e-06, val loss: 0.284505695104599\n",
      "Epoch 1783: train loss: 2.887407163143507e-06, val loss: 0.2839583158493042\n",
      "Epoch 1784: train loss: 1.0156485359402723e-06, val loss: 0.283743292093277\n",
      "Epoch 1785: train loss: 6.556290941261977e-07, val loss: 0.2842216193675995\n",
      "Epoch 1786: train loss: 1.992475745282718e-06, val loss: 0.28359681367874146\n",
      "Epoch 1787: train loss: 1.6056873164416174e-06, val loss: 0.2840026319026947\n",
      "Epoch 1788: train loss: 3.2939246352725604e-07, val loss: 0.2841361463069916\n",
      "Epoch 1789: train loss: 7.738377121313533e-07, val loss: 0.2834540903568268\n",
      "Epoch 1790: train loss: 1.5095974958967417e-06, val loss: 0.2840014696121216\n",
      "Epoch 1791: train loss: 8.02228100837965e-07, val loss: 0.2837865352630615\n",
      "Epoch 1792: train loss: 1.4261324565723044e-07, val loss: 0.2834145724773407\n",
      "Epoch 1793: train loss: 7.21572803286108e-07, val loss: 0.28386956453323364\n",
      "Epoch 1794: train loss: 1.0540294397287653e-06, val loss: 0.2834160327911377\n",
      "Epoch 1795: train loss: 3.9954491626303934e-07, val loss: 0.28352561593055725\n",
      "Epoch 1796: train loss: 1.003224241458156e-07, val loss: 0.28377166390419006\n",
      "Epoch 1797: train loss: 5.69696169350209e-07, val loss: 0.2832375466823578\n",
      "Epoch 1798: train loss: 7.200667937468097e-07, val loss: 0.2835295796394348\n",
      "Epoch 1799: train loss: 2.2855699910451222e-07, val loss: 0.2834192216396332\n",
      "Epoch 1800: train loss: 5.133769320764259e-08, val loss: 0.28314289450645447\n",
      "Epoch 1801: train loss: 3.99364949998926e-07, val loss: 0.28343868255615234\n",
      "Epoch 1802: train loss: 4.918642275697493e-07, val loss: 0.2830543518066406\n",
      "Epoch 1803: train loss: 1.6456093021588458e-07, val loss: 0.28308844566345215\n",
      "Epoch 1804: train loss: 2.1012095174910428e-08, val loss: 0.28321191668510437\n",
      "Epoch 1805: train loss: 2.4695606271052384e-07, val loss: 0.2828431725502014\n",
      "Epoch 1806: train loss: 3.545133608895412e-07, val loss: 0.2830725610256195\n",
      "Epoch 1807: train loss: 1.4869767994696304e-07, val loss: 0.2829257547855377\n",
      "Epoch 1808: train loss: 9.83822712186111e-09, val loss: 0.28274959325790405\n",
      "Epoch 1809: train loss: 1.246052079295623e-07, val loss: 0.28289955854415894\n",
      "Epoch 1810: train loss: 2.3591113063048397e-07, val loss: 0.28259024024009705\n",
      "Epoch 1811: train loss: 1.4285019744875171e-07, val loss: 0.28265494108200073\n",
      "Epoch 1812: train loss: 1.8182076289008364e-08, val loss: 0.28259944915771484\n",
      "Epoch 1813: train loss: 4.656282470705264e-08, val loss: 0.2823621928691864\n",
      "Epoch 1814: train loss: 1.4030035799805773e-07, val loss: 0.2825005352497101\n",
      "Epoch 1815: train loss: 1.31395395897016e-07, val loss: 0.28227880597114563\n",
      "Epoch 1816: train loss: 4.3020374818070195e-08, val loss: 0.2822422981262207\n",
      "Epoch 1817: train loss: 1.5283385224051926e-08, val loss: 0.2822268009185791\n",
      "Epoch 1818: train loss: 6.647712069707268e-08, val loss: 0.2820051908493042\n",
      "Epoch 1819: train loss: 9.823195057379053e-08, val loss: 0.28207993507385254\n",
      "Epoch 1820: train loss: 6.011138964368001e-08, val loss: 0.28188350796699524\n",
      "Epoch 1821: train loss: 1.3691379585623054e-08, val loss: 0.2817976176738739\n",
      "Epoch 1822: train loss: 1.8105422938674565e-08, val loss: 0.28180497884750366\n",
      "Epoch 1823: train loss: 5.475110143038364e-08, val loss: 0.2816009521484375\n",
      "Epoch 1824: train loss: 6.260691520765249e-08, val loss: 0.2816297113895416\n",
      "Epoch 1825: train loss: 3.0795121119808755e-08, val loss: 0.2814842164516449\n",
      "Epoch 1826: train loss: 5.517114320952032e-09, val loss: 0.28138065338134766\n",
      "Epoch 1827: train loss: 1.45076981539205e-08, val loss: 0.28136372566223145\n",
      "Epoch 1828: train loss: 3.8575056038325783e-08, val loss: 0.2811550796031952\n",
      "Epoch 1829: train loss: 4.314609469702191e-08, val loss: 0.28115034103393555\n",
      "Epoch 1830: train loss: 2.517490926834398e-08, val loss: 0.2810056805610657\n",
      "Epoch 1831: train loss: 1.3305325730073037e-08, val loss: 0.28090277314186096\n",
      "Epoch 1832: train loss: 3.379574664563734e-08, val loss: 0.2808438241481781\n",
      "Epoch 1833: train loss: 1.4154778682495817e-07, val loss: 0.2806973159313202\n",
      "Epoch 1834: train loss: 5.553266646529664e-07, val loss: 0.28062137961387634\n",
      "Epoch 1835: train loss: 1.5844733525227639e-06, val loss: 0.2805252969264984\n",
      "Epoch 1836: train loss: 1.2503909374572686e-06, val loss: 0.2803007662296295\n",
      "Epoch 1837: train loss: 1.7168238741760433e-07, val loss: 0.28029289841651917\n",
      "Epoch 1838: train loss: 7.245167807923281e-07, val loss: 0.28023961186408997\n",
      "Epoch 1839: train loss: 1.18055061193445e-06, val loss: 0.2800482213497162\n",
      "Epoch 1840: train loss: 5.636385935758881e-07, val loss: 0.27987053990364075\n",
      "Epoch 1841: train loss: 2.686867048851127e-07, val loss: 0.2799703776836395\n",
      "Epoch 1842: train loss: 6.269130494729325e-07, val loss: 0.2797498404979706\n",
      "Epoch 1843: train loss: 7.021650958449754e-07, val loss: 0.27958688139915466\n",
      "Epoch 1844: train loss: 3.559923129614617e-07, val loss: 0.2796262204647064\n",
      "Epoch 1845: train loss: 2.7949567993346136e-07, val loss: 0.27931591868400574\n",
      "Epoch 1846: train loss: 4.9599026397118e-07, val loss: 0.27935442328453064\n",
      "Epoch 1847: train loss: 5.274528689369617e-07, val loss: 0.2792675197124481\n",
      "Epoch 1848: train loss: 4.070483328177943e-07, val loss: 0.27901574969291687\n",
      "Epoch 1849: train loss: 4.841228928853525e-07, val loss: 0.2790084481239319\n",
      "Epoch 1850: train loss: 8.105425877147354e-07, val loss: 0.2789178788661957\n",
      "Epoch 1851: train loss: 1.3220503660704708e-06, val loss: 0.278667151927948\n",
      "Epoch 1852: train loss: 2.2348485799739137e-06, val loss: 0.27867138385772705\n",
      "Epoch 1853: train loss: 4.343336058809655e-06, val loss: 0.2784554660320282\n",
      "Epoch 1854: train loss: 8.917494596971665e-06, val loss: 0.27842625975608826\n",
      "Epoch 1855: train loss: 1.4665697563032154e-05, val loss: 0.2783883512020111\n",
      "Epoch 1856: train loss: 1.922466981341131e-05, val loss: 0.2781827449798584\n",
      "Epoch 1857: train loss: 2.1338457372621633e-05, val loss: 0.27854210138320923\n",
      "Epoch 1858: train loss: 2.1513500541914254e-05, val loss: 0.27644291520118713\n",
      "Epoch 1859: train loss: 2.467570811859332e-05, val loss: 0.2774026095867157\n",
      "Epoch 1860: train loss: 3.090632890234701e-05, val loss: 0.2734929025173187\n",
      "Epoch 1861: train loss: 3.668150748126209e-05, val loss: 0.2752339243888855\n",
      "Epoch 1862: train loss: 3.6211771657690406e-05, val loss: 0.2720736861228943\n",
      "Epoch 1863: train loss: 3.170322088408284e-05, val loss: 0.2742428481578827\n",
      "Epoch 1864: train loss: 2.8097289032302797e-05, val loss: 0.2714118957519531\n",
      "Epoch 1865: train loss: 2.7266683900961652e-05, val loss: 0.2728007435798645\n",
      "Epoch 1866: train loss: 2.8967144316993654e-05, val loss: 0.2707752287387848\n",
      "Epoch 1867: train loss: 3.054057378903963e-05, val loss: 0.2727280259132385\n",
      "Epoch 1868: train loss: 3.1242620025295764e-05, val loss: 0.2702215015888214\n",
      "Epoch 1869: train loss: 3.105381256318651e-05, val loss: 0.27272364497184753\n",
      "Epoch 1870: train loss: 3.299294621683657e-05, val loss: 0.2694163918495178\n",
      "Epoch 1871: train loss: 3.8191934436326846e-05, val loss: 0.2729230523109436\n",
      "Epoch 1872: train loss: 4.6233752073021606e-05, val loss: 0.26876339316368103\n",
      "Epoch 1873: train loss: 5.506970046553761e-05, val loss: 0.2727830410003662\n",
      "Epoch 1874: train loss: 6.624657544307411e-05, val loss: 0.2684699296951294\n",
      "Epoch 1875: train loss: 8.18177722976543e-05, val loss: 0.27299007773399353\n",
      "Epoch 1876: train loss: 0.0001038587506627664, val loss: 0.2681286036968231\n",
      "Epoch 1877: train loss: 0.00013188373122829944, val loss: 0.27346813678741455\n",
      "Epoch 1878: train loss: 0.00016973490710370243, val loss: 0.2674502730369568\n",
      "Epoch 1879: train loss: 0.00022189469018485397, val loss: 0.27433764934539795\n",
      "Epoch 1880: train loss: 0.0002945678716059774, val loss: 0.26670780777931213\n",
      "Epoch 1881: train loss: 0.0003910635714419186, val loss: 0.2752271294593811\n",
      "Epoch 1882: train loss: 0.0005208359798416495, val loss: 0.26579123735427856\n",
      "Epoch 1883: train loss: 0.0006884308531880379, val loss: 0.2760797142982483\n",
      "Epoch 1884: train loss: 0.000897546939086169, val loss: 0.2630508542060852\n",
      "Epoch 1885: train loss: 0.0011380944633856416, val loss: 0.27555280923843384\n",
      "Epoch 1886: train loss: 0.0013709228951483965, val loss: 0.2605448365211487\n",
      "Epoch 1887: train loss: 0.0015544395428150892, val loss: 0.2746504247188568\n",
      "Epoch 1888: train loss: 0.0016110535943880677, val loss: 0.2589321732521057\n",
      "Epoch 1889: train loss: 0.0014665877679362893, val loss: 0.27112558484077454\n",
      "Epoch 1890: train loss: 0.00110617745667696, val loss: 0.26818007230758667\n",
      "Epoch 1891: train loss: 0.0009683797252364457, val loss: 0.27498653531074524\n",
      "Epoch 1892: train loss: 0.0010768703650683165, val loss: 0.2722480595111847\n",
      "Epoch 1893: train loss: 0.0011963789584115148, val loss: 0.26738858222961426\n",
      "Epoch 1894: train loss: 0.0010894229635596275, val loss: 0.26968103647232056\n",
      "Epoch 1895: train loss: 0.0006794357905164361, val loss: 0.26000234484672546\n",
      "Epoch 1896: train loss: 0.0002914787328336388, val loss: 0.2658885419368744\n",
      "Epoch 1897: train loss: 0.00026123158750124276, val loss: 0.2595047652721405\n",
      "Epoch 1898: train loss: 0.0004809164965990931, val loss: 0.26255884766578674\n",
      "Epoch 1899: train loss: 0.0005218035657890141, val loss: 0.25954127311706543\n",
      "Epoch 1900: train loss: 0.00024525803746655583, val loss: 0.25809258222579956\n",
      "Epoch 1901: train loss: 3.621886207838543e-05, val loss: 0.2600894570350647\n",
      "Epoch 1902: train loss: 0.00016951186989899725, val loss: 0.25561049580574036\n",
      "Epoch 1903: train loss: 0.0003578936157282442, val loss: 0.258638858795166\n",
      "Epoch 1904: train loss: 0.00026228243950754404, val loss: 0.2551417052745819\n",
      "Epoch 1905: train loss: 5.3367377404356375e-05, val loss: 0.2554081380367279\n",
      "Epoch 1906: train loss: 4.688681656261906e-05, val loss: 0.2541026175022125\n",
      "Epoch 1907: train loss: 0.0001672526414040476, val loss: 0.2521614730358124\n",
      "Epoch 1908: train loss: 0.00016102151130326092, val loss: 0.25358477234840393\n",
      "Epoch 1909: train loss: 6.497187860077247e-05, val loss: 0.24962583184242249\n",
      "Epoch 1910: train loss: 6.375303928507492e-05, val loss: 0.2511228024959564\n",
      "Epoch 1911: train loss: 0.00011397423804737628, val loss: 0.249684140086174\n",
      "Epoch 1912: train loss: 7.71086779423058e-05, val loss: 0.24875393509864807\n",
      "Epoch 1913: train loss: 1.6250278349616565e-05, val loss: 0.25040480494499207\n",
      "Epoch 1914: train loss: 4.6217151975724846e-05, val loss: 0.247625932097435\n",
      "Epoch 1915: train loss: 9.509475057711825e-05, val loss: 0.24902112782001495\n",
      "Epoch 1916: train loss: 6.1026232287986204e-05, val loss: 0.24752114713191986\n",
      "Epoch 1917: train loss: 7.4346216933918186e-06, val loss: 0.24703268706798553\n",
      "Epoch 1918: train loss: 2.1408382963272743e-05, val loss: 0.24743543565273285\n",
      "Epoch 1919: train loss: 5.344111195881851e-05, val loss: 0.24616184830665588\n",
      "Epoch 1920: train loss: 3.7876532587688416e-05, val loss: 0.24719147384166718\n",
      "Epoch 1921: train loss: 1.4447400644712616e-05, val loss: 0.24582865834236145\n",
      "Epoch 1922: train loss: 2.5728642867761664e-05, val loss: 0.24667806923389435\n",
      "Epoch 1923: train loss: 3.548490349203348e-05, val loss: 0.24601700901985168\n",
      "Epoch 1924: train loss: 1.5843468645471148e-05, val loss: 0.2455279380083084\n",
      "Epoch 1925: train loss: 2.4392500108660897e-06, val loss: 0.2460927814245224\n",
      "Epoch 1926: train loss: 1.777688339643646e-05, val loss: 0.24484501779079437\n",
      "Epoch 1927: train loss: 2.9390510462690145e-05, val loss: 0.2458016723394394\n",
      "Epoch 1928: train loss: 1.6235100702033378e-05, val loss: 0.24474544823169708\n",
      "Epoch 1929: train loss: 3.0869434795022244e-06, val loss: 0.245018869638443\n",
      "Epoch 1930: train loss: 8.196762792067602e-06, val loss: 0.24481849372386932\n",
      "Epoch 1931: train loss: 1.486444307374768e-05, val loss: 0.24406330287456512\n",
      "Epoch 1932: train loss: 9.295486051996704e-06, val loss: 0.24470320343971252\n",
      "Epoch 1933: train loss: 3.91771573049482e-06, val loss: 0.24380040168762207\n",
      "Epoch 1934: train loss: 8.672753210703377e-06, val loss: 0.24410422146320343\n",
      "Epoch 1935: train loss: 1.2624996088561602e-05, val loss: 0.24354016780853271\n",
      "Epoch 1936: train loss: 6.971165475988528e-06, val loss: 0.24380898475646973\n",
      "Epoch 1937: train loss: 7.221740361273987e-07, val loss: 0.243536576628685\n",
      "Epoch 1938: train loss: 2.4911382752179634e-06, val loss: 0.24330385029315948\n",
      "Epoch 1939: train loss: 6.9026395976834465e-06, val loss: 0.24356460571289062\n",
      "Epoch 1940: train loss: 6.290813871601131e-06, val loss: 0.24294458329677582\n",
      "Epoch 1941: train loss: 2.963989800264244e-06, val loss: 0.2432951182126999\n",
      "Epoch 1942: train loss: 2.847032192221377e-06, val loss: 0.2426256388425827\n",
      "Epoch 1943: train loss: 4.990597517462447e-06, val loss: 0.24321162700653076\n",
      "Epoch 1944: train loss: 4.547482149064308e-06, val loss: 0.24262316524982452\n",
      "Epoch 1945: train loss: 1.5264522517099977e-06, val loss: 0.24257831275463104\n",
      "Epoch 1946: train loss: 8.062434631028736e-08, val loss: 0.242745503783226\n",
      "Epoch 1947: train loss: 1.6272058473987272e-06, val loss: 0.24236944317817688\n",
      "Epoch 1948: train loss: 3.293436748208478e-06, val loss: 0.2426978200674057\n",
      "Epoch 1949: train loss: 2.776691417238908e-06, val loss: 0.24205799400806427\n",
      "Epoch 1950: train loss: 1.4126685528026428e-06, val loss: 0.24247021973133087\n",
      "Epoch 1951: train loss: 1.2425815612004953e-06, val loss: 0.24209485948085785\n",
      "Epoch 1952: train loss: 2.12521035791724e-06, val loss: 0.24234409630298615\n",
      "Epoch 1953: train loss: 2.447936594762723e-06, val loss: 0.2421513795852661\n",
      "Epoch 1954: train loss: 1.5671802202632534e-06, val loss: 0.2422884702682495\n",
      "Epoch 1955: train loss: 5.526990776161256e-07, val loss: 0.2422027885913849\n",
      "Epoch 1956: train loss: 3.4978424423570686e-07, val loss: 0.24224853515625\n",
      "Epoch 1957: train loss: 7.845516734050761e-07, val loss: 0.2421775609254837\n",
      "Epoch 1958: train loss: 1.0511445225347416e-06, val loss: 0.24224959313869476\n",
      "Epoch 1959: train loss: 7.699281923123635e-07, val loss: 0.242136150598526\n",
      "Epoch 1960: train loss: 3.558778871592949e-07, val loss: 0.2421235591173172\n",
      "Epoch 1961: train loss: 2.5242260903723945e-07, val loss: 0.2422904521226883\n",
      "Epoch 1962: train loss: 5.407403023127699e-07, val loss: 0.24203245341777802\n",
      "Epoch 1963: train loss: 9.374862770528125e-07, val loss: 0.24238447844982147\n",
      "Epoch 1964: train loss: 1.149515242104826e-06, val loss: 0.24192218482494354\n",
      "Epoch 1965: train loss: 1.2124168051741435e-06, val loss: 0.24245420098304749\n",
      "Epoch 1966: train loss: 1.3015003332839115e-06, val loss: 0.24190393090248108\n",
      "Epoch 1967: train loss: 1.6657837704769918e-06, val loss: 0.2424655258655548\n",
      "Epoch 1968: train loss: 2.4503326585545437e-06, val loss: 0.24187254905700684\n",
      "Epoch 1969: train loss: 3.808568635577103e-06, val loss: 0.2426074743270874\n",
      "Epoch 1970: train loss: 6.0982833929301705e-06, val loss: 0.24166083335876465\n",
      "Epoch 1971: train loss: 9.945059900928754e-06, val loss: 0.2429238110780716\n",
      "Epoch 1972: train loss: 1.6853109627845697e-05, val loss: 0.24122487008571625\n",
      "Epoch 1973: train loss: 2.9286384233273566e-05, val loss: 0.2437727451324463\n",
      "Epoch 1974: train loss: 5.352480729925446e-05, val loss: 0.24015508592128754\n",
      "Epoch 1975: train loss: 9.96536182356067e-05, val loss: 0.245548278093338\n",
      "Epoch 1976: train loss: 0.00019018584862351418, val loss: 0.2382996827363968\n",
      "Epoch 1977: train loss: 0.00036902420106343925, val loss: 0.24908331036567688\n",
      "Epoch 1978: train loss: 0.0007224372820928693, val loss: 0.23538099229335785\n",
      "Epoch 1979: train loss: 0.0013554829638451338, val loss: 0.25713491439819336\n",
      "Epoch 1980: train loss: 0.0024961114395409822, val loss: 0.23376083374023438\n",
      "Epoch 1981: train loss: 0.004419112112373114, val loss: 0.2688688337802887\n",
      "Epoch 1982: train loss: 0.007270525675266981, val loss: 0.22843106091022491\n",
      "Epoch 1983: train loss: 0.0100081916898489, val loss: 0.2904065251350403\n",
      "Epoch 1984: train loss: 0.011191518977284431, val loss: 0.22827644646167755\n",
      "Epoch 1985: train loss: 0.007951290346682072, val loss: 0.2550102174282074\n",
      "Epoch 1986: train loss: 0.002296850550919771, val loss: 0.23818789422512054\n",
      "Epoch 1987: train loss: 0.0006442161975428462, val loss: 0.21699872612953186\n",
      "Epoch 1988: train loss: 0.003688597120344639, val loss: 0.2448340207338333\n",
      "Epoch 1989: train loss: 0.003718300024047494, val loss: 0.2153448909521103\n",
      "Epoch 1990: train loss: 0.0005526720196940005, val loss: 0.21065115928649902\n",
      "Epoch 1991: train loss: 0.001155184698291123, val loss: 0.23754961788654327\n",
      "Epoch 1992: train loss: 0.0025499912444502115, val loss: 0.21066641807556152\n",
      "Epoch 1993: train loss: 0.0006219593342393637, val loss: 0.21097750961780548\n",
      "Epoch 1994: train loss: 0.000625203421805054, val loss: 0.23580074310302734\n",
      "Epoch 1995: train loss: 0.0017269366653636098, val loss: 0.21210792660713196\n",
      "Epoch 1996: train loss: 0.0002480346884112805, val loss: 0.20994757115840912\n",
      "Epoch 1997: train loss: 0.0006926261121407151, val loss: 0.23238025605678558\n",
      "Epoch 1998: train loss: 0.001055091037414968, val loss: 0.21697330474853516\n",
      "Epoch 1999: train loss: 0.00012631434947252274, val loss: 0.20599280297756195\n",
      "Epoch 2000: train loss: 0.0007028522086329758, val loss: 0.21712957322597504\n",
      "Epoch 2001: train loss: 0.0005044116405770183, val loss: 0.2095063179731369\n",
      "Epoch 2002: train loss: 0.00014176468539517373, val loss: 0.19948378205299377\n",
      "Epoch 2003: train loss: 0.000627727888058871, val loss: 0.20998330414295197\n",
      "Epoch 2004: train loss: 0.00011586557229747996, val loss: 0.20906352996826172\n",
      "Epoch 2005: train loss: 0.00031534620211459696, val loss: 0.19499611854553223\n",
      "Epoch 2006: train loss: 0.00034650665475055575, val loss: 0.19761012494564056\n",
      "Epoch 2007: train loss: 4.455146336113103e-05, val loss: 0.20428477227687836\n",
      "Epoch 2008: train loss: 0.0003631811705417931, val loss: 0.1974995732307434\n",
      "Epoch 2009: train loss: 8.234893903136253e-05, val loss: 0.1952294260263443\n",
      "Epoch 2010: train loss: 0.0001626396260689944, val loss: 0.19979624450206757\n",
      "Epoch 2011: train loss: 0.00021076745179016143, val loss: 0.1970447599887848\n",
      "Epoch 2012: train loss: 2.4104849217110313e-05, val loss: 0.19469940662384033\n",
      "Epoch 2013: train loss: 0.00020881368254777044, val loss: 0.1987829953432083\n",
      "Epoch 2014: train loss: 4.6876179112587124e-05, val loss: 0.19774660468101501\n",
      "Epoch 2015: train loss: 9.722995309857652e-05, val loss: 0.19224154949188232\n",
      "Epoch 2016: train loss: 0.0001213047798955813, val loss: 0.19472447037696838\n",
      "Epoch 2017: train loss: 1.5054061805130914e-05, val loss: 0.19854219257831573\n",
      "Epoch 2018: train loss: 0.00012300042726565152, val loss: 0.19441527128219604\n",
      "Epoch 2019: train loss: 2.3222557501867414e-05, val loss: 0.19278521835803986\n",
      "Epoch 2020: train loss: 6.320003012660891e-05, val loss: 0.19605599343776703\n",
      "Epoch 2021: train loss: 6.634594319621101e-05, val loss: 0.1941610872745514\n",
      "Epoch 2022: train loss: 1.1205536793568172e-05, val loss: 0.1911163181066513\n",
      "Epoch 2023: train loss: 7.38042508601211e-05, val loss: 0.19367550313472748\n",
      "Epoch 2024: train loss: 1.1834687029477209e-05, val loss: 0.19438819587230682\n",
      "Epoch 2025: train loss: 4.016101956949569e-05, val loss: 0.19099758565425873\n",
      "Epoch 2026: train loss: 3.666873089969158e-05, val loss: 0.19129423797130585\n",
      "Epoch 2027: train loss: 8.22197489469545e-06, val loss: 0.19330629706382751\n",
      "Epoch 2028: train loss: 4.3959204049315304e-05, val loss: 0.19160819053649902\n",
      "Epoch 2029: train loss: 5.699224857380614e-06, val loss: 0.19100366532802582\n",
      "Epoch 2030: train loss: 2.5076797101064585e-05, val loss: 0.192936971783638\n",
      "Epoch 2031: train loss: 2.1065161490696482e-05, val loss: 0.19219499826431274\n",
      "Epoch 2032: train loss: 5.11568123329198e-06, val loss: 0.1908046305179596\n",
      "Epoch 2033: train loss: 2.6201189029961824e-05, val loss: 0.19239108264446259\n",
      "Epoch 2034: train loss: 3.2990362797136186e-06, val loss: 0.192706897854805\n",
      "Epoch 2035: train loss: 1.5096671631908976e-05, val loss: 0.19063320755958557\n",
      "Epoch 2036: train loss: 1.2831234926125035e-05, val loss: 0.1910340040922165\n",
      "Epoch 2037: train loss: 2.8041140467394143e-06, val loss: 0.19228681921958923\n",
      "Epoch 2038: train loss: 1.5584377251798287e-05, val loss: 0.1909843534231186\n",
      "Epoch 2039: train loss: 2.307377826582524e-06, val loss: 0.19048380851745605\n",
      "Epoch 2040: train loss: 8.545475793653168e-06, val loss: 0.19168208539485931\n",
      "Epoch 2041: train loss: 8.009762495930772e-06, val loss: 0.1912095993757248\n",
      "Epoch 2042: train loss: 1.3968270877739997e-06, val loss: 0.19041608273983002\n",
      "Epoch 2043: train loss: 9.311840585723985e-06, val loss: 0.19149847328662872\n",
      "Epoch 2044: train loss: 1.805962256185012e-06, val loss: 0.19171622395515442\n",
      "Epoch 2045: train loss: 4.403926595841767e-06, val loss: 0.1904456913471222\n",
      "Epoch 2046: train loss: 5.371189217839856e-06, val loss: 0.19062046706676483\n",
      "Epoch 2047: train loss: 6.373289807015681e-07, val loss: 0.1913057565689087\n",
      "Epoch 2048: train loss: 5.355167559173424e-06, val loss: 0.1906398981809616\n",
      "Epoch 2049: train loss: 1.513761276328296e-06, val loss: 0.19053637981414795\n",
      "Epoch 2050: train loss: 2.088338305838988e-06, val loss: 0.19114375114440918\n",
      "Epoch 2051: train loss: 3.579128588171443e-06, val loss: 0.19057409465312958\n",
      "Epoch 2052: train loss: 3.187263359905046e-07, val loss: 0.1902005523443222\n",
      "Epoch 2053: train loss: 2.9384764275164343e-06, val loss: 0.1910068392753601\n",
      "Epoch 2054: train loss: 1.3847110267306562e-06, val loss: 0.19098003208637238\n",
      "Epoch 2055: train loss: 8.557153137189744e-07, val loss: 0.19032718241214752\n",
      "Epoch 2056: train loss: 2.3196955680759856e-06, val loss: 0.19070075452327728\n",
      "Epoch 2057: train loss: 3.144150468870066e-07, val loss: 0.19094714522361755\n",
      "Epoch 2058: train loss: 1.4108114783084602e-06, val loss: 0.19039426743984222\n",
      "Epoch 2059: train loss: 1.2127140962547855e-06, val loss: 0.1905205100774765\n",
      "Epoch 2060: train loss: 2.573093240698654e-07, val loss: 0.19091632962226868\n",
      "Epoch 2061: train loss: 1.3919307093601674e-06, val loss: 0.19050155580043793\n",
      "Epoch 2062: train loss: 3.895053168889717e-07, val loss: 0.19037948548793793\n",
      "Epoch 2063: train loss: 5.594064305114443e-07, val loss: 0.190757617354393\n",
      "Epoch 2064: train loss: 9.283431836593081e-07, val loss: 0.19056837260723114\n",
      "Epoch 2065: train loss: 1.0635965708161166e-07, val loss: 0.1904246211051941\n",
      "Epoch 2066: train loss: 7.185731192294043e-07, val loss: 0.19076398015022278\n",
      "Epoch 2067: train loss: 4.288103525595943e-07, val loss: 0.1905975341796875\n",
      "Epoch 2068: train loss: 1.6540288072519616e-07, val loss: 0.1902836710214615\n",
      "Epoch 2069: train loss: 6.070884523978748e-07, val loss: 0.19062277674674988\n",
      "Epoch 2070: train loss: 1.3427128919829556e-07, val loss: 0.19076700508594513\n",
      "Epoch 2071: train loss: 2.890288044454792e-07, val loss: 0.1904466599225998\n",
      "Epoch 2072: train loss: 3.861158859308489e-07, val loss: 0.1904740184545517\n",
      "Epoch 2073: train loss: 6.389385021066118e-08, val loss: 0.1905970573425293\n",
      "Epoch 2074: train loss: 3.114384412583604e-07, val loss: 0.19046065211296082\n",
      "Epoch 2075: train loss: 1.9288006569695426e-07, val loss: 0.1905563324689865\n",
      "Epoch 2076: train loss: 7.689310166369978e-08, val loss: 0.19065940380096436\n",
      "Epoch 2077: train loss: 2.657800166616653e-07, val loss: 0.19041259586811066\n",
      "Epoch 2078: train loss: 7.623771836051674e-08, val loss: 0.19040213525295258\n",
      "Epoch 2079: train loss: 1.0363832103621462e-07, val loss: 0.1906050145626068\n",
      "Epoch 2080: train loss: 1.9128371775423147e-07, val loss: 0.19049207866191864\n",
      "Epoch 2081: train loss: 2.557428402383266e-08, val loss: 0.1904381811618805\n",
      "Epoch 2082: train loss: 1.2216493416872254e-07, val loss: 0.1906070113182068\n",
      "Epoch 2083: train loss: 1.1945466837914864e-07, val loss: 0.1905367374420166\n",
      "Epoch 2084: train loss: 1.5039887557577458e-08, val loss: 0.1904265433549881\n",
      "Epoch 2085: train loss: 1.1228340923707947e-07, val loss: 0.19052289426326752\n",
      "Epoch 2086: train loss: 6.797397134050698e-08, val loss: 0.19048474729061127\n",
      "Epoch 2087: train loss: 1.4832290062827269e-08, val loss: 0.1904146522283554\n",
      "Epoch 2088: train loss: 9.430888070482979e-08, val loss: 0.1905265599489212\n",
      "Epoch 2089: train loss: 3.453722285939875e-08, val loss: 0.19047795236110687\n",
      "Epoch 2090: train loss: 2.1459955590330537e-08, val loss: 0.19035866856575012\n",
      "Epoch 2091: train loss: 7.089025899631451e-08, val loss: 0.1904667615890503\n",
      "Epoch 2092: train loss: 1.956494166677203e-08, val loss: 0.19048629701137543\n",
      "Epoch 2093: train loss: 2.539639787357828e-08, val loss: 0.19036704301834106\n",
      "Epoch 2094: train loss: 5.030981142795099e-08, val loss: 0.19039778411388397\n",
      "Epoch 2095: train loss: 1.1858295678734976e-08, val loss: 0.19040925800800323\n",
      "Epoch 2096: train loss: 2.3828484074783773e-08, val loss: 0.19032108783721924\n",
      "Epoch 2097: train loss: 3.3687673095528226e-08, val loss: 0.19034305214881897\n",
      "Epoch 2098: train loss: 8.873269230491587e-09, val loss: 0.1903492659330368\n",
      "Epoch 2099: train loss: 2.206543392446747e-08, val loss: 0.19026604294776917\n",
      "Epoch 2100: train loss: 2.308951962959327e-08, val loss: 0.1902838945388794\n",
      "Epoch 2101: train loss: 6.135234986714977e-09, val loss: 0.1902984380722046\n",
      "Epoch 2102: train loss: 1.895634405002511e-08, val loss: 0.1902237981557846\n",
      "Epoch 2103: train loss: 1.7489915293822378e-08, val loss: 0.19022724032402039\n",
      "Epoch 2104: train loss: 4.245477303044254e-09, val loss: 0.19023728370666504\n",
      "Epoch 2105: train loss: 1.545474503927835e-08, val loss: 0.19016633927822113\n",
      "Epoch 2106: train loss: 1.3931488851426366e-08, val loss: 0.19015014171600342\n",
      "Epoch 2107: train loss: 3.4494380685146098e-09, val loss: 0.19014881551265717\n",
      "Epoch 2108: train loss: 1.1636197783104762e-08, val loss: 0.19009684026241302\n",
      "Epoch 2109: train loss: 1.1795628473976194e-08, val loss: 0.19008997082710266\n",
      "Epoch 2110: train loss: 3.4268941018211763e-09, val loss: 0.19008147716522217\n",
      "Epoch 2111: train loss: 8.138792750855828e-09, val loss: 0.19002573192119598\n",
      "Epoch 2112: train loss: 9.309056636652713e-09, val loss: 0.19001376628875732\n",
      "Epoch 2113: train loss: 4.09390121802744e-09, val loss: 0.19000189006328583\n",
      "Epoch 2114: train loss: 6.9959296133959015e-09, val loss: 0.18994884192943573\n",
      "Epoch 2115: train loss: 8.063973488958709e-09, val loss: 0.18994173407554626\n",
      "Epoch 2116: train loss: 3.897760780802173e-09, val loss: 0.18992768228054047\n",
      "Epoch 2117: train loss: 6.0219904618463715e-09, val loss: 0.18988077342510223\n",
      "Epoch 2118: train loss: 7.210426034021111e-09, val loss: 0.18987563252449036\n",
      "Epoch 2119: train loss: 4.186694102514821e-09, val loss: 0.18985512852668762\n",
      "Epoch 2120: train loss: 5.955792747869282e-09, val loss: 0.18980573117733002\n",
      "Epoch 2121: train loss: 9.516619492444534e-09, val loss: 0.18979988992214203\n",
      "Epoch 2122: train loss: 1.0065718036855742e-08, val loss: 0.1897723227739334\n",
      "Epoch 2123: train loss: 1.655888937079908e-08, val loss: 0.1897222399711609\n",
      "Epoch 2124: train loss: 3.3178057634586366e-08, val loss: 0.1897105723619461\n",
      "Epoch 2125: train loss: 7.057786888253759e-08, val loss: 0.18967556953430176\n",
      "Epoch 2126: train loss: 1.644649358922834e-07, val loss: 0.1896466165781021\n",
      "Epoch 2127: train loss: 4.1924110405489046e-07, val loss: 0.18961267173290253\n",
      "Epoch 2128: train loss: 1.0937255865428597e-06, val loss: 0.18960683047771454\n",
      "Epoch 2129: train loss: 2.990019993376336e-06, val loss: 0.18951201438903809\n",
      "Epoch 2130: train loss: 8.34828369988827e-06, val loss: 0.18960636854171753\n",
      "Epoch 2131: train loss: 2.36568484979216e-05, val loss: 0.1893531084060669\n",
      "Epoch 2132: train loss: 6.3657971622888e-05, val loss: 0.18956373631954193\n",
      "Epoch 2133: train loss: 0.00015889042697381228, val loss: 0.18956217169761658\n",
      "Epoch 2134: train loss: 0.00038748435326851904, val loss: 0.18909744918346405\n",
      "Epoch 2135: train loss: 0.0009185996605083346, val loss: 0.18997876346111298\n",
      "Epoch 2136: train loss: 0.001837218995206058, val loss: 0.18968723714351654\n",
      "Epoch 2137: train loss: 0.0027344038244336843, val loss: 0.19006627798080444\n",
      "Epoch 2138: train loss: 0.002439129166305065, val loss: 0.18569841980934143\n",
      "Epoch 2139: train loss: 0.0010139156365767121, val loss: 0.18637710809707642\n",
      "Epoch 2140: train loss: 4.7967238060664386e-05, val loss: 0.19287395477294922\n",
      "Epoch 2141: train loss: 0.0005385060794651508, val loss: 0.2027723789215088\n",
      "Epoch 2142: train loss: 0.0012396542588248849, val loss: 0.20389239490032196\n",
      "Epoch 2143: train loss: 0.0007886458770371974, val loss: 0.20445211231708527\n",
      "Epoch 2144: train loss: 7.35577559680678e-05, val loss: 0.20391802489757538\n",
      "Epoch 2145: train loss: 0.0003210653376299888, val loss: 0.20405149459838867\n",
      "Epoch 2146: train loss: 0.0007277561817318201, val loss: 0.2029477208852768\n",
      "Epoch 2147: train loss: 0.0003402699367143214, val loss: 0.20187890529632568\n",
      "Epoch 2148: train loss: 2.6523659471422434e-05, val loss: 0.20234933495521545\n",
      "Epoch 2149: train loss: 0.00036174821434542537, val loss: 0.2015460580587387\n",
      "Epoch 2150: train loss: 0.0004131667083129287, val loss: 0.19964638352394104\n",
      "Epoch 2151: train loss: 5.174556645215489e-05, val loss: 0.19797201454639435\n",
      "Epoch 2152: train loss: 0.00011676699068630114, val loss: 0.19652298092842102\n",
      "Epoch 2153: train loss: 0.00032636732794344425, val loss: 0.19482718408107758\n",
      "Epoch 2154: train loss: 0.0001197187157231383, val loss: 0.19344781339168549\n",
      "Epoch 2155: train loss: 1.8365337382419966e-05, val loss: 0.19258500635623932\n",
      "Epoch 2156: train loss: 0.0002041443804046139, val loss: 0.19173268973827362\n",
      "Epoch 2157: train loss: 0.00014652979734819382, val loss: 0.1912287473678589\n",
      "Epoch 2158: train loss: 1.8101784462487558e-06, val loss: 0.19091972708702087\n",
      "Epoch 2159: train loss: 0.00010826798825291917, val loss: 0.19051288068294525\n",
      "Epoch 2160: train loss: 0.00013458477042149752, val loss: 0.19024768471717834\n",
      "Epoch 2161: train loss: 1.3572650459536817e-05, val loss: 0.1898382157087326\n",
      "Epoch 2162: train loss: 5.1310169510543346e-05, val loss: 0.18943330645561218\n",
      "Epoch 2163: train loss: 0.00010478682816028595, val loss: 0.18943725526332855\n",
      "Epoch 2164: train loss: 2.398666947556194e-05, val loss: 0.18933425843715668\n",
      "Epoch 2165: train loss: 2.2311462089419365e-05, val loss: 0.1891404241323471\n",
      "Epoch 2166: train loss: 7.630647451151162e-05, val loss: 0.18930605053901672\n",
      "Epoch 2167: train loss: 2.9603123039123602e-05, val loss: 0.18916209042072296\n",
      "Epoch 2168: train loss: 7.619531970703974e-06, val loss: 0.1888851672410965\n",
      "Epoch 2169: train loss: 5.1953458751086146e-05, val loss: 0.18908903002738953\n",
      "Epoch 2170: train loss: 3.1892061088001356e-05, val loss: 0.1888517141342163\n",
      "Epoch 2171: train loss: 1.362492298540019e-06, val loss: 0.18853342533111572\n",
      "Epoch 2172: train loss: 3.172802462358959e-05, val loss: 0.1886884570121765\n",
      "Epoch 2173: train loss: 3.170116906403564e-05, val loss: 0.18852131068706512\n",
      "Epoch 2174: train loss: 1.7687729041426792e-06, val loss: 0.18853648006916046\n",
      "Epoch 2175: train loss: 1.5665096725570038e-05, val loss: 0.18865633010864258\n",
      "Epoch 2176: train loss: 2.7052885343437083e-05, val loss: 0.1885589361190796\n",
      "Epoch 2177: train loss: 5.344137662177673e-06, val loss: 0.18865655362606049\n",
      "Epoch 2178: train loss: 6.016385214024922e-06, val loss: 0.18853960931301117\n",
      "Epoch 2179: train loss: 1.9891056581400335e-05, val loss: 0.18837319314479828\n",
      "Epoch 2180: train loss: 8.501812772010453e-06, val loss: 0.18846245110034943\n",
      "Epoch 2181: train loss: 1.929895006469451e-06, val loss: 0.18839815258979797\n",
      "Epoch 2182: train loss: 1.2730725757137407e-05, val loss: 0.18840794265270233\n",
      "Epoch 2183: train loss: 9.671173756942153e-06, val loss: 0.18857614696025848\n",
      "Epoch 2184: train loss: 8.278175869236293e-07, val loss: 0.18843446671962738\n",
      "Epoch 2185: train loss: 6.741621291439515e-06, val loss: 0.18852029740810394\n",
      "Epoch 2186: train loss: 9.34331001190003e-06, val loss: 0.1885768473148346\n",
      "Epoch 2187: train loss: 1.7183980389745557e-06, val loss: 0.18846498429775238\n",
      "Epoch 2188: train loss: 2.222492412329302e-06, val loss: 0.18848367035388947\n",
      "Epoch 2189: train loss: 7.245922461152077e-06, val loss: 0.18844269216060638\n",
      "Epoch 2190: train loss: 3.5633872812468326e-06, val loss: 0.18846087157726288\n",
      "Epoch 2191: train loss: 2.9371747700679407e-07, val loss: 0.18856385350227356\n",
      "Epoch 2192: train loss: 3.939450380130438e-06, val loss: 0.18855071067810059\n",
      "Epoch 2193: train loss: 4.348857601144118e-06, val loss: 0.18859201669692993\n",
      "Epoch 2194: train loss: 7.022200634310138e-07, val loss: 0.18859568238258362\n",
      "Epoch 2195: train loss: 1.3858989404980093e-06, val loss: 0.1885405033826828\n",
      "Epoch 2196: train loss: 3.5703196772374213e-06, val loss: 0.18858492374420166\n",
      "Epoch 2197: train loss: 1.7290687992499443e-06, val loss: 0.18857614696025848\n",
      "Epoch 2198: train loss: 4.0577330651103694e-07, val loss: 0.18863673508167267\n",
      "Epoch 2199: train loss: 2.422269062662963e-06, val loss: 0.1886114627122879\n",
      "Epoch 2200: train loss: 3.3043195344362175e-06, val loss: 0.18871529400348663\n",
      "Epoch 2201: train loss: 3.093572559009772e-06, val loss: 0.18850290775299072\n",
      "Epoch 2202: train loss: 7.597142030135728e-06, val loss: 0.18882536888122559\n",
      "Epoch 2203: train loss: 2.1380932594183832e-05, val loss: 0.1883794665336609\n",
      "Epoch 2204: train loss: 5.341707947081886e-05, val loss: 0.18858210742473602\n",
      "Epoch 2205: train loss: 0.00013148084690328687, val loss: 0.18828025460243225\n",
      "Epoch 2206: train loss: 0.0002381386875640601, val loss: 0.18603162467479706\n",
      "Epoch 2207: train loss: 0.0002712968271225691, val loss: 0.18508028984069824\n",
      "Epoch 2208: train loss: 0.0001562518154969439, val loss: 0.18608562648296356\n",
      "Epoch 2209: train loss: 1.9804310795734636e-05, val loss: 0.18696145713329315\n",
      "Epoch 2210: train loss: 2.8635344278882258e-05, val loss: 0.1857081651687622\n",
      "Epoch 2211: train loss: 0.00011913709749933332, val loss: 0.18610072135925293\n",
      "Epoch 2212: train loss: 0.00011933564383070916, val loss: 0.18628989160060883\n",
      "Epoch 2213: train loss: 3.455883779679425e-05, val loss: 0.18626467883586884\n",
      "Epoch 2214: train loss: 6.336943442875054e-06, val loss: 0.18688656389713287\n",
      "Epoch 2215: train loss: 6.111517723184079e-05, val loss: 0.18695484101772308\n",
      "Epoch 2216: train loss: 8.180215081665665e-05, val loss: 0.1871819943189621\n",
      "Epoch 2217: train loss: 2.9816976166330278e-05, val loss: 0.1872415542602539\n",
      "Epoch 2218: train loss: 1.1890761015820317e-06, val loss: 0.18754282593727112\n",
      "Epoch 2219: train loss: 3.505817585391924e-05, val loss: 0.18791387975215912\n",
      "Epoch 2220: train loss: 5.565159153775312e-05, val loss: 0.18778134882450104\n",
      "Epoch 2221: train loss: 2.463945929775946e-05, val loss: 0.18818704783916473\n",
      "Epoch 2222: train loss: 7.81205869770929e-07, val loss: 0.18829788267612457\n",
      "Epoch 2223: train loss: 1.9622551917564124e-05, val loss: 0.18815697729587555\n",
      "Epoch 2224: train loss: 3.631761865108274e-05, val loss: 0.18815463781356812\n",
      "Epoch 2225: train loss: 1.9288187104393728e-05, val loss: 0.18808613717556\n",
      "Epoch 2226: train loss: 1.0768454785647918e-06, val loss: 0.18818458914756775\n",
      "Epoch 2227: train loss: 1.004174100671662e-05, val loss: 0.18822364509105682\n",
      "Epoch 2228: train loss: 2.3687724024057388e-05, val loss: 0.18816819787025452\n",
      "Epoch 2229: train loss: 1.600001814949792e-05, val loss: 0.1883656084537506\n",
      "Epoch 2230: train loss: 1.838099365158996e-06, val loss: 0.1884143203496933\n",
      "Epoch 2231: train loss: 4.321186224842677e-06, val loss: 0.18844273686408997\n",
      "Epoch 2232: train loss: 1.4494159586320166e-05, val loss: 0.1885639876127243\n",
      "Epoch 2233: train loss: 1.2785612852894701e-05, val loss: 0.1884014755487442\n",
      "Epoch 2234: train loss: 3.1320398647949332e-06, val loss: 0.1885046511888504\n",
      "Epoch 2235: train loss: 1.357949486191501e-06, val loss: 0.18849536776542664\n",
      "Epoch 2236: train loss: 7.534847554779844e-06, val loss: 0.18838568031787872\n",
      "Epoch 2237: train loss: 9.598687938705552e-06, val loss: 0.18849511444568634\n",
      "Epoch 2238: train loss: 4.40946314483881e-06, val loss: 0.18857704102993011\n",
      "Epoch 2239: train loss: 5.681642392119102e-07, val loss: 0.18860138952732086\n",
      "Epoch 2240: train loss: 2.9426598757709144e-06, val loss: 0.1886238306760788\n",
      "Epoch 2241: train loss: 6.252597813727334e-06, val loss: 0.18852460384368896\n",
      "Epoch 2242: train loss: 4.925756456941599e-06, val loss: 0.1884751319885254\n",
      "Epoch 2243: train loss: 1.33585069761466e-06, val loss: 0.18848471343517303\n",
      "Epoch 2244: train loss: 7.708468388045731e-07, val loss: 0.188657745718956\n",
      "Epoch 2245: train loss: 3.2834741432452574e-06, val loss: 0.18858924508094788\n",
      "Epoch 2246: train loss: 4.6269401536847e-06, val loss: 0.18864251673221588\n",
      "Epoch 2247: train loss: 3.43775218425435e-06, val loss: 0.18860112130641937\n",
      "Epoch 2248: train loss: 2.5277022359659895e-06, val loss: 0.18863984942436218\n",
      "Epoch 2249: train loss: 4.943943622492952e-06, val loss: 0.18847940862178802\n",
      "Epoch 2250: train loss: 1.107148182200035e-05, val loss: 0.18885456025600433\n",
      "Epoch 2251: train loss: 2.203466101491358e-05, val loss: 0.1884497106075287\n",
      "Epoch 2252: train loss: 4.456928218132816e-05, val loss: 0.1889406144618988\n",
      "Epoch 2253: train loss: 8.634939877083525e-05, val loss: 0.18799090385437012\n",
      "Epoch 2254: train loss: 0.00015141887706704438, val loss: 0.18879397213459015\n",
      "Epoch 2255: train loss: 0.0002403783000772819, val loss: 0.18799786269664764\n",
      "Epoch 2256: train loss: 0.0003317768278066069, val loss: 0.18790049850940704\n",
      "Epoch 2257: train loss: 0.0003665958938654512, val loss: 0.18723617494106293\n",
      "Epoch 2258: train loss: 0.00028781453147530556, val loss: 0.18676139414310455\n",
      "Epoch 2259: train loss: 0.0001518239441793412, val loss: 0.1859918087720871\n",
      "Epoch 2260: train loss: 5.644655175274238e-05, val loss: 0.18589608371257782\n",
      "Epoch 2261: train loss: 5.608722858596593e-05, val loss: 0.1845189332962036\n",
      "Epoch 2262: train loss: 9.517740545561537e-05, val loss: 0.185105562210083\n",
      "Epoch 2263: train loss: 9.471789962844923e-05, val loss: 0.18534807860851288\n",
      "Epoch 2264: train loss: 6.833540101069957e-05, val loss: 0.18480709195137024\n",
      "Epoch 2265: train loss: 5.79655788897071e-05, val loss: 0.18613028526306152\n",
      "Epoch 2266: train loss: 6.938043952686712e-05, val loss: 0.18436181545257568\n",
      "Epoch 2267: train loss: 8.041619730647653e-05, val loss: 0.1861274540424347\n",
      "Epoch 2268: train loss: 5.5643453379161656e-05, val loss: 0.18438498675823212\n",
      "Epoch 2269: train loss: 1.4019476111570839e-05, val loss: 0.18517474830150604\n",
      "Epoch 2270: train loss: 5.4507117965840735e-06, val loss: 0.18632999062538147\n",
      "Epoch 2271: train loss: 3.006832230312284e-05, val loss: 0.18483620882034302\n",
      "Epoch 2272: train loss: 4.503947639022954e-05, val loss: 0.18720240890979767\n",
      "Epoch 2273: train loss: 3.808744440902956e-05, val loss: 0.1862555891275406\n",
      "Epoch 2274: train loss: 2.8531036150525324e-05, val loss: 0.1873483955860138\n",
      "Epoch 2275: train loss: 2.5836890927166678e-05, val loss: 0.18715381622314453\n",
      "Epoch 2276: train loss: 1.9243478163843974e-05, val loss: 0.186824232339859\n",
      "Epoch 2277: train loss: 7.96990207163617e-06, val loss: 0.18698392808437347\n",
      "Epoch 2278: train loss: 4.990454726794269e-06, val loss: 0.18672409653663635\n",
      "Epoch 2279: train loss: 1.5357003576355055e-05, val loss: 0.18693779408931732\n",
      "Epoch 2280: train loss: 2.293695433763787e-05, val loss: 0.18629519641399384\n",
      "Epoch 2281: train loss: 1.950641490111593e-05, val loss: 0.18666572868824005\n",
      "Epoch 2282: train loss: 1.2865363714809064e-05, val loss: 0.186137393116951\n",
      "Epoch 2283: train loss: 1.1095907211711165e-05, val loss: 0.18656869232654572\n",
      "Epoch 2284: train loss: 9.919497642840724e-06, val loss: 0.18597324192523956\n",
      "Epoch 2285: train loss: 5.606473223451758e-06, val loss: 0.18623597919940948\n",
      "Epoch 2286: train loss: 2.2778144739277195e-06, val loss: 0.18627306818962097\n",
      "Epoch 2287: train loss: 4.081240149389487e-06, val loss: 0.18548505008220673\n",
      "Epoch 2288: train loss: 8.39553013065597e-06, val loss: 0.1863556206226349\n",
      "Epoch 2289: train loss: 9.789914656721521e-06, val loss: 0.1856476217508316\n",
      "Epoch 2290: train loss: 8.490177606290672e-06, val loss: 0.18581178784370422\n",
      "Epoch 2291: train loss: 7.64193737268215e-06, val loss: 0.18553279340267181\n",
      "Epoch 2292: train loss: 8.242459443863481e-06, val loss: 0.18568430840969086\n",
      "Epoch 2293: train loss: 7.945212928461842e-06, val loss: 0.18547223508358002\n",
      "Epoch 2294: train loss: 5.728313226427417e-06, val loss: 0.1855372041463852\n",
      "Epoch 2295: train loss: 3.7103498016222147e-06, val loss: 0.18529252707958221\n",
      "Epoch 2296: train loss: 3.261854999436764e-06, val loss: 0.18534530699253082\n",
      "Epoch 2297: train loss: 3.822400685749017e-06, val loss: 0.18505766987800598\n",
      "Epoch 2298: train loss: 3.873935384035576e-06, val loss: 0.1850689947605133\n",
      "Epoch 2299: train loss: 3.180919065925991e-06, val loss: 0.18519338965415955\n",
      "Epoch 2300: train loss: 2.8042381927662063e-06, val loss: 0.18481461703777313\n",
      "Epoch 2301: train loss: 3.3016335692082066e-06, val loss: 0.18495787680149078\n",
      "Epoch 2302: train loss: 3.932581876142649e-06, val loss: 0.1845971792936325\n",
      "Epoch 2303: train loss: 4.320774223742774e-06, val loss: 0.1847277730703354\n",
      "Epoch 2304: train loss: 4.55091503681615e-06, val loss: 0.18453490734100342\n",
      "Epoch 2305: train loss: 5.328922270564362e-06, val loss: 0.1844344586133957\n",
      "Epoch 2306: train loss: 7.120618192857364e-06, val loss: 0.1844361126422882\n",
      "Epoch 2307: train loss: 1.0452121387061197e-05, val loss: 0.18425725400447845\n",
      "Epoch 2308: train loss: 1.5965331840561703e-05, val loss: 0.18420608341693878\n",
      "Epoch 2309: train loss: 2.578423845989164e-05, val loss: 0.1840054839849472\n",
      "Epoch 2310: train loss: 4.420970435603522e-05, val loss: 0.184064581990242\n",
      "Epoch 2311: train loss: 7.971488230396062e-05, val loss: 0.1836882084608078\n",
      "Epoch 2312: train loss: 0.00014880370872560889, val loss: 0.18415617942810059\n",
      "Epoch 2313: train loss: 0.0002860665263142437, val loss: 0.18319933116436005\n",
      "Epoch 2314: train loss: 0.0005597142735496163, val loss: 0.1845751255750656\n",
      "Epoch 2315: train loss: 0.0010998727520927787, val loss: 0.18215346336364746\n",
      "Epoch 2316: train loss: 0.0020113505888730288, val loss: 0.18496809899806976\n",
      "Epoch 2317: train loss: 0.0030580104794353247, val loss: 0.17932657897472382\n",
      "Epoch 2318: train loss: 0.0036640558391809464, val loss: 0.18368160724639893\n",
      "Epoch 2319: train loss: 0.003190383082255721, val loss: 0.18027658760547638\n",
      "Epoch 2320: train loss: 0.0019006779184564948, val loss: 0.18180786073207855\n",
      "Epoch 2321: train loss: 0.0012792617781087756, val loss: 0.178054079413414\n",
      "Epoch 2322: train loss: 0.0019016147125512362, val loss: 0.1805136650800705\n",
      "Epoch 2323: train loss: 0.0024847364984452724, val loss: 0.17473259568214417\n",
      "Epoch 2324: train loss: 0.001743842731229961, val loss: 0.17859552800655365\n",
      "Epoch 2325: train loss: 0.0005810798611491919, val loss: 0.17307573556900024\n",
      "Epoch 2326: train loss: 0.0006115915020927787, val loss: 0.17392025887966156\n",
      "Epoch 2327: train loss: 0.0011535001685842872, val loss: 0.17370791733264923\n",
      "Epoch 2328: train loss: 0.0007822356419637799, val loss: 0.17034216225147247\n",
      "Epoch 2329: train loss: 0.00022492077550850809, val loss: 0.17221079766750336\n",
      "Epoch 2330: train loss: 0.0005133382510393858, val loss: 0.16967728734016418\n",
      "Epoch 2331: train loss: 0.000727869279216975, val loss: 0.16966386139392853\n",
      "Epoch 2332: train loss: 0.0003418113337829709, val loss: 0.16997994482517242\n",
      "Epoch 2333: train loss: 0.00018655741587281227, val loss: 0.16785606741905212\n",
      "Epoch 2334: train loss: 0.00036074299714528024, val loss: 0.16851533949375153\n",
      "Epoch 2335: train loss: 0.000330237002344802, val loss: 0.16803374886512756\n",
      "Epoch 2336: train loss: 0.00022292957874014974, val loss: 0.16656504571437836\n",
      "Epoch 2337: train loss: 0.00020614376990124583, val loss: 0.16636407375335693\n",
      "Epoch 2338: train loss: 0.0001740534498821944, val loss: 0.16702213883399963\n",
      "Epoch 2339: train loss: 0.00017577610560692847, val loss: 0.1663225144147873\n",
      "Epoch 2340: train loss: 0.00017427990678697824, val loss: 0.16542868316173553\n",
      "Epoch 2341: train loss: 0.00011960247502429411, val loss: 0.16591627895832062\n",
      "Epoch 2342: train loss: 0.0001248411281267181, val loss: 0.1648533046245575\n",
      "Epoch 2343: train loss: 0.00011619432916631922, val loss: 0.1647934913635254\n",
      "Epoch 2344: train loss: 7.463672227459028e-05, val loss: 0.16489605605602264\n",
      "Epoch 2345: train loss: 0.00010691989155020565, val loss: 0.16347149014472961\n",
      "Epoch 2346: train loss: 9.612023131921887e-05, val loss: 0.16448955237865448\n",
      "Epoch 2347: train loss: 3.4853143006330356e-05, val loss: 0.16466523706912994\n",
      "Epoch 2348: train loss: 7.042146171443164e-05, val loss: 0.16302768886089325\n",
      "Epoch 2349: train loss: 8.456321666017175e-05, val loss: 0.16383545100688934\n",
      "Epoch 2350: train loss: 2.893995406338945e-05, val loss: 0.16404950618743896\n",
      "Epoch 2351: train loss: 4.4800031901104376e-05, val loss: 0.1634702980518341\n",
      "Epoch 2352: train loss: 5.8036279369844124e-05, val loss: 0.16390322148799896\n",
      "Epoch 2353: train loss: 2.203002804890275e-05, val loss: 0.163480743765831\n",
      "Epoch 2354: train loss: 3.7742938729934394e-05, val loss: 0.16337673366069794\n",
      "Epoch 2355: train loss: 4.366205757833086e-05, val loss: 0.16369302570819855\n",
      "Epoch 2356: train loss: 1.0277713954565115e-05, val loss: 0.16363251209259033\n",
      "Epoch 2357: train loss: 2.6205805625068024e-05, val loss: 0.16371004283428192\n",
      "Epoch 2358: train loss: 3.657063643913716e-05, val loss: 0.16345073282718658\n",
      "Epoch 2359: train loss: 9.219216735800728e-06, val loss: 0.16359646618366241\n",
      "Epoch 2360: train loss: 1.6201071048271842e-05, val loss: 0.16377514600753784\n",
      "Epoch 2361: train loss: 2.4706228941795416e-05, val loss: 0.16354705393314362\n",
      "Epoch 2362: train loss: 9.031625268107746e-06, val loss: 0.16381505131721497\n",
      "Epoch 2363: train loss: 1.3335003131942358e-05, val loss: 0.16385827958583832\n",
      "Epoch 2364: train loss: 1.784422420314513e-05, val loss: 0.16379986703395844\n",
      "Epoch 2365: train loss: 5.935925400990527e-06, val loss: 0.16402779519557953\n",
      "Epoch 2366: train loss: 8.303511094709393e-06, val loss: 0.16409613192081451\n",
      "Epoch 2367: train loss: 1.4043171177036129e-05, val loss: 0.1641494780778885\n",
      "Epoch 2368: train loss: 6.960056907701073e-06, val loss: 0.1643640697002411\n",
      "Epoch 2369: train loss: 4.966447704646271e-06, val loss: 0.1645820587873459\n",
      "Epoch 2370: train loss: 7.974822437972762e-06, val loss: 0.16451776027679443\n",
      "Epoch 2371: train loss: 6.347071575873997e-06, val loss: 0.16494260728359222\n",
      "Epoch 2372: train loss: 5.128322754899273e-06, val loss: 0.1653238832950592\n",
      "Epoch 2373: train loss: 5.2703398978337646e-06, val loss: 0.16486434638500214\n",
      "Epoch 2374: train loss: 4.362385425338289e-06, val loss: 0.1652253121137619\n",
      "Epoch 2375: train loss: 3.7557010728050955e-06, val loss: 0.16526488959789276\n",
      "Epoch 2376: train loss: 3.548100494299433e-06, val loss: 0.16470937430858612\n",
      "Epoch 2377: train loss: 3.6018846003571525e-06, val loss: 0.16522307693958282\n",
      "Epoch 2378: train loss: 3.598414423322538e-06, val loss: 0.16512595117092133\n",
      "Epoch 2379: train loss: 2.3515979137300747e-06, val loss: 0.16493122279644012\n",
      "Epoch 2380: train loss: 1.7017595155266463e-06, val loss: 0.16531364619731903\n",
      "Epoch 2381: train loss: 2.697908939808258e-06, val loss: 0.16498465836048126\n",
      "Epoch 2382: train loss: 2.5930526135198306e-06, val loss: 0.16508889198303223\n",
      "Epoch 2383: train loss: 1.471861764912319e-06, val loss: 0.16528239846229553\n",
      "Epoch 2384: train loss: 1.5091986824700143e-06, val loss: 0.1650303602218628\n",
      "Epoch 2385: train loss: 1.7808928305385052e-06, val loss: 0.16516494750976562\n",
      "Epoch 2386: train loss: 1.1806150723714381e-06, val loss: 0.1654844433069229\n",
      "Epoch 2387: train loss: 1.0596053243716597e-06, val loss: 0.16552536189556122\n",
      "Epoch 2388: train loss: 1.5155148958001519e-06, val loss: 0.16567932069301605\n",
      "Epoch 2389: train loss: 1.1546516134330886e-06, val loss: 0.16601155698299408\n",
      "Epoch 2390: train loss: 6.064527156013355e-07, val loss: 0.1659517139196396\n",
      "Epoch 2391: train loss: 8.270990861092287e-07, val loss: 0.16597168147563934\n",
      "Epoch 2392: train loss: 9.333138564215915e-07, val loss: 0.1659799963235855\n",
      "Epoch 2393: train loss: 5.749589035985991e-07, val loss: 0.16576236486434937\n",
      "Epoch 2394: train loss: 6.040544349161792e-07, val loss: 0.16587547957897186\n",
      "Epoch 2395: train loss: 8.139670626405859e-07, val loss: 0.16581915318965912\n",
      "Epoch 2396: train loss: 5.623884362648823e-07, val loss: 0.16570162773132324\n",
      "Epoch 2397: train loss: 2.6812625719685457e-07, val loss: 0.16572032868862152\n",
      "Epoch 2398: train loss: 3.572783100480592e-07, val loss: 0.16562393307685852\n",
      "Epoch 2399: train loss: 4.7593968588444113e-07, val loss: 0.1655798703432083\n",
      "Epoch 2400: train loss: 4.2377919839964306e-07, val loss: 0.1655314415693283\n",
      "Epoch 2401: train loss: 3.487542130642396e-07, val loss: 0.16543219983577728\n",
      "Epoch 2402: train loss: 2.8222726200510806e-07, val loss: 0.1654278188943863\n",
      "Epoch 2403: train loss: 2.4415720645265537e-07, val loss: 0.1654406487941742\n",
      "Epoch 2404: train loss: 2.784897787933005e-07, val loss: 0.1653071939945221\n",
      "Epoch 2405: train loss: 2.553075262312632e-07, val loss: 0.16538886725902557\n",
      "Epoch 2406: train loss: 1.2158180595633894e-07, val loss: 0.16535572707653046\n",
      "Epoch 2407: train loss: 1.0800059868643075e-07, val loss: 0.16527286171913147\n",
      "Epoch 2408: train loss: 2.528208540297783e-07, val loss: 0.16535745561122894\n",
      "Epoch 2409: train loss: 2.6583100520838343e-07, val loss: 0.16524741053581238\n",
      "Epoch 2410: train loss: 1.2484024125569704e-07, val loss: 0.16527743637561798\n",
      "Epoch 2411: train loss: 8.412867913420996e-08, val loss: 0.16525790095329285\n",
      "Epoch 2412: train loss: 1.754911949092275e-07, val loss: 0.1652277410030365\n",
      "Epoch 2413: train loss: 1.8038261373476416e-07, val loss: 0.16522148251533508\n",
      "Epoch 2414: train loss: 8.090427172646741e-08, val loss: 0.16520346701145172\n",
      "Epoch 2415: train loss: 3.436893436514765e-08, val loss: 0.1651468575000763\n",
      "Epoch 2416: train loss: 6.75000535466097e-08, val loss: 0.1651819944381714\n",
      "Epoch 2417: train loss: 8.740590118350156e-08, val loss: 0.16513057053089142\n",
      "Epoch 2418: train loss: 6.963812637650335e-08, val loss: 0.165105402469635\n",
      "Epoch 2419: train loss: 5.9264802843017605e-08, val loss: 0.16513851284980774\n",
      "Epoch 2420: train loss: 7.01731153185392e-08, val loss: 0.16502954065799713\n",
      "Epoch 2421: train loss: 7.396128154368853e-08, val loss: 0.1651023030281067\n",
      "Epoch 2422: train loss: 7.282527292318264e-08, val loss: 0.165001779794693\n",
      "Epoch 2423: train loss: 7.973378757242244e-08, val loss: 0.16504153609275818\n",
      "Epoch 2424: train loss: 9.875177653384526e-08, val loss: 0.16498015820980072\n",
      "Epoch 2425: train loss: 1.3189725223128335e-07, val loss: 0.16500912606716156\n",
      "Epoch 2426: train loss: 1.8714695215749089e-07, val loss: 0.1649158000946045\n",
      "Epoch 2427: train loss: 2.9121454758751497e-07, val loss: 0.16500864923000336\n",
      "Epoch 2428: train loss: 5.401382168201962e-07, val loss: 0.16479499638080597\n",
      "Epoch 2429: train loss: 1.1835386430902872e-06, val loss: 0.16508594155311584\n",
      "Epoch 2430: train loss: 2.8168433345854282e-06, val loss: 0.16458766162395477\n",
      "Epoch 2431: train loss: 6.747394763806369e-06, val loss: 0.1652831882238388\n",
      "Epoch 2432: train loss: 1.5911049558781087e-05, val loss: 0.1642492562532425\n",
      "Epoch 2433: train loss: 3.465731060714461e-05, val loss: 0.16569311916828156\n",
      "Epoch 2434: train loss: 6.754413334419951e-05, val loss: 0.16403332352638245\n",
      "Epoch 2435: train loss: 0.00011936471128137782, val loss: 0.16600988805294037\n",
      "Epoch 2436: train loss: 0.00019040323968511075, val loss: 0.16392116248607635\n",
      "Epoch 2437: train loss: 0.0002812510938383639, val loss: 0.16582536697387695\n",
      "Epoch 2438: train loss: 0.0003603438672143966, val loss: 0.16446496546268463\n",
      "Epoch 2439: train loss: 0.0004055303579661995, val loss: 0.16540512442588806\n",
      "Epoch 2440: train loss: 0.0004239007248543203, val loss: 0.1663879007101059\n",
      "Epoch 2441: train loss: 0.0004864312068093568, val loss: 0.16486048698425293\n",
      "Epoch 2442: train loss: 0.00071590585866943, val loss: 0.1690979301929474\n",
      "Epoch 2443: train loss: 0.0013124659890308976, val loss: 0.16513073444366455\n",
      "Epoch 2444: train loss: 0.00246330164372921, val loss: 0.17276863753795624\n",
      "Epoch 2445: train loss: 0.004635930061340332, val loss: 0.16830360889434814\n",
      "Epoch 2446: train loss: 0.007561094593256712, val loss: 0.1805042028427124\n",
      "Epoch 2447: train loss: 0.009889476001262665, val loss: 0.16520552337169647\n",
      "Epoch 2448: train loss: 0.008588193915784359, val loss: 0.16263973712921143\n",
      "Epoch 2449: train loss: 0.003524231957271695, val loss: 0.15412260591983795\n",
      "Epoch 2450: train loss: 0.00015805299335625023, val loss: 0.15294478833675385\n",
      "Epoch 2451: train loss: 0.0023774346336722374, val loss: 0.1579713076353073\n",
      "Epoch 2452: train loss: 0.0037102289497852325, val loss: 0.1513330340385437\n",
      "Epoch 2453: train loss: 0.0009011445799842477, val loss: 0.14254869520664215\n",
      "Epoch 2454: train loss: 0.0007665204466320574, val loss: 0.14695315062999725\n",
      "Epoch 2455: train loss: 0.002245674142614007, val loss: 0.1445435732603073\n",
      "Epoch 2456: train loss: 0.000596197322010994, val loss: 0.140720397233963\n",
      "Epoch 2457: train loss: 0.0006683518877252936, val loss: 0.14426355063915253\n",
      "Epoch 2458: train loss: 0.0014421507949009538, val loss: 0.15233366191387177\n",
      "Epoch 2459: train loss: 0.00015434774104505777, val loss: 0.15560360252857208\n",
      "Epoch 2460: train loss: 0.0008442202815786004, val loss: 0.15518997609615326\n",
      "Epoch 2461: train loss: 0.0006981620681472123, val loss: 0.15498320758342743\n",
      "Epoch 2462: train loss: 0.00014574098167940974, val loss: 0.15649990737438202\n",
      "Epoch 2463: train loss: 0.0008088562171906233, val loss: 0.15578937530517578\n",
      "Epoch 2464: train loss: 0.00012349995085969567, val loss: 0.15850210189819336\n",
      "Epoch 2465: train loss: 0.00044121622340753675, val loss: 0.15630607306957245\n",
      "Epoch 2466: train loss: 0.00037097808672115207, val loss: 0.15759830176830292\n",
      "Epoch 2467: train loss: 0.0001070663274731487, val loss: 0.16054850816726685\n",
      "Epoch 2468: train loss: 0.00045319058699533343, val loss: 0.16010384261608124\n",
      "Epoch 2469: train loss: 3.162742723361589e-05, val loss: 0.16010530292987823\n",
      "Epoch 2470: train loss: 0.0003205662651453167, val loss: 0.15867701172828674\n",
      "Epoch 2471: train loss: 0.00012687309936154634, val loss: 0.15813492238521576\n",
      "Epoch 2472: train loss: 0.0001320122682955116, val loss: 0.15833762288093567\n",
      "Epoch 2473: train loss: 0.00021625729277729988, val loss: 0.15817046165466309\n",
      "Epoch 2474: train loss: 2.229538222309202e-05, val loss: 0.1577879637479782\n",
      "Epoch 2475: train loss: 0.00021532087703235447, val loss: 0.15725433826446533\n",
      "Epoch 2476: train loss: 1.8931606973637827e-05, val loss: 0.15796291828155518\n",
      "Epoch 2477: train loss: 0.00014013942563906312, val loss: 0.1583145409822464\n",
      "Epoch 2478: train loss: 6.808585749240592e-05, val loss: 0.15860697627067566\n",
      "Epoch 2479: train loss: 5.662588228005916e-05, val loss: 0.15980300307273865\n",
      "Epoch 2480: train loss: 0.00010457643656991422, val loss: 0.1602059155702591\n",
      "Epoch 2481: train loss: 1.0811393622134347e-05, val loss: 0.15978288650512695\n",
      "Epoch 2482: train loss: 0.0001010605483315885, val loss: 0.15857572853565216\n",
      "Epoch 2483: train loss: 9.31963040784467e-06, val loss: 0.15834937989711761\n",
      "Epoch 2484: train loss: 6.863863382022828e-05, val loss: 0.15850500762462616\n",
      "Epoch 2485: train loss: 2.8518246836028993e-05, val loss: 0.15825839340686798\n",
      "Epoch 2486: train loss: 3.221791484975256e-05, val loss: 0.15808768570423126\n",
      "Epoch 2487: train loss: 4.452315624803305e-05, val loss: 0.15832911431789398\n",
      "Epoch 2488: train loss: 9.441427209821995e-06, val loss: 0.1583072394132614\n",
      "Epoch 2489: train loss: 4.637346864910796e-05, val loss: 0.1576792448759079\n",
      "Epoch 2490: train loss: 4.1763632907532156e-06, val loss: 0.15760914981365204\n",
      "Epoch 2491: train loss: 3.4933604183606803e-05, val loss: 0.15777809917926788\n",
      "Epoch 2492: train loss: 1.1128500773338601e-05, val loss: 0.15745864808559418\n",
      "Epoch 2493: train loss: 1.7899306840263307e-05, val loss: 0.15694239735603333\n",
      "Epoch 2494: train loss: 2.0080155081814155e-05, val loss: 0.15681324899196625\n",
      "Epoch 2495: train loss: 4.889054252998903e-06, val loss: 0.15684767067432404\n",
      "Epoch 2496: train loss: 2.2777256162953563e-05, val loss: 0.15671758353710175\n",
      "Epoch 2497: train loss: 1.5943344351398991e-06, val loss: 0.156836599111557\n",
      "Epoch 2498: train loss: 1.693652848189231e-05, val loss: 0.15702617168426514\n",
      "Epoch 2499: train loss: 5.905406851525186e-06, val loss: 0.1567959189414978\n",
      "Epoch 2500: train loss: 7.733960046607535e-06, val loss: 0.15624059736728668\n",
      "Epoch 2501: train loss: 1.069381323759444e-05, val loss: 0.15600992739200592\n",
      "Epoch 2502: train loss: 1.922678848131909e-06, val loss: 0.15604321658611298\n",
      "Epoch 2503: train loss: 1.0903376278292853e-05, val loss: 0.15579038858413696\n",
      "Epoch 2504: train loss: 1.4967786228226032e-06, val loss: 0.15565119683742523\n",
      "Epoch 2505: train loss: 7.2339116741204634e-06, val loss: 0.15594878792762756\n",
      "Epoch 2506: train loss: 3.831793492281577e-06, val loss: 0.1560073345899582\n",
      "Epoch 2507: train loss: 3.0295564101834316e-06, val loss: 0.15557639300823212\n",
      "Epoch 2508: train loss: 5.614059773506597e-06, val loss: 0.15540042519569397\n",
      "Epoch 2509: train loss: 8.156106332535273e-07, val loss: 0.15543422102928162\n",
      "Epoch 2510: train loss: 5.230864644545363e-06, val loss: 0.155059352517128\n",
      "Epoch 2511: train loss: 1.0156999223909224e-06, val loss: 0.15483585000038147\n",
      "Epoch 2512: train loss: 3.0889227673469577e-06, val loss: 0.1550646722316742\n",
      "Epoch 2513: train loss: 2.431142320347135e-06, val loss: 0.15492086112499237\n",
      "Epoch 2514: train loss: 8.921832659325446e-07, val loss: 0.154539093375206\n",
      "Epoch 2515: train loss: 3.196335455868393e-06, val loss: 0.15466348826885223\n",
      "Epoch 2516: train loss: 2.477806333445187e-07, val loss: 0.15472330152988434\n",
      "Epoch 2517: train loss: 2.3139534732763423e-06, val loss: 0.1543297916650772\n",
      "Epoch 2518: train loss: 1.0483264532012981e-06, val loss: 0.15420523285865784\n",
      "Epoch 2519: train loss: 8.816455761007091e-07, val loss: 0.15423069894313812\n",
      "Epoch 2520: train loss: 1.6975779999484075e-06, val loss: 0.15386967360973358\n",
      "Epoch 2521: train loss: 2.119617477092106e-07, val loss: 0.15364383161067963\n",
      "Epoch 2522: train loss: 1.4773568182135932e-06, val loss: 0.15378454327583313\n",
      "Epoch 2523: train loss: 4.3574567598625435e-07, val loss: 0.1537308543920517\n",
      "Epoch 2524: train loss: 7.21679782600404e-07, val loss: 0.15347932279109955\n",
      "Epoch 2525: train loss: 8.856462159201328e-07, val loss: 0.15337230265140533\n",
      "Epoch 2526: train loss: 1.7704695665088366e-07, val loss: 0.15327031910419464\n",
      "Epoch 2527: train loss: 9.307303230343678e-07, val loss: 0.15303240716457367\n",
      "Epoch 2528: train loss: 1.9193535649719706e-07, val loss: 0.15285877883434296\n",
      "Epoch 2529: train loss: 4.918798026665172e-07, val loss: 0.15280131995677948\n",
      "Epoch 2530: train loss: 5.144709120941116e-07, val loss: 0.15266166627407074\n",
      "Epoch 2531: train loss: 9.472659456832844e-08, val loss: 0.15246272087097168\n",
      "Epoch 2532: train loss: 5.64613856113283e-07, val loss: 0.15239767730236053\n",
      "Epoch 2533: train loss: 1.199725119249706e-07, val loss: 0.15232621133327484\n",
      "Epoch 2534: train loss: 2.906836016336456e-07, val loss: 0.15210342407226562\n",
      "Epoch 2535: train loss: 3.174790492721513e-07, val loss: 0.15193386375904083\n",
      "Epoch 2536: train loss: 5.498107213952608e-08, val loss: 0.15180189907550812\n",
      "Epoch 2537: train loss: 3.512590183163411e-07, val loss: 0.15158553421497345\n",
      "Epoch 2538: train loss: 7.395873780069451e-08, val loss: 0.15145818889141083\n",
      "Epoch 2539: train loss: 1.5073814552124531e-07, val loss: 0.15138976275920868\n",
      "Epoch 2540: train loss: 2.2177525238475937e-07, val loss: 0.1512213796377182\n",
      "Epoch 2541: train loss: 2.1056063559399263e-08, val loss: 0.1510465443134308\n",
      "Epoch 2542: train loss: 1.9807244200364948e-07, val loss: 0.15090374648571014\n",
      "Epoch 2543: train loss: 7.511649613434201e-08, val loss: 0.1507275402545929\n",
      "Epoch 2544: train loss: 6.756475556812802e-08, val loss: 0.15054075419902802\n",
      "Epoch 2545: train loss: 1.5418414989198936e-07, val loss: 0.15040019154548645\n",
      "Epoch 2546: train loss: 9.568537073789685e-09, val loss: 0.15024513006210327\n",
      "Epoch 2547: train loss: 9.915257948023282e-08, val loss: 0.15002015233039856\n",
      "Epoch 2548: train loss: 7.912868937864914e-08, val loss: 0.14985886216163635\n",
      "Epoch 2549: train loss: 2.1359554125410796e-08, val loss: 0.1497707962989807\n",
      "Epoch 2550: train loss: 8.943290197294118e-08, val loss: 0.14957107603549957\n",
      "Epoch 2551: train loss: 2.3807070093084803e-08, val loss: 0.14934943616390228\n",
      "Epoch 2552: train loss: 4.355779381626235e-08, val loss: 0.14920225739479065\n",
      "Epoch 2553: train loss: 6.450429879123476e-08, val loss: 0.14899006485939026\n",
      "Epoch 2554: train loss: 1.0061174116060556e-08, val loss: 0.14879772067070007\n",
      "Epoch 2555: train loss: 4.6110269380506e-08, val loss: 0.14867658913135529\n",
      "Epoch 2556: train loss: 3.1336426786765514e-08, val loss: 0.14849427342414856\n",
      "Epoch 2557: train loss: 1.309475727850895e-08, val loss: 0.14828601479530334\n",
      "Epoch 2558: train loss: 4.33101448038542e-08, val loss: 0.1481023132801056\n",
      "Epoch 2559: train loss: 1.6518885814775786e-08, val loss: 0.14790840446949005\n",
      "Epoch 2560: train loss: 1.5071108805386757e-08, val loss: 0.1476944535970688\n",
      "Epoch 2561: train loss: 2.93464186285064e-08, val loss: 0.1475234478712082\n",
      "Epoch 2562: train loss: 8.542090590424323e-09, val loss: 0.1473565548658371\n",
      "Epoch 2563: train loss: 1.837117125091936e-08, val loss: 0.1471279114484787\n",
      "Epoch 2564: train loss: 2.2311835934374358e-08, val loss: 0.14695584774017334\n",
      "Epoch 2565: train loss: 8.6492564221885e-09, val loss: 0.1467670053243637\n",
      "Epoch 2566: train loss: 1.7978502242499417e-08, val loss: 0.14660951495170593\n",
      "Epoch 2567: train loss: 1.4895509714563104e-08, val loss: 0.14647991955280304\n",
      "Epoch 2568: train loss: 8.260847117469439e-09, val loss: 0.14639177918434143\n",
      "Epoch 2569: train loss: 1.7383863237796504e-08, val loss: 0.1462411880493164\n",
      "Epoch 2570: train loss: 1.7368725124811135e-08, val loss: 0.14617811143398285\n",
      "Epoch 2571: train loss: 3.132042181164252e-08, val loss: 0.14597749710083008\n",
      "Epoch 2572: train loss: 8.529802641987771e-08, val loss: 0.14602050185203552\n",
      "Epoch 2573: train loss: 2.5505420353511e-07, val loss: 0.14561820030212402\n",
      "Epoch 2574: train loss: 7.774522714498744e-07, val loss: 0.14602075517177582\n",
      "Epoch 2575: train loss: 2.0482286799961003e-06, val loss: 0.14510729908943176\n",
      "Epoch 2576: train loss: 3.8656789911328815e-06, val loss: 0.14595462381839752\n",
      "Epoch 2577: train loss: 5.798616712127114e-06, val loss: 0.14506931602954865\n",
      "Epoch 2578: train loss: 8.672804142406676e-06, val loss: 0.1455482691526413\n",
      "Epoch 2579: train loss: 1.7473512343713082e-05, val loss: 0.14532093703746796\n",
      "Epoch 2580: train loss: 4.178818562650122e-05, val loss: 0.1449386328458786\n",
      "Epoch 2581: train loss: 9.709743608254939e-05, val loss: 0.14479875564575195\n",
      "Epoch 2582: train loss: 0.00019191503815818578, val loss: 0.144826278090477\n",
      "Epoch 2583: train loss: 0.0003368113248143345, val loss: 0.14336617290973663\n",
      "Epoch 2584: train loss: 0.0005157675477676094, val loss: 0.14641304314136505\n",
      "Epoch 2585: train loss: 0.0006981530459597707, val loss: 0.14399316906929016\n",
      "Epoch 2586: train loss: 0.0007620361866429448, val loss: 0.14700822532176971\n",
      "Epoch 2587: train loss: 0.0005898095550946891, val loss: 0.1450747698545456\n",
      "Epoch 2588: train loss: 0.00025937255122698843, val loss: 0.14543774724006653\n",
      "Epoch 2589: train loss: 2.9428298148559406e-05, val loss: 0.1460612714290619\n",
      "Epoch 2590: train loss: 7.140538218664005e-05, val loss: 0.1453704982995987\n",
      "Epoch 2591: train loss: 0.00025450022076256573, val loss: 0.1464584320783615\n",
      "Epoch 2592: train loss: 0.0002996127004735172, val loss: 0.14604619145393372\n",
      "Epoch 2593: train loss: 0.00011978077964158729, val loss: 0.14582200348377228\n",
      "Epoch 2594: train loss: 4.260259629518259e-06, val loss: 0.14619062840938568\n",
      "Epoch 2595: train loss: 0.000117705792945344, val loss: 0.14583827555179596\n",
      "Epoch 2596: train loss: 0.00019297425751574337, val loss: 0.14578877389431\n",
      "Epoch 2597: train loss: 8.326020906679332e-05, val loss: 0.14561937749385834\n",
      "Epoch 2598: train loss: 2.5824126623774646e-06, val loss: 0.14541371166706085\n",
      "Epoch 2599: train loss: 7.4447896622587e-05, val loss: 0.1452537477016449\n",
      "Epoch 2600: train loss: 0.0001248196349479258, val loss: 0.14524845778942108\n",
      "Epoch 2601: train loss: 5.344758028513752e-05, val loss: 0.1455835998058319\n",
      "Epoch 2602: train loss: 2.1621542600769317e-06, val loss: 0.1455736607313156\n",
      "Epoch 2603: train loss: 5.05013667861931e-05, val loss: 0.14542408287525177\n",
      "Epoch 2604: train loss: 8.085189620032907e-05, val loss: 0.14522898197174072\n",
      "Epoch 2605: train loss: 3.2459258363815024e-05, val loss: 0.14504407346248627\n",
      "Epoch 2606: train loss: 2.3826373762858566e-06, val loss: 0.1451907455921173\n",
      "Epoch 2607: train loss: 3.542880585882813e-05, val loss: 0.1450679749250412\n",
      "Epoch 2608: train loss: 5.219196100370027e-05, val loss: 0.14506398141384125\n",
      "Epoch 2609: train loss: 1.9978773707407527e-05, val loss: 0.1451522558927536\n",
      "Epoch 2610: train loss: 2.9783236641378608e-06, val loss: 0.14472533762454987\n",
      "Epoch 2611: train loss: 2.432367909932509e-05, val loss: 0.14472609758377075\n",
      "Epoch 2612: train loss: 3.3266500395257026e-05, val loss: 0.14470349252223969\n",
      "Epoch 2613: train loss: 1.319382954534376e-05, val loss: 0.1443995088338852\n",
      "Epoch 2614: train loss: 3.3144181088573532e-06, val loss: 0.1445358842611313\n",
      "Epoch 2615: train loss: 1.599933602847159e-05, val loss: 0.1443624496459961\n",
      "Epoch 2616: train loss: 2.1181669580982998e-05, val loss: 0.14427204430103302\n",
      "Epoch 2617: train loss: 9.340551514469553e-06, val loss: 0.1443958580493927\n",
      "Epoch 2618: train loss: 3.1271338229998946e-06, val loss: 0.14403297007083893\n",
      "Epoch 2619: train loss: 1.0164384548261296e-05, val loss: 0.14393118023872375\n",
      "Epoch 2620: train loss: 1.3637748452310916e-05, val loss: 0.14385180175304413\n",
      "Epoch 2621: train loss: 6.90003025738406e-06, val loss: 0.14374397695064545\n",
      "Epoch 2622: train loss: 2.468746515660314e-06, val loss: 0.14386335015296936\n",
      "Epoch 2623: train loss: 6.372766165441135e-06, val loss: 0.14374828338623047\n",
      "Epoch 2624: train loss: 9.044030775839929e-06, val loss: 0.14355464279651642\n",
      "Epoch 2625: train loss: 5.0980593186977785e-06, val loss: 0.14355574548244476\n",
      "Epoch 2626: train loss: 1.6662689859003876e-06, val loss: 0.14344973862171173\n",
      "Epoch 2627: train loss: 3.768380338442512e-06, val loss: 0.14331357181072235\n",
      "Epoch 2628: train loss: 6.297648724284954e-06, val loss: 0.1433464139699936\n",
      "Epoch 2629: train loss: 4.240386260789819e-06, val loss: 0.14324775338172913\n",
      "Epoch 2630: train loss: 1.0643146879374399e-06, val loss: 0.1432129442691803\n",
      "Epoch 2631: train loss: 1.6021413102862425e-06, val loss: 0.14314375817775726\n",
      "Epoch 2632: train loss: 4.054802957398351e-06, val loss: 0.1430303305387497\n",
      "Epoch 2633: train loss: 4.007584720966406e-06, val loss: 0.14299236238002777\n",
      "Epoch 2634: train loss: 1.4800655208091484e-06, val loss: 0.14290638267993927\n",
      "Epoch 2635: train loss: 2.1965320229355711e-07, val loss: 0.14285743236541748\n",
      "Epoch 2636: train loss: 1.5696199398007593e-06, val loss: 0.14284446835517883\n",
      "Epoch 2637: train loss: 3.0123567285045283e-06, val loss: 0.14277313649654388\n",
      "Epoch 2638: train loss: 2.43707245317637e-06, val loss: 0.14275632798671722\n",
      "Epoch 2639: train loss: 7.424829391311505e-07, val loss: 0.1427135020494461\n",
      "Epoch 2640: train loss: 1.0454377274982107e-07, val loss: 0.1426190733909607\n",
      "Epoch 2641: train loss: 9.271478802475031e-07, val loss: 0.14262299239635468\n",
      "Epoch 2642: train loss: 1.8033064179689973e-06, val loss: 0.14247366786003113\n",
      "Epoch 2643: train loss: 1.6464845202790457e-06, val loss: 0.14245112240314484\n",
      "Epoch 2644: train loss: 7.718184633631608e-07, val loss: 0.14240896701812744\n",
      "Epoch 2645: train loss: 2.262531779706478e-07, val loss: 0.14231640100479126\n",
      "Epoch 2646: train loss: 3.964894688124332e-07, val loss: 0.14236228168010712\n",
      "Epoch 2647: train loss: 8.583774047110637e-07, val loss: 0.14216651022434235\n",
      "Epoch 2648: train loss: 1.0302509281245875e-06, val loss: 0.14224974811077118\n",
      "Epoch 2649: train loss: 7.850572387724242e-07, val loss: 0.14201317727565765\n",
      "Epoch 2650: train loss: 4.306800178710546e-07, val loss: 0.1420917958021164\n",
      "Epoch 2651: train loss: 2.4868947434697475e-07, val loss: 0.1419522762298584\n",
      "Epoch 2652: train loss: 3.0769245995543315e-07, val loss: 0.1419193595647812\n",
      "Epoch 2653: train loss: 4.4242062813282246e-07, val loss: 0.14194214344024658\n",
      "Epoch 2654: train loss: 5.230400006439595e-07, val loss: 0.14168934524059296\n",
      "Epoch 2655: train loss: 5.092397259431891e-07, val loss: 0.14190888404846191\n",
      "Epoch 2656: train loss: 4.5523356106969004e-07, val loss: 0.14150795340538025\n",
      "Epoch 2657: train loss: 4.3231784729869105e-07, val loss: 0.14186885952949524\n",
      "Epoch 2658: train loss: 4.920250376017066e-07, val loss: 0.14136934280395508\n",
      "Epoch 2659: train loss: 6.875918074911169e-07, val loss: 0.14185240864753723\n",
      "Epoch 2660: train loss: 1.0852392051674542e-06, val loss: 0.14118413627147675\n",
      "Epoch 2661: train loss: 1.8309746110389824e-06, val loss: 0.14192159473896027\n",
      "Epoch 2662: train loss: 3.2213963550020708e-06, val loss: 0.1408054530620575\n",
      "Epoch 2663: train loss: 5.938275990047259e-06, val loss: 0.1422695368528366\n",
      "Epoch 2664: train loss: 1.1177319720445666e-05, val loss: 0.14009395241737366\n",
      "Epoch 2665: train loss: 2.1471721993293613e-05, val loss: 0.1430765986442566\n",
      "Epoch 2666: train loss: 3.8140362448757514e-05, val loss: 0.1394156664609909\n",
      "Epoch 2667: train loss: 6.431411020457745e-05, val loss: 0.143987774848938\n",
      "Epoch 2668: train loss: 0.00010231228225165978, val loss: 0.13931678235530853\n",
      "Epoch 2669: train loss: 0.0001573185290908441, val loss: 0.14476095139980316\n",
      "Epoch 2670: train loss: 0.00022802117746323347, val loss: 0.14002616703510284\n",
      "Epoch 2671: train loss: 0.00031716551166027784, val loss: 0.14488807320594788\n",
      "Epoch 2672: train loss: 0.00040939878090284765, val loss: 0.14229540526866913\n",
      "Epoch 2673: train loss: 0.0005036024376749992, val loss: 0.14395588636398315\n",
      "Epoch 2674: train loss: 0.000711452797986567, val loss: 0.14300863444805145\n",
      "Epoch 2675: train loss: 0.0011127990437671542, val loss: 0.1450512409210205\n",
      "Epoch 2676: train loss: 0.001429487601853907, val loss: 0.1466580480337143\n",
      "Epoch 2677: train loss: 0.0011948408791795373, val loss: 0.1429506093263626\n",
      "Epoch 2678: train loss: 0.0005440092063508928, val loss: 0.1472824066877365\n",
      "Epoch 2679: train loss: 0.00024728660355322063, val loss: 0.1479964703321457\n",
      "Epoch 2680: train loss: 0.000469256192445755, val loss: 0.14331896603107452\n",
      "Epoch 2681: train loss: 0.0005859412485733628, val loss: 0.1471342295408249\n",
      "Epoch 2682: train loss: 0.0002855389902833849, val loss: 0.1439572423696518\n",
      "Epoch 2683: train loss: 0.000127016770420596, val loss: 0.14528736472129822\n",
      "Epoch 2684: train loss: 0.00027269526617601514, val loss: 0.14657679200172424\n",
      "Epoch 2685: train loss: 0.00032300996826961637, val loss: 0.14090971648693085\n",
      "Epoch 2686: train loss: 0.0002176756679546088, val loss: 0.14326594769954681\n",
      "Epoch 2687: train loss: 0.00016121118096634746, val loss: 0.1421136111021042\n",
      "Epoch 2688: train loss: 0.00012558091839309782, val loss: 0.13982290029525757\n",
      "Epoch 2689: train loss: 0.00010846120858332142, val loss: 0.14195823669433594\n",
      "Epoch 2690: train loss: 0.00015620561316609383, val loss: 0.138886496424675\n",
      "Epoch 2691: train loss: 0.00014307834499049932, val loss: 0.1386340856552124\n",
      "Epoch 2692: train loss: 8.92674142960459e-05, val loss: 0.1397622972726822\n",
      "Epoch 2693: train loss: 7.883922808105126e-05, val loss: 0.1369817554950714\n",
      "Epoch 2694: train loss: 7.022003410384059e-05, val loss: 0.1375923603773117\n",
      "Epoch 2695: train loss: 6.770229083485901e-05, val loss: 0.1379927396774292\n",
      "Epoch 2696: train loss: 8.489680476486683e-05, val loss: 0.13605637848377228\n",
      "Epoch 2697: train loss: 5.951512503088452e-05, val loss: 0.13649268448352814\n",
      "Epoch 2698: train loss: 3.728754381882027e-05, val loss: 0.13634854555130005\n",
      "Epoch 2699: train loss: 5.4222004109760746e-05, val loss: 0.13512472808361053\n",
      "Epoch 2700: train loss: 4.063393498654477e-05, val loss: 0.13524244725704193\n",
      "Epoch 2701: train loss: 3.049635051866062e-05, val loss: 0.13523522019386292\n",
      "Epoch 2702: train loss: 5.18983906658832e-05, val loss: 0.13441625237464905\n",
      "Epoch 2703: train loss: 3.216730692656711e-05, val loss: 0.134377583861351\n",
      "Epoch 2704: train loss: 7.963759344420396e-06, val loss: 0.13496644794940948\n",
      "Epoch 2705: train loss: 2.9430411814246327e-05, val loss: 0.1347123235464096\n",
      "Epoch 2706: train loss: 3.727484727278352e-05, val loss: 0.13442011177539825\n",
      "Epoch 2707: train loss: 1.5810799595783465e-05, val loss: 0.13432292640209198\n",
      "Epoch 2708: train loss: 1.3988561477162875e-05, val loss: 0.13377408683300018\n",
      "Epoch 2709: train loss: 1.7492771803517826e-05, val loss: 0.13441680371761322\n",
      "Epoch 2710: train loss: 1.4710298273712397e-05, val loss: 0.1352045238018036\n",
      "Epoch 2711: train loss: 1.6199481251533143e-05, val loss: 0.13421109318733215\n",
      "Epoch 2712: train loss: 1.4367883522936609e-05, val loss: 0.1340540498495102\n",
      "Epoch 2713: train loss: 8.871250429365318e-06, val loss: 0.13478247821331024\n",
      "Epoch 2714: train loss: 9.639277777750976e-06, val loss: 0.13463233411312103\n",
      "Epoch 2715: train loss: 9.157728527497966e-06, val loss: 0.1348806619644165\n",
      "Epoch 2716: train loss: 7.89948899182491e-06, val loss: 0.1347561925649643\n",
      "Epoch 2717: train loss: 9.439534551347606e-06, val loss: 0.1343984156847\n",
      "Epoch 2718: train loss: 8.266565600933973e-06, val loss: 0.13520941138267517\n",
      "Epoch 2719: train loss: 4.307263679947937e-06, val loss: 0.13540920615196228\n",
      "Epoch 2720: train loss: 4.399810222821543e-06, val loss: 0.13527169823646545\n",
      "Epoch 2721: train loss: 5.322147444530856e-06, val loss: 0.13539059460163116\n",
      "Epoch 2722: train loss: 5.59169529879e-06, val loss: 0.13520179688930511\n",
      "Epoch 2723: train loss: 5.644433713314356e-06, val loss: 0.13545212149620056\n",
      "Epoch 2724: train loss: 3.753674945983221e-06, val loss: 0.1356191188097\n",
      "Epoch 2725: train loss: 1.5611419712513452e-06, val loss: 0.13565069437026978\n",
      "Epoch 2726: train loss: 2.744994390013744e-06, val loss: 0.13578356802463531\n",
      "Epoch 2727: train loss: 4.055193130625412e-06, val loss: 0.1358020305633545\n",
      "Epoch 2728: train loss: 2.9468392312992364e-06, val loss: 0.1360807716846466\n",
      "Epoch 2729: train loss: 2.1487826415977906e-06, val loss: 0.13592353463172913\n",
      "Epoch 2730: train loss: 2.4689186375326244e-06, val loss: 0.13597194850444794\n",
      "Epoch 2731: train loss: 2.0008874344057404e-06, val loss: 0.13628776371479034\n",
      "Epoch 2732: train loss: 1.1349917485858896e-06, val loss: 0.13604457676410675\n",
      "Epoch 2733: train loss: 1.4029957355887746e-06, val loss: 0.13618095219135284\n",
      "Epoch 2734: train loss: 1.7646647165747709e-06, val loss: 0.1363133043050766\n",
      "Epoch 2735: train loss: 1.5757736946397927e-06, val loss: 0.13642732799053192\n",
      "Epoch 2736: train loss: 1.4626566553488374e-06, val loss: 0.13671299815177917\n",
      "Epoch 2737: train loss: 1.4987226677476428e-06, val loss: 0.13651219010353088\n",
      "Epoch 2738: train loss: 1.0440693358759745e-06, val loss: 0.1366911679506302\n",
      "Epoch 2739: train loss: 5.111214704811573e-07, val loss: 0.13687646389007568\n",
      "Epoch 2740: train loss: 4.408853726545203e-07, val loss: 0.13679750263690948\n",
      "Epoch 2741: train loss: 7.791887810526532e-07, val loss: 0.13710008561611176\n",
      "Epoch 2742: train loss: 1.0148255569220055e-06, val loss: 0.13705889880657196\n",
      "Epoch 2743: train loss: 9.348218554805499e-07, val loss: 0.13710467517375946\n",
      "Epoch 2744: train loss: 6.987787628531805e-07, val loss: 0.13724838197231293\n",
      "Epoch 2745: train loss: 5.780021865575691e-07, val loss: 0.13731388747692108\n",
      "Epoch 2746: train loss: 6.53074152978661e-07, val loss: 0.13749544322490692\n",
      "Epoch 2747: train loss: 6.612578999920515e-07, val loss: 0.13742852210998535\n",
      "Epoch 2748: train loss: 4.865411824539478e-07, val loss: 0.13753455877304077\n",
      "Epoch 2749: train loss: 2.4782161744951736e-07, val loss: 0.13760188221931458\n",
      "Epoch 2750: train loss: 1.7334019730697037e-07, val loss: 0.13765370845794678\n",
      "Epoch 2751: train loss: 2.487276447027398e-07, val loss: 0.13771472871303558\n",
      "Epoch 2752: train loss: 3.064865268243011e-07, val loss: 0.13769744336605072\n",
      "Epoch 2753: train loss: 2.541467267747066e-07, val loss: 0.13779453933238983\n",
      "Epoch 2754: train loss: 2.1617776724269788e-07, val loss: 0.13781148195266724\n",
      "Epoch 2755: train loss: 2.58105217199045e-07, val loss: 0.13786102831363678\n",
      "Epoch 2756: train loss: 3.5188585911782866e-07, val loss: 0.13787944614887238\n",
      "Epoch 2757: train loss: 3.706628035615722e-07, val loss: 0.13792060315608978\n",
      "Epoch 2758: train loss: 3.313501792945317e-07, val loss: 0.1379767656326294\n",
      "Epoch 2759: train loss: 3.4911792567982047e-07, val loss: 0.13800689578056335\n",
      "Epoch 2760: train loss: 5.076254865343799e-07, val loss: 0.13806511461734772\n",
      "Epoch 2761: train loss: 7.923153475530853e-07, val loss: 0.1381516009569168\n",
      "Epoch 2762: train loss: 1.2612650834853412e-06, val loss: 0.138235405087471\n",
      "Epoch 2763: train loss: 2.0726467937493e-06, val loss: 0.13827259838581085\n",
      "Epoch 2764: train loss: 3.559888455129112e-06, val loss: 0.138509139418602\n",
      "Epoch 2765: train loss: 6.352801847242517e-06, val loss: 0.13857893645763397\n",
      "Epoch 2766: train loss: 1.196393077407265e-05, val loss: 0.138932466506958\n",
      "Epoch 2767: train loss: 2.3209446226246655e-05, val loss: 0.13869518041610718\n",
      "Epoch 2768: train loss: 4.6540673793060705e-05, val loss: 0.1389419585466385\n",
      "Epoch 2769: train loss: 9.547575609758496e-05, val loss: 0.13865827023983002\n",
      "Epoch 2770: train loss: 0.0002016610960708931, val loss: 0.1391887217760086\n",
      "Epoch 2771: train loss: 0.00043004073086194694, val loss: 0.1388445943593979\n",
      "Epoch 2772: train loss: 0.0009286849526688457, val loss: 0.13920171558856964\n",
      "Epoch 2773: train loss: 0.001968314405530691, val loss: 0.13950255513191223\n",
      "Epoch 2774: train loss: 0.0038393496070057154, val loss: 0.14199285209178925\n",
      "Epoch 2775: train loss: 0.0060713086277246475, val loss: 0.14103206992149353\n",
      "Epoch 2776: train loss: 0.0072985365986824036, val loss: 0.14319263398647308\n",
      "Epoch 2777: train loss: 0.004542074631899595, val loss: 0.13974933326244354\n",
      "Epoch 2778: train loss: 0.0007748912903480232, val loss: 0.13896594941616058\n",
      "Epoch 2779: train loss: 0.0014544856967404485, val loss: 0.136937215924263\n",
      "Epoch 2780: train loss: 0.0026893680915236473, val loss: 0.13338226079940796\n",
      "Epoch 2781: train loss: 0.0011660874588415027, val loss: 0.13150516152381897\n",
      "Epoch 2782: train loss: 0.0009165580850094557, val loss: 0.1285395622253418\n",
      "Epoch 2783: train loss: 0.0012758346274495125, val loss: 0.13042162358760834\n",
      "Epoch 2784: train loss: 0.0007389917154796422, val loss: 0.1329762041568756\n",
      "Epoch 2785: train loss: 0.0008675971766933799, val loss: 0.12651656568050385\n",
      "Epoch 2786: train loss: 0.0006139117758721113, val loss: 0.12622052431106567\n",
      "Epoch 2787: train loss: 0.000630293448921293, val loss: 0.13160422444343567\n",
      "Epoch 2788: train loss: 0.0005145579925738275, val loss: 0.1258738785982132\n",
      "Epoch 2789: train loss: 0.0003804180887527764, val loss: 0.1251741498708725\n",
      "Epoch 2790: train loss: 0.0005467165610753, val loss: 0.1296766698360443\n",
      "Epoch 2791: train loss: 0.00021510016813408583, val loss: 0.1266518235206604\n",
      "Epoch 2792: train loss: 0.0004152392502874136, val loss: 0.12336237728595734\n",
      "Epoch 2793: train loss: 0.00022837061260361224, val loss: 0.1260254681110382\n",
      "Epoch 2794: train loss: 0.0002596574486233294, val loss: 0.1262439489364624\n",
      "Epoch 2795: train loss: 0.0002426791616016999, val loss: 0.12315943092107773\n",
      "Epoch 2796: train loss: 0.0001613367348909378, val loss: 0.1233658418059349\n",
      "Epoch 2797: train loss: 0.00021893622761126608, val loss: 0.12482178211212158\n",
      "Epoch 2798: train loss: 0.00011721772898454219, val loss: 0.12373043596744537\n",
      "Epoch 2799: train loss: 0.00018309355073142797, val loss: 0.12327714264392853\n",
      "Epoch 2800: train loss: 8.020315726753324e-05, val loss: 0.12428154796361923\n",
      "Epoch 2801: train loss: 0.00016507208056282252, val loss: 0.12335174530744553\n",
      "Epoch 2802: train loss: 5.015251736040227e-05, val loss: 0.12193359434604645\n",
      "Epoch 2803: train loss: 0.0001346163626294583, val loss: 0.12280014902353287\n",
      "Epoch 2804: train loss: 5.181948290555738e-05, val loss: 0.12359865009784698\n",
      "Epoch 2805: train loss: 8.74469697009772e-05, val loss: 0.12240574508905411\n",
      "Epoch 2806: train loss: 6.533949635922909e-05, val loss: 0.12191534042358398\n",
      "Epoch 2807: train loss: 5.048613456892781e-05, val loss: 0.12251134216785431\n",
      "Epoch 2808: train loss: 6.706872954964638e-05, val loss: 0.12185700982809067\n",
      "Epoch 2809: train loss: 3.163798464811407e-05, val loss: 0.12121925503015518\n",
      "Epoch 2810: train loss: 5.9593487094389275e-05, val loss: 0.12214856594800949\n",
      "Epoch 2811: train loss: 2.3117727323551662e-05, val loss: 0.12234510481357574\n",
      "Epoch 2812: train loss: 4.885623275185935e-05, val loss: 0.12095671147108078\n",
      "Epoch 2813: train loss: 1.8228623957838863e-05, val loss: 0.12079459428787231\n",
      "Epoch 2814: train loss: 4.0796072426019236e-05, val loss: 0.12165942043066025\n",
      "Epoch 2815: train loss: 1.3434220818453468e-05, val loss: 0.12135224789381027\n",
      "Epoch 2816: train loss: 3.298032970633358e-05, val loss: 0.12080223858356476\n",
      "Epoch 2817: train loss: 1.295040055993013e-05, val loss: 0.12131720036268234\n",
      "Epoch 2818: train loss: 2.329334893147461e-05, val loss: 0.12129058688879013\n",
      "Epoch 2819: train loss: 1.4408295101020485e-05, val loss: 0.1206565648317337\n",
      "Epoch 2820: train loss: 1.5389594409498386e-05, val loss: 0.12091577053070068\n",
      "Epoch 2821: train loss: 1.4526743143505882e-05, val loss: 0.12119519710540771\n",
      "Epoch 2822: train loss: 1.0360270607634448e-05, val loss: 0.12076466530561447\n",
      "Epoch 2823: train loss: 1.3413218766800128e-05, val loss: 0.12089188396930695\n",
      "Epoch 2824: train loss: 7.606564395246096e-06, val loss: 0.12125249952077866\n",
      "Epoch 2825: train loss: 1.0810877938638441e-05, val loss: 0.12072721868753433\n",
      "Epoch 2826: train loss: 6.999420747888507e-06, val loss: 0.12029632180929184\n",
      "Epoch 2827: train loss: 8.249571692431346e-06, val loss: 0.12077174335718155\n",
      "Epoch 2828: train loss: 5.391114882513648e-06, val loss: 0.12120848894119263\n",
      "Epoch 2829: train loss: 7.56847202865174e-06, val loss: 0.12095308303833008\n",
      "Epoch 2830: train loss: 3.5033478980039945e-06, val loss: 0.12060310691595078\n",
      "Epoch 2831: train loss: 6.70879262543167e-06, val loss: 0.1208096295595169\n",
      "Epoch 2832: train loss: 2.8057747840648517e-06, val loss: 0.12116897106170654\n",
      "Epoch 2833: train loss: 5.000205874239327e-06, val loss: 0.12091430276632309\n",
      "Epoch 2834: train loss: 3.266638032073388e-06, val loss: 0.12061978876590729\n",
      "Epoch 2835: train loss: 2.8942547487531556e-06, val loss: 0.1209527999162674\n",
      "Epoch 2836: train loss: 3.5980244774691528e-06, val loss: 0.12108178436756134\n",
      "Epoch 2837: train loss: 1.826623019951512e-06, val loss: 0.12079013884067535\n",
      "Epoch 2838: train loss: 3.2161528906726744e-06, val loss: 0.12106849998235703\n",
      "Epoch 2839: train loss: 1.2550016208479065e-06, val loss: 0.12147696316242218\n",
      "Epoch 2840: train loss: 2.7957535166933667e-06, val loss: 0.12113314121961594\n",
      "Epoch 2841: train loss: 1.1005341775671695e-06, val loss: 0.12100031226873398\n",
      "Epoch 2842: train loss: 1.9465237528493162e-06, val loss: 0.12148445099592209\n",
      "Epoch 2843: train loss: 1.3645453691424336e-06, val loss: 0.12149401009082794\n",
      "Epoch 2844: train loss: 1.1024507102774805e-06, val loss: 0.12133697420358658\n",
      "Epoch 2845: train loss: 1.5535431430180324e-06, val loss: 0.12177328020334244\n",
      "Epoch 2846: train loss: 5.644001817017852e-07, val loss: 0.12199421226978302\n",
      "Epoch 2847: train loss: 1.36935921091208e-06, val loss: 0.12167160958051682\n",
      "Epoch 2848: train loss: 6.572534516635642e-07, val loss: 0.12173249572515488\n",
      "Epoch 2849: train loss: 7.869131195548107e-07, val loss: 0.12201135605573654\n",
      "Epoch 2850: train loss: 8.070562103057455e-07, val loss: 0.12183433026075363\n",
      "Epoch 2851: train loss: 5.437326535684406e-07, val loss: 0.12172317504882812\n",
      "Epoch 2852: train loss: 5.878140996173897e-07, val loss: 0.12197954952716827\n",
      "Epoch 2853: train loss: 5.860596843376698e-07, val loss: 0.12205401808023453\n",
      "Epoch 2854: train loss: 3.891483686402353e-07, val loss: 0.12196207046508789\n",
      "Epoch 2855: train loss: 4.241001647642406e-07, val loss: 0.1220824345946312\n",
      "Epoch 2856: train loss: 4.843894316763908e-07, val loss: 0.12215306609869003\n",
      "Epoch 2857: train loss: 2.2190832282831252e-07, val loss: 0.1220536008477211\n",
      "Epoch 2858: train loss: 4.194841949356487e-07, val loss: 0.12212321907281876\n",
      "Epoch 2859: train loss: 2.3464752985091764e-07, val loss: 0.12219011038541794\n",
      "Epoch 2860: train loss: 2.493132456038438e-07, val loss: 0.12212715297937393\n",
      "Epoch 2861: train loss: 2.8325422363195685e-07, val loss: 0.12223093956708908\n",
      "Epoch 2862: train loss: 1.6065484942373587e-07, val loss: 0.122336246073246\n",
      "Epoch 2863: train loss: 2.473560414273379e-07, val loss: 0.12228609621524811\n",
      "Epoch 2864: train loss: 1.2654288639168954e-07, val loss: 0.12233159691095352\n",
      "Epoch 2865: train loss: 1.8986189331826608e-07, val loss: 0.12242908775806427\n",
      "Epoch 2866: train loss: 1.4326255382002273e-07, val loss: 0.12240252643823624\n",
      "Epoch 2867: train loss: 9.965996383698439e-08, val loss: 0.12242770195007324\n",
      "Epoch 2868: train loss: 1.7538700092245563e-07, val loss: 0.12255709618330002\n",
      "Epoch 2869: train loss: 6.493449689060071e-08, val loss: 0.12257621437311172\n",
      "Epoch 2870: train loss: 1.1202541827515233e-07, val loss: 0.12259838730096817\n",
      "Epoch 2871: train loss: 1.0733543120977629e-07, val loss: 0.12266054004430771\n",
      "Epoch 2872: train loss: 6.125411289303884e-08, val loss: 0.12270116060972214\n",
      "Epoch 2873: train loss: 8.481312363528559e-08, val loss: 0.1227157935500145\n",
      "Epoch 2874: train loss: 6.920475925653591e-08, val loss: 0.12276291847229004\n",
      "Epoch 2875: train loss: 6.45657962650148e-08, val loss: 0.12280821055173874\n",
      "Epoch 2876: train loss: 4.932055830408899e-08, val loss: 0.1228577122092247\n",
      "Epoch 2877: train loss: 7.474074692481736e-08, val loss: 0.12289851158857346\n",
      "Epoch 2878: train loss: 8.986625488205391e-08, val loss: 0.12298236042261124\n",
      "Epoch 2879: train loss: 1.4098586120780965e-07, val loss: 0.12294836342334747\n",
      "Epoch 2880: train loss: 3.697129500324081e-07, val loss: 0.12303882837295532\n",
      "Epoch 2881: train loss: 5.343288194126217e-07, val loss: 0.12303830683231354\n",
      "Epoch 2882: train loss: 4.2137870082115114e-07, val loss: 0.12311787903308868\n",
      "Epoch 2883: train loss: 2.056897727698015e-07, val loss: 0.12313248962163925\n",
      "Epoch 2884: train loss: 4.814328491420383e-08, val loss: 0.12318927049636841\n",
      "Epoch 2885: train loss: 8.771774417937195e-08, val loss: 0.12325415760278702\n",
      "Epoch 2886: train loss: 2.2790783305026707e-07, val loss: 0.12322928756475449\n",
      "Epoch 2887: train loss: 3.2049081255536294e-07, val loss: 0.12335159629583359\n",
      "Epoch 2888: train loss: 2.8469764856708935e-07, val loss: 0.12333094328641891\n",
      "Epoch 2889: train loss: 1.4879405796364154e-07, val loss: 0.12336025387048721\n",
      "Epoch 2890: train loss: 5.240293177166677e-08, val loss: 0.12345228344202042\n",
      "Epoch 2891: train loss: 2.217259442716113e-08, val loss: 0.1234695240855217\n",
      "Epoch 2892: train loss: 6.758858717148541e-08, val loss: 0.12351109832525253\n",
      "Epoch 2893: train loss: 1.528946285134225e-07, val loss: 0.12354963272809982\n",
      "Epoch 2894: train loss: 1.8866978734877193e-07, val loss: 0.12361013889312744\n",
      "Epoch 2895: train loss: 1.7581794509169413e-07, val loss: 0.12361575663089752\n",
      "Epoch 2896: train loss: 1.246509953034547e-07, val loss: 0.12368982285261154\n",
      "Epoch 2897: train loss: 6.734985191769738e-08, val loss: 0.12369811534881592\n",
      "Epoch 2898: train loss: 2.870373094765455e-08, val loss: 0.12375650554895401\n",
      "Epoch 2899: train loss: 1.1252776488390737e-08, val loss: 0.12379820644855499\n",
      "Epoch 2900: train loss: 1.9795239225572914e-08, val loss: 0.1238318458199501\n",
      "Epoch 2901: train loss: 4.9179678995869835e-08, val loss: 0.12389638274908066\n",
      "Epoch 2902: train loss: 7.97943258135092e-08, val loss: 0.12389222532510757\n",
      "Epoch 2903: train loss: 1.0421339879940206e-07, val loss: 0.12397104501724243\n",
      "Epoch 2904: train loss: 1.249152603577386e-07, val loss: 0.12395959347486496\n",
      "Epoch 2905: train loss: 1.348439013781899e-07, val loss: 0.12404777854681015\n",
      "Epoch 2906: train loss: 1.482223979110131e-07, val loss: 0.12403883039951324\n",
      "Epoch 2907: train loss: 1.6770694344359072e-07, val loss: 0.1241239532828331\n",
      "Epoch 2908: train loss: 2.037836139834326e-07, val loss: 0.12410175055265427\n",
      "Epoch 2909: train loss: 1.8298904080893408e-07, val loss: 0.12417230755090714\n",
      "Epoch 2910: train loss: 2.2369681573763955e-07, val loss: 0.1241440549492836\n",
      "Epoch 2911: train loss: 2.485813581643015e-07, val loss: 0.12412872165441513\n",
      "Epoch 2912: train loss: 3.2016123441280797e-07, val loss: 0.12409645318984985\n",
      "Epoch 2913: train loss: 4.2197774519081577e-07, val loss: 0.1241026520729065\n",
      "Epoch 2914: train loss: 6.118856958892138e-07, val loss: 0.12408977001905441\n",
      "Epoch 2915: train loss: 9.358681154481019e-07, val loss: 0.12409581243991852\n",
      "Epoch 2916: train loss: 1.5093130514287623e-06, val loss: 0.12404153496026993\n",
      "Epoch 2917: train loss: 2.5227957394236e-06, val loss: 0.12401054054498672\n",
      "Epoch 2918: train loss: 4.384021849546116e-06, val loss: 0.12399238348007202\n",
      "Epoch 2919: train loss: 7.869925866543781e-06, val loss: 0.12405981868505478\n",
      "Epoch 2920: train loss: 1.4675794773211237e-05, val loss: 0.12396781891584396\n",
      "Epoch 2921: train loss: 2.8253507480258122e-05, val loss: 0.12401288747787476\n",
      "Epoch 2922: train loss: 5.613366738543846e-05, val loss: 0.12395894527435303\n",
      "Epoch 2923: train loss: 0.00011508974421303719, val loss: 0.123954176902771\n",
      "Epoch 2924: train loss: 0.00023420776415150613, val loss: 0.12368104606866837\n",
      "Epoch 2925: train loss: 0.00040256534703075886, val loss: 0.12418973445892334\n",
      "Epoch 2926: train loss: 0.000575295474845916, val loss: 0.1232949048280716\n",
      "Epoch 2927: train loss: 0.0007095063920132816, val loss: 0.12413311004638672\n",
      "Epoch 2928: train loss: 0.0007447190582752228, val loss: 0.12302912771701813\n",
      "Epoch 2929: train loss: 0.0006376042147167027, val loss: 0.12369637936353683\n",
      "Epoch 2930: train loss: 0.0004049735434819013, val loss: 0.12231240421533585\n",
      "Epoch 2931: train loss: 0.00015837261162232608, val loss: 0.12215707451105118\n",
      "Epoch 2932: train loss: 2.520261477911845e-05, val loss: 0.1221301332116127\n",
      "Epoch 2933: train loss: 4.904731395072304e-05, val loss: 0.12033801525831223\n",
      "Epoch 2934: train loss: 0.00015051545051392168, val loss: 0.1215900331735611\n",
      "Epoch 2935: train loss: 0.0002154353423975408, val loss: 0.11964159458875656\n",
      "Epoch 2936: train loss: 0.0001902497751871124, val loss: 0.12012453377246857\n",
      "Epoch 2937: train loss: 0.00010563259274931625, val loss: 0.11922796070575714\n",
      "Epoch 2938: train loss: 4.121503297938034e-05, val loss: 0.11873909085988998\n",
      "Epoch 2939: train loss: 3.975758590968326e-05, val loss: 0.1190345287322998\n",
      "Epoch 2940: train loss: 7.35347275622189e-05, val loss: 0.11806974560022354\n",
      "Epoch 2941: train loss: 9.309502638643607e-05, val loss: 0.11829373985528946\n",
      "Epoch 2942: train loss: 8.162392623489723e-05, val loss: 0.117706298828125\n",
      "Epoch 2943: train loss: 5.107924516778439e-05, val loss: 0.11730172485113144\n",
      "Epoch 2944: train loss: 2.7229256374994293e-05, val loss: 0.1176723763346672\n",
      "Epoch 2945: train loss: 2.7955104087595828e-05, val loss: 0.11663953214883804\n",
      "Epoch 2946: train loss: 4.2444473365321755e-05, val loss: 0.11725155264139175\n",
      "Epoch 2947: train loss: 4.870609336649068e-05, val loss: 0.11650659888982773\n",
      "Epoch 2948: train loss: 3.777263918891549e-05, val loss: 0.11650409549474716\n",
      "Epoch 2949: train loss: 2.0057554138475098e-05, val loss: 0.11713339388370514\n",
      "Epoch 2950: train loss: 1.5294637705665082e-05, val loss: 0.11579958349466324\n",
      "Epoch 2951: train loss: 2.5285138690378517e-05, val loss: 0.11674034595489502\n",
      "Epoch 2952: train loss: 3.053908221772872e-05, val loss: 0.11593823879957199\n",
      "Epoch 2953: train loss: 2.1573516278294846e-05, val loss: 0.11601188033819199\n",
      "Epoch 2954: train loss: 1.0967151865770575e-05, val loss: 0.11632397025823593\n",
      "Epoch 2955: train loss: 1.0187197403865866e-05, val loss: 0.11530536413192749\n",
      "Epoch 2956: train loss: 1.655150845181197e-05, val loss: 0.11632349342107773\n",
      "Epoch 2957: train loss: 2.023860542976763e-05, val loss: 0.11547527462244034\n",
      "Epoch 2958: train loss: 1.5663077647332102e-05, val loss: 0.11594527214765549\n",
      "Epoch 2959: train loss: 7.66277480579447e-06, val loss: 0.11586284637451172\n",
      "Epoch 2960: train loss: 4.053766588185681e-06, val loss: 0.11534660309553146\n",
      "Epoch 2961: train loss: 6.151798970677191e-06, val loss: 0.11605536192655563\n",
      "Epoch 2962: train loss: 1.0068121810036246e-05, val loss: 0.11534304916858673\n",
      "Epoch 2963: train loss: 1.2315181265876163e-05, val loss: 0.1158982515335083\n",
      "Epoch 2964: train loss: 1.135048660216853e-05, val loss: 0.11550243943929672\n",
      "Epoch 2965: train loss: 7.380916031252127e-06, val loss: 0.1156853437423706\n",
      "Epoch 2966: train loss: 3.028421360795619e-06, val loss: 0.1156301498413086\n",
      "Epoch 2967: train loss: 9.390942068421282e-07, val loss: 0.11559303104877472\n",
      "Epoch 2968: train loss: 2.0188324469927466e-06, val loss: 0.11589107662439346\n",
      "Epoch 2969: train loss: 5.104734555061441e-06, val loss: 0.1155405044555664\n",
      "Epoch 2970: train loss: 7.656692105229013e-06, val loss: 0.11599171161651611\n",
      "Epoch 2971: train loss: 7.838224519218784e-06, val loss: 0.11550793796777725\n",
      "Epoch 2972: train loss: 5.856617462995928e-06, val loss: 0.11605971306562424\n",
      "Epoch 2973: train loss: 3.5922137158195255e-06, val loss: 0.11560254544019699\n",
      "Epoch 2974: train loss: 2.3451464130630484e-06, val loss: 0.11586473137140274\n",
      "Epoch 2975: train loss: 2.304381496287533e-06, val loss: 0.11577353626489639\n",
      "Epoch 2976: train loss: 2.884707782868645e-06, val loss: 0.11579375714063644\n",
      "Epoch 2977: train loss: 3.2822117645991966e-06, val loss: 0.11589228361845016\n",
      "Epoch 2978: train loss: 3.094990006502485e-06, val loss: 0.11582618206739426\n",
      "Epoch 2979: train loss: 2.4814910375425825e-06, val loss: 0.11592669785022736\n",
      "Epoch 2980: train loss: 1.89107788628462e-06, val loss: 0.11586596816778183\n",
      "Epoch 2981: train loss: 1.3931669400335522e-06, val loss: 0.11598140001296997\n",
      "Epoch 2982: train loss: 1.0612345704430481e-06, val loss: 0.11597242206335068\n",
      "Epoch 2983: train loss: 8.969393547886284e-07, val loss: 0.11597702652215958\n",
      "Epoch 2984: train loss: 8.732058631721884e-07, val loss: 0.1159830242395401\n",
      "Epoch 2985: train loss: 9.27485245938442e-07, val loss: 0.11607503145933151\n",
      "Epoch 2986: train loss: 1.1461468147899723e-06, val loss: 0.11605685949325562\n",
      "Epoch 2987: train loss: 1.5842891798456549e-06, val loss: 0.1161237508058548\n",
      "Epoch 2988: train loss: 2.233651457572705e-06, val loss: 0.11609990894794464\n",
      "Epoch 2989: train loss: 3.280451892351266e-06, val loss: 0.11615898460149765\n",
      "Epoch 2990: train loss: 5.134842012921581e-06, val loss: 0.11617455631494522\n",
      "Epoch 2991: train loss: 8.65276342665311e-06, val loss: 0.11623426526784897\n",
      "Epoch 2992: train loss: 1.5599309335811995e-05, val loss: 0.11619651317596436\n",
      "Epoch 2993: train loss: 3.0000317565281875e-05, val loss: 0.11638875305652618\n",
      "Epoch 2994: train loss: 6.071781535865739e-05, val loss: 0.11609649658203125\n",
      "Epoch 2995: train loss: 0.0001240070560015738, val loss: 0.11676879972219467\n",
      "Epoch 2996: train loss: 0.00025015586288645864, val loss: 0.11554741114377975\n",
      "Epoch 2997: train loss: 0.00048458934179507196, val loss: 0.11837568134069443\n",
      "Epoch 2998: train loss: 0.0008980552665889263, val loss: 0.11295981705188751\n",
      "Epoch 2999: train loss: 0.0014566099271178246, val loss: 0.1207590103149414\n",
      "Epoch 3000: train loss: 0.002006263704970479, val loss: 0.10982277244329453\n",
      "Epoch 3001: train loss: 0.00230996310710907, val loss: 0.12105386704206467\n",
      "Epoch 3002: train loss: 0.002031981945037842, val loss: 0.10959213972091675\n",
      "Epoch 3003: train loss: 0.001226694555953145, val loss: 0.11287428438663483\n",
      "Epoch 3004: train loss: 0.0005911932094022632, val loss: 0.11240708827972412\n",
      "Epoch 3005: train loss: 0.0006692574243061244, val loss: 0.10282867401838303\n",
      "Epoch 3006: train loss: 0.0010618448723107576, val loss: 0.11064916104078293\n",
      "Epoch 3007: train loss: 0.0009534911951050162, val loss: 0.09968017041683197\n",
      "Epoch 3008: train loss: 0.0003625985700637102, val loss: 0.0999753326177597\n",
      "Epoch 3009: train loss: 0.00015240017091855407, val loss: 0.10254959017038345\n",
      "Epoch 3010: train loss: 0.000452248816145584, val loss: 0.09420548379421234\n",
      "Epoch 3011: train loss: 0.0005443071713671088, val loss: 0.09843969345092773\n",
      "Epoch 3012: train loss: 0.0002675019786693156, val loss: 0.09715782850980759\n",
      "Epoch 3013: train loss: 0.0001427653623977676, val loss: 0.09420105814933777\n",
      "Epoch 3014: train loss: 0.0002424290869385004, val loss: 0.09811955690383911\n",
      "Epoch 3015: train loss: 0.00025944519438780844, val loss: 0.09498053044080734\n",
      "Epoch 3016: train loss: 0.00016831269022077322, val loss: 0.0949995145201683\n",
      "Epoch 3017: train loss: 0.00013440832844935358, val loss: 0.09819918870925903\n",
      "Epoch 3018: train loss: 0.0001457570615457371, val loss: 0.09564098715782166\n",
      "Epoch 3019: train loss: 0.0001338666770607233, val loss: 0.09728028625249863\n",
      "Epoch 3020: train loss: 0.00010971993469865993, val loss: 0.09954603761434555\n",
      "Epoch 3021: train loss: 9.989742829930037e-05, val loss: 0.0971987247467041\n",
      "Epoch 3022: train loss: 8.777771290624514e-05, val loss: 0.09915807843208313\n",
      "Epoch 3023: train loss: 7.29364764993079e-05, val loss: 0.09984458982944489\n",
      "Epoch 3024: train loss: 7.737559644738212e-05, val loss: 0.09746742248535156\n",
      "Epoch 3025: train loss: 7.005010411376134e-05, val loss: 0.09890305995941162\n",
      "Epoch 3026: train loss: 4.7041761717991903e-05, val loss: 0.09915024787187576\n",
      "Epoch 3027: train loss: 4.7897425247356296e-05, val loss: 0.09758185595273972\n",
      "Epoch 3028: train loss: 5.5709289881633595e-05, val loss: 0.09880664199590683\n",
      "Epoch 3029: train loss: 4.254503801348619e-05, val loss: 0.09903781861066818\n",
      "Epoch 3030: train loss: 2.709945874812547e-05, val loss: 0.09800850600004196\n",
      "Epoch 3031: train loss: 3.52191855199635e-05, val loss: 0.0987967774271965\n",
      "Epoch 3032: train loss: 3.984046998084523e-05, val loss: 0.09863807260990143\n",
      "Epoch 3033: train loss: 2.0480021703406237e-05, val loss: 0.0980471670627594\n",
      "Epoch 3034: train loss: 2.0092333215870894e-05, val loss: 0.09866231679916382\n",
      "Epoch 3035: train loss: 3.0266934118117206e-05, val loss: 0.09819706529378891\n",
      "Epoch 3036: train loss: 1.752627758833114e-05, val loss: 0.09818336367607117\n",
      "Epoch 3037: train loss: 1.4445431588683277e-05, val loss: 0.09895025938749313\n",
      "Epoch 3038: train loss: 2.0852636225754395e-05, val loss: 0.09818609058856964\n",
      "Epoch 3039: train loss: 1.2388381037453655e-05, val loss: 0.09821440279483795\n",
      "Epoch 3040: train loss: 1.1917964911845047e-05, val loss: 0.09866033494472504\n",
      "Epoch 3041: train loss: 1.6281366697512567e-05, val loss: 0.09788675606250763\n",
      "Epoch 3042: train loss: 7.99616736912867e-06, val loss: 0.09848198294639587\n",
      "Epoch 3043: train loss: 7.476890459656715e-06, val loss: 0.09890422970056534\n",
      "Epoch 3044: train loss: 1.3421675248537213e-05, val loss: 0.09814541041851044\n",
      "Epoch 3045: train loss: 7.399482001346769e-06, val loss: 0.09878694266080856\n",
      "Epoch 3046: train loss: 3.562369784049224e-06, val loss: 0.09880286455154419\n",
      "Epoch 3047: train loss: 8.900329703465104e-06, val loss: 0.09828048944473267\n",
      "Epoch 3048: train loss: 7.721248948655557e-06, val loss: 0.0992494747042656\n",
      "Epoch 3049: train loss: 2.8309839308349183e-06, val loss: 0.099126435816288\n",
      "Epoch 3050: train loss: 4.725710368802538e-06, val loss: 0.09889396280050278\n",
      "Epoch 3051: train loss: 6.284969003900187e-06, val loss: 0.09986850619316101\n",
      "Epoch 3052: train loss: 3.144972197333118e-06, val loss: 0.09968075901269913\n",
      "Epoch 3053: train loss: 2.721717919484945e-06, val loss: 0.0998455286026001\n",
      "Epoch 3054: train loss: 4.523369625530904e-06, val loss: 0.10066461563110352\n",
      "Epoch 3055: train loss: 3.06649349113286e-06, val loss: 0.10054922103881836\n",
      "Epoch 3056: train loss: 1.5112231039893231e-06, val loss: 0.10116562992334366\n",
      "Epoch 3057: train loss: 2.903648919527768e-06, val loss: 0.10178106278181076\n",
      "Epoch 3058: train loss: 3.2754048788774526e-06, val loss: 0.10142423957586288\n",
      "Epoch 3059: train loss: 1.3368444342631847e-06, val loss: 0.10164438933134079\n",
      "Epoch 3060: train loss: 9.363710660181823e-07, val loss: 0.1015772596001625\n",
      "Epoch 3061: train loss: 2.4237529032689054e-06, val loss: 0.10126657783985138\n",
      "Epoch 3062: train loss: 2.227590357506415e-06, val loss: 0.1015140563249588\n",
      "Epoch 3063: train loss: 6.360896236401459e-07, val loss: 0.10132427513599396\n",
      "Epoch 3064: train loss: 7.323350814658625e-07, val loss: 0.10116582363843918\n",
      "Epoch 3065: train loss: 1.7648105767875677e-06, val loss: 0.10135133564472198\n",
      "Epoch 3066: train loss: 1.3798381814922323e-06, val loss: 0.10108838230371475\n",
      "Epoch 3067: train loss: 4.6033545686441357e-07, val loss: 0.10108234733343124\n",
      "Epoch 3068: train loss: 6.746055873918522e-07, val loss: 0.10118963569402695\n",
      "Epoch 3069: train loss: 1.1857424624395208e-06, val loss: 0.1009177565574646\n",
      "Epoch 3070: train loss: 8.104216249193996e-07, val loss: 0.10103350132703781\n",
      "Epoch 3071: train loss: 3.120722453786584e-07, val loss: 0.10096254199743271\n",
      "Epoch 3072: train loss: 5.522219908016268e-07, val loss: 0.10078936070203781\n",
      "Epoch 3073: train loss: 8.917467084756936e-07, val loss: 0.10098719596862793\n",
      "Epoch 3074: train loss: 6.030230679243687e-07, val loss: 0.10071684420108795\n",
      "Epoch 3075: train loss: 1.3583509428372054e-07, val loss: 0.1007000207901001\n",
      "Epoch 3076: train loss: 2.4617710892016476e-07, val loss: 0.10080639272928238\n",
      "Epoch 3077: train loss: 6.44268652649771e-07, val loss: 0.1005476862192154\n",
      "Epoch 3078: train loss: 6.245808208404924e-07, val loss: 0.10070474445819855\n",
      "Epoch 3079: train loss: 2.913189689479623e-07, val loss: 0.10059326142072678\n",
      "Epoch 3080: train loss: 1.2347663869149983e-07, val loss: 0.10047619789838791\n",
      "Epoch 3081: train loss: 2.5448485985180014e-07, val loss: 0.10055899620056152\n",
      "Epoch 3082: train loss: 3.8969164961599745e-07, val loss: 0.10038907825946808\n",
      "Epoch 3083: train loss: 3.274565472111135e-07, val loss: 0.10047616064548492\n",
      "Epoch 3084: train loss: 2.0947007328686595e-07, val loss: 0.1003691777586937\n",
      "Epoch 3085: train loss: 1.6027659910378134e-07, val loss: 0.10037704557180405\n",
      "Epoch 3086: train loss: 1.762597179322256e-07, val loss: 0.10035349428653717\n",
      "Epoch 3087: train loss: 1.6240068134720786e-07, val loss: 0.10028520971536636\n",
      "Epoch 3088: train loss: 1.0393505789352275e-07, val loss: 0.10036146640777588\n",
      "Epoch 3089: train loss: 9.551020241360675e-08, val loss: 0.10034885257482529\n",
      "Epoch 3090: train loss: 1.5870438119236496e-07, val loss: 0.10046210139989853\n",
      "Epoch 3091: train loss: 2.7222645826441294e-07, val loss: 0.1003808006644249\n",
      "Epoch 3092: train loss: 3.9464563883484516e-07, val loss: 0.10057218372821808\n",
      "Epoch 3093: train loss: 5.490951480169315e-07, val loss: 0.1004277691245079\n",
      "Epoch 3094: train loss: 8.819844197205384e-07, val loss: 0.10069022327661514\n",
      "Epoch 3095: train loss: 1.707167825770739e-06, val loss: 0.10045943409204483\n",
      "Epoch 3096: train loss: 2.998919853780535e-06, val loss: 0.10080822557210922\n",
      "Epoch 3097: train loss: 3.4628319554030895e-06, val loss: 0.10071054846048355\n",
      "Epoch 3098: train loss: 2.8998733796470333e-06, val loss: 0.10070957988500595\n",
      "Epoch 3099: train loss: 2.8492049750639126e-06, val loss: 0.10110318660736084\n",
      "Epoch 3100: train loss: 3.859720891341567e-06, val loss: 0.10078670084476471\n",
      "Epoch 3101: train loss: 5.359881015465362e-06, val loss: 0.10131988674402237\n",
      "Epoch 3102: train loss: 6.931324605830014e-06, val loss: 0.10093235969543457\n",
      "Epoch 3103: train loss: 8.655206329422072e-06, val loss: 0.10152699798345566\n",
      "Epoch 3104: train loss: 1.1547484064067248e-05, val loss: 0.10114187002182007\n",
      "Epoch 3105: train loss: 1.6710026102373376e-05, val loss: 0.10175216197967529\n",
      "Epoch 3106: train loss: 2.5811512387008406e-05, val loss: 0.1012258529663086\n",
      "Epoch 3107: train loss: 3.898836803273298e-05, val loss: 0.10213261097669601\n",
      "Epoch 3108: train loss: 5.7859149819705635e-05, val loss: 0.10125946998596191\n",
      "Epoch 3109: train loss: 8.829807484289631e-05, val loss: 0.1026764065027237\n",
      "Epoch 3110: train loss: 0.00014068576274439692, val loss: 0.1010698676109314\n",
      "Epoch 3111: train loss: 0.00023355864686891437, val loss: 0.10325773060321808\n",
      "Epoch 3112: train loss: 0.00039672054117545485, val loss: 0.100724957883358\n",
      "Epoch 3113: train loss: 0.0006825586315244436, val loss: 0.10424917191267014\n",
      "Epoch 3114: train loss: 0.0011136751854792237, val loss: 0.10056185722351074\n",
      "Epoch 3115: train loss: 0.0016971845179796219, val loss: 0.10189080238342285\n",
      "Epoch 3116: train loss: 0.002289576455950737, val loss: 0.10186109691858292\n",
      "Epoch 3117: train loss: 0.0026271657552570105, val loss: 0.09845259040594101\n",
      "Epoch 3118: train loss: 0.002186816418543458, val loss: 0.10182342678308487\n",
      "Epoch 3119: train loss: 0.0010116370394825935, val loss: 0.09970460087060928\n",
      "Epoch 3120: train loss: 0.00012272081221453846, val loss: 0.09758692234754562\n",
      "Epoch 3121: train loss: 0.00034964419319294393, val loss: 0.09970442205667496\n",
      "Epoch 3122: train loss: 0.0008701804908923805, val loss: 0.0952012687921524\n",
      "Epoch 3123: train loss: 0.0006758139352314174, val loss: 0.0974351167678833\n",
      "Epoch 3124: train loss: 0.0001980651286430657, val loss: 0.09804399311542511\n",
      "Epoch 3125: train loss: 0.00022705612354911864, val loss: 0.0953950509428978\n",
      "Epoch 3126: train loss: 0.0004228515026625246, val loss: 0.09751478582620621\n",
      "Epoch 3127: train loss: 0.00030674980371259153, val loss: 0.09522910416126251\n",
      "Epoch 3128: train loss: 0.00016068604600150138, val loss: 0.09422890841960907\n",
      "Epoch 3129: train loss: 0.0001879466144600883, val loss: 0.09601923078298569\n",
      "Epoch 3130: train loss: 0.00020952515478711575, val loss: 0.09330018609762192\n",
      "Epoch 3131: train loss: 0.00017449160804972053, val loss: 0.09391017258167267\n",
      "Epoch 3132: train loss: 0.0001178296297439374, val loss: 0.09492328763008118\n",
      "Epoch 3133: train loss: 0.00010696724348235875, val loss: 0.09330437332391739\n",
      "Epoch 3134: train loss: 0.0001459852937841788, val loss: 0.09442387521266937\n",
      "Epoch 3135: train loss: 9.731170575832948e-05, val loss: 0.0943264365196228\n",
      "Epoch 3136: train loss: 5.714775397791527e-05, val loss: 0.09348960965871811\n",
      "Epoch 3137: train loss: 0.00010539465438341722, val loss: 0.09450801461935043\n",
      "Epoch 3138: train loss: 7.747263589408249e-05, val loss: 0.09387873858213425\n",
      "Epoch 3139: train loss: 3.786865636357106e-05, val loss: 0.09323738515377045\n",
      "Epoch 3140: train loss: 7.139708759495988e-05, val loss: 0.0943586677312851\n",
      "Epoch 3141: train loss: 5.833115938003175e-05, val loss: 0.09365858137607574\n",
      "Epoch 3142: train loss: 3.247418499086052e-05, val loss: 0.09310945123434067\n",
      "Epoch 3143: train loss: 4.46787744294852e-05, val loss: 0.09397243708372116\n",
      "Epoch 3144: train loss: 4.2213458073092625e-05, val loss: 0.09384077787399292\n",
      "Epoch 3145: train loss: 3.0320275982376188e-05, val loss: 0.0936930701136589\n",
      "Epoch 3146: train loss: 2.7517389753484167e-05, val loss: 0.09452298283576965\n",
      "Epoch 3147: train loss: 2.9129674658179283e-05, val loss: 0.09464042633771896\n",
      "Epoch 3148: train loss: 2.7158168450114317e-05, val loss: 0.09377818554639816\n",
      "Epoch 3149: train loss: 1.7408996427548118e-05, val loss: 0.09404914826154709\n",
      "Epoch 3150: train loss: 1.9722514480235986e-05, val loss: 0.0943777859210968\n",
      "Epoch 3151: train loss: 2.301299900864251e-05, val loss: 0.09403377026319504\n",
      "Epoch 3152: train loss: 1.1747662938432768e-05, val loss: 0.09445269405841827\n",
      "Epoch 3153: train loss: 1.2909618817502633e-05, val loss: 0.09470317512750626\n",
      "Epoch 3154: train loss: 1.8567197912489064e-05, val loss: 0.09429634362459183\n",
      "Epoch 3155: train loss: 8.82535914570326e-06, val loss: 0.09408127516508102\n",
      "Epoch 3156: train loss: 8.33445301395841e-06, val loss: 0.09432438760995865\n",
      "Epoch 3157: train loss: 1.420010084984824e-05, val loss: 0.09433737397193909\n",
      "Epoch 3158: train loss: 6.680287697236054e-06, val loss: 0.09420260041952133\n",
      "Epoch 3159: train loss: 5.972990948066581e-06, val loss: 0.09455593675374985\n",
      "Epoch 3160: train loss: 1.0337862477172166e-05, val loss: 0.09456083178520203\n",
      "Epoch 3161: train loss: 5.074946784588974e-06, val loss: 0.09420027583837509\n",
      "Epoch 3162: train loss: 4.650736173061887e-06, val loss: 0.09445536136627197\n",
      "Epoch 3163: train loss: 7.26005691831233e-06, val loss: 0.094521664083004\n",
      "Epoch 3164: train loss: 3.848227152047912e-06, val loss: 0.09410959482192993\n",
      "Epoch 3165: train loss: 3.7598158542095916e-06, val loss: 0.0943470448255539\n",
      "Epoch 3166: train loss: 5.017846433474915e-06, val loss: 0.09467253088951111\n",
      "Epoch 3167: train loss: 2.9464258659572806e-06, val loss: 0.094303660094738\n",
      "Epoch 3168: train loss: 2.8812930850108387e-06, val loss: 0.09455486387014389\n",
      "Epoch 3169: train loss: 3.5398036288825097e-06, val loss: 0.09481392055749893\n",
      "Epoch 3170: train loss: 2.45935939346964e-06, val loss: 0.09422097355127335\n",
      "Epoch 3171: train loss: 2.0550126009766245e-06, val loss: 0.0945814847946167\n",
      "Epoch 3172: train loss: 2.365326281505986e-06, val loss: 0.0948353037238121\n",
      "Epoch 3173: train loss: 2.112685251631774e-06, val loss: 0.09428500384092331\n",
      "Epoch 3174: train loss: 1.5760911082907114e-06, val loss: 0.0947687029838562\n",
      "Epoch 3175: train loss: 1.4569831137123401e-06, val loss: 0.09491860866546631\n",
      "Epoch 3176: train loss: 1.717644067866786e-06, val loss: 0.09448444843292236\n",
      "Epoch 3177: train loss: 1.3862620562576922e-06, val loss: 0.09488944709300995\n",
      "Epoch 3178: train loss: 8.804306617093971e-07, val loss: 0.09487556666135788\n",
      "Epoch 3179: train loss: 1.1366829539838363e-06, val loss: 0.09452995657920837\n",
      "Epoch 3180: train loss: 1.246279225597391e-06, val loss: 0.0949387177824974\n",
      "Epoch 3181: train loss: 7.250454814311524e-07, val loss: 0.09494242817163467\n",
      "Epoch 3182: train loss: 6.505317742266925e-07, val loss: 0.09470146149396896\n",
      "Epoch 3183: train loss: 9.476569857724826e-07, val loss: 0.095063216984272\n",
      "Epoch 3184: train loss: 6.975388942009886e-07, val loss: 0.09495094418525696\n",
      "Epoch 3185: train loss: 4.1798031702455773e-07, val loss: 0.09492311626672745\n",
      "Epoch 3186: train loss: 6.185603069752688e-07, val loss: 0.09530022740364075\n",
      "Epoch 3187: train loss: 5.987183726574585e-07, val loss: 0.09520260244607925\n",
      "Epoch 3188: train loss: 3.3280718980677193e-07, val loss: 0.09526780247688293\n",
      "Epoch 3189: train loss: 4.0958218505693367e-07, val loss: 0.09543599933385849\n",
      "Epoch 3190: train loss: 4.929746637571952e-07, val loss: 0.09541092067956924\n",
      "Epoch 3191: train loss: 2.801752998493612e-07, val loss: 0.09547106176614761\n",
      "Epoch 3192: train loss: 2.2459366277871595e-07, val loss: 0.09560654312372208\n",
      "Epoch 3193: train loss: 3.6283884696786117e-07, val loss: 0.09564852714538574\n",
      "Epoch 3194: train loss: 3.0519103688675386e-07, val loss: 0.09570364654064178\n",
      "Epoch 3195: train loss: 1.4819582361269568e-07, val loss: 0.09585168957710266\n",
      "Epoch 3196: train loss: 1.784389240810924e-07, val loss: 0.09588287770748138\n",
      "Epoch 3197: train loss: 2.6009564635387505e-07, val loss: 0.09592768549919128\n",
      "Epoch 3198: train loss: 1.946525713947267e-07, val loss: 0.09606396406888962\n",
      "Epoch 3199: train loss: 1.1902764640581154e-07, val loss: 0.0960768610239029\n",
      "Epoch 3200: train loss: 1.393082129652612e-07, val loss: 0.09614495933055878\n",
      "Epoch 3201: train loss: 1.541929606219128e-07, val loss: 0.09628784656524658\n",
      "Epoch 3202: train loss: 1.2041797958772804e-07, val loss: 0.09623457491397858\n",
      "Epoch 3203: train loss: 1.050097537813599e-07, val loss: 0.09639417380094528\n",
      "Epoch 3204: train loss: 1.236165587670257e-07, val loss: 0.09639819711446762\n",
      "Epoch 3205: train loss: 1.0131426364523577e-07, val loss: 0.09642380475997925\n",
      "Epoch 3206: train loss: 6.27161114152841e-08, val loss: 0.09648678451776505\n",
      "Epoch 3207: train loss: 7.369498433718036e-08, val loss: 0.09647919237613678\n",
      "Epoch 3208: train loss: 1.088213537059346e-07, val loss: 0.09651890397071838\n",
      "Epoch 3209: train loss: 1.269565075290302e-07, val loss: 0.09657054394483566\n",
      "Epoch 3210: train loss: 1.1735056659745169e-07, val loss: 0.09647782891988754\n",
      "Epoch 3211: train loss: 1.3824784161897696e-07, val loss: 0.0965854749083519\n",
      "Epoch 3212: train loss: 2.3379143954116444e-07, val loss: 0.09641604125499725\n",
      "Epoch 3213: train loss: 4.5364149059423653e-07, val loss: 0.0966339036822319\n",
      "Epoch 3214: train loss: 9.371584042128234e-07, val loss: 0.09636123478412628\n",
      "Epoch 3215: train loss: 1.7552858935232507e-06, val loss: 0.09676647931337357\n",
      "Epoch 3216: train loss: 3.488710490273661e-06, val loss: 0.09618120640516281\n",
      "Epoch 3217: train loss: 6.989175290073035e-06, val loss: 0.09699424356222153\n",
      "Epoch 3218: train loss: 1.4477384866040666e-05, val loss: 0.09580124169588089\n",
      "Epoch 3219: train loss: 2.9913477192167193e-05, val loss: 0.09757360070943832\n",
      "Epoch 3220: train loss: 6.279181980062276e-05, val loss: 0.09513461589813232\n",
      "Epoch 3221: train loss: 0.00013310366193763912, val loss: 0.09844521433115005\n",
      "Epoch 3222: train loss: 0.00028460027533583343, val loss: 0.09424491971731186\n",
      "Epoch 3223: train loss: 0.0005906243459321558, val loss: 0.10063029825687408\n",
      "Epoch 3224: train loss: 0.0012128157541155815, val loss: 0.09639810770750046\n",
      "Epoch 3225: train loss: 0.002151284134015441, val loss: 0.10541403293609619\n",
      "Epoch 3226: train loss: 0.0033372105099260807, val loss: 0.09708520770072937\n",
      "Epoch 3227: train loss: 0.003934849984943867, val loss: 0.10656299442052841\n",
      "Epoch 3228: train loss: 0.002953258575871587, val loss: 0.0976942852139473\n",
      "Epoch 3229: train loss: 0.0009729243465699255, val loss: 0.09708928316831589\n",
      "Epoch 3230: train loss: 6.724220293108374e-05, val loss: 0.10077004879713058\n",
      "Epoch 3231: train loss: 0.0009778117528185248, val loss: 0.09556795656681061\n",
      "Epoch 3232: train loss: 0.001372975530102849, val loss: 0.09663497656583786\n",
      "Epoch 3233: train loss: 0.0003581724304240197, val loss: 0.09687723964452744\n",
      "Epoch 3234: train loss: 0.00021006655879318714, val loss: 0.09293901920318604\n",
      "Epoch 3235: train loss: 0.0008122195722535253, val loss: 0.09454598277807236\n",
      "Epoch 3236: train loss: 0.00038600535481236875, val loss: 0.09246040880680084\n",
      "Epoch 3237: train loss: 0.00013904165825806558, val loss: 0.08808982372283936\n",
      "Epoch 3238: train loss: 0.00048290641279891133, val loss: 0.09066953510046005\n",
      "Epoch 3239: train loss: 0.0002286181115778163, val loss: 0.08974498510360718\n",
      "Epoch 3240: train loss: 0.0001659906847635284, val loss: 0.08639563620090485\n",
      "Epoch 3241: train loss: 0.00030174467246979475, val loss: 0.0894717127084732\n",
      "Epoch 3242: train loss: 0.00011495998478494585, val loss: 0.09096194058656693\n",
      "Epoch 3243: train loss: 0.00018287218699697405, val loss: 0.08727334439754486\n",
      "Epoch 3244: train loss: 0.00016524015518371016, val loss: 0.08751070499420166\n",
      "Epoch 3245: train loss: 8.173858077498153e-05, val loss: 0.0894988477230072\n",
      "Epoch 3246: train loss: 0.00016331674123648554, val loss: 0.08698039501905441\n",
      "Epoch 3247: train loss: 7.280912541318685e-05, val loss: 0.08636044710874557\n",
      "Epoch 3248: train loss: 7.898162584751844e-05, val loss: 0.08896078169345856\n",
      "Epoch 3249: train loss: 0.00011860828089993447, val loss: 0.08789663761854172\n",
      "Epoch 3250: train loss: 3.09773713524919e-05, val loss: 0.08627618849277496\n",
      "Epoch 3251: train loss: 8.235839050030336e-05, val loss: 0.08815321326255798\n",
      "Epoch 3252: train loss: 6.505723285954446e-05, val loss: 0.08858051151037216\n",
      "Epoch 3253: train loss: 2.684958053578157e-05, val loss: 0.08671339601278305\n",
      "Epoch 3254: train loss: 6.914396362844855e-05, val loss: 0.08716540783643723\n",
      "Epoch 3255: train loss: 2.9930284654255956e-05, val loss: 0.08816242963075638\n",
      "Epoch 3256: train loss: 3.455913974903524e-05, val loss: 0.0869266614317894\n",
      "Epoch 3257: train loss: 3.8520458474522457e-05, val loss: 0.08673489838838577\n",
      "Epoch 3258: train loss: 2.639910417201463e-05, val loss: 0.08815621584653854\n",
      "Epoch 3259: train loss: 2.6737759981187992e-05, val loss: 0.08796518296003342\n",
      "Epoch 3260: train loss: 2.2845588318887167e-05, val loss: 0.08719432353973389\n",
      "Epoch 3261: train loss: 2.5314926460850984e-05, val loss: 0.0878157764673233\n",
      "Epoch 3262: train loss: 1.4584972632292192e-05, val loss: 0.08806225657463074\n",
      "Epoch 3263: train loss: 2.0042654796270654e-05, val loss: 0.08751609921455383\n",
      "Epoch 3264: train loss: 1.6530439097550698e-05, val loss: 0.08797625452280045\n",
      "Epoch 3265: train loss: 1.2178132237750106e-05, val loss: 0.08847694844007492\n",
      "Epoch 3266: train loss: 1.3747809134656563e-05, val loss: 0.08792666345834732\n",
      "Epoch 3267: train loss: 1.2081180102541111e-05, val loss: 0.08791584521532059\n",
      "Epoch 3268: train loss: 1.0239697076031007e-05, val loss: 0.08854147046804428\n",
      "Epoch 3269: train loss: 7.777461178193334e-06, val loss: 0.08842432498931885\n",
      "Epoch 3270: train loss: 1.1348312909831293e-05, val loss: 0.08825471252202988\n",
      "Epoch 3271: train loss: 6.145296083559515e-06, val loss: 0.08880452066659927\n",
      "Epoch 3272: train loss: 5.446932846098207e-06, val loss: 0.08895038068294525\n",
      "Epoch 3273: train loss: 9.890602086670697e-06, val loss: 0.08861847966909409\n",
      "Epoch 3274: train loss: 2.728610979829682e-06, val loss: 0.08875874429941177\n",
      "Epoch 3275: train loss: 5.127923486725194e-06, val loss: 0.0890766978263855\n",
      "Epoch 3276: train loss: 7.251255738083273e-06, val loss: 0.08898105472326279\n",
      "Epoch 3277: train loss: 1.1212326853637933e-06, val loss: 0.08902575820684433\n",
      "Epoch 3278: train loss: 5.238364792603534e-06, val loss: 0.08936866372823715\n",
      "Epoch 3279: train loss: 3.95724191548652e-06, val loss: 0.08944319933652878\n",
      "Epoch 3280: train loss: 1.7073718936444493e-06, val loss: 0.08935068547725677\n",
      "Epoch 3281: train loss: 3.79712469111837e-06, val loss: 0.08947926759719849\n",
      "Epoch 3282: train loss: 2.1465652935148682e-06, val loss: 0.08967945724725723\n",
      "Epoch 3283: train loss: 2.3983245682757115e-06, val loss: 0.08973465114831924\n",
      "Epoch 3284: train loss: 1.9978413092758274e-06, val loss: 0.08971772342920303\n",
      "Epoch 3285: train loss: 1.6906337805266958e-06, val loss: 0.08982747048139572\n",
      "Epoch 3286: train loss: 2.242539494545781e-06, val loss: 0.08996523171663284\n",
      "Epoch 3287: train loss: 9.946234058588743e-07, val loss: 0.09000877290964127\n",
      "Epoch 3288: train loss: 1.410514641975169e-06, val loss: 0.09013307094573975\n",
      "Epoch 3289: train loss: 1.6967494502750924e-06, val loss: 0.09033125638961792\n",
      "Epoch 3290: train loss: 7.317581207644253e-07, val loss: 0.09050346165895462\n",
      "Epoch 3291: train loss: 1.0037448419097927e-06, val loss: 0.09065410494804382\n",
      "Epoch 3292: train loss: 1.2203560117995949e-06, val loss: 0.091058149933815\n",
      "Epoch 3293: train loss: 7.006157147770864e-07, val loss: 0.09122289717197418\n",
      "Epoch 3294: train loss: 6.354001698127831e-07, val loss: 0.09119328111410141\n",
      "Epoch 3295: train loss: 8.198504133360984e-07, val loss: 0.09153808653354645\n",
      "Epoch 3296: train loss: 6.930460472176492e-07, val loss: 0.09182291477918625\n",
      "Epoch 3297: train loss: 3.937462054182106e-07, val loss: 0.09207257628440857\n",
      "Epoch 3298: train loss: 5.717308795283316e-07, val loss: 0.0926150232553482\n",
      "Epoch 3299: train loss: 5.615633540401177e-07, val loss: 0.09279781579971313\n",
      "Epoch 3300: train loss: 2.98898839901085e-07, val loss: 0.0927881971001625\n",
      "Epoch 3301: train loss: 4.474911179386254e-07, val loss: 0.09315286576747894\n",
      "Epoch 3302: train loss: 3.391802465557703e-07, val loss: 0.09339404106140137\n",
      "Epoch 3303: train loss: 2.5712952833600866e-07, val loss: 0.0934649333357811\n",
      "Epoch 3304: train loss: 3.9140820717875613e-07, val loss: 0.09374434500932693\n",
      "Epoch 3305: train loss: 1.9664199157887197e-07, val loss: 0.093956857919693\n",
      "Epoch 3306: train loss: 1.705192715917292e-07, val loss: 0.09415801614522934\n",
      "Epoch 3307: train loss: 3.407561450785579e-07, val loss: 0.09446901082992554\n",
      "Epoch 3308: train loss: 1.5753748527913558e-07, val loss: 0.09466436505317688\n",
      "Epoch 3309: train loss: 9.365147235484983e-08, val loss: 0.09481753408908844\n",
      "Epoch 3310: train loss: 2.3127113024656865e-07, val loss: 0.09503056854009628\n",
      "Epoch 3311: train loss: 1.5596192781686113e-07, val loss: 0.09519365429878235\n",
      "Epoch 3312: train loss: 9.29677810290741e-08, val loss: 0.09537156671285629\n",
      "Epoch 3313: train loss: 1.3379630559029465e-07, val loss: 0.09563758224248886\n",
      "Epoch 3314: train loss: 1.0950854090197026e-07, val loss: 0.0958818569779396\n",
      "Epoch 3315: train loss: 9.729762240340278e-08, val loss: 0.09601558744907379\n",
      "Epoch 3316: train loss: 1.0671143257923177e-07, val loss: 0.09621214121580124\n",
      "Epoch 3317: train loss: 6.639508853822917e-08, val loss: 0.09649132937192917\n",
      "Epoch 3318: train loss: 5.975794437063087e-08, val loss: 0.09661569446325302\n",
      "Epoch 3319: train loss: 8.892966718576645e-08, val loss: 0.09676449745893478\n",
      "Epoch 3320: train loss: 7.527698642206815e-08, val loss: 0.09703028202056885\n",
      "Epoch 3321: train loss: 4.126338026821941e-08, val loss: 0.097154401242733\n",
      "Epoch 3322: train loss: 4.232127537306951e-08, val loss: 0.09732281416654587\n",
      "Epoch 3323: train loss: 5.189582097386847e-08, val loss: 0.09760859608650208\n",
      "Epoch 3324: train loss: 5.734860408779241e-08, val loss: 0.09776759147644043\n",
      "Epoch 3325: train loss: 4.2092551666428335e-08, val loss: 0.09793728590011597\n",
      "Epoch 3326: train loss: 1.9391768191212577e-08, val loss: 0.09817082434892654\n",
      "Epoch 3327: train loss: 3.179093255312182e-08, val loss: 0.09833692759275436\n",
      "Epoch 3328: train loss: 4.4949032940166944e-08, val loss: 0.09850854426622391\n",
      "Epoch 3329: train loss: 2.9107017240903588e-08, val loss: 0.09869592636823654\n",
      "Epoch 3330: train loss: 2.101789320363423e-08, val loss: 0.09887168556451797\n",
      "Epoch 3331: train loss: 2.3348032840431188e-08, val loss: 0.09905362129211426\n",
      "Epoch 3332: train loss: 1.9617020896589565e-08, val loss: 0.09926032274961472\n",
      "Epoch 3333: train loss: 1.7433054111393176e-08, val loss: 0.09945043176412582\n",
      "Epoch 3334: train loss: 2.4266514131454642e-08, val loss: 0.09959913790225983\n",
      "Epoch 3335: train loss: 2.5360749944525196e-08, val loss: 0.09980390965938568\n",
      "Epoch 3336: train loss: 1.1200674165934288e-08, val loss: 0.09997721761465073\n",
      "Epoch 3337: train loss: 6.509761618644916e-09, val loss: 0.10011281073093414\n",
      "Epoch 3338: train loss: 1.4603966036474958e-08, val loss: 0.1003381535410881\n",
      "Epoch 3339: train loss: 1.6349124720704822e-08, val loss: 0.10050298273563385\n",
      "Epoch 3340: train loss: 1.2755175582412903e-08, val loss: 0.10065608471632004\n",
      "Epoch 3341: train loss: 1.2007965288773903e-08, val loss: 0.10086306184530258\n",
      "Epoch 3342: train loss: 1.256082970968464e-08, val loss: 0.10100924968719482\n",
      "Epoch 3343: train loss: 8.84035511461434e-09, val loss: 0.10117695480585098\n",
      "Epoch 3344: train loss: 6.811194719347213e-09, val loss: 0.10135132074356079\n",
      "Epoch 3345: train loss: 7.328210038792804e-09, val loss: 0.10149337351322174\n",
      "Epoch 3346: train loss: 7.223953435442354e-09, val loss: 0.10165810585021973\n",
      "Epoch 3347: train loss: 9.9101686856784e-09, val loss: 0.10182709991931915\n",
      "Epoch 3348: train loss: 1.1013506551194041e-08, val loss: 0.1019800454378128\n",
      "Epoch 3349: train loss: 1.0220611024180926e-08, val loss: 0.10214393585920334\n",
      "Epoch 3350: train loss: 7.232829002390417e-09, val loss: 0.10231103748083115\n",
      "Epoch 3351: train loss: 9.277210999414365e-09, val loss: 0.1024443656206131\n",
      "Epoch 3352: train loss: 9.51109324631716e-09, val loss: 0.10261747986078262\n",
      "Epoch 3353: train loss: 1.1257258236696543e-08, val loss: 0.10276468843221664\n",
      "Epoch 3354: train loss: 1.1927495435770652e-08, val loss: 0.10290954262018204\n",
      "Epoch 3355: train loss: 2.0631564012774106e-08, val loss: 0.10307115316390991\n",
      "Epoch 3356: train loss: 3.4428431661126524e-08, val loss: 0.10320603102445602\n",
      "Epoch 3357: train loss: 6.753305825668576e-08, val loss: 0.10335328429937363\n",
      "Epoch 3358: train loss: 1.339340514050491e-07, val loss: 0.1035110279917717\n",
      "Epoch 3359: train loss: 2.9883773322580964e-07, val loss: 0.10365509241819382\n",
      "Epoch 3360: train loss: 6.691507792311313e-07, val loss: 0.10379929840564728\n",
      "Epoch 3361: train loss: 1.5941942592689884e-06, val loss: 0.10395693778991699\n",
      "Epoch 3362: train loss: 3.875226411764743e-06, val loss: 0.10407944023609161\n",
      "Epoch 3363: train loss: 9.805830814002547e-06, val loss: 0.1042461171746254\n",
      "Epoch 3364: train loss: 2.5205394194927067e-05, val loss: 0.10440852493047714\n",
      "Epoch 3365: train loss: 6.655086326645687e-05, val loss: 0.10446667671203613\n",
      "Epoch 3366: train loss: 0.00017346476670354605, val loss: 0.10477451235055923\n",
      "Epoch 3367: train loss: 0.00040273877675645053, val loss: 0.10518630594015121\n",
      "Epoch 3368: train loss: 0.0008704885840415955, val loss: 0.10554371029138565\n",
      "Epoch 3369: train loss: 0.001751937554217875, val loss: 0.10744893550872803\n",
      "Epoch 3370: train loss: 0.0030954163521528244, val loss: 0.10645017772912979\n",
      "Epoch 3371: train loss: 0.004301728680729866, val loss: 0.10471508651971817\n",
      "Epoch 3372: train loss: 0.0038040082436054945, val loss: 0.09758210927248001\n",
      "Epoch 3373: train loss: 0.0017509603640064597, val loss: 0.09735389053821564\n",
      "Epoch 3374: train loss: 0.00132399657741189, val loss: 0.09422183036804199\n",
      "Epoch 3375: train loss: 0.0012986348010599613, val loss: 0.0865577757358551\n",
      "Epoch 3376: train loss: 0.0008333914447575808, val loss: 0.08536803722381592\n",
      "Epoch 3377: train loss: 0.0011532383505254984, val loss: 0.08332961797714233\n",
      "Epoch 3378: train loss: 0.0007011665147729218, val loss: 0.08120568841695786\n",
      "Epoch 3379: train loss: 0.0005082647548988461, val loss: 0.08679851144552231\n",
      "Epoch 3380: train loss: 0.0006635240279138088, val loss: 0.08465557545423508\n",
      "Epoch 3381: train loss: 0.0005300125339999795, val loss: 0.08379824459552765\n",
      "Epoch 3382: train loss: 0.000270505843218416, val loss: 0.08865063637495041\n",
      "Epoch 3383: train loss: 0.0005286617088131607, val loss: 0.08652620762586594\n",
      "Epoch 3384: train loss: 0.00023231138766277581, val loss: 0.08324513584375381\n",
      "Epoch 3385: train loss: 0.00030658606556244195, val loss: 0.08714757859706879\n",
      "Epoch 3386: train loss: 0.0002882312110159546, val loss: 0.08885248005390167\n",
      "Epoch 3387: train loss: 0.00016632046026643366, val loss: 0.0846056118607521\n",
      "Epoch 3388: train loss: 0.00024887488689273596, val loss: 0.08363349735736847\n",
      "Epoch 3389: train loss: 0.0001350599341094494, val loss: 0.08654191344976425\n",
      "Epoch 3390: train loss: 0.00017729376850184053, val loss: 0.08522500097751617\n",
      "Epoch 3391: train loss: 0.00013119805953465402, val loss: 0.0827442854642868\n",
      "Epoch 3392: train loss: 0.00011540571722434834, val loss: 0.08385702222585678\n",
      "Epoch 3393: train loss: 0.00012581404007505625, val loss: 0.08481388539075851\n",
      "Epoch 3394: train loss: 7.598940283060074e-05, val loss: 0.08293183892965317\n",
      "Epoch 3395: train loss: 0.00010907962860073894, val loss: 0.08162534981966019\n",
      "Epoch 3396: train loss: 5.715660881833173e-05, val loss: 0.08224841952323914\n",
      "Epoch 3397: train loss: 8.999354031402618e-05, val loss: 0.0825740173459053\n",
      "Epoch 3398: train loss: 4.682266808231361e-05, val loss: 0.08231525868177414\n",
      "Epoch 3399: train loss: 6.989869143581018e-05, val loss: 0.08218508958816528\n",
      "Epoch 3400: train loss: 4.0304756112163886e-05, val loss: 0.0818910002708435\n",
      "Epoch 3401: train loss: 5.66356502531562e-05, val loss: 0.08165158331394196\n",
      "Epoch 3402: train loss: 3.069313970627263e-05, val loss: 0.08171121031045914\n",
      "Epoch 3403: train loss: 4.734053436550312e-05, val loss: 0.08132940530776978\n",
      "Epoch 3404: train loss: 2.411090645182412e-05, val loss: 0.08090884238481522\n",
      "Epoch 3405: train loss: 3.875164475175552e-05, val loss: 0.08146263659000397\n",
      "Epoch 3406: train loss: 1.9177261492586695e-05, val loss: 0.08173786848783493\n",
      "Epoch 3407: train loss: 3.1294232030631974e-05, val loss: 0.08116328716278076\n",
      "Epoch 3408: train loss: 1.680071909504477e-05, val loss: 0.08129718154668808\n",
      "Epoch 3409: train loss: 2.235738793388009e-05, val loss: 0.08182486146688461\n",
      "Epoch 3410: train loss: 1.8020344214164652e-05, val loss: 0.08130815625190735\n",
      "Epoch 3411: train loss: 1.3586013665189967e-05, val loss: 0.08105552941560745\n",
      "Epoch 3412: train loss: 1.7979204130824655e-05, val loss: 0.0818038359284401\n",
      "Epoch 3413: train loss: 9.490051525062881e-06, val loss: 0.08174414932727814\n",
      "Epoch 3414: train loss: 1.4915317478880752e-05, val loss: 0.08118357509374619\n",
      "Epoch 3415: train loss: 7.875216397223994e-06, val loss: 0.08172543346881866\n",
      "Epoch 3416: train loss: 1.2062718269589823e-05, val loss: 0.08224377036094666\n",
      "Epoch 3417: train loss: 6.3758320720808115e-06, val loss: 0.08179090917110443\n",
      "Epoch 3418: train loss: 9.726665666676126e-06, val loss: 0.08181780576705933\n",
      "Epoch 3419: train loss: 5.333323315426242e-06, val loss: 0.08240284025669098\n",
      "Epoch 3420: train loss: 7.900133823568467e-06, val loss: 0.08228658884763718\n",
      "Epoch 3421: train loss: 4.2957622099493165e-06, val loss: 0.082126684486866\n",
      "Epoch 3422: train loss: 6.418828434107127e-06, val loss: 0.0825614482164383\n",
      "Epoch 3423: train loss: 3.7763418276881566e-06, val loss: 0.08257652074098587\n",
      "Epoch 3424: train loss: 4.427571184351109e-06, val loss: 0.08225651830434799\n",
      "Epoch 3425: train loss: 4.274096681911033e-06, val loss: 0.08277004957199097\n",
      "Epoch 3426: train loss: 2.452559101584484e-06, val loss: 0.08330713212490082\n",
      "Epoch 3427: train loss: 4.178387825959362e-06, val loss: 0.08284785598516464\n",
      "Epoch 3428: train loss: 1.9092465208814247e-06, val loss: 0.08272057771682739\n",
      "Epoch 3429: train loss: 3.1394845336762955e-06, val loss: 0.08349255472421646\n",
      "Epoch 3430: train loss: 1.8815222802004428e-06, val loss: 0.08358940482139587\n",
      "Epoch 3431: train loss: 2.2758722479920834e-06, val loss: 0.0830574780702591\n",
      "Epoch 3432: train loss: 1.7492644701633253e-06, val loss: 0.0833650454878807\n",
      "Epoch 3433: train loss: 1.7715187823341694e-06, val loss: 0.08391829580068588\n",
      "Epoch 3434: train loss: 1.3454263125822763e-06, val loss: 0.08371394872665405\n",
      "Epoch 3435: train loss: 1.6731520418034052e-06, val loss: 0.08362691104412079\n",
      "Epoch 3436: train loss: 8.938135920288914e-07, val loss: 0.08402308821678162\n",
      "Epoch 3437: train loss: 1.352415438304888e-06, val loss: 0.0841485783457756\n",
      "Epoch 3438: train loss: 9.890666206047172e-07, val loss: 0.08413149416446686\n",
      "Epoch 3439: train loss: 7.281541911652312e-07, val loss: 0.08431321382522583\n",
      "Epoch 3440: train loss: 1.1188952839802369e-06, val loss: 0.08443187177181244\n",
      "Epoch 3441: train loss: 4.7418291160283843e-07, val loss: 0.08450382202863693\n",
      "Epoch 3442: train loss: 8.508825430908473e-07, val loss: 0.08465947210788727\n",
      "Epoch 3443: train loss: 5.744598183810012e-07, val loss: 0.08474131673574448\n",
      "Epoch 3444: train loss: 4.4394033693606616e-07, val loss: 0.08481775969266891\n",
      "Epoch 3445: train loss: 6.966642445149773e-07, val loss: 0.08499448001384735\n",
      "Epoch 3446: train loss: 2.3922191871861287e-07, val loss: 0.08511101454496384\n",
      "Epoch 3447: train loss: 5.763007493442274e-07, val loss: 0.08519267290830612\n",
      "Epoch 3448: train loss: 2.7461143758955586e-07, val loss: 0.08531435579061508\n",
      "Epoch 3449: train loss: 3.700412207763293e-07, val loss: 0.08537556231021881\n",
      "Epoch 3450: train loss: 2.9286454150678765e-07, val loss: 0.0854525938630104\n",
      "Epoch 3451: train loss: 2.7506126798471087e-07, val loss: 0.08563228696584702\n",
      "Epoch 3452: train loss: 2.4435846057713206e-07, val loss: 0.08572053164243698\n",
      "Epoch 3453: train loss: 2.2401526678095252e-07, val loss: 0.0857231616973877\n",
      "Epoch 3454: train loss: 2.074726381806613e-07, val loss: 0.085877425968647\n",
      "Epoch 3455: train loss: 1.790832726555891e-07, val loss: 0.0860331803560257\n",
      "Epoch 3456: train loss: 1.7545023922593828e-07, val loss: 0.08602965623140335\n",
      "Epoch 3457: train loss: 1.343412350252038e-07, val loss: 0.08614363521337509\n",
      "Epoch 3458: train loss: 1.4433832973281824e-07, val loss: 0.08634068071842194\n",
      "Epoch 3459: train loss: 1.357816898917008e-07, val loss: 0.08634816855192184\n",
      "Epoch 3460: train loss: 8.061440581741408e-08, val loss: 0.08639632165431976\n",
      "Epoch 3461: train loss: 1.3506183904610225e-07, val loss: 0.08659650385379791\n",
      "Epoch 3462: train loss: 7.303943760916809e-08, val loss: 0.08666640520095825\n",
      "Epoch 3463: train loss: 8.530064121714531e-08, val loss: 0.08669109642505646\n",
      "Epoch 3464: train loss: 8.762928871419717e-08, val loss: 0.08684370666742325\n",
      "Epoch 3465: train loss: 4.478575377220295e-08, val loss: 0.08696919679641724\n",
      "Epoch 3466: train loss: 8.741292134573087e-08, val loss: 0.08703260868787766\n",
      "Epoch 3467: train loss: 4.421207933091864e-08, val loss: 0.08713004738092422\n",
      "Epoch 3468: train loss: 4.888008575676395e-08, val loss: 0.08723779022693634\n",
      "Epoch 3469: train loss: 5.6648620017085705e-08, val loss: 0.0873323306441307\n",
      "Epoch 3470: train loss: 3.492656119874482e-08, val loss: 0.08742900937795639\n",
      "Epoch 3471: train loss: 4.17842187516726e-08, val loss: 0.08752862364053726\n",
      "Epoch 3472: train loss: 3.039839668872446e-08, val loss: 0.08763141185045242\n",
      "Epoch 3473: train loss: 3.684277771753841e-08, val loss: 0.08773469179868698\n",
      "Epoch 3474: train loss: 2.842372026634621e-08, val loss: 0.08783083409070969\n",
      "Epoch 3475: train loss: 2.272109256296062e-08, val loss: 0.08792945742607117\n",
      "Epoch 3476: train loss: 3.055063402257474e-08, val loss: 0.08803451061248779\n",
      "Epoch 3477: train loss: 1.7563325016567433e-08, val loss: 0.08812450617551804\n",
      "Epoch 3478: train loss: 1.934883897547479e-08, val loss: 0.08820308744907379\n",
      "Epoch 3479: train loss: 2.404511967313283e-08, val loss: 0.08830870687961578\n",
      "Epoch 3480: train loss: 1.3221482575431764e-08, val loss: 0.08840525150299072\n",
      "Epoch 3481: train loss: 1.6916180456405527e-08, val loss: 0.08848360925912857\n",
      "Epoch 3482: train loss: 1.4043324725321327e-08, val loss: 0.0885738655924797\n",
      "Epoch 3483: train loss: 1.5064891556448856e-08, val loss: 0.08866623789072037\n",
      "Epoch 3484: train loss: 1.1891719609025131e-08, val loss: 0.08876674622297287\n",
      "Epoch 3485: train loss: 8.73352146157913e-09, val loss: 0.0888557955622673\n",
      "Epoch 3486: train loss: 1.4307978801753052e-08, val loss: 0.08893799781799316\n",
      "Epoch 3487: train loss: 7.863580897549127e-09, val loss: 0.08904267847537994\n",
      "Epoch 3488: train loss: 8.16031686667884e-09, val loss: 0.08913714438676834\n",
      "Epoch 3489: train loss: 1.1041364267327936e-08, val loss: 0.08921472728252411\n",
      "Epoch 3490: train loss: 6.509755401395978e-09, val loss: 0.08931469172239304\n",
      "Epoch 3491: train loss: 5.728392427073459e-09, val loss: 0.08941959589719772\n",
      "Epoch 3492: train loss: 9.130503464405137e-09, val loss: 0.08949928730726242\n",
      "Epoch 3493: train loss: 6.317072642758603e-09, val loss: 0.089588962495327\n",
      "Epoch 3494: train loss: 4.456930380314361e-09, val loss: 0.08969065546989441\n",
      "Epoch 3495: train loss: 7.269364665773992e-09, val loss: 0.08977243304252625\n",
      "Epoch 3496: train loss: 5.224379595603068e-09, val loss: 0.08986978977918625\n",
      "Epoch 3497: train loss: 4.187442836922628e-09, val loss: 0.0899646207690239\n",
      "Epoch 3498: train loss: 5.471342046092786e-09, val loss: 0.09004249423742294\n",
      "Epoch 3499: train loss: 4.453897695100295e-09, val loss: 0.09014568477869034\n",
      "Epoch 3500: train loss: 4.59122428964065e-09, val loss: 0.09023813158273697\n",
      "Epoch 3501: train loss: 4.396655484129042e-09, val loss: 0.09032236039638519\n",
      "Epoch 3502: train loss: 3.445556950865125e-09, val loss: 0.09042197465896606\n",
      "Epoch 3503: train loss: 4.141389453593547e-09, val loss: 0.09051103889942169\n",
      "Epoch 3504: train loss: 3.866541309349714e-09, val loss: 0.09060575067996979\n",
      "Epoch 3505: train loss: 2.944768429102851e-09, val loss: 0.09070707857608795\n",
      "Epoch 3506: train loss: 3.926555969258061e-09, val loss: 0.09079743176698685\n",
      "Epoch 3507: train loss: 3.739694331983401e-09, val loss: 0.09089633077383041\n",
      "Epoch 3508: train loss: 2.771483709196332e-09, val loss: 0.09099668264389038\n",
      "Epoch 3509: train loss: 3.198591169706333e-09, val loss: 0.09108857065439224\n",
      "Epoch 3510: train loss: 3.3663301035602444e-09, val loss: 0.09119205921888351\n",
      "Epoch 3511: train loss: 2.805436993824628e-09, val loss: 0.09128760546445847\n",
      "Epoch 3512: train loss: 2.7952657966068273e-09, val loss: 0.09138566255569458\n",
      "Epoch 3513: train loss: 3.034543949453905e-09, val loss: 0.09148837625980377\n",
      "Epoch 3514: train loss: 2.8193571921519833e-09, val loss: 0.09158321470022202\n",
      "Epoch 3515: train loss: 2.8141504682110963e-09, val loss: 0.09169342368841171\n",
      "Epoch 3516: train loss: 2.9516447064281692e-09, val loss: 0.09178858995437622\n",
      "Epoch 3517: train loss: 2.761513462345988e-09, val loss: 0.09188178181648254\n",
      "Epoch 3518: train loss: 2.5922943791556463e-09, val loss: 0.09198013693094254\n",
      "Epoch 3519: train loss: 2.787672759296811e-09, val loss: 0.09207460284233093\n",
      "Epoch 3520: train loss: 2.790230935190152e-09, val loss: 0.09217727929353714\n",
      "Epoch 3521: train loss: 2.4499335893324314e-09, val loss: 0.09227675199508667\n",
      "Epoch 3522: train loss: 2.7575337568919167e-09, val loss: 0.09236295521259308\n",
      "Epoch 3523: train loss: 2.671580734414647e-09, val loss: 0.09245490282773972\n",
      "Epoch 3524: train loss: 2.7450208772705764e-09, val loss: 0.09253531694412231\n",
      "Epoch 3525: train loss: 2.892234229889823e-09, val loss: 0.09263507276773453\n",
      "Epoch 3526: train loss: 2.9387365874100624e-09, val loss: 0.0927194282412529\n",
      "Epoch 3527: train loss: 2.843239643723905e-09, val loss: 0.09281589835882187\n",
      "Epoch 3528: train loss: 2.9923070687942754e-09, val loss: 0.09290274977684021\n",
      "Epoch 3529: train loss: 3.2188312015790643e-09, val loss: 0.09299848973751068\n",
      "Epoch 3530: train loss: 4.275706011469538e-09, val loss: 0.09308227151632309\n",
      "Epoch 3531: train loss: 5.41816769228376e-09, val loss: 0.09317739307880402\n",
      "Epoch 3532: train loss: 8.697335296403708e-09, val loss: 0.09326046705245972\n",
      "Epoch 3533: train loss: 1.4772712830790624e-08, val loss: 0.09336753934621811\n",
      "Epoch 3534: train loss: 2.9224493047763644e-08, val loss: 0.09343200922012329\n",
      "Epoch 3535: train loss: 6.013884501498978e-08, val loss: 0.09356491267681122\n",
      "Epoch 3536: train loss: 1.3332974901913985e-07, val loss: 0.09360052645206451\n",
      "Epoch 3537: train loss: 3.02461359069639e-07, val loss: 0.0937875285744667\n",
      "Epoch 3538: train loss: 7.186664561231737e-07, val loss: 0.09373175352811813\n",
      "Epoch 3539: train loss: 1.743667326081777e-06, val loss: 0.09405671805143356\n",
      "Epoch 3540: train loss: 4.347709818830481e-06, val loss: 0.09378446638584137\n",
      "Epoch 3541: train loss: 1.1022558283002581e-05, val loss: 0.09450428187847137\n",
      "Epoch 3542: train loss: 2.8085500161978416e-05, val loss: 0.09362662583589554\n",
      "Epoch 3543: train loss: 7.188682502601296e-05, val loss: 0.09540452063083649\n",
      "Epoch 3544: train loss: 0.00018680178618524224, val loss: 0.09301896393299103\n",
      "Epoch 3545: train loss: 0.00046412035590037704, val loss: 0.09779270738363266\n",
      "Epoch 3546: train loss: 0.0007856590091250837, val loss: 0.09238773584365845\n",
      "Epoch 3547: train loss: 0.0008273759158328176, val loss: 0.0974632129073143\n",
      "Epoch 3548: train loss: 0.0005877837538719177, val loss: 0.09324311465024948\n",
      "Epoch 3549: train loss: 0.00033078575506806374, val loss: 0.09422507882118225\n",
      "Epoch 3550: train loss: 0.0003482322790659964, val loss: 0.09392900764942169\n",
      "Epoch 3551: train loss: 0.0005208620568737388, val loss: 0.09317584335803986\n",
      "Epoch 3552: train loss: 0.0005233552656136453, val loss: 0.09487112611532211\n",
      "Epoch 3553: train loss: 0.0003316385846119374, val loss: 0.09391143172979355\n",
      "Epoch 3554: train loss: 0.00017317107995040715, val loss: 0.0936371311545372\n",
      "Epoch 3555: train loss: 0.00018327638099435717, val loss: 0.09479419142007828\n",
      "Epoch 3556: train loss: 0.00024151471734512597, val loss: 0.09343049675226212\n",
      "Epoch 3557: train loss: 0.00021334462508093566, val loss: 0.09431155771017075\n",
      "Epoch 3558: train loss: 0.00014603471208829433, val loss: 0.09504364430904388\n",
      "Epoch 3559: train loss: 0.00011815772450063378, val loss: 0.09414253383874893\n",
      "Epoch 3560: train loss: 0.00011645641643553972, val loss: 0.0947650671005249\n",
      "Epoch 3561: train loss: 0.00011882803664775565, val loss: 0.09449484199285507\n",
      "Epoch 3562: train loss: 0.00011077444651164114, val loss: 0.09419889748096466\n",
      "Epoch 3563: train loss: 7.804504275554791e-05, val loss: 0.09507790952920914\n",
      "Epoch 3564: train loss: 5.824493564432487e-05, val loss: 0.09485812485218048\n",
      "Epoch 3565: train loss: 7.294667011592537e-05, val loss: 0.09445901960134506\n",
      "Epoch 3566: train loss: 7.84438889240846e-05, val loss: 0.09458800405263901\n",
      "Epoch 3567: train loss: 4.749833169626072e-05, val loss: 0.09469636529684067\n",
      "Epoch 3568: train loss: 3.140760600217618e-05, val loss: 0.09504951536655426\n",
      "Epoch 3569: train loss: 5.173083263798617e-05, val loss: 0.09500410407781601\n",
      "Epoch 3570: train loss: 5.2455703553278e-05, val loss: 0.09473049640655518\n",
      "Epoch 3571: train loss: 2.4437402316834778e-05, val loss: 0.09525325149297714\n",
      "Epoch 3572: train loss: 2.1753661712864414e-05, val loss: 0.09534116089344025\n",
      "Epoch 3573: train loss: 3.9334598113782704e-05, val loss: 0.0953570008277893\n",
      "Epoch 3574: train loss: 3.081279282923788e-05, val loss: 0.09606757014989853\n",
      "Epoch 3575: train loss: 1.2724675798381213e-05, val loss: 0.09548740833997726\n",
      "Epoch 3576: train loss: 1.8666600226424634e-05, val loss: 0.09533596783876419\n",
      "Epoch 3577: train loss: 2.6706336939241737e-05, val loss: 0.09621598571538925\n",
      "Epoch 3578: train loss: 1.731353950162884e-05, val loss: 0.09577824175357819\n",
      "Epoch 3579: train loss: 8.82955555425724e-06, val loss: 0.09606688469648361\n",
      "Epoch 3580: train loss: 1.4125877896731254e-05, val loss: 0.09669244289398193\n",
      "Epoch 3581: train loss: 1.77665933733806e-05, val loss: 0.09585683792829514\n",
      "Epoch 3582: train loss: 9.651508662500419e-06, val loss: 0.09629575908184052\n",
      "Epoch 3583: train loss: 6.070937615731964e-06, val loss: 0.09682057797908783\n",
      "Epoch 3584: train loss: 1.2096093087166082e-05, val loss: 0.09619221836328506\n",
      "Epoch 3585: train loss: 1.0380711501056794e-05, val loss: 0.0966506153345108\n",
      "Epoch 3586: train loss: 4.269730652595172e-06, val loss: 0.09676650911569595\n",
      "Epoch 3587: train loss: 6.6204529503011145e-06, val loss: 0.09634347259998322\n",
      "Epoch 3588: train loss: 9.014876923174597e-06, val loss: 0.09698348492383957\n",
      "Epoch 3589: train loss: 4.8012052502599545e-06, val loss: 0.0969419851899147\n",
      "Epoch 3590: train loss: 2.7805951958725927e-06, val loss: 0.09663625061511993\n",
      "Epoch 3591: train loss: 5.884879556106171e-06, val loss: 0.09715867042541504\n",
      "Epoch 3592: train loss: 6.168119853100507e-06, val loss: 0.09696074575185776\n",
      "Epoch 3593: train loss: 2.392972874076804e-06, val loss: 0.09693410247564316\n",
      "Epoch 3594: train loss: 1.780085995051195e-06, val loss: 0.09740056842565536\n",
      "Epoch 3595: train loss: 4.5201722969068214e-06, val loss: 0.09716630727052689\n",
      "Epoch 3596: train loss: 4.311457814765163e-06, val loss: 0.0973324328660965\n",
      "Epoch 3597: train loss: 1.4805145838181488e-06, val loss: 0.09747254103422165\n",
      "Epoch 3598: train loss: 1.1441969718362088e-06, val loss: 0.09736279398202896\n",
      "Epoch 3599: train loss: 2.9411280593194533e-06, val loss: 0.09767793864011765\n",
      "Epoch 3600: train loss: 2.920756969615468e-06, val loss: 0.09757574647665024\n",
      "Epoch 3601: train loss: 1.3539663541450864e-06, val loss: 0.09770504385232925\n",
      "Epoch 3602: train loss: 1.0318179874957423e-06, val loss: 0.09793040156364441\n",
      "Epoch 3603: train loss: 1.6972503544820938e-06, val loss: 0.09783314913511276\n",
      "Epoch 3604: train loss: 1.6073544202299672e-06, val loss: 0.0980086550116539\n",
      "Epoch 3605: train loss: 1.1132838153571356e-06, val loss: 0.09802507609128952\n",
      "Epoch 3606: train loss: 1.1281505294391536e-06, val loss: 0.09802518039941788\n",
      "Epoch 3607: train loss: 1.2286243418202503e-06, val loss: 0.09831393510103226\n",
      "Epoch 3608: train loss: 8.584387387600145e-07, val loss: 0.09825590997934341\n",
      "Epoch 3609: train loss: 5.421077844403044e-07, val loss: 0.09832099825143814\n",
      "Epoch 3610: train loss: 7.826978958291875e-07, val loss: 0.09845810383558273\n",
      "Epoch 3611: train loss: 1.1035872375941835e-06, val loss: 0.0983690470457077\n",
      "Epoch 3612: train loss: 9.373156331093924e-07, val loss: 0.09866789728403091\n",
      "Epoch 3613: train loss: 4.470522299016011e-07, val loss: 0.09859203547239304\n",
      "Epoch 3614: train loss: 2.1610526346194092e-07, val loss: 0.09874089807271957\n",
      "Epoch 3615: train loss: 4.3462904386615264e-07, val loss: 0.09872125834226608\n",
      "Epoch 3616: train loss: 7.73802298681403e-07, val loss: 0.09884854406118393\n",
      "Epoch 3617: train loss: 8.585389537074661e-07, val loss: 0.09890071302652359\n",
      "Epoch 3618: train loss: 6.598817208214314e-07, val loss: 0.09905186295509338\n",
      "Epoch 3619: train loss: 5.676648129337991e-07, val loss: 0.09892253577709198\n",
      "Epoch 3620: train loss: 8.473114689877548e-07, val loss: 0.09929940849542618\n",
      "Epoch 3621: train loss: 1.614476673239551e-06, val loss: 0.09895485639572144\n",
      "Epoch 3622: train loss: 3.137635530947591e-06, val loss: 0.0996735617518425\n",
      "Epoch 3623: train loss: 6.390100679709576e-06, val loss: 0.09887620061635971\n",
      "Epoch 3624: train loss: 1.3820391359331552e-05, val loss: 0.10027440637350082\n",
      "Epoch 3625: train loss: 3.158441904815845e-05, val loss: 0.09833803027868271\n",
      "Epoch 3626: train loss: 7.510613068006933e-05, val loss: 0.10169488191604614\n",
      "Epoch 3627: train loss: 0.00018308580911252648, val loss: 0.09673096239566803\n",
      "Epoch 3628: train loss: 0.00044557751971296966, val loss: 0.10494404286146164\n",
      "Epoch 3629: train loss: 0.0009798216633498669, val loss: 0.09439007192850113\n",
      "Epoch 3630: train loss: 0.0016692320350557566, val loss: 0.10669026523828506\n",
      "Epoch 3631: train loss: 0.0020521157421171665, val loss: 0.0980415865778923\n",
      "Epoch 3632: train loss: 0.0018175732111558318, val loss: 0.1026422306895256\n",
      "Epoch 3633: train loss: 0.0012326118303462863, val loss: 0.10454404354095459\n",
      "Epoch 3634: train loss: 0.0007587631116621196, val loss: 0.09926199913024902\n",
      "Epoch 3635: train loss: 0.0004957362543791533, val loss: 0.1051461324095726\n",
      "Epoch 3636: train loss: 0.0005719143082387745, val loss: 0.10217050462961197\n",
      "Epoch 3637: train loss: 0.0006037693237885833, val loss: 0.10445892810821533\n",
      "Epoch 3638: train loss: 0.0003506916400510818, val loss: 0.10679962486028671\n",
      "Epoch 3639: train loss: 0.0002689764660317451, val loss: 0.10582621395587921\n",
      "Epoch 3640: train loss: 0.0003673638275358826, val loss: 0.10935472697019577\n",
      "Epoch 3641: train loss: 0.00026901965611614287, val loss: 0.10630407184362411\n",
      "Epoch 3642: train loss: 0.00018683960661292076, val loss: 0.10857144743204117\n",
      "Epoch 3643: train loss: 0.00025349666248075664, val loss: 0.11098285764455795\n",
      "Epoch 3644: train loss: 0.00028616556664928794, val loss: 0.10917848348617554\n",
      "Epoch 3645: train loss: 0.0001541870879009366, val loss: 0.11146505177021027\n",
      "Epoch 3646: train loss: 0.00019626924768090248, val loss: 0.11204709857702255\n",
      "Epoch 3647: train loss: 0.00016474458971060812, val loss: 0.10878278315067291\n",
      "Epoch 3648: train loss: 9.960552415577695e-05, val loss: 0.10975062847137451\n",
      "Epoch 3649: train loss: 0.00016694088117219508, val loss: 0.11285734176635742\n",
      "Epoch 3650: train loss: 8.751743007451296e-05, val loss: 0.11398287117481232\n",
      "Epoch 3651: train loss: 9.00083759916015e-05, val loss: 0.11403819173574448\n",
      "Epoch 3652: train loss: 0.0001239265693584457, val loss: 0.11322035640478134\n",
      "Epoch 3653: train loss: 5.8307508879806846e-05, val loss: 0.11550018936395645\n",
      "Epoch 3654: train loss: 9.237298218067735e-05, val loss: 0.11576900631189346\n",
      "Epoch 3655: train loss: 6.161181227071211e-05, val loss: 0.11551392078399658\n",
      "Epoch 3656: train loss: 5.699901157640852e-05, val loss: 0.11861785501241684\n",
      "Epoch 3657: train loss: 6.590961129404604e-05, val loss: 0.1166817918419838\n",
      "Epoch 3658: train loss: 3.7003981560701504e-05, val loss: 0.11374390125274658\n",
      "Epoch 3659: train loss: 4.953250754624605e-05, val loss: 0.11643703281879425\n",
      "Epoch 3660: train loss: 4.422090569278225e-05, val loss: 0.12026200443506241\n",
      "Epoch 3661: train loss: 2.2847319996799342e-05, val loss: 0.11834754794836044\n",
      "Epoch 3662: train loss: 4.5078420953359455e-05, val loss: 0.1178717389702797\n",
      "Epoch 3663: train loss: 2.1474776076502167e-05, val loss: 0.11827166378498077\n",
      "Epoch 3664: train loss: 2.5622362954891287e-05, val loss: 0.118805430829525\n",
      "Epoch 3665: train loss: 2.8403363103279844e-05, val loss: 0.12033601850271225\n",
      "Epoch 3666: train loss: 1.360567330266349e-05, val loss: 0.12164364010095596\n",
      "Epoch 3667: train loss: 2.5836638087639585e-05, val loss: 0.12205133587121964\n",
      "Epoch 3668: train loss: 1.2281169802008662e-05, val loss: 0.12056183069944382\n",
      "Epoch 3669: train loss: 1.6659289030940272e-05, val loss: 0.11907124519348145\n",
      "Epoch 3670: train loss: 1.4567300240742043e-05, val loss: 0.12105264514684677\n",
      "Epoch 3671: train loss: 1.1360593816789333e-05, val loss: 0.12315124273300171\n",
      "Epoch 3672: train loss: 1.1860775884997565e-05, val loss: 0.12324682623147964\n",
      "Epoch 3673: train loss: 9.190622222376987e-06, val loss: 0.12259893864393234\n",
      "Epoch 3674: train loss: 1.042411895468831e-05, val loss: 0.1213655024766922\n",
      "Epoch 3675: train loss: 7.275620646396419e-06, val loss: 0.12165992707014084\n",
      "Epoch 3676: train loss: 7.5371017373981886e-06, val loss: 0.12295949459075928\n",
      "Epoch 3677: train loss: 7.177823590609478e-06, val loss: 0.12300057709217072\n",
      "Epoch 3678: train loss: 5.667106506734854e-06, val loss: 0.12348566204309464\n",
      "Epoch 3679: train loss: 5.485134352056775e-06, val loss: 0.12381193786859512\n",
      "Epoch 3680: train loss: 5.258807050267933e-06, val loss: 0.12257685512304306\n",
      "Epoch 3681: train loss: 3.886431841237936e-06, val loss: 0.12214744091033936\n",
      "Epoch 3682: train loss: 4.459918272914365e-06, val loss: 0.12343473732471466\n",
      "Epoch 3683: train loss: 3.897017450071871e-06, val loss: 0.12467700242996216\n",
      "Epoch 3684: train loss: 2.3825111838959856e-06, val loss: 0.12413336336612701\n",
      "Epoch 3685: train loss: 3.7295531001291238e-06, val loss: 0.1234162449836731\n",
      "Epoch 3686: train loss: 2.706268787733279e-06, val loss: 0.12361669540405273\n",
      "Epoch 3687: train loss: 1.6610924831184093e-06, val loss: 0.12363433837890625\n",
      "Epoch 3688: train loss: 3.2210155040957034e-06, val loss: 0.12456157058477402\n",
      "Epoch 3689: train loss: 1.1520982070578611e-06, val loss: 0.1255640834569931\n",
      "Epoch 3690: train loss: 2.118137899742578e-06, val loss: 0.12483479082584381\n",
      "Epoch 3691: train loss: 1.870125856839877e-06, val loss: 0.1243262067437172\n",
      "Epoch 3692: train loss: 9.724689107315498e-07, val loss: 0.12444861233234406\n",
      "Epoch 3693: train loss: 1.6523496242371039e-06, val loss: 0.12504695355892181\n",
      "Epoch 3694: train loss: 1.1585077572817681e-06, val loss: 0.12567320466041565\n",
      "Epoch 3695: train loss: 9.830101816987735e-07, val loss: 0.1252947598695755\n",
      "Epoch 3696: train loss: 1.1419941756685148e-06, val loss: 0.12542271614074707\n",
      "Epoch 3697: train loss: 7.976122446962108e-07, val loss: 0.12547142803668976\n",
      "Epoch 3698: train loss: 7.277282634277071e-07, val loss: 0.1256057769060135\n",
      "Epoch 3699: train loss: 9.088493584386015e-07, val loss: 0.12586185336112976\n",
      "Epoch 3700: train loss: 5.618321665679105e-07, val loss: 0.12615714967250824\n",
      "Epoch 3701: train loss: 5.855152949152398e-07, val loss: 0.12635944783687592\n",
      "Epoch 3702: train loss: 6.427982270906796e-07, val loss: 0.12637044489383698\n",
      "Epoch 3703: train loss: 3.6407425341167254e-07, val loss: 0.12658663094043732\n",
      "Epoch 3704: train loss: 4.951992877977318e-07, val loss: 0.12695381045341492\n",
      "Epoch 3705: train loss: 5.021896640755585e-07, val loss: 0.1271549016237259\n",
      "Epoch 3706: train loss: 2.423326748157706e-07, val loss: 0.1273757517337799\n",
      "Epoch 3707: train loss: 3.8257536516539403e-07, val loss: 0.12737233936786652\n",
      "Epoch 3708: train loss: 3.249022881846031e-07, val loss: 0.12728898227214813\n",
      "Epoch 3709: train loss: 2.0276554835163552e-07, val loss: 0.127621129155159\n",
      "Epoch 3710: train loss: 3.169085402987548e-07, val loss: 0.1277959793806076\n",
      "Epoch 3711: train loss: 2.288841756126203e-07, val loss: 0.1278030425310135\n",
      "Epoch 3712: train loss: 1.4966622075007763e-07, val loss: 0.12801720201969147\n",
      "Epoch 3713: train loss: 2.4005191789910896e-07, val loss: 0.12817500531673431\n",
      "Epoch 3714: train loss: 1.6925152124258602e-07, val loss: 0.12823979556560516\n",
      "Epoch 3715: train loss: 9.348894991489942e-08, val loss: 0.1284596472978592\n",
      "Epoch 3716: train loss: 1.6711783246137202e-07, val loss: 0.12867502868175507\n",
      "Epoch 3717: train loss: 1.7242840044673358e-07, val loss: 0.12878407537937164\n",
      "Epoch 3718: train loss: 7.263196266649175e-08, val loss: 0.1289934366941452\n",
      "Epoch 3719: train loss: 1.1254967091645085e-07, val loss: 0.1293962001800537\n",
      "Epoch 3720: train loss: 1.3505339779840142e-07, val loss: 0.12982402741909027\n",
      "Epoch 3721: train loss: 6.515445249988261e-08, val loss: 0.13022971153259277\n",
      "Epoch 3722: train loss: 7.47141442047905e-08, val loss: 0.13067509233951569\n",
      "Epoch 3723: train loss: 8.150900754344548e-08, val loss: 0.13103674352169037\n",
      "Epoch 3724: train loss: 5.7105062012396957e-08, val loss: 0.1315220296382904\n",
      "Epoch 3725: train loss: 5.679415693293777e-08, val loss: 0.13207033276557922\n",
      "Epoch 3726: train loss: 5.3546926892522606e-08, val loss: 0.1324763298034668\n",
      "Epoch 3727: train loss: 5.553172144345808e-08, val loss: 0.13295303285121918\n",
      "Epoch 3728: train loss: 4.608798676031256e-08, val loss: 0.1333097666501999\n",
      "Epoch 3729: train loss: 4.450198076710876e-08, val loss: 0.1334298849105835\n",
      "Epoch 3730: train loss: 5.7183257240467356e-08, val loss: 0.13353417813777924\n",
      "Epoch 3731: train loss: 5.015925452767078e-08, val loss: 0.13364088535308838\n",
      "Epoch 3732: train loss: 6.261753071612475e-08, val loss: 0.13364748656749725\n",
      "Epoch 3733: train loss: 1.0056952248760354e-07, val loss: 0.13385215401649475\n",
      "Epoch 3734: train loss: 1.422341568968477e-07, val loss: 0.13388526439666748\n",
      "Epoch 3735: train loss: 2.2494829465813382e-07, val loss: 0.13404656946659088\n",
      "Epoch 3736: train loss: 4.3662279836098605e-07, val loss: 0.13399718701839447\n",
      "Epoch 3737: train loss: 7.822015959391138e-07, val loss: 0.1342865228652954\n",
      "Epoch 3738: train loss: 1.4214341490514926e-06, val loss: 0.13415968418121338\n",
      "Epoch 3739: train loss: 2.596977765279007e-06, val loss: 0.1345248520374298\n",
      "Epoch 3740: train loss: 4.938669917464722e-06, val loss: 0.13424520194530487\n",
      "Epoch 3741: train loss: 9.8247874120716e-06, val loss: 0.1348404884338379\n",
      "Epoch 3742: train loss: 2.035168836300727e-05, val loss: 0.13422171771526337\n",
      "Epoch 3743: train loss: 4.349743903730996e-05, val loss: 0.13531629741191864\n",
      "Epoch 3744: train loss: 9.59529570536688e-05, val loss: 0.13395823538303375\n",
      "Epoch 3745: train loss: 0.0002168623177567497, val loss: 0.13615112006664276\n",
      "Epoch 3746: train loss: 0.0004937354242429137, val loss: 0.12730608880519867\n",
      "Epoch 3747: train loss: 0.0010080976644530892, val loss: 0.13617919385433197\n",
      "Epoch 3748: train loss: 0.0013256625970825553, val loss: 0.12648068368434906\n",
      "Epoch 3749: train loss: 0.0009234528988599777, val loss: 0.13415108621120453\n",
      "Epoch 3750: train loss: 0.00026895629707723856, val loss: 0.1318516582250595\n",
      "Epoch 3751: train loss: 0.00016568969294894487, val loss: 0.12794962525367737\n",
      "Epoch 3752: train loss: 0.00048563824384473264, val loss: 0.1306484192609787\n",
      "Epoch 3753: train loss: 0.00048153920215554535, val loss: 0.12857191264629364\n",
      "Epoch 3754: train loss: 0.00019421690376475453, val loss: 0.12352436035871506\n",
      "Epoch 3755: train loss: 0.00018271853332407773, val loss: 0.12876923382282257\n",
      "Epoch 3756: train loss: 0.0002655839780345559, val loss: 0.12186968326568604\n",
      "Epoch 3757: train loss: 0.00015157184679992497, val loss: 0.12045877426862717\n",
      "Epoch 3758: train loss: 0.00012601417256519198, val loss: 0.1273239403963089\n",
      "Epoch 3759: train loss: 0.0001675922831054777, val loss: 0.11727531254291534\n",
      "Epoch 3760: train loss: 9.269801375921816e-05, val loss: 0.11896415054798126\n",
      "Epoch 3761: train loss: 0.0001009651823551394, val loss: 0.12182637304067612\n",
      "Epoch 3762: train loss: 0.0001240116253029555, val loss: 0.11507274210453033\n",
      "Epoch 3763: train loss: 5.315050657372922e-05, val loss: 0.11624818295240402\n",
      "Epoch 3764: train loss: 7.705774623900652e-05, val loss: 0.1169440969824791\n",
      "Epoch 3765: train loss: 8.178147254511714e-05, val loss: 0.11432862281799316\n",
      "Epoch 3766: train loss: 2.936635064543225e-05, val loss: 0.11462674289941788\n",
      "Epoch 3767: train loss: 6.975670112296939e-05, val loss: 0.11491864174604416\n",
      "Epoch 3768: train loss: 5.508460526471026e-05, val loss: 0.11448156088590622\n",
      "Epoch 3769: train loss: 1.4893209481670056e-05, val loss: 0.11305978149175644\n",
      "Epoch 3770: train loss: 5.398315261118114e-05, val loss: 0.11369504779577255\n",
      "Epoch 3771: train loss: 3.450941949267872e-05, val loss: 0.11385132372379303\n",
      "Epoch 3772: train loss: 1.6685360606061295e-05, val loss: 0.11172538250684738\n",
      "Epoch 3773: train loss: 4.0579117921879515e-05, val loss: 0.1129281297326088\n",
      "Epoch 3774: train loss: 1.7333015421172604e-05, val loss: 0.11331502348184586\n",
      "Epoch 3775: train loss: 1.7011361705954187e-05, val loss: 0.11135505884885788\n",
      "Epoch 3776: train loss: 2.827323078236077e-05, val loss: 0.11271212249994278\n",
      "Epoch 3777: train loss: 1.1613502465479542e-05, val loss: 0.11300799995660782\n",
      "Epoch 3778: train loss: 1.7353970179101452e-05, val loss: 0.11128125339746475\n",
      "Epoch 3779: train loss: 1.566847822687123e-05, val loss: 0.11215268820524216\n",
      "Epoch 3780: train loss: 7.964365977386478e-06, val loss: 0.11264955997467041\n",
      "Epoch 3781: train loss: 1.6284519006148912e-05, val loss: 0.11143012344837189\n",
      "Epoch 3782: train loss: 9.718679393699858e-06, val loss: 0.1119241714477539\n",
      "Epoch 3783: train loss: 6.865462637506425e-06, val loss: 0.11256251484155655\n",
      "Epoch 3784: train loss: 1.1338165677443612e-05, val loss: 0.11153197288513184\n",
      "Epoch 3785: train loss: 5.574046554102097e-06, val loss: 0.11186225712299347\n",
      "Epoch 3786: train loss: 7.129467121558264e-06, val loss: 0.11244004964828491\n",
      "Epoch 3787: train loss: 8.034264283196535e-06, val loss: 0.11165516823530197\n",
      "Epoch 3788: train loss: 3.7459562918229494e-06, val loss: 0.11187664419412613\n",
      "Epoch 3789: train loss: 6.182979632285424e-06, val loss: 0.11228945106267929\n",
      "Epoch 3790: train loss: 4.3588861444732174e-06, val loss: 0.11197483539581299\n",
      "Epoch 3791: train loss: 2.7164508082933025e-06, val loss: 0.11197187006473541\n",
      "Epoch 3792: train loss: 6.224544904398499e-06, val loss: 0.11224322766065598\n",
      "Epoch 3793: train loss: 2.9377238206507172e-06, val loss: 0.11233732849359512\n",
      "Epoch 3794: train loss: 1.3982424889036338e-06, val loss: 0.11206036061048508\n",
      "Epoch 3795: train loss: 4.552524387690937e-06, val loss: 0.11239969730377197\n",
      "Epoch 3796: train loss: 2.580000227681012e-06, val loss: 0.11238332837820053\n",
      "Epoch 3797: train loss: 9.829403779804124e-07, val loss: 0.11212356388568878\n",
      "Epoch 3798: train loss: 3.158213985443581e-06, val loss: 0.11255878210067749\n",
      "Epoch 3799: train loss: 2.4458397547277855e-06, val loss: 0.11240102350711823\n",
      "Epoch 3800: train loss: 9.467912605032325e-07, val loss: 0.11248884350061417\n",
      "Epoch 3801: train loss: 1.8670657482289243e-06, val loss: 0.11265617609024048\n",
      "Epoch 3802: train loss: 1.6195033367694123e-06, val loss: 0.11251461505889893\n",
      "Epoch 3803: train loss: 8.084703040367458e-07, val loss: 0.11273308098316193\n",
      "Epoch 3804: train loss: 1.406311639584601e-06, val loss: 0.1128033772110939\n",
      "Epoch 3805: train loss: 1.381718107040797e-06, val loss: 0.11291269212961197\n",
      "Epoch 3806: train loss: 8.755682188166247e-07, val loss: 0.11285167187452316\n",
      "Epoch 3807: train loss: 1.0379885679867584e-06, val loss: 0.11309883743524551\n",
      "Epoch 3808: train loss: 8.871977001945197e-07, val loss: 0.11314071714878082\n",
      "Epoch 3809: train loss: 6.237472689463175e-07, val loss: 0.11310923099517822\n",
      "Epoch 3810: train loss: 8.80638992839522e-07, val loss: 0.11342823505401611\n",
      "Epoch 3811: train loss: 6.984153628764034e-07, val loss: 0.11323753744363785\n",
      "Epoch 3812: train loss: 2.7330193574925943e-07, val loss: 0.1134747788310051\n",
      "Epoch 3813: train loss: 5.161169838174828e-07, val loss: 0.113639235496521\n",
      "Epoch 3814: train loss: 6.788420137127105e-07, val loss: 0.11365289986133575\n",
      "Epoch 3815: train loss: 3.3559723533471697e-07, val loss: 0.11377022415399551\n",
      "Epoch 3816: train loss: 2.89365374328554e-07, val loss: 0.11389438062906265\n",
      "Epoch 3817: train loss: 5.016806881030789e-07, val loss: 0.11388599872589111\n",
      "Epoch 3818: train loss: 3.6389610613696277e-07, val loss: 0.11414358764886856\n",
      "Epoch 3819: train loss: 1.9341925394655846e-07, val loss: 0.11405491083860397\n",
      "Epoch 3820: train loss: 3.812909028511058e-07, val loss: 0.11429198086261749\n",
      "Epoch 3821: train loss: 5.703571446247224e-07, val loss: 0.11429846286773682\n",
      "Epoch 3822: train loss: 5.480158620230213e-07, val loss: 0.11448147147893906\n",
      "Epoch 3823: train loss: 7.58316900828504e-07, val loss: 0.11451852321624756\n",
      "Epoch 3824: train loss: 1.4687983593830722e-06, val loss: 0.11471398174762726\n",
      "Epoch 3825: train loss: 2.8735744308505673e-06, val loss: 0.11466189473867416\n",
      "Epoch 3826: train loss: 5.957248959020944e-06, val loss: 0.11496923118829727\n",
      "Epoch 3827: train loss: 1.3277904145070352e-05, val loss: 0.11475878208875656\n",
      "Epoch 3828: train loss: 2.8785443646484055e-05, val loss: 0.11522185057401657\n",
      "Epoch 3829: train loss: 6.093910269555636e-05, val loss: 0.1146843433380127\n",
      "Epoch 3830: train loss: 0.0001297283306485042, val loss: 0.11563028395175934\n",
      "Epoch 3831: train loss: 0.00028395658591762185, val loss: 0.11418928951025009\n",
      "Epoch 3832: train loss: 0.0006187359103932977, val loss: 0.11640993505716324\n",
      "Epoch 3833: train loss: 0.0012487226631492376, val loss: 0.1122937723994255\n",
      "Epoch 3834: train loss: 0.0013223455753177404, val loss: 0.11843893676996231\n",
      "Epoch 3835: train loss: 0.0008898696978576481, val loss: 0.11715223640203476\n",
      "Epoch 3836: train loss: 0.000823653070256114, val loss: 0.12403412163257599\n",
      "Epoch 3837: train loss: 0.0005909150349907577, val loss: 0.11322934925556183\n",
      "Epoch 3838: train loss: 0.0003530966932885349, val loss: 0.12253699451684952\n",
      "Epoch 3839: train loss: 0.0005184428882785141, val loss: 0.11643063277006149\n",
      "Epoch 3840: train loss: 0.0005173413665033877, val loss: 0.12192501127719879\n",
      "Epoch 3841: train loss: 0.0002864001435227692, val loss: 0.11825692653656006\n",
      "Epoch 3842: train loss: 0.00019588603754527867, val loss: 0.11373639106750488\n",
      "Epoch 3843: train loss: 0.00025679057580418885, val loss: 0.1193726658821106\n",
      "Epoch 3844: train loss: 0.00023974693613126874, val loss: 0.11441824585199356\n",
      "Epoch 3845: train loss: 0.00015937944408506155, val loss: 0.11831851303577423\n",
      "Epoch 3846: train loss: 0.00016250726184807718, val loss: 0.1184806376695633\n",
      "Epoch 3847: train loss: 0.00013872655108571053, val loss: 0.11627250164747238\n",
      "Epoch 3848: train loss: 0.00010835513967322186, val loss: 0.11979971081018448\n",
      "Epoch 3849: train loss: 0.00012070286175003275, val loss: 0.11741381138563156\n",
      "Epoch 3850: train loss: 9.365080040879548e-05, val loss: 0.11795439571142197\n",
      "Epoch 3851: train loss: 7.775915582897142e-05, val loss: 0.12157215923070908\n",
      "Epoch 3852: train loss: 7.524723332608119e-05, val loss: 0.11692459881305695\n",
      "Epoch 3853: train loss: 7.698409899603575e-05, val loss: 0.11690931767225266\n",
      "Epoch 3854: train loss: 5.1281302148709074e-05, val loss: 0.1206713542342186\n",
      "Epoch 3855: train loss: 4.983052349416539e-05, val loss: 0.11909865587949753\n",
      "Epoch 3856: train loss: 6.358267273753881e-05, val loss: 0.11746702343225479\n",
      "Epoch 3857: train loss: 2.731380664044991e-05, val loss: 0.11766510456800461\n",
      "Epoch 3858: train loss: 4.6050383389228955e-05, val loss: 0.11889667809009552\n",
      "Epoch 3859: train loss: 3.7204448744887486e-05, val loss: 0.11900164932012558\n",
      "Epoch 3860: train loss: 2.6611562134348787e-05, val loss: 0.11683773994445801\n",
      "Epoch 3861: train loss: 3.0201435947674327e-05, val loss: 0.11857600510120392\n",
      "Epoch 3862: train loss: 2.6007326596300118e-05, val loss: 0.11984441429376602\n",
      "Epoch 3863: train loss: 2.071601920761168e-05, val loss: 0.1163860335946083\n",
      "Epoch 3864: train loss: 2.2506106688524596e-05, val loss: 0.11800567060709\n",
      "Epoch 3865: train loss: 1.6185445929295383e-05, val loss: 0.1208457499742508\n",
      "Epoch 3866: train loss: 1.7907719666254707e-05, val loss: 0.11716779321432114\n",
      "Epoch 3867: train loss: 1.595646608620882e-05, val loss: 0.11773685365915298\n",
      "Epoch 3868: train loss: 9.158738066616934e-06, val loss: 0.1208118423819542\n",
      "Epoch 3869: train loss: 1.7867792848846875e-05, val loss: 0.11811428517103195\n",
      "Epoch 3870: train loss: 6.494418357760878e-06, val loss: 0.11831889301538467\n",
      "Epoch 3871: train loss: 1.1112862921436317e-05, val loss: 0.12085924297571182\n",
      "Epoch 3872: train loss: 1.0076291800942272e-05, val loss: 0.11843005567789078\n",
      "Epoch 3873: train loss: 6.169419975776691e-06, val loss: 0.1183236837387085\n",
      "Epoch 3874: train loss: 8.527218597009778e-06, val loss: 0.1206609383225441\n",
      "Epoch 3875: train loss: 5.80599180466379e-06, val loss: 0.11917199194431305\n",
      "Epoch 3876: train loss: 6.481971468019765e-06, val loss: 0.11901755630970001\n",
      "Epoch 3877: train loss: 4.935381639370462e-06, val loss: 0.1208106279373169\n",
      "Epoch 3878: train loss: 4.860845365328714e-06, val loss: 0.1205560714006424\n",
      "Epoch 3879: train loss: 4.445462764124386e-06, val loss: 0.1203867569565773\n",
      "Epoch 3880: train loss: 4.1846465137496125e-06, val loss: 0.12102065235376358\n",
      "Epoch 3881: train loss: 2.9219172574812546e-06, val loss: 0.12119410187005997\n",
      "Epoch 3882: train loss: 3.6691412788059097e-06, val loss: 0.12154475599527359\n",
      "Epoch 3883: train loss: 3.117805363217485e-06, val loss: 0.121315598487854\n",
      "Epoch 3884: train loss: 1.999715095735155e-06, val loss: 0.12188491970300674\n",
      "Epoch 3885: train loss: 2.7447374577604933e-06, val loss: 0.12262218445539474\n",
      "Epoch 3886: train loss: 2.30234331866086e-06, val loss: 0.12162665277719498\n",
      "Epoch 3887: train loss: 1.7242562080355128e-06, val loss: 0.12203943729400635\n",
      "Epoch 3888: train loss: 1.5503433132835198e-06, val loss: 0.1230684444308281\n",
      "Epoch 3889: train loss: 2.088893324980745e-06, val loss: 0.12225284427404404\n",
      "Epoch 3890: train loss: 1.1503235555210267e-06, val loss: 0.1224270835518837\n",
      "Epoch 3891: train loss: 1.105350861507759e-06, val loss: 0.12290408462285995\n",
      "Epoch 3892: train loss: 1.6351234535250114e-06, val loss: 0.12261825054883957\n",
      "Epoch 3893: train loss: 6.922445550117118e-07, val loss: 0.12322725355625153\n",
      "Epoch 3894: train loss: 1.106319132304634e-06, val loss: 0.12311138957738876\n",
      "Epoch 3895: train loss: 9.869220320979366e-07, val loss: 0.1228935718536377\n",
      "Epoch 3896: train loss: 5.265521849651122e-07, val loss: 0.12351858615875244\n",
      "Epoch 3897: train loss: 9.641365750212572e-07, val loss: 0.12306816875934601\n",
      "Epoch 3898: train loss: 6.262765168685291e-07, val loss: 0.12311773747205734\n",
      "Epoch 3899: train loss: 4.678505547417444e-07, val loss: 0.12365295737981796\n",
      "Epoch 3900: train loss: 5.875976398783678e-07, val loss: 0.12334072589874268\n",
      "Epoch 3901: train loss: 5.627865107271646e-07, val loss: 0.12359755486249924\n",
      "Epoch 3902: train loss: 4.437573579707532e-07, val loss: 0.12353749573230743\n",
      "Epoch 3903: train loss: 2.711518618525588e-07, val loss: 0.12353446334600449\n",
      "Epoch 3904: train loss: 4.122685481888766e-07, val loss: 0.12413759529590607\n",
      "Epoch 3905: train loss: 4.450444635040185e-07, val loss: 0.12364599853754044\n",
      "Epoch 3906: train loss: 1.8417797775782674e-07, val loss: 0.12375757843255997\n",
      "Epoch 3907: train loss: 2.3741866073123674e-07, val loss: 0.12436962127685547\n",
      "Epoch 3908: train loss: 3.282829084128025e-07, val loss: 0.12392008304595947\n",
      "Epoch 3909: train loss: 2.1198357558205316e-07, val loss: 0.12422257661819458\n",
      "Epoch 3910: train loss: 1.656000705452243e-07, val loss: 0.12460853904485703\n",
      "Epoch 3911: train loss: 1.9378184390461684e-07, val loss: 0.12434480339288712\n",
      "Epoch 3912: train loss: 1.8068550389216398e-07, val loss: 0.12449812889099121\n",
      "Epoch 3913: train loss: 1.2653271141971345e-07, val loss: 0.12475663423538208\n",
      "Epoch 3914: train loss: 1.5990683266409178e-07, val loss: 0.12483058124780655\n",
      "Epoch 3915: train loss: 1.4232840328531893e-07, val loss: 0.12475764751434326\n",
      "Epoch 3916: train loss: 7.787646438828233e-08, val loss: 0.12507210671901703\n",
      "Epoch 3917: train loss: 1.1598847038385429e-07, val loss: 0.12519308924674988\n",
      "Epoch 3918: train loss: 1.1079423956061873e-07, val loss: 0.12510791420936584\n",
      "Epoch 3919: train loss: 7.754094610845641e-08, val loss: 0.12518194317817688\n",
      "Epoch 3920: train loss: 6.928944884521115e-08, val loss: 0.12522341310977936\n",
      "Epoch 3921: train loss: 8.566233589135663e-08, val loss: 0.12526081502437592\n",
      "Epoch 3922: train loss: 8.385590888337902e-08, val loss: 0.1252739429473877\n",
      "Epoch 3923: train loss: 3.531586756366778e-08, val loss: 0.12525005638599396\n",
      "Epoch 3924: train loss: 4.553952237529302e-08, val loss: 0.12535320222377777\n",
      "Epoch 3925: train loss: 6.998412516168173e-08, val loss: 0.1253533959388733\n",
      "Epoch 3926: train loss: 5.1877670159683476e-08, val loss: 0.12539730966091156\n",
      "Epoch 3927: train loss: 3.905052636810069e-08, val loss: 0.12548774480819702\n",
      "Epoch 3928: train loss: 3.583610919122293e-08, val loss: 0.12550805509090424\n",
      "Epoch 3929: train loss: 3.9320127598330146e-08, val loss: 0.12552602589130402\n",
      "Epoch 3930: train loss: 3.9192411094290946e-08, val loss: 0.12559917569160461\n",
      "Epoch 3931: train loss: 3.2622363477230465e-08, val loss: 0.12562315165996552\n",
      "Epoch 3932: train loss: 2.1755615975393994e-08, val loss: 0.1256818175315857\n",
      "Epoch 3933: train loss: 1.8400983847755015e-08, val loss: 0.12571217119693756\n",
      "Epoch 3934: train loss: 2.7716644979136618e-08, val loss: 0.12576068937778473\n",
      "Epoch 3935: train loss: 2.7280810499519248e-08, val loss: 0.12581928074359894\n",
      "Epoch 3936: train loss: 2.3496200540762402e-08, val loss: 0.1258333921432495\n",
      "Epoch 3937: train loss: 2.5209999421349494e-08, val loss: 0.12592069804668427\n",
      "Epoch 3938: train loss: 2.592617853736101e-08, val loss: 0.12593691051006317\n",
      "Epoch 3939: train loss: 2.4268340226285545e-08, val loss: 0.12601645290851593\n",
      "Epoch 3940: train loss: 2.315583103040808e-08, val loss: 0.12602846324443817\n",
      "Epoch 3941: train loss: 2.3467780607688837e-08, val loss: 0.12612587213516235\n",
      "Epoch 3942: train loss: 2.8478025271283514e-08, val loss: 0.12612169981002808\n",
      "Epoch 3943: train loss: 3.838565021396789e-08, val loss: 0.12624308466911316\n",
      "Epoch 3944: train loss: 5.6644438473085756e-08, val loss: 0.12619641423225403\n",
      "Epoch 3945: train loss: 8.961313824329409e-08, val loss: 0.12636078894138336\n",
      "Epoch 3946: train loss: 1.560185438620465e-07, val loss: 0.12624923884868622\n",
      "Epoch 3947: train loss: 2.8128488338552415e-07, val loss: 0.12654533982276917\n",
      "Epoch 3948: train loss: 5.537224865292956e-07, val loss: 0.12623432278633118\n",
      "Epoch 3949: train loss: 1.1462386737548513e-06, val loss: 0.1268051564693451\n",
      "Epoch 3950: train loss: 2.486406174284639e-06, val loss: 0.12606917321681976\n",
      "Epoch 3951: train loss: 5.6059561757138e-06, val loss: 0.1272875964641571\n",
      "Epoch 3952: train loss: 1.315186182182515e-05, val loss: 0.12556497752666473\n",
      "Epoch 3953: train loss: 3.1995587050914764e-05, val loss: 0.1285005360841751\n",
      "Epoch 3954: train loss: 7.9888675827533e-05, val loss: 0.12301567941904068\n",
      "Epoch 3955: train loss: 0.00019249827892053872, val loss: 0.1310577243566513\n",
      "Epoch 3956: train loss: 0.00045636462164111435, val loss: 0.11601117998361588\n",
      "Epoch 3957: train loss: 0.0010422111954540014, val loss: 0.1368783414363861\n",
      "Epoch 3958: train loss: 0.0021925477776676416, val loss: 0.10974641889333725\n",
      "Epoch 3959: train loss: 0.0038189589977264404, val loss: 0.14802220463752747\n",
      "Epoch 3960: train loss: 0.0047340309247374535, val loss: 0.11140334606170654\n",
      "Epoch 3961: train loss: 0.0025675143115222454, val loss: 0.1486445665359497\n",
      "Epoch 3962: train loss: 0.00030684450757689774, val loss: 0.15600338578224182\n",
      "Epoch 3963: train loss: 0.0016606542048975825, val loss: 0.12987589836120605\n",
      "Epoch 3964: train loss: 0.0010360749438405037, val loss: 0.1436123251914978\n",
      "Epoch 3965: train loss: 0.00037632373278029263, val loss: 0.15864799916744232\n",
      "Epoch 3966: train loss: 0.0010963920503854752, val loss: 0.1427660435438156\n",
      "Epoch 3967: train loss: 0.00019079697085544467, val loss: 0.13440723717212677\n",
      "Epoch 3968: train loss: 0.000730435480363667, val loss: 0.15464815497398376\n",
      "Epoch 3969: train loss: 0.00021697838383261114, val loss: 0.15369586646556854\n",
      "Epoch 3970: train loss: 0.0004868729447480291, val loss: 0.13248780369758606\n",
      "Epoch 3971: train loss: 0.00020028075959999114, val loss: 0.12992604076862335\n",
      "Epoch 3972: train loss: 0.0003529887180775404, val loss: 0.13944736123085022\n",
      "Epoch 3973: train loss: 0.00015432023792527616, val loss: 0.14692552387714386\n",
      "Epoch 3974: train loss: 0.0002601228188723326, val loss: 0.13618235290050507\n",
      "Epoch 3975: train loss: 0.00013500852219294757, val loss: 0.1298888772726059\n",
      "Epoch 3976: train loss: 0.00018200675549451262, val loss: 0.13456793129444122\n",
      "Epoch 3977: train loss: 0.00013293664960656315, val loss: 0.14245712757110596\n",
      "Epoch 3978: train loss: 0.00011344879021635279, val loss: 0.1372860223054886\n",
      "Epoch 3979: train loss: 0.00013134971959516406, val loss: 0.13083432614803314\n",
      "Epoch 3980: train loss: 6.706772546749562e-05, val loss: 0.13101115822792053\n",
      "Epoch 3981: train loss: 0.00011835602344945073, val loss: 0.13671763241291046\n",
      "Epoch 3982: train loss: 4.9291204049950466e-05, val loss: 0.13641038537025452\n",
      "Epoch 3983: train loss: 8.783270459389314e-05, val loss: 0.13229575753211975\n",
      "Epoch 3984: train loss: 5.2882303862133995e-05, val loss: 0.13148191571235657\n",
      "Epoch 3985: train loss: 5.535713353310712e-05, val loss: 0.13451488316059113\n",
      "Epoch 3986: train loss: 5.4974534577922896e-05, val loss: 0.1374605894088745\n",
      "Epoch 3987: train loss: 3.638386260718107e-05, val loss: 0.13756611943244934\n",
      "Epoch 3988: train loss: 4.662819628720172e-05, val loss: 0.1339387744665146\n",
      "Epoch 3989: train loss: 3.120873589068651e-05, val loss: 0.13181154429912567\n",
      "Epoch 3990: train loss: 3.4035118005704135e-05, val loss: 0.13404028117656708\n",
      "Epoch 3991: train loss: 2.7710037102224305e-05, val loss: 0.13744619488716125\n",
      "Epoch 3992: train loss: 2.6851399525185116e-05, val loss: 0.13874788582324982\n",
      "Epoch 3993: train loss: 2.1520450900425203e-05, val loss: 0.1344776600599289\n",
      "Epoch 3994: train loss: 2.3316408260143362e-05, val loss: 0.1327807903289795\n",
      "Epoch 3995: train loss: 1.5687097402405925e-05, val loss: 0.13622811436653137\n",
      "Epoch 3996: train loss: 1.989449629036244e-05, val loss: 0.13765938580036163\n",
      "Epoch 3997: train loss: 1.30117896333104e-05, val loss: 0.13804210722446442\n",
      "Epoch 3998: train loss: 1.4991403986641672e-05, val loss: 0.13360421359539032\n",
      "Epoch 3999: train loss: 1.2013892956019845e-05, val loss: 0.1328241378068924\n",
      "Epoch 4000: train loss: 1.0618246051308233e-05, val loss: 0.13665805757045746\n",
      "Epoch 4001: train loss: 1.1189421456947457e-05, val loss: 0.13762442767620087\n",
      "Epoch 4002: train loss: 7.570984053018037e-06, val loss: 0.13731513917446136\n",
      "Epoch 4003: train loss: 9.680237781140022e-06, val loss: 0.13408082723617554\n",
      "Epoch 4004: train loss: 5.8830619309446774e-06, val loss: 0.13479067385196686\n",
      "Epoch 4005: train loss: 7.918341907497961e-06, val loss: 0.13705606758594513\n",
      "Epoch 4006: train loss: 5.12920905748615e-06, val loss: 0.13757193088531494\n",
      "Epoch 4007: train loss: 5.759436135122087e-06, val loss: 0.1364980787038803\n",
      "Epoch 4008: train loss: 4.839346729568206e-06, val loss: 0.1343756914138794\n",
      "Epoch 4009: train loss: 4.185380021226592e-06, val loss: 0.13545523583889008\n",
      "Epoch 4010: train loss: 4.243722742103273e-06, val loss: 0.13693223893642426\n",
      "Epoch 4011: train loss: 3.3343474115099525e-06, val loss: 0.13696959614753723\n",
      "Epoch 4012: train loss: 3.2917630505835405e-06, val loss: 0.13636372983455658\n",
      "Epoch 4013: train loss: 3.0054175113036763e-06, val loss: 0.13502807915210724\n",
      "Epoch 4014: train loss: 2.4028536245168652e-06, val loss: 0.13649338483810425\n",
      "Epoch 4015: train loss: 2.6642067041393602e-06, val loss: 0.13682976365089417\n",
      "Epoch 4016: train loss: 1.8295091877007508e-06, val loss: 0.13680073618888855\n",
      "Epoch 4017: train loss: 2.0937479803251335e-06, val loss: 0.13649804890155792\n",
      "Epoch 4018: train loss: 1.7684783415461425e-06, val loss: 0.1363784521818161\n",
      "Epoch 4019: train loss: 1.398748395331495e-06, val loss: 0.1363724321126938\n",
      "Epoch 4020: train loss: 1.6969818261713954e-06, val loss: 0.1361021250486374\n",
      "Epoch 4021: train loss: 9.231897593053873e-07, val loss: 0.1357325166463852\n",
      "Epoch 4022: train loss: 1.495269771112362e-06, val loss: 0.13571493327617645\n",
      "Epoch 4023: train loss: 7.898840408415708e-07, val loss: 0.13597367703914642\n",
      "Epoch 4024: train loss: 1.1089064173575025e-06, val loss: 0.13598549365997314\n",
      "Epoch 4025: train loss: 7.708717362220341e-07, val loss: 0.13559676706790924\n",
      "Epoch 4026: train loss: 7.739904503978323e-07, val loss: 0.1351941078901291\n",
      "Epoch 4027: train loss: 7.399778496619547e-07, val loss: 0.13505037128925323\n",
      "Epoch 4028: train loss: 5.844703423463216e-07, val loss: 0.1351161003112793\n",
      "Epoch 4029: train loss: 5.874739485989267e-07, val loss: 0.13503241539001465\n",
      "Epoch 4030: train loss: 5.123348501001601e-07, val loss: 0.13481733202934265\n",
      "Epoch 4031: train loss: 4.27722341100889e-07, val loss: 0.13467846810817719\n",
      "Epoch 4032: train loss: 4.938896722705977e-07, val loss: 0.13467422127723694\n",
      "Epoch 4033: train loss: 2.904085079080687e-07, val loss: 0.13468846678733826\n",
      "Epoch 4034: train loss: 4.2029770952467516e-07, val loss: 0.1345239132642746\n",
      "Epoch 4035: train loss: 2.3114711211746908e-07, val loss: 0.13421586155891418\n",
      "Epoch 4036: train loss: 3.4730166476037994e-07, val loss: 0.13418403267860413\n",
      "Epoch 4037: train loss: 2.1221353563305456e-07, val loss: 0.13442125916481018\n",
      "Epoch 4038: train loss: 2.4488556960022834e-07, val loss: 0.13451498746871948\n",
      "Epoch 4039: train loss: 1.9632462056051736e-07, val loss: 0.1344684362411499\n",
      "Epoch 4040: train loss: 1.848862467568324e-07, val loss: 0.1345670074224472\n",
      "Epoch 4041: train loss: 1.7875734670269594e-07, val loss: 0.13477230072021484\n",
      "Epoch 4042: train loss: 1.3897735584578186e-07, val loss: 0.1348957121372223\n",
      "Epoch 4043: train loss: 1.4363078548740305e-07, val loss: 0.1349131315946579\n",
      "Epoch 4044: train loss: 1.259516011486994e-07, val loss: 0.13497686386108398\n",
      "Epoch 4045: train loss: 1.1477033012852189e-07, val loss: 0.13515177369117737\n",
      "Epoch 4046: train loss: 1.0119774174199847e-07, val loss: 0.13532324135303497\n",
      "Epoch 4047: train loss: 8.310968269142904e-08, val loss: 0.13539712131023407\n",
      "Epoch 4048: train loss: 9.757249586073158e-08, val loss: 0.13543836772441864\n",
      "Epoch 4049: train loss: 6.994662982151567e-08, val loss: 0.13555346429347992\n",
      "Epoch 4050: train loss: 7.857259021193386e-08, val loss: 0.13573555648326874\n",
      "Epoch 4051: train loss: 4.5213788268938515e-08, val loss: 0.13589270412921906\n",
      "Epoch 4052: train loss: 7.372071308964223e-08, val loss: 0.13599243760108948\n",
      "Epoch 4053: train loss: 3.9782232619245406e-08, val loss: 0.1361301839351654\n",
      "Epoch 4054: train loss: 6.097495486301341e-08, val loss: 0.1363292932510376\n",
      "Epoch 4055: train loss: 2.4767532025293804e-08, val loss: 0.13650663197040558\n",
      "Epoch 4056: train loss: 5.465958352601774e-08, val loss: 0.1366604119539261\n",
      "Epoch 4057: train loss: 2.4319170677244983e-08, val loss: 0.13687916100025177\n",
      "Epoch 4058: train loss: 4.337753622962737e-08, val loss: 0.13712461292743683\n",
      "Epoch 4059: train loss: 1.802935045702725e-08, val loss: 0.13729707896709442\n",
      "Epoch 4060: train loss: 3.562135120205312e-08, val loss: 0.13746170699596405\n",
      "Epoch 4061: train loss: 2.0315146898042258e-08, val loss: 0.1377202868461609\n",
      "Epoch 4062: train loss: 2.6707908773460076e-08, val loss: 0.13800562918186188\n",
      "Epoch 4063: train loss: 1.5503486494594654e-08, val loss: 0.1381997913122177\n",
      "Epoch 4064: train loss: 2.0946471224192464e-08, val loss: 0.13818643987178802\n",
      "Epoch 4065: train loss: 1.7909588478914884e-08, val loss: 0.13822272419929504\n",
      "Epoch 4066: train loss: 1.5305067435633646e-08, val loss: 0.13824912905693054\n",
      "Epoch 4067: train loss: 1.4199267539538596e-08, val loss: 0.13822121918201447\n",
      "Epoch 4068: train loss: 1.3830161016414877e-08, val loss: 0.1382119506597519\n",
      "Epoch 4069: train loss: 1.3033486290225937e-08, val loss: 0.13826167583465576\n",
      "Epoch 4070: train loss: 1.0687037033108027e-08, val loss: 0.13829566538333893\n",
      "Epoch 4071: train loss: 1.0904075864459628e-08, val loss: 0.13827760517597198\n",
      "Epoch 4072: train loss: 1.0318774279483023e-08, val loss: 0.1382770985364914\n",
      "Epoch 4073: train loss: 9.280745949524771e-09, val loss: 0.13830922544002533\n",
      "Epoch 4074: train loss: 8.649661431547884e-09, val loss: 0.13831570744514465\n",
      "Epoch 4075: train loss: 6.466936763871445e-09, val loss: 0.13830147683620453\n",
      "Epoch 4076: train loss: 8.654860828016808e-09, val loss: 0.13835600018501282\n",
      "Epoch 4077: train loss: 7.0599375234792205e-09, val loss: 0.13845263421535492\n",
      "Epoch 4078: train loss: 7.082214370512929e-09, val loss: 0.13852766156196594\n",
      "Epoch 4079: train loss: 4.794054930812308e-09, val loss: 0.1386057585477829\n",
      "Epoch 4080: train loss: 6.633454230353664e-09, val loss: 0.13871560990810394\n",
      "Epoch 4081: train loss: 5.385075940722572e-09, val loss: 0.13881061971187592\n",
      "Epoch 4082: train loss: 5.3698752111586145e-09, val loss: 0.138886496424675\n",
      "Epoch 4083: train loss: 4.731215419440105e-09, val loss: 0.13898928463459015\n",
      "Epoch 4084: train loss: 5.2132920203007416e-09, val loss: 0.13909173011779785\n",
      "Epoch 4085: train loss: 4.391079500010164e-09, val loss: 0.1391637772321701\n",
      "Epoch 4086: train loss: 4.17597423307825e-09, val loss: 0.13925209641456604\n",
      "Epoch 4087: train loss: 4.226121674832939e-09, val loss: 0.13936100900173187\n",
      "Epoch 4088: train loss: 4.318390089963486e-09, val loss: 0.13945502042770386\n",
      "Epoch 4089: train loss: 4.1509009562901156e-09, val loss: 0.13953480124473572\n",
      "Epoch 4090: train loss: 3.2088369739113887e-09, val loss: 0.13962280750274658\n",
      "Epoch 4091: train loss: 3.846265972384799e-09, val loss: 0.1397092193365097\n",
      "Epoch 4092: train loss: 3.6376950340866188e-09, val loss: 0.13978569209575653\n",
      "Epoch 4093: train loss: 3.609514909186373e-09, val loss: 0.13986487686634064\n",
      "Epoch 4094: train loss: 3.054622332854251e-09, val loss: 0.13993510603904724\n",
      "Epoch 4095: train loss: 3.5136158427206965e-09, val loss: 0.14000415802001953\n",
      "Epoch 4096: train loss: 3.4097940027066898e-09, val loss: 0.14009372889995575\n",
      "Epoch 4097: train loss: 2.905049312218466e-09, val loss: 0.14017029106616974\n",
      "Epoch 4098: train loss: 2.9954798641540492e-09, val loss: 0.1402311623096466\n",
      "Epoch 4099: train loss: 3.295472561504198e-09, val loss: 0.14030282199382782\n",
      "Epoch 4100: train loss: 3.2568254759723914e-09, val loss: 0.14036928117275238\n",
      "Epoch 4101: train loss: 2.615320404686372e-09, val loss: 0.14043104648590088\n",
      "Epoch 4102: train loss: 3.0650966209577746e-09, val loss: 0.1405014544725418\n",
      "Epoch 4103: train loss: 2.8511437655254213e-09, val loss: 0.14056821167469025\n",
      "Epoch 4104: train loss: 2.824733336126428e-09, val loss: 0.14062438905239105\n",
      "Epoch 4105: train loss: 2.7572446548163043e-09, val loss: 0.14068730175495148\n",
      "Epoch 4106: train loss: 2.925688136201643e-09, val loss: 0.14075632393360138\n",
      "Epoch 4107: train loss: 2.714491742494829e-09, val loss: 0.14081226289272308\n",
      "Epoch 4108: train loss: 2.5385484825335425e-09, val loss: 0.14086247980594635\n",
      "Epoch 4109: train loss: 2.9860660610836476e-09, val loss: 0.14091478288173676\n",
      "Epoch 4110: train loss: 2.8301132548591568e-09, val loss: 0.14097164571285248\n",
      "Epoch 4111: train loss: 2.774302565455855e-09, val loss: 0.14102201163768768\n",
      "Epoch 4112: train loss: 2.7915569855707645e-09, val loss: 0.14107315242290497\n",
      "Epoch 4113: train loss: 2.9252977817861847e-09, val loss: 0.14112482964992523\n",
      "Epoch 4114: train loss: 3.4072955568120733e-09, val loss: 0.14117561280727386\n",
      "Epoch 4115: train loss: 4.1989274279785604e-09, val loss: 0.14122112095355988\n",
      "Epoch 4116: train loss: 6.646603711857324e-09, val loss: 0.14127302169799805\n",
      "Epoch 4117: train loss: 1.1407822242404109e-08, val loss: 0.14131690561771393\n",
      "Epoch 4118: train loss: 2.5225981303833578e-08, val loss: 0.14136971533298492\n",
      "Epoch 4119: train loss: 5.959310556136188e-08, val loss: 0.14140279591083527\n",
      "Epoch 4120: train loss: 1.563838765150649e-07, val loss: 0.1414661854505539\n",
      "Epoch 4121: train loss: 4.2260796817572555e-07, val loss: 0.14147460460662842\n",
      "Epoch 4122: train loss: 1.2022304645142867e-06, val loss: 0.14158625900745392\n",
      "Epoch 4123: train loss: 3.556746833055513e-06, val loss: 0.14156082272529602\n",
      "Epoch 4124: train loss: 1.0988377653120551e-05, val loss: 0.14179818332195282\n",
      "Epoch 4125: train loss: 3.3647462259978056e-05, val loss: 0.14154662191867828\n",
      "Epoch 4126: train loss: 9.853936353465542e-05, val loss: 0.14215047657489777\n",
      "Epoch 4127: train loss: 0.0002588211791589856, val loss: 0.14203250408172607\n",
      "Epoch 4128: train loss: 0.0006233967724256217, val loss: 0.14247344434261322\n",
      "Epoch 4129: train loss: 0.0013784910552203655, val loss: 0.13936835527420044\n",
      "Epoch 4130: train loss: 0.001829147688113153, val loss: 0.1396791636943817\n",
      "Epoch 4131: train loss: 0.0011397863272577524, val loss: 0.14423416554927826\n",
      "Epoch 4132: train loss: 0.0005817057681269944, val loss: 0.1378544718027115\n",
      "Epoch 4133: train loss: 0.0009459595894441009, val loss: 0.14051103591918945\n",
      "Epoch 4134: train loss: 0.0007912768633104861, val loss: 0.13980743288993835\n",
      "Epoch 4135: train loss: 0.0002481700212229043, val loss: 0.13688455522060394\n",
      "Epoch 4136: train loss: 0.0006223462405614555, val loss: 0.14417196810245514\n",
      "Epoch 4137: train loss: 0.00025346537586301565, val loss: 0.14680591225624084\n",
      "Epoch 4138: train loss: 0.00029600883135572076, val loss: 0.13776154816150665\n",
      "Epoch 4139: train loss: 0.000337372679496184, val loss: 0.14208105206489563\n",
      "Epoch 4140: train loss: 0.00014288783131632954, val loss: 0.14626310765743256\n",
      "Epoch 4141: train loss: 0.00027136769494973123, val loss: 0.1392417699098587\n",
      "Epoch 4142: train loss: 0.00012847568723373115, val loss: 0.1388658881187439\n",
      "Epoch 4143: train loss: 0.00018305593403056264, val loss: 0.1447627991437912\n",
      "Epoch 4144: train loss: 0.00012081229215255007, val loss: 0.14310626685619354\n",
      "Epoch 4145: train loss: 0.0001259761629626155, val loss: 0.13981863856315613\n",
      "Epoch 4146: train loss: 0.00011231073585804552, val loss: 0.14191052317619324\n",
      "Epoch 4147: train loss: 8.744780643610284e-05, val loss: 0.14311553537845612\n",
      "Epoch 4148: train loss: 9.69001412158832e-05, val loss: 0.1401330828666687\n",
      "Epoch 4149: train loss: 6.236378976609558e-05, val loss: 0.13839375972747803\n",
      "Epoch 4150: train loss: 8.425464329775423e-05, val loss: 0.14015673100948334\n",
      "Epoch 4151: train loss: 4.383517079986632e-05, val loss: 0.14087741076946259\n",
      "Epoch 4152: train loss: 7.129320147214457e-05, val loss: 0.1391136646270752\n",
      "Epoch 4153: train loss: 3.292441033408977e-05, val loss: 0.13831327855587006\n",
      "Epoch 4154: train loss: 5.713732389267534e-05, val loss: 0.13935796916484833\n",
      "Epoch 4155: train loss: 2.804285213642288e-05, val loss: 0.13960668444633484\n",
      "Epoch 4156: train loss: 4.308661664254032e-05, val loss: 0.13826224207878113\n",
      "Epoch 4157: train loss: 2.5848015866358764e-05, val loss: 0.1377200335264206\n",
      "Epoch 4158: train loss: 3.128346361336298e-05, val loss: 0.1384037286043167\n",
      "Epoch 4159: train loss: 2.318860060768202e-05, val loss: 0.13813208043575287\n",
      "Epoch 4160: train loss: 2.3579603293910623e-05, val loss: 0.13693009316921234\n",
      "Epoch 4161: train loss: 2.0164819943602197e-05, val loss: 0.13765406608581543\n",
      "Epoch 4162: train loss: 1.7774362277123146e-05, val loss: 0.13807454705238342\n",
      "Epoch 4163: train loss: 1.7487160221207887e-05, val loss: 0.13757771253585815\n",
      "Epoch 4164: train loss: 1.3361604032979812e-05, val loss: 0.13696503639221191\n",
      "Epoch 4165: train loss: 1.543344114907086e-05, val loss: 0.1374950259923935\n",
      "Epoch 4166: train loss: 9.248736205336172e-06, val loss: 0.13753508031368256\n",
      "Epoch 4167: train loss: 1.399466873408528e-05, val loss: 0.1366184800863266\n",
      "Epoch 4168: train loss: 6.242247764021158e-06, val loss: 0.13600313663482666\n",
      "Epoch 4169: train loss: 1.2441824765119236e-05, val loss: 0.13629038631916046\n",
      "Epoch 4170: train loss: 4.236915174260503e-06, val loss: 0.13677562773227692\n",
      "Epoch 4171: train loss: 1.0595239473332185e-05, val loss: 0.13658227026462555\n",
      "Epoch 4172: train loss: 3.404492417757865e-06, val loss: 0.13611911237239838\n",
      "Epoch 4173: train loss: 8.28052270662738e-06, val loss: 0.1362040489912033\n",
      "Epoch 4174: train loss: 3.2304585602105362e-06, val loss: 0.13640108704566956\n",
      "Epoch 4175: train loss: 6.375837074301671e-06, val loss: 0.13590045273303986\n",
      "Epoch 4176: train loss: 2.980940507768537e-06, val loss: 0.13558152318000793\n",
      "Epoch 4177: train loss: 4.835625986743253e-06, val loss: 0.1360473781824112\n",
      "Epoch 4178: train loss: 2.6846162199944956e-06, val loss: 0.13600794970989227\n",
      "Epoch 4179: train loss: 3.7025240544608096e-06, val loss: 0.1352177858352661\n",
      "Epoch 4180: train loss: 2.5642343643994536e-06, val loss: 0.135329008102417\n",
      "Epoch 4181: train loss: 2.5349588668177603e-06, val loss: 0.13611344993114471\n",
      "Epoch 4182: train loss: 2.632509904287872e-06, val loss: 0.13581055402755737\n",
      "Epoch 4183: train loss: 1.5317498309741495e-06, val loss: 0.1351480633020401\n",
      "Epoch 4184: train loss: 2.619410679471912e-06, val loss: 0.1356029063463211\n",
      "Epoch 4185: train loss: 8.986994544102345e-07, val loss: 0.13591141998767853\n",
      "Epoch 4186: train loss: 2.346585688428604e-06, val loss: 0.13508565723896027\n",
      "Epoch 4187: train loss: 6.929246865183813e-07, val loss: 0.13486790657043457\n",
      "Epoch 4188: train loss: 1.833275177887117e-06, val loss: 0.13562755286693573\n",
      "Epoch 4189: train loss: 6.933034342182509e-07, val loss: 0.13545459508895874\n",
      "Epoch 4190: train loss: 1.3819106925438973e-06, val loss: 0.1346263885498047\n",
      "Epoch 4191: train loss: 6.646133101639862e-07, val loss: 0.13493776321411133\n",
      "Epoch 4192: train loss: 1.0596494348646956e-06, val loss: 0.13561880588531494\n",
      "Epoch 4193: train loss: 6.25782433871791e-07, val loss: 0.13522295653820038\n",
      "Epoch 4194: train loss: 7.402463211292343e-07, val loss: 0.13480904698371887\n",
      "Epoch 4195: train loss: 6.621020247621345e-07, val loss: 0.13533423840999603\n",
      "Epoch 4196: train loss: 4.5427276518239523e-07, val loss: 0.1357688307762146\n",
      "Epoch 4197: train loss: 6.953578690627182e-07, val loss: 0.1355181634426117\n",
      "Epoch 4198: train loss: 2.559586107508949e-07, val loss: 0.1355806440114975\n",
      "Epoch 4199: train loss: 6.006985699968936e-07, val loss: 0.13618454337120056\n",
      "Epoch 4200: train loss: 2.5399867809028365e-07, val loss: 0.1364070028066635\n",
      "Epoch 4201: train loss: 4.181474366760085e-07, val loss: 0.13626216351985931\n",
      "Epoch 4202: train loss: 2.8812846153414284e-07, val loss: 0.1366051435470581\n",
      "Epoch 4203: train loss: 2.62376687487631e-07, val loss: 0.13704298436641693\n",
      "Epoch 4204: train loss: 3.223420605991123e-07, val loss: 0.13728636503219604\n",
      "Epoch 4205: train loss: 1.422349953372759e-07, val loss: 0.13721279799938202\n",
      "Epoch 4206: train loss: 2.942364005775744e-07, val loss: 0.13750676810741425\n",
      "Epoch 4207: train loss: 1.3137760390691255e-07, val loss: 0.13792437314987183\n",
      "Epoch 4208: train loss: 2.0525834543150268e-07, val loss: 0.1380842924118042\n",
      "Epoch 4209: train loss: 1.5914720563614537e-07, val loss: 0.13819777965545654\n",
      "Epoch 4210: train loss: 1.0228369973219742e-07, val loss: 0.1385377049446106\n",
      "Epoch 4211: train loss: 1.951606520833593e-07, val loss: 0.13881757855415344\n",
      "Epoch 4212: train loss: 4.881568571590833e-08, val loss: 0.13896937668323517\n",
      "Epoch 4213: train loss: 1.6328043273006188e-07, val loss: 0.13919655978679657\n",
      "Epoch 4214: train loss: 6.0862305417686e-08, val loss: 0.13942110538482666\n",
      "Epoch 4215: train loss: 1.019810440539004e-07, val loss: 0.139592245221138\n",
      "Epoch 4216: train loss: 9.288081770364442e-08, val loss: 0.13980045914649963\n",
      "Epoch 4217: train loss: 3.786713520526064e-08, val loss: 0.13996759057044983\n",
      "Epoch 4218: train loss: 1.036243304497475e-07, val loss: 0.1400759071111679\n",
      "Epoch 4219: train loss: 3.7561754595571983e-08, val loss: 0.1402408480644226\n",
      "Epoch 4220: train loss: 6.066185420650072e-08, val loss: 0.14037464559078217\n",
      "Epoch 4221: train loss: 5.396796254331093e-08, val loss: 0.1403818577528\n",
      "Epoch 4222: train loss: 3.577918050723383e-08, val loss: 0.14038267731666565\n",
      "Epoch 4223: train loss: 5.345621545416179e-08, val loss: 0.14039550721645355\n",
      "Epoch 4224: train loss: 2.6637302141807595e-08, val loss: 0.14034608006477356\n",
      "Epoch 4225: train loss: 3.863285513716619e-08, val loss: 0.14032508432865143\n",
      "Epoch 4226: train loss: 3.918602331509646e-08, val loss: 0.14033286273479462\n",
      "Epoch 4227: train loss: 1.3834278611568607e-08, val loss: 0.14029152691364288\n",
      "Epoch 4228: train loss: 3.6310698448005496e-08, val loss: 0.14023533463478088\n",
      "Epoch 4229: train loss: 2.483492878013749e-08, val loss: 0.14027093350887299\n",
      "Epoch 4230: train loss: 1.1775105335232183e-08, val loss: 0.14034484326839447\n",
      "Epoch 4231: train loss: 3.086456601408827e-08, val loss: 0.14035998284816742\n",
      "Epoch 4232: train loss: 1.3401182386019173e-08, val loss: 0.14037345349788666\n",
      "Epoch 4233: train loss: 1.4536701620215808e-08, val loss: 0.1404401808977127\n",
      "Epoch 4234: train loss: 2.146335731367799e-08, val loss: 0.1404673159122467\n",
      "Epoch 4235: train loss: 8.587655031533359e-09, val loss: 0.14047205448150635\n",
      "Epoch 4236: train loss: 1.6378328027144562e-08, val loss: 0.14053820073604584\n",
      "Epoch 4237: train loss: 1.3170425638975303e-08, val loss: 0.14057402312755585\n",
      "Epoch 4238: train loss: 7.625188480631095e-09, val loss: 0.1405838578939438\n",
      "Epoch 4239: train loss: 1.3504968698896391e-08, val loss: 0.1406458467245102\n",
      "Epoch 4240: train loss: 8.886508418015637e-09, val loss: 0.14067594707012177\n",
      "Epoch 4241: train loss: 9.553427382513746e-09, val loss: 0.14068853855133057\n",
      "Epoch 4242: train loss: 9.091261077287527e-09, val loss: 0.1407439410686493\n",
      "Epoch 4243: train loss: 6.509323746684004e-09, val loss: 0.14077095687389374\n",
      "Epoch 4244: train loss: 9.283902535628386e-09, val loss: 0.14079296588897705\n",
      "Epoch 4245: train loss: 6.272156127806738e-09, val loss: 0.140836700797081\n",
      "Epoch 4246: train loss: 5.264459979059666e-09, val loss: 0.14086182415485382\n",
      "Epoch 4247: train loss: 8.527002215430457e-09, val loss: 0.14088541269302368\n",
      "Epoch 4248: train loss: 5.2346886825205274e-09, val loss: 0.14090876281261444\n",
      "Epoch 4249: train loss: 4.084406146631636e-09, val loss: 0.14093489944934845\n",
      "Epoch 4250: train loss: 6.779595551620332e-09, val loss: 0.14096586406230927\n",
      "Epoch 4251: train loss: 5.013050419222509e-09, val loss: 0.14098100364208221\n",
      "Epoch 4252: train loss: 4.0613059582028654e-09, val loss: 0.14100511372089386\n",
      "Epoch 4253: train loss: 5.519149581800775e-09, val loss: 0.14103664457798004\n",
      "Epoch 4254: train loss: 3.993754216224943e-09, val loss: 0.14105656743049622\n",
      "Epoch 4255: train loss: 3.48161788288337e-09, val loss: 0.14107660949230194\n",
      "Epoch 4256: train loss: 4.91655161027893e-09, val loss: 0.1411038637161255\n",
      "Epoch 4257: train loss: 3.957874916693527e-09, val loss: 0.14113342761993408\n",
      "Epoch 4258: train loss: 3.383747282370564e-09, val loss: 0.14115309715270996\n",
      "Epoch 4259: train loss: 4.223468685893295e-09, val loss: 0.14118503034114838\n",
      "Epoch 4260: train loss: 3.229243095148604e-09, val loss: 0.14121472835540771\n",
      "Epoch 4261: train loss: 3.4862388531564648e-09, val loss: 0.14123137295246124\n",
      "Epoch 4262: train loss: 3.989989227903834e-09, val loss: 0.14127206802368164\n",
      "Epoch 4263: train loss: 3.0924105498542076e-09, val loss: 0.1413044035434723\n",
      "Epoch 4264: train loss: 3.3807832089394196e-09, val loss: 0.14131739735603333\n",
      "Epoch 4265: train loss: 3.0518489957387374e-09, val loss: 0.14134636521339417\n",
      "Epoch 4266: train loss: 2.847261759697517e-09, val loss: 0.14137475192546844\n",
      "Epoch 4267: train loss: 3.3891032202859606e-09, val loss: 0.14139916002750397\n",
      "Epoch 4268: train loss: 3.0244520221600624e-09, val loss: 0.14142659306526184\n",
      "Epoch 4269: train loss: 3.179620122750748e-09, val loss: 0.14145277440547943\n",
      "Epoch 4270: train loss: 2.9938529433337635e-09, val loss: 0.14146628975868225\n",
      "Epoch 4271: train loss: 2.8835214216371696e-09, val loss: 0.1415024995803833\n",
      "Epoch 4272: train loss: 2.809432908534859e-09, val loss: 0.14153003692626953\n",
      "Epoch 4273: train loss: 2.7420778980769e-09, val loss: 0.1415545791387558\n",
      "Epoch 4274: train loss: 3.0026292563434254e-09, val loss: 0.14158304035663605\n",
      "Epoch 4275: train loss: 2.954835265356337e-09, val loss: 0.14160187542438507\n",
      "Epoch 4276: train loss: 3.0869078404549555e-09, val loss: 0.14162997901439667\n",
      "Epoch 4277: train loss: 3.3432445700753988e-09, val loss: 0.1416529417037964\n",
      "Epoch 4278: train loss: 4.470626979724557e-09, val loss: 0.14167273044586182\n",
      "Epoch 4279: train loss: 6.179276201834227e-09, val loss: 0.1417013257741928\n",
      "Epoch 4280: train loss: 1.1071231043047192e-08, val loss: 0.14171521365642548\n",
      "Epoch 4281: train loss: 2.3281458538804145e-08, val loss: 0.1417526751756668\n",
      "Epoch 4282: train loss: 5.599661889732488e-08, val loss: 0.14175397157669067\n",
      "Epoch 4283: train loss: 1.457943454852284e-07, val loss: 0.14181439578533173\n",
      "Epoch 4284: train loss: 4.0273314994010434e-07, val loss: 0.14176207780838013\n",
      "Epoch 4285: train loss: 1.1568537274797563e-06, val loss: 0.14189493656158447\n",
      "Epoch 4286: train loss: 3.2969765015877783e-06, val loss: 0.14169436693191528\n",
      "Epoch 4287: train loss: 9.470349141338374e-06, val loss: 0.14210453629493713\n",
      "Epoch 4288: train loss: 2.7257452529738657e-05, val loss: 0.14126135408878326\n",
      "Epoch 4289: train loss: 7.81021808506921e-05, val loss: 0.14266157150268555\n",
      "Epoch 4290: train loss: 0.00021724063844885677, val loss: 0.13825969398021698\n",
      "Epoch 4291: train loss: 0.0005600574077107012, val loss: 0.1435355395078659\n",
      "Epoch 4292: train loss: 0.0012462470913305879, val loss: 0.1308429092168808\n",
      "Epoch 4293: train loss: 0.0022156275808811188, val loss: 0.14618973433971405\n",
      "Epoch 4294: train loss: 0.0023756599985063076, val loss: 0.13180506229400635\n",
      "Epoch 4295: train loss: 0.0013013860443606973, val loss: 0.13359394669532776\n",
      "Epoch 4296: train loss: 0.00034585193498060107, val loss: 0.14062707126140594\n",
      "Epoch 4297: train loss: 0.0008163967286236584, val loss: 0.1307048350572586\n",
      "Epoch 4298: train loss: 0.0006369006587192416, val loss: 0.13520030677318573\n",
      "Epoch 4299: train loss: 0.0002781676303129643, val loss: 0.1409529745578766\n",
      "Epoch 4300: train loss: 0.0005297748139128089, val loss: 0.13672807812690735\n",
      "Epoch 4301: train loss: 0.0002177082933485508, val loss: 0.13540373742580414\n",
      "Epoch 4302: train loss: 0.0003418559499550611, val loss: 0.13754194974899292\n",
      "Epoch 4303: train loss: 0.00019713524670805782, val loss: 0.13591420650482178\n",
      "Epoch 4304: train loss: 0.00022963555238675326, val loss: 0.1352892369031906\n",
      "Epoch 4305: train loss: 0.00017004316032398492, val loss: 0.1362312138080597\n",
      "Epoch 4306: train loss: 0.0001492830488132313, val loss: 0.13381819427013397\n",
      "Epoch 4307: train loss: 0.00016818090807646513, val loss: 0.1318371444940567\n",
      "Epoch 4308: train loss: 9.297542419517413e-05, val loss: 0.13385875523090363\n",
      "Epoch 4309: train loss: 0.00013296610268298537, val loss: 0.13527975976467133\n",
      "Epoch 4310: train loss: 7.221053965622559e-05, val loss: 0.1338866949081421\n",
      "Epoch 4311: train loss: 9.77538584265858e-05, val loss: 0.13255982100963593\n",
      "Epoch 4312: train loss: 6.445813778555021e-05, val loss: 0.133443221449852\n",
      "Epoch 4313: train loss: 6.800449773436412e-05, val loss: 0.1347564160823822\n",
      "Epoch 4314: train loss: 5.9011828852817416e-05, val loss: 0.1345217078924179\n",
      "Epoch 4315: train loss: 4.777778667630628e-05, val loss: 0.13341310620307922\n",
      "Epoch 4316: train loss: 5.166853588889353e-05, val loss: 0.13388915359973907\n",
      "Epoch 4317: train loss: 3.601817297749221e-05, val loss: 0.13545440137386322\n",
      "Epoch 4318: train loss: 4.08017476729583e-05, val loss: 0.13599036633968353\n",
      "Epoch 4319: train loss: 3.067568832193501e-05, val loss: 0.1347375363111496\n",
      "Epoch 4320: train loss: 3.070646562264301e-05, val loss: 0.13421447575092316\n",
      "Epoch 4321: train loss: 2.6988114768755622e-05, val loss: 0.13524655997753143\n",
      "Epoch 4322: train loss: 2.310362833668478e-05, val loss: 0.13589397072792053\n",
      "Epoch 4323: train loss: 2.2017406081431545e-05, val loss: 0.13504841923713684\n",
      "Epoch 4324: train loss: 1.8741800886346027e-05, val loss: 0.13418211042881012\n",
      "Epoch 4325: train loss: 1.744627115840558e-05, val loss: 0.13450658321380615\n",
      "Epoch 4326: train loss: 1.587092992849648e-05, val loss: 0.1350879669189453\n",
      "Epoch 4327: train loss: 1.3376046808843967e-05, val loss: 0.13495679199695587\n",
      "Epoch 4328: train loss: 1.3435368600767106e-05, val loss: 0.13471762835979462\n",
      "Epoch 4329: train loss: 1.017097838484915e-05, val loss: 0.13499848544597626\n",
      "Epoch 4330: train loss: 1.1372443623258732e-05, val loss: 0.13548623025417328\n",
      "Epoch 4331: train loss: 8.26379437057767e-06, val loss: 0.13548366725444794\n",
      "Epoch 4332: train loss: 8.820982657198329e-06, val loss: 0.1349812000989914\n",
      "Epoch 4333: train loss: 7.409219506371301e-06, val loss: 0.13485853374004364\n",
      "Epoch 4334: train loss: 6.300304448814131e-06, val loss: 0.1350586712360382\n",
      "Epoch 4335: train loss: 6.796928573749028e-06, val loss: 0.1354953795671463\n",
      "Epoch 4336: train loss: 4.494795575737953e-06, val loss: 0.13572007417678833\n",
      "Epoch 4337: train loss: 5.96294603383285e-06, val loss: 0.13564322888851166\n",
      "Epoch 4338: train loss: 3.537874363246374e-06, val loss: 0.13570848107337952\n",
      "Epoch 4339: train loss: 4.922464995615883e-06, val loss: 0.13600750267505646\n",
      "Epoch 4340: train loss: 2.7359340037946822e-06, val loss: 0.13625545799732208\n",
      "Epoch 4341: train loss: 4.069368060299894e-06, val loss: 0.13631539046764374\n",
      "Epoch 4342: train loss: 2.17930778489972e-06, val loss: 0.13639621436595917\n",
      "Epoch 4343: train loss: 3.4218403470731573e-06, val loss: 0.13653351366519928\n",
      "Epoch 4344: train loss: 1.674006398388883e-06, val loss: 0.13676594197750092\n",
      "Epoch 4345: train loss: 2.81119628198212e-06, val loss: 0.1370411515235901\n",
      "Epoch 4346: train loss: 1.2562348956635105e-06, val loss: 0.13715267181396484\n",
      "Epoch 4347: train loss: 2.387939957770868e-06, val loss: 0.1370498687028885\n",
      "Epoch 4348: train loss: 1.0159329804082518e-06, val loss: 0.13712714612483978\n",
      "Epoch 4349: train loss: 2.012712684518192e-06, val loss: 0.13743360340595245\n",
      "Epoch 4350: train loss: 7.550371492470731e-07, val loss: 0.1376350373029709\n",
      "Epoch 4351: train loss: 1.6219626104430063e-06, val loss: 0.1377129852771759\n",
      "Epoch 4352: train loss: 6.401319296855945e-07, val loss: 0.13786080479621887\n",
      "Epoch 4353: train loss: 1.3453826568365912e-06, val loss: 0.13787655532360077\n",
      "Epoch 4354: train loss: 5.34519813299994e-07, val loss: 0.13767075538635254\n",
      "Epoch 4355: train loss: 1.0845531051018042e-06, val loss: 0.13761858642101288\n",
      "Epoch 4356: train loss: 4.0596768258183147e-07, val loss: 0.13773834705352783\n",
      "Epoch 4357: train loss: 9.220531183018466e-07, val loss: 0.13760633766651154\n",
      "Epoch 4358: train loss: 2.8626448056456866e-07, val loss: 0.13723351061344147\n",
      "Epoch 4359: train loss: 8.391651817873935e-07, val loss: 0.13684701919555664\n",
      "Epoch 4360: train loss: 1.6750499298723298e-07, val loss: 0.13646447658538818\n",
      "Epoch 4361: train loss: 7.100194352460676e-07, val loss: 0.13642767071723938\n",
      "Epoch 4362: train loss: 1.1275048450443137e-07, val loss: 0.136471688747406\n",
      "Epoch 4363: train loss: 5.957820121693658e-07, val loss: 0.1365395188331604\n",
      "Epoch 4364: train loss: 1.0493919688769893e-07, val loss: 0.13650193810462952\n",
      "Epoch 4365: train loss: 4.886078386334702e-07, val loss: 0.1364876627922058\n",
      "Epoch 4366: train loss: 7.672126400848356e-08, val loss: 0.13656321167945862\n",
      "Epoch 4367: train loss: 3.8879917951817333e-07, val loss: 0.13654814660549164\n",
      "Epoch 4368: train loss: 7.04934208783925e-08, val loss: 0.13647817075252533\n",
      "Epoch 4369: train loss: 3.1945029377311585e-07, val loss: 0.13653862476348877\n",
      "Epoch 4370: train loss: 9.30592491954485e-08, val loss: 0.1366051882505417\n",
      "Epoch 4371: train loss: 2.0954858825916745e-07, val loss: 0.13651955127716064\n",
      "Epoch 4372: train loss: 9.868428918480276e-08, val loss: 0.13645902276039124\n",
      "Epoch 4373: train loss: 1.5747535542232072e-07, val loss: 0.13657210767269135\n",
      "Epoch 4374: train loss: 1.120499746321002e-07, val loss: 0.13665015995502472\n",
      "Epoch 4375: train loss: 1.0564529873136053e-07, val loss: 0.13654731214046478\n",
      "Epoch 4376: train loss: 1.099750903676977e-07, val loss: 0.1364586353302002\n",
      "Epoch 4377: train loss: 6.039791600187527e-08, val loss: 0.1365271359682083\n",
      "Epoch 4378: train loss: 1.1402875088606379e-07, val loss: 0.1366005539894104\n",
      "Epoch 4379: train loss: 4.074744808235664e-08, val loss: 0.13655899465084076\n",
      "Epoch 4380: train loss: 9.83454100378367e-08, val loss: 0.13651403784751892\n",
      "Epoch 4381: train loss: 3.50459181674978e-08, val loss: 0.13653632998466492\n",
      "Epoch 4382: train loss: 6.948252462279925e-08, val loss: 0.13655328750610352\n",
      "Epoch 4383: train loss: 3.8769169208308085e-08, val loss: 0.13654033839702606\n",
      "Epoch 4384: train loss: 5.109689027449349e-08, val loss: 0.13653789460659027\n",
      "Epoch 4385: train loss: 4.102185613419351e-08, val loss: 0.13654546439647675\n",
      "Epoch 4386: train loss: 3.6483879028992305e-08, val loss: 0.13659603893756866\n",
      "Epoch 4387: train loss: 4.080180460164229e-08, val loss: 0.13660790026187897\n",
      "Epoch 4388: train loss: 1.9351526603372804e-08, val loss: 0.13657380640506744\n",
      "Epoch 4389: train loss: 3.999214470695733e-08, val loss: 0.136597141623497\n",
      "Epoch 4390: train loss: 1.9218191482650582e-08, val loss: 0.13665275275707245\n",
      "Epoch 4391: train loss: 3.037044749021334e-08, val loss: 0.13665278255939484\n",
      "Epoch 4392: train loss: 1.7159029752633614e-08, val loss: 0.13665549457073212\n",
      "Epoch 4393: train loss: 2.0516257137614957e-08, val loss: 0.13673201203346252\n",
      "Epoch 4394: train loss: 2.0258429600517047e-08, val loss: 0.13680122792720795\n",
      "Epoch 4395: train loss: 1.3580969238091711e-08, val loss: 0.13683897256851196\n",
      "Epoch 4396: train loss: 2.0430963587614315e-08, val loss: 0.13688205182552338\n",
      "Epoch 4397: train loss: 1.249276859738302e-08, val loss: 0.13695783913135529\n",
      "Epoch 4398: train loss: 1.205231114909111e-08, val loss: 0.13704587519168854\n",
      "Epoch 4399: train loss: 1.2867839238595025e-08, val loss: 0.1371036320924759\n",
      "Epoch 4400: train loss: 1.0453228505014067e-08, val loss: 0.1371525526046753\n",
      "Epoch 4401: train loss: 1.4517548052594975e-08, val loss: 0.13724718987941742\n",
      "Epoch 4402: train loss: 6.431223553704513e-09, val loss: 0.1373310536146164\n",
      "Epoch 4403: train loss: 9.504740994259464e-09, val loss: 0.13738252222537994\n",
      "Epoch 4404: train loss: 8.393846506749014e-09, val loss: 0.1374702900648117\n",
      "Epoch 4405: train loss: 6.96335433758577e-09, val loss: 0.13756945729255676\n",
      "Epoch 4406: train loss: 1.0185339682777794e-08, val loss: 0.13761626183986664\n",
      "Epoch 4407: train loss: 4.828628608066765e-09, val loss: 0.13767467439174652\n",
      "Epoch 4408: train loss: 6.73561961761493e-09, val loss: 0.13776206970214844\n",
      "Epoch 4409: train loss: 6.552308029483811e-09, val loss: 0.13782837986946106\n",
      "Epoch 4410: train loss: 4.893099259106748e-09, val loss: 0.13789436221122742\n",
      "Epoch 4411: train loss: 7.496074871937708e-09, val loss: 0.13797327876091003\n",
      "Epoch 4412: train loss: 4.389143271055218e-09, val loss: 0.13803523778915405\n",
      "Epoch 4413: train loss: 4.73629313546553e-09, val loss: 0.13808874785900116\n",
      "Epoch 4414: train loss: 5.023391924652287e-09, val loss: 0.1381528526544571\n",
      "Epoch 4415: train loss: 3.742204768286683e-09, val loss: 0.13821585476398468\n",
      "Epoch 4416: train loss: 5.863749485968128e-09, val loss: 0.1382780224084854\n",
      "Epoch 4417: train loss: 3.95407528941405e-09, val loss: 0.13833723962306976\n",
      "Epoch 4418: train loss: 3.0771873937851524e-09, val loss: 0.1383947730064392\n",
      "Epoch 4419: train loss: 4.7108974499110445e-09, val loss: 0.13845650851726532\n",
      "Epoch 4420: train loss: 3.5176022095129156e-09, val loss: 0.13852201402187347\n",
      "Epoch 4421: train loss: 3.5874065940078026e-09, val loss: 0.13858380913734436\n",
      "Epoch 4422: train loss: 4.176822443469064e-09, val loss: 0.13863812386989594\n",
      "Epoch 4423: train loss: 2.9312900995392965e-09, val loss: 0.13869723677635193\n",
      "Epoch 4424: train loss: 3.2108684600018478e-09, val loss: 0.13876043260097504\n",
      "Epoch 4425: train loss: 3.6969469707770486e-09, val loss: 0.13881538808345795\n",
      "Epoch 4426: train loss: 3.1010796153196907e-09, val loss: 0.1388893574476242\n",
      "Epoch 4427: train loss: 3.2535436567115994e-09, val loss: 0.1389733999967575\n",
      "Epoch 4428: train loss: 3.3438563029619672e-09, val loss: 0.13904081284999847\n",
      "Epoch 4429: train loss: 2.7964934812274578e-09, val loss: 0.13911817967891693\n",
      "Epoch 4430: train loss: 2.654547026637033e-09, val loss: 0.1391924023628235\n",
      "Epoch 4431: train loss: 3.239230883522737e-09, val loss: 0.139252707362175\n",
      "Epoch 4432: train loss: 3.0259894590045633e-09, val loss: 0.13932935893535614\n",
      "Epoch 4433: train loss: 2.7149138492887914e-09, val loss: 0.13940180838108063\n",
      "Epoch 4434: train loss: 3.020069083703447e-09, val loss: 0.13946636021137238\n",
      "Epoch 4435: train loss: 2.7447690786885914e-09, val loss: 0.13953495025634766\n",
      "Epoch 4436: train loss: 2.6483057968818002e-09, val loss: 0.139600470662117\n",
      "Epoch 4437: train loss: 3.002615267533315e-09, val loss: 0.13965988159179688\n",
      "Epoch 4438: train loss: 2.7726483331491636e-09, val loss: 0.1397203654050827\n",
      "Epoch 4439: train loss: 2.473964366700443e-09, val loss: 0.1397884488105774\n",
      "Epoch 4440: train loss: 2.6879194425788455e-09, val loss: 0.1398511677980423\n",
      "Epoch 4441: train loss: 2.883731031744219e-09, val loss: 0.139910489320755\n",
      "Epoch 4442: train loss: 2.670469401166997e-09, val loss: 0.13996945321559906\n",
      "Epoch 4443: train loss: 2.8033131371785203e-09, val loss: 0.14003229141235352\n",
      "Epoch 4444: train loss: 2.5938644565570712e-09, val loss: 0.1400911808013916\n",
      "Epoch 4445: train loss: 2.4622159866538595e-09, val loss: 0.14014649391174316\n",
      "Epoch 4446: train loss: 2.608023130790116e-09, val loss: 0.1402016133069992\n",
      "Epoch 4447: train loss: 2.6918880458026706e-09, val loss: 0.14025841653347015\n",
      "Epoch 4448: train loss: 2.8923068384756334e-09, val loss: 0.14031842350959778\n",
      "Epoch 4449: train loss: 3.034899220821785e-09, val loss: 0.14037612080574036\n",
      "Epoch 4450: train loss: 4.259775643333796e-09, val loss: 0.14044418931007385\n",
      "Epoch 4451: train loss: 6.341206226778695e-09, val loss: 0.14049068093299866\n",
      "Epoch 4452: train loss: 1.2655338998968091e-08, val loss: 0.1405365914106369\n",
      "Epoch 4453: train loss: 3.0084162716548235e-08, val loss: 0.1405779868364334\n",
      "Epoch 4454: train loss: 8.040634469352881e-08, val loss: 0.1406284123659134\n",
      "Epoch 4455: train loss: 2.3183089581380045e-07, val loss: 0.1406724750995636\n",
      "Epoch 4456: train loss: 7.005774023127742e-07, val loss: 0.14071647822856903\n",
      "Epoch 4457: train loss: 2.1590788037428865e-06, val loss: 0.14073359966278076\n",
      "Epoch 4458: train loss: 6.778424449294107e-06, val loss: 0.1408219337463379\n",
      "Epoch 4459: train loss: 2.1805628421134315e-05, val loss: 0.1407119631767273\n",
      "Epoch 4460: train loss: 7.214040670078248e-05, val loss: 0.14111553132534027\n",
      "Epoch 4461: train loss: 0.00024161869077943265, val loss: 0.14029325544834137\n",
      "Epoch 4462: train loss: 0.0007977286586537957, val loss: 0.14588303864002228\n",
      "Epoch 4463: train loss: 0.0021856683306396008, val loss: 0.1321246176958084\n",
      "Epoch 4464: train loss: 0.003594820387661457, val loss: 0.1476467400789261\n",
      "Epoch 4465: train loss: 0.001821005716919899, val loss: 0.13660064339637756\n",
      "Epoch 4466: train loss: 0.0006377585814334452, val loss: 0.12922357022762299\n",
      "Epoch 4467: train loss: 0.0010708575136959553, val loss: 0.1348227560520172\n",
      "Epoch 4468: train loss: 0.0006923857145011425, val loss: 0.13206855952739716\n",
      "Epoch 4469: train loss: 0.000656904885545373, val loss: 0.1239701434969902\n",
      "Epoch 4470: train loss: 0.00042651768308132887, val loss: 0.12297745794057846\n",
      "Epoch 4471: train loss: 0.0005133726517669857, val loss: 0.12457435578107834\n",
      "Epoch 4472: train loss: 0.00032994928187690675, val loss: 0.12252204865217209\n",
      "Epoch 4473: train loss: 0.000321377330692485, val loss: 0.117902971804142\n",
      "Epoch 4474: train loss: 0.00027036177925765514, val loss: 0.11744674295186996\n",
      "Epoch 4475: train loss: 0.00025047018425539136, val loss: 0.11850681155920029\n",
      "Epoch 4476: train loss: 0.00015295702905859798, val loss: 0.117008738219738\n",
      "Epoch 4477: train loss: 0.00025012766127474606, val loss: 0.11525493115186691\n",
      "Epoch 4478: train loss: 8.304254879476503e-05, val loss: 0.11484384536743164\n",
      "Epoch 4479: train loss: 0.0001788488298188895, val loss: 0.11490833759307861\n",
      "Epoch 4480: train loss: 0.00011180425644852221, val loss: 0.11367940902709961\n",
      "Epoch 4481: train loss: 9.333874186268076e-05, val loss: 0.11125864833593369\n",
      "Epoch 4482: train loss: 9.795245568966493e-05, val loss: 0.1095375344157219\n",
      "Epoch 4483: train loss: 9.627282270230353e-05, val loss: 0.1094532236456871\n",
      "Epoch 4484: train loss: 5.105136733618565e-05, val loss: 0.11006977409124374\n",
      "Epoch 4485: train loss: 7.694268424529582e-05, val loss: 0.11007259041070938\n",
      "Epoch 4486: train loss: 6.297314394032583e-05, val loss: 0.10869776457548141\n",
      "Epoch 4487: train loss: 3.657436536741443e-05, val loss: 0.10642609745264053\n",
      "Epoch 4488: train loss: 5.591629815171473e-05, val loss: 0.10489591211080551\n",
      "Epoch 4489: train loss: 4.21313161496073e-05, val loss: 0.10449519008398056\n",
      "Epoch 4490: train loss: 2.855734055629e-05, val loss: 0.10516326874494553\n",
      "Epoch 4491: train loss: 3.890973312081769e-05, val loss: 0.1053737998008728\n",
      "Epoch 4492: train loss: 3.054690023418516e-05, val loss: 0.10478279739618301\n",
      "Epoch 4493: train loss: 2.1219222617219202e-05, val loss: 0.1036909967660904\n",
      "Epoch 4494: train loss: 2.6868019631365314e-05, val loss: 0.10300679504871368\n",
      "Epoch 4495: train loss: 2.0490204406087287e-05, val loss: 0.1034158244729042\n",
      "Epoch 4496: train loss: 1.9605075067374855e-05, val loss: 0.10417062044143677\n",
      "Epoch 4497: train loss: 1.3395342648436781e-05, val loss: 0.10403783619403839\n",
      "Epoch 4498: train loss: 2.058124664472416e-05, val loss: 0.10317017883062363\n",
      "Epoch 4499: train loss: 9.150264304480515e-06, val loss: 0.1028686910867691\n",
      "Epoch 4500: train loss: 1.3463713912642561e-05, val loss: 0.10342665761709213\n",
      "Epoch 4501: train loss: 1.2370606782496907e-05, val loss: 0.10376923531293869\n",
      "Epoch 4502: train loss: 8.74173656484345e-06, val loss: 0.10338417440652847\n",
      "Epoch 4503: train loss: 8.425937267020345e-06, val loss: 0.10312819480895996\n",
      "Epoch 4504: train loss: 9.704069270810578e-06, val loss: 0.10353324562311172\n",
      "Epoch 4505: train loss: 6.306345767370658e-06, val loss: 0.10383757203817368\n",
      "Epoch 4506: train loss: 5.952210813120473e-06, val loss: 0.10344302654266357\n",
      "Epoch 4507: train loss: 7.302064659597818e-06, val loss: 0.1029498353600502\n",
      "Epoch 4508: train loss: 4.354677002993412e-06, val loss: 0.1032068133354187\n",
      "Epoch 4509: train loss: 5.090134891361231e-06, val loss: 0.10393481701612473\n",
      "Epoch 4510: train loss: 4.1996186155301984e-06, val loss: 0.10424058884382248\n",
      "Epoch 4511: train loss: 4.305145012040157e-06, val loss: 0.10398383438587189\n",
      "Epoch 4512: train loss: 3.029071422133711e-06, val loss: 0.10370909422636032\n",
      "Epoch 4513: train loss: 3.400343757675728e-06, val loss: 0.10379574447870255\n",
      "Epoch 4514: train loss: 3.15575107379118e-06, val loss: 0.1041666641831398\n",
      "Epoch 4515: train loss: 2.0186621441098396e-06, val loss: 0.10453631728887558\n",
      "Epoch 4516: train loss: 2.617431164253503e-06, val loss: 0.10466239601373672\n",
      "Epoch 4517: train loss: 2.4643438791827066e-06, val loss: 0.1045307144522667\n",
      "Epoch 4518: train loss: 1.2337216048763366e-06, val loss: 0.10442943871021271\n",
      "Epoch 4519: train loss: 2.2182266548043117e-06, val loss: 0.10461302101612091\n",
      "Epoch 4520: train loss: 1.4477432159765158e-06, val loss: 0.1049368605017662\n",
      "Epoch 4521: train loss: 1.2017814015052863e-06, val loss: 0.1051047220826149\n",
      "Epoch 4522: train loss: 1.5173783367572469e-06, val loss: 0.1051454097032547\n",
      "Epoch 4523: train loss: 1.0697609695853316e-06, val loss: 0.10527302324771881\n",
      "Epoch 4524: train loss: 9.814998520596419e-07, val loss: 0.10546863079071045\n",
      "Epoch 4525: train loss: 9.527065003567259e-07, val loss: 0.1056072860956192\n",
      "Epoch 4526: train loss: 8.653541385683639e-07, val loss: 0.10574102401733398\n",
      "Epoch 4527: train loss: 7.435882594108989e-07, val loss: 0.10593769699335098\n",
      "Epoch 4528: train loss: 6.994100658630487e-07, val loss: 0.10608221590518951\n",
      "Epoch 4529: train loss: 6.821637157372606e-07, val loss: 0.10611176490783691\n",
      "Epoch 4530: train loss: 4.3259530002615065e-07, val loss: 0.10619895905256271\n",
      "Epoch 4531: train loss: 5.722593527934805e-07, val loss: 0.1064179316163063\n",
      "Epoch 4532: train loss: 5.179613253858406e-07, val loss: 0.10661780089139938\n",
      "Epoch 4533: train loss: 3.194082864865777e-07, val loss: 0.10673486441373825\n",
      "Epoch 4534: train loss: 4.504677235672716e-07, val loss: 0.10686272382736206\n",
      "Epoch 4535: train loss: 2.887036885113048e-07, val loss: 0.10702331364154816\n",
      "Epoch 4536: train loss: 3.040960336875287e-07, val loss: 0.10714798420667648\n",
      "Epoch 4537: train loss: 3.3238674745916796e-07, val loss: 0.10727915912866592\n",
      "Epoch 4538: train loss: 2.0794263377865718e-07, val loss: 0.10747542232275009\n",
      "Epoch 4539: train loss: 2.265251879407515e-07, val loss: 0.10765010118484497\n",
      "Epoch 4540: train loss: 2.0934842837050383e-07, val loss: 0.10774584114551544\n",
      "Epoch 4541: train loss: 1.8256720579756802e-07, val loss: 0.10786107927560806\n",
      "Epoch 4542: train loss: 1.8556721670393017e-07, val loss: 0.10804998874664307\n",
      "Epoch 4543: train loss: 1.3732916670505801e-07, val loss: 0.10823450237512589\n",
      "Epoch 4544: train loss: 1.3482242877671524e-07, val loss: 0.10837388038635254\n",
      "Epoch 4545: train loss: 1.231756101560677e-07, val loss: 0.10849843174219131\n",
      "Epoch 4546: train loss: 1.204965514034484e-07, val loss: 0.10860877484083176\n",
      "Epoch 4547: train loss: 1.0194371924399093e-07, val loss: 0.10871150344610214\n",
      "Epoch 4548: train loss: 9.274000234427149e-08, val loss: 0.10887068510055542\n",
      "Epoch 4549: train loss: 7.868246143516444e-08, val loss: 0.10907962173223495\n",
      "Epoch 4550: train loss: 7.750120545324535e-08, val loss: 0.10925813764333725\n",
      "Epoch 4551: train loss: 6.49440181632599e-08, val loss: 0.10940294712781906\n",
      "Epoch 4552: train loss: 7.676432289827062e-08, val loss: 0.1095471903681755\n",
      "Epoch 4553: train loss: 4.68378047457918e-08, val loss: 0.10967284440994263\n",
      "Epoch 4554: train loss: 5.202995367881158e-08, val loss: 0.1097814068198204\n",
      "Epoch 4555: train loss: 4.4125403775296945e-08, val loss: 0.10994106531143188\n",
      "Epoch 4556: train loss: 4.915573370567472e-08, val loss: 0.11014304310083389\n",
      "Epoch 4557: train loss: 3.8025717685741256e-08, val loss: 0.11030454933643341\n",
      "Epoch 4558: train loss: 3.657031655279752e-08, val loss: 0.11040296405553818\n",
      "Epoch 4559: train loss: 2.5517071122749257e-08, val loss: 0.1104988381266594\n",
      "Epoch 4560: train loss: 3.3649705244442885e-08, val loss: 0.11063199490308762\n",
      "Epoch 4561: train loss: 2.533861476194943e-08, val loss: 0.1108139306306839\n",
      "Epoch 4562: train loss: 2.6874506176000068e-08, val loss: 0.11097534000873566\n",
      "Epoch 4563: train loss: 2.264781784333536e-08, val loss: 0.111050084233284\n",
      "Epoch 4564: train loss: 1.7506033955783096e-08, val loss: 0.11112608760595322\n",
      "Epoch 4565: train loss: 1.9779751170290183e-08, val loss: 0.11129570007324219\n",
      "Epoch 4566: train loss: 1.7752753933564236e-08, val loss: 0.11146817356348038\n",
      "Epoch 4567: train loss: 1.5450519086357417e-08, val loss: 0.1115715280175209\n",
      "Epoch 4568: train loss: 1.6059599872164654e-08, val loss: 0.11168237030506134\n",
      "Epoch 4569: train loss: 1.163993612607328e-08, val loss: 0.11182575672864914\n",
      "Epoch 4570: train loss: 1.3122255282382866e-08, val loss: 0.11195345222949982\n",
      "Epoch 4571: train loss: 1.049270093034238e-08, val loss: 0.11208420246839523\n",
      "Epoch 4572: train loss: 1.1406845246142439e-08, val loss: 0.11222713440656662\n",
      "Epoch 4573: train loss: 1.1299418289922869e-08, val loss: 0.1123390793800354\n",
      "Epoch 4574: train loss: 7.132973767198791e-09, val loss: 0.11244148015975952\n",
      "Epoch 4575: train loss: 8.05899347255945e-09, val loss: 0.1125773936510086\n",
      "Epoch 4576: train loss: 7.748499619708582e-09, val loss: 0.11272479593753815\n",
      "Epoch 4577: train loss: 9.389482968913399e-09, val loss: 0.11286341398954391\n",
      "Epoch 4578: train loss: 6.3756835366746145e-09, val loss: 0.11298596858978271\n",
      "Epoch 4579: train loss: 5.677279535376556e-09, val loss: 0.11308449506759644\n",
      "Epoch 4580: train loss: 5.570604866278472e-09, val loss: 0.11319675296545029\n",
      "Epoch 4581: train loss: 6.547252962008088e-09, val loss: 0.11334357410669327\n",
      "Epoch 4582: train loss: 6.271287489312272e-09, val loss: 0.11348060518503189\n",
      "Epoch 4583: train loss: 4.3729939669390205e-09, val loss: 0.113594651222229\n",
      "Epoch 4584: train loss: 4.587480617601614e-09, val loss: 0.11370886862277985\n",
      "Epoch 4585: train loss: 4.210962245565497e-09, val loss: 0.11382950842380524\n",
      "Epoch 4586: train loss: 5.763777455314312e-09, val loss: 0.11395919322967529\n",
      "Epoch 4587: train loss: 4.634368444556003e-09, val loss: 0.11408709734678268\n",
      "Epoch 4588: train loss: 3.5289411393080172e-09, val loss: 0.11419875919818878\n",
      "Epoch 4589: train loss: 3.5568354928017243e-09, val loss: 0.11431385576725006\n",
      "Epoch 4590: train loss: 4.149028232092178e-09, val loss: 0.1144382581114769\n",
      "Epoch 4591: train loss: 4.20517931587483e-09, val loss: 0.11455720663070679\n",
      "Epoch 4592: train loss: 3.4949259042349468e-09, val loss: 0.11465727537870407\n",
      "Epoch 4593: train loss: 3.384442726073189e-09, val loss: 0.11475985497236252\n",
      "Epoch 4594: train loss: 2.9918156840835763e-09, val loss: 0.11487378925085068\n",
      "Epoch 4595: train loss: 3.5159790634509136e-09, val loss: 0.11498520523309708\n",
      "Epoch 4596: train loss: 3.7569956035099494e-09, val loss: 0.11509110778570175\n",
      "Epoch 4597: train loss: 3.162566653003296e-09, val loss: 0.11520715057849884\n",
      "Epoch 4598: train loss: 2.83666423683826e-09, val loss: 0.1153206154704094\n",
      "Epoch 4599: train loss: 2.5775062084676392e-09, val loss: 0.11542492359876633\n",
      "Epoch 4600: train loss: 3.2933735738538417e-09, val loss: 0.1155349537730217\n",
      "Epoch 4601: train loss: 3.438857643089932e-09, val loss: 0.11564067751169205\n",
      "Epoch 4602: train loss: 2.7462079277285056e-09, val loss: 0.11574747413396835\n",
      "Epoch 4603: train loss: 2.577344337950649e-09, val loss: 0.11586618423461914\n",
      "Epoch 4604: train loss: 2.8210542790674253e-09, val loss: 0.11597104370594025\n",
      "Epoch 4605: train loss: 3.0561901898096266e-09, val loss: 0.11607765406370163\n",
      "Epoch 4606: train loss: 3.0346278823145667e-09, val loss: 0.11619611084461212\n",
      "Epoch 4607: train loss: 2.6132525032807052e-09, val loss: 0.11630069464445114\n",
      "Epoch 4608: train loss: 2.460863957054471e-09, val loss: 0.11640014499425888\n",
      "Epoch 4609: train loss: 2.6754716220267483e-09, val loss: 0.1165049821138382\n",
      "Epoch 4610: train loss: 3.0464637479354906e-09, val loss: 0.11661379784345627\n",
      "Epoch 4611: train loss: 2.618201877524484e-09, val loss: 0.11671636253595352\n",
      "Epoch 4612: train loss: 2.6025650523564536e-09, val loss: 0.11681783199310303\n",
      "Epoch 4613: train loss: 2.586214131738984e-09, val loss: 0.1169230118393898\n",
      "Epoch 4614: train loss: 2.646660224314701e-09, val loss: 0.11702632158994675\n",
      "Epoch 4615: train loss: 2.8740523294601417e-09, val loss: 0.11713156849145889\n",
      "Epoch 4616: train loss: 2.6834439115219766e-09, val loss: 0.11722888052463531\n",
      "Epoch 4617: train loss: 2.594913173226132e-09, val loss: 0.11733417958021164\n",
      "Epoch 4618: train loss: 2.8312920896667038e-09, val loss: 0.1174391657114029\n",
      "Epoch 4619: train loss: 2.7783850775620067e-09, val loss: 0.11754877865314484\n",
      "Epoch 4620: train loss: 3.3324023540615144e-09, val loss: 0.11764407157897949\n",
      "Epoch 4621: train loss: 3.909637502630403e-09, val loss: 0.11774943023920059\n",
      "Epoch 4622: train loss: 6.404593744235854e-09, val loss: 0.11785101890563965\n",
      "Epoch 4623: train loss: 1.0741811884429353e-08, val loss: 0.1179717555642128\n",
      "Epoch 4624: train loss: 2.4817056853976283e-08, val loss: 0.11805420368909836\n",
      "Epoch 4625: train loss: 5.8564275207118044e-08, val loss: 0.11818349361419678\n",
      "Epoch 4626: train loss: 1.5897067839887313e-07, val loss: 0.1182309165596962\n",
      "Epoch 4627: train loss: 4.263343953425647e-07, val loss: 0.1184396967291832\n",
      "Epoch 4628: train loss: 1.1409762237235554e-06, val loss: 0.11841167509555817\n",
      "Epoch 4629: train loss: 2.942617129519931e-06, val loss: 0.11878182739019394\n",
      "Epoch 4630: train loss: 7.626702881680103e-06, val loss: 0.11851531267166138\n",
      "Epoch 4631: train loss: 1.908270496642217e-05, val loss: 0.11933181434869766\n",
      "Epoch 4632: train loss: 4.7057732444955036e-05, val loss: 0.11822772026062012\n",
      "Epoch 4633: train loss: 0.00011155546962982044, val loss: 0.11990570276975632\n",
      "Epoch 4634: train loss: 0.00022911951236892492, val loss: 0.11655361950397491\n",
      "Epoch 4635: train loss: 0.0003723400877788663, val loss: 0.11852540820837021\n",
      "Epoch 4636: train loss: 0.0004695168463513255, val loss: 0.1150892972946167\n",
      "Epoch 4637: train loss: 0.0004245275049470365, val loss: 0.11471018940210342\n",
      "Epoch 4638: train loss: 0.0002085643936879933, val loss: 0.11454921215772629\n",
      "Epoch 4639: train loss: 5.1196388085372746e-05, val loss: 0.11212541908025742\n",
      "Epoch 4640: train loss: 0.00011205924238311127, val loss: 0.11256833374500275\n",
      "Epoch 4641: train loss: 0.00016705173766240478, val loss: 0.11227696388959885\n",
      "Epoch 4642: train loss: 9.433105878997594e-05, val loss: 0.11045270413160324\n",
      "Epoch 4643: train loss: 6.27352055744268e-05, val loss: 0.11123555153608322\n",
      "Epoch 4644: train loss: 8.2713711890392e-05, val loss: 0.11034345626831055\n",
      "Epoch 4645: train loss: 5.392296225181781e-05, val loss: 0.1099383607506752\n",
      "Epoch 4646: train loss: 5.889837120776065e-05, val loss: 0.11137431114912033\n",
      "Epoch 4647: train loss: 5.464605783345178e-05, val loss: 0.1108321100473404\n",
      "Epoch 4648: train loss: 2.8756756364600733e-05, val loss: 0.10998417437076569\n",
      "Epoch 4649: train loss: 4.784407065017149e-05, val loss: 0.11053837835788727\n",
      "Epoch 4650: train loss: 3.4640102967387065e-05, val loss: 0.11020197719335556\n",
      "Epoch 4651: train loss: 2.2573329260922037e-05, val loss: 0.10994911193847656\n",
      "Epoch 4652: train loss: 3.338561509735882e-05, val loss: 0.11074318736791611\n",
      "Epoch 4653: train loss: 2.159506948373746e-05, val loss: 0.11039502918720245\n",
      "Epoch 4654: train loss: 2.112819674948696e-05, val loss: 0.11023210734128952\n",
      "Epoch 4655: train loss: 1.9223116396460682e-05, val loss: 0.11086747795343399\n",
      "Epoch 4656: train loss: 1.6792913811514154e-05, val loss: 0.11009742319583893\n",
      "Epoch 4657: train loss: 1.680298737483099e-05, val loss: 0.10994177311658859\n",
      "Epoch 4658: train loss: 1.0039929293270689e-05, val loss: 0.11110347509384155\n",
      "Epoch 4659: train loss: 1.6335732652805746e-05, val loss: 0.11047141999006271\n",
      "Epoch 4660: train loss: 8.484924364893232e-06, val loss: 0.11024029552936554\n",
      "Epoch 4661: train loss: 8.944442015490495e-06, val loss: 0.11123871058225632\n",
      "Epoch 4662: train loss: 1.2727349712804426e-05, val loss: 0.11060114204883575\n",
      "Epoch 4663: train loss: 3.3092924240918364e-06, val loss: 0.11031794548034668\n",
      "Epoch 4664: train loss: 1.0044771443062928e-05, val loss: 0.11102329939603806\n",
      "Epoch 4665: train loss: 6.7283804128237534e-06, val loss: 0.1104576364159584\n",
      "Epoch 4666: train loss: 3.1681104246672476e-06, val loss: 0.1102154403924942\n",
      "Epoch 4667: train loss: 8.266604709206149e-06, val loss: 0.11115163564682007\n",
      "Epoch 4668: train loss: 2.872482127713738e-06, val loss: 0.11090519279241562\n",
      "Epoch 4669: train loss: 4.379908205009997e-06, val loss: 0.11017515510320663\n",
      "Epoch 4670: train loss: 4.993838047084864e-06, val loss: 0.11096443980932236\n",
      "Epoch 4671: train loss: 1.926446202560328e-06, val loss: 0.11096873134374619\n",
      "Epoch 4672: train loss: 4.382445695227943e-06, val loss: 0.11007573455572128\n",
      "Epoch 4673: train loss: 2.443485300318571e-06, val loss: 0.11072611808776855\n",
      "Epoch 4674: train loss: 2.0526845219137613e-06, val loss: 0.11101742833852768\n",
      "Epoch 4675: train loss: 3.408530119486386e-06, val loss: 0.11019726097583771\n",
      "Epoch 4676: train loss: 1.2106106623832602e-06, val loss: 0.11057604849338531\n",
      "Epoch 4677: train loss: 2.091282340188627e-06, val loss: 0.11108949035406113\n",
      "Epoch 4678: train loss: 2.309286173840519e-06, val loss: 0.11047979444265366\n",
      "Epoch 4679: train loss: 7.946662776703306e-07, val loss: 0.1104813739657402\n",
      "Epoch 4680: train loss: 1.8140725615012343e-06, val loss: 0.11096664518117905\n",
      "Epoch 4681: train loss: 1.4719789760420099e-06, val loss: 0.11059153079986572\n",
      "Epoch 4682: train loss: 7.851445502637944e-07, val loss: 0.11042018234729767\n",
      "Epoch 4683: train loss: 1.2981332702111104e-06, val loss: 0.11096686124801636\n",
      "Epoch 4684: train loss: 9.604441402188968e-07, val loss: 0.11094705015420914\n",
      "Epoch 4685: train loss: 8.573461514060909e-07, val loss: 0.1106136366724968\n",
      "Epoch 4686: train loss: 7.870731906223227e-07, val loss: 0.11096447706222534\n",
      "Epoch 4687: train loss: 6.800630103498406e-07, val loss: 0.11110087484121323\n",
      "Epoch 4688: train loss: 8.257450758719642e-07, val loss: 0.1107516884803772\n",
      "Epoch 4689: train loss: 4.854084636463085e-07, val loss: 0.11100559681653976\n",
      "Epoch 4690: train loss: 4.806936431123177e-07, val loss: 0.11124935001134872\n",
      "Epoch 4691: train loss: 6.728805033162644e-07, val loss: 0.11100540310144424\n",
      "Epoch 4692: train loss: 3.679205349271797e-07, val loss: 0.11119049787521362\n",
      "Epoch 4693: train loss: 3.6425345228963124e-07, val loss: 0.11133458465337753\n",
      "Epoch 4694: train loss: 4.802055855179788e-07, val loss: 0.1111873909831047\n",
      "Epoch 4695: train loss: 2.828583660630102e-07, val loss: 0.11142313480377197\n",
      "Epoch 4696: train loss: 3.0529545824720117e-07, val loss: 0.11146370321512222\n",
      "Epoch 4697: train loss: 3.567298563211807e-07, val loss: 0.11140978336334229\n",
      "Epoch 4698: train loss: 1.9182999722033856e-07, val loss: 0.11164505779743195\n",
      "Epoch 4699: train loss: 2.326050605461205e-07, val loss: 0.11164795607328415\n",
      "Epoch 4700: train loss: 2.91122034923319e-07, val loss: 0.11170540004968643\n",
      "Epoch 4701: train loss: 1.5884478443695116e-07, val loss: 0.111843541264534\n",
      "Epoch 4702: train loss: 1.5728647895230097e-07, val loss: 0.11185839027166367\n",
      "Epoch 4703: train loss: 2.0121261457006767e-07, val loss: 0.11195281893014908\n",
      "Epoch 4704: train loss: 1.430435929705709e-07, val loss: 0.11203284561634064\n",
      "Epoch 4705: train loss: 1.3013409727591352e-07, val loss: 0.11218497902154922\n",
      "Epoch 4706: train loss: 1.4923611502126732e-07, val loss: 0.11223026365041733\n",
      "Epoch 4707: train loss: 1.0932956939768701e-07, val loss: 0.11229300498962402\n",
      "Epoch 4708: train loss: 9.028244107867067e-08, val loss: 0.11248364299535751\n",
      "Epoch 4709: train loss: 1.3079981897590187e-07, val loss: 0.11245029419660568\n",
      "Epoch 4710: train loss: 1.3433728440759296e-07, val loss: 0.1126534715294838\n",
      "Epoch 4711: train loss: 6.364906113276447e-08, val loss: 0.11273379623889923\n",
      "Epoch 4712: train loss: 5.5318842839824356e-08, val loss: 0.11274776607751846\n",
      "Epoch 4713: train loss: 9.386187826976311e-08, val loss: 0.11289485543966293\n",
      "Epoch 4714: train loss: 6.604424385159291e-08, val loss: 0.11297543346881866\n",
      "Epoch 4715: train loss: 4.137754316957398e-08, val loss: 0.11305554211139679\n",
      "Epoch 4716: train loss: 6.917878181411652e-08, val loss: 0.11313813179731369\n",
      "Epoch 4717: train loss: 9.152994095984468e-08, val loss: 0.11333322525024414\n",
      "Epoch 4718: train loss: 7.739735963241401e-08, val loss: 0.11334830522537231\n",
      "Epoch 4719: train loss: 7.579443916938544e-08, val loss: 0.11365630477666855\n",
      "Epoch 4720: train loss: 9.599563099982333e-08, val loss: 0.11364109814167023\n",
      "Epoch 4721: train loss: 1.1807379962647246e-07, val loss: 0.11389322578907013\n",
      "Epoch 4722: train loss: 1.1270476818481256e-07, val loss: 0.1139107495546341\n",
      "Epoch 4723: train loss: 1.4324285757538746e-07, val loss: 0.11416888236999512\n",
      "Epoch 4724: train loss: 2.2103344576862582e-07, val loss: 0.11409461498260498\n",
      "Epoch 4725: train loss: 3.793454084188852e-07, val loss: 0.11448998749256134\n",
      "Epoch 4726: train loss: 6.599794346584531e-07, val loss: 0.11420466750860214\n",
      "Epoch 4727: train loss: 1.2889585150333005e-06, val loss: 0.11490949243307114\n",
      "Epoch 4728: train loss: 2.635573991938145e-06, val loss: 0.1141037866473198\n",
      "Epoch 4729: train loss: 5.759300165664172e-06, val loss: 0.11549361050128937\n",
      "Epoch 4730: train loss: 1.2515247362898663e-05, val loss: 0.1134367361664772\n",
      "Epoch 4731: train loss: 2.7439060431788675e-05, val loss: 0.11664652824401855\n",
      "Epoch 4732: train loss: 6.273461622186005e-05, val loss: 0.11157982796430588\n",
      "Epoch 4733: train loss: 0.0001530681474832818, val loss: 0.11923835426568985\n",
      "Epoch 4734: train loss: 0.0003562415367923677, val loss: 0.10676971822977066\n",
      "Epoch 4735: train loss: 0.0008010981837287545, val loss: 0.1227685809135437\n",
      "Epoch 4736: train loss: 0.001410412834957242, val loss: 0.10665800422430038\n",
      "Epoch 4737: train loss: 0.0018433573422953486, val loss: 0.11789698898792267\n",
      "Epoch 4738: train loss: 0.0011791266733780503, val loss: 0.113275907933712\n",
      "Epoch 4739: train loss: 0.00021495633700396866, val loss: 0.10626404732465744\n",
      "Epoch 4740: train loss: 0.00041348571539856493, val loss: 0.11280584335327148\n",
      "Epoch 4741: train loss: 0.0005430706078186631, val loss: 0.11273224651813507\n",
      "Epoch 4742: train loss: 0.00014041538815945387, val loss: 0.10607538372278214\n",
      "Epoch 4743: train loss: 0.00032820296473801136, val loss: 0.10729707777500153\n",
      "Epoch 4744: train loss: 0.00017996733367908746, val loss: 0.11207329481840134\n",
      "Epoch 4745: train loss: 0.0001983505644602701, val loss: 0.10962040722370148\n",
      "Epoch 4746: train loss: 0.00014848759747110307, val loss: 0.10754864662885666\n",
      "Epoch 4747: train loss: 0.00013392430264502764, val loss: 0.10974649339914322\n",
      "Epoch 4748: train loss: 0.00011914408969460055, val loss: 0.11124694347381592\n",
      "Epoch 4749: train loss: 9.843610314419493e-05, val loss: 0.11037950962781906\n",
      "Epoch 4750: train loss: 9.675585170043632e-05, val loss: 0.11042541265487671\n",
      "Epoch 4751: train loss: 6.99248630553484e-05, val loss: 0.11131588369607925\n",
      "Epoch 4752: train loss: 7.323562022065744e-05, val loss: 0.11102314293384552\n",
      "Epoch 4753: train loss: 5.832841998199001e-05, val loss: 0.1103215366601944\n",
      "Epoch 4754: train loss: 5.106790558784269e-05, val loss: 0.11073625087738037\n",
      "Epoch 4755: train loss: 5.378349305829033e-05, val loss: 0.11120032519102097\n",
      "Epoch 4756: train loss: 3.256613126723096e-05, val loss: 0.11064819246530533\n",
      "Epoch 4757: train loss: 4.743735189549625e-05, val loss: 0.11065854132175446\n",
      "Epoch 4758: train loss: 2.189240331063047e-05, val loss: 0.11219501495361328\n",
      "Epoch 4759: train loss: 4.0160928620025516e-05, val loss: 0.11301594227552414\n",
      "Epoch 4760: train loss: 1.716468614176847e-05, val loss: 0.11162107437849045\n",
      "Epoch 4761: train loss: 3.1054896680871025e-05, val loss: 0.11021418869495392\n",
      "Epoch 4762: train loss: 1.568257539474871e-05, val loss: 0.11113045364618301\n",
      "Epoch 4763: train loss: 2.26217871386325e-05, val loss: 0.11291201412677765\n",
      "Epoch 4764: train loss: 1.4770292182220146e-05, val loss: 0.11270544677972794\n",
      "Epoch 4765: train loss: 1.665202944423072e-05, val loss: 0.11102470010519028\n",
      "Epoch 4766: train loss: 1.3016583579883445e-05, val loss: 0.11063458025455475\n",
      "Epoch 4767: train loss: 1.265045466425363e-05, val loss: 0.11203409731388092\n",
      "Epoch 4768: train loss: 1.0966896297759376e-05, val loss: 0.11291422694921494\n",
      "Epoch 4769: train loss: 9.916100680129603e-06, val loss: 0.11207230389118195\n",
      "Epoch 4770: train loss: 9.186164788843598e-06, val loss: 0.11119438707828522\n",
      "Epoch 4771: train loss: 7.991228812898044e-06, val loss: 0.1117636188864708\n",
      "Epoch 4772: train loss: 7.421597274515079e-06, val loss: 0.11264276504516602\n",
      "Epoch 4773: train loss: 6.355101049848599e-06, val loss: 0.11231467872858047\n",
      "Epoch 4774: train loss: 6.163937996461755e-06, val loss: 0.11157243698835373\n",
      "Epoch 4775: train loss: 5.048274942964781e-06, val loss: 0.11197187006473541\n",
      "Epoch 4776: train loss: 5.441533630801132e-06, val loss: 0.11296971142292023\n",
      "Epoch 4777: train loss: 3.530120011419058e-06, val loss: 0.11293935775756836\n",
      "Epoch 4778: train loss: 4.707796506409068e-06, val loss: 0.11204607784748077\n",
      "Epoch 4779: train loss: 2.800060201479937e-06, val loss: 0.1118081733584404\n",
      "Epoch 4780: train loss: 4.000307853857521e-06, val loss: 0.11244402080774307\n",
      "Epoch 4781: train loss: 2.1944808850093978e-06, val loss: 0.1128045916557312\n",
      "Epoch 4782: train loss: 3.107928250756231e-06, val loss: 0.11253805458545685\n",
      "Epoch 4783: train loss: 1.922464889503317e-06, val loss: 0.11243078857660294\n",
      "Epoch 4784: train loss: 2.5358660877827788e-06, val loss: 0.11273243278265\n",
      "Epoch 4785: train loss: 1.6831789935167762e-06, val loss: 0.11287181824445724\n",
      "Epoch 4786: train loss: 1.8267010091221891e-06, val loss: 0.11267880350351334\n",
      "Epoch 4787: train loss: 1.470227402933233e-06, val loss: 0.11264415085315704\n",
      "Epoch 4788: train loss: 1.500173198110133e-06, val loss: 0.11293242126703262\n",
      "Epoch 4789: train loss: 1.3251246855361387e-06, val loss: 0.11315112560987473\n",
      "Epoch 4790: train loss: 1.0996152468578657e-06, val loss: 0.11310303211212158\n",
      "Epoch 4791: train loss: 1.0489490023246617e-06, val loss: 0.11303471773862839\n",
      "Epoch 4792: train loss: 9.135025038631284e-07, val loss: 0.11311348527669907\n",
      "Epoch 4793: train loss: 9.67838218457473e-07, val loss: 0.11322733014822006\n",
      "Epoch 4794: train loss: 6.962252427911153e-07, val loss: 0.11326996237039566\n",
      "Epoch 4795: train loss: 6.629608719777025e-07, val loss: 0.11326940357685089\n",
      "Epoch 4796: train loss: 5.879190894120256e-07, val loss: 0.11340220272541046\n",
      "Epoch 4797: train loss: 5.7045207313422e-07, val loss: 0.11361522972583771\n",
      "Epoch 4798: train loss: 5.146905550645897e-07, val loss: 0.11368159204721451\n",
      "Epoch 4799: train loss: 4.1008752305060625e-07, val loss: 0.11358169466257095\n",
      "Epoch 4800: train loss: 4.169485237071058e-07, val loss: 0.1135503277182579\n",
      "Epoch 4801: train loss: 3.493834128676099e-07, val loss: 0.11366768181324005\n",
      "Epoch 4802: train loss: 3.7735497926405515e-07, val loss: 0.11379372328519821\n",
      "Epoch 4803: train loss: 2.4320135594280146e-07, val loss: 0.11385705322027206\n",
      "Epoch 4804: train loss: 3.244506956434634e-07, val loss: 0.11393582820892334\n",
      "Epoch 4805: train loss: 1.8444129068484472e-07, val loss: 0.1140323281288147\n",
      "Epoch 4806: train loss: 2.9455682692969276e-07, val loss: 0.11406364291906357\n",
      "Epoch 4807: train loss: 1.2897554313440196e-07, val loss: 0.1140192300081253\n",
      "Epoch 4808: train loss: 2.578054250079731e-07, val loss: 0.11402621120214462\n",
      "Epoch 4809: train loss: 9.695146019339518e-08, val loss: 0.11408980190753937\n",
      "Epoch 4810: train loss: 2.1188644439007476e-07, val loss: 0.11415507644414902\n",
      "Epoch 4811: train loss: 7.953212843858637e-08, val loss: 0.11421585083007812\n",
      "Epoch 4812: train loss: 1.7469210433773696e-07, val loss: 0.11426005512475967\n",
      "Epoch 4813: train loss: 7.624154108043513e-08, val loss: 0.11424420028924942\n",
      "Epoch 4814: train loss: 1.2726054876566195e-07, val loss: 0.1142122745513916\n",
      "Epoch 4815: train loss: 7.47232107300988e-08, val loss: 0.11425752937793732\n",
      "Epoch 4816: train loss: 9.237095355274505e-08, val loss: 0.11435499042272568\n",
      "Epoch 4817: train loss: 8.064065326607306e-08, val loss: 0.11439511924982071\n",
      "Epoch 4818: train loss: 6.439825739334992e-08, val loss: 0.11436877399682999\n",
      "Epoch 4819: train loss: 6.940598495930317e-08, val loss: 0.11437397450208664\n",
      "Epoch 4820: train loss: 4.7943970571395766e-08, val loss: 0.11443766206502914\n",
      "Epoch 4821: train loss: 6.360772886182531e-08, val loss: 0.11448538303375244\n",
      "Epoch 4822: train loss: 4.1390652683048756e-08, val loss: 0.1144971176981926\n",
      "Epoch 4823: train loss: 5.050639018122638e-08, val loss: 0.11453147232532501\n",
      "Epoch 4824: train loss: 3.2695048446385044e-08, val loss: 0.11458154022693634\n",
      "Epoch 4825: train loss: 3.845271834279629e-08, val loss: 0.11459501087665558\n",
      "Epoch 4826: train loss: 3.5753224381096516e-08, val loss: 0.11461444199085236\n",
      "Epoch 4827: train loss: 3.115088276217648e-08, val loss: 0.11467234045267105\n",
      "Epoch 4828: train loss: 2.5404911951909526e-08, val loss: 0.11472325772047043\n",
      "Epoch 4829: train loss: 2.2208649141930437e-08, val loss: 0.11475870758295059\n",
      "Epoch 4830: train loss: 2.813600730178223e-08, val loss: 0.11481750011444092\n",
      "Epoch 4831: train loss: 1.880801292486467e-08, val loss: 0.11486393213272095\n",
      "Epoch 4832: train loss: 1.980194852535533e-08, val loss: 0.11487209796905518\n",
      "Epoch 4833: train loss: 1.581164177366645e-08, val loss: 0.11491554975509644\n",
      "Epoch 4834: train loss: 1.8010505087318052e-08, val loss: 0.11499326676130295\n",
      "Epoch 4835: train loss: 1.5179400847387114e-08, val loss: 0.1150251179933548\n",
      "Epoch 4836: train loss: 1.1180436132463001e-08, val loss: 0.1150493249297142\n",
      "Epoch 4837: train loss: 1.717189235250771e-08, val loss: 0.11511677503585815\n",
      "Epoch 4838: train loss: 9.629470554273212e-09, val loss: 0.11515369266271591\n",
      "Epoch 4839: train loss: 9.28060206462078e-09, val loss: 0.11516731977462769\n",
      "Epoch 4840: train loss: 1.3692956990496441e-08, val loss: 0.11523705720901489\n",
      "Epoch 4841: train loss: 8.49444425909951e-09, val loss: 0.11529290676116943\n",
      "Epoch 4842: train loss: 8.538342477493188e-09, val loss: 0.11530251801013947\n",
      "Epoch 4843: train loss: 6.61006760438454e-09, val loss: 0.11534757912158966\n",
      "Epoch 4844: train loss: 1.0437082309522339e-08, val loss: 0.11540986597537994\n",
      "Epoch 4845: train loss: 7.126839118853923e-09, val loss: 0.11544821411371231\n",
      "Epoch 4846: train loss: 4.729493241484306e-09, val loss: 0.11548095941543579\n",
      "Epoch 4847: train loss: 8.624147618263578e-09, val loss: 0.11552868038415909\n",
      "Epoch 4848: train loss: 4.881607562623458e-09, val loss: 0.1155707836151123\n",
      "Epoch 4849: train loss: 6.013692654960323e-09, val loss: 0.11560074239969254\n",
      "Epoch 4850: train loss: 6.299649246699346e-09, val loss: 0.11564741283655167\n",
      "Epoch 4851: train loss: 4.435002143310385e-09, val loss: 0.11570357531309128\n",
      "Epoch 4852: train loss: 4.981852264052122e-09, val loss: 0.11573147773742676\n",
      "Epoch 4853: train loss: 4.420670052240894e-09, val loss: 0.11576119810342789\n",
      "Epoch 4854: train loss: 5.413744119664443e-09, val loss: 0.11582297086715698\n",
      "Epoch 4855: train loss: 4.538957654176556e-09, val loss: 0.11586667597293854\n",
      "Epoch 4856: train loss: 3.920539892732222e-09, val loss: 0.11589918285608292\n",
      "Epoch 4857: train loss: 4.433036604467588e-09, val loss: 0.11595837026834488\n",
      "Epoch 4858: train loss: 4.53474058303982e-09, val loss: 0.11599951237440109\n",
      "Epoch 4859: train loss: 3.9436440779638815e-09, val loss: 0.11602609604597092\n",
      "Epoch 4860: train loss: 3.3917477715306177e-09, val loss: 0.11607459932565689\n",
      "Epoch 4861: train loss: 3.392158332005124e-09, val loss: 0.11611848324537277\n",
      "Epoch 4862: train loss: 3.490194577793204e-09, val loss: 0.11615592241287231\n",
      "Epoch 4863: train loss: 3.5490599348264595e-09, val loss: 0.11620128154754639\n",
      "Epoch 4864: train loss: 3.512645507797174e-09, val loss: 0.11624184995889664\n",
      "Epoch 4865: train loss: 2.977955437799551e-09, val loss: 0.11628462374210358\n",
      "Epoch 4866: train loss: 3.35822814001574e-09, val loss: 0.11633381992578506\n",
      "Epoch 4867: train loss: 3.792229641419453e-09, val loss: 0.11637353897094727\n",
      "Epoch 4868: train loss: 3.1821310031432404e-09, val loss: 0.11640947312116623\n",
      "Epoch 4869: train loss: 2.8546562891307303e-09, val loss: 0.11644541472196579\n",
      "Epoch 4870: train loss: 3.0328064504203667e-09, val loss: 0.11648049205541611\n",
      "Epoch 4871: train loss: 3.216806376826753e-09, val loss: 0.11653169244527817\n",
      "Epoch 4872: train loss: 2.6803339547853966e-09, val loss: 0.11657998710870743\n",
      "Epoch 4873: train loss: 3.105272261549885e-09, val loss: 0.1166171208024025\n",
      "Epoch 4874: train loss: 2.986553004902248e-09, val loss: 0.11666536331176758\n",
      "Epoch 4875: train loss: 2.737706283895136e-09, val loss: 0.11670184135437012\n",
      "Epoch 4876: train loss: 3.08163716766785e-09, val loss: 0.11673956364393234\n",
      "Epoch 4877: train loss: 3.193235897924751e-09, val loss: 0.11678566783666611\n",
      "Epoch 4878: train loss: 2.8657543005294883e-09, val loss: 0.11682441085577011\n",
      "Epoch 4879: train loss: 3.4409475269114864e-09, val loss: 0.11686766147613525\n",
      "Epoch 4880: train loss: 3.4664400239137194e-09, val loss: 0.11690445244312286\n",
      "Epoch 4881: train loss: 5.3943667310818455e-09, val loss: 0.11694853752851486\n",
      "Epoch 4882: train loss: 7.455603245887232e-09, val loss: 0.11699429899454117\n",
      "Epoch 4883: train loss: 1.6566723104460834e-08, val loss: 0.1170373186469078\n",
      "Epoch 4884: train loss: 3.4747120736255965e-08, val loss: 0.11707252264022827\n",
      "Epoch 4885: train loss: 9.758855412655976e-08, val loss: 0.11711889505386353\n",
      "Epoch 4886: train loss: 2.689140217171371e-07, val loss: 0.11713800579309464\n",
      "Epoch 4887: train loss: 8.358964578292216e-07, val loss: 0.11720146983861923\n",
      "Epoch 4888: train loss: 2.6078578230226412e-06, val loss: 0.11708895117044449\n",
      "Epoch 4889: train loss: 8.05771560408175e-06, val loss: 0.11727374792098999\n",
      "Epoch 4890: train loss: 2.4713870516279712e-05, val loss: 0.11660051345825195\n",
      "Epoch 4891: train loss: 7.92649807408452e-05, val loss: 0.11734693497419357\n",
      "Epoch 4892: train loss: 0.00023872384917922318, val loss: 0.11468523740768433\n",
      "Epoch 4893: train loss: 0.0006336814258247614, val loss: 0.1161666288971901\n",
      "Epoch 4894: train loss: 0.0012452179798856378, val loss: 0.11130224913358688\n",
      "Epoch 4895: train loss: 0.0017346087843179703, val loss: 0.11648984253406525\n",
      "Epoch 4896: train loss: 0.00110789411701262, val loss: 0.11467629671096802\n",
      "Epoch 4897: train loss: 0.00047779950546100736, val loss: 0.11749935150146484\n",
      "Epoch 4898: train loss: 0.0004088720306754112, val loss: 0.1138785257935524\n",
      "Epoch 4899: train loss: 0.0004054225573781878, val loss: 0.11436808109283447\n",
      "Epoch 4900: train loss: 0.00028820481384173036, val loss: 0.11585447937250137\n",
      "Epoch 4901: train loss: 0.0002381746016908437, val loss: 0.11245767027139664\n",
      "Epoch 4902: train loss: 0.00023036797938402742, val loss: 0.11173781007528305\n",
      "Epoch 4903: train loss: 0.00018166881636716425, val loss: 0.11336381733417511\n",
      "Epoch 4904: train loss: 0.00013170573220122606, val loss: 0.11185953766107559\n",
      "Epoch 4905: train loss: 0.00016842380864545703, val loss: 0.11016624420881271\n",
      "Epoch 4906: train loss: 8.25068200356327e-05, val loss: 0.11185742914676666\n",
      "Epoch 4907: train loss: 0.00013771500380244106, val loss: 0.11289482563734055\n",
      "Epoch 4908: train loss: 8.419710502494127e-05, val loss: 0.11148088425397873\n",
      "Epoch 4909: train loss: 9.56396252149716e-05, val loss: 0.11011277884244919\n",
      "Epoch 4910: train loss: 6.584911170648411e-05, val loss: 0.11096614599227905\n",
      "Epoch 4911: train loss: 7.301373989321291e-05, val loss: 0.11241283267736435\n",
      "Epoch 4912: train loss: 4.9482154281577095e-05, val loss: 0.11215722560882568\n",
      "Epoch 4913: train loss: 6.108433444751427e-05, val loss: 0.11082728952169418\n",
      "Epoch 4914: train loss: 4.306160917622037e-05, val loss: 0.1116037592291832\n",
      "Epoch 4915: train loss: 4.533371247816831e-05, val loss: 0.11274939030408859\n",
      "Epoch 4916: train loss: 3.761081461561844e-05, val loss: 0.11224649101495743\n",
      "Epoch 4917: train loss: 3.160758933518082e-05, val loss: 0.11126239597797394\n",
      "Epoch 4918: train loss: 3.492338510113768e-05, val loss: 0.11195530742406845\n",
      "Epoch 4919: train loss: 2.0976396626792848e-05, val loss: 0.11311855167150497\n",
      "Epoch 4920: train loss: 3.3962816814891994e-05, val loss: 0.11242921650409698\n",
      "Epoch 4921: train loss: 1.3959920579509344e-05, val loss: 0.11138367652893066\n",
      "Epoch 4922: train loss: 2.8980632123420946e-05, val loss: 0.11164369434118271\n",
      "Epoch 4923: train loss: 1.3308736924955156e-05, val loss: 0.1115410327911377\n",
      "Epoch 4924: train loss: 1.8864238882088102e-05, val loss: 0.11127375811338425\n",
      "Epoch 4925: train loss: 1.4323170034913346e-05, val loss: 0.11210348457098007\n",
      "Epoch 4926: train loss: 1.2298206456762273e-05, val loss: 0.11279416084289551\n",
      "Epoch 4927: train loss: 1.339896425633924e-05, val loss: 0.11220164597034454\n",
      "Epoch 4928: train loss: 9.066870916285552e-06, val loss: 0.11157428473234177\n",
      "Epoch 4929: train loss: 1.1048876331187785e-05, val loss: 0.1117151603102684\n",
      "Epoch 4930: train loss: 6.944570031919284e-06, val loss: 0.11168599128723145\n",
      "Epoch 4931: train loss: 9.093305379792582e-06, val loss: 0.11126643419265747\n",
      "Epoch 4932: train loss: 5.867895197297912e-06, val loss: 0.11145051568746567\n",
      "Epoch 4933: train loss: 6.715612471452914e-06, val loss: 0.1119987741112709\n",
      "Epoch 4934: train loss: 5.424371465778677e-06, val loss: 0.11208295822143555\n",
      "Epoch 4935: train loss: 4.8855654313229024e-06, val loss: 0.11177688091993332\n",
      "Epoch 4936: train loss: 5.352639618649846e-06, val loss: 0.1115451380610466\n",
      "Epoch 4937: train loss: 2.8674310215137666e-06, val loss: 0.1114305779337883\n",
      "Epoch 4938: train loss: 4.995577455702005e-06, val loss: 0.11163480579853058\n",
      "Epoch 4939: train loss: 2.1231742266536457e-06, val loss: 0.11222433298826218\n",
      "Epoch 4940: train loss: 4.2325100366724655e-06, val loss: 0.11252494156360626\n",
      "Epoch 4941: train loss: 1.6426859019702533e-06, val loss: 0.11216308921575546\n",
      "Epoch 4942: train loss: 3.3505850751680555e-06, val loss: 0.11191399395465851\n",
      "Epoch 4943: train loss: 1.4975175872677937e-06, val loss: 0.11229594051837921\n",
      "Epoch 4944: train loss: 2.5666981855465565e-06, val loss: 0.11267125606536865\n",
      "Epoch 4945: train loss: 1.4753168215975165e-06, val loss: 0.11263598501682281\n",
      "Epoch 4946: train loss: 1.8059715785057051e-06, val loss: 0.11271138489246368\n",
      "Epoch 4947: train loss: 1.437327341591299e-06, val loss: 0.11307127773761749\n",
      "Epoch 4948: train loss: 1.3181009990148596e-06, val loss: 0.11320000886917114\n",
      "Epoch 4949: train loss: 1.3343675391297438e-06, val loss: 0.11303293704986572\n",
      "Epoch 4950: train loss: 9.951940000973991e-07, val loss: 0.11312653124332428\n",
      "Epoch 4951: train loss: 1.1531668633324443e-06, val loss: 0.11345495283603668\n",
      "Epoch 4952: train loss: 8.302452556563367e-07, val loss: 0.1137075200676918\n",
      "Epoch 4953: train loss: 9.279509072257497e-07, val loss: 0.11381518095731735\n",
      "Epoch 4954: train loss: 7.027159085737367e-07, val loss: 0.11383900791406631\n",
      "Epoch 4955: train loss: 7.311138574550569e-07, val loss: 0.11388104408979416\n",
      "Epoch 4956: train loss: 5.608789024336147e-07, val loss: 0.11410246044397354\n",
      "Epoch 4957: train loss: 6.34896707651933e-07, val loss: 0.11440389603376389\n",
      "Epoch 4958: train loss: 4.409635891988728e-07, val loss: 0.11450560390949249\n",
      "Epoch 4959: train loss: 5.356895371733117e-07, val loss: 0.11452479660511017\n",
      "Epoch 4960: train loss: 3.252991405133798e-07, val loss: 0.11477624624967575\n",
      "Epoch 4961: train loss: 4.49352882014864e-07, val loss: 0.11507485061883926\n",
      "Epoch 4962: train loss: 3.185000991834386e-07, val loss: 0.1150672659277916\n",
      "Epoch 4963: train loss: 2.964356440315896e-07, val loss: 0.11498750746250153\n",
      "Epoch 4964: train loss: 3.272892854511156e-07, val loss: 0.11520587652921677\n",
      "Epoch 4965: train loss: 1.813951229223676e-07, val loss: 0.11552627384662628\n",
      "Epoch 4966: train loss: 2.9219029329397017e-07, val loss: 0.11559593677520752\n",
      "Epoch 4967: train loss: 1.8075813557061338e-07, val loss: 0.11558439582586288\n",
      "Epoch 4968: train loss: 1.895120220751778e-07, val loss: 0.11575460433959961\n",
      "Epoch 4969: train loss: 1.8252164579735108e-07, val loss: 0.1159048080444336\n",
      "Epoch 4970: train loss: 1.3858694103419111e-07, val loss: 0.11592505127191544\n",
      "Epoch 4971: train loss: 1.5509340300923213e-07, val loss: 0.11606204509735107\n",
      "Epoch 4972: train loss: 1.3214149419127352e-07, val loss: 0.11627770960330963\n",
      "Epoch 4973: train loss: 1.070390212021266e-07, val loss: 0.11631520092487335\n",
      "Epoch 4974: train loss: 1.1387724896394502e-07, val loss: 0.11631198972463608\n",
      "Epoch 4975: train loss: 9.609036055735487e-08, val loss: 0.11647158116102219\n",
      "Epoch 4976: train loss: 8.730727785177805e-08, val loss: 0.11663700640201569\n",
      "Epoch 4977: train loss: 9.152354607522284e-08, val loss: 0.11668621748685837\n",
      "Epoch 4978: train loss: 4.8241698635820285e-08, val loss: 0.11673634499311447\n",
      "Epoch 4979: train loss: 9.905978259894255e-08, val loss: 0.11686521768569946\n",
      "Epoch 4980: train loss: 3.473533993769706e-08, val loss: 0.11699102073907852\n",
      "Epoch 4981: train loss: 7.766937670794505e-08, val loss: 0.11702585220336914\n",
      "Epoch 4982: train loss: 4.198652447939821e-08, val loss: 0.11706848442554474\n",
      "Epoch 4983: train loss: 4.644203954740078e-08, val loss: 0.11721289157867432\n",
      "Epoch 4984: train loss: 4.420387256232061e-08, val loss: 0.11732801049947739\n",
      "Epoch 4985: train loss: 4.84185989080288e-08, val loss: 0.11738155037164688\n",
      "Epoch 4986: train loss: 3.035488660430019e-08, val loss: 0.11748967319726944\n",
      "Epoch 4987: train loss: 2.5935770864293772e-08, val loss: 0.1175873801112175\n",
      "Epoch 4988: train loss: 4.859701974169184e-08, val loss: 0.11763155460357666\n",
      "Epoch 4989: train loss: 2.7468489705029242e-08, val loss: 0.1177155002951622\n",
      "Epoch 4990: train loss: 1.7902824112070448e-08, val loss: 0.11783505976200104\n",
      "Epoch 4991: train loss: 2.723786174385623e-08, val loss: 0.11791501194238663\n",
      "Epoch 4992: train loss: 3.152831595798489e-08, val loss: 0.11798448860645294\n",
      "Epoch 4993: train loss: 1.818481720761156e-08, val loss: 0.11809583008289337\n",
      "Epoch 4994: train loss: 1.5941267861308006e-08, val loss: 0.11820472776889801\n",
      "Epoch 4995: train loss: 1.8652801969665234e-08, val loss: 0.11826278269290924\n",
      "Epoch 4996: train loss: 1.8721269867683077e-08, val loss: 0.11834397166967392\n",
      "Epoch 4997: train loss: 2.0178708481921603e-08, val loss: 0.11845123767852783\n",
      "Epoch 4998: train loss: 1.1096456198345095e-08, val loss: 0.1184978038072586\n",
      "Epoch 4999: train loss: 9.162635983273049e-09, val loss: 0.11857473850250244\n",
      "Epoch 5000: train loss: 1.6450400153189548e-08, val loss: 0.1187073141336441\n",
      "Epoch 5001: train loss: 1.1693495061138037e-08, val loss: 0.11877013742923737\n",
      "Epoch 5002: train loss: 1.3423601785689243e-08, val loss: 0.11882799118757248\n",
      "Epoch 5003: train loss: 1.0546233220054546e-08, val loss: 0.11891331523656845\n",
      "Epoch 5004: train loss: 6.793220652667742e-09, val loss: 0.11900418251752853\n",
      "Epoch 5005: train loss: 7.653832234666424e-09, val loss: 0.11907017230987549\n",
      "Epoch 5006: train loss: 8.218942859627987e-09, val loss: 0.11915554851293564\n",
      "Epoch 5007: train loss: 1.0625353930038273e-08, val loss: 0.11921381205320358\n",
      "Epoch 5008: train loss: 1.1642836028613601e-08, val loss: 0.11930370330810547\n",
      "Epoch 5009: train loss: 1.1437721880724894e-08, val loss: 0.1193763017654419\n",
      "Epoch 5010: train loss: 1.1815295408723614e-08, val loss: 0.11946471035480499\n",
      "Epoch 5011: train loss: 1.4269815551415377e-08, val loss: 0.11954065412282944\n",
      "Epoch 5012: train loss: 1.6054137574883498e-08, val loss: 0.11960139125585556\n",
      "Epoch 5013: train loss: 2.1087986468160125e-08, val loss: 0.11969425529241562\n",
      "Epoch 5014: train loss: 3.089584055260275e-08, val loss: 0.1197633296251297\n",
      "Epoch 5015: train loss: 5.5663726072907593e-08, val loss: 0.11985182762145996\n",
      "Epoch 5016: train loss: 1.0496769675683026e-07, val loss: 0.11988281458616257\n",
      "Epoch 5017: train loss: 2.1061585186998855e-07, val loss: 0.12006274610757828\n",
      "Epoch 5018: train loss: 3.521116411775438e-07, val loss: 0.11991137266159058\n",
      "Epoch 5019: train loss: 6.461444854721776e-07, val loss: 0.12042617797851562\n",
      "Epoch 5020: train loss: 1.1543772870936664e-06, val loss: 0.11982624977827072\n",
      "Epoch 5021: train loss: 2.0030799987580394e-06, val loss: 0.12075386196374893\n",
      "Epoch 5022: train loss: 2.7501127988216467e-06, val loss: 0.11988426744937897\n",
      "Epoch 5023: train loss: 3.1280274015443865e-06, val loss: 0.1209123358130455\n",
      "Epoch 5024: train loss: 3.0947765026212437e-06, val loss: 0.12010236084461212\n",
      "Epoch 5025: train loss: 2.9705465749430005e-06, val loss: 0.12102595716714859\n",
      "Epoch 5026: train loss: 3.1603474326402647e-06, val loss: 0.12025826424360275\n",
      "Epoch 5027: train loss: 4.751956566906301e-06, val loss: 0.12117701768875122\n",
      "Epoch 5028: train loss: 1.012520442600362e-05, val loss: 0.12033169716596603\n",
      "Epoch 5029: train loss: 2.6489802621654235e-05, val loss: 0.12178661674261093\n",
      "Epoch 5030: train loss: 7.559133518952876e-05, val loss: 0.11976706236600876\n",
      "Epoch 5031: train loss: 0.00022288106265477836, val loss: 0.1232200413942337\n",
      "Epoch 5032: train loss: 0.0005957606481388211, val loss: 0.11904176324605942\n",
      "Epoch 5033: train loss: 0.0011383167002350092, val loss: 0.12352745980024338\n",
      "Epoch 5034: train loss: 0.001066177967004478, val loss: 0.12036166340112686\n",
      "Epoch 5035: train loss: 0.00012250381405465305, val loss: 0.1181870698928833\n",
      "Epoch 5036: train loss: 0.00046078936429694295, val loss: 0.1194973737001419\n",
      "Epoch 5037: train loss: 0.00026211701333522797, val loss: 0.11839214712381363\n",
      "Epoch 5038: train loss: 0.0002624022599775344, val loss: 0.11253031343221664\n",
      "Epoch 5039: train loss: 0.00017112235946115106, val loss: 0.11587512493133545\n",
      "Epoch 5040: train loss: 0.0002173831162508577, val loss: 0.1197873130440712\n",
      "Epoch 5041: train loss: 9.821244020713493e-05, val loss: 0.11649291962385178\n",
      "Epoch 5042: train loss: 0.0001741572777973488, val loss: 0.11318474262952805\n",
      "Epoch 5043: train loss: 9.001911530504003e-05, val loss: 0.11278914660215378\n",
      "Epoch 5044: train loss: 0.00010362717148382217, val loss: 0.11413123458623886\n",
      "Epoch 5045: train loss: 9.238996426574886e-05, val loss: 0.11606927216053009\n",
      "Epoch 5046: train loss: 7.272880611708388e-05, val loss: 0.11529163271188736\n",
      "Epoch 5047: train loss: 6.587848474737257e-05, val loss: 0.11307362467050552\n",
      "Epoch 5048: train loss: 6.959191523492336e-05, val loss: 0.11199412494897842\n",
      "Epoch 5049: train loss: 4.2818657675525174e-05, val loss: 0.11250083893537521\n",
      "Epoch 5050: train loss: 5.3724059398518875e-05, val loss: 0.11406064033508301\n",
      "Epoch 5051: train loss: 4.113885370315984e-05, val loss: 0.11502920836210251\n",
      "Epoch 5052: train loss: 3.637032205006108e-05, val loss: 0.11355151981115341\n",
      "Epoch 5053: train loss: 3.213984746253118e-05, val loss: 0.11187491565942764\n",
      "Epoch 5054: train loss: 3.2576805097050965e-05, val loss: 0.11193206161260605\n",
      "Epoch 5055: train loss: 2.172712811443489e-05, val loss: 0.11277995258569717\n",
      "Epoch 5056: train loss: 2.781324292300269e-05, val loss: 0.11402614414691925\n",
      "Epoch 5057: train loss: 1.820307079469785e-05, val loss: 0.11503523588180542\n",
      "Epoch 5058: train loss: 1.9469696781015955e-05, val loss: 0.11423325538635254\n",
      "Epoch 5059: train loss: 1.867675018729642e-05, val loss: 0.11222874373197556\n",
      "Epoch 5060: train loss: 1.2032833183184266e-05, val loss: 0.11168519407510757\n",
      "Epoch 5061: train loss: 1.662712929828558e-05, val loss: 0.11341553926467896\n",
      "Epoch 5062: train loss: 1.0187423868046608e-05, val loss: 0.1150394082069397\n",
      "Epoch 5063: train loss: 1.1055016329919454e-05, val loss: 0.11440303176641464\n",
      "Epoch 5064: train loss: 1.0659631698217709e-05, val loss: 0.1129080280661583\n",
      "Epoch 5065: train loss: 8.038161467993632e-06, val loss: 0.11272037029266357\n",
      "Epoch 5066: train loss: 8.071548109001014e-06, val loss: 0.11351026594638824\n",
      "Epoch 5067: train loss: 7.639266186743043e-06, val loss: 0.11422207206487656\n",
      "Epoch 5068: train loss: 5.782838343293406e-06, val loss: 0.11436352878808975\n",
      "Epoch 5069: train loss: 5.872990186617244e-06, val loss: 0.11397890001535416\n",
      "Epoch 5070: train loss: 6.007750016578939e-06, val loss: 0.11339501291513443\n",
      "Epoch 5071: train loss: 3.2906468732107896e-06, val loss: 0.11326555162668228\n",
      "Epoch 5072: train loss: 5.576169769483386e-06, val loss: 0.11405116319656372\n",
      "Epoch 5073: train loss: 3.0332539608934894e-06, val loss: 0.11473745107650757\n",
      "Epoch 5074: train loss: 3.5899133763450664e-06, val loss: 0.11432995647192001\n",
      "Epoch 5075: train loss: 3.2295820346917026e-06, val loss: 0.11380260437726974\n",
      "Epoch 5076: train loss: 2.7342603061697446e-06, val loss: 0.11388484388589859\n",
      "Epoch 5077: train loss: 2.3959833015396725e-06, val loss: 0.11411482095718384\n",
      "Epoch 5078: train loss: 2.5127203571173595e-06, val loss: 0.11427859216928482\n",
      "Epoch 5079: train loss: 1.9491708371788263e-06, val loss: 0.11451955884695053\n",
      "Epoch 5080: train loss: 1.6928141803873586e-06, val loss: 0.11461050808429718\n",
      "Epoch 5081: train loss: 2.1318201106623746e-06, val loss: 0.11429913341999054\n",
      "Epoch 5082: train loss: 9.71907297753205e-07, val loss: 0.1141590103507042\n",
      "Epoch 5083: train loss: 1.8412254121358274e-06, val loss: 0.11479245871305466\n",
      "Epoch 5084: train loss: 1.0357700830354588e-06, val loss: 0.11532187461853027\n",
      "Epoch 5085: train loss: 1.1142994935653405e-06, val loss: 0.1149938628077507\n",
      "Epoch 5086: train loss: 1.124543132391409e-06, val loss: 0.11451869457960129\n",
      "Epoch 5087: train loss: 8.598796625847172e-07, val loss: 0.11459546536207199\n",
      "Epoch 5088: train loss: 8.421670258940139e-07, val loss: 0.11502959579229355\n",
      "Epoch 5089: train loss: 8.145441370288609e-07, val loss: 0.1152442917227745\n",
      "Epoch 5090: train loss: 5.639749360852875e-07, val loss: 0.11526940017938614\n",
      "Epoch 5091: train loss: 7.550434588665667e-07, val loss: 0.11528102308511734\n",
      "Epoch 5092: train loss: 4.823370431950025e-07, val loss: 0.11518826335668564\n",
      "Epoch 5093: train loss: 5.186121825317969e-07, val loss: 0.11519666016101837\n",
      "Epoch 5094: train loss: 4.880233177573245e-07, val loss: 0.11546134203672409\n",
      "Epoch 5095: train loss: 4.0186182559409644e-07, val loss: 0.11574435234069824\n",
      "Epoch 5096: train loss: 3.758047455448832e-07, val loss: 0.11575283855199814\n",
      "Epoch 5097: train loss: 3.6411222481547156e-07, val loss: 0.11562955379486084\n",
      "Epoch 5098: train loss: 2.7163304139321554e-07, val loss: 0.1156221404671669\n",
      "Epoch 5099: train loss: 3.1734830940877146e-07, val loss: 0.11580238491296768\n",
      "Epoch 5100: train loss: 2.3587341502206982e-07, val loss: 0.1160287857055664\n",
      "Epoch 5101: train loss: 2.463990540491068e-07, val loss: 0.116022028028965\n",
      "Epoch 5102: train loss: 2.033853974126032e-07, val loss: 0.11600103229284286\n",
      "Epoch 5103: train loss: 2.1903009894685965e-07, val loss: 0.11603867262601852\n",
      "Epoch 5104: train loss: 1.578756183562291e-07, val loss: 0.11626851558685303\n",
      "Epoch 5105: train loss: 2.121953457390191e-07, val loss: 0.11637149006128311\n",
      "Epoch 5106: train loss: 1.5369349171123758e-07, val loss: 0.11643965542316437\n",
      "Epoch 5107: train loss: 2.3641251800654572e-07, val loss: 0.11632721871137619\n",
      "Epoch 5108: train loss: 2.8087310965929646e-07, val loss: 0.11660333722829819\n",
      "Epoch 5109: train loss: 5.292190508043859e-07, val loss: 0.1164940744638443\n",
      "Epoch 5110: train loss: 1.1257296819167095e-06, val loss: 0.11692261695861816\n",
      "Epoch 5111: train loss: 2.807591954478994e-06, val loss: 0.1162860319018364\n",
      "Epoch 5112: train loss: 7.76366960053565e-06, val loss: 0.11758098751306534\n",
      "Epoch 5113: train loss: 2.251117621199228e-05, val loss: 0.11587981134653091\n",
      "Epoch 5114: train loss: 5.917234011576511e-05, val loss: 0.11842232197523117\n",
      "Epoch 5115: train loss: 0.00011066199658671394, val loss: 0.11524524539709091\n",
      "Epoch 5116: train loss: 0.0001482829247834161, val loss: 0.11953452974557877\n",
      "Epoch 5117: train loss: 0.00014107827155385166, val loss: 0.11652647703886032\n",
      "Epoch 5118: train loss: 8.706057269591838e-05, val loss: 0.11967407912015915\n",
      "Epoch 5119: train loss: 3.231775190215558e-05, val loss: 0.1182183176279068\n",
      "Epoch 5120: train loss: 2.5530294806230813e-05, val loss: 0.11748790740966797\n",
      "Epoch 5121: train loss: 4.0526716475142166e-05, val loss: 0.11896850913763046\n",
      "Epoch 5122: train loss: 3.310006286483258e-05, val loss: 0.11742188781499863\n",
      "Epoch 5123: train loss: 2.057231540675275e-05, val loss: 0.11845242232084274\n",
      "Epoch 5124: train loss: 2.8545291570480913e-05, val loss: 0.11715739220380783\n",
      "Epoch 5125: train loss: 3.102356640738435e-05, val loss: 0.11800737679004669\n",
      "Epoch 5126: train loss: 1.4684196685266215e-05, val loss: 0.11604063957929611\n",
      "Epoch 5127: train loss: 1.2108899682061747e-05, val loss: 0.11395473778247833\n",
      "Epoch 5128: train loss: 1.6460973711218685e-05, val loss: 0.11506911367177963\n",
      "Epoch 5129: train loss: 1.1220269698242191e-05, val loss: 0.11335311084985733\n",
      "Epoch 5130: train loss: 1.2429428352334071e-05, val loss: 0.11359278112649918\n",
      "Epoch 5131: train loss: 1.6619196685496718e-05, val loss: 0.11320822685956955\n",
      "Epoch 5132: train loss: 7.613491106894799e-06, val loss: 0.11273156851530075\n",
      "Epoch 5133: train loss: 1.720983391351183e-06, val loss: 0.11284741014242172\n",
      "Epoch 5134: train loss: 9.162828064290807e-06, val loss: 0.11194540560245514\n",
      "Epoch 5135: train loss: 1.1507662748044822e-05, val loss: 0.11263322085142136\n",
      "Epoch 5136: train loss: 6.051983291399665e-06, val loss: 0.11163511127233505\n",
      "Epoch 5137: train loss: 3.5377486256038537e-06, val loss: 0.11170034855604172\n",
      "Epoch 5138: train loss: 3.4266527109139133e-06, val loss: 0.11233482509851456\n",
      "Epoch 5139: train loss: 4.423133759701159e-06, val loss: 0.11156002432107925\n",
      "Epoch 5140: train loss: 6.461443717853399e-06, val loss: 0.11205506324768066\n",
      "Epoch 5141: train loss: 4.730513410322601e-06, val loss: 0.11176507920026779\n",
      "Epoch 5142: train loss: 1.1576717042771634e-06, val loss: 0.11176460981369019\n",
      "Epoch 5143: train loss: 2.067000878014369e-06, val loss: 0.11196786165237427\n",
      "Epoch 5144: train loss: 4.2306810428272e-06, val loss: 0.11180903762578964\n",
      "Epoch 5145: train loss: 2.973084065160947e-06, val loss: 0.11212966591119766\n",
      "Epoch 5146: train loss: 1.5326297670981148e-06, val loss: 0.11167895793914795\n",
      "Epoch 5147: train loss: 2.2392725895770127e-06, val loss: 0.11199072748422623\n",
      "Epoch 5148: train loss: 2.720221573326853e-06, val loss: 0.11181534826755524\n",
      "Epoch 5149: train loss: 2.1493094664037926e-06, val loss: 0.11204490810632706\n",
      "Epoch 5150: train loss: 1.5072375845193164e-06, val loss: 0.11216848343610764\n",
      "Epoch 5151: train loss: 1.0239818948321044e-06, val loss: 0.11191240698099136\n",
      "Epoch 5152: train loss: 8.411363978666486e-07, val loss: 0.11229970306158066\n",
      "Epoch 5153: train loss: 1.3555995792557951e-06, val loss: 0.11197695881128311\n",
      "Epoch 5154: train loss: 2.1711309727834305e-06, val loss: 0.11232757568359375\n",
      "Epoch 5155: train loss: 2.3555262487207074e-06, val loss: 0.11220496892929077\n",
      "Epoch 5156: train loss: 1.7364388895657612e-06, val loss: 0.11250962316989899\n",
      "Epoch 5157: train loss: 1.4622942217101809e-06, val loss: 0.11218207329511642\n",
      "Epoch 5158: train loss: 2.03856643565814e-06, val loss: 0.11294694244861603\n",
      "Epoch 5159: train loss: 2.9604145765915746e-06, val loss: 0.1121332049369812\n",
      "Epoch 5160: train loss: 3.938597728847526e-06, val loss: 0.11327948421239853\n",
      "Epoch 5161: train loss: 5.384794803831028e-06, val loss: 0.11210625618696213\n",
      "Epoch 5162: train loss: 8.554795385862235e-06, val loss: 0.1137048527598381\n",
      "Epoch 5163: train loss: 1.5038472156447824e-05, val loss: 0.1119609847664833\n",
      "Epoch 5164: train loss: 2.894004683184903e-05, val loss: 0.11472000181674957\n",
      "Epoch 5165: train loss: 5.7675257266964763e-05, val loss: 0.11094186455011368\n",
      "Epoch 5166: train loss: 0.00012601799971889704, val loss: 0.11735121160745621\n",
      "Epoch 5167: train loss: 0.00027191100525669754, val loss: 0.10925569385290146\n",
      "Epoch 5168: train loss: 0.0006019217544235289, val loss: 0.12394821643829346\n",
      "Epoch 5169: train loss: 0.0010431751143187284, val loss: 0.11163715273141861\n",
      "Epoch 5170: train loss: 0.0014480429235845804, val loss: 0.12483859062194824\n",
      "Epoch 5171: train loss: 0.0008590383804403245, val loss: 0.12244755029678345\n",
      "Epoch 5172: train loss: 0.00010274014493916184, val loss: 0.11864679306745529\n",
      "Epoch 5173: train loss: 0.00039843027479946613, val loss: 0.11857280880212784\n",
      "Epoch 5174: train loss: 0.0003815937088802457, val loss: 0.12150163948535919\n",
      "Epoch 5175: train loss: 8.30187345854938e-05, val loss: 0.11486158519983292\n",
      "Epoch 5176: train loss: 0.00030281953513622284, val loss: 0.11722548305988312\n",
      "Epoch 5177: train loss: 8.399665239267051e-05, val loss: 0.12165599316358566\n",
      "Epoch 5178: train loss: 0.00018708666902966797, val loss: 0.11729405075311661\n",
      "Epoch 5179: train loss: 8.939446706790477e-05, val loss: 0.11255206912755966\n",
      "Epoch 5180: train loss: 0.00012544606579467654, val loss: 0.11350035667419434\n",
      "Epoch 5181: train loss: 6.836738612037152e-05, val loss: 0.11604266613721848\n",
      "Epoch 5182: train loss: 9.951470565283671e-05, val loss: 0.11381371319293976\n",
      "Epoch 5183: train loss: 4.781682218890637e-05, val loss: 0.11272211372852325\n",
      "Epoch 5184: train loss: 8.428114233538508e-05, val loss: 0.11676450073719025\n",
      "Epoch 5185: train loss: 3.30451839545276e-05, val loss: 0.11881335079669952\n",
      "Epoch 5186: train loss: 6.948410737095401e-05, val loss: 0.11758043617010117\n",
      "Epoch 5187: train loss: 2.7227717509958893e-05, val loss: 0.11723808199167252\n",
      "Epoch 5188: train loss: 4.91383507323917e-05, val loss: 0.12025795131921768\n",
      "Epoch 5189: train loss: 2.6032093956018798e-05, val loss: 0.1209338903427124\n",
      "Epoch 5190: train loss: 3.180478961439803e-05, val loss: 0.11898907274007797\n",
      "Epoch 5191: train loss: 2.816406231431756e-05, val loss: 0.1193191334605217\n",
      "Epoch 5192: train loss: 2.0244902771082707e-05, val loss: 0.12138988077640533\n",
      "Epoch 5193: train loss: 2.5837734938249923e-05, val loss: 0.12097042053937912\n",
      "Epoch 5194: train loss: 1.3859748833056074e-05, val loss: 0.11904960870742798\n",
      "Epoch 5195: train loss: 2.1426509192679077e-05, val loss: 0.11957152187824249\n",
      "Epoch 5196: train loss: 1.2817985407309607e-05, val loss: 0.12140549719333649\n",
      "Epoch 5197: train loss: 1.4360573004523758e-05, val loss: 0.12111303955316544\n",
      "Epoch 5198: train loss: 1.3018841855227947e-05, val loss: 0.11936457455158234\n",
      "Epoch 5199: train loss: 9.21410810406087e-06, val loss: 0.11901457607746124\n",
      "Epoch 5200: train loss: 1.2710261216852814e-05, val loss: 0.11998796463012695\n",
      "Epoch 5201: train loss: 6.233708063518861e-06, val loss: 0.12035750597715378\n",
      "Epoch 5202: train loss: 1.009702100418508e-05, val loss: 0.1198091059923172\n",
      "Epoch 5203: train loss: 6.0256229517108295e-06, val loss: 0.11948694288730621\n",
      "Epoch 5204: train loss: 6.665078672085656e-06, val loss: 0.11976354569196701\n",
      "Epoch 5205: train loss: 6.78458172842511e-06, val loss: 0.11990193277597427\n",
      "Epoch 5206: train loss: 3.374558900759439e-06, val loss: 0.11974368244409561\n",
      "Epoch 5207: train loss: 6.944875167391729e-06, val loss: 0.11940915882587433\n",
      "Epoch 5208: train loss: 2.2529504803969758e-06, val loss: 0.11935076862573624\n",
      "Epoch 5209: train loss: 5.542816325032618e-06, val loss: 0.11949949711561203\n",
      "Epoch 5210: train loss: 2.2230888134799898e-06, val loss: 0.11967655271291733\n",
      "Epoch 5211: train loss: 3.928996193280909e-06, val loss: 0.11985037475824356\n",
      "Epoch 5212: train loss: 2.5845542950264644e-06, val loss: 0.11980994045734406\n",
      "Epoch 5213: train loss: 2.532142389100045e-06, val loss: 0.1193973645567894\n",
      "Epoch 5214: train loss: 2.549493274273118e-06, val loss: 0.11915650218725204\n",
      "Epoch 5215: train loss: 1.8750114350041258e-06, val loss: 0.11952495574951172\n",
      "Epoch 5216: train loss: 2.1364385247579776e-06, val loss: 0.1196630522608757\n",
      "Epoch 5217: train loss: 1.5638038348697592e-06, val loss: 0.11918691545724869\n",
      "Epoch 5218: train loss: 1.6226698562604724e-06, val loss: 0.11913152039051056\n",
      "Epoch 5219: train loss: 1.8787850422086194e-06, val loss: 0.11984517425298691\n",
      "Epoch 5220: train loss: 1.1928603953492711e-06, val loss: 0.1200208067893982\n",
      "Epoch 5221: train loss: 1.5127437791306875e-06, val loss: 0.11954879760742188\n",
      "Epoch 5222: train loss: 1.1587135304580443e-06, val loss: 0.11951757967472076\n",
      "Epoch 5223: train loss: 1.0572019846222247e-06, val loss: 0.11990859359502792\n",
      "Epoch 5224: train loss: 1.0558667327131843e-06, val loss: 0.11992696672677994\n",
      "Epoch 5225: train loss: 7.779532325002947e-07, val loss: 0.11969368904829025\n",
      "Epoch 5226: train loss: 9.168945211968094e-07, val loss: 0.11983766406774521\n",
      "Epoch 5227: train loss: 6.273209578466776e-07, val loss: 0.12012998014688492\n",
      "Epoch 5228: train loss: 7.522771170442866e-07, val loss: 0.12015663832426071\n",
      "Epoch 5229: train loss: 4.849315473620663e-07, val loss: 0.12007579952478409\n",
      "Epoch 5230: train loss: 6.698364813928492e-07, val loss: 0.12008011341094971\n",
      "Epoch 5231: train loss: 3.7181337120273383e-07, val loss: 0.1201956495642662\n",
      "Epoch 5232: train loss: 5.261455839900009e-07, val loss: 0.12034419924020767\n",
      "Epoch 5233: train loss: 3.8440956018348515e-07, val loss: 0.12032222747802734\n",
      "Epoch 5234: train loss: 3.282546003902098e-07, val loss: 0.12025431543588638\n",
      "Epoch 5235: train loss: 4.23660992510122e-07, val loss: 0.12042474746704102\n",
      "Epoch 5236: train loss: 1.7814382147207652e-07, val loss: 0.12056624889373779\n",
      "Epoch 5237: train loss: 3.923783253867441e-07, val loss: 0.12045680731534958\n",
      "Epoch 5238: train loss: 1.66470456974821e-07, val loss: 0.1204819455742836\n",
      "Epoch 5239: train loss: 2.759126402906986e-07, val loss: 0.12067385017871857\n",
      "Epoch 5240: train loss: 1.653485099950558e-07, val loss: 0.12075405567884445\n",
      "Epoch 5241: train loss: 2.1810137695865706e-07, val loss: 0.12068010866641998\n",
      "Epoch 5242: train loss: 1.4770468226288358e-07, val loss: 0.1206502914428711\n",
      "Epoch 5243: train loss: 1.7549021436025214e-07, val loss: 0.12081138044595718\n",
      "Epoch 5244: train loss: 1.0855961107836265e-07, val loss: 0.12105336040258408\n",
      "Epoch 5245: train loss: 1.675811063250876e-07, val loss: 0.12105105072259903\n",
      "Epoch 5246: train loss: 7.617932595849197e-08, val loss: 0.12095572799444199\n",
      "Epoch 5247: train loss: 1.385105434792422e-07, val loss: 0.12111265957355499\n",
      "Epoch 5248: train loss: 8.685248076290009e-08, val loss: 0.12128742039203644\n",
      "Epoch 5249: train loss: 7.67029746384651e-08, val loss: 0.12131350487470627\n",
      "Epoch 5250: train loss: 9.992093907840172e-08, val loss: 0.12135771661996841\n",
      "Epoch 5251: train loss: 7.162253012893416e-08, val loss: 0.12145864963531494\n",
      "Epoch 5252: train loss: 5.392702817630379e-08, val loss: 0.12152352184057236\n",
      "Epoch 5253: train loss: 7.919785360854803e-08, val loss: 0.12162547558546066\n",
      "Epoch 5254: train loss: 5.0542769969297296e-08, val loss: 0.12174072116613388\n",
      "Epoch 5255: train loss: 4.6426269051380586e-08, val loss: 0.12181424349546432\n",
      "Epoch 5256: train loss: 6.159102383662685e-08, val loss: 0.12186594307422638\n",
      "Epoch 5257: train loss: 4.6301487088840076e-08, val loss: 0.12196774780750275\n",
      "Epoch 5258: train loss: 4.8971607213843527e-08, val loss: 0.12209416925907135\n",
      "Epoch 5259: train loss: 6.266532892595933e-08, val loss: 0.1221306324005127\n",
      "Epoch 5260: train loss: 5.728169583107956e-08, val loss: 0.12221697717905045\n",
      "Epoch 5261: train loss: 3.5309962953533613e-08, val loss: 0.12233760207891464\n",
      "Epoch 5262: train loss: 3.341605037121553e-08, val loss: 0.12245076894760132\n",
      "Epoch 5263: train loss: 3.50671349735876e-08, val loss: 0.1225038543343544\n",
      "Epoch 5264: train loss: 3.106178780853952e-08, val loss: 0.12259881943464279\n",
      "Epoch 5265: train loss: 2.031161727700237e-08, val loss: 0.12265908718109131\n",
      "Epoch 5266: train loss: 2.2771645902253113e-08, val loss: 0.12275095283985138\n",
      "Epoch 5267: train loss: 3.493077826988156e-08, val loss: 0.12286975234746933\n",
      "Epoch 5268: train loss: 2.2457678383602797e-08, val loss: 0.12291166931390762\n",
      "Epoch 5269: train loss: 2.161612222550957e-08, val loss: 0.12307913601398468\n",
      "Epoch 5270: train loss: 2.166750867615974e-08, val loss: 0.12320154160261154\n",
      "Epoch 5271: train loss: 2.0676738543556894e-08, val loss: 0.12333405017852783\n",
      "Epoch 5272: train loss: 1.816573558244272e-08, val loss: 0.12344842404127121\n",
      "Epoch 5273: train loss: 2.7871990937455848e-08, val loss: 0.12355583906173706\n",
      "Epoch 5274: train loss: 3.536266035553126e-08, val loss: 0.12368788570165634\n",
      "Epoch 5275: train loss: 4.8859920553923075e-08, val loss: 0.12384692579507828\n",
      "Epoch 5276: train loss: 7.073635543974888e-08, val loss: 0.12393376976251602\n",
      "Epoch 5277: train loss: 1.2895148415736912e-07, val loss: 0.1241532564163208\n",
      "Epoch 5278: train loss: 2.459526911025023e-07, val loss: 0.12409398704767227\n",
      "Epoch 5279: train loss: 5.177274715606472e-07, val loss: 0.12454789131879807\n",
      "Epoch 5280: train loss: 1.0843888276212965e-06, val loss: 0.12425389140844345\n",
      "Epoch 5281: train loss: 2.381972308285185e-06, val loss: 0.12505243718624115\n",
      "Epoch 5282: train loss: 5.252392838883679e-06, val loss: 0.12431633472442627\n",
      "Epoch 5283: train loss: 1.2264344150025863e-05, val loss: 0.12593472003936768\n",
      "Epoch 5284: train loss: 2.936130294983741e-05, val loss: 0.12385249137878418\n",
      "Epoch 5285: train loss: 7.497023761970922e-05, val loss: 0.12737703323364258\n",
      "Epoch 5286: train loss: 0.00018319634546060115, val loss: 0.12249968200922012\n",
      "Epoch 5287: train loss: 0.0004028391558676958, val loss: 0.1295025795698166\n",
      "Epoch 5288: train loss: 0.0005334374727681279, val loss: 0.1231854185461998\n",
      "Epoch 5289: train loss: 0.00042051260243169963, val loss: 0.12605033814907074\n",
      "Epoch 5290: train loss: 0.0002552539517637342, val loss: 0.1300450563430786\n",
      "Epoch 5291: train loss: 0.0002660432073753327, val loss: 0.12291206419467926\n",
      "Epoch 5292: train loss: 0.0002806763513945043, val loss: 0.12921462953090668\n",
      "Epoch 5293: train loss: 0.00013807830691803247, val loss: 0.12814751267433167\n",
      "Epoch 5294: train loss: 8.10179190011695e-05, val loss: 0.12497707456350327\n",
      "Epoch 5295: train loss: 0.0001162136613857001, val loss: 0.12728522717952728\n",
      "Epoch 5296: train loss: 0.00011174430983373895, val loss: 0.123659148812294\n",
      "Epoch 5297: train loss: 8.224765042541549e-05, val loss: 0.12182408571243286\n",
      "Epoch 5298: train loss: 5.1361359510337934e-05, val loss: 0.12404244393110275\n",
      "Epoch 5299: train loss: 7.279268902493641e-05, val loss: 0.12330790609121323\n",
      "Epoch 5300: train loss: 5.910581603529863e-05, val loss: 0.12164902687072754\n",
      "Epoch 5301: train loss: 2.9467822969309054e-05, val loss: 0.12245535850524902\n",
      "Epoch 5302: train loss: 5.9958474594168365e-05, val loss: 0.12236013263463974\n",
      "Epoch 5303: train loss: 2.4909211788326502e-05, val loss: 0.12152751535177231\n",
      "Epoch 5304: train loss: 3.585840022424236e-05, val loss: 0.12130458652973175\n",
      "Epoch 5305: train loss: 3.216092227376066e-05, val loss: 0.12077820301055908\n",
      "Epoch 5306: train loss: 1.8413258658256382e-05, val loss: 0.12063189595937729\n",
      "Epoch 5307: train loss: 3.1185729312710464e-05, val loss: 0.12076932191848755\n",
      "Epoch 5308: train loss: 1.3412236512522213e-05, val loss: 0.12025435268878937\n",
      "Epoch 5309: train loss: 2.3531669285148382e-05, val loss: 0.11996431648731232\n",
      "Epoch 5310: train loss: 1.2833103028242476e-05, val loss: 0.12018127739429474\n",
      "Epoch 5311: train loss: 1.7175318134832196e-05, val loss: 0.11972840875387192\n",
      "Epoch 5312: train loss: 1.1396708032407332e-05, val loss: 0.1190953180193901\n",
      "Epoch 5313: train loss: 1.3122655218467116e-05, val loss: 0.11948468536138535\n",
      "Epoch 5314: train loss: 1.0177928743360098e-05, val loss: 0.11950415372848511\n",
      "Epoch 5315: train loss: 8.914117643143982e-06, val loss: 0.1186152920126915\n",
      "Epoch 5316: train loss: 1.0493252375454176e-05, val loss: 0.11879930645227432\n",
      "Epoch 5317: train loss: 4.9652071538730524e-06, val loss: 0.1196463331580162\n",
      "Epoch 5318: train loss: 1.0386539543105755e-05, val loss: 0.11887593567371368\n",
      "Epoch 5319: train loss: 3.2559864848735742e-06, val loss: 0.1180473193526268\n",
      "Epoch 5320: train loss: 7.894650480011478e-06, val loss: 0.118829645216465\n",
      "Epoch 5321: train loss: 4.4005250856571365e-06, val loss: 0.11914660781621933\n",
      "Epoch 5322: train loss: 4.340600753494073e-06, val loss: 0.11828329414129257\n",
      "Epoch 5323: train loss: 5.417366537585622e-06, val loss: 0.11830830574035645\n",
      "Epoch 5324: train loss: 2.696702949833707e-06, val loss: 0.11881151050329208\n",
      "Epoch 5325: train loss: 4.856542091147276e-06, val loss: 0.11847823113203049\n",
      "Epoch 5326: train loss: 4.2766127990034875e-06, val loss: 0.11816620081663132\n",
      "Epoch 5327: train loss: 1.0944774658128154e-05, val loss: 0.11908569186925888\n",
      "Epoch 5328: train loss: 1.8591710613691248e-05, val loss: 0.11861417442560196\n",
      "Epoch 5329: train loss: 5.4417955652752426e-06, val loss: 0.1179022341966629\n",
      "Epoch 5330: train loss: 6.351373031066032e-06, val loss: 0.11891653388738632\n",
      "Epoch 5331: train loss: 5.793279342469759e-06, val loss: 0.11946520954370499\n",
      "Epoch 5332: train loss: 4.331924174039159e-06, val loss: 0.1185539960861206\n",
      "Epoch 5333: train loss: 5.09676237925305e-06, val loss: 0.11810703575611115\n",
      "Epoch 5334: train loss: 3.1958086310623912e-06, val loss: 0.11869249492883682\n",
      "Epoch 5335: train loss: 4.03550347982673e-06, val loss: 0.11894213408231735\n",
      "Epoch 5336: train loss: 3.143978574371431e-06, val loss: 0.11839108914136887\n",
      "Epoch 5337: train loss: 2.6271500246366486e-06, val loss: 0.11803658306598663\n",
      "Epoch 5338: train loss: 3.242638285883004e-06, val loss: 0.11852826178073883\n",
      "Epoch 5339: train loss: 1.5555739310002537e-06, val loss: 0.1187300831079483\n",
      "Epoch 5340: train loss: 3.1363081234303536e-06, val loss: 0.11802636831998825\n",
      "Epoch 5341: train loss: 1.0489402484381571e-06, val loss: 0.11773840337991714\n",
      "Epoch 5342: train loss: 2.5197734885296086e-06, val loss: 0.11820309609174728\n",
      "Epoch 5343: train loss: 1.1233124723730725e-06, val loss: 0.118158720433712\n",
      "Epoch 5344: train loss: 1.7355375803163042e-06, val loss: 0.11755447834730148\n",
      "Epoch 5345: train loss: 1.303726207879663e-06, val loss: 0.11734531074762344\n",
      "Epoch 5346: train loss: 1.0755013590824092e-06, val loss: 0.11773643642663956\n",
      "Epoch 5347: train loss: 1.2894577139377361e-06, val loss: 0.11787011474370956\n",
      "Epoch 5348: train loss: 9.148166668637714e-07, val loss: 0.11733990162611008\n",
      "Epoch 5349: train loss: 8.533834829904663e-07, val loss: 0.11720170825719833\n",
      "Epoch 5350: train loss: 9.608792197468574e-07, val loss: 0.11759807914495468\n",
      "Epoch 5351: train loss: 6.390994258254068e-07, val loss: 0.1174665242433548\n",
      "Epoch 5352: train loss: 7.738881890873017e-07, val loss: 0.11715064197778702\n",
      "Epoch 5353: train loss: 5.681678771907173e-07, val loss: 0.1172439381480217\n",
      "Epoch 5354: train loss: 5.559787155107188e-07, val loss: 0.1173916608095169\n",
      "Epoch 5355: train loss: 6.162348427096731e-07, val loss: 0.11727458238601685\n",
      "Epoch 5356: train loss: 3.6833031913374725e-07, val loss: 0.1171242818236351\n",
      "Epoch 5357: train loss: 4.971440716872166e-07, val loss: 0.11729218065738678\n",
      "Epoch 5358: train loss: 3.344836443375243e-07, val loss: 0.11736679077148438\n",
      "Epoch 5359: train loss: 4.411053851072211e-07, val loss: 0.11726122349500656\n",
      "Epoch 5360: train loss: 2.963386691590131e-07, val loss: 0.11723541468381882\n",
      "Epoch 5361: train loss: 2.8696592835331103e-07, val loss: 0.11722864210605621\n",
      "Epoch 5362: train loss: 2.655216349012335e-07, val loss: 0.1173836961388588\n",
      "Epoch 5363: train loss: 2.7972686211796827e-07, val loss: 0.11736810207366943\n",
      "Epoch 5364: train loss: 2.8440916821637074e-07, val loss: 0.11727332323789597\n",
      "Epoch 5365: train loss: 3.1088347896002233e-07, val loss: 0.11738011986017227\n",
      "Epoch 5366: train loss: 3.891976518843876e-07, val loss: 0.11754310131072998\n",
      "Epoch 5367: train loss: 5.005845764571859e-07, val loss: 0.11724604666233063\n",
      "Epoch 5368: train loss: 8.444121135653404e-07, val loss: 0.11765595525503159\n",
      "Epoch 5369: train loss: 1.692013597676123e-06, val loss: 0.11722471565008163\n",
      "Epoch 5370: train loss: 4.056995294376975e-06, val loss: 0.11791878938674927\n",
      "Epoch 5371: train loss: 1.1239462764933705e-05, val loss: 0.11686462163925171\n",
      "Epoch 5372: train loss: 2.7631609555101022e-05, val loss: 0.11889507621526718\n",
      "Epoch 5373: train loss: 5.8302517572883517e-05, val loss: 0.11645422130823135\n",
      "Epoch 5374: train loss: 0.00012115795107092708, val loss: 0.12075992673635483\n",
      "Epoch 5375: train loss: 0.00022250741312745959, val loss: 0.11521432548761368\n",
      "Epoch 5376: train loss: 0.0003957111912313849, val loss: 0.12250592559576035\n",
      "Epoch 5377: train loss: 0.0004933699383400381, val loss: 0.11255637556314468\n",
      "Epoch 5378: train loss: 0.00046684162225574255, val loss: 0.11863742023706436\n",
      "Epoch 5379: train loss: 0.00017321210179943591, val loss: 0.11475677788257599\n",
      "Epoch 5380: train loss: 6.611777644138783e-05, val loss: 0.11069431155920029\n",
      "Epoch 5381: train loss: 0.0003271331370342523, val loss: 0.11356498301029205\n",
      "Epoch 5382: train loss: 0.000517377513460815, val loss: 0.11321159452199936\n",
      "Epoch 5383: train loss: 0.0006227435660548508, val loss: 0.1089278981089592\n",
      "Epoch 5384: train loss: 0.00037154709571041167, val loss: 0.11330868303775787\n",
      "Epoch 5385: train loss: 9.435128595214337e-05, val loss: 0.11451029777526855\n",
      "Epoch 5386: train loss: 0.00031929873512126505, val loss: 0.10854573547840118\n",
      "Epoch 5387: train loss: 0.00015289129805751145, val loss: 0.10863225907087326\n",
      "Epoch 5388: train loss: 0.00013188854791224003, val loss: 0.11187111586332321\n",
      "Epoch 5389: train loss: 0.00017182115698233247, val loss: 0.1090245395898819\n",
      "Epoch 5390: train loss: 7.126148557290435e-05, val loss: 0.10889159888029099\n",
      "Epoch 5391: train loss: 0.00014022368122823536, val loss: 0.10918742418289185\n",
      "Epoch 5392: train loss: 5.526371387531981e-05, val loss: 0.10808997601270676\n",
      "Epoch 5393: train loss: 0.00010059386113425717, val loss: 0.10852225124835968\n",
      "Epoch 5394: train loss: 4.52493222837802e-05, val loss: 0.109817735850811\n",
      "Epoch 5395: train loss: 7.325964543269947e-05, val loss: 0.1084863543510437\n",
      "Epoch 5396: train loss: 3.981574627687223e-05, val loss: 0.10747748613357544\n",
      "Epoch 5397: train loss: 5.341955693438649e-05, val loss: 0.1088210716843605\n",
      "Epoch 5398: train loss: 3.2705633202567697e-05, val loss: 0.1099468469619751\n",
      "Epoch 5399: train loss: 4.04083875764627e-05, val loss: 0.10862232744693756\n",
      "Epoch 5400: train loss: 2.7843110729008913e-05, val loss: 0.10821058601140976\n",
      "Epoch 5401: train loss: 2.926977140305098e-05, val loss: 0.10941267013549805\n",
      "Epoch 5402: train loss: 2.5509094484732486e-05, val loss: 0.10991932451725006\n",
      "Epoch 5403: train loss: 2.0898409275105223e-05, val loss: 0.10917504876852036\n",
      "Epoch 5404: train loss: 2.2048661776352674e-05, val loss: 0.10923421382904053\n",
      "Epoch 5405: train loss: 1.5767653167131357e-05, val loss: 0.10999567806720734\n",
      "Epoch 5406: train loss: 1.7963911886909045e-05, val loss: 0.10984956473112106\n",
      "Epoch 5407: train loss: 1.3015577678743284e-05, val loss: 0.10936801880598068\n",
      "Epoch 5408: train loss: 1.4036472748557571e-05, val loss: 0.10977830737829208\n",
      "Epoch 5409: train loss: 1.0810987078002654e-05, val loss: 0.1109599843621254\n",
      "Epoch 5410: train loss: 1.109954155253945e-05, val loss: 0.11120738834142685\n",
      "Epoch 5411: train loss: 9.161041816696525e-06, val loss: 0.11029567569494247\n",
      "Epoch 5412: train loss: 8.451236681139562e-06, val loss: 0.11016390472650528\n",
      "Epoch 5413: train loss: 8.020587301871274e-06, val loss: 0.11134278774261475\n",
      "Epoch 5414: train loss: 6.156862582429312e-06, val loss: 0.11203672736883163\n",
      "Epoch 5415: train loss: 7.369537797785597e-06, val loss: 0.11163435131311417\n",
      "Epoch 5416: train loss: 4.228862508171005e-06, val loss: 0.11117875576019287\n",
      "Epoch 5417: train loss: 6.551102160301525e-06, val loss: 0.11143608391284943\n",
      "Epoch 5418: train loss: 3.150093334625126e-06, val loss: 0.1122775450348854\n",
      "Epoch 5419: train loss: 5.385086296882946e-06, val loss: 0.11275041103363037\n",
      "Epoch 5420: train loss: 2.802845756377792e-06, val loss: 0.11224959045648575\n",
      "Epoch 5421: train loss: 3.973395450884709e-06, val loss: 0.11163213104009628\n",
      "Epoch 5422: train loss: 2.7316907562635606e-06, val loss: 0.11194860190153122\n",
      "Epoch 5423: train loss: 2.9989098493388155e-06, val loss: 0.11289620399475098\n",
      "Epoch 5424: train loss: 2.3960401449585333e-06, val loss: 0.11310487240552902\n",
      "Epoch 5425: train loss: 2.2914296096132603e-06, val loss: 0.11240984499454498\n",
      "Epoch 5426: train loss: 2.0406862404342974e-06, val loss: 0.1121775433421135\n",
      "Epoch 5427: train loss: 1.954000936166267e-06, val loss: 0.11283353716135025\n",
      "Epoch 5428: train loss: 1.5797452306287596e-06, val loss: 0.1130187138915062\n",
      "Epoch 5429: train loss: 1.5658840766263893e-06, val loss: 0.11276854574680328\n",
      "Epoch 5430: train loss: 1.3838227914675372e-06, val loss: 0.11307664215564728\n",
      "Epoch 5431: train loss: 1.3076141840429045e-06, val loss: 0.11330356448888779\n",
      "Epoch 5432: train loss: 1.0307684306098963e-06, val loss: 0.113010935485363\n",
      "Epoch 5433: train loss: 1.0968055903504137e-06, val loss: 0.1130760908126831\n",
      "Epoch 5434: train loss: 9.151133895102248e-07, val loss: 0.113456130027771\n",
      "Epoch 5435: train loss: 8.728347324904462e-07, val loss: 0.1133565902709961\n",
      "Epoch 5436: train loss: 7.053067179185746e-07, val loss: 0.11303197592496872\n",
      "Epoch 5437: train loss: 7.444139100698521e-07, val loss: 0.11325786262750626\n",
      "Epoch 5438: train loss: 5.968463483441155e-07, val loss: 0.11371483653783798\n",
      "Epoch 5439: train loss: 6.500254698948993e-07, val loss: 0.11365218460559845\n",
      "Epoch 5440: train loss: 3.746044683339278e-07, val loss: 0.11338899284601212\n",
      "Epoch 5441: train loss: 6.272034624998923e-07, val loss: 0.11349201202392578\n",
      "Epoch 5442: train loss: 3.1023276392261323e-07, val loss: 0.11375873535871506\n",
      "Epoch 5443: train loss: 4.6373463646887103e-07, val loss: 0.11371246725320816\n",
      "Epoch 5444: train loss: 2.955336526611063e-07, val loss: 0.1135513111948967\n",
      "Epoch 5445: train loss: 3.6360702893034613e-07, val loss: 0.11382695287466049\n",
      "Epoch 5446: train loss: 2.7314163730807195e-07, val loss: 0.11401398479938507\n",
      "Epoch 5447: train loss: 2.579198223884305e-07, val loss: 0.1137566938996315\n",
      "Epoch 5448: train loss: 2.450138083531783e-07, val loss: 0.11378064006567001\n",
      "Epoch 5449: train loss: 2.2425140855375503e-07, val loss: 0.11408635228872299\n",
      "Epoch 5450: train loss: 2.2238182850742305e-07, val loss: 0.11412717401981354\n",
      "Epoch 5451: train loss: 1.2937439919369353e-07, val loss: 0.11396129429340363\n",
      "Epoch 5452: train loss: 2.2573134117465088e-07, val loss: 0.11406116932630539\n",
      "Epoch 5453: train loss: 1.1373801811487283e-07, val loss: 0.11422685533761978\n",
      "Epoch 5454: train loss: 1.5140497566790145e-07, val loss: 0.1142362579703331\n",
      "Epoch 5455: train loss: 1.1783152586986034e-07, val loss: 0.11423315852880478\n",
      "Epoch 5456: train loss: 1.2098874435650941e-07, val loss: 0.11434662342071533\n",
      "Epoch 5457: train loss: 1.1717830972202137e-07, val loss: 0.11443229764699936\n",
      "Epoch 5458: train loss: 7.384654310271799e-08, val loss: 0.11438339203596115\n",
      "Epoch 5459: train loss: 1.0750731860298401e-07, val loss: 0.11444798856973648\n",
      "Epoch 5460: train loss: 1.060044709788599e-07, val loss: 0.11453817039728165\n",
      "Epoch 5461: train loss: 8.261767447947932e-08, val loss: 0.11466238647699356\n",
      "Epoch 5462: train loss: 1.9056211897350295e-07, val loss: 0.11448830366134644\n",
      "Epoch 5463: train loss: 2.840032209405763e-07, val loss: 0.11475702375173569\n",
      "Epoch 5464: train loss: 8.073392905316723e-07, val loss: 0.11452742666006088\n",
      "Epoch 5465: train loss: 2.367318074902869e-06, val loss: 0.11510063707828522\n",
      "Epoch 5466: train loss: 7.916816684883088e-06, val loss: 0.11388766020536423\n",
      "Epoch 5467: train loss: 2.7892234356841072e-05, val loss: 0.11587438732385635\n",
      "Epoch 5468: train loss: 0.00010418420424684882, val loss: 0.11225944012403488\n",
      "Epoch 5469: train loss: 0.00036208052188158035, val loss: 0.11247804015874863\n",
      "Epoch 5470: train loss: 0.0010243031429126859, val loss: 0.11232496798038483\n",
      "Epoch 5471: train loss: 0.001685739611275494, val loss: 0.10679314285516739\n",
      "Epoch 5472: train loss: 0.0017399699427187443, val loss: 0.1106649860739708\n",
      "Epoch 5473: train loss: 0.000423463701736182, val loss: 0.11628296226263046\n",
      "Epoch 5474: train loss: 0.0006232555024325848, val loss: 0.10702767223119736\n",
      "Epoch 5475: train loss: 0.0007004541112110019, val loss: 0.10154826939105988\n",
      "Epoch 5476: train loss: 0.00034494418650865555, val loss: 0.1101500615477562\n",
      "Epoch 5477: train loss: 0.0004322552413213998, val loss: 0.10984017699956894\n",
      "Epoch 5478: train loss: 0.0003044678014703095, val loss: 0.09575340896844864\n",
      "Epoch 5479: train loss: 0.0002568634517956525, val loss: 0.1002923846244812\n",
      "Epoch 5480: train loss: 0.00024969090009108186, val loss: 0.10371372848749161\n",
      "Epoch 5481: train loss: 0.00018232734873890877, val loss: 0.10433409363031387\n",
      "Epoch 5482: train loss: 0.00018540846940595657, val loss: 0.10161556303501129\n",
      "Epoch 5483: train loss: 0.0001600961113581434, val loss: 0.10131781548261642\n",
      "Epoch 5484: train loss: 0.00011884790001204237, val loss: 0.10279061645269394\n",
      "Epoch 5485: train loss: 0.00013557358761318028, val loss: 0.1024274230003357\n",
      "Epoch 5486: train loss: 9.838128607952967e-05, val loss: 0.10115640610456467\n",
      "Epoch 5487: train loss: 9.462106390856206e-05, val loss: 0.1010068878531456\n",
      "Epoch 5488: train loss: 9.845750173553824e-05, val loss: 0.10175468027591705\n",
      "Epoch 5489: train loss: 4.95366875838954e-05, val loss: 0.10266921669244766\n",
      "Epoch 5490: train loss: 9.610359120415524e-05, val loss: 0.10249396413564682\n",
      "Epoch 5491: train loss: 3.925690907635726e-05, val loss: 0.10090257972478867\n",
      "Epoch 5492: train loss: 6.532089901156723e-05, val loss: 0.09991993755102158\n",
      "Epoch 5493: train loss: 4.374553827801719e-05, val loss: 0.10128185898065567\n",
      "Epoch 5494: train loss: 4.518053901847452e-05, val loss: 0.10282976925373077\n",
      "Epoch 5495: train loss: 3.641336297732778e-05, val loss: 0.1016356572508812\n",
      "Epoch 5496: train loss: 3.902680691680871e-05, val loss: 0.09953761845827103\n",
      "Epoch 5497: train loss: 2.484656397427898e-05, val loss: 0.09988805651664734\n",
      "Epoch 5498: train loss: 3.450979056651704e-05, val loss: 0.10136543959379196\n",
      "Epoch 5499: train loss: 2.1646694221999496e-05, val loss: 0.10092522203922272\n",
      "Epoch 5500: train loss: 2.1792347979499027e-05, val loss: 0.0999300479888916\n",
      "Epoch 5501: train loss: 2.488749123585876e-05, val loss: 0.10089333355426788\n",
      "Epoch 5502: train loss: 1.2389485164021607e-05, val loss: 0.10230600833892822\n",
      "Epoch 5503: train loss: 2.2730750060873106e-05, val loss: 0.10144811868667603\n",
      "Epoch 5504: train loss: 1.0284747986588627e-05, val loss: 0.0999840795993805\n",
      "Epoch 5505: train loss: 1.601718395249918e-05, val loss: 0.10024136304855347\n",
      "Epoch 5506: train loss: 1.2143541425757576e-05, val loss: 0.10108887404203415\n",
      "Epoch 5507: train loss: 1.005149260890903e-05, val loss: 0.1008346676826477\n",
      "Epoch 5508: train loss: 1.1111220374004915e-05, val loss: 0.10057549923658371\n",
      "Epoch 5509: train loss: 8.81715095601976e-06, val loss: 0.101105235517025\n",
      "Epoch 5510: train loss: 7.72289240558166e-06, val loss: 0.10129367560148239\n",
      "Epoch 5511: train loss: 8.643465662316885e-06, val loss: 0.10082490742206573\n",
      "Epoch 5512: train loss: 4.958549197908724e-06, val loss: 0.1007533073425293\n",
      "Epoch 5513: train loss: 7.116180313460063e-06, val loss: 0.10130860656499863\n",
      "Epoch 5514: train loss: 4.363581865618471e-06, val loss: 0.10150909423828125\n",
      "Epoch 5515: train loss: 4.488140803005081e-06, val loss: 0.10114874690771103\n",
      "Epoch 5516: train loss: 4.3356590140319895e-06, val loss: 0.10113682597875595\n",
      "Epoch 5517: train loss: 3.288379730292945e-06, val loss: 0.1017395481467247\n",
      "Epoch 5518: train loss: 3.8137243336677784e-06, val loss: 0.10208630561828613\n",
      "Epoch 5519: train loss: 2.1349437702156138e-06, val loss: 0.10192079842090607\n",
      "Epoch 5520: train loss: 3.4781312479026383e-06, val loss: 0.10194571316242218\n",
      "Epoch 5521: train loss: 1.8039237374978256e-06, val loss: 0.102178655564785\n",
      "Epoch 5522: train loss: 2.4669507183716632e-06, val loss: 0.10233881324529648\n",
      "Epoch 5523: train loss: 1.8594317907627556e-06, val loss: 0.10236801207065582\n",
      "Epoch 5524: train loss: 1.8143952047466883e-06, val loss: 0.10239613056182861\n",
      "Epoch 5525: train loss: 1.6658360664223437e-06, val loss: 0.10246604681015015\n",
      "Epoch 5526: train loss: 1.2449606856534956e-06, val loss: 0.10246578603982925\n",
      "Epoch 5527: train loss: 1.4794489970881841e-06, val loss: 0.10254835337400436\n",
      "Epoch 5528: train loss: 1.6024563365135691e-06, val loss: 0.10278357565402985\n",
      "Epoch 5529: train loss: 1.4739576954525546e-06, val loss: 0.10287630558013916\n",
      "Epoch 5530: train loss: 8.107368785204017e-07, val loss: 0.10296320170164108\n",
      "Epoch 5531: train loss: 1.5087789506651461e-06, val loss: 0.10311146080493927\n",
      "Epoch 5532: train loss: 8.576900540901988e-07, val loss: 0.10304631292819977\n",
      "Epoch 5533: train loss: 8.067614203355333e-07, val loss: 0.10307823866605759\n",
      "Epoch 5534: train loss: 9.846911552813253e-07, val loss: 0.10346543043851852\n",
      "Epoch 5535: train loss: 6.838797048658307e-07, val loss: 0.10356663912534714\n",
      "Epoch 5536: train loss: 5.846603698955732e-07, val loss: 0.10330598801374435\n",
      "Epoch 5537: train loss: 7.860379014346108e-07, val loss: 0.10342929512262344\n",
      "Epoch 5538: train loss: 4.1897038727256586e-07, val loss: 0.103833869099617\n",
      "Epoch 5539: train loss: 5.301521355249861e-07, val loss: 0.10393060743808746\n",
      "Epoch 5540: train loss: 5.698913128071581e-07, val loss: 0.10387440025806427\n",
      "Epoch 5541: train loss: 3.177035807766515e-07, val loss: 0.10399717092514038\n",
      "Epoch 5542: train loss: 3.9543104435324494e-07, val loss: 0.10419698059558868\n",
      "Epoch 5543: train loss: 4.181784731827065e-07, val loss: 0.10427522659301758\n",
      "Epoch 5544: train loss: 3.9354694081339403e-07, val loss: 0.10429024696350098\n",
      "Epoch 5545: train loss: 1.5629920824267174e-07, val loss: 0.10438952594995499\n",
      "Epoch 5546: train loss: 3.089861593252863e-07, val loss: 0.10454442352056503\n",
      "Epoch 5547: train loss: 3.011832916399726e-07, val loss: 0.10461757332086563\n",
      "Epoch 5548: train loss: 1.840327570334921e-07, val loss: 0.10477817058563232\n",
      "Epoch 5549: train loss: 1.6834452765124297e-07, val loss: 0.1049162745475769\n",
      "Epoch 5550: train loss: 1.5008764364665694e-07, val loss: 0.1050279289484024\n",
      "Epoch 5551: train loss: 2.0062682892785233e-07, val loss: 0.10521189123392105\n",
      "Epoch 5552: train loss: 1.1123674426016805e-07, val loss: 0.10522377490997314\n",
      "Epoch 5553: train loss: 1.044025452756614e-07, val loss: 0.10522874444723129\n",
      "Epoch 5554: train loss: 1.1709457226061204e-07, val loss: 0.10541526228189468\n",
      "Epoch 5555: train loss: 1.2087538436844625e-07, val loss: 0.105551578104496\n",
      "Epoch 5556: train loss: 8.893309910718017e-08, val loss: 0.10567569732666016\n",
      "Epoch 5557: train loss: 7.495968645798712e-08, val loss: 0.10572351515293121\n",
      "Epoch 5558: train loss: 5.948312775672093e-08, val loss: 0.10578422993421555\n",
      "Epoch 5559: train loss: 9.016284252538753e-08, val loss: 0.10594683140516281\n",
      "Epoch 5560: train loss: 7.097356302665503e-08, val loss: 0.10598719120025635\n",
      "Epoch 5561: train loss: 5.82149688455047e-08, val loss: 0.10611152648925781\n",
      "Epoch 5562: train loss: 4.5272727788869815e-08, val loss: 0.10619666427373886\n",
      "Epoch 5563: train loss: 3.3569886426221274e-08, val loss: 0.10625167191028595\n",
      "Epoch 5564: train loss: 5.973589622954023e-08, val loss: 0.10647492855787277\n",
      "Epoch 5565: train loss: 5.863904561920208e-08, val loss: 0.10655149072408676\n",
      "Epoch 5566: train loss: 5.8470856600933985e-08, val loss: 0.10669215023517609\n",
      "Epoch 5567: train loss: 5.191305874063801e-08, val loss: 0.10673725605010986\n",
      "Epoch 5568: train loss: 6.107641326025259e-08, val loss: 0.10685596615076065\n",
      "Epoch 5569: train loss: 6.707469424327428e-08, val loss: 0.10689911991357803\n",
      "Epoch 5570: train loss: 1.442234065507364e-07, val loss: 0.10706102848052979\n",
      "Epoch 5571: train loss: 3.451349641636625e-07, val loss: 0.10703795403242111\n",
      "Epoch 5572: train loss: 1.208785306516802e-06, val loss: 0.10738811641931534\n",
      "Epoch 5573: train loss: 2.4432852114841808e-06, val loss: 0.10730411112308502\n",
      "Epoch 5574: train loss: 4.118133347219555e-06, val loss: 0.10772037506103516\n",
      "Epoch 5575: train loss: 6.238333753572078e-06, val loss: 0.10718419402837753\n",
      "Epoch 5576: train loss: 8.50319156597834e-06, val loss: 0.10803695768117905\n",
      "Epoch 5577: train loss: 1.0858662790269591e-05, val loss: 0.10737872123718262\n",
      "Epoch 5578: train loss: 1.3944796592113562e-05, val loss: 0.10816452652215958\n",
      "Epoch 5579: train loss: 1.8594631910673343e-05, val loss: 0.10765273869037628\n",
      "Epoch 5580: train loss: 2.495282024028711e-05, val loss: 0.10850899666547775\n",
      "Epoch 5581: train loss: 3.31276714859996e-05, val loss: 0.10748646408319473\n",
      "Epoch 5582: train loss: 4.19170901295729e-05, val loss: 0.10908833891153336\n",
      "Epoch 5583: train loss: 4.864493530476466e-05, val loss: 0.10750176012516022\n",
      "Epoch 5584: train loss: 4.7334659029729664e-05, val loss: 0.10936718434095383\n",
      "Epoch 5585: train loss: 3.501400351524353e-05, val loss: 0.10695605725049973\n",
      "Epoch 5586: train loss: 1.9701203200384043e-05, val loss: 0.10686273872852325\n",
      "Epoch 5587: train loss: 9.678745300334413e-06, val loss: 0.10631308704614639\n",
      "Epoch 5588: train loss: 7.701097274548374e-06, val loss: 0.10532768070697784\n",
      "Epoch 5589: train loss: 9.069887710211333e-06, val loss: 0.10644189268350601\n",
      "Epoch 5590: train loss: 8.934651305025909e-06, val loss: 0.10526233166456223\n",
      "Epoch 5591: train loss: 8.780240932537708e-06, val loss: 0.1061091423034668\n",
      "Epoch 5592: train loss: 1.0476343959453516e-05, val loss: 0.10601694881916046\n",
      "Epoch 5593: train loss: 1.3003701496927533e-05, val loss: 0.10565940290689468\n",
      "Epoch 5594: train loss: 1.2578331734403037e-05, val loss: 0.10683637857437134\n",
      "Epoch 5595: train loss: 7.900170203356538e-06, val loss: 0.1059364303946495\n",
      "Epoch 5596: train loss: 2.8290914997342043e-06, val loss: 0.1067679151892662\n",
      "Epoch 5597: train loss: 7.225724516501941e-07, val loss: 0.10682789236307144\n",
      "Epoch 5598: train loss: 3.012766455867677e-06, val loss: 0.10643460601568222\n",
      "Epoch 5599: train loss: 7.31414138499531e-06, val loss: 0.1074141263961792\n",
      "Epoch 5600: train loss: 9.768111340235919e-06, val loss: 0.10657632350921631\n",
      "Epoch 5601: train loss: 9.177585525321774e-06, val loss: 0.10752329975366592\n",
      "Epoch 5602: train loss: 6.0081838455516845e-06, val loss: 0.10729938000440598\n",
      "Epoch 5603: train loss: 2.8118527097831247e-06, val loss: 0.10745920985937119\n",
      "Epoch 5604: train loss: 9.470978170611488e-07, val loss: 0.10795100033283234\n",
      "Epoch 5605: train loss: 6.976092095101194e-07, val loss: 0.10750322788953781\n",
      "Epoch 5606: train loss: 1.6128518609548337e-06, val loss: 0.10839022696018219\n",
      "Epoch 5607: train loss: 3.0042126581975026e-06, val loss: 0.10756158828735352\n",
      "Epoch 5608: train loss: 4.762127446156228e-06, val loss: 0.10862579196691513\n",
      "Epoch 5609: train loss: 6.316356575553073e-06, val loss: 0.10778281837701797\n",
      "Epoch 5610: train loss: 8.145223546307534e-06, val loss: 0.10867302864789963\n",
      "Epoch 5611: train loss: 1.073140992957633e-05, val loss: 0.10813434422016144\n",
      "Epoch 5612: train loss: 1.5284465916920453e-05, val loss: 0.1089761033654213\n",
      "Epoch 5613: train loss: 2.2980246285442263e-05, val loss: 0.10839010775089264\n",
      "Epoch 5614: train loss: 3.748058225028217e-05, val loss: 0.10942442715167999\n",
      "Epoch 5615: train loss: 5.380121729103848e-05, val loss: 0.1088808923959732\n",
      "Epoch 5616: train loss: 6.601253699045628e-05, val loss: 0.11007297039031982\n",
      "Epoch 5617: train loss: 6.7824890720658e-05, val loss: 0.10949189960956573\n",
      "Epoch 5618: train loss: 6.150224362500012e-05, val loss: 0.1108720675110817\n",
      "Epoch 5619: train loss: 4.477138281799853e-05, val loss: 0.10965760797262192\n",
      "Epoch 5620: train loss: 2.7416203010943718e-05, val loss: 0.11139072477817535\n",
      "Epoch 5621: train loss: 1.6841289834701456e-05, val loss: 0.10948698967695236\n",
      "Epoch 5622: train loss: 2.5884082788252272e-05, val loss: 0.11350881308317184\n",
      "Epoch 5623: train loss: 6.0471007600426674e-05, val loss: 0.10829328745603561\n",
      "Epoch 5624: train loss: 0.00013327300257515162, val loss: 0.11604633182287216\n",
      "Epoch 5625: train loss: 0.00023233202227856964, val loss: 0.10590650886297226\n",
      "Epoch 5626: train loss: 0.0004044227534905076, val loss: 0.11941049247980118\n",
      "Epoch 5627: train loss: 0.0004526737902779132, val loss: 0.10733243077993393\n",
      "Epoch 5628: train loss: 0.00036018897662870586, val loss: 0.11389940977096558\n",
      "Epoch 5629: train loss: 6.109446258051321e-05, val loss: 0.11319174617528915\n",
      "Epoch 5630: train loss: 6.628349365200847e-05, val loss: 0.10626907646656036\n",
      "Epoch 5631: train loss: 0.00020808221597690135, val loss: 0.10970838367938995\n",
      "Epoch 5632: train loss: 7.539249781984836e-05, val loss: 0.11039614677429199\n",
      "Epoch 5633: train loss: 3.63754625141155e-05, val loss: 0.10743316262960434\n",
      "Epoch 5634: train loss: 0.0001249868655577302, val loss: 0.1079121008515358\n",
      "Epoch 5635: train loss: 4.257524778950028e-05, val loss: 0.1091766282916069\n",
      "Epoch 5636: train loss: 4.191849802737124e-05, val loss: 0.10800720751285553\n",
      "Epoch 5637: train loss: 7.586037827422842e-05, val loss: 0.1072278767824173\n",
      "Epoch 5638: train loss: 2.32982165471185e-05, val loss: 0.11018192023038864\n",
      "Epoch 5639: train loss: 4.71374842163641e-05, val loss: 0.11103726923465729\n",
      "Epoch 5640: train loss: 3.651893348433077e-05, val loss: 0.10916067659854889\n",
      "Epoch 5641: train loss: 2.505815245967824e-05, val loss: 0.11091132462024689\n",
      "Epoch 5642: train loss: 3.472631215117872e-05, val loss: 0.1125427708029747\n",
      "Epoch 5643: train loss: 1.7899044905789196e-05, val loss: 0.11080174893140793\n",
      "Epoch 5644: train loss: 2.6703233743319288e-05, val loss: 0.1116863489151001\n",
      "Epoch 5645: train loss: 1.4891972568875644e-05, val loss: 0.11412455886602402\n",
      "Epoch 5646: train loss: 2.1355188437155448e-05, val loss: 0.1128242164850235\n",
      "Epoch 5647: train loss: 1.1229976735194214e-05, val loss: 0.1123756691813469\n",
      "Epoch 5648: train loss: 1.749244074744638e-05, val loss: 0.11448123306035995\n",
      "Epoch 5649: train loss: 1.0428072528156918e-05, val loss: 0.11375901848077774\n",
      "Epoch 5650: train loss: 1.1012137292709667e-05, val loss: 0.11286938190460205\n",
      "Epoch 5651: train loss: 1.1671774700516835e-05, val loss: 0.11503497511148453\n",
      "Epoch 5652: train loss: 6.962853149161674e-06, val loss: 0.11512584984302521\n",
      "Epoch 5653: train loss: 9.884416613203939e-06, val loss: 0.11305995285511017\n",
      "Epoch 5654: train loss: 6.613322511839215e-06, val loss: 0.11415189504623413\n",
      "Epoch 5655: train loss: 7.03044770489214e-06, val loss: 0.11556756496429443\n",
      "Epoch 5656: train loss: 5.699530447600409e-06, val loss: 0.11436069011688232\n",
      "Epoch 5657: train loss: 6.182941433507949e-06, val loss: 0.11406808346509933\n",
      "Epoch 5658: train loss: 4.011867986264406e-06, val loss: 0.11537625640630722\n",
      "Epoch 5659: train loss: 5.183288976695621e-06, val loss: 0.11535137891769409\n",
      "Epoch 5660: train loss: 4.099107172805816e-06, val loss: 0.11449553072452545\n",
      "Epoch 5661: train loss: 2.864338966901414e-06, val loss: 0.1148126870393753\n",
      "Epoch 5662: train loss: 4.450310825632187e-06, val loss: 0.11554522812366486\n",
      "Epoch 5663: train loss: 2.3288548618438654e-06, val loss: 0.1154387965798378\n",
      "Epoch 5664: train loss: 2.793220119201578e-06, val loss: 0.11515548080205917\n",
      "Epoch 5665: train loss: 2.7342880457581487e-06, val loss: 0.11545286327600479\n",
      "Epoch 5666: train loss: 2.1841326542926254e-06, val loss: 0.11574605852365494\n",
      "Epoch 5667: train loss: 1.9098183656751644e-06, val loss: 0.11566157639026642\n",
      "Epoch 5668: train loss: 1.948599219758762e-06, val loss: 0.11565907299518585\n",
      "Epoch 5669: train loss: 1.8549762899056077e-06, val loss: 0.11584959179162979\n",
      "Epoch 5670: train loss: 1.3784235761704622e-06, val loss: 0.11596598476171494\n",
      "Epoch 5671: train loss: 1.3148883226676844e-06, val loss: 0.11592953652143478\n",
      "Epoch 5672: train loss: 1.571981329107075e-06, val loss: 0.11614198982715607\n",
      "Epoch 5673: train loss: 1.0018092098107445e-06, val loss: 0.11642751842737198\n",
      "Epoch 5674: train loss: 1.0680447530830861e-06, val loss: 0.11637284606695175\n",
      "Epoch 5675: train loss: 1.0121973446075572e-06, val loss: 0.11642169952392578\n",
      "Epoch 5676: train loss: 8.709625376468466e-07, val loss: 0.11669452488422394\n",
      "Epoch 5677: train loss: 9.23184984458203e-07, val loss: 0.1166539192199707\n",
      "Epoch 5678: train loss: 6.396342087100493e-07, val loss: 0.11670487374067307\n",
      "Epoch 5679: train loss: 6.22911500158807e-07, val loss: 0.1169663667678833\n",
      "Epoch 5680: train loss: 7.772926551297132e-07, val loss: 0.11696092039346695\n",
      "Epoch 5681: train loss: 4.941414317727322e-07, val loss: 0.1170227900147438\n",
      "Epoch 5682: train loss: 5.155353051122802e-07, val loss: 0.11737538874149323\n",
      "Epoch 5683: train loss: 4.809348865819629e-07, val loss: 0.11742096394300461\n",
      "Epoch 5684: train loss: 3.7134932995286363e-07, val loss: 0.11729585379362106\n",
      "Epoch 5685: train loss: 5.166037340131879e-07, val loss: 0.11774390190839767\n",
      "Epoch 5686: train loss: 3.63549901294391e-07, val loss: 0.1178266778588295\n",
      "Epoch 5687: train loss: 2.706528619000892e-07, val loss: 0.1176273375749588\n",
      "Epoch 5688: train loss: 3.167465081332921e-07, val loss: 0.11803384870290756\n",
      "Epoch 5689: train loss: 2.8293385412325733e-07, val loss: 0.11815615743398666\n",
      "Epoch 5690: train loss: 3.0332151368384075e-07, val loss: 0.11810674518346786\n",
      "Epoch 5691: train loss: 2.841478305981582e-07, val loss: 0.11837051063776016\n",
      "Epoch 5692: train loss: 2.978813142817671e-07, val loss: 0.11861200630664825\n",
      "Epoch 5693: train loss: 4.6777981310697214e-07, val loss: 0.1182834655046463\n",
      "Epoch 5694: train loss: 7.150757710405742e-07, val loss: 0.11897571384906769\n",
      "Epoch 5695: train loss: 1.3537472796087968e-06, val loss: 0.11857997626066208\n",
      "Epoch 5696: train loss: 3.333778749947669e-06, val loss: 0.11932456493377686\n",
      "Epoch 5697: train loss: 8.46000239107525e-06, val loss: 0.11833784729242325\n",
      "Epoch 5698: train loss: 2.154715548385866e-05, val loss: 0.12051612138748169\n",
      "Epoch 5699: train loss: 5.501683335751295e-05, val loss: 0.11715392023324966\n",
      "Epoch 5700: train loss: 0.00014469683810602874, val loss: 0.1216159239411354\n",
      "Epoch 5701: train loss: 0.00034985519596375525, val loss: 0.1138225793838501\n",
      "Epoch 5702: train loss: 0.0008363517699763179, val loss: 0.12051540613174438\n",
      "Epoch 5703: train loss: 0.0014971820637583733, val loss: 0.11305494606494904\n",
      "Epoch 5704: train loss: 0.0016246481100097299, val loss: 0.1157599687576294\n",
      "Epoch 5705: train loss: 0.0005070246988907456, val loss: 0.12122844904661179\n",
      "Epoch 5706: train loss: 0.0002881775435525924, val loss: 0.11348402500152588\n",
      "Epoch 5707: train loss: 0.0007147110300138593, val loss: 0.10523807257413864\n",
      "Epoch 5708: train loss: 0.0001908940466819331, val loss: 0.11589562892913818\n",
      "Epoch 5709: train loss: 0.00040042350883595645, val loss: 0.1176876574754715\n",
      "Epoch 5710: train loss: 0.00022125306713860482, val loss: 0.1095699891448021\n",
      "Epoch 5711: train loss: 0.00023403667728416622, val loss: 0.11026489734649658\n",
      "Epoch 5712: train loss: 0.00018840310804080218, val loss: 0.11291193962097168\n",
      "Epoch 5713: train loss: 0.00015772870392538607, val loss: 0.11187978088855743\n",
      "Epoch 5714: train loss: 0.00015011741197668016, val loss: 0.11124908179044724\n",
      "Epoch 5715: train loss: 0.00012185021478217095, val loss: 0.11168056726455688\n",
      "Epoch 5716: train loss: 0.00010646062582964078, val loss: 0.11170130968093872\n",
      "Epoch 5717: train loss: 0.0001070209254976362, val loss: 0.11294388025999069\n",
      "Epoch 5718: train loss: 6.914553523529321e-05, val loss: 0.11336886882781982\n",
      "Epoch 5719: train loss: 9.585222142050043e-05, val loss: 0.1109938770532608\n",
      "Epoch 5720: train loss: 4.5449101889971644e-05, val loss: 0.11095219850540161\n",
      "Epoch 5721: train loss: 7.849116082070395e-05, val loss: 0.1130448505282402\n",
      "Epoch 5722: train loss: 4.2229657992720604e-05, val loss: 0.11262035369873047\n",
      "Epoch 5723: train loss: 4.793347034137696e-05, val loss: 0.11093971878290176\n",
      "Epoch 5724: train loss: 4.6158635086612776e-05, val loss: 0.11232955753803253\n",
      "Epoch 5725: train loss: 2.902442247432191e-05, val loss: 0.1135452538728714\n",
      "Epoch 5726: train loss: 4.21287513745483e-05, val loss: 0.11159569025039673\n",
      "Epoch 5727: train loss: 2.2868349333293736e-05, val loss: 0.11070246994495392\n",
      "Epoch 5728: train loss: 2.9790360713377595e-05, val loss: 0.1124434843659401\n",
      "Epoch 5729: train loss: 2.4692424631211907e-05, val loss: 0.11307378113269806\n",
      "Epoch 5730: train loss: 1.7501344700576738e-05, val loss: 0.11219165474176407\n",
      "Epoch 5731: train loss: 2.3787417376297526e-05, val loss: 0.11253998428583145\n",
      "Epoch 5732: train loss: 1.3318091987457592e-05, val loss: 0.11346196383237839\n",
      "Epoch 5733: train loss: 1.750680894474499e-05, val loss: 0.11292105168104172\n",
      "Epoch 5734: train loss: 1.387942029396072e-05, val loss: 0.11220251768827438\n",
      "Epoch 5735: train loss: 1.0861464033951052e-05, val loss: 0.11229649931192398\n",
      "Epoch 5736: train loss: 1.395218805555487e-05, val loss: 0.1121690422296524\n",
      "Epoch 5737: train loss: 7.066484158713138e-06, val loss: 0.11233730614185333\n",
      "Epoch 5738: train loss: 1.1816659934993368e-05, val loss: 0.11356176435947418\n",
      "Epoch 5739: train loss: 6.751156888640253e-06, val loss: 0.11399058252573013\n",
      "Epoch 5740: train loss: 8.03823240858037e-06, val loss: 0.11284740269184113\n",
      "Epoch 5741: train loss: 6.8537187871697824e-06, val loss: 0.11244872957468033\n",
      "Epoch 5742: train loss: 5.551806680159643e-06, val loss: 0.11323799937963486\n",
      "Epoch 5743: train loss: 6.294189915934112e-06, val loss: 0.11339161545038223\n",
      "Epoch 5744: train loss: 3.839416422124486e-06, val loss: 0.11315310001373291\n",
      "Epoch 5745: train loss: 5.454543043015292e-06, val loss: 0.11355191469192505\n",
      "Epoch 5746: train loss: 3.1524971291219117e-06, val loss: 0.11352749913930893\n",
      "Epoch 5747: train loss: 4.2952692638209555e-06, val loss: 0.11286093294620514\n",
      "Epoch 5748: train loss: 2.7892529033124447e-06, val loss: 0.11329960823059082\n",
      "Epoch 5749: train loss: 3.341094497955055e-06, val loss: 0.1143927201628685\n",
      "Epoch 5750: train loss: 2.4237501747848e-06, val loss: 0.1141233965754509\n",
      "Epoch 5751: train loss: 2.4986218249978265e-06, val loss: 0.11330608278512955\n",
      "Epoch 5752: train loss: 2.219334874098422e-06, val loss: 0.11368872970342636\n",
      "Epoch 5753: train loss: 2.001625034608878e-06, val loss: 0.11441848427057266\n",
      "Epoch 5754: train loss: 1.7089433868022752e-06, val loss: 0.11425571888685226\n",
      "Epoch 5755: train loss: 1.7651185544309556e-06, val loss: 0.11402261257171631\n",
      "Epoch 5756: train loss: 1.3772026932201697e-06, val loss: 0.11419134587049484\n",
      "Epoch 5757: train loss: 1.3754263363807695e-06, val loss: 0.11436226218938828\n",
      "Epoch 5758: train loss: 1.206406864184828e-06, val loss: 0.1146821528673172\n",
      "Epoch 5759: train loss: 1.1110237210232299e-06, val loss: 0.11485471576452255\n",
      "Epoch 5760: train loss: 1.0378464594396064e-06, val loss: 0.11448085308074951\n",
      "Epoch 5761: train loss: 8.424003681284375e-07, val loss: 0.11449015140533447\n",
      "Epoch 5762: train loss: 8.440032388534746e-07, val loss: 0.11523862928152084\n",
      "Epoch 5763: train loss: 8.481217150801967e-07, val loss: 0.1155204102396965\n",
      "Epoch 5764: train loss: 5.176945592211268e-07, val loss: 0.11522568762302399\n",
      "Epoch 5765: train loss: 7.721075121480681e-07, val loss: 0.11533599346876144\n",
      "Epoch 5766: train loss: 4.624201039860054e-07, val loss: 0.11553996056318283\n",
      "Epoch 5767: train loss: 5.26040196291433e-07, val loss: 0.1154445931315422\n",
      "Epoch 5768: train loss: 4.5467660925169184e-07, val loss: 0.11560820788145065\n",
      "Epoch 5769: train loss: 4.18330387219612e-07, val loss: 0.11580472439527512\n",
      "Epoch 5770: train loss: 3.0657389515909017e-07, val loss: 0.11579006165266037\n",
      "Epoch 5771: train loss: 4.4442754187912215e-07, val loss: 0.11596403270959854\n",
      "Epoch 5772: train loss: 1.8679340030303138e-07, val loss: 0.11617652326822281\n",
      "Epoch 5773: train loss: 3.4946529581247887e-07, val loss: 0.11622107028961182\n",
      "Epoch 5774: train loss: 2.2152590872792643e-07, val loss: 0.11633211374282837\n",
      "Epoch 5775: train loss: 2.510412002720841e-07, val loss: 0.11633201688528061\n",
      "Epoch 5776: train loss: 1.579759327796637e-07, val loss: 0.11637871712446213\n",
      "Epoch 5777: train loss: 2.4580430135756615e-07, val loss: 0.11671141535043716\n",
      "Epoch 5778: train loss: 1.436341960925347e-07, val loss: 0.11679601669311523\n",
      "Epoch 5779: train loss: 1.4610640164391953e-07, val loss: 0.1167105957865715\n",
      "Epoch 5780: train loss: 1.5879400905305374e-07, val loss: 0.11687978357076645\n",
      "Epoch 5781: train loss: 1.4108066181961476e-07, val loss: 0.1170279011130333\n",
      "Epoch 5782: train loss: 9.224774544236425e-08, val loss: 0.11716756969690323\n",
      "Epoch 5783: train loss: 1.1563032131789441e-07, val loss: 0.11727998405694962\n",
      "Epoch 5784: train loss: 1.0922212823061273e-07, val loss: 0.11726279556751251\n",
      "Epoch 5785: train loss: 8.953399799338513e-08, val loss: 0.11746517568826675\n",
      "Epoch 5786: train loss: 7.924259648461884e-08, val loss: 0.11759861558675766\n",
      "Epoch 5787: train loss: 6.690017784194424e-08, val loss: 0.11756893247365952\n",
      "Epoch 5788: train loss: 8.058397327204148e-08, val loss: 0.11772944778203964\n",
      "Epoch 5789: train loss: 7.217204256448895e-08, val loss: 0.11786108464002609\n",
      "Epoch 5790: train loss: 5.966114713373827e-08, val loss: 0.11801829189062119\n",
      "Epoch 5791: train loss: 4.5779938062651127e-08, val loss: 0.11806757748126984\n",
      "Epoch 5792: train loss: 4.1716099019595276e-08, val loss: 0.11810968071222305\n",
      "Epoch 5793: train loss: 5.492197274747923e-08, val loss: 0.11835336685180664\n",
      "Epoch 5794: train loss: 6.129046425940032e-08, val loss: 0.11841461807489395\n",
      "Epoch 5795: train loss: 4.2934214405931925e-08, val loss: 0.11854114383459091\n",
      "Epoch 5796: train loss: 4.9718497763251435e-08, val loss: 0.11859637498855591\n",
      "Epoch 5797: train loss: 3.701032724734432e-08, val loss: 0.11877173185348511\n",
      "Epoch 5798: train loss: 3.336425180577862e-08, val loss: 0.11887186765670776\n",
      "Epoch 5799: train loss: 2.74263740607239e-08, val loss: 0.11900223791599274\n",
      "Epoch 5800: train loss: 2.0291823332740933e-08, val loss: 0.11907738447189331\n",
      "Epoch 5801: train loss: 1.989746145625304e-08, val loss: 0.11920137703418732\n",
      "Epoch 5802: train loss: 2.493914941226194e-08, val loss: 0.11932269483804703\n",
      "Epoch 5803: train loss: 1.6654157164452954e-08, val loss: 0.11938557773828506\n",
      "Epoch 5804: train loss: 2.5807832315649648e-08, val loss: 0.11954858154058456\n",
      "Epoch 5805: train loss: 4.690201294010876e-08, val loss: 0.11953458935022354\n",
      "Epoch 5806: train loss: 1.3558039313465997e-07, val loss: 0.1198405772447586\n",
      "Epoch 5807: train loss: 3.1685405588177673e-07, val loss: 0.11958903074264526\n",
      "Epoch 5808: train loss: 9.094672464016185e-07, val loss: 0.12039230018854141\n",
      "Epoch 5809: train loss: 2.5591632493160432e-06, val loss: 0.11921369284391403\n",
      "Epoch 5810: train loss: 6.994680461502867e-06, val loss: 0.12151509523391724\n",
      "Epoch 5811: train loss: 1.9511977370711975e-05, val loss: 0.11817643791437149\n",
      "Epoch 5812: train loss: 5.725247319787741e-05, val loss: 0.12338709831237793\n",
      "Epoch 5813: train loss: 0.00016651971964165568, val loss: 0.11450444906949997\n",
      "Epoch 5814: train loss: 0.0005232554394751787, val loss: 0.13102160394191742\n",
      "Epoch 5815: train loss: 0.0012202347861602902, val loss: 0.1111082211136818\n",
      "Epoch 5816: train loss: 0.002328440546989441, val loss: 0.12949052453041077\n",
      "Epoch 5817: train loss: 0.0010884510120376945, val loss: 0.1258818358182907\n",
      "Epoch 5818: train loss: 0.00010562290117377415, val loss: 0.11716105788946152\n",
      "Epoch 5819: train loss: 0.0008028423762880266, val loss: 0.1171843558549881\n",
      "Epoch 5820: train loss: 0.00019468243408482522, val loss: 0.12342502921819687\n",
      "Epoch 5821: train loss: 0.0003907876380253583, val loss: 0.12281014770269394\n",
      "Epoch 5822: train loss: 0.00028603020473383367, val loss: 0.1187782809138298\n",
      "Epoch 5823: train loss: 0.00018733390606939793, val loss: 0.12030012905597687\n",
      "Epoch 5824: train loss: 0.00021364784333854914, val loss: 0.12652169167995453\n",
      "Epoch 5825: train loss: 0.00013663232675753534, val loss: 0.12748821079730988\n",
      "Epoch 5826: train loss: 0.00014867701975163072, val loss: 0.12215552479028702\n",
      "Epoch 5827: train loss: 0.00012108862574677914, val loss: 0.12079989910125732\n",
      "Epoch 5828: train loss: 0.00010323916649213061, val loss: 0.12484557926654816\n",
      "Epoch 5829: train loss: 9.764463902683929e-05, val loss: 0.12726958096027374\n",
      "Epoch 5830: train loss: 8.28688353067264e-05, val loss: 0.12432385981082916\n",
      "Epoch 5831: train loss: 7.371453830273822e-05, val loss: 0.12232210487127304\n",
      "Epoch 5832: train loss: 6.942527397768572e-05, val loss: 0.12405746430158615\n",
      "Epoch 5833: train loss: 5.163094829185866e-05, val loss: 0.12573818862438202\n",
      "Epoch 5834: train loss: 6.144764483906329e-05, val loss: 0.12468335777521133\n",
      "Epoch 5835: train loss: 3.4427022910676897e-05, val loss: 0.1228947564959526\n",
      "Epoch 5836: train loss: 4.8677185986889526e-05, val loss: 0.12256453186273575\n",
      "Epoch 5837: train loss: 3.241316881030798e-05, val loss: 0.12275948375463486\n",
      "Epoch 5838: train loss: 3.1919920729706064e-05, val loss: 0.12276796251535416\n",
      "Epoch 5839: train loss: 3.1934265280142426e-05, val loss: 0.12292557209730148\n",
      "Epoch 5840: train loss: 2.1625708541250788e-05, val loss: 0.12321221828460693\n",
      "Epoch 5841: train loss: 3.0134655389701948e-05, val loss: 0.12236452102661133\n",
      "Epoch 5842: train loss: 1.3044244042248465e-05, val loss: 0.12238626927137375\n",
      "Epoch 5843: train loss: 2.530155143176671e-05, val loss: 0.12365911155939102\n",
      "Epoch 5844: train loss: 1.2452955161279533e-05, val loss: 0.12415333092212677\n",
      "Epoch 5845: train loss: 1.768938273016829e-05, val loss: 0.12265272438526154\n",
      "Epoch 5846: train loss: 1.2392310054565314e-05, val loss: 0.12162931263446808\n",
      "Epoch 5847: train loss: 1.3205170944274869e-05, val loss: 0.1225898489356041\n",
      "Epoch 5848: train loss: 1.004700970952399e-05, val loss: 0.12356233596801758\n",
      "Epoch 5849: train loss: 1.1453624210844282e-05, val loss: 0.1231716051697731\n",
      "Epoch 5850: train loss: 6.777462203899631e-06, val loss: 0.12241639941930771\n",
      "Epoch 5851: train loss: 1.0816040230565704e-05, val loss: 0.12220869213342667\n",
      "Epoch 5852: train loss: 3.898599970852956e-06, val loss: 0.12239187210798264\n",
      "Epoch 5853: train loss: 1.0203161764366087e-05, val loss: 0.12262728065252304\n",
      "Epoch 5854: train loss: 2.7642036002362147e-06, val loss: 0.12306606769561768\n",
      "Epoch 5855: train loss: 7.502651442337083e-06, val loss: 0.12332232296466827\n",
      "Epoch 5856: train loss: 3.7221038837742526e-06, val loss: 0.12304671853780746\n",
      "Epoch 5857: train loss: 4.524674750427948e-06, val loss: 0.1227494478225708\n",
      "Epoch 5858: train loss: 4.676323897001566e-06, val loss: 0.12298601120710373\n",
      "Epoch 5859: train loss: 2.197718004026683e-06, val loss: 0.1233663335442543\n",
      "Epoch 5860: train loss: 4.806073320651194e-06, val loss: 0.12319651991128922\n",
      "Epoch 5861: train loss: 1.625752474865294e-06, val loss: 0.12286689132452011\n",
      "Epoch 5862: train loss: 3.594406962292851e-06, val loss: 0.123138926923275\n",
      "Epoch 5863: train loss: 1.9120561773888767e-06, val loss: 0.12357858568429947\n",
      "Epoch 5864: train loss: 2.4686642063898034e-06, val loss: 0.1234961748123169\n",
      "Epoch 5865: train loss: 1.7439423345422256e-06, val loss: 0.12321574985980988\n",
      "Epoch 5866: train loss: 1.8473701857146807e-06, val loss: 0.12341108173131943\n",
      "Epoch 5867: train loss: 1.5261819044098957e-06, val loss: 0.12383510172367096\n",
      "Epoch 5868: train loss: 1.558778762955626e-06, val loss: 0.12388624995946884\n",
      "Epoch 5869: train loss: 1.1821391581179341e-06, val loss: 0.12375164031982422\n",
      "Epoch 5870: train loss: 1.3195985957281664e-06, val loss: 0.12381463497877121\n",
      "Epoch 5871: train loss: 9.231684998667333e-07, val loss: 0.1239665076136589\n",
      "Epoch 5872: train loss: 1.1275697033852339e-06, val loss: 0.12413015216588974\n",
      "Epoch 5873: train loss: 8.079181270659319e-07, val loss: 0.12432756274938583\n",
      "Epoch 5874: train loss: 8.354653573405812e-07, val loss: 0.12434076517820358\n",
      "Epoch 5875: train loss: 7.553840077889618e-07, val loss: 0.12424758821725845\n",
      "Epoch 5876: train loss: 6.337627951324976e-07, val loss: 0.12440582364797592\n",
      "Epoch 5877: train loss: 6.350043690872553e-07, val loss: 0.12469803541898727\n",
      "Epoch 5878: train loss: 5.074564342066878e-07, val loss: 0.12474781274795532\n",
      "Epoch 5879: train loss: 5.180858693165646e-07, val loss: 0.12472482025623322\n",
      "Epoch 5880: train loss: 4.363711241239798e-07, val loss: 0.12486882507801056\n",
      "Epoch 5881: train loss: 4.261478352418635e-07, val loss: 0.12499990314245224\n",
      "Epoch 5882: train loss: 3.181664851581445e-07, val loss: 0.12502525746822357\n",
      "Epoch 5883: train loss: 4.148571122186695e-07, val loss: 0.12506914138793945\n",
      "Epoch 5884: train loss: 2.3302185070406267e-07, val loss: 0.12519226968288422\n",
      "Epoch 5885: train loss: 3.2630325108584657e-07, val loss: 0.12530043721199036\n",
      "Epoch 5886: train loss: 2.4524368313905143e-07, val loss: 0.125359907746315\n",
      "Epoch 5887: train loss: 2.266778551529569e-07, val loss: 0.1254555732011795\n",
      "Epoch 5888: train loss: 1.9790303440458956e-07, val loss: 0.1255672425031662\n",
      "Epoch 5889: train loss: 2.1457677235048322e-07, val loss: 0.12563391029834747\n",
      "Epoch 5890: train loss: 1.5639793105037825e-07, val loss: 0.12559571862220764\n",
      "Epoch 5891: train loss: 1.6540118963348505e-07, val loss: 0.12564043700695038\n",
      "Epoch 5892: train loss: 1.5056272673064086e-07, val loss: 0.1258731633424759\n",
      "Epoch 5893: train loss: 1.2360455059479136e-07, val loss: 0.12599493563175201\n",
      "Epoch 5894: train loss: 1.1794687537758364e-07, val loss: 0.12593401968479156\n",
      "Epoch 5895: train loss: 1.2412034777753433e-07, val loss: 0.12601907551288605\n",
      "Epoch 5896: train loss: 8.398266970743862e-08, val loss: 0.12621495127677917\n",
      "Epoch 5897: train loss: 1.0623096358131079e-07, val loss: 0.12625785171985626\n",
      "Epoch 5898: train loss: 7.785700262274986e-08, val loss: 0.1263076215982437\n",
      "Epoch 5899: train loss: 1.0403608996512048e-07, val loss: 0.12642602622509003\n",
      "Epoch 5900: train loss: 5.4406864791189946e-08, val loss: 0.12657473981380463\n",
      "Epoch 5901: train loss: 6.938629582009526e-08, val loss: 0.12658467888832092\n",
      "Epoch 5902: train loss: 5.904188071781391e-08, val loss: 0.12659147381782532\n",
      "Epoch 5903: train loss: 5.28142471978299e-08, val loss: 0.12676183879375458\n",
      "Epoch 5904: train loss: 5.660983148914056e-08, val loss: 0.12690339982509613\n",
      "Epoch 5905: train loss: 7.009555957893099e-08, val loss: 0.126906618475914\n",
      "Epoch 5906: train loss: 4.298198419405708e-08, val loss: 0.12692002952098846\n",
      "Epoch 5907: train loss: 3.230635314821484e-08, val loss: 0.12708088755607605\n",
      "Epoch 5908: train loss: 4.365767836134182e-08, val loss: 0.12717746198177338\n",
      "Epoch 5909: train loss: 2.9730076178680065e-08, val loss: 0.12721095979213715\n",
      "Epoch 5910: train loss: 3.4596123299479586e-08, val loss: 0.1273469775915146\n",
      "Epoch 5911: train loss: 4.994788227463687e-08, val loss: 0.12734797596931458\n",
      "Epoch 5912: train loss: 9.903326514404398e-08, val loss: 0.12747183442115784\n",
      "Epoch 5913: train loss: 2.429345329346688e-07, val loss: 0.12772861123085022\n",
      "Epoch 5914: train loss: 4.1382401150258374e-07, val loss: 0.12753254175186157\n",
      "Epoch 5915: train loss: 5.955317874395405e-07, val loss: 0.12789805233478546\n",
      "Epoch 5916: train loss: 9.01311636880564e-07, val loss: 0.12793408334255219\n",
      "Epoch 5917: train loss: 1.0613194945108262e-06, val loss: 0.12806884944438934\n",
      "Epoch 5918: train loss: 7.984525041138113e-07, val loss: 0.12802837789058685\n",
      "Epoch 5919: train loss: 8.994381914817495e-07, val loss: 0.12835966050624847\n",
      "Epoch 5920: train loss: 1.2869109013990965e-06, val loss: 0.1282024383544922\n",
      "Epoch 5921: train loss: 1.6770164847912383e-06, val loss: 0.12860475480556488\n",
      "Epoch 5922: train loss: 2.002531346079195e-06, val loss: 0.12827168405056\n",
      "Epoch 5923: train loss: 2.7249341201240895e-06, val loss: 0.1288263201713562\n",
      "Epoch 5924: train loss: 4.1852990761981346e-06, val loss: 0.12835590541362762\n",
      "Epoch 5925: train loss: 7.483497938665096e-06, val loss: 0.12913848459720612\n",
      "Epoch 5926: train loss: 1.4399225619854406e-05, val loss: 0.128309428691864\n",
      "Epoch 5927: train loss: 2.9289143640198745e-05, val loss: 0.12967056035995483\n",
      "Epoch 5928: train loss: 6.340719846775755e-05, val loss: 0.12784092128276825\n",
      "Epoch 5929: train loss: 0.000133883353555575, val loss: 0.13051390647888184\n",
      "Epoch 5930: train loss: 0.0002758239861577749, val loss: 0.1264369785785675\n",
      "Epoch 5931: train loss: 0.000463530101114884, val loss: 0.13068504631519318\n",
      "Epoch 5932: train loss: 0.0005359228234738111, val loss: 0.12437579780817032\n",
      "Epoch 5933: train loss: 0.0003036276903003454, val loss: 0.12614552676677704\n",
      "Epoch 5934: train loss: 1.2434996278898325e-05, val loss: 0.12593266367912292\n",
      "Epoch 5935: train loss: 0.0001478205667808652, val loss: 0.12028541415929794\n",
      "Epoch 5936: train loss: 0.00023844469978939742, val loss: 0.12201301008462906\n",
      "Epoch 5937: train loss: 4.4828157115262e-05, val loss: 0.12203118950128555\n",
      "Epoch 5938: train loss: 8.007404539966956e-05, val loss: 0.12033095210790634\n",
      "Epoch 5939: train loss: 0.00014194998948369175, val loss: 0.12062157690525055\n",
      "Epoch 5940: train loss: 3.089573874603957e-05, val loss: 0.12044258415699005\n",
      "Epoch 5941: train loss: 7.70685073803179e-05, val loss: 0.11978822946548462\n",
      "Epoch 5942: train loss: 7.02378893038258e-05, val loss: 0.11960525810718536\n",
      "Epoch 5943: train loss: 3.075005588470958e-05, val loss: 0.11854169517755508\n",
      "Epoch 5944: train loss: 6.087795918574557e-05, val loss: 0.11758299171924591\n",
      "Epoch 5945: train loss: 3.1739364203531295e-05, val loss: 0.11825426667928696\n",
      "Epoch 5946: train loss: 3.880974691128358e-05, val loss: 0.11818071454763412\n",
      "Epoch 5947: train loss: 3.122491034446284e-05, val loss: 0.11750030517578125\n",
      "Epoch 5948: train loss: 2.7575100830290467e-05, val loss: 0.11823631823062897\n",
      "Epoch 5949: train loss: 2.601753658382222e-05, val loss: 0.1185493990778923\n",
      "Epoch 5950: train loss: 2.1171645130380057e-05, val loss: 0.11764094978570938\n",
      "Epoch 5951: train loss: 2.2072317733545788e-05, val loss: 0.11753766983747482\n",
      "Epoch 5952: train loss: 1.5269892173819244e-05, val loss: 0.11792609840631485\n",
      "Epoch 5953: train loss: 2.038768434431404e-05, val loss: 0.11720802634954453\n",
      "Epoch 5954: train loss: 1.0316932275600266e-05, val loss: 0.11696624755859375\n",
      "Epoch 5955: train loss: 1.630705810384825e-05, val loss: 0.11765926331281662\n",
      "Epoch 5956: train loss: 1.1156113941979129e-05, val loss: 0.11744794994592667\n",
      "Epoch 5957: train loss: 8.895372957340442e-06, val loss: 0.11696658283472061\n",
      "Epoch 5958: train loss: 1.3547441085393075e-05, val loss: 0.1174076721072197\n",
      "Epoch 5959: train loss: 4.346738933236338e-06, val loss: 0.11751165241003036\n",
      "Epoch 5960: train loss: 1.168378094007494e-05, val loss: 0.11690733581781387\n",
      "Epoch 5961: train loss: 5.2429559218580835e-06, val loss: 0.11704995483160019\n",
      "Epoch 5962: train loss: 6.260887857933994e-06, val loss: 0.11753127723932266\n",
      "Epoch 5963: train loss: 7.961659321154002e-06, val loss: 0.11721771955490112\n",
      "Epoch 5964: train loss: 2.408520913377288e-06, val loss: 0.1170138344168663\n",
      "Epoch 5965: train loss: 7.278677912836429e-06, val loss: 0.11753764003515244\n",
      "Epoch 5966: train loss: 3.363898031238932e-06, val loss: 0.1174674779176712\n",
      "Epoch 5967: train loss: 3.4080844670825172e-06, val loss: 0.11678450554609299\n",
      "Epoch 5968: train loss: 5.176987087907037e-06, val loss: 0.11715947836637497\n",
      "Epoch 5969: train loss: 1.6113682477225666e-06, val loss: 0.11788389831781387\n",
      "Epoch 5970: train loss: 4.105663720110897e-06, val loss: 0.11749467998743057\n",
      "Epoch 5971: train loss: 2.3392988168779993e-06, val loss: 0.11738038063049316\n",
      "Epoch 5972: train loss: 2.19061666939524e-06, val loss: 0.11794210970401764\n",
      "Epoch 5973: train loss: 2.9796494800393702e-06, val loss: 0.11759590357542038\n",
      "Epoch 5974: train loss: 1.1815475318144308e-06, val loss: 0.1173357293009758\n",
      "Epoch 5975: train loss: 2.474173470545793e-06, val loss: 0.11802132427692413\n",
      "Epoch 5976: train loss: 1.6948276879702462e-06, val loss: 0.11794109642505646\n",
      "Epoch 5977: train loss: 9.723274843054242e-07, val loss: 0.11773166805505753\n",
      "Epoch 5978: train loss: 2.1648565962095745e-06, val loss: 0.1183481365442276\n",
      "Epoch 5979: train loss: 9.4936712002891e-07, val loss: 0.11820151656866074\n",
      "Epoch 5980: train loss: 9.539995744489715e-07, val loss: 0.11801695078611374\n",
      "Epoch 5981: train loss: 1.6105955182865728e-06, val loss: 0.11864405870437622\n",
      "Epoch 5982: train loss: 5.995812557557656e-07, val loss: 0.11853554099798203\n",
      "Epoch 5983: train loss: 8.7035959950299e-07, val loss: 0.11831140518188477\n",
      "Epoch 5984: train loss: 1.1933955192944268e-06, val loss: 0.11876247078180313\n",
      "Epoch 5985: train loss: 4.6236331741056347e-07, val loss: 0.11882888525724411\n",
      "Epoch 5986: train loss: 7.412071454382385e-07, val loss: 0.1188092827796936\n",
      "Epoch 5987: train loss: 8.811786642581865e-07, val loss: 0.1190149337053299\n",
      "Epoch 5988: train loss: 4.501378043642035e-07, val loss: 0.11912915855646133\n",
      "Epoch 5989: train loss: 5.237890263742884e-07, val loss: 0.11907558888196945\n",
      "Epoch 5990: train loss: 6.222617230378091e-07, val loss: 0.11927570402622223\n",
      "Epoch 5991: train loss: 4.5641573365173826e-07, val loss: 0.11953802406787872\n",
      "Epoch 5992: train loss: 4.4080201178076095e-07, val loss: 0.11941094696521759\n",
      "Epoch 5993: train loss: 4.634319168417278e-07, val loss: 0.11962666362524033\n",
      "Epoch 5994: train loss: 3.6352517440718657e-07, val loss: 0.11969652026891708\n",
      "Epoch 5995: train loss: 3.072565846196085e-07, val loss: 0.11975784599781036\n",
      "Epoch 5996: train loss: 4.099711077287793e-07, val loss: 0.11996134370565414\n",
      "Epoch 5997: train loss: 4.293266329113976e-07, val loss: 0.12002134323120117\n",
      "Epoch 5998: train loss: 2.543085315664939e-07, val loss: 0.1201518326997757\n",
      "Epoch 5999: train loss: 2.4200872417168284e-07, val loss: 0.12021780014038086\n",
      "Epoch 6000: train loss: 4.696428277384257e-07, val loss: 0.12030865997076035\n",
      "Epoch 6001: train loss: 7.46348860047874e-07, val loss: 0.12045444548130035\n",
      "Epoch 6002: train loss: 1.227570805895084e-06, val loss: 0.12051037698984146\n",
      "Epoch 6003: train loss: 2.5295134946645703e-06, val loss: 0.1208147183060646\n",
      "Epoch 6004: train loss: 6.593499620066723e-06, val loss: 0.12079256027936935\n",
      "Epoch 6005: train loss: 1.9996830815216526e-05, val loss: 0.12103383988142014\n",
      "Epoch 6006: train loss: 6.607878458453342e-05, val loss: 0.12052500247955322\n",
      "Epoch 6007: train loss: 0.00022957213514018804, val loss: 0.12263736873865128\n",
      "Epoch 6008: train loss: 0.0007727124029770494, val loss: 0.12081704288721085\n",
      "Epoch 6009: train loss: 0.0017398798372596502, val loss: 0.12526774406433105\n",
      "Epoch 6010: train loss: 0.001770675415173173, val loss: 0.12307281792163849\n",
      "Epoch 6011: train loss: 0.0009537450969219208, val loss: 0.12588004767894745\n",
      "Epoch 6012: train loss: 0.00032925987034104764, val loss: 0.12480197101831436\n",
      "Epoch 6013: train loss: 0.0007099726353771985, val loss: 0.12331824749708176\n",
      "Epoch 6014: train loss: 0.0002970118948724121, val loss: 0.12380456924438477\n",
      "Epoch 6015: train loss: 0.0003932090476155281, val loss: 0.12486465275287628\n",
      "Epoch 6016: train loss: 0.0002350875292904675, val loss: 0.12506502866744995\n",
      "Epoch 6017: train loss: 0.0002792800951283425, val loss: 0.12589941918849945\n",
      "Epoch 6018: train loss: 0.00013523818051908165, val loss: 0.12808765470981598\n",
      "Epoch 6019: train loss: 0.00022971737780608237, val loss: 0.13011088967323303\n",
      "Epoch 6020: train loss: 9.256543853553012e-05, val loss: 0.1304466426372528\n",
      "Epoch 6021: train loss: 0.00015954661648720503, val loss: 0.13091570138931274\n",
      "Epoch 6022: train loss: 8.109593181870878e-05, val loss: 0.1326959878206253\n",
      "Epoch 6023: train loss: 9.202537330565974e-05, val loss: 0.13436275720596313\n",
      "Epoch 6024: train loss: 8.472907938994467e-05, val loss: 0.13387927412986755\n",
      "Epoch 6025: train loss: 5.174021498532966e-05, val loss: 0.13256090879440308\n",
      "Epoch 6026: train loss: 7.844832725822926e-05, val loss: 0.1323891133069992\n",
      "Epoch 6027: train loss: 3.7091042031534016e-05, val loss: 0.133090078830719\n",
      "Epoch 6028: train loss: 5.240166137809865e-05, val loss: 0.13312412798404694\n",
      "Epoch 6029: train loss: 4.488084960030392e-05, val loss: 0.13210859894752502\n",
      "Epoch 6030: train loss: 2.4860262783477083e-05, val loss: 0.1317426711320877\n",
      "Epoch 6031: train loss: 4.315695696277544e-05, val loss: 0.13264167308807373\n",
      "Epoch 6032: train loss: 2.446092366881203e-05, val loss: 0.13353602588176727\n",
      "Epoch 6033: train loss: 2.0815516108996235e-05, val loss: 0.13339176774024963\n",
      "Epoch 6034: train loss: 3.134984217467718e-05, val loss: 0.13240651786327362\n",
      "Epoch 6035: train loss: 1.5570447430945933e-05, val loss: 0.13193674385547638\n",
      "Epoch 6036: train loss: 1.7355956515530124e-05, val loss: 0.13215693831443787\n",
      "Epoch 6037: train loss: 2.0936095097567886e-05, val loss: 0.1324472725391388\n",
      "Epoch 6038: train loss: 1.1961369636992458e-05, val loss: 0.13289888203144073\n",
      "Epoch 6039: train loss: 1.3032259630563203e-05, val loss: 0.1335550844669342\n",
      "Epoch 6040: train loss: 1.3339576980797574e-05, val loss: 0.1335093080997467\n",
      "Epoch 6041: train loss: 1.0251406820316333e-05, val loss: 0.13253489136695862\n",
      "Epoch 6042: train loss: 9.382169992022682e-06, val loss: 0.13179969787597656\n",
      "Epoch 6043: train loss: 9.371866326546296e-06, val loss: 0.1321357935667038\n",
      "Epoch 6044: train loss: 7.236650162667502e-06, val loss: 0.13295607268810272\n",
      "Epoch 6045: train loss: 8.053489182202611e-06, val loss: 0.13326095044612885\n",
      "Epoch 6046: train loss: 6.191441116243368e-06, val loss: 0.13286936283111572\n",
      "Epoch 6047: train loss: 4.909011749987258e-06, val loss: 0.13239361345767975\n",
      "Epoch 6048: train loss: 6.482658591266954e-06, val loss: 0.13224253058433533\n",
      "Epoch 6049: train loss: 3.7207942114036996e-06, val loss: 0.13212814927101135\n",
      "Epoch 6050: train loss: 4.230948889016872e-06, val loss: 0.13184551894664764\n",
      "Epoch 6051: train loss: 4.838012500840705e-06, val loss: 0.1316206008195877\n",
      "Epoch 6052: train loss: 2.1119553821336012e-06, val loss: 0.1313953548669815\n",
      "Epoch 6053: train loss: 4.081810857314849e-06, val loss: 0.13098834455013275\n",
      "Epoch 6054: train loss: 2.845662947947858e-06, val loss: 0.13064634799957275\n",
      "Epoch 6055: train loss: 1.7174775166495237e-06, val loss: 0.1304820030927658\n",
      "Epoch 6056: train loss: 3.4613046864251373e-06, val loss: 0.1301029771566391\n",
      "Epoch 6057: train loss: 1.3277186781124328e-06, val loss: 0.12939386069774628\n",
      "Epoch 6058: train loss: 1.9927631456084782e-06, val loss: 0.1292060911655426\n",
      "Epoch 6059: train loss: 1.9993537989648758e-06, val loss: 0.12963585555553436\n",
      "Epoch 6060: train loss: 9.252138397641829e-07, val loss: 0.13002429902553558\n",
      "Epoch 6061: train loss: 1.9069428844886716e-06, val loss: 0.12996463477611542\n",
      "Epoch 6062: train loss: 9.818543276196579e-07, val loss: 0.12969379127025604\n",
      "Epoch 6063: train loss: 1.0986253755618236e-06, val loss: 0.1297294646501541\n",
      "Epoch 6064: train loss: 1.2107901739000226e-06, val loss: 0.13008975982666016\n",
      "Epoch 6065: train loss: 6.82979930388683e-07, val loss: 0.1303447037935257\n",
      "Epoch 6066: train loss: 9.311174267168099e-07, val loss: 0.13031531870365143\n",
      "Epoch 6067: train loss: 7.91802563071542e-07, val loss: 0.1301836222410202\n",
      "Epoch 6068: train loss: 5.840188350703102e-07, val loss: 0.1301494538784027\n",
      "Epoch 6069: train loss: 7.147961582631979e-07, val loss: 0.13029231131076813\n",
      "Epoch 6070: train loss: 4.889329829893541e-07, val loss: 0.1305520385503769\n",
      "Epoch 6071: train loss: 5.576224566539167e-07, val loss: 0.1308058351278305\n",
      "Epoch 6072: train loss: 4.4012148237015936e-07, val loss: 0.1309187263250351\n",
      "Epoch 6073: train loss: 4.196373879494786e-07, val loss: 0.13102507591247559\n",
      "Epoch 6074: train loss: 4.3251534975752293e-07, val loss: 0.13124936819076538\n",
      "Epoch 6075: train loss: 2.4309028390234744e-07, val loss: 0.1315198689699173\n",
      "Epoch 6076: train loss: 4.5579437824017077e-07, val loss: 0.13168518245220184\n",
      "Epoch 6077: train loss: 1.73753321064396e-07, val loss: 0.1317901760339737\n",
      "Epoch 6078: train loss: 3.5244869422967895e-07, val loss: 0.1320076882839203\n",
      "Epoch 6079: train loss: 1.8769225107462262e-07, val loss: 0.1322977989912033\n",
      "Epoch 6080: train loss: 2.276832304914933e-07, val loss: 0.13249655067920685\n",
      "Epoch 6081: train loss: 2.2004705613198894e-07, val loss: 0.13264338672161102\n",
      "Epoch 6082: train loss: 1.4317667762497877e-07, val loss: 0.13285218179225922\n",
      "Epoch 6083: train loss: 2.0555708601932565e-07, val loss: 0.13309256732463837\n",
      "Epoch 6084: train loss: 1.1362409679804841e-07, val loss: 0.13326017558574677\n",
      "Epoch 6085: train loss: 1.6285544290894904e-07, val loss: 0.1334242969751358\n",
      "Epoch 6086: train loss: 1.1679306766154696e-07, val loss: 0.13365311920642853\n",
      "Epoch 6087: train loss: 1.1418764955806182e-07, val loss: 0.13380785286426544\n",
      "Epoch 6088: train loss: 1.0727805488386366e-07, val loss: 0.13390934467315674\n",
      "Epoch 6089: train loss: 9.010002344211898e-08, val loss: 0.13408778607845306\n",
      "Epoch 6090: train loss: 8.979091603578127e-08, val loss: 0.13429699838161469\n",
      "Epoch 6091: train loss: 7.60237526264973e-08, val loss: 0.13447606563568115\n",
      "Epoch 6092: train loss: 7.779642885452631e-08, val loss: 0.13462093472480774\n",
      "Epoch 6093: train loss: 5.61625910222574e-08, val loss: 0.13477034866809845\n",
      "Epoch 6094: train loss: 7.099097132368115e-08, val loss: 0.1349436193704605\n",
      "Epoch 6095: train loss: 4.8847780931282614e-08, val loss: 0.13509738445281982\n",
      "Epoch 6096: train loss: 5.625095411687653e-08, val loss: 0.13529090583324432\n",
      "Epoch 6097: train loss: 4.7188617458004956e-08, val loss: 0.13546358048915863\n",
      "Epoch 6098: train loss: 4.726276259248152e-08, val loss: 0.13564151525497437\n",
      "Epoch 6099: train loss: 4.431287337069989e-08, val loss: 0.13578210771083832\n",
      "Epoch 6100: train loss: 5.359021315598511e-08, val loss: 0.1359957754611969\n",
      "Epoch 6101: train loss: 6.625312920505166e-08, val loss: 0.13610196113586426\n",
      "Epoch 6102: train loss: 1.3543433396989712e-07, val loss: 0.13637857139110565\n",
      "Epoch 6103: train loss: 3.581153293907846e-07, val loss: 0.13631735742092133\n",
      "Epoch 6104: train loss: 1.1786992217821535e-06, val loss: 0.13689753413200378\n",
      "Epoch 6105: train loss: 4.309955784265185e-06, val loss: 0.13642318546772003\n",
      "Epoch 6106: train loss: 1.796637116058264e-05, val loss: 0.1382511705160141\n",
      "Epoch 6107: train loss: 6.864577153464779e-05, val loss: 0.1353384107351303\n",
      "Epoch 6108: train loss: 0.00021819191169925034, val loss: 0.1409740149974823\n",
      "Epoch 6109: train loss: 0.0004584005509968847, val loss: 0.13071756064891815\n",
      "Epoch 6110: train loss: 0.0008173756650649011, val loss: 0.13795767724514008\n",
      "Epoch 6111: train loss: 0.0003566454688552767, val loss: 0.13518105447292328\n",
      "Epoch 6112: train loss: 3.498194564599544e-05, val loss: 0.1289060115814209\n",
      "Epoch 6113: train loss: 0.00035710897645913064, val loss: 0.12948058545589447\n",
      "Epoch 6114: train loss: 0.00019789161160588264, val loss: 0.12702439725399017\n",
      "Epoch 6115: train loss: 5.856503048562445e-05, val loss: 0.12204338610172272\n",
      "Epoch 6116: train loss: 0.00023523636627942324, val loss: 0.11954081058502197\n",
      "Epoch 6117: train loss: 5.1321079808985814e-05, val loss: 0.11972556263208389\n",
      "Epoch 6118: train loss: 0.0001356632128590718, val loss: 0.11623356491327286\n",
      "Epoch 6119: train loss: 0.00010742102313088253, val loss: 0.11252722889184952\n",
      "Epoch 6120: train loss: 5.2192768634995446e-05, val loss: 0.11382468044757843\n",
      "Epoch 6121: train loss: 0.00011598904529819265, val loss: 0.11324474960565567\n",
      "Epoch 6122: train loss: 2.6502046239329502e-05, val loss: 0.11007239669561386\n",
      "Epoch 6123: train loss: 8.700925536686555e-05, val loss: 0.10814614593982697\n",
      "Epoch 6124: train loss: 3.6780256778001785e-05, val loss: 0.10791392624378204\n",
      "Epoch 6125: train loss: 4.928984344587661e-05, val loss: 0.10670888423919678\n",
      "Epoch 6126: train loss: 4.9440935981692746e-05, val loss: 0.10573519766330719\n",
      "Epoch 6127: train loss: 2.3453952962881885e-05, val loss: 0.1059897169470787\n",
      "Epoch 6128: train loss: 4.898615225101821e-05, val loss: 0.10703092068433762\n",
      "Epoch 6129: train loss: 1.5858950064284727e-05, val loss: 0.10706658661365509\n",
      "Epoch 6130: train loss: 3.665870099212043e-05, val loss: 0.10614623874425888\n",
      "Epoch 6131: train loss: 1.598070412001107e-05, val loss: 0.10590098053216934\n",
      "Epoch 6132: train loss: 2.6019215511041693e-05, val loss: 0.10644371807575226\n",
      "Epoch 6133: train loss: 1.8474169337423518e-05, val loss: 0.10618150234222412\n",
      "Epoch 6134: train loss: 1.500586404290516e-05, val loss: 0.10600290447473526\n",
      "Epoch 6135: train loss: 1.960279405466281e-05, val loss: 0.1073755994439125\n",
      "Epoch 6136: train loss: 7.722031114099082e-06, val loss: 0.10789672285318375\n",
      "Epoch 6137: train loss: 1.9254393919254653e-05, val loss: 0.10686469078063965\n",
      "Epoch 6138: train loss: 4.927635472995462e-06, val loss: 0.10714467614889145\n",
      "Epoch 6139: train loss: 1.5504607290495187e-05, val loss: 0.10829144716262817\n",
      "Epoch 6140: train loss: 4.872435056313407e-06, val loss: 0.10782501846551895\n",
      "Epoch 6141: train loss: 1.144921679951949e-05, val loss: 0.10746707767248154\n",
      "Epoch 6142: train loss: 5.421357400337001e-06, val loss: 0.1085544005036354\n",
      "Epoch 6143: train loss: 7.345257472479716e-06, val loss: 0.10879049450159073\n",
      "Epoch 6144: train loss: 6.732131168973865e-06, val loss: 0.10797014087438583\n",
      "Epoch 6145: train loss: 3.5190146263630595e-06, val loss: 0.10762593895196915\n",
      "Epoch 6146: train loss: 7.540359092672588e-06, val loss: 0.10798831284046173\n",
      "Epoch 6147: train loss: 1.6953848671619198e-06, val loss: 0.10807156562805176\n",
      "Epoch 6148: train loss: 6.505494184239069e-06, val loss: 0.10772370547056198\n",
      "Epoch 6149: train loss: 1.8751180732579087e-06, val loss: 0.10757429897785187\n",
      "Epoch 6150: train loss: 4.388726665638387e-06, val loss: 0.10800688713788986\n",
      "Epoch 6151: train loss: 2.528436652937671e-06, val loss: 0.1082218810915947\n",
      "Epoch 6152: train loss: 2.6854906991502503e-06, val loss: 0.10802783071994781\n",
      "Epoch 6153: train loss: 2.9251018531795125e-06, val loss: 0.10833344608545303\n",
      "Epoch 6154: train loss: 1.6687380366420257e-06, val loss: 0.1086442843079567\n",
      "Epoch 6155: train loss: 2.5595441002224106e-06, val loss: 0.10837545245885849\n",
      "Epoch 6156: train loss: 1.5653049558750354e-06, val loss: 0.10839402675628662\n",
      "Epoch 6157: train loss: 1.8016642115981085e-06, val loss: 0.10863866657018661\n",
      "Epoch 6158: train loss: 1.5401573136841762e-06, val loss: 0.10860111564397812\n",
      "Epoch 6159: train loss: 1.4045622265257407e-06, val loss: 0.10892591625452042\n",
      "Epoch 6160: train loss: 1.1356913773852284e-06, val loss: 0.10932710021734238\n",
      "Epoch 6161: train loss: 1.4217664556781529e-06, val loss: 0.10905207693576813\n",
      "Epoch 6162: train loss: 8.510975817443978e-07, val loss: 0.10899686813354492\n",
      "Epoch 6163: train loss: 1.002734848043474e-06, val loss: 0.10940130054950714\n",
      "Epoch 6164: train loss: 1.0580049547570525e-06, val loss: 0.10947250574827194\n",
      "Epoch 6165: train loss: 5.359503916224639e-07, val loss: 0.109555184841156\n",
      "Epoch 6166: train loss: 9.693245601738454e-07, val loss: 0.10980715602636337\n",
      "Epoch 6167: train loss: 6.032762485119747e-07, val loss: 0.109841488301754\n",
      "Epoch 6168: train loss: 5.716681812373281e-07, val loss: 0.10997281223535538\n",
      "Epoch 6169: train loss: 6.651754915765196e-07, val loss: 0.11024357378482819\n",
      "Epoch 6170: train loss: 4.963586661688169e-07, val loss: 0.11032791435718536\n",
      "Epoch 6171: train loss: 4.3953647832495335e-07, val loss: 0.11040730774402618\n",
      "Epoch 6172: train loss: 4.5148576077735925e-07, val loss: 0.11047754436731339\n",
      "Epoch 6173: train loss: 4.6492635874528787e-07, val loss: 0.11057865619659424\n",
      "Epoch 6174: train loss: 3.0013833907105436e-07, val loss: 0.11091925203800201\n",
      "Epoch 6175: train loss: 3.2510098435523105e-07, val loss: 0.11108820885419846\n",
      "Epoch 6176: train loss: 3.631098763889895e-07, val loss: 0.11114511638879776\n",
      "Epoch 6177: train loss: 2.6220004656352103e-07, val loss: 0.11145651340484619\n",
      "Epoch 6178: train loss: 2.4422010369562486e-07, val loss: 0.11155705899000168\n",
      "Epoch 6179: train loss: 2.474009761499474e-07, val loss: 0.11160681396722794\n",
      "Epoch 6180: train loss: 2.3879144350757997e-07, val loss: 0.11182387173175812\n",
      "Epoch 6181: train loss: 2.0888163021481887e-07, val loss: 0.11177816241979599\n",
      "Epoch 6182: train loss: 1.7267602459014597e-07, val loss: 0.11188448965549469\n",
      "Epoch 6183: train loss: 1.444917074877594e-07, val loss: 0.11219301074743271\n",
      "Epoch 6184: train loss: 1.8611322616379766e-07, val loss: 0.11225803196430206\n",
      "Epoch 6185: train loss: 2.071632962952208e-07, val loss: 0.11241792887449265\n",
      "Epoch 6186: train loss: 1.1951179601510376e-07, val loss: 0.11246500164270401\n",
      "Epoch 6187: train loss: 9.618478458151003e-08, val loss: 0.112551748752594\n",
      "Epoch 6188: train loss: 1.7896618942359055e-07, val loss: 0.11279801279306412\n",
      "Epoch 6189: train loss: 2.369851443972948e-07, val loss: 0.11283927410840988\n",
      "Epoch 6190: train loss: 2.8388828354763973e-07, val loss: 0.11294801533222198\n",
      "Epoch 6191: train loss: 5.029738758821622e-07, val loss: 0.11317291110754013\n",
      "Epoch 6192: train loss: 1.2569031468956382e-06, val loss: 0.1130332499742508\n",
      "Epoch 6193: train loss: 3.5922635106544476e-06, val loss: 0.1136036068201065\n",
      "Epoch 6194: train loss: 1.2120804058213253e-05, val loss: 0.11299633979797363\n",
      "Epoch 6195: train loss: 3.9196798752527684e-05, val loss: 0.11402261257171631\n",
      "Epoch 6196: train loss: 0.00010265741002513096, val loss: 0.11202283203601837\n",
      "Epoch 6197: train loss: 0.00022956389875616878, val loss: 0.11424177139997482\n",
      "Epoch 6198: train loss: 0.000343357736710459, val loss: 0.11027085781097412\n",
      "Epoch 6199: train loss: 0.0003831539361272007, val loss: 0.11369142681360245\n",
      "Epoch 6200: train loss: 0.00020467246940825135, val loss: 0.11427073925733566\n",
      "Epoch 6201: train loss: 0.00014987394388299435, val loss: 0.11201949417591095\n",
      "Epoch 6202: train loss: 0.00010841988842003047, val loss: 0.11525025218725204\n",
      "Epoch 6203: train loss: 5.867877553100698e-05, val loss: 0.11336296051740646\n",
      "Epoch 6204: train loss: 0.00010919699707301334, val loss: 0.11299479007720947\n",
      "Epoch 6205: train loss: 0.00011834018368972465, val loss: 0.11432089656591415\n",
      "Epoch 6206: train loss: 4.923714732285589e-05, val loss: 0.11426099389791489\n",
      "Epoch 6207: train loss: 4.7415629524039105e-05, val loss: 0.11365611851215363\n",
      "Epoch 6208: train loss: 6.912802928127348e-05, val loss: 0.1140792965888977\n",
      "Epoch 6209: train loss: 6.255874177441001e-05, val loss: 0.11431463062763214\n",
      "Epoch 6210: train loss: 3.4848279028665274e-05, val loss: 0.1159430518746376\n",
      "Epoch 6211: train loss: 4.39508403360378e-05, val loss: 0.11669006198644638\n",
      "Epoch 6212: train loss: 5.424923074315302e-05, val loss: 0.11394523829221725\n",
      "Epoch 6213: train loss: 4.103006358491257e-05, val loss: 0.11638615280389786\n",
      "Epoch 6214: train loss: 2.7336160201230086e-05, val loss: 0.11784561723470688\n",
      "Epoch 6215: train loss: 3.4325865271966904e-05, val loss: 0.11752229928970337\n",
      "Epoch 6216: train loss: 2.100288838846609e-05, val loss: 0.11708801239728928\n",
      "Epoch 6217: train loss: 1.5129630810406525e-05, val loss: 0.11827293783426285\n",
      "Epoch 6218: train loss: 2.0234450857969932e-05, val loss: 0.11977795511484146\n",
      "Epoch 6219: train loss: 1.9690358385560103e-05, val loss: 0.11875780671834946\n",
      "Epoch 6220: train loss: 1.6962530935415998e-05, val loss: 0.11915311962366104\n",
      "Epoch 6221: train loss: 1.5441009963979013e-05, val loss: 0.11941583454608917\n",
      "Epoch 6222: train loss: 1.3778003449260723e-05, val loss: 0.11962034553289413\n",
      "Epoch 6223: train loss: 8.311412784678396e-06, val loss: 0.11975276470184326\n",
      "Epoch 6224: train loss: 8.353742487088311e-06, val loss: 0.11999007314443588\n",
      "Epoch 6225: train loss: 1.147747207141947e-05, val loss: 0.12048480659723282\n",
      "Epoch 6226: train loss: 6.790240604459541e-06, val loss: 0.11975204944610596\n",
      "Epoch 6227: train loss: 1.1170738616783638e-05, val loss: 0.12082754820585251\n",
      "Epoch 6228: train loss: 7.479969099222217e-06, val loss: 0.12086384743452072\n",
      "Epoch 6229: train loss: 5.889530712011037e-06, val loss: 0.12087716907262802\n",
      "Epoch 6230: train loss: 5.876458544662455e-06, val loss: 0.12112980335950851\n",
      "Epoch 6231: train loss: 4.513844942266587e-06, val loss: 0.12145960330963135\n",
      "Epoch 6232: train loss: 3.423705493332818e-06, val loss: 0.12132181227207184\n",
      "Epoch 6233: train loss: 4.000037733931094e-06, val loss: 0.1216687560081482\n",
      "Epoch 6234: train loss: 4.956914835929638e-06, val loss: 0.12245789915323257\n",
      "Epoch 6235: train loss: 3.7461811643879628e-06, val loss: 0.12139176577329636\n",
      "Epoch 6236: train loss: 4.624399480235297e-06, val loss: 0.12289508432149887\n",
      "Epoch 6237: train loss: 5.395713287725812e-06, val loss: 0.12250292301177979\n",
      "Epoch 6238: train loss: 6.150604349386413e-06, val loss: 0.12320101261138916\n",
      "Epoch 6239: train loss: 7.206645477708662e-06, val loss: 0.12239386886358261\n",
      "Epoch 6240: train loss: 1.0854761057998985e-05, val loss: 0.12429489940404892\n",
      "Epoch 6241: train loss: 1.5600300685036927e-05, val loss: 0.12235325574874878\n",
      "Epoch 6242: train loss: 2.5954343072953634e-05, val loss: 0.12556542456150055\n",
      "Epoch 6243: train loss: 4.8161102313315496e-05, val loss: 0.12150519341230392\n",
      "Epoch 6244: train loss: 9.895760013023391e-05, val loss: 0.12735168635845184\n",
      "Epoch 6245: train loss: 0.00020233599934726954, val loss: 0.11992993205785751\n",
      "Epoch 6246: train loss: 0.0004505155375227332, val loss: 0.1327311396598816\n",
      "Epoch 6247: train loss: 0.0007480976055376232, val loss: 0.12183328717947006\n",
      "Epoch 6248: train loss: 0.0012587617384269834, val loss: 0.12930546700954437\n",
      "Epoch 6249: train loss: 0.0009230433497577906, val loss: 0.12338515371084213\n",
      "Epoch 6250: train loss: 0.0001478133926866576, val loss: 0.12090950459241867\n",
      "Epoch 6251: train loss: 0.00019691404304467142, val loss: 0.12308436632156372\n",
      "Epoch 6252: train loss: 0.000686728279106319, val loss: 0.11134016513824463\n",
      "Epoch 6253: train loss: 0.0009319812525063753, val loss: 0.11858239024877548\n",
      "Epoch 6254: train loss: 0.00022070705017540604, val loss: 0.12478189915418625\n",
      "Epoch 6255: train loss: 0.0005437208455987275, val loss: 0.12204195559024811\n",
      "Epoch 6256: train loss: 0.00015214089944493026, val loss: 0.11763620376586914\n",
      "Epoch 6257: train loss: 0.0003542852064128965, val loss: 0.11417236179113388\n",
      "Epoch 6258: train loss: 0.00015545220230706036, val loss: 0.11651188135147095\n",
      "Epoch 6259: train loss: 0.0002157126145903021, val loss: 0.11770375072956085\n",
      "Epoch 6260: train loss: 0.00014622211165260524, val loss: 0.11606060713529587\n",
      "Epoch 6261: train loss: 0.00015162522322498262, val loss: 0.11542613804340363\n",
      "Epoch 6262: train loss: 0.00010676665988285094, val loss: 0.11529681831598282\n",
      "Epoch 6263: train loss: 0.0001259175332961604, val loss: 0.11414980888366699\n",
      "Epoch 6264: train loss: 7.872618152759969e-05, val loss: 0.11363305151462555\n",
      "Epoch 6265: train loss: 8.41130968183279e-05, val loss: 0.11373899132013321\n",
      "Epoch 6266: train loss: 8.097029058262706e-05, val loss: 0.11568257957696915\n",
      "Epoch 6267: train loss: 5.1939787226729095e-05, val loss: 0.11684782803058624\n",
      "Epoch 6268: train loss: 6.582667992915958e-05, val loss: 0.11517279595136642\n",
      "Epoch 6269: train loss: 4.781946336152032e-05, val loss: 0.11517371237277985\n",
      "Epoch 6270: train loss: 4.3271080357953906e-05, val loss: 0.11697907745838165\n",
      "Epoch 6271: train loss: 4.2529347410891205e-05, val loss: 0.1168266087770462\n",
      "Epoch 6272: train loss: 3.663215829874389e-05, val loss: 0.11642750352621078\n",
      "Epoch 6273: train loss: 3.2058207580121234e-05, val loss: 0.11839371919631958\n",
      "Epoch 6274: train loss: 2.5622814064263366e-05, val loss: 0.11966951191425323\n",
      "Epoch 6275: train loss: 3.2527404982829466e-05, val loss: 0.11903456598520279\n",
      "Epoch 6276: train loss: 1.781144419510383e-05, val loss: 0.11855244636535645\n",
      "Epoch 6277: train loss: 2.3571417841594666e-05, val loss: 0.1190977618098259\n",
      "Epoch 6278: train loss: 1.9753186279558577e-05, val loss: 0.11996197700500488\n",
      "Epoch 6279: train loss: 1.7298818420385942e-05, val loss: 0.11996255069971085\n",
      "Epoch 6280: train loss: 1.4499039934889879e-05, val loss: 0.11997800320386887\n",
      "Epoch 6281: train loss: 1.8067679775413126e-05, val loss: 0.12122893333435059\n",
      "Epoch 6282: train loss: 9.693570973468013e-06, val loss: 0.12176375836133957\n",
      "Epoch 6283: train loss: 1.496846743975766e-05, val loss: 0.12127885967493057\n",
      "Epoch 6284: train loss: 1.168524340755539e-05, val loss: 0.12195863574743271\n",
      "Epoch 6285: train loss: 7.972550520207733e-06, val loss: 0.12232927232980728\n",
      "Epoch 6286: train loss: 1.1141163668071385e-05, val loss: 0.12191908806562424\n",
      "Epoch 6287: train loss: 5.619720923277782e-06, val loss: 0.12234659492969513\n",
      "Epoch 6288: train loss: 8.091315066849347e-06, val loss: 0.12281402200460434\n",
      "Epoch 6289: train loss: 4.555908617476234e-06, val loss: 0.12247685343027115\n",
      "Epoch 6290: train loss: 6.771098014723975e-06, val loss: 0.12249214947223663\n",
      "Epoch 6291: train loss: 3.367089220773778e-06, val loss: 0.12311599403619766\n",
      "Epoch 6292: train loss: 5.094173502584454e-06, val loss: 0.1235426664352417\n",
      "Epoch 6293: train loss: 3.57122189598158e-06, val loss: 0.12354012578725815\n",
      "Epoch 6294: train loss: 3.226155058655422e-06, val loss: 0.12338592857122421\n",
      "Epoch 6295: train loss: 3.3303015243291156e-06, val loss: 0.12361625581979752\n",
      "Epoch 6296: train loss: 2.7367232178221457e-06, val loss: 0.12369177490472794\n",
      "Epoch 6297: train loss: 2.4019016109377844e-06, val loss: 0.12334077805280685\n",
      "Epoch 6298: train loss: 2.697780246307957e-06, val loss: 0.1236410140991211\n",
      "Epoch 6299: train loss: 1.5280292018360342e-06, val loss: 0.12422765791416168\n",
      "Epoch 6300: train loss: 2.3628924736840418e-06, val loss: 0.12404058128595352\n",
      "Epoch 6301: train loss: 1.4693118828290608e-06, val loss: 0.1235133558511734\n",
      "Epoch 6302: train loss: 1.4906344176779385e-06, val loss: 0.12354165315628052\n",
      "Epoch 6303: train loss: 1.6577380392845953e-06, val loss: 0.12379316240549088\n",
      "Epoch 6304: train loss: 1.0507391152714263e-06, val loss: 0.12417683750391006\n",
      "Epoch 6305: train loss: 1.3257230193630676e-06, val loss: 0.1244320422410965\n",
      "Epoch 6306: train loss: 9.6975145424949e-07, val loss: 0.1243835836648941\n",
      "Epoch 6307: train loss: 9.648523473515525e-07, val loss: 0.12444950640201569\n",
      "Epoch 6308: train loss: 9.539639904687647e-07, val loss: 0.12464047968387604\n",
      "Epoch 6309: train loss: 7.008558213783544e-07, val loss: 0.12476348876953125\n",
      "Epoch 6310: train loss: 7.582060561617254e-07, val loss: 0.12493900209665298\n",
      "Epoch 6311: train loss: 7.775132075948932e-07, val loss: 0.1250152885913849\n",
      "Epoch 6312: train loss: 5.506622642315051e-07, val loss: 0.12492288649082184\n",
      "Epoch 6313: train loss: 1.0603783948681667e-06, val loss: 0.12530314922332764\n",
      "Epoch 6314: train loss: 4.5823385335097555e-06, val loss: 0.12521123886108398\n",
      "Epoch 6315: train loss: 9.995790605898947e-06, val loss: 0.12556831538677216\n",
      "Epoch 6316: train loss: 1.1003610325133195e-06, val loss: 0.12529318034648895\n",
      "Epoch 6317: train loss: 6.802360985602718e-06, val loss: 0.12492527067661285\n",
      "Epoch 6318: train loss: 2.6360601168562425e-06, val loss: 0.12526236474514008\n",
      "Epoch 6319: train loss: 4.265648385626264e-06, val loss: 0.1257435530424118\n",
      "Epoch 6320: train loss: 2.7313874397805193e-06, val loss: 0.125650554895401\n",
      "Epoch 6321: train loss: 3.7008844628871884e-06, val loss: 0.12536047399044037\n",
      "Epoch 6322: train loss: 1.8617834030010272e-06, val loss: 0.1254734843969345\n",
      "Epoch 6323: train loss: 3.2638567972753663e-06, val loss: 0.12590403854846954\n",
      "Epoch 6324: train loss: 1.6073599908850156e-06, val loss: 0.1261558085680008\n",
      "Epoch 6325: train loss: 2.3562377009511692e-06, val loss: 0.12605993449687958\n",
      "Epoch 6326: train loss: 1.6596711702732136e-06, val loss: 0.12609390914440155\n",
      "Epoch 6327: train loss: 1.9439826246525627e-06, val loss: 0.12624607980251312\n",
      "Epoch 6328: train loss: 1.0157169754165807e-06, val loss: 0.12638694047927856\n",
      "Epoch 6329: train loss: 1.9657552456919802e-06, val loss: 0.12654593586921692\n",
      "Epoch 6330: train loss: 9.835954415393644e-07, val loss: 0.12661810219287872\n",
      "Epoch 6331: train loss: 9.971033705369337e-07, val loss: 0.1267451047897339\n",
      "Epoch 6332: train loss: 1.5070138488226803e-06, val loss: 0.126906618475914\n",
      "Epoch 6333: train loss: 6.160660745990754e-07, val loss: 0.12690161168575287\n",
      "Epoch 6334: train loss: 1.0441382300996338e-06, val loss: 0.12705910205841064\n",
      "Epoch 6335: train loss: 7.291403676390473e-07, val loss: 0.12726043164730072\n",
      "Epoch 6336: train loss: 8.816116405796492e-07, val loss: 0.12726245820522308\n",
      "Epoch 6337: train loss: 7.633558425368392e-07, val loss: 0.1275155246257782\n",
      "Epoch 6338: train loss: 4.6654309926452697e-07, val loss: 0.12780439853668213\n",
      "Epoch 6339: train loss: 7.757684556963795e-07, val loss: 0.127830371260643\n",
      "Epoch 6340: train loss: 7.174545544330613e-07, val loss: 0.12803520262241364\n",
      "Epoch 6341: train loss: 5.603335466730641e-07, val loss: 0.12832902371883392\n",
      "Epoch 6342: train loss: 7.41376823043538e-07, val loss: 0.12834064662456512\n",
      "Epoch 6343: train loss: 1.019600517793151e-06, val loss: 0.1282837986946106\n",
      "Epoch 6344: train loss: 2.9368529794737697e-06, val loss: 0.12879478931427002\n",
      "Epoch 6345: train loss: 1.1989705853920896e-05, val loss: 0.12803229689598083\n",
      "Epoch 6346: train loss: 2.9321678084670566e-05, val loss: 0.1284361034631729\n",
      "Epoch 6347: train loss: 4.679067569668405e-05, val loss: 0.12807703018188477\n",
      "Epoch 6348: train loss: 5.5780266848159954e-05, val loss: 0.12815028429031372\n",
      "Epoch 6349: train loss: 5.264322680886835e-05, val loss: 0.12971115112304688\n",
      "Epoch 6350: train loss: 4.509525024332106e-05, val loss: 0.12713010609149933\n",
      "Epoch 6351: train loss: 3.868440762744285e-05, val loss: 0.13010218739509583\n",
      "Epoch 6352: train loss: 3.098365414189175e-05, val loss: 0.1271752417087555\n",
      "Epoch 6353: train loss: 2.7541273084352724e-05, val loss: 0.12920653820037842\n",
      "Epoch 6354: train loss: 2.665796819201205e-05, val loss: 0.12730558216571808\n",
      "Epoch 6355: train loss: 3.240935257053934e-05, val loss: 0.1272500604391098\n",
      "Epoch 6356: train loss: 4.23946839873679e-05, val loss: 0.12765765190124512\n",
      "Epoch 6357: train loss: 5.672586121363565e-05, val loss: 0.12596754729747772\n",
      "Epoch 6358: train loss: 4.4260094000492245e-05, val loss: 0.12776164710521698\n",
      "Epoch 6359: train loss: 1.78321006387705e-05, val loss: 0.1262521594762802\n",
      "Epoch 6360: train loss: 1.6126381524372846e-05, val loss: 0.1271533966064453\n",
      "Epoch 6361: train loss: 2.6232015443383716e-05, val loss: 0.12699949741363525\n",
      "Epoch 6362: train loss: 2.87917246168945e-05, val loss: 0.12601128220558167\n",
      "Epoch 6363: train loss: 2.1240643036435358e-05, val loss: 0.1278037130832672\n",
      "Epoch 6364: train loss: 1.682162292127032e-05, val loss: 0.12611384689807892\n",
      "Epoch 6365: train loss: 1.7131364074884914e-05, val loss: 0.1274542361497879\n",
      "Epoch 6366: train loss: 1.4313351130113006e-05, val loss: 0.12711454927921295\n",
      "Epoch 6367: train loss: 1.1117539543192834e-05, val loss: 0.12672531604766846\n",
      "Epoch 6368: train loss: 1.255393090104917e-05, val loss: 0.1274978369474411\n",
      "Epoch 6369: train loss: 1.4356925021274947e-05, val loss: 0.1269182562828064\n",
      "Epoch 6370: train loss: 1.1810067007900216e-05, val loss: 0.12762928009033203\n",
      "Epoch 6371: train loss: 8.868395525496453e-06, val loss: 0.12723571062088013\n",
      "Epoch 6372: train loss: 1.076941771316342e-05, val loss: 0.1280946284532547\n",
      "Epoch 6373: train loss: 1.5264286048477516e-05, val loss: 0.12655818462371826\n",
      "Epoch 6374: train loss: 1.9780225557042286e-05, val loss: 0.12875385582447052\n",
      "Epoch 6375: train loss: 2.830928315233905e-05, val loss: 0.12649647891521454\n",
      "Epoch 6376: train loss: 5.573471207753755e-05, val loss: 0.13001678884029388\n",
      "Epoch 6377: train loss: 0.00012019674613839015, val loss: 0.12475746870040894\n",
      "Epoch 6378: train loss: 0.00027195800794288516, val loss: 0.1320405900478363\n",
      "Epoch 6379: train loss: 0.0005440232926048338, val loss: 0.12149333208799362\n",
      "Epoch 6380: train loss: 0.0009932916145771742, val loss: 0.13253238797187805\n",
      "Epoch 6381: train loss: 0.0008992481743916869, val loss: 0.12447251379489899\n",
      "Epoch 6382: train loss: 0.000415061047533527, val loss: 0.12333589047193527\n",
      "Epoch 6383: train loss: 0.00010449760156916454, val loss: 0.12831677496433258\n",
      "Epoch 6384: train loss: 0.00034750415943562984, val loss: 0.12100302428007126\n",
      "Epoch 6385: train loss: 0.00022989085118751973, val loss: 0.11923747509717941\n",
      "Epoch 6386: train loss: 0.00012318772496655583, val loss: 0.12462818622589111\n",
      "Epoch 6387: train loss: 0.00018178783648181707, val loss: 0.12136509269475937\n",
      "Epoch 6388: train loss: 0.00013093808956909925, val loss: 0.1175500899553299\n",
      "Epoch 6389: train loss: 9.242557280231267e-05, val loss: 0.12023840099573135\n",
      "Epoch 6390: train loss: 0.0001092078018700704, val loss: 0.12188892811536789\n",
      "Epoch 6391: train loss: 6.740814569639042e-05, val loss: 0.11949781328439713\n",
      "Epoch 6392: train loss: 7.984906551428139e-05, val loss: 0.118259958922863\n",
      "Epoch 6393: train loss: 5.381499795475975e-05, val loss: 0.11995475739240646\n",
      "Epoch 6394: train loss: 6.227357516763732e-05, val loss: 0.12005453556776047\n",
      "Epoch 6395: train loss: 5.027257793699391e-05, val loss: 0.1185418963432312\n",
      "Epoch 6396: train loss: 4.6478915464831516e-05, val loss: 0.1196625754237175\n",
      "Epoch 6397: train loss: 3.973867205786519e-05, val loss: 0.12077245861291885\n",
      "Epoch 6398: train loss: 3.1969804695108905e-05, val loss: 0.11910011619329453\n",
      "Epoch 6399: train loss: 3.135848601232283e-05, val loss: 0.11940743774175644\n",
      "Epoch 6400: train loss: 2.820189911290072e-05, val loss: 0.12080961465835571\n",
      "Epoch 6401: train loss: 2.437163311697077e-05, val loss: 0.11960947513580322\n",
      "Epoch 6402: train loss: 2.3272266844287515e-05, val loss: 0.11927396059036255\n",
      "Epoch 6403: train loss: 1.7733111235429533e-05, val loss: 0.12018783390522003\n",
      "Epoch 6404: train loss: 1.9069047993980348e-05, val loss: 0.11988856643438339\n",
      "Epoch 6405: train loss: 1.560373857500963e-05, val loss: 0.11895895004272461\n",
      "Epoch 6406: train loss: 1.4868046491756104e-05, val loss: 0.11894694715738297\n",
      "Epoch 6407: train loss: 1.089654688257724e-05, val loss: 0.12072636187076569\n",
      "Epoch 6408: train loss: 1.2287269782973453e-05, val loss: 0.12095589935779572\n",
      "Epoch 6409: train loss: 9.050781955011189e-06, val loss: 0.11948206275701523\n",
      "Epoch 6410: train loss: 1.0621010005706921e-05, val loss: 0.11960043758153915\n",
      "Epoch 6411: train loss: 6.118894361861749e-06, val loss: 0.12025719881057739\n",
      "Epoch 6412: train loss: 8.871514182828832e-06, val loss: 0.11978518962860107\n",
      "Epoch 6413: train loss: 5.4279494179354515e-06, val loss: 0.11965014785528183\n",
      "Epoch 6414: train loss: 7.122879196685972e-06, val loss: 0.12003034353256226\n",
      "Epoch 6415: train loss: 4.349285973148653e-06, val loss: 0.12031157314777374\n",
      "Epoch 6416: train loss: 5.3728240345662925e-06, val loss: 0.1202237606048584\n",
      "Epoch 6417: train loss: 4.359907961770659e-06, val loss: 0.11997520923614502\n",
      "Epoch 6418: train loss: 4.1346243051521014e-06, val loss: 0.12029659748077393\n",
      "Epoch 6419: train loss: 3.4133468034269754e-06, val loss: 0.12048270553350449\n",
      "Epoch 6420: train loss: 3.3816918403317686e-06, val loss: 0.12021669000387192\n",
      "Epoch 6421: train loss: 2.747666940194904e-06, val loss: 0.12023309618234634\n",
      "Epoch 6422: train loss: 3.1148563266469864e-06, val loss: 0.12035975605249405\n",
      "Epoch 6423: train loss: 2.1962807750242064e-06, val loss: 0.1206294521689415\n",
      "Epoch 6424: train loss: 2.4152118385245558e-06, val loss: 0.12073061615228653\n",
      "Epoch 6425: train loss: 1.9621604678832227e-06, val loss: 0.12038405239582062\n",
      "Epoch 6426: train loss: 1.6692515600880142e-06, val loss: 0.12047594785690308\n",
      "Epoch 6427: train loss: 1.898708887893008e-06, val loss: 0.12093200534582138\n",
      "Epoch 6428: train loss: 1.4042996099306038e-06, val loss: 0.1208459660410881\n",
      "Epoch 6429: train loss: 1.711415393401694e-06, val loss: 0.12063782662153244\n",
      "Epoch 6430: train loss: 1.3665537608176237e-06, val loss: 0.12094338983297348\n",
      "Epoch 6431: train loss: 1.4101931355980923e-06, val loss: 0.12107397615909576\n",
      "Epoch 6432: train loss: 1.349169679087936e-06, val loss: 0.12090511620044708\n",
      "Epoch 6433: train loss: 1.337532012257725e-06, val loss: 0.12103714048862457\n",
      "Epoch 6434: train loss: 1.3692573475054814e-06, val loss: 0.12138014286756516\n",
      "Epoch 6435: train loss: 2.0050433704454917e-06, val loss: 0.1211826428771019\n",
      "Epoch 6436: train loss: 2.663780833245255e-06, val loss: 0.12127693742513657\n",
      "Epoch 6437: train loss: 5.797786343464395e-06, val loss: 0.12167767435312271\n",
      "Epoch 6438: train loss: 1.3206070434534922e-05, val loss: 0.12166843563318253\n",
      "Epoch 6439: train loss: 3.602349534048699e-05, val loss: 0.12124484032392502\n",
      "Epoch 6440: train loss: 0.00010464847582625225, val loss: 0.12184774875640869\n",
      "Epoch 6441: train loss: 0.0003071528917644173, val loss: 0.12109135836362839\n",
      "Epoch 6442: train loss: 0.0007779665756970644, val loss: 0.11972720921039581\n",
      "Epoch 6443: train loss: 0.0013802861794829369, val loss: 0.1182694211602211\n",
      "Epoch 6444: train loss: 0.0004603612469509244, val loss: 0.12103444337844849\n",
      "Epoch 6445: train loss: 0.00041310093365609646, val loss: 0.12042617797851562\n",
      "Epoch 6446: train loss: 0.00021913900854997337, val loss: 0.11659121513366699\n",
      "Epoch 6447: train loss: 0.0002847414289135486, val loss: 0.11863477528095245\n",
      "Epoch 6448: train loss: 0.0001497682387707755, val loss: 0.11625534296035767\n",
      "Epoch 6449: train loss: 0.0001783625193638727, val loss: 0.11554200947284698\n",
      "Epoch 6450: train loss: 0.0001647901808610186, val loss: 0.1166779026389122\n",
      "Epoch 6451: train loss: 8.607137715443969e-05, val loss: 0.11442273110151291\n",
      "Epoch 6452: train loss: 0.0001158358936663717, val loss: 0.11348579078912735\n",
      "Epoch 6453: train loss: 0.00010844608186744153, val loss: 0.11305544525384903\n",
      "Epoch 6454: train loss: 7.688217738177627e-05, val loss: 0.11417417973279953\n",
      "Epoch 6455: train loss: 5.2171817515045404e-05, val loss: 0.11482341587543488\n",
      "Epoch 6456: train loss: 6.229113205336034e-05, val loss: 0.11113772541284561\n",
      "Epoch 6457: train loss: 4.5424669224303216e-05, val loss: 0.11225493252277374\n",
      "Epoch 6458: train loss: 3.208740599802695e-05, val loss: 0.11682329326868057\n",
      "Epoch 6459: train loss: 4.5906392188044265e-05, val loss: 0.11758430302143097\n",
      "Epoch 6460: train loss: 4.048519622301683e-05, val loss: 0.11721628904342651\n",
      "Epoch 6461: train loss: 2.6315710783819668e-05, val loss: 0.11922190338373184\n",
      "Epoch 6462: train loss: 2.4036233298829757e-05, val loss: 0.1180141493678093\n",
      "Epoch 6463: train loss: 2.676688200153876e-05, val loss: 0.11775598675012589\n",
      "Epoch 6464: train loss: 2.1007292161812074e-05, val loss: 0.11681243032217026\n",
      "Epoch 6465: train loss: 1.3362913705350365e-05, val loss: 0.11714942753314972\n",
      "Epoch 6466: train loss: 1.8867491235141642e-05, val loss: 0.11866219341754913\n",
      "Epoch 6467: train loss: 1.8213830117019825e-05, val loss: 0.11681807041168213\n",
      "Epoch 6468: train loss: 8.959107617556583e-06, val loss: 0.11674294620752335\n",
      "Epoch 6469: train loss: 1.2464966857805848e-05, val loss: 0.11812766641378403\n",
      "Epoch 6470: train loss: 1.1756434105336666e-05, val loss: 0.1172303706407547\n",
      "Epoch 6471: train loss: 1.1036236173822545e-05, val loss: 0.11770522594451904\n",
      "Epoch 6472: train loss: 7.189909410953987e-06, val loss: 0.11737308651208878\n",
      "Epoch 6473: train loss: 7.458370419044513e-06, val loss: 0.11659961193799973\n",
      "Epoch 6474: train loss: 8.847758181218524e-06, val loss: 0.11775638908147812\n",
      "Epoch 6475: train loss: 5.3283692977856845e-06, val loss: 0.11705417931079865\n",
      "Epoch 6476: train loss: 6.882979050715221e-06, val loss: 0.11619625240564346\n",
      "Epoch 6477: train loss: 4.445795639185235e-06, val loss: 0.11736481636762619\n",
      "Epoch 6478: train loss: 4.871663350058952e-06, val loss: 0.11811194568872452\n",
      "Epoch 6479: train loss: 5.202095962886233e-06, val loss: 0.11860308796167374\n",
      "Epoch 6480: train loss: 3.1254248824552633e-06, val loss: 0.11786996573209763\n",
      "Epoch 6481: train loss: 4.8910842451732606e-06, val loss: 0.11802852153778076\n",
      "Epoch 6482: train loss: 2.083570507238619e-06, val loss: 0.11892075836658478\n",
      "Epoch 6483: train loss: 3.7080105812492548e-06, val loss: 0.11820521205663681\n",
      "Epoch 6484: train loss: 2.841787363649928e-06, val loss: 0.11870681494474411\n",
      "Epoch 6485: train loss: 2.55281088357151e-06, val loss: 0.11908803135156631\n",
      "Epoch 6486: train loss: 2.7881205824087374e-06, val loss: 0.11925353854894638\n",
      "Epoch 6487: train loss: 2.7417233923188178e-06, val loss: 0.11949143558740616\n",
      "Epoch 6488: train loss: 2.5167073545162566e-06, val loss: 0.11952850967645645\n",
      "Epoch 6489: train loss: 4.481011274037883e-06, val loss: 0.11954312771558762\n",
      "Epoch 6490: train loss: 1.0373061741120182e-05, val loss: 0.11909075081348419\n",
      "Epoch 6491: train loss: 2.2919919501873665e-05, val loss: 0.11887925863265991\n",
      "Epoch 6492: train loss: 3.6368925066199154e-05, val loss: 0.1191776767373085\n",
      "Epoch 6493: train loss: 4.986169733456336e-05, val loss: 0.1191539317369461\n",
      "Epoch 6494: train loss: 7.691275095567107e-05, val loss: 0.11771712452173233\n",
      "Epoch 6495: train loss: 0.00015709242143202573, val loss: 0.12106815725564957\n",
      "Epoch 6496: train loss: 0.0003728735900949687, val loss: 0.12006952613592148\n",
      "Epoch 6497: train loss: 0.0007853913703002036, val loss: 0.12140381336212158\n",
      "Epoch 6498: train loss: 0.001309530925936997, val loss: 0.12738639116287231\n",
      "Epoch 6499: train loss: 0.0009734255145303905, val loss: 0.12201764434576035\n",
      "Epoch 6500: train loss: 0.00018391870253253728, val loss: 0.12297876924276352\n",
      "Epoch 6501: train loss: 0.0003335306828375906, val loss: 0.122255839407444\n",
      "Epoch 6502: train loss: 0.000333731877617538, val loss: 0.11873359978199005\n",
      "Epoch 6503: train loss: 0.0001269523927476257, val loss: 0.11628478020429611\n",
      "Epoch 6504: train loss: 0.0002643056504894048, val loss: 0.1179211363196373\n",
      "Epoch 6505: train loss: 6.659864448010921e-05, val loss: 0.11965253204107285\n",
      "Epoch 6506: train loss: 0.00020202384621370584, val loss: 0.11638303846120834\n",
      "Epoch 6507: train loss: 6.540399772347882e-05, val loss: 0.11699662357568741\n",
      "Epoch 6508: train loss: 0.00014644366456195712, val loss: 0.11790759861469269\n",
      "Epoch 6509: train loss: 4.9618502089288086e-05, val loss: 0.11974077671766281\n",
      "Epoch 6510: train loss: 9.384800068801269e-05, val loss: 0.12045486271381378\n",
      "Epoch 6511: train loss: 6.640495121246204e-05, val loss: 0.11900349706411362\n",
      "Epoch 6512: train loss: 6.0183723689988256e-05, val loss: 0.11880874633789062\n",
      "Epoch 6513: train loss: 5.546929605770856e-05, val loss: 0.11967327445745468\n",
      "Epoch 6514: train loss: 3.840144199784845e-05, val loss: 0.12059850990772247\n",
      "Epoch 6515: train loss: 4.91426580992993e-05, val loss: 0.12102202326059341\n",
      "Epoch 6516: train loss: 3.9776965422788635e-05, val loss: 0.11829733848571777\n",
      "Epoch 6517: train loss: 2.5397748686373234e-05, val loss: 0.11803312599658966\n",
      "Epoch 6518: train loss: 3.4792527003446594e-05, val loss: 0.12115850299596786\n",
      "Epoch 6519: train loss: 2.1005491362302564e-05, val loss: 0.12161066383123398\n",
      "Epoch 6520: train loss: 2.9412882213364355e-05, val loss: 0.12021320313215256\n",
      "Epoch 6521: train loss: 1.642362622078508e-05, val loss: 0.11950864642858505\n",
      "Epoch 6522: train loss: 1.9482875359244645e-05, val loss: 0.11986415833234787\n",
      "Epoch 6523: train loss: 1.980186061700806e-05, val loss: 0.12098176777362823\n",
      "Epoch 6524: train loss: 1.3010196198592894e-05, val loss: 0.12089043110609055\n",
      "Epoch 6525: train loss: 1.8463706510374323e-05, val loss: 0.12078895419836044\n",
      "Epoch 6526: train loss: 1.5232965779432561e-05, val loss: 0.12010450661182404\n",
      "Epoch 6527: train loss: 1.170039740827633e-05, val loss: 0.11943109333515167\n",
      "Epoch 6528: train loss: 1.5711173546151258e-05, val loss: 0.12048864364624023\n",
      "Epoch 6529: train loss: 6.645929261139827e-06, val loss: 0.12098866701126099\n",
      "Epoch 6530: train loss: 1.1878283658006694e-05, val loss: 0.11934752762317657\n",
      "Epoch 6531: train loss: 7.482275577785913e-06, val loss: 0.11870171874761581\n",
      "Epoch 6532: train loss: 9.9440576377674e-06, val loss: 0.11963901668787003\n",
      "Epoch 6533: train loss: 5.4447759794129524e-06, val loss: 0.12055998295545578\n",
      "Epoch 6534: train loss: 7.173971425800119e-06, val loss: 0.12006213515996933\n",
      "Epoch 6535: train loss: 5.284989583742572e-06, val loss: 0.11896289885044098\n",
      "Epoch 6536: train loss: 6.327279152174015e-06, val loss: 0.11941961199045181\n",
      "Epoch 6537: train loss: 4.149801952735288e-06, val loss: 0.11966215819120407\n",
      "Epoch 6538: train loss: 4.1684888856252655e-06, val loss: 0.11919834464788437\n",
      "Epoch 6539: train loss: 4.04350112148677e-06, val loss: 0.11923161894083023\n",
      "Epoch 6540: train loss: 4.044541128678247e-06, val loss: 0.11929545551538467\n",
      "Epoch 6541: train loss: 2.8660217594733695e-06, val loss: 0.11938207596540451\n",
      "Epoch 6542: train loss: 2.8769470645784168e-06, val loss: 0.11931681632995605\n",
      "Epoch 6543: train loss: 2.7938092443946516e-06, val loss: 0.11906891316175461\n",
      "Epoch 6544: train loss: 2.3415714167640544e-06, val loss: 0.11928524821996689\n",
      "Epoch 6545: train loss: 2.8536701393022668e-06, val loss: 0.11922996491193771\n",
      "Epoch 6546: train loss: 9.622372090234421e-07, val loss: 0.1188318058848381\n",
      "Epoch 6547: train loss: 2.6628586056176573e-06, val loss: 0.1187564879655838\n",
      "Epoch 6548: train loss: 1.3640477618537261e-06, val loss: 0.11895933002233505\n",
      "Epoch 6549: train loss: 1.5811665434739552e-06, val loss: 0.11918795108795166\n",
      "Epoch 6550: train loss: 1.6201757944145356e-06, val loss: 0.11883746832609177\n",
      "Epoch 6551: train loss: 8.06414448106807e-07, val loss: 0.11859805881977081\n",
      "Epoch 6552: train loss: 1.5468456240341766e-06, val loss: 0.11890044063329697\n",
      "Epoch 6553: train loss: 1.0118362752109533e-06, val loss: 0.11878974735736847\n",
      "Epoch 6554: train loss: 1.063625290953496e-06, val loss: 0.11887337267398834\n",
      "Epoch 6555: train loss: 6.523494562316046e-07, val loss: 0.1188572570681572\n",
      "Epoch 6556: train loss: 9.283842246077256e-07, val loss: 0.11859185993671417\n",
      "Epoch 6557: train loss: 1.1230750942559098e-06, val loss: 0.11882457882165909\n",
      "Epoch 6558: train loss: 8.169419629666663e-07, val loss: 0.11875062435865402\n",
      "Epoch 6559: train loss: 6.062691113584151e-07, val loss: 0.11858668178319931\n",
      "Epoch 6560: train loss: 9.440847748010128e-07, val loss: 0.11893071234226227\n",
      "Epoch 6561: train loss: 1.1635924010988674e-06, val loss: 0.11892234534025192\n",
      "Epoch 6562: train loss: 7.806082180650264e-07, val loss: 0.1189735159277916\n",
      "Epoch 6563: train loss: 4.373622743969463e-07, val loss: 0.11887427419424057\n",
      "Epoch 6564: train loss: 3.518739219998679e-07, val loss: 0.1188511848449707\n",
      "Epoch 6565: train loss: 8.002216418390162e-07, val loss: 0.11916601657867432\n",
      "Epoch 6566: train loss: 9.966843208530918e-07, val loss: 0.11900202184915543\n",
      "Epoch 6567: train loss: 9.665437801231747e-07, val loss: 0.1188850998878479\n",
      "Epoch 6568: train loss: 9.97364963950531e-07, val loss: 0.11901283264160156\n",
      "Epoch 6569: train loss: 1.0331507382943528e-06, val loss: 0.11901302635669708\n",
      "Epoch 6570: train loss: 1.3580574886873364e-06, val loss: 0.11900299042463303\n",
      "Epoch 6571: train loss: 1.9020467334485147e-06, val loss: 0.11883101612329483\n",
      "Epoch 6572: train loss: 3.7044619602966122e-06, val loss: 0.11921405792236328\n",
      "Epoch 6573: train loss: 8.621706001576968e-06, val loss: 0.11918248981237411\n",
      "Epoch 6574: train loss: 2.384499202889856e-05, val loss: 0.11905534565448761\n",
      "Epoch 6575: train loss: 6.666669651167467e-05, val loss: 0.11954667419195175\n",
      "Epoch 6576: train loss: 0.0001849543332355097, val loss: 0.11922331899404526\n",
      "Epoch 6577: train loss: 0.0002735067973844707, val loss: 0.1217796728014946\n",
      "Epoch 6578: train loss: 0.0002132680092472583, val loss: 0.11765556782484055\n",
      "Epoch 6579: train loss: 7.479388295905665e-05, val loss: 0.11993887275457382\n",
      "Epoch 6580: train loss: 0.00012201339995954186, val loss: 0.11806672066450119\n",
      "Epoch 6581: train loss: 0.00013005016080569476, val loss: 0.11637794971466064\n",
      "Epoch 6582: train loss: 5.8369343605590984e-05, val loss: 0.11776001751422882\n",
      "Epoch 6583: train loss: 6.649734859820455e-05, val loss: 0.11539845913648605\n",
      "Epoch 6584: train loss: 7.503756205551326e-05, val loss: 0.11697884649038315\n",
      "Epoch 6585: train loss: 6.432532973121852e-05, val loss: 0.1146063581109047\n",
      "Epoch 6586: train loss: 3.2661653676768765e-05, val loss: 0.11483035236597061\n",
      "Epoch 6587: train loss: 4.2398914956720546e-05, val loss: 0.11438383162021637\n",
      "Epoch 6588: train loss: 3.545254730852321e-05, val loss: 0.11162853240966797\n",
      "Epoch 6589: train loss: 3.6097928386880085e-05, val loss: 0.11398102343082428\n",
      "Epoch 6590: train loss: 4.0162816731026396e-05, val loss: 0.1131378561258316\n",
      "Epoch 6591: train loss: 1.7129183106590062e-05, val loss: 0.11291144043207169\n",
      "Epoch 6592: train loss: 2.236094223917462e-05, val loss: 0.11299766600131989\n",
      "Epoch 6593: train loss: 2.1693345843232237e-05, val loss: 0.1114393025636673\n",
      "Epoch 6594: train loss: 2.1831699996255338e-05, val loss: 0.1131996288895607\n",
      "Epoch 6595: train loss: 2.05391497729579e-05, val loss: 0.11184524744749069\n",
      "Epoch 6596: train loss: 1.559311567689292e-05, val loss: 0.11109500378370285\n",
      "Epoch 6597: train loss: 1.0372208635089919e-05, val loss: 0.1122194454073906\n",
      "Epoch 6598: train loss: 1.2303358744247817e-05, val loss: 0.11208090931177139\n",
      "Epoch 6599: train loss: 1.588016311870888e-05, val loss: 0.11221611499786377\n",
      "Epoch 6600: train loss: 8.911845725378953e-06, val loss: 0.11147675663232803\n",
      "Epoch 6601: train loss: 1.2592490747920237e-05, val loss: 0.11251723766326904\n",
      "Epoch 6602: train loss: 8.099617843981832e-06, val loss: 0.11196650564670563\n",
      "Epoch 6603: train loss: 8.272616469184868e-06, val loss: 0.11192037165164948\n",
      "Epoch 6604: train loss: 7.791305506543722e-06, val loss: 0.11238770931959152\n",
      "Epoch 6605: train loss: 7.724920578766614e-06, val loss: 0.11146366596221924\n",
      "Epoch 6606: train loss: 8.225522833527066e-06, val loss: 0.11238040030002594\n",
      "Epoch 6607: train loss: 5.021663127990905e-06, val loss: 0.11201902478933334\n",
      "Epoch 6608: train loss: 7.102527433744399e-06, val loss: 0.11199965327978134\n",
      "Epoch 6609: train loss: 9.086441423278302e-06, val loss: 0.11281844228506088\n",
      "Epoch 6610: train loss: 7.469036518159555e-06, val loss: 0.11323552578687668\n",
      "Epoch 6611: train loss: 9.915978807839565e-06, val loss: 0.1115514263510704\n",
      "Epoch 6612: train loss: 1.4925482901162468e-05, val loss: 0.11307324469089508\n",
      "Epoch 6613: train loss: 2.2198508304427378e-05, val loss: 0.11185522377490997\n",
      "Epoch 6614: train loss: 4.1069572034757584e-05, val loss: 0.11424114555120468\n",
      "Epoch 6615: train loss: 7.465234375558794e-05, val loss: 0.11071120947599411\n",
      "Epoch 6616: train loss: 0.00014453813491854817, val loss: 0.11707887798547745\n",
      "Epoch 6617: train loss: 0.0002725287340581417, val loss: 0.10926713794469833\n",
      "Epoch 6618: train loss: 0.0004931643488816917, val loss: 0.1196732148528099\n",
      "Epoch 6619: train loss: 0.000620342034380883, val loss: 0.10952623933553696\n",
      "Epoch 6620: train loss: 0.0006696092896163464, val loss: 0.11790001392364502\n",
      "Epoch 6621: train loss: 0.0006195864989422262, val loss: 0.11576160043478012\n",
      "Epoch 6622: train loss: 0.0006625479436479509, val loss: 0.11555995792150497\n",
      "Epoch 6623: train loss: 0.00034535228041931987, val loss: 0.12085392326116562\n",
      "Epoch 6624: train loss: 0.00016078780754469335, val loss: 0.11889781802892685\n",
      "Epoch 6625: train loss: 0.0003572232380975038, val loss: 0.12082280218601227\n",
      "Epoch 6626: train loss: 0.0002208775549661368, val loss: 0.12418539822101593\n",
      "Epoch 6627: train loss: 9.127955854637548e-05, val loss: 0.1246303841471672\n",
      "Epoch 6628: train loss: 0.00022944661031942815, val loss: 0.12488328665494919\n",
      "Epoch 6629: train loss: 0.00010412555275252089, val loss: 0.12604105472564697\n",
      "Epoch 6630: train loss: 0.00012642444926314056, val loss: 0.12776564061641693\n",
      "Epoch 6631: train loss: 0.00011448289296822622, val loss: 0.12403807789087296\n",
      "Epoch 6632: train loss: 5.656918801832944e-05, val loss: 0.12394646555185318\n",
      "Epoch 6633: train loss: 0.00010709316848078743, val loss: 0.1283961832523346\n",
      "Epoch 6634: train loss: 5.29567405465059e-05, val loss: 0.12614747881889343\n",
      "Epoch 6635: train loss: 7.707601616857573e-05, val loss: 0.12477686256170273\n",
      "Epoch 6636: train loss: 4.214239015709609e-05, val loss: 0.12659521400928497\n",
      "Epoch 6637: train loss: 4.998648364562541e-05, val loss: 0.12707774341106415\n",
      "Epoch 6638: train loss: 4.268655175110325e-05, val loss: 0.12652480602264404\n",
      "Epoch 6639: train loss: 4.160394746577367e-05, val loss: 0.12561658024787903\n",
      "Epoch 6640: train loss: 3.450498479651287e-05, val loss: 0.12713368237018585\n",
      "Epoch 6641: train loss: 2.9065033231745474e-05, val loss: 0.1266622394323349\n",
      "Epoch 6642: train loss: 2.4542028768337332e-05, val loss: 0.12526582181453705\n",
      "Epoch 6643: train loss: 2.8076954549760558e-05, val loss: 0.12633047997951508\n",
      "Epoch 6644: train loss: 2.1140009266673587e-05, val loss: 0.1265421211719513\n",
      "Epoch 6645: train loss: 2.2520243874168955e-05, val loss: 0.1265963315963745\n",
      "Epoch 6646: train loss: 1.2555590728879906e-05, val loss: 0.12628410756587982\n",
      "Epoch 6647: train loss: 1.736656849971041e-05, val loss: 0.12603998184204102\n",
      "Epoch 6648: train loss: 1.1069953870901372e-05, val loss: 0.1266046017408371\n",
      "Epoch 6649: train loss: 1.6703770597814582e-05, val loss: 0.12546612322330475\n",
      "Epoch 6650: train loss: 7.169395303208148e-06, val loss: 0.12504421174526215\n",
      "Epoch 6651: train loss: 1.2944176887685899e-05, val loss: 0.1256144940853119\n",
      "Epoch 6652: train loss: 4.184494628134416e-06, val loss: 0.12576547265052795\n",
      "Epoch 6653: train loss: 1.2834436347475275e-05, val loss: 0.1250889152288437\n",
      "Epoch 6654: train loss: 3.8613293327216525e-06, val loss: 0.12385114282369614\n",
      "Epoch 6655: train loss: 1.0021127309300937e-05, val loss: 0.12469758838415146\n",
      "Epoch 6656: train loss: 2.4382254650845425e-06, val loss: 0.12477786839008331\n",
      "Epoch 6657: train loss: 7.395123247988522e-06, val loss: 0.12303397804498672\n",
      "Epoch 6658: train loss: 3.5326986562722595e-06, val loss: 0.12287607043981552\n",
      "Epoch 6659: train loss: 5.43914211448282e-06, val loss: 0.12335499376058578\n",
      "Epoch 6660: train loss: 4.33432023783098e-06, val loss: 0.12338703125715256\n",
      "Epoch 6661: train loss: 2.7099958970211446e-06, val loss: 0.12299201637506485\n",
      "Epoch 6662: train loss: 3.232898279748042e-06, val loss: 0.12301182746887207\n",
      "Epoch 6663: train loss: 2.3389090983982896e-06, val loss: 0.1236390620470047\n",
      "Epoch 6664: train loss: 2.9100260690029245e-06, val loss: 0.12321360409259796\n",
      "Epoch 6665: train loss: 2.23471852223156e-06, val loss: 0.12308037281036377\n",
      "Epoch 6666: train loss: 1.9241319932916667e-06, val loss: 0.12325775623321533\n",
      "Epoch 6667: train loss: 1.7385492583343876e-06, val loss: 0.12331291288137436\n",
      "Epoch 6668: train loss: 1.625783966119343e-06, val loss: 0.1233353391289711\n",
      "Epoch 6669: train loss: 1.6303930578942527e-06, val loss: 0.12320621311664581\n",
      "Epoch 6670: train loss: 1.5361296163973748e-06, val loss: 0.12352678924798965\n",
      "Epoch 6671: train loss: 1.2190628240205115e-06, val loss: 0.12337841093540192\n",
      "Epoch 6672: train loss: 1.0469406106494716e-06, val loss: 0.12341153621673584\n",
      "Epoch 6673: train loss: 1.1706181339832256e-06, val loss: 0.12360059469938278\n",
      "Epoch 6674: train loss: 7.128865604499879e-07, val loss: 0.1233685240149498\n",
      "Epoch 6675: train loss: 1.1959798484895146e-06, val loss: 0.12363772839307785\n",
      "Epoch 6676: train loss: 7.063920861583028e-07, val loss: 0.12361838668584824\n",
      "Epoch 6677: train loss: 7.768552450215793e-07, val loss: 0.12346066534519196\n",
      "Epoch 6678: train loss: 7.738865406281548e-07, val loss: 0.12362811714410782\n",
      "Epoch 6679: train loss: 4.922284233543905e-07, val loss: 0.12378450483083725\n",
      "Epoch 6680: train loss: 7.138045816645899e-07, val loss: 0.12372850626707077\n",
      "Epoch 6681: train loss: 5.47588399513188e-07, val loss: 0.12371248006820679\n",
      "Epoch 6682: train loss: 5.198076564738585e-07, val loss: 0.1239817664027214\n",
      "Epoch 6683: train loss: 5.638755737891188e-07, val loss: 0.12375252693891525\n",
      "Epoch 6684: train loss: 3.2980946684801893e-07, val loss: 0.12390897423028946\n",
      "Epoch 6685: train loss: 3.8063905094531947e-07, val loss: 0.12397897243499756\n",
      "Epoch 6686: train loss: 4.313950512369047e-07, val loss: 0.12395167350769043\n",
      "Epoch 6687: train loss: 4.441171483904327e-07, val loss: 0.12431486696004868\n",
      "Epoch 6688: train loss: 4.2293393676118285e-07, val loss: 0.12395479530096054\n",
      "Epoch 6689: train loss: 3.7583248513328726e-07, val loss: 0.12426834553480148\n",
      "Epoch 6690: train loss: 3.435322355471726e-07, val loss: 0.1242084726691246\n",
      "Epoch 6691: train loss: 7.73542183196696e-07, val loss: 0.12441208213567734\n",
      "Epoch 6692: train loss: 2.0598135961336084e-06, val loss: 0.12425442039966583\n",
      "Epoch 6693: train loss: 6.85967324898229e-06, val loss: 0.12454386055469513\n",
      "Epoch 6694: train loss: 2.0694737031590194e-05, val loss: 0.12426187098026276\n",
      "Epoch 6695: train loss: 5.521926505025476e-05, val loss: 0.12625928223133087\n",
      "Epoch 6696: train loss: 0.0001272738736588508, val loss: 0.12597040832042694\n",
      "Epoch 6697: train loss: 0.00026688145590014756, val loss: 0.130559042096138\n",
      "Epoch 6698: train loss: 0.0004504362877923995, val loss: 0.12130194157361984\n",
      "Epoch 6699: train loss: 0.0007785993511788547, val loss: 0.13404087722301483\n",
      "Epoch 6700: train loss: 0.0008043319103308022, val loss: 0.11987221240997314\n",
      "Epoch 6701: train loss: 0.0007320018485188484, val loss: 0.1220245361328125\n",
      "Epoch 6702: train loss: 0.0005012709298171103, val loss: 0.12855863571166992\n",
      "Epoch 6703: train loss: 0.00037389612407423556, val loss: 0.11854264885187149\n",
      "Epoch 6704: train loss: 0.0003765220462810248, val loss: 0.11691297590732574\n",
      "Epoch 6705: train loss: 0.0002933366340585053, val loss: 0.12607382237911224\n",
      "Epoch 6706: train loss: 0.00018583402561489493, val loss: 0.12088090181350708\n",
      "Epoch 6707: train loss: 0.00025407428620383143, val loss: 0.11282916367053986\n",
      "Epoch 6708: train loss: 0.0001405261136824265, val loss: 0.11580046266317368\n",
      "Epoch 6709: train loss: 0.00017163532902486622, val loss: 0.11543946713209152\n",
      "Epoch 6710: train loss: 0.00012099923333153129, val loss: 0.11188607662916183\n",
      "Epoch 6711: train loss: 0.00011561647988855839, val loss: 0.1135014072060585\n",
      "Epoch 6712: train loss: 0.00010590266901999712, val loss: 0.11175502836704254\n",
      "Epoch 6713: train loss: 7.36921647330746e-05, val loss: 0.10801124572753906\n",
      "Epoch 6714: train loss: 8.62109154695645e-05, val loss: 0.11021345853805542\n",
      "Epoch 6715: train loss: 6.441680307034403e-05, val loss: 0.110835961997509\n",
      "Epoch 6716: train loss: 5.541967402677983e-05, val loss: 0.1074022427201271\n",
      "Epoch 6717: train loss: 6.185287202242762e-05, val loss: 0.10613609850406647\n",
      "Epoch 6718: train loss: 3.85010898753535e-05, val loss: 0.1064571663737297\n",
      "Epoch 6719: train loss: 4.688604531111196e-05, val loss: 0.1059020385146141\n",
      "Epoch 6720: train loss: 3.424681563046761e-05, val loss: 0.10610667616128922\n",
      "Epoch 6721: train loss: 3.4626904380274937e-05, val loss: 0.10616085678339005\n",
      "Epoch 6722: train loss: 2.919681355706416e-05, val loss: 0.10507688671350479\n",
      "Epoch 6723: train loss: 2.7936182959820144e-05, val loss: 0.10497435182332993\n",
      "Epoch 6724: train loss: 2.486454468453303e-05, val loss: 0.10477028042078018\n",
      "Epoch 6725: train loss: 1.9904175132978708e-05, val loss: 0.10478784888982773\n",
      "Epoch 6726: train loss: 2.1266885596560314e-05, val loss: 0.10526303201913834\n",
      "Epoch 6727: train loss: 1.7224061593879014e-05, val loss: 0.10473237186670303\n",
      "Epoch 6728: train loss: 1.5076628187671304e-05, val loss: 0.10441350191831589\n",
      "Epoch 6729: train loss: 1.6158521248144098e-05, val loss: 0.10532855987548828\n",
      "Epoch 6730: train loss: 1.0916610335698351e-05, val loss: 0.1051141545176506\n",
      "Epoch 6731: train loss: 1.3183140254113823e-05, val loss: 0.10417479276657104\n",
      "Epoch 6732: train loss: 9.84068446996389e-06, val loss: 0.10536553710699081\n",
      "Epoch 6733: train loss: 9.819244951358996e-06, val loss: 0.10587509721517563\n",
      "Epoch 6734: train loss: 8.033729500311892e-06, val loss: 0.10481784492731094\n",
      "Epoch 6735: train loss: 8.335551683558151e-06, val loss: 0.10530833154916763\n",
      "Epoch 6736: train loss: 6.540469257743098e-06, val loss: 0.10603351891040802\n",
      "Epoch 6737: train loss: 6.5002332121366635e-06, val loss: 0.10529910773038864\n",
      "Epoch 6738: train loss: 5.80683308726293e-06, val loss: 0.10514726489782333\n",
      "Epoch 6739: train loss: 5.167006293049781e-06, val loss: 0.10615575313568115\n",
      "Epoch 6740: train loss: 4.59643251815578e-06, val loss: 0.1064596176147461\n",
      "Epoch 6741: train loss: 4.470795829547569e-06, val loss: 0.10614688694477081\n",
      "Epoch 6742: train loss: 3.857648607663577e-06, val loss: 0.10614198446273804\n",
      "Epoch 6743: train loss: 3.2744655982241966e-06, val loss: 0.1065930500626564\n",
      "Epoch 6744: train loss: 3.6821645608142717e-06, val loss: 0.10710418224334717\n",
      "Epoch 6745: train loss: 2.437723424009164e-06, val loss: 0.10662119835615158\n",
      "Epoch 6746: train loss: 3.0070038974372437e-06, val loss: 0.10651091486215591\n",
      "Epoch 6747: train loss: 2.082282435367233e-06, val loss: 0.10717232525348663\n",
      "Epoch 6748: train loss: 2.5886054118018365e-06, val loss: 0.10722317546606064\n",
      "Epoch 6749: train loss: 1.5891836255832459e-06, val loss: 0.10712585598230362\n",
      "Epoch 6750: train loss: 2.067700052066357e-06, val loss: 0.10724502056837082\n",
      "Epoch 6751: train loss: 1.652253558859229e-06, val loss: 0.10708212107419968\n",
      "Epoch 6752: train loss: 1.4698850918648532e-06, val loss: 0.10727613419294357\n",
      "Epoch 6753: train loss: 1.3508654319593916e-06, val loss: 0.10750599950551987\n",
      "Epoch 6754: train loss: 1.360123292215576e-06, val loss: 0.10692556947469711\n",
      "Epoch 6755: train loss: 1.1454080777184572e-06, val loss: 0.107005275785923\n",
      "Epoch 6756: train loss: 1.0272743793393602e-06, val loss: 0.10736053436994553\n",
      "Epoch 6757: train loss: 9.801066198633634e-07, val loss: 0.10702677816152573\n",
      "Epoch 6758: train loss: 8.928057582124893e-07, val loss: 0.10685088485479355\n",
      "Epoch 6759: train loss: 8.876669426172157e-07, val loss: 0.10703225433826447\n",
      "Epoch 6760: train loss: 6.705758437419718e-07, val loss: 0.10715945810079575\n",
      "Epoch 6761: train loss: 7.098810215211415e-07, val loss: 0.10684344917535782\n",
      "Epoch 6762: train loss: 7.051357897580601e-07, val loss: 0.1070629134774208\n",
      "Epoch 6763: train loss: 5.375433715926192e-07, val loss: 0.10730946063995361\n",
      "Epoch 6764: train loss: 5.501598252521944e-07, val loss: 0.10711216181516647\n",
      "Epoch 6765: train loss: 4.914355145047011e-07, val loss: 0.10719781368970871\n",
      "Epoch 6766: train loss: 5.91292575791158e-07, val loss: 0.10750658810138702\n",
      "Epoch 6767: train loss: 2.5028433014995244e-07, val loss: 0.10769951343536377\n",
      "Epoch 6768: train loss: 3.97844559074656e-07, val loss: 0.10755777359008789\n",
      "Epoch 6769: train loss: 5.085320822217909e-07, val loss: 0.10798795521259308\n",
      "Epoch 6770: train loss: 7.989257255758275e-07, val loss: 0.1081468015909195\n",
      "Epoch 6771: train loss: 3.2319669571734266e-06, val loss: 0.10746514052152634\n",
      "Epoch 6772: train loss: 4.798344434675528e-06, val loss: 0.10828884690999985\n",
      "Epoch 6773: train loss: 3.672045977509697e-06, val loss: 0.10760734230279922\n",
      "Epoch 6774: train loss: 6.871326604596106e-06, val loss: 0.10918030887842178\n",
      "Epoch 6775: train loss: 2.078810939565301e-05, val loss: 0.1072874441742897\n",
      "Epoch 6776: train loss: 6.81411984260194e-05, val loss: 0.11133153736591339\n",
      "Epoch 6777: train loss: 0.0002269457618240267, val loss: 0.10465853661298752\n",
      "Epoch 6778: train loss: 0.0007324217003770173, val loss: 0.12212129682302475\n",
      "Epoch 6779: train loss: 0.0018451126525178552, val loss: 0.10545506328344345\n",
      "Epoch 6780: train loss: 0.003612250555306673, val loss: 0.1289072483778\n",
      "Epoch 6781: train loss: 0.0024982814211398363, val loss: 0.1267666071653366\n",
      "Epoch 6782: train loss: 0.0016877688467502594, val loss: 0.124885693192482\n",
      "Epoch 6783: train loss: 0.001412595040164888, val loss: 0.11881709098815918\n",
      "Epoch 6784: train loss: 0.00038914481410756707, val loss: 0.13176988065242767\n",
      "Epoch 6785: train loss: 0.0010247707832604647, val loss: 0.12992173433303833\n",
      "Epoch 6786: train loss: 0.0007488946430385113, val loss: 0.12404179573059082\n",
      "Epoch 6787: train loss: 0.0007616463699378073, val loss: 0.1361895054578781\n",
      "Epoch 6788: train loss: 0.00038512208266183734, val loss: 0.13370488584041595\n",
      "Epoch 6789: train loss: 0.00030947383493185043, val loss: 0.12570130825042725\n",
      "Epoch 6790: train loss: 0.0004831391270272434, val loss: 0.1318458765745163\n",
      "Epoch 6791: train loss: 0.00039964693132787943, val loss: 0.13239578902721405\n",
      "Epoch 6792: train loss: 0.00023857968335505575, val loss: 0.12940387427806854\n",
      "Epoch 6793: train loss: 0.00013751933875028044, val loss: 0.12939748167991638\n",
      "Epoch 6794: train loss: 0.0002857419312931597, val loss: 0.13032589852809906\n",
      "Epoch 6795: train loss: 0.0002730191918089986, val loss: 0.1337295025587082\n",
      "Epoch 6796: train loss: 0.00011493414058350027, val loss: 0.1322319358587265\n",
      "Epoch 6797: train loss: 0.00012498181604314595, val loss: 0.12978120148181915\n",
      "Epoch 6798: train loss: 0.00013764668256044388, val loss: 0.1315392106771469\n",
      "Epoch 6799: train loss: 0.00018787833687383682, val loss: 0.1308409720659256\n",
      "Epoch 6800: train loss: 8.166115003405139e-05, val loss: 0.1316521316766739\n",
      "Epoch 6801: train loss: 5.905962098040618e-05, val loss: 0.1327366828918457\n",
      "Epoch 6802: train loss: 9.829088958213106e-05, val loss: 0.1308322250843048\n",
      "Epoch 6803: train loss: 0.00012716735363937914, val loss: 0.13100215792655945\n",
      "Epoch 6804: train loss: 3.8489812141051516e-05, val loss: 0.13149544596672058\n",
      "Epoch 6805: train loss: 4.531931699602865e-05, val loss: 0.13200947642326355\n",
      "Epoch 6806: train loss: 7.426738739013672e-05, val loss: 0.13266323506832123\n",
      "Epoch 6807: train loss: 6.602520443266258e-05, val loss: 0.1303211748600006\n",
      "Epoch 6808: train loss: 3.653012026916258e-05, val loss: 0.13059024512767792\n",
      "Epoch 6809: train loss: 2.1738678697147407e-05, val loss: 0.13266783952713013\n",
      "Epoch 6810: train loss: 4.603516572387889e-05, val loss: 0.1323504000902176\n",
      "Epoch 6811: train loss: 4.249103585607372e-05, val loss: 0.13223138451576233\n",
      "Epoch 6812: train loss: 2.6471379896975122e-05, val loss: 0.1319868415594101\n",
      "Epoch 6813: train loss: 9.612077519705053e-06, val loss: 0.1324867159128189\n",
      "Epoch 6814: train loss: 3.407392796361819e-05, val loss: 0.13347728550434113\n",
      "Epoch 6815: train loss: 2.7909543860005215e-05, val loss: 0.1320836842060089\n",
      "Epoch 6816: train loss: 1.3698530892725103e-05, val loss: 0.13200441002845764\n",
      "Epoch 6817: train loss: 1.1258342055953108e-05, val loss: 0.13308583199977875\n",
      "Epoch 6818: train loss: 1.8397342500975356e-05, val loss: 0.1321844905614853\n",
      "Epoch 6819: train loss: 2.106631472997833e-05, val loss: 0.13158687949180603\n",
      "Epoch 6820: train loss: 8.412370334553998e-06, val loss: 0.13177886605262756\n",
      "Epoch 6821: train loss: 7.021678356977645e-06, val loss: 0.13294175267219543\n",
      "Epoch 6822: train loss: 1.2751104804920033e-05, val loss: 0.13395850360393524\n",
      "Epoch 6823: train loss: 1.4278628441388719e-05, val loss: 0.13265357911586761\n",
      "Epoch 6824: train loss: 4.519619778875494e-06, val loss: 0.13240677118301392\n",
      "Epoch 6825: train loss: 6.180715899972711e-06, val loss: 0.13348309695720673\n",
      "Epoch 6826: train loss: 7.418450877594296e-06, val loss: 0.13331133127212524\n",
      "Epoch 6827: train loss: 9.440192116016988e-06, val loss: 0.13297446072101593\n",
      "Epoch 6828: train loss: 4.1515268094372004e-06, val loss: 0.13271759450435638\n",
      "Epoch 6829: train loss: 2.9285174605320208e-06, val loss: 0.13314004242420197\n",
      "Epoch 6830: train loss: 5.6202047744591255e-06, val loss: 0.13384419679641724\n",
      "Epoch 6831: train loss: 6.203152224770747e-06, val loss: 0.13324038684368134\n",
      "Epoch 6832: train loss: 2.6158681976085063e-06, val loss: 0.13321331143379211\n",
      "Epoch 6833: train loss: 2.50816515290353e-06, val loss: 0.1337396800518036\n",
      "Epoch 6834: train loss: 3.073956349908258e-06, val loss: 0.1336071491241455\n",
      "Epoch 6835: train loss: 4.4454886847233865e-06, val loss: 0.13369141519069672\n",
      "Epoch 6836: train loss: 2.06592403628747e-06, val loss: 0.13348011672496796\n",
      "Epoch 6837: train loss: 1.1040106073778588e-06, val loss: 0.1335444301366806\n",
      "Epoch 6838: train loss: 2.2221088329388294e-06, val loss: 0.1340528279542923\n",
      "Epoch 6839: train loss: 2.2701822217641165e-06, val loss: 0.13389621675014496\n",
      "Epoch 6840: train loss: 1.8204203797722585e-06, val loss: 0.13405652344226837\n",
      "Epoch 6841: train loss: 4.826589474760112e-07, val loss: 0.13428232073783875\n",
      "Epoch 6842: train loss: 1.3549615687225014e-06, val loss: 0.13409695029258728\n",
      "Epoch 6843: train loss: 1.6709258261471405e-06, val loss: 0.13416822254657745\n",
      "Epoch 6844: train loss: 1.1430790891608922e-06, val loss: 0.13406632840633392\n",
      "Epoch 6845: train loss: 5.783847996099212e-07, val loss: 0.13429252803325653\n",
      "Epoch 6846: train loss: 6.450500222854316e-07, val loss: 0.13447211682796478\n",
      "Epoch 6847: train loss: 1.184293410005921e-06, val loss: 0.134089395403862\n",
      "Epoch 6848: train loss: 8.245581284427317e-07, val loss: 0.1342301219701767\n",
      "Epoch 6849: train loss: 5.218644218984991e-07, val loss: 0.13423596322536469\n",
      "Epoch 6850: train loss: 2.94698025982143e-07, val loss: 0.13410139083862305\n",
      "Epoch 6851: train loss: 8.040428269850963e-07, val loss: 0.13445013761520386\n",
      "Epoch 6852: train loss: 6.462675514740113e-07, val loss: 0.1342650055885315\n",
      "Epoch 6853: train loss: 3.4238797752550454e-07, val loss: 0.13409964740276337\n",
      "Epoch 6854: train loss: 3.1937673838910996e-07, val loss: 0.13418446481227875\n",
      "Epoch 6855: train loss: 3.139185764666763e-07, val loss: 0.13393893837928772\n",
      "Epoch 6856: train loss: 5.962530735814653e-07, val loss: 0.13397809863090515\n",
      "Epoch 6857: train loss: 2.870282287403825e-07, val loss: 0.13405078649520874\n",
      "Epoch 6858: train loss: 1.926572394950199e-07, val loss: 0.13391990959644318\n",
      "Epoch 6859: train loss: 2.1888808987569064e-07, val loss: 0.13378112018108368\n",
      "Epoch 6860: train loss: 3.192956228303956e-07, val loss: 0.13385410606861115\n",
      "Epoch 6861: train loss: 3.35681960450529e-07, val loss: 0.13380621373653412\n",
      "Epoch 6862: train loss: 1.3564317669079173e-07, val loss: 0.13346102833747864\n",
      "Epoch 6863: train loss: 2.39846684735312e-07, val loss: 0.13365642726421356\n",
      "Epoch 6864: train loss: 3.873244054375391e-07, val loss: 0.1335512101650238\n",
      "Epoch 6865: train loss: 4.226366172588314e-07, val loss: 0.13368652760982513\n",
      "Epoch 6866: train loss: 4.961856348018046e-07, val loss: 0.13348372280597687\n",
      "Epoch 6867: train loss: 1.0122041658178205e-06, val loss: 0.13368265330791473\n",
      "Epoch 6868: train loss: 2.2793817606725497e-06, val loss: 0.13359351456165314\n",
      "Epoch 6869: train loss: 5.675501142832218e-06, val loss: 0.13384664058685303\n",
      "Epoch 6870: train loss: 1.2022347618767526e-05, val loss: 0.13332384824752808\n",
      "Epoch 6871: train loss: 2.2887765226187184e-05, val loss: 0.1348196119070053\n",
      "Epoch 6872: train loss: 3.698849104694091e-05, val loss: 0.13242705166339874\n",
      "Epoch 6873: train loss: 6.912825483595952e-05, val loss: 0.1380644589662552\n",
      "Epoch 6874: train loss: 0.00013767740165349096, val loss: 0.13035321235656738\n",
      "Epoch 6875: train loss: 0.00028839404694736004, val loss: 0.14353199303150177\n",
      "Epoch 6876: train loss: 0.00035672038211487234, val loss: 0.1313600093126297\n",
      "Epoch 6877: train loss: 0.0003607288817875087, val loss: 0.14418275654315948\n",
      "Epoch 6878: train loss: 0.0002801906375680119, val loss: 0.1359434425830841\n",
      "Epoch 6879: train loss: 0.00030170645914040506, val loss: 0.14014749228954315\n",
      "Epoch 6880: train loss: 0.00013356910494621843, val loss: 0.13725827634334564\n",
      "Epoch 6881: train loss: 2.631946699693799e-05, val loss: 0.1365191787481308\n",
      "Epoch 6882: train loss: 8.681904000695795e-05, val loss: 0.13960473239421844\n",
      "Epoch 6883: train loss: 0.0001259026030311361, val loss: 0.1328117847442627\n",
      "Epoch 6884: train loss: 8.673265983816236e-05, val loss: 0.13638310134410858\n",
      "Epoch 6885: train loss: 2.590387884993106e-05, val loss: 0.13672135770320892\n",
      "Epoch 6886: train loss: 5.6542528909631073e-05, val loss: 0.13171446323394775\n",
      "Epoch 6887: train loss: 6.952973490115255e-05, val loss: 0.1351131647825241\n",
      "Epoch 6888: train loss: 3.5505614505382255e-05, val loss: 0.1340004801750183\n",
      "Epoch 6889: train loss: 4.019633706775494e-05, val loss: 0.1316431760787964\n",
      "Epoch 6890: train loss: 3.8450096326414496e-05, val loss: 0.1335488110780716\n",
      "Epoch 6891: train loss: 2.8609743822016753e-05, val loss: 0.13258153200149536\n",
      "Epoch 6892: train loss: 2.203070471296087e-05, val loss: 0.1315644532442093\n",
      "Epoch 6893: train loss: 3.751028998522088e-05, val loss: 0.1320318728685379\n",
      "Epoch 6894: train loss: 2.4736827981541865e-05, val loss: 0.13162294030189514\n",
      "Epoch 6895: train loss: 7.775174708513077e-06, val loss: 0.13079018890857697\n",
      "Epoch 6896: train loss: 2.5355622710776515e-05, val loss: 0.13122187554836273\n",
      "Epoch 6897: train loss: 2.3036249331198633e-05, val loss: 0.1313108205795288\n",
      "Epoch 6898: train loss: 8.977252946351655e-06, val loss: 0.1306162327528\n",
      "Epoch 6899: train loss: 1.6184008927666582e-05, val loss: 0.13174918293952942\n",
      "Epoch 6900: train loss: 1.4280546565714758e-05, val loss: 0.13081493973731995\n",
      "Epoch 6901: train loss: 5.718197826354299e-06, val loss: 0.12977388501167297\n",
      "Epoch 6902: train loss: 1.463963053538464e-05, val loss: 0.13142138719558716\n",
      "Epoch 6903: train loss: 1.2938735380885191e-05, val loss: 0.13004975020885468\n",
      "Epoch 6904: train loss: 3.511200020511751e-06, val loss: 0.1299305409193039\n",
      "Epoch 6905: train loss: 6.620413842028938e-06, val loss: 0.131492480635643\n",
      "Epoch 6906: train loss: 1.1186687515873928e-05, val loss: 0.13002559542655945\n",
      "Epoch 6907: train loss: 6.566892807313707e-06, val loss: 0.13048315048217773\n",
      "Epoch 6908: train loss: 3.7416170926007908e-06, val loss: 0.13093049824237823\n",
      "Epoch 6909: train loss: 6.30294562142808e-06, val loss: 0.13003277778625488\n",
      "Epoch 6910: train loss: 5.786105703009525e-06, val loss: 0.13072793185710907\n",
      "Epoch 6911: train loss: 4.171010459685931e-06, val loss: 0.1303088366985321\n",
      "Epoch 6912: train loss: 4.872333192906808e-06, val loss: 0.13032859563827515\n",
      "Epoch 6913: train loss: 4.190889285382582e-06, val loss: 0.130747988820076\n",
      "Epoch 6914: train loss: 3.1043368835526053e-06, val loss: 0.13022077083587646\n",
      "Epoch 6915: train loss: 3.519180609146133e-06, val loss: 0.13057513535022736\n",
      "Epoch 6916: train loss: 4.029678166261874e-06, val loss: 0.1306333988904953\n",
      "Epoch 6917: train loss: 3.168123384966748e-06, val loss: 0.13067898154258728\n",
      "Epoch 6918: train loss: 1.935037289513275e-06, val loss: 0.13052789866924286\n",
      "Epoch 6919: train loss: 2.138671788998181e-06, val loss: 0.1306857168674469\n",
      "Epoch 6920: train loss: 2.871615606636624e-06, val loss: 0.13099640607833862\n",
      "Epoch 6921: train loss: 3.63359640687122e-06, val loss: 0.1304858922958374\n",
      "Epoch 6922: train loss: 3.945366643165471e-06, val loss: 0.1312742531299591\n",
      "Epoch 6923: train loss: 3.116261723334901e-06, val loss: 0.13084134459495544\n",
      "Epoch 6924: train loss: 3.0256724130595103e-06, val loss: 0.13144950568675995\n",
      "Epoch 6925: train loss: 4.01300894736778e-06, val loss: 0.13121317327022552\n",
      "Epoch 6926: train loss: 5.191346190258628e-06, val loss: 0.1316598355770111\n",
      "Epoch 6927: train loss: 7.52631422074046e-06, val loss: 0.13089267909526825\n",
      "Epoch 6928: train loss: 1.3552464224630967e-05, val loss: 0.13232243061065674\n",
      "Epoch 6929: train loss: 2.9674234610865824e-05, val loss: 0.13045869767665863\n",
      "Epoch 6930: train loss: 7.276083488250151e-05, val loss: 0.13305549323558807\n",
      "Epoch 6931: train loss: 0.00017834351456258446, val loss: 0.12943606078624725\n",
      "Epoch 6932: train loss: 0.00045232169213704765, val loss: 0.13433502614498138\n",
      "Epoch 6933: train loss: 0.0011437111534178257, val loss: 0.1275930404663086\n",
      "Epoch 6934: train loss: 0.0024018390104174614, val loss: 0.1423734426498413\n",
      "Epoch 6935: train loss: 0.0025520806666463614, val loss: 0.12641148269176483\n",
      "Epoch 6936: train loss: 0.0010778546566143632, val loss: 0.14004911482334137\n",
      "Epoch 6937: train loss: 0.0008103010477498174, val loss: 0.13600359857082367\n",
      "Epoch 6938: train loss: 0.0007484467932954431, val loss: 0.12380009144544601\n",
      "Epoch 6939: train loss: 0.0005847063730470836, val loss: 0.12866215407848358\n",
      "Epoch 6940: train loss: 0.0004497951304074377, val loss: 0.1350361853837967\n",
      "Epoch 6941: train loss: 0.00042041606502607465, val loss: 0.12569984793663025\n",
      "Epoch 6942: train loss: 0.00026936898939311504, val loss: 0.12277362495660782\n",
      "Epoch 6943: train loss: 0.000369952991604805, val loss: 0.12959395349025726\n",
      "Epoch 6944: train loss: 0.00015805715520400554, val loss: 0.1281944066286087\n",
      "Epoch 6945: train loss: 0.0002819163491949439, val loss: 0.1213824525475502\n",
      "Epoch 6946: train loss: 0.00013066625979263335, val loss: 0.12109905481338501\n",
      "Epoch 6947: train loss: 0.00016673751815687865, val loss: 0.12618522346019745\n",
      "Epoch 6948: train loss: 0.00015192970749922097, val loss: 0.12759092450141907\n",
      "Epoch 6949: train loss: 0.00010919300257228315, val loss: 0.12327396124601364\n",
      "Epoch 6950: train loss: 0.00012301516835577786, val loss: 0.12196480482816696\n",
      "Epoch 6951: train loss: 7.031056884443387e-05, val loss: 0.12371288985013962\n",
      "Epoch 6952: train loss: 0.00011416339839342982, val loss: 0.12403333187103271\n",
      "Epoch 6953: train loss: 4.61104791611433e-05, val loss: 0.12410765141248703\n",
      "Epoch 6954: train loss: 8.771698776399717e-05, val loss: 0.12442856281995773\n",
      "Epoch 6955: train loss: 6.414011295419186e-05, val loss: 0.12359794229269028\n",
      "Epoch 6956: train loss: 2.9627573894686066e-05, val loss: 0.12099909037351608\n",
      "Epoch 6957: train loss: 6.091708564781584e-05, val loss: 0.12054982036352158\n",
      "Epoch 6958: train loss: 4.232723222230561e-05, val loss: 0.12429433315992355\n",
      "Epoch 6959: train loss: 3.21901825373061e-05, val loss: 0.1255989521741867\n",
      "Epoch 6960: train loss: 3.807681059697643e-05, val loss: 0.12341278046369553\n",
      "Epoch 6961: train loss: 2.2704878574586473e-05, val loss: 0.12177299708127975\n",
      "Epoch 6962: train loss: 2.4262672013719566e-05, val loss: 0.12267636507749557\n",
      "Epoch 6963: train loss: 2.128689084202051e-05, val loss: 0.12421002238988876\n",
      "Epoch 6964: train loss: 2.5545972675899975e-05, val loss: 0.1236177310347557\n",
      "Epoch 6965: train loss: 1.1557862308109179e-05, val loss: 0.12359683960676193\n",
      "Epoch 6966: train loss: 1.7564558220328763e-05, val loss: 0.12467370182275772\n",
      "Epoch 6967: train loss: 1.532452915853355e-05, val loss: 0.12488458305597305\n",
      "Epoch 6968: train loss: 1.0978090358548798e-05, val loss: 0.12449910491704941\n",
      "Epoch 6969: train loss: 1.380294361297274e-05, val loss: 0.12441344559192657\n",
      "Epoch 6970: train loss: 1.0218351235380396e-05, val loss: 0.12512560188770294\n",
      "Epoch 6971: train loss: 7.80321079218993e-06, val loss: 0.12484096735715866\n",
      "Epoch 6972: train loss: 9.213822522724513e-06, val loss: 0.12386807054281235\n",
      "Epoch 6973: train loss: 8.566700671508443e-06, val loss: 0.1244380995631218\n",
      "Epoch 6974: train loss: 6.646980637015076e-06, val loss: 0.12525136768817902\n",
      "Epoch 6975: train loss: 6.803904852858977e-06, val loss: 0.12498724460601807\n",
      "Epoch 6976: train loss: 5.19832383361063e-06, val loss: 0.12435450404882431\n",
      "Epoch 6977: train loss: 5.297316420183051e-06, val loss: 0.1248667761683464\n",
      "Epoch 6978: train loss: 5.321289336279733e-06, val loss: 0.1260332316160202\n",
      "Epoch 6979: train loss: 4.233551862853346e-06, val loss: 0.12568554282188416\n",
      "Epoch 6980: train loss: 3.7454515222634654e-06, val loss: 0.12504737079143524\n",
      "Epoch 6981: train loss: 3.401630920052412e-06, val loss: 0.12539204955101013\n",
      "Epoch 6982: train loss: 2.165906380469096e-06, val loss: 0.1258465051651001\n",
      "Epoch 6983: train loss: 3.2077866762847407e-06, val loss: 0.12578265368938446\n",
      "Epoch 6984: train loss: 2.4330663563887356e-06, val loss: 0.1256081461906433\n",
      "Epoch 6985: train loss: 1.9860221982526127e-06, val loss: 0.1260347217321396\n",
      "Epoch 6986: train loss: 1.993751084228279e-06, val loss: 0.12597383558750153\n",
      "Epoch 6987: train loss: 1.649288378757774e-06, val loss: 0.1254802793264389\n",
      "Epoch 6988: train loss: 1.4801154293309082e-06, val loss: 0.1257597655057907\n",
      "Epoch 6989: train loss: 1.7316375533482642e-06, val loss: 0.12636752426624298\n",
      "Epoch 6990: train loss: 1.122175376622181e-06, val loss: 0.12661291658878326\n",
      "Epoch 6991: train loss: 1.3173629440643708e-06, val loss: 0.12640626728534698\n",
      "Epoch 6992: train loss: 8.565210691813263e-07, val loss: 0.12650085985660553\n",
      "Epoch 6993: train loss: 1.1411370905989315e-06, val loss: 0.1267949342727661\n",
      "Epoch 6994: train loss: 7.937025543469645e-07, val loss: 0.12669217586517334\n",
      "Epoch 6995: train loss: 1.0167365189772681e-06, val loss: 0.12678103148937225\n",
      "Epoch 6996: train loss: 5.901842996536288e-07, val loss: 0.12702599167823792\n",
      "Epoch 6997: train loss: 7.599831519655709e-07, val loss: 0.1270255744457245\n",
      "Epoch 6998: train loss: 4.845338139602973e-07, val loss: 0.12688544392585754\n",
      "Epoch 6999: train loss: 7.171702804953384e-07, val loss: 0.12704940140247345\n",
      "Epoch 7000: train loss: 4.069238173087797e-07, val loss: 0.12748980522155762\n",
      "Epoch 7001: train loss: 6.788453106310044e-07, val loss: 0.1274077147245407\n",
      "Epoch 7002: train loss: 5.090137733532174e-07, val loss: 0.1273832619190216\n",
      "Epoch 7003: train loss: 3.8000317204023304e-07, val loss: 0.12765096127986908\n",
      "Epoch 7004: train loss: 3.7523372498071694e-07, val loss: 0.1276087462902069\n",
      "Epoch 7005: train loss: 4.5676804916183755e-07, val loss: 0.12757611274719238\n",
      "Epoch 7006: train loss: 4.2430079361110984e-07, val loss: 0.1277366727590561\n",
      "Epoch 7007: train loss: 1.93323373309795e-07, val loss: 0.12786956131458282\n",
      "Epoch 7008: train loss: 2.877311544580152e-07, val loss: 0.1278601437807083\n",
      "Epoch 7009: train loss: 3.4692214967435575e-07, val loss: 0.12801337242126465\n",
      "Epoch 7010: train loss: 4.153132806550275e-07, val loss: 0.12819194793701172\n",
      "Epoch 7011: train loss: 3.3768827734093065e-07, val loss: 0.12818078696727753\n",
      "Epoch 7012: train loss: 2.8403778173924366e-07, val loss: 0.12831833958625793\n",
      "Epoch 7013: train loss: 2.678063992789248e-07, val loss: 0.12839184701442719\n",
      "Epoch 7014: train loss: 3.924258749066212e-07, val loss: 0.12855063378810883\n",
      "Epoch 7015: train loss: 6.495905040537764e-07, val loss: 0.1284008026123047\n",
      "Epoch 7016: train loss: 1.623915181880875e-06, val loss: 0.12892717123031616\n",
      "Epoch 7017: train loss: 4.9115838010038715e-06, val loss: 0.12916286289691925\n",
      "Epoch 7018: train loss: 1.4571370229532477e-05, val loss: 0.13078586757183075\n",
      "Epoch 7019: train loss: 2.111209323629737e-05, val loss: 0.1306164711713791\n",
      "Epoch 7020: train loss: 2.07349585252814e-05, val loss: 0.13071785867214203\n",
      "Epoch 7021: train loss: 8.268040801340248e-06, val loss: 0.1308647096157074\n",
      "Epoch 7022: train loss: 1.0114458746102173e-05, val loss: 0.13019561767578125\n",
      "Epoch 7023: train loss: 1.2081915883754846e-05, val loss: 0.13121256232261658\n",
      "Epoch 7024: train loss: 3.7250426885293564e-06, val loss: 0.13126759231090546\n",
      "Epoch 7025: train loss: 7.148272288759472e-06, val loss: 0.13086643815040588\n",
      "Epoch 7026: train loss: 1.0405315151729155e-05, val loss: 0.13083766400814056\n",
      "Epoch 7027: train loss: 4.317660568631254e-06, val loss: 0.13065393269062042\n",
      "Epoch 7028: train loss: 4.470573458092986e-06, val loss: 0.12993118166923523\n",
      "Epoch 7029: train loss: 6.818031579314265e-06, val loss: 0.13100162148475647\n",
      "Epoch 7030: train loss: 3.7929532936686883e-06, val loss: 0.13102595508098602\n",
      "Epoch 7031: train loss: 3.935610948246904e-06, val loss: 0.12985502183437347\n",
      "Epoch 7032: train loss: 5.59694308321923e-06, val loss: 0.1308828890323639\n",
      "Epoch 7033: train loss: 2.530318170101964e-06, val loss: 0.13068805634975433\n",
      "Epoch 7034: train loss: 2.7861460694111884e-06, val loss: 0.1303977072238922\n",
      "Epoch 7035: train loss: 3.520799054967938e-06, val loss: 0.13082800805568695\n",
      "Epoch 7036: train loss: 2.102762891809107e-06, val loss: 0.1306554079055786\n",
      "Epoch 7037: train loss: 2.694418071769178e-06, val loss: 0.130745992064476\n",
      "Epoch 7038: train loss: 4.426679424796021e-06, val loss: 0.13067296147346497\n",
      "Epoch 7039: train loss: 5.216078989178641e-06, val loss: 0.13124459981918335\n",
      "Epoch 7040: train loss: 7.00184864399489e-06, val loss: 0.13026399910449982\n",
      "Epoch 7041: train loss: 1.5480982256121933e-05, val loss: 0.13152854144573212\n",
      "Epoch 7042: train loss: 3.5756780562223867e-05, val loss: 0.1304003894329071\n",
      "Epoch 7043: train loss: 8.620821608928964e-05, val loss: 0.1321449726819992\n",
      "Epoch 7044: train loss: 0.00020485483400989324, val loss: 0.13027164340019226\n",
      "Epoch 7045: train loss: 0.0004448934632819146, val loss: 0.1360733062028885\n",
      "Epoch 7046: train loss: 0.0006288838922046125, val loss: 0.1293688863515854\n",
      "Epoch 7047: train loss: 0.0005715272272937, val loss: 0.13874755799770355\n",
      "Epoch 7048: train loss: 0.00013251896598376334, val loss: 0.13440199196338654\n",
      "Epoch 7049: train loss: 8.499527029925957e-05, val loss: 0.13121406733989716\n",
      "Epoch 7050: train loss: 0.0002534717496018857, val loss: 0.13877378404140472\n",
      "Epoch 7051: train loss: 0.00014799025666434318, val loss: 0.13351067900657654\n",
      "Epoch 7052: train loss: 7.235325028887019e-05, val loss: 0.13104014098644257\n",
      "Epoch 7053: train loss: 0.0001304605248151347, val loss: 0.1376352310180664\n",
      "Epoch 7054: train loss: 9.040100121637806e-05, val loss: 0.13306288421154022\n",
      "Epoch 7055: train loss: 5.1706305384868756e-05, val loss: 0.13155530393123627\n",
      "Epoch 7056: train loss: 7.798829028615728e-05, val loss: 0.13599355518817902\n",
      "Epoch 7057: train loss: 5.0361388275632635e-05, val loss: 0.13348586857318878\n",
      "Epoch 7058: train loss: 4.039037594338879e-05, val loss: 0.1310684233903885\n",
      "Epoch 7059: train loss: 6.077797297621146e-05, val loss: 0.13374494016170502\n",
      "Epoch 7060: train loss: 2.6566298402030952e-05, val loss: 0.13406561315059662\n",
      "Epoch 7061: train loss: 4.6442717575700954e-05, val loss: 0.13022196292877197\n",
      "Epoch 7062: train loss: 2.8537224352476187e-05, val loss: 0.1320338398218155\n",
      "Epoch 7063: train loss: 2.6476776838535443e-05, val loss: 0.13312114775180817\n",
      "Epoch 7064: train loss: 3.0136550776660442e-05, val loss: 0.1299000233411789\n",
      "Epoch 7065: train loss: 1.5729061487945728e-05, val loss: 0.130419060587883\n",
      "Epoch 7066: train loss: 2.9090000680298544e-05, val loss: 0.13182419538497925\n",
      "Epoch 7067: train loss: 1.2645710739889182e-05, val loss: 0.1304633766412735\n",
      "Epoch 7068: train loss: 2.2007710867910646e-05, val loss: 0.12946823239326477\n",
      "Epoch 7069: train loss: 1.4073143574933056e-05, val loss: 0.1306459903717041\n",
      "Epoch 7070: train loss: 1.0965772162307985e-05, val loss: 0.1302116960287094\n",
      "Epoch 7071: train loss: 1.717890518193599e-05, val loss: 0.12954823672771454\n",
      "Epoch 7072: train loss: 6.267697699513519e-06, val loss: 0.12984530627727509\n",
      "Epoch 7073: train loss: 1.3447997844195925e-05, val loss: 0.1291605681180954\n",
      "Epoch 7074: train loss: 1.041884024743922e-05, val loss: 0.12929962575435638\n",
      "Epoch 7075: train loss: 5.3326948545873165e-06, val loss: 0.12947435677051544\n",
      "Epoch 7076: train loss: 1.2109196177334525e-05, val loss: 0.12943588197231293\n",
      "Epoch 7077: train loss: 3.798783382080728e-06, val loss: 0.12926043570041656\n",
      "Epoch 7078: train loss: 6.709216449962696e-06, val loss: 0.12931238114833832\n",
      "Epoch 7079: train loss: 7.089462542353431e-06, val loss: 0.12920185923576355\n",
      "Epoch 7080: train loss: 2.8606414161913563e-06, val loss: 0.129272922873497\n",
      "Epoch 7081: train loss: 6.672976724075852e-06, val loss: 0.12927773594856262\n",
      "Epoch 7082: train loss: 3.8395160117943306e-06, val loss: 0.12847773730754852\n",
      "Epoch 7083: train loss: 3.7926326967863133e-06, val loss: 0.1295820027589798\n",
      "Epoch 7084: train loss: 4.918614195048576e-06, val loss: 0.12924085557460785\n",
      "Epoch 7085: train loss: 5.349802449927665e-06, val loss: 0.1293470710515976\n",
      "Epoch 7086: train loss: 4.448102117748931e-06, val loss: 0.12922431528568268\n",
      "Epoch 7087: train loss: 3.816297066805419e-06, val loss: 0.12965649366378784\n",
      "Epoch 7088: train loss: 4.193076620140346e-06, val loss: 0.1290721893310547\n",
      "Epoch 7089: train loss: 3.7797494769620243e-06, val loss: 0.12958195805549622\n",
      "Epoch 7090: train loss: 2.3935492663440527e-06, val loss: 0.12948517501354218\n",
      "Epoch 7091: train loss: 2.4625069272588007e-06, val loss: 0.1293700784444809\n",
      "Epoch 7092: train loss: 3.530281219354947e-06, val loss: 0.12964534759521484\n",
      "Epoch 7093: train loss: 1.7979836002268712e-06, val loss: 0.12961900234222412\n",
      "Epoch 7094: train loss: 2.5150220608338714e-06, val loss: 0.1292106658220291\n",
      "Epoch 7095: train loss: 5.211551524553215e-06, val loss: 0.13024349510669708\n",
      "Epoch 7096: train loss: 8.231102583522443e-06, val loss: 0.12894077599048615\n",
      "Epoch 7097: train loss: 2.1999441742082126e-05, val loss: 0.13088996708393097\n",
      "Epoch 7098: train loss: 7.37830632715486e-05, val loss: 0.12724032998085022\n",
      "Epoch 7099: train loss: 0.000249947770498693, val loss: 0.13330519199371338\n",
      "Epoch 7100: train loss: 0.0004908146220259368, val loss: 0.13014213740825653\n",
      "Epoch 7101: train loss: 0.00010358033614465967, val loss: 0.13483479619026184\n",
      "Epoch 7102: train loss: 0.0002775397733785212, val loss: 0.13268183171749115\n",
      "Epoch 7103: train loss: 0.0001470229180995375, val loss: 0.14077594876289368\n",
      "Epoch 7104: train loss: 0.00024213323194999248, val loss: 0.13168998062610626\n",
      "Epoch 7105: train loss: 0.00023220761795528233, val loss: 0.14001668989658356\n",
      "Epoch 7106: train loss: 0.00018946027557831258, val loss: 0.1353471577167511\n",
      "Epoch 7107: train loss: 0.00022845952480565757, val loss: 0.1480262726545334\n",
      "Epoch 7108: train loss: 0.00023647025227546692, val loss: 0.1362912654876709\n",
      "Epoch 7109: train loss: 0.0002756546309683472, val loss: 0.1460917443037033\n",
      "Epoch 7110: train loss: 0.0003380651760380715, val loss: 0.1344766616821289\n",
      "Epoch 7111: train loss: 0.0003263901162426919, val loss: 0.1442132294178009\n",
      "Epoch 7112: train loss: 0.00020442054665181786, val loss: 0.14124533534049988\n",
      "Epoch 7113: train loss: 0.00012834745575673878, val loss: 0.1429322510957718\n",
      "Epoch 7114: train loss: 0.0001055798857123591, val loss: 0.14415740966796875\n",
      "Epoch 7115: train loss: 9.867687913356349e-05, val loss: 0.14003367722034454\n",
      "Epoch 7116: train loss: 7.73279825807549e-05, val loss: 0.14053882658481598\n",
      "Epoch 7117: train loss: 8.288634853670374e-05, val loss: 0.14067058265209198\n",
      "Epoch 7118: train loss: 7.312973320949823e-05, val loss: 0.1404045671224594\n",
      "Epoch 7119: train loss: 6.194745219545439e-05, val loss: 0.13717229664325714\n",
      "Epoch 7120: train loss: 0.000100016659416724, val loss: 0.1416783630847931\n",
      "Epoch 7121: train loss: 7.500373612856492e-05, val loss: 0.1395518183708191\n",
      "Epoch 7122: train loss: 3.44698601111304e-05, val loss: 0.1385553628206253\n",
      "Epoch 7123: train loss: 4.567757423501462e-05, val loss: 0.14070414006710052\n",
      "Epoch 7124: train loss: 4.265232564648613e-05, val loss: 0.14120933413505554\n",
      "Epoch 7125: train loss: 2.012760342040565e-05, val loss: 0.1372576504945755\n",
      "Epoch 7126: train loss: 2.9861514121876098e-05, val loss: 0.13667289912700653\n",
      "Epoch 7127: train loss: 3.2712327083572745e-05, val loss: 0.14066731929779053\n",
      "Epoch 7128: train loss: 3.070761158596724e-05, val loss: 0.13847868144512177\n",
      "Epoch 7129: train loss: 2.833170219673775e-05, val loss: 0.13938085734844208\n",
      "Epoch 7130: train loss: 1.839774449763354e-05, val loss: 0.14049585163593292\n",
      "Epoch 7131: train loss: 2.398143806203734e-05, val loss: 0.1391531080007553\n",
      "Epoch 7132: train loss: 1.5322408216889016e-05, val loss: 0.13816706836223602\n",
      "Epoch 7133: train loss: 6.070306881156284e-06, val loss: 0.13916698098182678\n",
      "Epoch 7134: train loss: 1.6244774087681435e-05, val loss: 0.13861234486103058\n",
      "Epoch 7135: train loss: 1.3426360055746045e-05, val loss: 0.1382584422826767\n",
      "Epoch 7136: train loss: 1.0506802937015891e-05, val loss: 0.14043818414211273\n",
      "Epoch 7137: train loss: 1.2561319636006374e-05, val loss: 0.13900868594646454\n",
      "Epoch 7138: train loss: 1.2504646292654797e-05, val loss: 0.13917486369609833\n",
      "Epoch 7139: train loss: 1.052456718753092e-05, val loss: 0.13952073454856873\n",
      "Epoch 7140: train loss: 7.158454536693171e-06, val loss: 0.13935677707195282\n",
      "Epoch 7141: train loss: 4.761528998642461e-06, val loss: 0.1389087587594986\n",
      "Epoch 7142: train loss: 7.053953595459461e-06, val loss: 0.1399925947189331\n",
      "Epoch 7143: train loss: 5.9241283452138305e-06, val loss: 0.13996802270412445\n",
      "Epoch 7144: train loss: 4.07323386752978e-06, val loss: 0.13864877820014954\n",
      "Epoch 7145: train loss: 4.950858055963181e-06, val loss: 0.1401684582233429\n",
      "Epoch 7146: train loss: 5.522644642041996e-06, val loss: 0.14009596407413483\n",
      "Epoch 7147: train loss: 6.816635959694395e-06, val loss: 0.14019612967967987\n",
      "Epoch 7148: train loss: 6.11501172897988e-06, val loss: 0.14005707204341888\n",
      "Epoch 7149: train loss: 5.979356956231641e-06, val loss: 0.13973359763622284\n",
      "Epoch 7150: train loss: 5.53370136913145e-06, val loss: 0.14020131528377533\n",
      "Epoch 7151: train loss: 4.404344963404583e-06, val loss: 0.14113068580627441\n",
      "Epoch 7152: train loss: 2.805788881232729e-06, val loss: 0.1404353678226471\n",
      "Epoch 7153: train loss: 2.4885621314751916e-06, val loss: 0.14046959578990936\n",
      "Epoch 7154: train loss: 4.38924098489224e-06, val loss: 0.14003677666187286\n",
      "Epoch 7155: train loss: 1.2005907592538279e-05, val loss: 0.14117924869060516\n",
      "Epoch 7156: train loss: 4.1491235606372356e-05, val loss: 0.14045356214046478\n",
      "Epoch 7157: train loss: 0.00011565630848053843, val loss: 0.1412362903356552\n",
      "Epoch 7158: train loss: 0.0002444713900331408, val loss: 0.14016063511371613\n",
      "Epoch 7159: train loss: 0.0005388166173361242, val loss: 0.1446988880634308\n",
      "Epoch 7160: train loss: 0.000789858226198703, val loss: 0.13989228010177612\n",
      "Epoch 7161: train loss: 0.0007413217681460083, val loss: 0.15343676507472992\n",
      "Epoch 7162: train loss: 0.0010289365891367197, val loss: 0.1388503611087799\n",
      "Epoch 7163: train loss: 0.002236525295302272, val loss: 0.147636279463768\n",
      "Epoch 7164: train loss: 0.0021095406264066696, val loss: 0.14255483448505402\n",
      "Epoch 7165: train loss: 0.0005659126327373087, val loss: 0.14213626086711884\n",
      "Epoch 7166: train loss: 0.0007179423701018095, val loss: 0.13982035219669342\n",
      "Epoch 7167: train loss: 0.0006989811081439257, val loss: 0.13384489715099335\n",
      "Epoch 7168: train loss: 0.000311034731566906, val loss: 0.1351063996553421\n",
      "Epoch 7169: train loss: 0.000476538494694978, val loss: 0.13483810424804688\n",
      "Epoch 7170: train loss: 0.0002105837338604033, val loss: 0.13194869458675385\n",
      "Epoch 7171: train loss: 0.0003353464126121253, val loss: 0.13149069249629974\n",
      "Epoch 7172: train loss: 0.00018886549514718354, val loss: 0.1301116943359375\n",
      "Epoch 7173: train loss: 0.0001982501707971096, val loss: 0.1273527592420578\n",
      "Epoch 7174: train loss: 0.0002639588783495128, val loss: 0.12678591907024384\n",
      "Epoch 7175: train loss: 0.00014219102740753442, val loss: 0.13013345003128052\n",
      "Epoch 7176: train loss: 0.0001815084833651781, val loss: 0.13097885251045227\n",
      "Epoch 7177: train loss: 0.00011543494474608451, val loss: 0.12833064794540405\n",
      "Epoch 7178: train loss: 0.00010707046749303117, val loss: 0.1267530769109726\n",
      "Epoch 7179: train loss: 0.00011837050260510296, val loss: 0.12761346995830536\n",
      "Epoch 7180: train loss: 7.040089985821396e-05, val loss: 0.12891702353954315\n",
      "Epoch 7181: train loss: 0.00010044389637187123, val loss: 0.1277216672897339\n",
      "Epoch 7182: train loss: 6.016012412146665e-05, val loss: 0.12743397057056427\n",
      "Epoch 7183: train loss: 6.351424235617742e-05, val loss: 0.12753282487392426\n",
      "Epoch 7184: train loss: 6.129330722615123e-05, val loss: 0.12765131890773773\n",
      "Epoch 7185: train loss: 4.3411870137788355e-05, val loss: 0.12687888741493225\n",
      "Epoch 7186: train loss: 3.8155048969201744e-05, val loss: 0.12677112221717834\n",
      "Epoch 7187: train loss: 4.0189675928559154e-05, val loss: 0.12694349884986877\n",
      "Epoch 7188: train loss: 3.620400093495846e-05, val loss: 0.12661239504814148\n",
      "Epoch 7189: train loss: 1.97376539290417e-05, val loss: 0.12686417996883392\n",
      "Epoch 7190: train loss: 3.628678678069264e-05, val loss: 0.127376526594162\n",
      "Epoch 7191: train loss: 2.5607863790355623e-05, val loss: 0.12763838469982147\n",
      "Epoch 7192: train loss: 1.85143780981889e-05, val loss: 0.12620821595191956\n",
      "Epoch 7193: train loss: 2.6333549612900242e-05, val loss: 0.1258636713027954\n",
      "Epoch 7194: train loss: 1.5424191587953828e-05, val loss: 0.12635374069213867\n",
      "Epoch 7195: train loss: 1.684161543380469e-05, val loss: 0.12691882252693176\n",
      "Epoch 7196: train loss: 1.4948415810067672e-05, val loss: 0.1264650970697403\n",
      "Epoch 7197: train loss: 1.1545353117980994e-05, val loss: 0.1265518218278885\n",
      "Epoch 7198: train loss: 1.2398932994983625e-05, val loss: 0.1270921677350998\n",
      "Epoch 7199: train loss: 9.51126821746584e-06, val loss: 0.12706902623176575\n",
      "Epoch 7200: train loss: 9.544598469801713e-06, val loss: 0.12718521058559418\n",
      "Epoch 7201: train loss: 9.225870599038899e-06, val loss: 0.12759122252464294\n",
      "Epoch 7202: train loss: 7.981631824804936e-06, val loss: 0.12796854972839355\n",
      "Epoch 7203: train loss: 6.847403255960671e-06, val loss: 0.12739001214504242\n",
      "Epoch 7204: train loss: 7.322334568016231e-06, val loss: 0.12716059386730194\n",
      "Epoch 7205: train loss: 5.8785394685401116e-06, val loss: 0.12733083963394165\n",
      "Epoch 7206: train loss: 4.506491677602753e-06, val loss: 0.1275091916322708\n",
      "Epoch 7207: train loss: 5.1641750360431615e-06, val loss: 0.12725210189819336\n",
      "Epoch 7208: train loss: 3.666589236672735e-06, val loss: 0.12726517021656036\n",
      "Epoch 7209: train loss: 3.4708316434262088e-06, val loss: 0.1274811327457428\n",
      "Epoch 7210: train loss: 4.110226200282341e-06, val loss: 0.1273772269487381\n",
      "Epoch 7211: train loss: 2.0148397652519634e-06, val loss: 0.12745003402233124\n",
      "Epoch 7212: train loss: 3.903904598701047e-06, val loss: 0.12751451134681702\n",
      "Epoch 7213: train loss: 2.2169608655531192e-06, val loss: 0.12764734029769897\n",
      "Epoch 7214: train loss: 2.491290388206835e-06, val loss: 0.1274338960647583\n",
      "Epoch 7215: train loss: 2.4696444143046392e-06, val loss: 0.1276468187570572\n",
      "Epoch 7216: train loss: 1.8621666413309867e-06, val loss: 0.127640500664711\n",
      "Epoch 7217: train loss: 2.086631411657436e-06, val loss: 0.12735308706760406\n",
      "Epoch 7218: train loss: 1.4845307987343404e-06, val loss: 0.12731853127479553\n",
      "Epoch 7219: train loss: 1.6433360769951832e-06, val loss: 0.12797656655311584\n",
      "Epoch 7220: train loss: 1.090741420739505e-06, val loss: 0.12822985649108887\n",
      "Epoch 7221: train loss: 1.2386446996970335e-06, val loss: 0.1280270665884018\n",
      "Epoch 7222: train loss: 1.054512267728569e-06, val loss: 0.12786446511745453\n",
      "Epoch 7223: train loss: 8.972591558631393e-07, val loss: 0.12788976728916168\n",
      "Epoch 7224: train loss: 1.019336764329637e-06, val loss: 0.1279616802930832\n",
      "Epoch 7225: train loss: 6.792682256673288e-07, val loss: 0.12794482707977295\n",
      "Epoch 7226: train loss: 9.143796546595695e-07, val loss: 0.128153994679451\n",
      "Epoch 7227: train loss: 5.844883617101004e-07, val loss: 0.12833721935749054\n",
      "Epoch 7228: train loss: 7.655424951735768e-07, val loss: 0.1284729689359665\n",
      "Epoch 7229: train loss: 5.647806915476394e-07, val loss: 0.12833304703235626\n",
      "Epoch 7230: train loss: 6.506813292617153e-07, val loss: 0.1284029334783554\n",
      "Epoch 7231: train loss: 4.6352056415344123e-07, val loss: 0.12842409312725067\n",
      "Epoch 7232: train loss: 6.053909373804345e-07, val loss: 0.12849105894565582\n",
      "Epoch 7233: train loss: 3.545883942024375e-07, val loss: 0.12841802835464478\n",
      "Epoch 7234: train loss: 5.105304126118426e-07, val loss: 0.12858371436595917\n",
      "Epoch 7235: train loss: 4.1270806150350836e-07, val loss: 0.12859246134757996\n",
      "Epoch 7236: train loss: 4.030055151815759e-07, val loss: 0.12870901823043823\n",
      "Epoch 7237: train loss: 4.935668016514683e-07, val loss: 0.1285591572523117\n",
      "Epoch 7238: train loss: 5.644477027999528e-07, val loss: 0.1288604587316513\n",
      "Epoch 7239: train loss: 7.694391115364851e-07, val loss: 0.12870380282402039\n",
      "Epoch 7240: train loss: 1.2056877949362388e-06, val loss: 0.1289720982313156\n",
      "Epoch 7241: train loss: 1.0746043699327856e-06, val loss: 0.12887679040431976\n",
      "Epoch 7242: train loss: 1.5700404674134916e-06, val loss: 0.12925565242767334\n",
      "Epoch 7243: train loss: 2.052169747912558e-06, val loss: 0.1285220831632614\n",
      "Epoch 7244: train loss: 3.1306881282944232e-06, val loss: 0.12924309074878693\n",
      "Epoch 7245: train loss: 3.81089603251894e-06, val loss: 0.12887437641620636\n",
      "Epoch 7246: train loss: 5.920682724536164e-06, val loss: 0.12968137860298157\n",
      "Epoch 7247: train loss: 8.450894711131696e-06, val loss: 0.12853024899959564\n",
      "Epoch 7248: train loss: 1.4693321645609103e-05, val loss: 0.13016429543495178\n",
      "Epoch 7249: train loss: 2.4751911041676067e-05, val loss: 0.1284741312265396\n",
      "Epoch 7250: train loss: 5.765051537309773e-05, val loss: 0.13157521188259125\n",
      "Epoch 7251: train loss: 0.00015638185141142458, val loss: 0.12792403995990753\n",
      "Epoch 7252: train loss: 0.0002880169195123017, val loss: 0.13203662633895874\n",
      "Epoch 7253: train loss: 0.00029514264315366745, val loss: 0.12913386523723602\n",
      "Epoch 7254: train loss: 0.00036757352063432336, val loss: 0.13502462208271027\n",
      "Epoch 7255: train loss: 0.0004536765336524695, val loss: 0.1281207948923111\n",
      "Epoch 7256: train loss: 0.00047808108502067626, val loss: 0.1379321813583374\n",
      "Epoch 7257: train loss: 0.0005304952501319349, val loss: 0.12861566245555878\n",
      "Epoch 7258: train loss: 0.00046468916116282344, val loss: 0.1365109533071518\n",
      "Epoch 7259: train loss: 0.0002751789870671928, val loss: 0.1334013193845749\n",
      "Epoch 7260: train loss: 0.0001553164329379797, val loss: 0.13581395149230957\n",
      "Epoch 7261: train loss: 4.8640711611369625e-05, val loss: 0.13641388714313507\n",
      "Epoch 7262: train loss: 3.61909915227443e-05, val loss: 0.1353537142276764\n",
      "Epoch 7263: train loss: 8.866038842825219e-05, val loss: 0.13936661183834076\n",
      "Epoch 7264: train loss: 0.00018264474056195468, val loss: 0.13610629737377167\n",
      "Epoch 7265: train loss: 0.000205703399842605, val loss: 0.1413850039243698\n",
      "Epoch 7266: train loss: 0.00017231218225788325, val loss: 0.1364416927099228\n",
      "Epoch 7267: train loss: 0.0001056272376445122, val loss: 0.1403614729642868\n",
      "Epoch 7268: train loss: 3.462925451458432e-05, val loss: 0.13964615762233734\n",
      "Epoch 7269: train loss: 2.1219422706053592e-05, val loss: 0.13707618415355682\n",
      "Epoch 7270: train loss: 4.121705569559708e-05, val loss: 0.14124652743339539\n",
      "Epoch 7271: train loss: 7.619684038218111e-05, val loss: 0.13750898838043213\n",
      "Epoch 7272: train loss: 9.59343378781341e-05, val loss: 0.14082081615924835\n",
      "Epoch 7273: train loss: 8.515160152455792e-05, val loss: 0.13842810690402985\n",
      "Epoch 7274: train loss: 5.183900793781504e-05, val loss: 0.13866093754768372\n",
      "Epoch 7275: train loss: 1.8087230273522437e-05, val loss: 0.1398567259311676\n",
      "Epoch 7276: train loss: 1.4820009710092563e-05, val loss: 0.1382359266281128\n",
      "Epoch 7277: train loss: 2.39157543546753e-05, val loss: 0.14003562927246094\n",
      "Epoch 7278: train loss: 3.790951814153232e-05, val loss: 0.13783247768878937\n",
      "Epoch 7279: train loss: 5.051754487794824e-05, val loss: 0.14083249866962433\n",
      "Epoch 7280: train loss: 4.3773081415565684e-05, val loss: 0.13778570294380188\n",
      "Epoch 7281: train loss: 2.8513768484117463e-05, val loss: 0.1398731917142868\n",
      "Epoch 7282: train loss: 1.0121650120709091e-05, val loss: 0.14013557136058807\n",
      "Epoch 7283: train loss: 4.872541467193514e-06, val loss: 0.1381407082080841\n",
      "Epoch 7284: train loss: 1.3583554391516373e-05, val loss: 0.1410248577594757\n",
      "Epoch 7285: train loss: 2.7043535737902857e-05, val loss: 0.137693852186203\n",
      "Epoch 7286: train loss: 4.293722668080591e-05, val loss: 0.1418663114309311\n",
      "Epoch 7287: train loss: 6.577529711648822e-05, val loss: 0.13642750680446625\n",
      "Epoch 7288: train loss: 0.00013303682499099523, val loss: 0.14518848061561584\n",
      "Epoch 7289: train loss: 0.0003161807544529438, val loss: 0.13322821259498596\n",
      "Epoch 7290: train loss: 0.0008442547987215221, val loss: 0.15117383003234863\n",
      "Epoch 7291: train loss: 0.0018410662887617946, val loss: 0.12805889546871185\n",
      "Epoch 7292: train loss: 0.0029370298143476248, val loss: 0.134402796626091\n",
      "Epoch 7293: train loss: 0.001398306107148528, val loss: 0.1382259577512741\n",
      "Epoch 7294: train loss: 0.0006359930848702788, val loss: 0.1214580088853836\n",
      "Epoch 7295: train loss: 0.0010954776080325246, val loss: 0.11188938468694687\n",
      "Epoch 7296: train loss: 0.0005993571830913424, val loss: 0.11778409779071808\n",
      "Epoch 7297: train loss: 0.0006740399985574186, val loss: 0.11613227427005768\n",
      "Epoch 7298: train loss: 0.0003927455982193351, val loss: 0.1040329560637474\n",
      "Epoch 7299: train loss: 0.00036859812098555267, val loss: 0.10022816807031631\n",
      "Epoch 7300: train loss: 0.0003756969526875764, val loss: 0.10661623626947403\n",
      "Epoch 7301: train loss: 0.0002531812933739275, val loss: 0.10460598766803741\n",
      "Epoch 7302: train loss: 0.00027877234970219433, val loss: 0.09884343296289444\n",
      "Epoch 7303: train loss: 0.0002175983099732548, val loss: 0.0971018597483635\n",
      "Epoch 7304: train loss: 0.0001556430506752804, val loss: 0.09950987994670868\n",
      "Epoch 7305: train loss: 0.0001834257273003459, val loss: 0.09923218190670013\n",
      "Epoch 7306: train loss: 0.00011242020264035091, val loss: 0.0967150330543518\n",
      "Epoch 7307: train loss: 0.00015248896670527756, val loss: 0.09720750898122787\n",
      "Epoch 7308: train loss: 0.00010119786747964099, val loss: 0.09714747965335846\n",
      "Epoch 7309: train loss: 0.0001078811619663611, val loss: 0.09573181718587875\n",
      "Epoch 7310: train loss: 7.20812677172944e-05, val loss: 0.09519530087709427\n",
      "Epoch 7311: train loss: 7.312076195375994e-05, val loss: 0.09685877710580826\n",
      "Epoch 7312: train loss: 7.507263217121363e-05, val loss: 0.09757337719202042\n",
      "Epoch 7313: train loss: 5.573599992203526e-05, val loss: 0.09562370181083679\n",
      "Epoch 7314: train loss: 5.9832022088812664e-05, val loss: 0.09583469480276108\n",
      "Epoch 7315: train loss: 5.6052798754535615e-05, val loss: 0.0974600687623024\n",
      "Epoch 7316: train loss: 3.2863037631614134e-05, val loss: 0.09753579646348953\n",
      "Epoch 7317: train loss: 3.82343350793235e-05, val loss: 0.09618931263685226\n",
      "Epoch 7318: train loss: 3.899761941283941e-05, val loss: 0.09635020792484283\n",
      "Epoch 7319: train loss: 2.770743412838783e-05, val loss: 0.09806614369153976\n",
      "Epoch 7320: train loss: 3.082786497543566e-05, val loss: 0.097403384745121\n",
      "Epoch 7321: train loss: 2.98022641800344e-05, val loss: 0.09619808197021484\n",
      "Epoch 7322: train loss: 2.033723285421729e-05, val loss: 0.0964953675866127\n",
      "Epoch 7323: train loss: 1.662599061091896e-05, val loss: 0.09763377159833908\n",
      "Epoch 7324: train loss: 2.125465289282147e-05, val loss: 0.09784664213657379\n",
      "Epoch 7325: train loss: 1.4817748706263956e-05, val loss: 0.09763674437999725\n",
      "Epoch 7326: train loss: 1.6161504390765913e-05, val loss: 0.09852801263332367\n",
      "Epoch 7327: train loss: 1.3815174497722182e-05, val loss: 0.09853609651327133\n",
      "Epoch 7328: train loss: 1.38017430799664e-05, val loss: 0.09794507175683975\n",
      "Epoch 7329: train loss: 9.269125257560518e-06, val loss: 0.09792115539312363\n",
      "Epoch 7330: train loss: 8.317019819514826e-06, val loss: 0.09858308732509613\n",
      "Epoch 7331: train loss: 1.0703728548833169e-05, val loss: 0.09862621128559113\n",
      "Epoch 7332: train loss: 6.771121661586221e-06, val loss: 0.0980725884437561\n",
      "Epoch 7333: train loss: 8.070924195635598e-06, val loss: 0.09868379682302475\n",
      "Epoch 7334: train loss: 8.134103154588956e-06, val loss: 0.099217489361763\n",
      "Epoch 7335: train loss: 4.659866135625634e-06, val loss: 0.09905469417572021\n",
      "Epoch 7336: train loss: 5.488521310326178e-06, val loss: 0.0989845022559166\n",
      "Epoch 7337: train loss: 5.213974418438738e-06, val loss: 0.09964068233966827\n",
      "Epoch 7338: train loss: 3.6399146665644366e-06, val loss: 0.09985817968845367\n",
      "Epoch 7339: train loss: 4.520109087025048e-06, val loss: 0.09914549440145493\n",
      "Epoch 7340: train loss: 4.692358743341174e-06, val loss: 0.09937413781881332\n",
      "Epoch 7341: train loss: 2.6375776087661507e-06, val loss: 0.10016918182373047\n",
      "Epoch 7342: train loss: 3.7943045754218474e-06, val loss: 0.1004272922873497\n",
      "Epoch 7343: train loss: 2.3418849650624907e-06, val loss: 0.10017412900924683\n",
      "Epoch 7344: train loss: 2.308328930666903e-06, val loss: 0.1004374623298645\n",
      "Epoch 7345: train loss: 2.517833081583376e-06, val loss: 0.10098934173583984\n",
      "Epoch 7346: train loss: 2.0644802134484053e-06, val loss: 0.10090728104114532\n",
      "Epoch 7347: train loss: 2.184233608204522e-06, val loss: 0.10100753605365753\n",
      "Epoch 7348: train loss: 1.7374935623593046e-06, val loss: 0.10133583843708038\n",
      "Epoch 7349: train loss: 1.9142198652843945e-06, val loss: 0.10164090245962143\n",
      "Epoch 7350: train loss: 9.076105698113679e-07, val loss: 0.1017034575343132\n",
      "Epoch 7351: train loss: 1.6983293562589097e-06, val loss: 0.1018393412232399\n",
      "Epoch 7352: train loss: 8.513947591382021e-07, val loss: 0.10196208953857422\n",
      "Epoch 7353: train loss: 1.251866251550382e-06, val loss: 0.10194047540426254\n",
      "Epoch 7354: train loss: 1.1516209497131058e-06, val loss: 0.10225828737020493\n",
      "Epoch 7355: train loss: 8.474190167362394e-07, val loss: 0.10246225446462631\n",
      "Epoch 7356: train loss: 1.0174647968597128e-06, val loss: 0.10259091854095459\n",
      "Epoch 7357: train loss: 7.446702738889144e-07, val loss: 0.10268057882785797\n",
      "Epoch 7358: train loss: 5.941575977885805e-07, val loss: 0.10280535370111465\n",
      "Epoch 7359: train loss: 7.432114443872706e-07, val loss: 0.1029047966003418\n",
      "Epoch 7360: train loss: 4.845436478717602e-07, val loss: 0.10311385244131088\n",
      "Epoch 7361: train loss: 6.320219085864665e-07, val loss: 0.10337819159030914\n",
      "Epoch 7362: train loss: 4.958832846568839e-07, val loss: 0.10332798957824707\n",
      "Epoch 7363: train loss: 5.937632181485242e-07, val loss: 0.10342253744602203\n",
      "Epoch 7364: train loss: 3.489078039820015e-07, val loss: 0.10358051210641861\n",
      "Epoch 7365: train loss: 4.769341899191204e-07, val loss: 0.10381259769201279\n",
      "Epoch 7366: train loss: 3.555050795966963e-07, val loss: 0.10393968969583511\n",
      "Epoch 7367: train loss: 3.3292386092398374e-07, val loss: 0.10391364246606827\n",
      "Epoch 7368: train loss: 2.1714012632401136e-07, val loss: 0.10402707010507584\n",
      "Epoch 7369: train loss: 4.209938992971729e-07, val loss: 0.10425477474927902\n",
      "Epoch 7370: train loss: 3.070086904699565e-07, val loss: 0.10438168048858643\n",
      "Epoch 7371: train loss: 2.0992132476749248e-07, val loss: 0.10443063825368881\n",
      "Epoch 7372: train loss: 2.249078931981785e-07, val loss: 0.10468337684869766\n",
      "Epoch 7373: train loss: 3.1698829161541653e-07, val loss: 0.1047033816576004\n",
      "Epoch 7374: train loss: 2.5412401782887173e-07, val loss: 0.10486400127410889\n",
      "Epoch 7375: train loss: 2.813903563492204e-07, val loss: 0.1050586923956871\n",
      "Epoch 7376: train loss: 4.1252781102230074e-07, val loss: 0.10516398400068283\n",
      "Epoch 7377: train loss: 7.283574063876586e-07, val loss: 0.10527070611715317\n",
      "Epoch 7378: train loss: 1.4905963325873017e-06, val loss: 0.10549721866846085\n",
      "Epoch 7379: train loss: 3.713435035024304e-06, val loss: 0.10555815696716309\n",
      "Epoch 7380: train loss: 1.1248114788031671e-05, val loss: 0.10564728081226349\n",
      "Epoch 7381: train loss: 3.723725603776984e-05, val loss: 0.10598351061344147\n",
      "Epoch 7382: train loss: 0.00011294432624708861, val loss: 0.1058635339140892\n",
      "Epoch 7383: train loss: 0.00028316982206888497, val loss: 0.10693762451410294\n",
      "Epoch 7384: train loss: 0.00038290012162178755, val loss: 0.10602747648954391\n",
      "Epoch 7385: train loss: 0.000243943024543114, val loss: 0.107665054500103\n",
      "Epoch 7386: train loss: 0.00010590611782390624, val loss: 0.107813261449337\n",
      "Epoch 7387: train loss: 0.00011940307740587741, val loss: 0.10866843909025192\n",
      "Epoch 7388: train loss: 0.00011978959810221568, val loss: 0.10891083627939224\n",
      "Epoch 7389: train loss: 9.743838745635003e-05, val loss: 0.10939931869506836\n",
      "Epoch 7390: train loss: 6.348954775603488e-05, val loss: 0.11008166521787643\n",
      "Epoch 7391: train loss: 9.424061863683164e-05, val loss: 0.1109791174530983\n",
      "Epoch 7392: train loss: 6.27041154075414e-05, val loss: 0.1094125285744667\n",
      "Epoch 7393: train loss: 7.041444041533396e-05, val loss: 0.11233756691217422\n",
      "Epoch 7394: train loss: 4.6856890548951924e-05, val loss: 0.111052505671978\n",
      "Epoch 7395: train loss: 4.365177301224321e-05, val loss: 0.11054662615060806\n",
      "Epoch 7396: train loss: 4.1952993342420086e-05, val loss: 0.11202918738126755\n",
      "Epoch 7397: train loss: 3.630407445598394e-05, val loss: 0.11157534271478653\n",
      "Epoch 7398: train loss: 2.82342080026865e-05, val loss: 0.11184263229370117\n",
      "Epoch 7399: train loss: 2.4757333449088037e-05, val loss: 0.11214850097894669\n",
      "Epoch 7400: train loss: 3.183216176694259e-05, val loss: 0.11167407035827637\n",
      "Epoch 7401: train loss: 2.1490917788469233e-05, val loss: 0.11228332668542862\n",
      "Epoch 7402: train loss: 2.413631409581285e-05, val loss: 0.11319636553525925\n",
      "Epoch 7403: train loss: 1.967673779290635e-05, val loss: 0.11158286780118942\n",
      "Epoch 7404: train loss: 2.2581585653824732e-05, val loss: 0.11160848289728165\n",
      "Epoch 7405: train loss: 1.7508104065200314e-05, val loss: 0.11242196708917618\n",
      "Epoch 7406: train loss: 1.8163440472562797e-05, val loss: 0.11322151869535446\n",
      "Epoch 7407: train loss: 1.1069551874243189e-05, val loss: 0.1123293787240982\n",
      "Epoch 7408: train loss: 1.3531426702684257e-05, val loss: 0.11317577213048935\n",
      "Epoch 7409: train loss: 9.858581506705377e-06, val loss: 0.11337907612323761\n",
      "Epoch 7410: train loss: 1.134197555074934e-05, val loss: 0.11338617652654648\n",
      "Epoch 7411: train loss: 1.124790924222907e-05, val loss: 0.11301236599683762\n",
      "Epoch 7412: train loss: 1.0201657460129354e-05, val loss: 0.1132349967956543\n",
      "Epoch 7413: train loss: 8.67285780259408e-06, val loss: 0.1139531284570694\n",
      "Epoch 7414: train loss: 8.144899766193703e-06, val loss: 0.1134277805685997\n",
      "Epoch 7415: train loss: 6.460402346419869e-06, val loss: 0.11365987360477448\n",
      "Epoch 7416: train loss: 6.744904112565564e-06, val loss: 0.11424638330936432\n",
      "Epoch 7417: train loss: 6.360472980304621e-06, val loss: 0.1144203171133995\n",
      "Epoch 7418: train loss: 5.56874374524341e-06, val loss: 0.11440104246139526\n",
      "Epoch 7419: train loss: 6.371119525283575e-06, val loss: 0.11459467560052872\n",
      "Epoch 7420: train loss: 6.4409664446429815e-06, val loss: 0.11468677967786789\n",
      "Epoch 7421: train loss: 1.0114250471815467e-05, val loss: 0.11427786201238632\n",
      "Epoch 7422: train loss: 1.8121809262083843e-05, val loss: 0.11579886823892593\n",
      "Epoch 7423: train loss: 4.6737502998439595e-05, val loss: 0.11314908415079117\n",
      "Epoch 7424: train loss: 0.0001365452044410631, val loss: 0.11875293403863907\n",
      "Epoch 7425: train loss: 0.00039858761010691524, val loss: 0.10931914299726486\n",
      "Epoch 7426: train loss: 0.0012085167691111565, val loss: 0.12348777055740356\n",
      "Epoch 7427: train loss: 0.0022614127956330776, val loss: 0.11330889910459518\n",
      "Epoch 7428: train loss: 0.002192178275436163, val loss: 0.12024176120758057\n",
      "Epoch 7429: train loss: 0.00025307468604296446, val loss: 0.13515371084213257\n",
      "Epoch 7430: train loss: 0.0010919837513938546, val loss: 0.12826718389987946\n",
      "Epoch 7431: train loss: 0.0007252935320138931, val loss: 0.1191975399851799\n",
      "Epoch 7432: train loss: 0.00048823171528056264, val loss: 0.12710130214691162\n",
      "Epoch 7433: train loss: 0.00044861738570034504, val loss: 0.1356227844953537\n",
      "Epoch 7434: train loss: 0.00043753793579526246, val loss: 0.12832529842853546\n",
      "Epoch 7435: train loss: 0.0002522087888792157, val loss: 0.11951223760843277\n",
      "Epoch 7436: train loss: 0.00036607467336580157, val loss: 0.12634290754795074\n",
      "Epoch 7437: train loss: 0.00015147471276577562, val loss: 0.13506975769996643\n",
      "Epoch 7438: train loss: 0.00026758480817079544, val loss: 0.13116423785686493\n",
      "Epoch 7439: train loss: 0.00012039209104841575, val loss: 0.12561745941638947\n",
      "Epoch 7440: train loss: 0.0001800287136575207, val loss: 0.12712709605693817\n",
      "Epoch 7441: train loss: 0.00011977075337199494, val loss: 0.1290215700864792\n",
      "Epoch 7442: train loss: 0.00010536509216763079, val loss: 0.1273864358663559\n",
      "Epoch 7443: train loss: 0.00010106022818945348, val loss: 0.12780345976352692\n",
      "Epoch 7444: train loss: 6.420436693588272e-05, val loss: 0.12798695266246796\n",
      "Epoch 7445: train loss: 9.292709000874311e-05, val loss: 0.12716227769851685\n",
      "Epoch 7446: train loss: 4.602834451361559e-05, val loss: 0.12568174302577972\n",
      "Epoch 7447: train loss: 6.036877675796859e-05, val loss: 0.1266329437494278\n",
      "Epoch 7448: train loss: 5.027464067097753e-05, val loss: 0.12731951475143433\n",
      "Epoch 7449: train loss: 2.8579115678439848e-05, val loss: 0.12714818120002747\n",
      "Epoch 7450: train loss: 3.977834421675652e-05, val loss: 0.1271955668926239\n",
      "Epoch 7451: train loss: 3.2141091651283205e-05, val loss: 0.12757089734077454\n",
      "Epoch 7452: train loss: 2.1884468878852203e-05, val loss: 0.12696053087711334\n",
      "Epoch 7453: train loss: 3.210170689271763e-05, val loss: 0.12552566826343536\n",
      "Epoch 7454: train loss: 2.1437830582726747e-05, val loss: 0.126259446144104\n",
      "Epoch 7455: train loss: 2.3206584955914877e-05, val loss: 0.12750859558582306\n",
      "Epoch 7456: train loss: 2.380954902037047e-05, val loss: 0.12801729142665863\n",
      "Epoch 7457: train loss: 1.6888896425371058e-05, val loss: 0.12703512609004974\n",
      "Epoch 7458: train loss: 1.9915109078283422e-05, val loss: 0.1271442174911499\n",
      "Epoch 7459: train loss: 1.3661069715453777e-05, val loss: 0.1273641139268875\n",
      "Epoch 7460: train loss: 1.452838387194788e-05, val loss: 0.12784335017204285\n",
      "Epoch 7461: train loss: 9.929034604283515e-06, val loss: 0.12786142528057098\n",
      "Epoch 7462: train loss: 1.0994745935022365e-05, val loss: 0.12787671387195587\n",
      "Epoch 7463: train loss: 8.325757335114758e-06, val loss: 0.12775219976902008\n",
      "Epoch 7464: train loss: 6.527074219775386e-06, val loss: 0.1281612664461136\n",
      "Epoch 7465: train loss: 8.103909749479499e-06, val loss: 0.1287933588027954\n",
      "Epoch 7466: train loss: 5.259657427814091e-06, val loss: 0.12845821678638458\n",
      "Epoch 7467: train loss: 5.836989203089615e-06, val loss: 0.12829576432704926\n",
      "Epoch 7468: train loss: 5.475186753756134e-06, val loss: 0.12862586975097656\n",
      "Epoch 7469: train loss: 5.572815098275896e-06, val loss: 0.12926487624645233\n",
      "Epoch 7470: train loss: 5.123968549014535e-06, val loss: 0.1290738433599472\n",
      "Epoch 7471: train loss: 5.6140247579605784e-06, val loss: 0.12944276630878448\n",
      "Epoch 7472: train loss: 4.897529834124725e-06, val loss: 0.12952077388763428\n",
      "Epoch 7473: train loss: 4.561069545161445e-06, val loss: 0.12962262332439423\n",
      "Epoch 7474: train loss: 4.816190084966365e-06, val loss: 0.1292545348405838\n",
      "Epoch 7475: train loss: 3.589566858863691e-06, val loss: 0.12959276139736176\n",
      "Epoch 7476: train loss: 3.981174359068973e-06, val loss: 0.12951207160949707\n",
      "Epoch 7477: train loss: 2.9756702133454382e-06, val loss: 0.12947306036949158\n",
      "Epoch 7478: train loss: 3.0467122087429743e-06, val loss: 0.12926094233989716\n",
      "Epoch 7479: train loss: 2.4182434117392404e-06, val loss: 0.12969498336315155\n",
      "Epoch 7480: train loss: 2.271847051815712e-06, val loss: 0.1298273354768753\n",
      "Epoch 7481: train loss: 1.8813434508047067e-06, val loss: 0.12985362112522125\n",
      "Epoch 7482: train loss: 1.7412984334441717e-06, val loss: 0.12952008843421936\n",
      "Epoch 7483: train loss: 1.3442032695820672e-06, val loss: 0.1296609491109848\n",
      "Epoch 7484: train loss: 1.4458781834036927e-06, val loss: 0.12987756729125977\n",
      "Epoch 7485: train loss: 1.0676133115339326e-06, val loss: 0.129939466714859\n",
      "Epoch 7486: train loss: 9.834396905716858e-07, val loss: 0.12957511842250824\n",
      "Epoch 7487: train loss: 1.013403789329459e-06, val loss: 0.1296883076429367\n",
      "Epoch 7488: train loss: 6.682821549475193e-07, val loss: 0.12999393045902252\n",
      "Epoch 7489: train loss: 9.01782527762407e-07, val loss: 0.1300528347492218\n",
      "Epoch 7490: train loss: 5.544711711991113e-07, val loss: 0.12979727983474731\n",
      "Epoch 7491: train loss: 7.162061024246213e-07, val loss: 0.129924938082695\n",
      "Epoch 7492: train loss: 4.7192202146106865e-07, val loss: 0.13007889688014984\n",
      "Epoch 7493: train loss: 7.154324066505069e-07, val loss: 0.13013681769371033\n",
      "Epoch 7494: train loss: 4.505640163188218e-07, val loss: 0.12991124391555786\n",
      "Epoch 7495: train loss: 5.573886028287234e-07, val loss: 0.13001620769500732\n",
      "Epoch 7496: train loss: 5.298169867273828e-07, val loss: 0.13014017045497894\n",
      "Epoch 7497: train loss: 5.042950306233251e-07, val loss: 0.13013993203639984\n",
      "Epoch 7498: train loss: 5.192340495341341e-07, val loss: 0.13006539642810822\n",
      "Epoch 7499: train loss: 6.592733257093641e-07, val loss: 0.1304951012134552\n",
      "Epoch 7500: train loss: 7.616108632646501e-07, val loss: 0.13025341928005219\n",
      "Epoch 7501: train loss: 1.1095545460193534e-06, val loss: 0.13032186031341553\n",
      "Epoch 7502: train loss: 1.1235325700909016e-06, val loss: 0.13025034964084625\n",
      "Epoch 7503: train loss: 1.940170250236406e-06, val loss: 0.13055305182933807\n",
      "Epoch 7504: train loss: 2.311672005816945e-06, val loss: 0.13025186955928802\n",
      "Epoch 7505: train loss: 3.937600467907032e-06, val loss: 0.1307576447725296\n",
      "Epoch 7506: train loss: 5.794319349661237e-06, val loss: 0.13012924790382385\n",
      "Epoch 7507: train loss: 1.0256866517011076e-05, val loss: 0.13103561103343964\n",
      "Epoch 7508: train loss: 1.6568110368098132e-05, val loss: 0.13010148704051971\n",
      "Epoch 7509: train loss: 3.0157048968249e-05, val loss: 0.1311575025320053\n",
      "Epoch 7510: train loss: 5.277869786368683e-05, val loss: 0.12990155816078186\n",
      "Epoch 7511: train loss: 9.870952635537833e-05, val loss: 0.1320701390504837\n",
      "Epoch 7512: train loss: 0.00017691192624624819, val loss: 0.12984052300453186\n",
      "Epoch 7513: train loss: 0.00032544333953410387, val loss: 0.13319292664527893\n",
      "Epoch 7514: train loss: 0.0005394229665398598, val loss: 0.12924714386463165\n",
      "Epoch 7515: train loss: 0.0009124082862399518, val loss: 0.1325726956129074\n",
      "Epoch 7516: train loss: 0.0012765841092914343, val loss: 0.12956123054027557\n",
      "Epoch 7517: train loss: 0.0015570006798952818, val loss: 0.13340266048908234\n",
      "Epoch 7518: train loss: 0.0011735159205272794, val loss: 0.133136585354805\n",
      "Epoch 7519: train loss: 0.0005281846970319748, val loss: 0.1347566843032837\n",
      "Epoch 7520: train loss: 0.00010359190491726622, val loss: 0.13232944905757904\n",
      "Epoch 7521: train loss: 0.0002165186742786318, val loss: 0.13461600244045258\n",
      "Epoch 7522: train loss: 0.00046814887900836766, val loss: 0.1343473494052887\n",
      "Epoch 7523: train loss: 0.0005107519100420177, val loss: 0.12917190790176392\n",
      "Epoch 7524: train loss: 0.0004160445823799819, val loss: 0.13640283048152924\n",
      "Epoch 7525: train loss: 0.00020534760551527143, val loss: 0.12968230247497559\n",
      "Epoch 7526: train loss: 7.508440467063338e-05, val loss: 0.13002455234527588\n",
      "Epoch 7527: train loss: 0.00015583325875923038, val loss: 0.13707426190376282\n",
      "Epoch 7528: train loss: 0.0003225236723665148, val loss: 0.12756197154521942\n",
      "Epoch 7529: train loss: 0.00025229540187865496, val loss: 0.13049136102199554\n",
      "Epoch 7530: train loss: 5.604332545772195e-05, val loss: 0.13309799134731293\n",
      "Epoch 7531: train loss: 6.878918065922335e-05, val loss: 0.12756375968456268\n",
      "Epoch 7532: train loss: 0.0001282568700844422, val loss: 0.1303083449602127\n",
      "Epoch 7533: train loss: 0.00014990645286161453, val loss: 0.1305549591779709\n",
      "Epoch 7534: train loss: 0.00014455438940785825, val loss: 0.12951771914958954\n",
      "Epoch 7535: train loss: 3.9386472053593025e-05, val loss: 0.12941694259643555\n",
      "Epoch 7536: train loss: 2.093381954182405e-05, val loss: 0.13035793602466583\n",
      "Epoch 7537: train loss: 9.811220661504194e-05, val loss: 0.13046899437904358\n",
      "Epoch 7538: train loss: 9.717673674458638e-05, val loss: 0.12851832807064056\n",
      "Epoch 7539: train loss: 5.5279066145885736e-05, val loss: 0.13115845620632172\n",
      "Epoch 7540: train loss: 2.412231515336316e-05, val loss: 0.1297180950641632\n",
      "Epoch 7541: train loss: 2.267390300403349e-05, val loss: 0.128143310546875\n",
      "Epoch 7542: train loss: 5.811371374875307e-05, val loss: 0.13130997121334076\n",
      "Epoch 7543: train loss: 5.9798621805384755e-05, val loss: 0.12890313565731049\n",
      "Epoch 7544: train loss: 2.4216338715632446e-05, val loss: 0.12887144088745117\n",
      "Epoch 7545: train loss: 9.953084372682497e-06, val loss: 0.1302575170993805\n",
      "Epoch 7546: train loss: 2.299974948982708e-05, val loss: 0.12836168706417084\n",
      "Epoch 7547: train loss: 3.813645162153989e-05, val loss: 0.1295676976442337\n",
      "Epoch 7548: train loss: 3.3258333132835105e-05, val loss: 0.12915478646755219\n",
      "Epoch 7549: train loss: 1.2580602742673364e-05, val loss: 0.1286201924085617\n",
      "Epoch 7550: train loss: 5.132210390002001e-06, val loss: 0.12918397784233093\n",
      "Epoch 7551: train loss: 1.9094448362011462e-05, val loss: 0.12849242985248566\n",
      "Epoch 7552: train loss: 2.453973502269946e-05, val loss: 0.12918858230113983\n",
      "Epoch 7553: train loss: 1.5417475879075937e-05, val loss: 0.1285804957151413\n",
      "Epoch 7554: train loss: 9.920770025928505e-06, val loss: 0.12886714935302734\n",
      "Epoch 7555: train loss: 6.60213163428125e-06, val loss: 0.12927591800689697\n",
      "Epoch 7556: train loss: 7.989943696884438e-06, val loss: 0.12839114665985107\n",
      "Epoch 7557: train loss: 1.4858905160508584e-05, val loss: 0.12931135296821594\n",
      "Epoch 7558: train loss: 1.478213380323723e-05, val loss: 0.1287764608860016\n",
      "Epoch 7559: train loss: 8.682131920068059e-06, val loss: 0.12887652218341827\n",
      "Epoch 7560: train loss: 4.781782081408892e-06, val loss: 0.12953826785087585\n",
      "Epoch 7561: train loss: 5.267909273243276e-06, val loss: 0.12831459939479828\n",
      "Epoch 7562: train loss: 7.497524620703189e-06, val loss: 0.12981495261192322\n",
      "Epoch 7563: train loss: 6.446496172429761e-06, val loss: 0.12921340763568878\n",
      "Epoch 7564: train loss: 6.0298366406641435e-06, val loss: 0.12929759919643402\n",
      "Epoch 7565: train loss: 8.671023351780605e-06, val loss: 0.13083584606647491\n",
      "Epoch 7566: train loss: 1.4500793440674897e-05, val loss: 0.12870898842811584\n",
      "Epoch 7567: train loss: 2.381593731115572e-05, val loss: 0.1314856857061386\n",
      "Epoch 7568: train loss: 3.7673413316952065e-05, val loss: 0.12866349518299103\n",
      "Epoch 7569: train loss: 7.056895265122876e-05, val loss: 0.1328025609254837\n",
      "Epoch 7570: train loss: 0.0001419554027961567, val loss: 0.12768876552581787\n",
      "Epoch 7571: train loss: 0.00032826405367814004, val loss: 0.1350921243429184\n",
      "Epoch 7572: train loss: 0.0006535901338793337, val loss: 0.1261948198080063\n",
      "Epoch 7573: train loss: 0.0012003518640995026, val loss: 0.13566811382770538\n",
      "Epoch 7574: train loss: 0.0011849218280985951, val loss: 0.1316349059343338\n",
      "Epoch 7575: train loss: 0.0006064394256100059, val loss: 0.13012276589870453\n",
      "Epoch 7576: train loss: 0.00018385400471743196, val loss: 0.13129466772079468\n",
      "Epoch 7577: train loss: 0.000506254902575165, val loss: 0.12808717787265778\n",
      "Epoch 7578: train loss: 0.00030172363040037453, val loss: 0.12215639650821686\n",
      "Epoch 7579: train loss: 0.00020147550094407052, val loss: 0.12312190979719162\n",
      "Epoch 7580: train loss: 0.00028315288363955915, val loss: 0.12233757972717285\n",
      "Epoch 7581: train loss: 0.0001815104333218187, val loss: 0.12154882401227951\n",
      "Epoch 7582: train loss: 0.00010724955063778907, val loss: 0.11854054033756256\n",
      "Epoch 7583: train loss: 0.0001746436901157722, val loss: 0.1201205626130104\n",
      "Epoch 7584: train loss: 7.123527757357806e-05, val loss: 0.12298303842544556\n",
      "Epoch 7585: train loss: 0.0001689617201918736, val loss: 0.1181207224726677\n",
      "Epoch 7586: train loss: 5.717313979403116e-05, val loss: 0.11768360435962677\n",
      "Epoch 7587: train loss: 8.740996418055147e-05, val loss: 0.1200694590806961\n",
      "Epoch 7588: train loss: 5.495182631420903e-05, val loss: 0.11870019882917404\n",
      "Epoch 7589: train loss: 6.911330274306238e-05, val loss: 0.11708582937717438\n",
      "Epoch 7590: train loss: 6.311031029326841e-05, val loss: 0.11678167432546616\n",
      "Epoch 7591: train loss: 5.0830261898227036e-05, val loss: 0.11840974539518356\n",
      "Epoch 7592: train loss: 3.584066143957898e-05, val loss: 0.11755476146936417\n",
      "Epoch 7593: train loss: 3.466788257355802e-05, val loss: 0.11611192673444748\n",
      "Epoch 7594: train loss: 4.276210529496893e-05, val loss: 0.11757423728704453\n",
      "Epoch 7595: train loss: 3.057363210245967e-05, val loss: 0.1172986775636673\n",
      "Epoch 7596: train loss: 3.2612981158308685e-05, val loss: 0.11645066738128662\n",
      "Epoch 7597: train loss: 1.881583193608094e-05, val loss: 0.11609115451574326\n",
      "Epoch 7598: train loss: 2.073199357255362e-05, val loss: 0.11670207232236862\n",
      "Epoch 7599: train loss: 2.529453377064783e-05, val loss: 0.11782874912023544\n",
      "Epoch 7600: train loss: 1.9768478523474187e-05, val loss: 0.11638970673084259\n",
      "Epoch 7601: train loss: 1.6204612620640546e-05, val loss: 0.11628599464893341\n",
      "Epoch 7602: train loss: 1.5249275747919455e-05, val loss: 0.11734100431203842\n",
      "Epoch 7603: train loss: 9.478106221649796e-06, val loss: 0.11716754734516144\n",
      "Epoch 7604: train loss: 1.6973701349343173e-05, val loss: 0.11711368709802628\n",
      "Epoch 7605: train loss: 1.1371316759323236e-05, val loss: 0.11720102280378342\n",
      "Epoch 7606: train loss: 1.028472343023168e-05, val loss: 0.11778634041547775\n",
      "Epoch 7607: train loss: 8.683454325364437e-06, val loss: 0.11743489652872086\n",
      "Epoch 7608: train loss: 6.662551186309429e-06, val loss: 0.11709728091955185\n",
      "Epoch 7609: train loss: 9.530746865493711e-06, val loss: 0.11776985228061676\n",
      "Epoch 7610: train loss: 6.741095148754539e-06, val loss: 0.11740273237228394\n",
      "Epoch 7611: train loss: 7.731205187155865e-06, val loss: 0.1177687793970108\n",
      "Epoch 7612: train loss: 3.5810596727969823e-06, val loss: 0.11782260984182358\n",
      "Epoch 7613: train loss: 5.792327101517003e-06, val loss: 0.11769671738147736\n",
      "Epoch 7614: train loss: 4.341807652963325e-06, val loss: 0.11865273863077164\n",
      "Epoch 7615: train loss: 5.326008249539882e-06, val loss: 0.11814633756875992\n",
      "Epoch 7616: train loss: 4.292322500987211e-06, val loss: 0.11795277893543243\n",
      "Epoch 7617: train loss: 3.0882470127835404e-06, val loss: 0.11840217560529709\n",
      "Epoch 7618: train loss: 2.8906765692227054e-06, val loss: 0.11833026260137558\n",
      "Epoch 7619: train loss: 3.141769184367149e-06, val loss: 0.11814615875482559\n",
      "Epoch 7620: train loss: 3.038670911337249e-06, val loss: 0.11831176280975342\n",
      "Epoch 7621: train loss: 2.734426288952818e-06, val loss: 0.11898873001337051\n",
      "Epoch 7622: train loss: 2.668821935003507e-06, val loss: 0.11826346069574356\n",
      "Epoch 7623: train loss: 1.5176771057667793e-06, val loss: 0.11846091598272324\n",
      "Epoch 7624: train loss: 1.8152156826545252e-06, val loss: 0.11915826797485352\n",
      "Epoch 7625: train loss: 2.1787820969620952e-06, val loss: 0.1187606230378151\n",
      "Epoch 7626: train loss: 1.3797252904623747e-06, val loss: 0.11870431900024414\n",
      "Epoch 7627: train loss: 1.629563939786749e-06, val loss: 0.11888771504163742\n",
      "Epoch 7628: train loss: 1.5603455949531053e-06, val loss: 0.11937934160232544\n",
      "Epoch 7629: train loss: 1.8515418105380377e-06, val loss: 0.11870481818914413\n",
      "Epoch 7630: train loss: 2.175975396312424e-06, val loss: 0.11963427066802979\n",
      "Epoch 7631: train loss: 2.975350525957765e-06, val loss: 0.11874788254499435\n",
      "Epoch 7632: train loss: 6.4072778513946105e-06, val loss: 0.12036764621734619\n",
      "Epoch 7633: train loss: 1.5018528756627347e-05, val loss: 0.11770301312208176\n",
      "Epoch 7634: train loss: 4.2114686948480085e-05, val loss: 0.12328588217496872\n",
      "Epoch 7635: train loss: 0.0001295636175200343, val loss: 0.11395130306482315\n",
      "Epoch 7636: train loss: 0.00042530978680588305, val loss: 0.1297667771577835\n",
      "Epoch 7637: train loss: 0.001097206142731011, val loss: 0.10583925247192383\n",
      "Epoch 7638: train loss: 0.0018781787948682904, val loss: 0.12582066655158997\n",
      "Epoch 7639: train loss: 0.0007343229372054338, val loss: 0.12567970156669617\n",
      "Epoch 7640: train loss: 0.000815714243799448, val loss: 0.1107792854309082\n",
      "Epoch 7641: train loss: 0.0003372551000211388, val loss: 0.11072228103876114\n",
      "Epoch 7642: train loss: 0.0004607390728779137, val loss: 0.11914428323507309\n",
      "Epoch 7643: train loss: 0.0003814410883933306, val loss: 0.11627906560897827\n",
      "Epoch 7644: train loss: 0.00030854527722112834, val loss: 0.10886514186859131\n",
      "Epoch 7645: train loss: 0.0003081322938669473, val loss: 0.11265642940998077\n",
      "Epoch 7646: train loss: 0.0002545735042076558, val loss: 0.11410248279571533\n",
      "Epoch 7647: train loss: 0.00012597609020303935, val loss: 0.11061123758554459\n",
      "Epoch 7648: train loss: 0.0002274665021104738, val loss: 0.10988687723875046\n",
      "Epoch 7649: train loss: 0.00014366061077453196, val loss: 0.11037936061620712\n",
      "Epoch 7650: train loss: 0.0001634789805393666, val loss: 0.11171799153089523\n",
      "Epoch 7651: train loss: 0.00012750702444463968, val loss: 0.11086048185825348\n",
      "Epoch 7652: train loss: 6.696089258184657e-05, val loss: 0.10911407321691513\n",
      "Epoch 7653: train loss: 0.00011476530198706314, val loss: 0.10955210030078888\n",
      "Epoch 7654: train loss: 7.837992598069832e-05, val loss: 0.11125782877206802\n",
      "Epoch 7655: train loss: 8.489914034726098e-05, val loss: 0.11210592091083527\n",
      "Epoch 7656: train loss: 6.163526995806023e-05, val loss: 0.10890181362628937\n",
      "Epoch 7657: train loss: 3.763175118365325e-05, val loss: 0.10683455318212509\n",
      "Epoch 7658: train loss: 4.621913831215352e-05, val loss: 0.10810190439224243\n",
      "Epoch 7659: train loss: 4.821700349566527e-05, val loss: 0.10912823677062988\n",
      "Epoch 7660: train loss: 3.669464058475569e-05, val loss: 0.1096927672624588\n",
      "Epoch 7661: train loss: 3.700252273119986e-05, val loss: 0.10978267341852188\n",
      "Epoch 7662: train loss: 2.232528458989691e-05, val loss: 0.11012482643127441\n",
      "Epoch 7663: train loss: 2.2421565518015996e-05, val loss: 0.11007421463727951\n",
      "Epoch 7664: train loss: 2.861307075363584e-05, val loss: 0.10914251953363419\n",
      "Epoch 7665: train loss: 2.4792614567559212e-05, val loss: 0.10915573686361313\n",
      "Epoch 7666: train loss: 1.7194786778418347e-05, val loss: 0.10900359600782394\n",
      "Epoch 7667: train loss: 1.7542468413012102e-05, val loss: 0.10923796147108078\n",
      "Epoch 7668: train loss: 1.1172573977091815e-05, val loss: 0.11015229672193527\n",
      "Epoch 7669: train loss: 1.6845382560859434e-05, val loss: 0.11064986139535904\n",
      "Epoch 7670: train loss: 1.450585295970086e-05, val loss: 0.11112501472234726\n",
      "Epoch 7671: train loss: 1.2575304026540834e-05, val loss: 0.11083831638097763\n",
      "Epoch 7672: train loss: 8.881141184247099e-06, val loss: 0.1103568896651268\n",
      "Epoch 7673: train loss: 7.000604455242865e-06, val loss: 0.11042366176843643\n",
      "Epoch 7674: train loss: 9.797051461646333e-06, val loss: 0.11131013929843903\n",
      "Epoch 7675: train loss: 7.728205673629418e-06, val loss: 0.11245498806238174\n",
      "Epoch 7676: train loss: 8.148611414071638e-06, val loss: 0.11200547218322754\n",
      "Epoch 7677: train loss: 5.9911531025136355e-06, val loss: 0.11154844611883163\n",
      "Epoch 7678: train loss: 3.856194780382793e-06, val loss: 0.1116960421204567\n",
      "Epoch 7679: train loss: 5.633767159451963e-06, val loss: 0.11187461763620377\n",
      "Epoch 7680: train loss: 4.8579727263131645e-06, val loss: 0.11272739619016647\n",
      "Epoch 7681: train loss: 4.844230716116726e-06, val loss: 0.11290182173252106\n",
      "Epoch 7682: train loss: 3.860077413264662e-06, val loss: 0.11277168244123459\n",
      "Epoch 7683: train loss: 2.7976416276942473e-06, val loss: 0.1130451112985611\n",
      "Epoch 7684: train loss: 2.585086122053326e-06, val loss: 0.1136971265077591\n",
      "Epoch 7685: train loss: 3.3825695027189795e-06, val loss: 0.1139642596244812\n",
      "Epoch 7686: train loss: 2.8563595151354093e-06, val loss: 0.11359677463769913\n",
      "Epoch 7687: train loss: 2.43112958742131e-06, val loss: 0.11386837810277939\n",
      "Epoch 7688: train loss: 2.4336361548193963e-06, val loss: 0.11434655636548996\n",
      "Epoch 7689: train loss: 1.1936406281165546e-06, val loss: 0.11480753868818283\n",
      "Epoch 7690: train loss: 2.0715222035505576e-06, val loss: 0.11479554325342178\n",
      "Epoch 7691: train loss: 1.5824161891941912e-06, val loss: 0.11434370279312134\n",
      "Epoch 7692: train loss: 2.210069169450435e-06, val loss: 0.11487483978271484\n",
      "Epoch 7693: train loss: 1.1205198688912787e-06, val loss: 0.11529316753149033\n",
      "Epoch 7694: train loss: 1.5426837762788637e-06, val loss: 0.11557836830615997\n",
      "Epoch 7695: train loss: 7.814794003024872e-07, val loss: 0.11542975902557373\n",
      "Epoch 7696: train loss: 1.1966658348683268e-06, val loss: 0.11540871858596802\n",
      "Epoch 7697: train loss: 1.0686453606467694e-06, val loss: 0.11610092967748642\n",
      "Epoch 7698: train loss: 1.2185678315290716e-06, val loss: 0.11569315195083618\n",
      "Epoch 7699: train loss: 1.3849462447979022e-06, val loss: 0.11604540795087814\n",
      "Epoch 7700: train loss: 1.9530414192558965e-06, val loss: 0.11555906385183334\n",
      "Epoch 7701: train loss: 3.390136271264055e-06, val loss: 0.1167413517832756\n",
      "Epoch 7702: train loss: 8.402934327023104e-06, val loss: 0.11558838188648224\n",
      "Epoch 7703: train loss: 2.3644959583180025e-05, val loss: 0.11755018681287766\n",
      "Epoch 7704: train loss: 5.959168993285857e-05, val loss: 0.11531853675842285\n",
      "Epoch 7705: train loss: 0.0001329733495367691, val loss: 0.11778221279382706\n",
      "Epoch 7706: train loss: 0.0002650246606208384, val loss: 0.11581303924322128\n",
      "Epoch 7707: train loss: 0.000456154637504369, val loss: 0.11723724752664566\n",
      "Epoch 7708: train loss: 0.0006743368576280773, val loss: 0.11541717499494553\n",
      "Epoch 7709: train loss: 0.0006200015777722001, val loss: 0.112299345433712\n",
      "Epoch 7710: train loss: 0.00023500177485402673, val loss: 0.11032824963331223\n",
      "Epoch 7711: train loss: 8.812404121272266e-05, val loss: 0.11007175594568253\n",
      "Epoch 7712: train loss: 0.0002541799913160503, val loss: 0.10639183968305588\n",
      "Epoch 7713: train loss: 0.00016436197620350868, val loss: 0.1056697890162468\n",
      "Epoch 7714: train loss: 7.021970668574795e-05, val loss: 0.10584228485822678\n",
      "Epoch 7715: train loss: 0.00019440476899035275, val loss: 0.10314267128705978\n",
      "Epoch 7716: train loss: 0.00011231411917833611, val loss: 0.10419958084821701\n",
      "Epoch 7717: train loss: 7.278768316609785e-05, val loss: 0.1040148064494133\n",
      "Epoch 7718: train loss: 9.692513413028792e-05, val loss: 0.1036323681473732\n",
      "Epoch 7719: train loss: 5.5026786867529154e-05, val loss: 0.10550154745578766\n",
      "Epoch 7720: train loss: 7.799555896781385e-05, val loss: 0.10497285425662994\n",
      "Epoch 7721: train loss: 6.270884477999061e-05, val loss: 0.10410469025373459\n",
      "Epoch 7722: train loss: 7.44777571526356e-05, val loss: 0.10383336991071701\n",
      "Epoch 7723: train loss: 3.165204543620348e-05, val loss: 0.10409901291131973\n",
      "Epoch 7724: train loss: 4.341831663623452e-05, val loss: 0.10288114845752716\n",
      "Epoch 7725: train loss: 4.48961291112937e-05, val loss: 0.10261709988117218\n",
      "Epoch 7726: train loss: 1.9470475308480673e-05, val loss: 0.10357736796140671\n",
      "Epoch 7727: train loss: 5.45032016816549e-05, val loss: 0.10232143849134445\n",
      "Epoch 7728: train loss: 2.2254367650020868e-05, val loss: 0.1024676114320755\n",
      "Epoch 7729: train loss: 2.345799293834716e-05, val loss: 0.10301794111728668\n",
      "Epoch 7730: train loss: 2.349273745494429e-05, val loss: 0.10236251354217529\n",
      "Epoch 7731: train loss: 1.5030713257147e-05, val loss: 0.10224111378192902\n",
      "Epoch 7732: train loss: 2.3603197405464016e-05, val loss: 0.10267647355794907\n",
      "Epoch 7733: train loss: 1.6520558347110637e-05, val loss: 0.10269880294799805\n",
      "Epoch 7734: train loss: 1.945569965755567e-05, val loss: 0.10195378214120865\n",
      "Epoch 7735: train loss: 1.1658975381578784e-05, val loss: 0.10275991261005402\n",
      "Epoch 7736: train loss: 1.1674413144646678e-05, val loss: 0.1027139201760292\n",
      "Epoch 7737: train loss: 1.0905407179961912e-05, val loss: 0.10226873308420181\n",
      "Epoch 7738: train loss: 8.839469956001267e-06, val loss: 0.10327345132827759\n",
      "Epoch 7739: train loss: 1.3800180568068754e-05, val loss: 0.103301502764225\n",
      "Epoch 7740: train loss: 1.0161978025280405e-05, val loss: 0.10327599197626114\n",
      "Epoch 7741: train loss: 5.696933840226848e-06, val loss: 0.10299275070428848\n",
      "Epoch 7742: train loss: 7.084433946147328e-06, val loss: 0.10324310511350632\n",
      "Epoch 7743: train loss: 7.357849426625762e-06, val loss: 0.10380247980356216\n",
      "Epoch 7744: train loss: 5.68194536754163e-06, val loss: 0.10306878387928009\n",
      "Epoch 7745: train loss: 6.539973583130632e-06, val loss: 0.1034342423081398\n",
      "Epoch 7746: train loss: 6.594365459022811e-06, val loss: 0.10417187213897705\n",
      "Epoch 7747: train loss: 5.970085567241767e-06, val loss: 0.10399322956800461\n",
      "Epoch 7748: train loss: 3.4414686069794698e-06, val loss: 0.10386357456445694\n",
      "Epoch 7749: train loss: 2.4447517716907896e-06, val loss: 0.10450136661529541\n",
      "Epoch 7750: train loss: 3.7778063415316865e-06, val loss: 0.10422413796186447\n",
      "Epoch 7751: train loss: 4.864547463512281e-06, val loss: 0.10423393547534943\n",
      "Epoch 7752: train loss: 4.694489689427428e-06, val loss: 0.10496580600738525\n",
      "Epoch 7753: train loss: 3.8001492157491157e-06, val loss: 0.10459210723638535\n",
      "Epoch 7754: train loss: 6.4599316829117015e-06, val loss: 0.10497434437274933\n",
      "Epoch 7755: train loss: 1.4223513971955981e-05, val loss: 0.10476066917181015\n",
      "Epoch 7756: train loss: 3.8020501961000264e-05, val loss: 0.10587888211011887\n",
      "Epoch 7757: train loss: 0.0001233302609762177, val loss: 0.10434005409479141\n",
      "Epoch 7758: train loss: 0.00035920823574997485, val loss: 0.10636478662490845\n",
      "Epoch 7759: train loss: 0.0005502363783307374, val loss: 0.10536545515060425\n",
      "Epoch 7760: train loss: 0.00031767727341502905, val loss: 0.10590368509292603\n",
      "Epoch 7761: train loss: 0.000145637386594899, val loss: 0.10900948196649551\n",
      "Epoch 7762: train loss: 0.00028953971923328936, val loss: 0.10436046123504639\n",
      "Epoch 7763: train loss: 0.0004211038176435977, val loss: 0.10689747333526611\n",
      "Epoch 7764: train loss: 0.0003858693817164749, val loss: 0.10786880552768707\n",
      "Epoch 7765: train loss: 0.0002690832188818604, val loss: 0.1089855432510376\n",
      "Epoch 7766: train loss: 0.00018979408196173608, val loss: 0.10946496576070786\n",
      "Epoch 7767: train loss: 0.00010756958363344893, val loss: 0.11083134263753891\n",
      "Epoch 7768: train loss: 0.00015454096137546003, val loss: 0.11371525377035141\n",
      "Epoch 7769: train loss: 0.00015480404545087367, val loss: 0.1103772521018982\n",
      "Epoch 7770: train loss: 6.882617890369147e-05, val loss: 0.11030318588018417\n",
      "Epoch 7771: train loss: 8.226268255384639e-05, val loss: 0.114240862429142\n",
      "Epoch 7772: train loss: 0.00011561045539565384, val loss: 0.11240041255950928\n",
      "Epoch 7773: train loss: 5.686053555109538e-05, val loss: 0.11265846341848373\n",
      "Epoch 7774: train loss: 5.8171401178697124e-05, val loss: 0.11393177509307861\n",
      "Epoch 7775: train loss: 6.973524432396516e-05, val loss: 0.11329209059476852\n",
      "Epoch 7776: train loss: 3.694768020068295e-05, val loss: 0.11402641981840134\n",
      "Epoch 7777: train loss: 4.114686089451425e-05, val loss: 0.11493780463933945\n",
      "Epoch 7778: train loss: 3.9853632188169286e-05, val loss: 0.11411035060882568\n",
      "Epoch 7779: train loss: 3.969741010223515e-05, val loss: 0.1151435375213623\n",
      "Epoch 7780: train loss: 2.228768244094681e-05, val loss: 0.11675391346216202\n",
      "Epoch 7781: train loss: 4.033835284644738e-05, val loss: 0.11442480236291885\n",
      "Epoch 7782: train loss: 1.970006815099623e-05, val loss: 0.11499178409576416\n",
      "Epoch 7783: train loss: 1.9855080608977005e-05, val loss: 0.11742331832647324\n",
      "Epoch 7784: train loss: 2.4834322175593115e-05, val loss: 0.11634345352649689\n",
      "Epoch 7785: train loss: 1.573971167090349e-05, val loss: 0.1160820946097374\n",
      "Epoch 7786: train loss: 1.6690873962943442e-05, val loss: 0.1174829974770546\n",
      "Epoch 7787: train loss: 1.8321214156458154e-05, val loss: 0.11752506345510483\n",
      "Epoch 7788: train loss: 1.3385129022935871e-05, val loss: 0.11673426628112793\n",
      "Epoch 7789: train loss: 1.0674890290829353e-05, val loss: 0.1175176128745079\n",
      "Epoch 7790: train loss: 1.2179707482573576e-05, val loss: 0.11819278448820114\n",
      "Epoch 7791: train loss: 1.0595305866445415e-05, val loss: 0.11853005737066269\n",
      "Epoch 7792: train loss: 6.515238055726513e-06, val loss: 0.11865245550870895\n",
      "Epoch 7793: train loss: 1.0870374353544321e-05, val loss: 0.11809244006872177\n",
      "Epoch 7794: train loss: 8.744380465941504e-06, val loss: 0.11908795684576035\n",
      "Epoch 7795: train loss: 5.709572633350035e-06, val loss: 0.11941023170948029\n",
      "Epoch 7796: train loss: 7.023636499070562e-06, val loss: 0.11871886253356934\n",
      "Epoch 7797: train loss: 6.45577574687195e-06, val loss: 0.11929565668106079\n",
      "Epoch 7798: train loss: 5.039617008151254e-06, val loss: 0.11957595497369766\n",
      "Epoch 7799: train loss: 3.3275794066867093e-06, val loss: 0.1193513423204422\n",
      "Epoch 7800: train loss: 6.051989203115227e-06, val loss: 0.11985702812671661\n",
      "Epoch 7801: train loss: 4.499634542298736e-06, val loss: 0.12014748901128769\n",
      "Epoch 7802: train loss: 3.506179382384289e-06, val loss: 0.12015672028064728\n",
      "Epoch 7803: train loss: 3.754221324925311e-06, val loss: 0.12045873701572418\n",
      "Epoch 7804: train loss: 2.937779527201201e-06, val loss: 0.12043280899524689\n",
      "Epoch 7805: train loss: 3.664872792796814e-06, val loss: 0.12091682106256485\n",
      "Epoch 7806: train loss: 3.3798169170040637e-06, val loss: 0.12075980007648468\n",
      "Epoch 7807: train loss: 2.0423965452209814e-06, val loss: 0.12125887721776962\n",
      "Epoch 7808: train loss: 1.5458143707292038e-06, val loss: 0.12145372480154037\n",
      "Epoch 7809: train loss: 2.5288954930147156e-06, val loss: 0.1210235133767128\n",
      "Epoch 7810: train loss: 3.446997652645223e-06, val loss: 0.12218614667654037\n",
      "Epoch 7811: train loss: 2.5439917408220936e-06, val loss: 0.12149826437234879\n",
      "Epoch 7812: train loss: 3.2316738725057803e-06, val loss: 0.12211736291646957\n",
      "Epoch 7813: train loss: 4.778250058734557e-06, val loss: 0.12209191173315048\n",
      "Epoch 7814: train loss: 5.98789256400778e-06, val loss: 0.1230231299996376\n",
      "Epoch 7815: train loss: 9.465676157560665e-06, val loss: 0.12179305404424667\n",
      "Epoch 7816: train loss: 2.0355477317934856e-05, val loss: 0.1239386796951294\n",
      "Epoch 7817: train loss: 5.014931593905203e-05, val loss: 0.1207868829369545\n",
      "Epoch 7818: train loss: 0.00014956481754779816, val loss: 0.12762196362018585\n",
      "Epoch 7819: train loss: 0.00044878412154503167, val loss: 0.11744020134210587\n",
      "Epoch 7820: train loss: 0.001432384131476283, val loss: 0.13531504571437836\n",
      "Epoch 7821: train loss: 0.003194405697286129, val loss: 0.11532062292098999\n",
      "Epoch 7822: train loss: 0.004778609611093998, val loss: 0.11977161467075348\n",
      "Epoch 7823: train loss: 0.0009055071859620512, val loss: 0.1340751051902771\n",
      "Epoch 7824: train loss: 0.0018433796940371394, val loss: 0.1237473413348198\n",
      "Epoch 7825: train loss: 0.001271813060157001, val loss: 0.1146794781088829\n",
      "Epoch 7826: train loss: 0.0008948140311986208, val loss: 0.11741956323385239\n",
      "Epoch 7827: train loss: 0.0007282911101356149, val loss: 0.1252010613679886\n",
      "Epoch 7828: train loss: 0.00070193997817114, val loss: 0.11947851628065109\n",
      "Epoch 7829: train loss: 0.00036434034700505435, val loss: 0.11452104896306992\n",
      "Epoch 7830: train loss: 0.0005340041825547814, val loss: 0.11425922065973282\n",
      "Epoch 7831: train loss: 0.0002572019584476948, val loss: 0.11496374756097794\n",
      "Epoch 7832: train loss: 0.0004081008373759687, val loss: 0.1128050833940506\n",
      "Epoch 7833: train loss: 0.0002323778171557933, val loss: 0.1115044355392456\n",
      "Epoch 7834: train loss: 0.0002471070038154721, val loss: 0.11645946651697159\n",
      "Epoch 7835: train loss: 0.0002658872108440846, val loss: 0.11776643246412277\n",
      "Epoch 7836: train loss: 0.00012611408601514995, val loss: 0.114209845662117\n",
      "Epoch 7837: train loss: 0.00015324224659707397, val loss: 0.11226575821638107\n",
      "Epoch 7838: train loss: 0.0001887530816020444, val loss: 0.11388925462961197\n",
      "Epoch 7839: train loss: 7.36568181309849e-05, val loss: 0.11470387130975723\n",
      "Epoch 7840: train loss: 0.00012496800627559423, val loss: 0.11219246685504913\n",
      "Epoch 7841: train loss: 0.00013390110689215362, val loss: 0.11177995055913925\n",
      "Epoch 7842: train loss: 6.7146000219509e-05, val loss: 0.11341847479343414\n",
      "Epoch 7843: train loss: 7.338294381042942e-05, val loss: 0.11389581114053726\n",
      "Epoch 7844: train loss: 9.399308328283951e-05, val loss: 0.11191792786121368\n",
      "Epoch 7845: train loss: 4.6015346015337855e-05, val loss: 0.11164267361164093\n",
      "Epoch 7846: train loss: 4.8615318519296125e-05, val loss: 0.1136058121919632\n",
      "Epoch 7847: train loss: 5.89803748880513e-05, val loss: 0.11355872452259064\n",
      "Epoch 7848: train loss: 4.7891848225845024e-05, val loss: 0.11217945069074631\n",
      "Epoch 7849: train loss: 3.719058804563247e-05, val loss: 0.11177437752485275\n",
      "Epoch 7850: train loss: 3.966584336012602e-05, val loss: 0.11362264305353165\n",
      "Epoch 7851: train loss: 3.2109081075759605e-05, val loss: 0.1142171174287796\n",
      "Epoch 7852: train loss: 2.954440969915595e-05, val loss: 0.11305557936429977\n",
      "Epoch 7853: train loss: 2.167797902075108e-05, val loss: 0.11237738281488419\n",
      "Epoch 7854: train loss: 2.3016835257294588e-05, val loss: 0.11293082684278488\n",
      "Epoch 7855: train loss: 2.2873438865644857e-05, val loss: 0.11307580769062042\n",
      "Epoch 7856: train loss: 2.0065654098289087e-05, val loss: 0.11200516670942307\n",
      "Epoch 7857: train loss: 1.6127427443279885e-05, val loss: 0.11258553713560104\n",
      "Epoch 7858: train loss: 1.769734444678761e-05, val loss: 0.11411578953266144\n",
      "Epoch 7859: train loss: 1.3617413969768677e-05, val loss: 0.1146451011300087\n",
      "Epoch 7860: train loss: 1.2236118891451042e-05, val loss: 0.11345928907394409\n",
      "Epoch 7861: train loss: 9.386820238432847e-06, val loss: 0.11292839050292969\n",
      "Epoch 7862: train loss: 9.892241905617993e-06, val loss: 0.113789401948452\n",
      "Epoch 7863: train loss: 9.131434126175009e-06, val loss: 0.11429911851882935\n",
      "Epoch 7864: train loss: 8.157693628163543e-06, val loss: 0.11422210186719894\n",
      "Epoch 7865: train loss: 8.06780644779792e-06, val loss: 0.11426761001348495\n",
      "Epoch 7866: train loss: 7.951289262564387e-06, val loss: 0.11485029757022858\n",
      "Epoch 7867: train loss: 5.152491212356836e-06, val loss: 0.11452770233154297\n",
      "Epoch 7868: train loss: 6.150534773041727e-06, val loss: 0.11407069116830826\n",
      "Epoch 7869: train loss: 5.178789706405951e-06, val loss: 0.1146320104598999\n",
      "Epoch 7870: train loss: 3.4245279039168963e-06, val loss: 0.1156749501824379\n",
      "Epoch 7871: train loss: 4.674734100262867e-06, val loss: 0.11578778177499771\n",
      "Epoch 7872: train loss: 3.2018092497310136e-06, val loss: 0.1152147650718689\n",
      "Epoch 7873: train loss: 3.6304545574239455e-06, val loss: 0.11548896133899689\n",
      "Epoch 7874: train loss: 3.513889623718569e-06, val loss: 0.11605089157819748\n",
      "Epoch 7875: train loss: 2.4968765046651242e-06, val loss: 0.11624278128147125\n",
      "Epoch 7876: train loss: 2.783107902359916e-06, val loss: 0.11599551886320114\n",
      "Epoch 7877: train loss: 2.5048741463251645e-06, val loss: 0.11617026478052139\n",
      "Epoch 7878: train loss: 1.335401861979335e-06, val loss: 0.11638855934143066\n",
      "Epoch 7879: train loss: 2.433347162877908e-06, val loss: 0.1164536252617836\n",
      "Epoch 7880: train loss: 1.0816505664479337e-06, val loss: 0.11672382801771164\n",
      "Epoch 7881: train loss: 1.5078620663189213e-06, val loss: 0.11722420901060104\n",
      "Epoch 7882: train loss: 1.4298595942818793e-06, val loss: 0.11734248697757721\n",
      "Epoch 7883: train loss: 1.0572792916718754e-06, val loss: 0.11698906868696213\n",
      "Epoch 7884: train loss: 1.289074702981452e-06, val loss: 0.11718814820051193\n",
      "Epoch 7885: train loss: 8.443334991170559e-07, val loss: 0.11761607229709625\n",
      "Epoch 7886: train loss: 1.1349018222972518e-06, val loss: 0.11760342121124268\n",
      "Epoch 7887: train loss: 6.806241117374157e-07, val loss: 0.11724784225225449\n",
      "Epoch 7888: train loss: 8.764405379224627e-07, val loss: 0.1174454465508461\n",
      "Epoch 7889: train loss: 5.312239750310255e-07, val loss: 0.11791207641363144\n",
      "Epoch 7890: train loss: 6.488795634140843e-07, val loss: 0.11794982105493546\n",
      "Epoch 7891: train loss: 4.964792879036395e-07, val loss: 0.11787854880094528\n",
      "Epoch 7892: train loss: 5.00948829085246e-07, val loss: 0.11807328462600708\n",
      "Epoch 7893: train loss: 5.345479507923301e-07, val loss: 0.11821673065423965\n",
      "Epoch 7894: train loss: 2.8105128535571566e-07, val loss: 0.11815302819013596\n",
      "Epoch 7895: train loss: 5.138520577929739e-07, val loss: 0.11832749843597412\n",
      "Epoch 7896: train loss: 2.768881017800595e-07, val loss: 0.11845246702432632\n",
      "Epoch 7897: train loss: 3.4460924780432833e-07, val loss: 0.11841738224029541\n",
      "Epoch 7898: train loss: 3.084257400587376e-07, val loss: 0.11844632774591446\n",
      "Epoch 7899: train loss: 2.3278842320451076e-07, val loss: 0.11862953007221222\n",
      "Epoch 7900: train loss: 2.8142807195763453e-07, val loss: 0.1186445951461792\n",
      "Epoch 7901: train loss: 1.8367498455518216e-07, val loss: 0.11870338022708893\n",
      "Epoch 7902: train loss: 2.146569642036411e-07, val loss: 0.11879656463861465\n",
      "Epoch 7903: train loss: 1.9080562196904793e-07, val loss: 0.11881659179925919\n",
      "Epoch 7904: train loss: 1.5712518575128342e-07, val loss: 0.11895773559808731\n",
      "Epoch 7905: train loss: 1.5573027667414863e-07, val loss: 0.11905372142791748\n",
      "Epoch 7906: train loss: 1.7947100161563867e-07, val loss: 0.11902918666601181\n",
      "Epoch 7907: train loss: 1.4961928229695332e-07, val loss: 0.1190115213394165\n",
      "Epoch 7908: train loss: 1.4907104173289554e-07, val loss: 0.1192220076918602\n",
      "Epoch 7909: train loss: 1.5499966821153066e-07, val loss: 0.1192747950553894\n",
      "Epoch 7910: train loss: 1.6417145332070504e-07, val loss: 0.11942392587661743\n",
      "Epoch 7911: train loss: 1.1881324724072329e-07, val loss: 0.11942875385284424\n",
      "Epoch 7912: train loss: 2.221233046384441e-07, val loss: 0.11954154074192047\n",
      "Epoch 7913: train loss: 2.2633358298662642e-07, val loss: 0.11965453624725342\n",
      "Epoch 7914: train loss: 5.916094210078882e-07, val loss: 0.11954393237829208\n",
      "Epoch 7915: train loss: 1.1081739330620621e-06, val loss: 0.12010013312101364\n",
      "Epoch 7916: train loss: 8.341886541529675e-07, val loss: 0.12006759643554688\n",
      "Epoch 7917: train loss: 3.130685399810318e-07, val loss: 0.12027360498905182\n",
      "Epoch 7918: train loss: 5.735263925998879e-07, val loss: 0.12055070698261261\n",
      "Epoch 7919: train loss: 6.998095614108024e-07, val loss: 0.12041354179382324\n",
      "Epoch 7920: train loss: 4.454962549971242e-07, val loss: 0.12090958654880524\n",
      "Epoch 7921: train loss: 6.037417392690259e-07, val loss: 0.12102394551038742\n",
      "Epoch 7922: train loss: 4.819381729248562e-07, val loss: 0.12104733288288116\n",
      "Epoch 7923: train loss: 4.2892406781902537e-07, val loss: 0.12139492481946945\n",
      "Epoch 7924: train loss: 4.6850689727762074e-07, val loss: 0.12122698873281479\n",
      "Epoch 7925: train loss: 3.9193619727484474e-07, val loss: 0.1216031089425087\n",
      "Epoch 7926: train loss: 2.0510144338459213e-07, val loss: 0.12164788693189621\n",
      "Epoch 7927: train loss: 2.8252142669771274e-07, val loss: 0.12199041992425919\n",
      "Epoch 7928: train loss: 6.12240910413675e-07, val loss: 0.12179918587207794\n",
      "Epoch 7929: train loss: 1.4218500155038782e-06, val loss: 0.12242674827575684\n",
      "Epoch 7930: train loss: 3.692346126626944e-06, val loss: 0.12180318683385849\n",
      "Epoch 7931: train loss: 1.0168521839659661e-05, val loss: 0.12338276207447052\n",
      "Epoch 7932: train loss: 2.9620196073665284e-05, val loss: 0.1212828978896141\n",
      "Epoch 7933: train loss: 8.568575867684558e-05, val loss: 0.12467550486326218\n",
      "Epoch 7934: train loss: 0.0002260256587760523, val loss: 0.12130560725927353\n",
      "Epoch 7935: train loss: 0.0004253357765264809, val loss: 0.12551060318946838\n",
      "Epoch 7936: train loss: 0.0006092574330978096, val loss: 0.12076141685247421\n",
      "Epoch 7937: train loss: 0.0007237022509798408, val loss: 0.12807327508926392\n",
      "Epoch 7938: train loss: 0.0008044081041589379, val loss: 0.12248432636260986\n",
      "Epoch 7939: train loss: 0.001078976085409522, val loss: 0.1252930909395218\n",
      "Epoch 7940: train loss: 0.001133274519816041, val loss: 0.12094145268201828\n",
      "Epoch 7941: train loss: 0.0011954109650105238, val loss: 0.12571857869625092\n",
      "Epoch 7942: train loss: 0.0008079849649220705, val loss: 0.11948777735233307\n",
      "Epoch 7943: train loss: 0.0002698765601962805, val loss: 0.12180423736572266\n",
      "Epoch 7944: train loss: 0.00015119713498279452, val loss: 0.12673123180866241\n",
      "Epoch 7945: train loss: 0.0002514338702894747, val loss: 0.12252078205347061\n",
      "Epoch 7946: train loss: 0.000532762729562819, val loss: 0.12412410974502563\n",
      "Epoch 7947: train loss: 0.0004975977353751659, val loss: 0.12409323453903198\n",
      "Epoch 7948: train loss: 0.00026636008988134563, val loss: 0.126739963889122\n",
      "Epoch 7949: train loss: 7.415199070237577e-05, val loss: 0.12474945932626724\n",
      "Epoch 7950: train loss: 0.00011270597315160558, val loss: 0.12197186797857285\n",
      "Epoch 7951: train loss: 0.0002840144152287394, val loss: 0.1255863457918167\n",
      "Epoch 7952: train loss: 0.00028006546199321747, val loss: 0.1245977059006691\n",
      "Epoch 7953: train loss: 0.0001478177000535652, val loss: 0.12499233335256577\n",
      "Epoch 7954: train loss: 3.606086829677224e-05, val loss: 0.1251440793275833\n",
      "Epoch 7955: train loss: 8.447304571745917e-05, val loss: 0.12407340854406357\n",
      "Epoch 7956: train loss: 0.000170630679349415, val loss: 0.126029372215271\n",
      "Epoch 7957: train loss: 0.0001617633388377726, val loss: 0.1244107112288475\n",
      "Epoch 7958: train loss: 5.3275285608833656e-05, val loss: 0.12386687099933624\n",
      "Epoch 7959: train loss: 2.8903292331960984e-05, val loss: 0.1241217777132988\n",
      "Epoch 7960: train loss: 6.123864295659587e-05, val loss: 0.12368287891149521\n",
      "Epoch 7961: train loss: 0.00011685711069731042, val loss: 0.1253134310245514\n",
      "Epoch 7962: train loss: 7.223550346679986e-05, val loss: 0.12386400997638702\n",
      "Epoch 7963: train loss: 2.016531470871996e-05, val loss: 0.123565673828125\n",
      "Epoch 7964: train loss: 2.178696740884334e-05, val loss: 0.12499697506427765\n",
      "Epoch 7965: train loss: 5.456638609757647e-05, val loss: 0.12426959723234177\n",
      "Epoch 7966: train loss: 6.599810876650736e-05, val loss: 0.12446349114179611\n",
      "Epoch 7967: train loss: 3.12642878270708e-05, val loss: 0.12367767095565796\n",
      "Epoch 7968: train loss: 6.565446710737888e-06, val loss: 0.12378904968500137\n",
      "Epoch 7969: train loss: 2.2896860173204914e-05, val loss: 0.12475526332855225\n",
      "Epoch 7970: train loss: 4.0299604734173045e-05, val loss: 0.12381613254547119\n",
      "Epoch 7971: train loss: 3.353393913130276e-05, val loss: 0.12429294735193253\n",
      "Epoch 7972: train loss: 1.138779134635115e-05, val loss: 0.12415516376495361\n",
      "Epoch 7973: train loss: 5.4671577345288824e-06, val loss: 0.12393470108509064\n",
      "Epoch 7974: train loss: 2.0165363821433857e-05, val loss: 0.12480621784925461\n",
      "Epoch 7975: train loss: 2.4771479729679413e-05, val loss: 0.12393201887607574\n",
      "Epoch 7976: train loss: 1.688375778030604e-05, val loss: 0.12406690418720245\n",
      "Epoch 7977: train loss: 2.840946535798139e-06, val loss: 0.12450075149536133\n",
      "Epoch 7978: train loss: 6.837598903075559e-06, val loss: 0.12447855621576309\n",
      "Epoch 7979: train loss: 1.5061522390169557e-05, val loss: 0.124874547123909\n",
      "Epoch 7980: train loss: 1.4169040696287993e-05, val loss: 0.12407676130533218\n",
      "Epoch 7981: train loss: 7.420425845339196e-06, val loss: 0.12443091720342636\n",
      "Epoch 7982: train loss: 1.978142790903803e-06, val loss: 0.1248406171798706\n",
      "Epoch 7983: train loss: 6.100651262386236e-06, val loss: 0.124394491314888\n",
      "Epoch 7984: train loss: 9.223728739016224e-06, val loss: 0.12498743832111359\n",
      "Epoch 7985: train loss: 8.294595318147913e-06, val loss: 0.1247720941901207\n",
      "Epoch 7986: train loss: 4.016712409793399e-06, val loss: 0.12471117824316025\n",
      "Epoch 7987: train loss: 2.0126888102822704e-06, val loss: 0.12499097734689713\n",
      "Epoch 7988: train loss: 3.797205863520503e-06, val loss: 0.1249907985329628\n",
      "Epoch 7989: train loss: 5.747921477450291e-06, val loss: 0.12550118565559387\n",
      "Epoch 7990: train loss: 4.8681858970667236e-06, val loss: 0.1250433474779129\n",
      "Epoch 7991: train loss: 3.3840869946288876e-06, val loss: 0.12506258487701416\n",
      "Epoch 7992: train loss: 3.7639401853084564e-06, val loss: 0.12557487189769745\n",
      "Epoch 7993: train loss: 5.398112080001738e-06, val loss: 0.12501080334186554\n",
      "Epoch 7994: train loss: 7.084992830641568e-06, val loss: 0.12577474117279053\n",
      "Epoch 7995: train loss: 6.62214870317257e-06, val loss: 0.12496385723352432\n",
      "Epoch 7996: train loss: 6.669421054539271e-06, val loss: 0.12542308866977692\n",
      "Epoch 7997: train loss: 9.427757504454348e-06, val loss: 0.1256270706653595\n",
      "Epoch 7998: train loss: 1.6628409866825677e-05, val loss: 0.12538889050483704\n",
      "Epoch 7999: train loss: 3.4374348615529016e-05, val loss: 0.12560394406318665\n",
      "Epoch 8000: train loss: 8.111643546726555e-05, val loss: 0.12560729682445526\n",
      "Epoch 8001: train loss: 0.000214717976632528, val loss: 0.12411312013864517\n",
      "Epoch 8002: train loss: 0.0005626981728710234, val loss: 0.12535075843334198\n",
      "Epoch 8003: train loss: 0.0011312637943774462, val loss: 0.123256616294384\n",
      "Epoch 8004: train loss: 0.001469651353545487, val loss: 0.12770599126815796\n",
      "Epoch 8005: train loss: 0.0005590783548541367, val loss: 0.13017307221889496\n",
      "Epoch 8006: train loss: 0.0006145670195110142, val loss: 0.12874968349933624\n",
      "Epoch 8007: train loss: 0.0003669590805657208, val loss: 0.12706111371517181\n",
      "Epoch 8008: train loss: 0.0004901908105239272, val loss: 0.12605810165405273\n",
      "Epoch 8009: train loss: 0.00020074922940693796, val loss: 0.12685775756835938\n",
      "Epoch 8010: train loss: 0.00040733805508352816, val loss: 0.12670055031776428\n",
      "Epoch 8011: train loss: 0.0001073515450116247, val loss: 0.12636995315551758\n",
      "Epoch 8012: train loss: 0.0002532504149712622, val loss: 0.12644153833389282\n",
      "Epoch 8013: train loss: 0.00011760195047827438, val loss: 0.1269516944885254\n",
      "Epoch 8014: train loss: 0.00015170445840340108, val loss: 0.12603895366191864\n",
      "Epoch 8015: train loss: 0.000114724702143576, val loss: 0.12753909826278687\n",
      "Epoch 8016: train loss: 9.653893357608467e-05, val loss: 0.12945185601711273\n",
      "Epoch 8017: train loss: 9.072459215531126e-05, val loss: 0.1288314014673233\n",
      "Epoch 8018: train loss: 7.717143307672814e-05, val loss: 0.12642772495746613\n",
      "Epoch 8019: train loss: 6.699088407913223e-05, val loss: 0.12534382939338684\n",
      "Epoch 8020: train loss: 6.285066046984866e-05, val loss: 0.12752901017665863\n",
      "Epoch 8021: train loss: 4.976236596121453e-05, val loss: 0.13022147119045258\n",
      "Epoch 8022: train loss: 5.1593811804195866e-05, val loss: 0.1313711255788803\n",
      "Epoch 8023: train loss: 3.738475788850337e-05, val loss: 0.1308114230632782\n",
      "Epoch 8024: train loss: 4.068769339937717e-05, val loss: 0.12846344709396362\n",
      "Epoch 8025: train loss: 3.4680771932471544e-05, val loss: 0.12774477899074554\n",
      "Epoch 8026: train loss: 2.6133024221053347e-05, val loss: 0.12965404987335205\n",
      "Epoch 8027: train loss: 2.8729025871143676e-05, val loss: 0.1312863677740097\n",
      "Epoch 8028: train loss: 2.4309441869263537e-05, val loss: 0.13017788529396057\n",
      "Epoch 8029: train loss: 1.7598833437659778e-05, val loss: 0.12857256829738617\n",
      "Epoch 8030: train loss: 2.333813790755812e-05, val loss: 0.13015981018543243\n",
      "Epoch 8031: train loss: 1.231309215654619e-05, val loss: 0.13221058249473572\n",
      "Epoch 8032: train loss: 1.5254419849952683e-05, val loss: 0.13143083453178406\n",
      "Epoch 8033: train loss: 1.3043760191067122e-05, val loss: 0.12995202839374542\n",
      "Epoch 8034: train loss: 1.1606860425672494e-05, val loss: 0.13032980263233185\n",
      "Epoch 8035: train loss: 9.721426977193914e-06, val loss: 0.13177983462810516\n",
      "Epoch 8036: train loss: 1.094268463930348e-05, val loss: 0.1319226771593094\n",
      "Epoch 8037: train loss: 6.538639354403131e-06, val loss: 0.13156388700008392\n",
      "Epoch 8038: train loss: 9.498973668087274e-06, val loss: 0.1322403848171234\n",
      "Epoch 8039: train loss: 5.447053808893543e-06, val loss: 0.13259375095367432\n",
      "Epoch 8040: train loss: 7.609190106450114e-06, val loss: 0.13216891884803772\n",
      "Epoch 8041: train loss: 5.1907786655647215e-06, val loss: 0.13206258416175842\n",
      "Epoch 8042: train loss: 5.21294941790984e-06, val loss: 0.13283668458461761\n",
      "Epoch 8043: train loss: 4.789442300534574e-06, val loss: 0.13314664363861084\n",
      "Epoch 8044: train loss: 4.420339791977312e-06, val loss: 0.13238465785980225\n",
      "Epoch 8045: train loss: 4.147908839513548e-06, val loss: 0.13250921666622162\n",
      "Epoch 8046: train loss: 3.2511948120372836e-06, val loss: 0.13348494470119476\n",
      "Epoch 8047: train loss: 3.669511897896882e-06, val loss: 0.1337137669324875\n",
      "Epoch 8048: train loss: 2.540816467444529e-06, val loss: 0.13309140503406525\n",
      "Epoch 8049: train loss: 3.0010176033101743e-06, val loss: 0.13309882581233978\n",
      "Epoch 8050: train loss: 2.2997201085672714e-06, val loss: 0.13387171924114227\n",
      "Epoch 8051: train loss: 2.403685584795312e-06, val loss: 0.1338752806186676\n",
      "Epoch 8052: train loss: 1.928720394062111e-06, val loss: 0.13357429206371307\n",
      "Epoch 8053: train loss: 1.924632897498668e-06, val loss: 0.1338348388671875\n",
      "Epoch 8054: train loss: 1.6223393686232157e-06, val loss: 0.134006068110466\n",
      "Epoch 8055: train loss: 1.686446580606571e-06, val loss: 0.13385295867919922\n",
      "Epoch 8056: train loss: 1.5373138921859208e-06, val loss: 0.13393881916999817\n",
      "Epoch 8057: train loss: 1.2452338751245406e-06, val loss: 0.13426700234413147\n",
      "Epoch 8058: train loss: 1.3084668353258166e-06, val loss: 0.13434842228889465\n",
      "Epoch 8059: train loss: 1.1988684036623454e-06, val loss: 0.13445155322551727\n",
      "Epoch 8060: train loss: 9.05299089026812e-07, val loss: 0.1345982551574707\n",
      "Epoch 8061: train loss: 1.0257689382342505e-06, val loss: 0.13448834419250488\n",
      "Epoch 8062: train loss: 9.713467079563998e-07, val loss: 0.1347123682498932\n",
      "Epoch 8063: train loss: 7.918219466773735e-07, val loss: 0.1351500153541565\n",
      "Epoch 8064: train loss: 6.764819318050286e-07, val loss: 0.13522295653820038\n",
      "Epoch 8065: train loss: 7.430596156154934e-07, val loss: 0.13515646755695343\n",
      "Epoch 8066: train loss: 7.756128752589575e-07, val loss: 0.1351301074028015\n",
      "Epoch 8067: train loss: 5.18250715231261e-07, val loss: 0.13541662693023682\n",
      "Epoch 8068: train loss: 6.647282475569227e-07, val loss: 0.13580675423145294\n",
      "Epoch 8069: train loss: 8.67605876919697e-07, val loss: 0.13536226749420166\n",
      "Epoch 8070: train loss: 1.7806036112233414e-06, val loss: 0.13566282391548157\n",
      "Epoch 8071: train loss: 5.834322109876666e-06, val loss: 0.13584642112255096\n",
      "Epoch 8072: train loss: 2.4691622456884943e-05, val loss: 0.1364726573228836\n",
      "Epoch 8073: train loss: 6.759868847439066e-05, val loss: 0.1362920105457306\n",
      "Epoch 8074: train loss: 0.00012587083620019257, val loss: 0.13741715252399445\n",
      "Epoch 8075: train loss: 0.00018597858434077352, val loss: 0.13681535422801971\n",
      "Epoch 8076: train loss: 0.00028795129037462175, val loss: 0.13968400657176971\n",
      "Epoch 8077: train loss: 0.000523557944688946, val loss: 0.13127665221691132\n",
      "Epoch 8078: train loss: 0.0008922409033402801, val loss: 0.14361439645290375\n",
      "Epoch 8079: train loss: 0.000734024855773896, val loss: 0.13449321687221527\n",
      "Epoch 8080: train loss: 0.0003472228127066046, val loss: 0.13548462092876434\n",
      "Epoch 8081: train loss: 0.00015275819168891758, val loss: 0.1407269984483719\n",
      "Epoch 8082: train loss: 0.00030976178823038936, val loss: 0.13414235413074493\n",
      "Epoch 8083: train loss: 0.00018092624668497592, val loss: 0.13158929347991943\n",
      "Epoch 8084: train loss: 0.0001640565606066957, val loss: 0.13604548573493958\n",
      "Epoch 8085: train loss: 0.00017616397235542536, val loss: 0.1356191486120224\n",
      "Epoch 8086: train loss: 0.00011470783647382632, val loss: 0.13227479159832\n",
      "Epoch 8087: train loss: 0.00012595813313964754, val loss: 0.13009361922740936\n",
      "Epoch 8088: train loss: 9.267657151212916e-05, val loss: 0.13299034535884857\n",
      "Epoch 8089: train loss: 7.488408300559968e-05, val loss: 0.13384109735488892\n",
      "Epoch 8090: train loss: 0.00011512124910950661, val loss: 0.12911753356456757\n",
      "Epoch 8091: train loss: 4.3401429138612e-05, val loss: 0.12894392013549805\n",
      "Epoch 8092: train loss: 8.941493433667347e-05, val loss: 0.1320345252752304\n",
      "Epoch 8093: train loss: 4.376119250082411e-05, val loss: 0.1318400353193283\n",
      "Epoch 8094: train loss: 4.740836448036134e-05, val loss: 0.12975819408893585\n",
      "Epoch 8095: train loss: 4.804516356671229e-05, val loss: 0.12996111810207367\n",
      "Epoch 8096: train loss: 4.189888568362221e-05, val loss: 0.1309206485748291\n",
      "Epoch 8097: train loss: 3.429685966693796e-05, val loss: 0.12878066301345825\n",
      "Epoch 8098: train loss: 3.9713646401651204e-05, val loss: 0.1282818764448166\n",
      "Epoch 8099: train loss: 1.7433283574064262e-05, val loss: 0.12894609570503235\n",
      "Epoch 8100: train loss: 3.357899186084978e-05, val loss: 0.1281280219554901\n",
      "Epoch 8101: train loss: 1.6729765775380656e-05, val loss: 0.12765292823314667\n",
      "Epoch 8102: train loss: 2.6478075596969575e-05, val loss: 0.12817706167697906\n",
      "Epoch 8103: train loss: 1.778958539944142e-05, val loss: 0.12829752266407013\n",
      "Epoch 8104: train loss: 1.7807065887609497e-05, val loss: 0.12729525566101074\n",
      "Epoch 8105: train loss: 1.5206661373667885e-05, val loss: 0.1279011219739914\n",
      "Epoch 8106: train loss: 1.1297300261503551e-05, val loss: 0.12891753017902374\n",
      "Epoch 8107: train loss: 1.610838808119297e-05, val loss: 0.1276979297399521\n",
      "Epoch 8108: train loss: 8.024301678233314e-06, val loss: 0.12680687010288239\n",
      "Epoch 8109: train loss: 1.3701686839340255e-05, val loss: 0.127343088388443\n",
      "Epoch 8110: train loss: 9.442370355827734e-06, val loss: 0.1279979795217514\n",
      "Epoch 8111: train loss: 6.189372925291536e-06, val loss: 0.1272517740726471\n",
      "Epoch 8112: train loss: 8.933316166803706e-06, val loss: 0.1270032376050949\n",
      "Epoch 8113: train loss: 6.950143415451748e-06, val loss: 0.1279626339673996\n",
      "Epoch 8114: train loss: 6.390426733560162e-06, val loss: 0.12792447209358215\n",
      "Epoch 8115: train loss: 5.9169628912059125e-06, val loss: 0.1272299736738205\n",
      "Epoch 8116: train loss: 6.5327003540005535e-06, val loss: 0.12707820534706116\n",
      "Epoch 8117: train loss: 3.6623116557166213e-06, val loss: 0.1280955821275711\n",
      "Epoch 8118: train loss: 5.2630480240623e-06, val loss: 0.12828049063682556\n",
      "Epoch 8119: train loss: 2.2512463146995287e-06, val loss: 0.12725509703159332\n",
      "Epoch 8120: train loss: 4.998168151360005e-06, val loss: 0.12739335000514984\n",
      "Epoch 8121: train loss: 3.971788373746676e-06, val loss: 0.1280953586101532\n",
      "Epoch 8122: train loss: 2.6986763259628788e-06, val loss: 0.12793903052806854\n",
      "Epoch 8123: train loss: 2.523033799661789e-06, val loss: 0.12727807462215424\n",
      "Epoch 8124: train loss: 2.6840862119570374e-06, val loss: 0.12778794765472412\n",
      "Epoch 8125: train loss: 1.7843560726760188e-06, val loss: 0.1282425969839096\n",
      "Epoch 8126: train loss: 2.943617346318206e-06, val loss: 0.12763775885105133\n",
      "Epoch 8127: train loss: 3.124612248939229e-06, val loss: 0.12780654430389404\n",
      "Epoch 8128: train loss: 1.916825112857623e-06, val loss: 0.12803836166858673\n",
      "Epoch 8129: train loss: 4.304328740545316e-06, val loss: 0.12811635434627533\n",
      "Epoch 8130: train loss: 1.3088843843434006e-05, val loss: 0.12792766094207764\n",
      "Epoch 8131: train loss: 6.12730800639838e-05, val loss: 0.12827135622501373\n",
      "Epoch 8132: train loss: 0.00025380123406648636, val loss: 0.12726472318172455\n",
      "Epoch 8133: train loss: 0.0006597719620913267, val loss: 0.12885136902332306\n",
      "Epoch 8134: train loss: 0.0007000264595262706, val loss: 0.1277269572019577\n",
      "Epoch 8135: train loss: 0.0002827947319019586, val loss: 0.12992815673351288\n",
      "Epoch 8136: train loss: 0.00019236323714721948, val loss: 0.1306164562702179\n",
      "Epoch 8137: train loss: 0.00017712591215968132, val loss: 0.1268693208694458\n",
      "Epoch 8138: train loss: 0.00021855639351997524, val loss: 0.12460465729236603\n",
      "Epoch 8139: train loss: 0.00019717926625162363, val loss: 0.12625084817409515\n",
      "Epoch 8140: train loss: 0.00013179804955143481, val loss: 0.12756696343421936\n",
      "Epoch 8141: train loss: 0.00011815037578344345, val loss: 0.12368190288543701\n",
      "Epoch 8142: train loss: 0.00010165578714804724, val loss: 0.12346208095550537\n",
      "Epoch 8143: train loss: 8.228892693296075e-05, val loss: 0.1255331039428711\n",
      "Epoch 8144: train loss: 9.697958739707246e-05, val loss: 0.12221386283636093\n",
      "Epoch 8145: train loss: 6.425603351090103e-05, val loss: 0.12085232883691788\n",
      "Epoch 8146: train loss: 6.143488280940801e-05, val loss: 0.12380140274763107\n",
      "Epoch 8147: train loss: 5.759267878602259e-05, val loss: 0.12036221474409103\n",
      "Epoch 8148: train loss: 6.110623507993296e-05, val loss: 0.1214519664645195\n",
      "Epoch 8149: train loss: 3.0577564757550135e-05, val loss: 0.12372063845396042\n",
      "Epoch 8150: train loss: 4.641400300897658e-05, val loss: 0.12002724409103394\n",
      "Epoch 8151: train loss: 3.718610605574213e-05, val loss: 0.12071019411087036\n",
      "Epoch 8152: train loss: 3.0792583856964484e-05, val loss: 0.12260469049215317\n",
      "Epoch 8153: train loss: 3.3727119443938136e-05, val loss: 0.12093900889158249\n",
      "Epoch 8154: train loss: 2.1754396584583446e-05, val loss: 0.12048506736755371\n",
      "Epoch 8155: train loss: 2.4540244339732453e-05, val loss: 0.12198138236999512\n",
      "Epoch 8156: train loss: 2.2332520529744215e-05, val loss: 0.12103065103292465\n",
      "Epoch 8157: train loss: 2.134054921043571e-05, val loss: 0.12058931589126587\n",
      "Epoch 8158: train loss: 1.4065783034311607e-05, val loss: 0.1220410093665123\n",
      "Epoch 8159: train loss: 1.3320102880243212e-05, val loss: 0.12211008369922638\n",
      "Epoch 8160: train loss: 2.0194633179926313e-05, val loss: 0.1204507127404213\n",
      "Epoch 8161: train loss: 1.0710453352658078e-05, val loss: 0.12058911472558975\n",
      "Epoch 8162: train loss: 9.295631571148988e-06, val loss: 0.1225188747048378\n",
      "Epoch 8163: train loss: 1.2848491678596474e-05, val loss: 0.1219540387392044\n",
      "Epoch 8164: train loss: 9.795319783734158e-06, val loss: 0.12078821659088135\n",
      "Epoch 8165: train loss: 9.420977221452631e-06, val loss: 0.12131979316473007\n",
      "Epoch 8166: train loss: 5.265080289973412e-06, val loss: 0.12295683473348618\n",
      "Epoch 8167: train loss: 9.230543582816608e-06, val loss: 0.1220284253358841\n",
      "Epoch 8168: train loss: 6.352513992169406e-06, val loss: 0.12158022075891495\n",
      "Epoch 8169: train loss: 6.490548912552185e-06, val loss: 0.12310760468244553\n",
      "Epoch 8170: train loss: 6.562560884049162e-06, val loss: 0.12228536605834961\n",
      "Epoch 8171: train loss: 6.213048436620738e-06, val loss: 0.12294671684503555\n",
      "Epoch 8172: train loss: 5.9202352531428915e-06, val loss: 0.12316198647022247\n",
      "Epoch 8173: train loss: 9.835937817115337e-06, val loss: 0.1234167218208313\n",
      "Epoch 8174: train loss: 1.4053417544346303e-05, val loss: 0.12255243211984634\n",
      "Epoch 8175: train loss: 2.7412215786171146e-05, val loss: 0.12534989416599274\n",
      "Epoch 8176: train loss: 6.683920946670696e-05, val loss: 0.12141437828540802\n",
      "Epoch 8177: train loss: 0.00020524364663287997, val loss: 0.12708061933517456\n",
      "Epoch 8178: train loss: 0.000596615020185709, val loss: 0.1204288974404335\n",
      "Epoch 8179: train loss: 0.0014308906393125653, val loss: 0.12477407604455948\n",
      "Epoch 8180: train loss: 0.0011759294429793954, val loss: 0.13593672215938568\n",
      "Epoch 8181: train loss: 0.00036558028659783304, val loss: 0.1269409954547882\n",
      "Epoch 8182: train loss: 0.0005756553146056831, val loss: 0.1352720558643341\n",
      "Epoch 8183: train loss: 0.0003560219774954021, val loss: 0.14057599008083344\n",
      "Epoch 8184: train loss: 0.0003631695290096104, val loss: 0.13976526260375977\n",
      "Epoch 8185: train loss: 0.000358777615474537, val loss: 0.14116817712783813\n",
      "Epoch 8186: train loss: 0.0001966228592209518, val loss: 0.14671950042247772\n",
      "Epoch 8187: train loss: 0.00022006935614626855, val loss: 0.14537863433361053\n",
      "Epoch 8188: train loss: 0.00013553150347433984, val loss: 0.14359183609485626\n",
      "Epoch 8189: train loss: 0.00014286499936133623, val loss: 0.14617283642292023\n",
      "Epoch 8190: train loss: 0.00012375008373055607, val loss: 0.1472395658493042\n",
      "Epoch 8191: train loss: 0.00013592687901109457, val loss: 0.14715082943439484\n",
      "Epoch 8192: train loss: 9.833750664256513e-05, val loss: 0.14459963142871857\n",
      "Epoch 8193: train loss: 0.00011511825141496956, val loss: 0.14507411420345306\n",
      "Epoch 8194: train loss: 6.24127933406271e-05, val loss: 0.14497418701648712\n",
      "Epoch 8195: train loss: 7.302385347429663e-05, val loss: 0.1453152894973755\n",
      "Epoch 8196: train loss: 5.0957580242538825e-05, val loss: 0.14537157118320465\n",
      "Epoch 8197: train loss: 4.8229041567537934e-05, val loss: 0.14453472197055817\n",
      "Epoch 8198: train loss: 5.5432941735489294e-05, val loss: 0.14538170397281647\n",
      "Epoch 8199: train loss: 4.748029095935635e-05, val loss: 0.1453949511051178\n",
      "Epoch 8200: train loss: 4.617804734152742e-05, val loss: 0.14573439955711365\n",
      "Epoch 8201: train loss: 3.639393617049791e-05, val loss: 0.1445963829755783\n",
      "Epoch 8202: train loss: 3.600684794946574e-05, val loss: 0.1441897600889206\n",
      "Epoch 8203: train loss: 1.5587258531013504e-05, val loss: 0.14415869116783142\n",
      "Epoch 8204: train loss: 3.2239826396107674e-05, val loss: 0.14421261847019196\n",
      "Epoch 8205: train loss: 1.2752419024764095e-05, val loss: 0.14487771689891815\n",
      "Epoch 8206: train loss: 2.9289314625202678e-05, val loss: 0.14504794776439667\n",
      "Epoch 8207: train loss: 1.7499269233667292e-05, val loss: 0.14568807184696198\n",
      "Epoch 8208: train loss: 2.2313499357551336e-05, val loss: 0.14443162083625793\n",
      "Epoch 8209: train loss: 1.5318235455197282e-05, val loss: 0.14363504946231842\n",
      "Epoch 8210: train loss: 1.5042501217976678e-05, val loss: 0.14408652484416962\n",
      "Epoch 8211: train loss: 9.184541340800934e-06, val loss: 0.14564365148544312\n",
      "Epoch 8212: train loss: 1.1183487004018389e-05, val loss: 0.1456964761018753\n",
      "Epoch 8213: train loss: 8.755101589486003e-06, val loss: 0.14395281672477722\n",
      "Epoch 8214: train loss: 9.371749001729768e-06, val loss: 0.1439102739095688\n",
      "Epoch 8215: train loss: 1.0943483175651636e-05, val loss: 0.14524543285369873\n",
      "Epoch 8216: train loss: 7.648096470802557e-06, val loss: 0.1463247537612915\n",
      "Epoch 8217: train loss: 9.292948561778758e-06, val loss: 0.14482492208480835\n",
      "Epoch 8218: train loss: 6.382219453371363e-06, val loss: 0.14396142959594727\n",
      "Epoch 8219: train loss: 5.3037724683235865e-06, val loss: 0.1448851078748703\n",
      "Epoch 8220: train loss: 4.396304575493559e-06, val loss: 0.1457863599061966\n",
      "Epoch 8221: train loss: 4.715608611149946e-06, val loss: 0.1454380601644516\n",
      "Epoch 8222: train loss: 4.617828381014988e-06, val loss: 0.14506888389587402\n",
      "Epoch 8223: train loss: 4.0631252886669245e-06, val loss: 0.14574721455574036\n",
      "Epoch 8224: train loss: 4.398972123453859e-06, val loss: 0.14555010199546814\n",
      "Epoch 8225: train loss: 4.1024973143066745e-06, val loss: 0.14531604945659637\n",
      "Epoch 8226: train loss: 4.022577741125133e-06, val loss: 0.14520594477653503\n",
      "Epoch 8227: train loss: 3.2869686492631445e-06, val loss: 0.14592574536800385\n",
      "Epoch 8228: train loss: 2.0790319013030967e-06, val loss: 0.14593546092510223\n",
      "Epoch 8229: train loss: 2.429979303997243e-06, val loss: 0.14557385444641113\n",
      "Epoch 8230: train loss: 1.7181762359541608e-06, val loss: 0.14546746015548706\n",
      "Epoch 8231: train loss: 1.6677915937179932e-06, val loss: 0.1458476483821869\n",
      "Epoch 8232: train loss: 2.0652732928283513e-06, val loss: 0.14664068818092346\n",
      "Epoch 8233: train loss: 2.611777517813607e-06, val loss: 0.14560669660568237\n",
      "Epoch 8234: train loss: 5.211720690567745e-06, val loss: 0.14566375315189362\n",
      "Epoch 8235: train loss: 1.3452179700834677e-05, val loss: 0.1461976319551468\n",
      "Epoch 8236: train loss: 4.477231050259434e-05, val loss: 0.14679156243801117\n",
      "Epoch 8237: train loss: 0.0001429229014320299, val loss: 0.14479203522205353\n",
      "Epoch 8238: train loss: 0.00037826967309229076, val loss: 0.14878563582897186\n",
      "Epoch 8239: train loss: 0.0006398956174962223, val loss: 0.14443600177764893\n",
      "Epoch 8240: train loss: 0.0007484338711947203, val loss: 0.14793673157691956\n",
      "Epoch 8241: train loss: 0.0003679975925479084, val loss: 0.14888741075992584\n",
      "Epoch 8242: train loss: 0.0002917496603913605, val loss: 0.14599479734897614\n",
      "Epoch 8243: train loss: 0.0004255467210896313, val loss: 0.14126229286193848\n",
      "Epoch 8244: train loss: 0.00020926151773892343, val loss: 0.14589831233024597\n",
      "Epoch 8245: train loss: 0.0002766053657978773, val loss: 0.14215980470180511\n",
      "Epoch 8246: train loss: 0.00021278198983054608, val loss: 0.13813148438930511\n",
      "Epoch 8247: train loss: 0.00011525537411216646, val loss: 0.13745419681072235\n",
      "Epoch 8248: train loss: 0.0001302126474911347, val loss: 0.14099657535552979\n",
      "Epoch 8249: train loss: 9.117033914662898e-05, val loss: 0.1406286507844925\n",
      "Epoch 8250: train loss: 8.21734793134965e-05, val loss: 0.1365513652563095\n",
      "Epoch 8251: train loss: 0.00011428812285885215, val loss: 0.13729777932167053\n",
      "Epoch 8252: train loss: 8.676087600179017e-05, val loss: 0.13693979382514954\n",
      "Epoch 8253: train loss: 0.00012638414045795798, val loss: 0.1373070776462555\n",
      "Epoch 8254: train loss: 6.545655196532607e-05, val loss: 0.13565364480018616\n",
      "Epoch 8255: train loss: 8.556310785934329e-05, val loss: 0.13605321943759918\n",
      "Epoch 8256: train loss: 3.7836878618691117e-05, val loss: 0.1349424570798874\n",
      "Epoch 8257: train loss: 4.116601849091239e-05, val loss: 0.13481493294239044\n",
      "Epoch 8258: train loss: 3.1511986890109256e-05, val loss: 0.13482952117919922\n",
      "Epoch 8259: train loss: 3.553232818376273e-05, val loss: 0.133670836687088\n",
      "Epoch 8260: train loss: 3.798082616413012e-05, val loss: 0.13422521948814392\n",
      "Epoch 8261: train loss: 3.9808375731809065e-05, val loss: 0.13326989114284515\n",
      "Epoch 8262: train loss: 4.130572415306233e-05, val loss: 0.13421566784381866\n",
      "Epoch 8263: train loss: 3.0496265026158653e-05, val loss: 0.13387839496135712\n",
      "Epoch 8264: train loss: 2.5903533241944388e-05, val loss: 0.13375313580036163\n",
      "Epoch 8265: train loss: 2.2149142751004547e-05, val loss: 0.1328643411397934\n",
      "Epoch 8266: train loss: 9.75594866758911e-06, val loss: 0.13348542153835297\n",
      "Epoch 8267: train loss: 1.691202851361595e-05, val loss: 0.1340310275554657\n",
      "Epoch 8268: train loss: 1.0260228009428829e-05, val loss: 0.13323603570461273\n",
      "Epoch 8269: train loss: 1.8790355170494877e-05, val loss: 0.1335204392671585\n",
      "Epoch 8270: train loss: 1.1948444807785563e-05, val loss: 0.1328062266111374\n",
      "Epoch 8271: train loss: 1.9762910596909933e-05, val loss: 0.1337333768606186\n",
      "Epoch 8272: train loss: 1.3393489098234568e-05, val loss: 0.13353443145751953\n",
      "Epoch 8273: train loss: 1.2871108083345462e-05, val loss: 0.13376030325889587\n",
      "Epoch 8274: train loss: 9.694027539808303e-06, val loss: 0.13308890163898468\n",
      "Epoch 8275: train loss: 8.556949069316033e-06, val loss: 0.1339407116174698\n",
      "Epoch 8276: train loss: 4.818405159312533e-06, val loss: 0.13391707837581635\n",
      "Epoch 8277: train loss: 5.332084128895076e-06, val loss: 0.1335630863904953\n",
      "Epoch 8278: train loss: 5.335441528586671e-06, val loss: 0.13430391252040863\n",
      "Epoch 8279: train loss: 5.188521754462272e-06, val loss: 0.13408124446868896\n",
      "Epoch 8280: train loss: 6.9223215177771635e-06, val loss: 0.1341491937637329\n",
      "Epoch 8281: train loss: 5.476929800352082e-06, val loss: 0.13376998901367188\n",
      "Epoch 8282: train loss: 7.779070983815473e-06, val loss: 0.13506080210208893\n",
      "Epoch 8283: train loss: 7.6245260061114095e-06, val loss: 0.13429169356822968\n",
      "Epoch 8284: train loss: 6.986980224610306e-06, val loss: 0.13468916714191437\n",
      "Epoch 8285: train loss: 6.595436389034148e-06, val loss: 0.1342526525259018\n",
      "Epoch 8286: train loss: 7.1959170782065485e-06, val loss: 0.13524483144283295\n",
      "Epoch 8287: train loss: 1.0411449693492614e-05, val loss: 0.13436095416545868\n",
      "Epoch 8288: train loss: 1.9335730030434206e-05, val loss: 0.13495445251464844\n",
      "Epoch 8289: train loss: 4.6841101720929146e-05, val loss: 0.13439808785915375\n",
      "Epoch 8290: train loss: 0.00012161210179328918, val loss: 0.13476411998271942\n",
      "Epoch 8291: train loss: 0.0003423370944801718, val loss: 0.1342652589082718\n",
      "Epoch 8292: train loss: 0.0008431185851804912, val loss: 0.1302659809589386\n",
      "Epoch 8293: train loss: 0.0014579601120203733, val loss: 0.13239271938800812\n",
      "Epoch 8294: train loss: 0.0011077765375375748, val loss: 0.13463173806667328\n",
      "Epoch 8295: train loss: 0.0002903446729760617, val loss: 0.12964048981666565\n",
      "Epoch 8296: train loss: 0.0006505321362055838, val loss: 0.1258498877286911\n",
      "Epoch 8297: train loss: 0.000442746706539765, val loss: 0.12274688482284546\n",
      "Epoch 8298: train loss: 0.000208512123208493, val loss: 0.1270102709531784\n",
      "Epoch 8299: train loss: 0.0002641651954036206, val loss: 0.1256486475467682\n",
      "Epoch 8300: train loss: 0.0001432269491488114, val loss: 0.12151439487934113\n",
      "Epoch 8301: train loss: 0.00013967744598630816, val loss: 0.1217842847108841\n",
      "Epoch 8302: train loss: 0.00015611456183250993, val loss: 0.1233171597123146\n",
      "Epoch 8303: train loss: 0.000133863853989169, val loss: 0.12262075394392014\n",
      "Epoch 8304: train loss: 0.00013442298222798854, val loss: 0.11925417184829712\n",
      "Epoch 8305: train loss: 0.00011202986934222281, val loss: 0.11988754570484161\n",
      "Epoch 8306: train loss: 0.00010271376959281042, val loss: 0.11918022483587265\n",
      "Epoch 8307: train loss: 7.240021659526974e-05, val loss: 0.11725831031799316\n",
      "Epoch 8308: train loss: 5.947360841673799e-05, val loss: 0.11649705469608307\n",
      "Epoch 8309: train loss: 6.0986491007497534e-05, val loss: 0.11773085594177246\n",
      "Epoch 8310: train loss: 3.666709017124958e-05, val loss: 0.11807630211114883\n",
      "Epoch 8311: train loss: 6.498058064607903e-05, val loss: 0.11662657558917999\n",
      "Epoch 8312: train loss: 3.881240263581276e-05, val loss: 0.11725256592035294\n",
      "Epoch 8313: train loss: 5.7909925089916214e-05, val loss: 0.1171804890036583\n",
      "Epoch 8314: train loss: 3.6538920539896935e-05, val loss: 0.11664431542158127\n",
      "Epoch 8315: train loss: 3.8888825656613335e-05, val loss: 0.11629185825586319\n",
      "Epoch 8316: train loss: 2.6261886887368746e-05, val loss: 0.11784515529870987\n",
      "Epoch 8317: train loss: 2.075723205052782e-05, val loss: 0.11842425167560577\n",
      "Epoch 8318: train loss: 2.3515249267802574e-05, val loss: 0.11728773266077042\n",
      "Epoch 8319: train loss: 1.2909579709230457e-05, val loss: 0.1165514588356018\n",
      "Epoch 8320: train loss: 2.5289265977335162e-05, val loss: 0.11687572300434113\n",
      "Epoch 8321: train loss: 1.5977368093444966e-05, val loss: 0.11799275130033493\n",
      "Epoch 8322: train loss: 2.054555807262659e-05, val loss: 0.11766914278268814\n",
      "Epoch 8323: train loss: 1.7685564671410248e-05, val loss: 0.1177656427025795\n",
      "Epoch 8324: train loss: 1.2527864782896359e-05, val loss: 0.11762429773807526\n",
      "Epoch 8325: train loss: 1.3173400475352537e-05, val loss: 0.11736191809177399\n",
      "Epoch 8326: train loss: 8.020551831577905e-06, val loss: 0.11703542619943619\n",
      "Epoch 8327: train loss: 8.187406820070464e-06, val loss: 0.11758780479431152\n",
      "Epoch 8328: train loss: 7.67902656662045e-06, val loss: 0.11817169189453125\n",
      "Epoch 8329: train loss: 7.371203537331894e-06, val loss: 0.11753308773040771\n",
      "Epoch 8330: train loss: 8.297680324176326e-06, val loss: 0.11749789863824844\n",
      "Epoch 8331: train loss: 7.674292646697722e-06, val loss: 0.11781253665685654\n",
      "Epoch 8332: train loss: 7.662712050660048e-06, val loss: 0.11798534542322159\n",
      "Epoch 8333: train loss: 5.683396011590958e-06, val loss: 0.11718851327896118\n",
      "Epoch 8334: train loss: 6.466647391789593e-06, val loss: 0.11731181293725967\n",
      "Epoch 8335: train loss: 3.2804373404360376e-06, val loss: 0.11762294918298721\n",
      "Epoch 8336: train loss: 4.1589883039705455e-06, val loss: 0.11756163090467453\n",
      "Epoch 8337: train loss: 3.1579706956108566e-06, val loss: 0.11760753393173218\n",
      "Epoch 8338: train loss: 2.7973919713986106e-06, val loss: 0.11797826737165451\n",
      "Epoch 8339: train loss: 2.8989466045459267e-06, val loss: 0.1179978996515274\n",
      "Epoch 8340: train loss: 3.834026301774429e-06, val loss: 0.11740489304065704\n",
      "Epoch 8341: train loss: 3.0623584734712495e-06, val loss: 0.11779377609491348\n",
      "Epoch 8342: train loss: 4.63115202364861e-06, val loss: 0.11779393255710602\n",
      "Epoch 8343: train loss: 3.2903601550060557e-06, val loss: 0.11793941259384155\n",
      "Epoch 8344: train loss: 3.3485489439044613e-06, val loss: 0.11817938089370728\n",
      "Epoch 8345: train loss: 2.5580036435712827e-06, val loss: 0.11816036701202393\n",
      "Epoch 8346: train loss: 2.5387269033672055e-06, val loss: 0.11789751052856445\n",
      "Epoch 8347: train loss: 2.104534360114485e-06, val loss: 0.11845101416110992\n",
      "Epoch 8348: train loss: 1.3341841622604989e-06, val loss: 0.11860127747058868\n",
      "Epoch 8349: train loss: 2.163503040719661e-06, val loss: 0.11839552223682404\n",
      "Epoch 8350: train loss: 1.9793121737166075e-06, val loss: 0.11841149628162384\n",
      "Epoch 8351: train loss: 2.0415379822225077e-06, val loss: 0.1186593770980835\n",
      "Epoch 8352: train loss: 1.926682671182789e-06, val loss: 0.11873743683099747\n",
      "Epoch 8353: train loss: 1.8505871821616893e-06, val loss: 0.11889946460723877\n",
      "Epoch 8354: train loss: 2.4717148789932253e-06, val loss: 0.1189158707857132\n",
      "Epoch 8355: train loss: 4.284413535060594e-06, val loss: 0.11923830956220627\n",
      "Epoch 8356: train loss: 1.0786116035887972e-05, val loss: 0.11852013319730759\n",
      "Epoch 8357: train loss: 3.300180833321065e-05, val loss: 0.12004542350769043\n",
      "Epoch 8358: train loss: 0.00011972858919762075, val loss: 0.11826784908771515\n",
      "Epoch 8359: train loss: 0.00040980090852826834, val loss: 0.1187184602022171\n",
      "Epoch 8360: train loss: 0.0010092322481796145, val loss: 0.12227821350097656\n",
      "Epoch 8361: train loss: 0.0019434916321188211, val loss: 0.11460994929075241\n",
      "Epoch 8362: train loss: 0.0016833459958434105, val loss: 0.13150110840797424\n",
      "Epoch 8363: train loss: 0.00043680734233930707, val loss: 0.1331753432750702\n",
      "Epoch 8364: train loss: 0.0007059979834593832, val loss: 0.12495299428701401\n",
      "Epoch 8365: train loss: 0.0005575476679950953, val loss: 0.1255328208208084\n",
      "Epoch 8366: train loss: 0.0003253924078308046, val loss: 0.13162420690059662\n",
      "Epoch 8367: train loss: 0.00031269871396943927, val loss: 0.13298659026622772\n",
      "Epoch 8368: train loss: 0.0002993460511788726, val loss: 0.12581060826778412\n",
      "Epoch 8369: train loss: 0.00016881768533494323, val loss: 0.12238078564405441\n",
      "Epoch 8370: train loss: 0.00024278323689941317, val loss: 0.1276882141828537\n",
      "Epoch 8371: train loss: 0.00014988637121859938, val loss: 0.1283714771270752\n",
      "Epoch 8372: train loss: 0.00018081125745084137, val loss: 0.12558482587337494\n",
      "Epoch 8373: train loss: 0.00013907307584304363, val loss: 0.1244802251458168\n",
      "Epoch 8374: train loss: 0.00012804138532374054, val loss: 0.1256733238697052\n",
      "Epoch 8375: train loss: 0.0001165666981250979, val loss: 0.12330979108810425\n",
      "Epoch 8376: train loss: 8.413354953518137e-05, val loss: 0.12334980815649033\n",
      "Epoch 8377: train loss: 8.671711111674085e-05, val loss: 0.12578971683979034\n",
      "Epoch 8378: train loss: 6.691360613331199e-05, val loss: 0.1239008828997612\n",
      "Epoch 8379: train loss: 5.4766198445577174e-05, val loss: 0.11950328201055527\n",
      "Epoch 8380: train loss: 5.932411295361817e-05, val loss: 0.12041818350553513\n",
      "Epoch 8381: train loss: 4.83306976093445e-05, val loss: 0.1245417594909668\n",
      "Epoch 8382: train loss: 4.251988139003515e-05, val loss: 0.12369924038648605\n",
      "Epoch 8383: train loss: 5.016006252844818e-05, val loss: 0.12040803581476212\n",
      "Epoch 8384: train loss: 3.250194640713744e-05, val loss: 0.12002509087324142\n",
      "Epoch 8385: train loss: 4.0038426959654316e-05, val loss: 0.1223793774843216\n",
      "Epoch 8386: train loss: 3.250349618610926e-05, val loss: 0.12236000597476959\n",
      "Epoch 8387: train loss: 2.7493570087244734e-05, val loss: 0.12194737046957016\n",
      "Epoch 8388: train loss: 2.5254563297494315e-05, val loss: 0.12236865609884262\n",
      "Epoch 8389: train loss: 2.2425145289162174e-05, val loss: 0.12241118401288986\n",
      "Epoch 8390: train loss: 1.7770795238902792e-05, val loss: 0.12163084000349045\n",
      "Epoch 8391: train loss: 1.7175010725622997e-05, val loss: 0.12201734632253647\n",
      "Epoch 8392: train loss: 1.4270008250605315e-05, val loss: 0.12243097275495529\n",
      "Epoch 8393: train loss: 1.5244147107296158e-05, val loss: 0.12132354825735092\n",
      "Epoch 8394: train loss: 1.0182106962020043e-05, val loss: 0.12166354805231094\n",
      "Epoch 8395: train loss: 1.5174644431681372e-05, val loss: 0.12258841842412949\n",
      "Epoch 8396: train loss: 8.77430102264043e-06, val loss: 0.12252012640237808\n",
      "Epoch 8397: train loss: 1.1740444278984796e-05, val loss: 0.12128334492444992\n",
      "Epoch 8398: train loss: 1.1122456271550618e-05, val loss: 0.1217518076300621\n",
      "Epoch 8399: train loss: 7.495591944461921e-06, val loss: 0.12175877392292023\n",
      "Epoch 8400: train loss: 1.0693674994399771e-05, val loss: 0.12112561613321304\n",
      "Epoch 8401: train loss: 6.014000064169522e-06, val loss: 0.12099387496709824\n",
      "Epoch 8402: train loss: 8.01345959189348e-06, val loss: 0.12231270223855972\n",
      "Epoch 8403: train loss: 5.4540359997190535e-06, val loss: 0.12226457893848419\n",
      "Epoch 8404: train loss: 5.328450697561493e-06, val loss: 0.12156873941421509\n",
      "Epoch 8405: train loss: 4.530276328296168e-06, val loss: 0.12158017605543137\n",
      "Epoch 8406: train loss: 3.846131676255027e-06, val loss: 0.12229011207818985\n",
      "Epoch 8407: train loss: 3.6067342534806812e-06, val loss: 0.12205024808645248\n",
      "Epoch 8408: train loss: 2.765626732070814e-06, val loss: 0.12156840413808823\n",
      "Epoch 8409: train loss: 2.9836012345185736e-06, val loss: 0.12169714272022247\n",
      "Epoch 8410: train loss: 2.506050805095583e-06, val loss: 0.12241890281438828\n",
      "Epoch 8411: train loss: 2.1558337266469607e-06, val loss: 0.12278334051370621\n",
      "Epoch 8412: train loss: 2.3885654627520125e-06, val loss: 0.122167207300663\n",
      "Epoch 8413: train loss: 2.1114040009706514e-06, val loss: 0.12185881286859512\n",
      "Epoch 8414: train loss: 1.8139635358238593e-06, val loss: 0.12223968654870987\n",
      "Epoch 8415: train loss: 2.0014333586004796e-06, val loss: 0.12276732176542282\n",
      "Epoch 8416: train loss: 2.1011380795243895e-06, val loss: 0.12246010452508926\n",
      "Epoch 8417: train loss: 1.7204076812049607e-06, val loss: 0.12255817651748657\n",
      "Epoch 8418: train loss: 1.8863017885450972e-06, val loss: 0.12264678627252579\n",
      "Epoch 8419: train loss: 1.8011759266300942e-06, val loss: 0.12280403822660446\n",
      "Epoch 8420: train loss: 1.717058808026195e-06, val loss: 0.12270396947860718\n",
      "Epoch 8421: train loss: 1.8256566818308784e-06, val loss: 0.12304025143384933\n",
      "Epoch 8422: train loss: 1.8114751583198085e-06, val loss: 0.12268348038196564\n",
      "Epoch 8423: train loss: 2.216216898887069e-06, val loss: 0.12336224317550659\n",
      "Epoch 8424: train loss: 2.55822283179441e-06, val loss: 0.12271744012832642\n",
      "Epoch 8425: train loss: 3.7852119021408726e-06, val loss: 0.12386574596166611\n",
      "Epoch 8426: train loss: 4.038292900077067e-06, val loss: 0.12276559323072433\n",
      "Epoch 8427: train loss: 6.582176865777001e-06, val loss: 0.12383873760700226\n",
      "Epoch 8428: train loss: 9.165396477328613e-06, val loss: 0.12249457836151123\n",
      "Epoch 8429: train loss: 1.4860314877296332e-05, val loss: 0.12485583126544952\n",
      "Epoch 8430: train loss: 2.4410242986050434e-05, val loss: 0.12181130796670914\n",
      "Epoch 8431: train loss: 4.590629760059528e-05, val loss: 0.1263253092765808\n",
      "Epoch 8432: train loss: 8.873130718711764e-05, val loss: 0.12028586864471436\n",
      "Epoch 8433: train loss: 0.00017160359129775316, val loss: 0.1272604912519455\n",
      "Epoch 8434: train loss: 0.00024665246019139886, val loss: 0.12128599733114243\n",
      "Epoch 8435: train loss: 0.0003151560085825622, val loss: 0.1259356439113617\n",
      "Epoch 8436: train loss: 0.0003897924907505512, val loss: 0.1231452152132988\n",
      "Epoch 8437: train loss: 0.0005061867414042354, val loss: 0.12698447704315186\n",
      "Epoch 8438: train loss: 0.0005626397323794663, val loss: 0.12076836079359055\n",
      "Epoch 8439: train loss: 0.0005907008307985961, val loss: 0.12750549614429474\n",
      "Epoch 8440: train loss: 0.00043299878598190844, val loss: 0.12289793789386749\n",
      "Epoch 8441: train loss: 0.0002372007438680157, val loss: 0.1242935061454773\n",
      "Epoch 8442: train loss: 6.638163176830858e-05, val loss: 0.12428648769855499\n",
      "Epoch 8443: train loss: 1.9824834453174844e-05, val loss: 0.12307657301425934\n",
      "Epoch 8444: train loss: 8.851186430547386e-05, val loss: 0.125155508518219\n",
      "Epoch 8445: train loss: 0.00018353470659349114, val loss: 0.1209823489189148\n",
      "Epoch 8446: train loss: 0.00023504036653321236, val loss: 0.12575308978557587\n",
      "Epoch 8447: train loss: 0.00018514982366468757, val loss: 0.1219271793961525\n",
      "Epoch 8448: train loss: 0.00010257547546643764, val loss: 0.1224970743060112\n",
      "Epoch 8449: train loss: 3.862196535919793e-05, val loss: 0.12406370788812637\n",
      "Epoch 8450: train loss: 3.0547005735570565e-05, val loss: 0.12254796177148819\n",
      "Epoch 8451: train loss: 6.368668255163357e-05, val loss: 0.12382221221923828\n",
      "Epoch 8452: train loss: 9.100540773943067e-05, val loss: 0.12177145481109619\n",
      "Epoch 8453: train loss: 9.9937453342136e-05, val loss: 0.12401165813207626\n",
      "Epoch 8454: train loss: 8.16192477941513e-05, val loss: 0.12304367125034332\n",
      "Epoch 8455: train loss: 6.851033685961738e-05, val loss: 0.12182775884866714\n",
      "Epoch 8456: train loss: 7.241598359541968e-05, val loss: 0.12492378056049347\n",
      "Epoch 8457: train loss: 0.00010211273183813319, val loss: 0.12161238491535187\n",
      "Epoch 8458: train loss: 0.00017362943617627025, val loss: 0.12725548446178436\n",
      "Epoch 8459: train loss: 0.0003023618191946298, val loss: 0.12256769090890884\n",
      "Epoch 8460: train loss: 0.00043834655662067235, val loss: 0.12926022708415985\n",
      "Epoch 8461: train loss: 0.0003920070012100041, val loss: 0.1275177150964737\n",
      "Epoch 8462: train loss: 0.0001287737541133538, val loss: 0.12616659700870514\n",
      "Epoch 8463: train loss: 0.00010966253466904163, val loss: 0.12792515754699707\n",
      "Epoch 8464: train loss: 0.00018089753575623035, val loss: 0.12556354701519012\n",
      "Epoch 8465: train loss: 7.886039384175092e-05, val loss: 0.1259927749633789\n",
      "Epoch 8466: train loss: 8.594905375503004e-05, val loss: 0.12753871083259583\n",
      "Epoch 8467: train loss: 7.56930312491022e-05, val loss: 0.1267784684896469\n",
      "Epoch 8468: train loss: 6.653205491602421e-05, val loss: 0.12532930076122284\n",
      "Epoch 8469: train loss: 6.135345756774768e-05, val loss: 0.12571600079536438\n",
      "Epoch 8470: train loss: 3.9989594370126724e-05, val loss: 0.1268957406282425\n",
      "Epoch 8471: train loss: 4.676230309996754e-05, val loss: 0.1253436654806137\n",
      "Epoch 8472: train loss: 4.401093246997334e-05, val loss: 0.12437333911657333\n",
      "Epoch 8473: train loss: 3.347140955156647e-05, val loss: 0.12469784170389175\n",
      "Epoch 8474: train loss: 3.291976099717431e-05, val loss: 0.12606585025787354\n",
      "Epoch 8475: train loss: 2.3062069885781966e-05, val loss: 0.12526944279670715\n",
      "Epoch 8476: train loss: 2.9309680030564778e-05, val loss: 0.12286114692687988\n",
      "Epoch 8477: train loss: 2.191936619055923e-05, val loss: 0.12335631996393204\n",
      "Epoch 8478: train loss: 2.430605309200473e-05, val loss: 0.12405534833669662\n",
      "Epoch 8479: train loss: 2.3460199372493662e-05, val loss: 0.12407728284597397\n",
      "Epoch 8480: train loss: 1.7445869161747396e-05, val loss: 0.12321112304925919\n",
      "Epoch 8481: train loss: 2.2197587895789184e-05, val loss: 0.12424197047948837\n",
      "Epoch 8482: train loss: 1.2989333299628925e-05, val loss: 0.12550397217273712\n",
      "Epoch 8483: train loss: 1.7807295080274343e-05, val loss: 0.12414083629846573\n",
      "Epoch 8484: train loss: 1.8605025616125204e-05, val loss: 0.1240273267030716\n",
      "Epoch 8485: train loss: 2.4521868908777833e-05, val loss: 0.12443902343511581\n",
      "Epoch 8486: train loss: 2.981407305924222e-05, val loss: 0.12573206424713135\n",
      "Epoch 8487: train loss: 4.0740007534623146e-05, val loss: 0.12422940880060196\n",
      "Epoch 8488: train loss: 6.149138062028214e-05, val loss: 0.12499292939901352\n",
      "Epoch 8489: train loss: 8.457008516415954e-05, val loss: 0.12521104514598846\n",
      "Epoch 8490: train loss: 0.00011406225530663505, val loss: 0.12686216831207275\n",
      "Epoch 8491: train loss: 0.00012589692778419703, val loss: 0.12547433376312256\n",
      "Epoch 8492: train loss: 0.00023544537543784827, val loss: 0.1279904693365097\n",
      "Epoch 8493: train loss: 0.0004912842996418476, val loss: 0.12486167252063751\n",
      "Epoch 8494: train loss: 0.0006446954212151468, val loss: 0.1268691122531891\n",
      "Epoch 8495: train loss: 0.0002811098238453269, val loss: 0.1297382414340973\n",
      "Epoch 8496: train loss: 0.00011100285337306559, val loss: 0.125790074467659\n",
      "Epoch 8497: train loss: 0.00021911461954005063, val loss: 0.12488193809986115\n",
      "Epoch 8498: train loss: 0.00010273734369548038, val loss: 0.12767675518989563\n",
      "Epoch 8499: train loss: 0.00022953652660362422, val loss: 0.1262577772140503\n",
      "Epoch 8500: train loss: 6.489240331575274e-05, val loss: 0.12326667457818985\n",
      "Epoch 8501: train loss: 0.00011402080417610705, val loss: 0.12558001279830933\n",
      "Epoch 8502: train loss: 8.842233364703134e-05, val loss: 0.12600170075893402\n",
      "Epoch 8503: train loss: 9.346092701889575e-05, val loss: 0.12630927562713623\n",
      "Epoch 8504: train loss: 4.9995651352219284e-05, val loss: 0.12502948939800262\n",
      "Epoch 8505: train loss: 6.576583109563217e-05, val loss: 0.12254466861486435\n",
      "Epoch 8506: train loss: 4.679096309700981e-05, val loss: 0.12472604960203171\n",
      "Epoch 8507: train loss: 6.168428808450699e-05, val loss: 0.12638264894485474\n",
      "Epoch 8508: train loss: 3.4753822546917945e-05, val loss: 0.12327251583337784\n",
      "Epoch 8509: train loss: 4.098304270883091e-05, val loss: 0.12220793217420578\n",
      "Epoch 8510: train loss: 3.129259857814759e-05, val loss: 0.12528181076049805\n",
      "Epoch 8511: train loss: 3.066069621127099e-05, val loss: 0.12467384338378906\n",
      "Epoch 8512: train loss: 3.052422107430175e-05, val loss: 0.1238318458199501\n",
      "Epoch 8513: train loss: 1.5494686522288248e-05, val loss: 0.12345873564481735\n",
      "Epoch 8514: train loss: 2.237780427094549e-05, val loss: 0.12342458218336105\n",
      "Epoch 8515: train loss: 2.573881283751689e-05, val loss: 0.12499644607305527\n",
      "Epoch 8516: train loss: 2.288455652887933e-05, val loss: 0.12344098091125488\n",
      "Epoch 8517: train loss: 2.6245999833918177e-05, val loss: 0.12332136929035187\n",
      "Epoch 8518: train loss: 2.8515949452412315e-05, val loss: 0.12439980357885361\n",
      "Epoch 8519: train loss: 4.3216205085627735e-05, val loss: 0.12396673113107681\n",
      "Epoch 8520: train loss: 6.375701195793226e-05, val loss: 0.12381332367658615\n",
      "Epoch 8521: train loss: 0.00011033201008103788, val loss: 0.12472768127918243\n",
      "Epoch 8522: train loss: 0.00019321872969157994, val loss: 0.12322473526000977\n",
      "Epoch 8523: train loss: 0.0002730575215537101, val loss: 0.12258666753768921\n",
      "Epoch 8524: train loss: 0.00029684454784728587, val loss: 0.12316121906042099\n",
      "Epoch 8525: train loss: 0.00018173402349930257, val loss: 0.12360996007919312\n",
      "Epoch 8526: train loss: 3.7610418075928465e-05, val loss: 0.12224306166172028\n",
      "Epoch 8527: train loss: 4.456327224033885e-05, val loss: 0.12189389765262604\n",
      "Epoch 8528: train loss: 0.0001153452176367864, val loss: 0.1211717277765274\n",
      "Epoch 8529: train loss: 7.366390491370112e-05, val loss: 0.12177552282810211\n",
      "Epoch 8530: train loss: 1.6334061001543887e-05, val loss: 0.12353970855474472\n",
      "Epoch 8531: train loss: 5.564703678828664e-05, val loss: 0.12113344669342041\n",
      "Epoch 8532: train loss: 6.537460285471752e-05, val loss: 0.12096291035413742\n",
      "Epoch 8533: train loss: 2.7164014682057314e-05, val loss: 0.12386748939752579\n",
      "Epoch 8534: train loss: 3.225059845135547e-05, val loss: 0.12282387167215347\n",
      "Epoch 8535: train loss: 3.4592318115755916e-05, val loss: 0.12018042802810669\n",
      "Epoch 8536: train loss: 2.8686024961643852e-05, val loss: 0.12299870699644089\n",
      "Epoch 8537: train loss: 2.2835954951005988e-05, val loss: 0.1229265108704567\n",
      "Epoch 8538: train loss: 1.631676423130557e-05, val loss: 0.12152200192213058\n",
      "Epoch 8539: train loss: 2.503366704331711e-05, val loss: 0.12360116094350815\n",
      "Epoch 8540: train loss: 2.159901850973256e-05, val loss: 0.12185744196176529\n",
      "Epoch 8541: train loss: 1.12452053144807e-05, val loss: 0.12157871574163437\n",
      "Epoch 8542: train loss: 1.7839018255472183e-05, val loss: 0.12337999790906906\n",
      "Epoch 8543: train loss: 1.621168303245213e-05, val loss: 0.12185074388980865\n",
      "Epoch 8544: train loss: 1.0781584023789037e-05, val loss: 0.12181591987609863\n",
      "Epoch 8545: train loss: 1.056749897543341e-05, val loss: 0.12415192276239395\n",
      "Epoch 8546: train loss: 1.0416156328574289e-05, val loss: 0.12312410026788712\n",
      "Epoch 8547: train loss: 1.0431148439238314e-05, val loss: 0.1233559176325798\n",
      "Epoch 8548: train loss: 7.92101400293177e-06, val loss: 0.12486661970615387\n",
      "Epoch 8549: train loss: 7.085513971105684e-06, val loss: 0.12402262538671494\n",
      "Epoch 8550: train loss: 9.82459732767893e-06, val loss: 0.1246606856584549\n",
      "Epoch 8551: train loss: 8.382896339753643e-06, val loss: 0.12476815283298492\n",
      "Epoch 8552: train loss: 8.455255738226697e-06, val loss: 0.12539085745811462\n",
      "Epoch 8553: train loss: 1.1989756785624195e-05, val loss: 0.12497016042470932\n",
      "Epoch 8554: train loss: 1.5321218597819097e-05, val loss: 0.12540505826473236\n",
      "Epoch 8555: train loss: 2.689647226361558e-05, val loss: 0.12560497224330902\n",
      "Epoch 8556: train loss: 6.991536793066189e-05, val loss: 0.12649111449718475\n",
      "Epoch 8557: train loss: 0.00018817443924490362, val loss: 0.12662583589553833\n",
      "Epoch 8558: train loss: 0.0004465187666937709, val loss: 0.12751853466033936\n",
      "Epoch 8559: train loss: 0.0007646106532774866, val loss: 0.1275118887424469\n",
      "Epoch 8560: train loss: 0.0006422169972211123, val loss: 0.1295122355222702\n",
      "Epoch 8561: train loss: 0.0007477879989892244, val loss: 0.13160966336727142\n",
      "Epoch 8562: train loss: 0.0006304543931037188, val loss: 0.12937204539775848\n",
      "Epoch 8563: train loss: 0.00024954992113634944, val loss: 0.1304631382226944\n",
      "Epoch 8564: train loss: 0.0003961721376981586, val loss: 0.13864043354988098\n",
      "Epoch 8565: train loss: 0.00035257721901871264, val loss: 0.13268963992595673\n",
      "Epoch 8566: train loss: 0.00020676138228736818, val loss: 0.1310449093580246\n",
      "Epoch 8567: train loss: 0.00016790477093309164, val loss: 0.13365207612514496\n",
      "Epoch 8568: train loss: 0.00018875818932428956, val loss: 0.13233605027198792\n",
      "Epoch 8569: train loss: 0.00010865045624086633, val loss: 0.13180053234100342\n",
      "Epoch 8570: train loss: 0.0001346484204987064, val loss: 0.13043086230754852\n",
      "Epoch 8571: train loss: 0.0001480685023125261, val loss: 0.12902319431304932\n",
      "Epoch 8572: train loss: 0.00010860568727366626, val loss: 0.12915752828121185\n",
      "Epoch 8573: train loss: 0.00011278125020908192, val loss: 0.13327322900295258\n",
      "Epoch 8574: train loss: 8.583228191128e-05, val loss: 0.13206802308559418\n",
      "Epoch 8575: train loss: 6.125829531811178e-05, val loss: 0.12828241288661957\n",
      "Epoch 8576: train loss: 5.9673620853573084e-05, val loss: 0.12705634534358978\n",
      "Epoch 8577: train loss: 4.56080560979899e-05, val loss: 0.12849195301532745\n",
      "Epoch 8578: train loss: 5.884143683942966e-05, val loss: 0.1307869255542755\n",
      "Epoch 8579: train loss: 4.2952291551046073e-05, val loss: 0.1298879086971283\n",
      "Epoch 8580: train loss: 5.845215491717681e-05, val loss: 0.12841032445430756\n",
      "Epoch 8581: train loss: 3.6394667404238135e-05, val loss: 0.12711690366268158\n",
      "Epoch 8582: train loss: 3.621271389420144e-05, val loss: 0.1283813863992691\n",
      "Epoch 8583: train loss: 3.093255145358853e-05, val loss: 0.12889693677425385\n",
      "Epoch 8584: train loss: 1.5696337868575938e-05, val loss: 0.12853959202766418\n",
      "Epoch 8585: train loss: 2.8978573027416132e-05, val loss: 0.12862537801265717\n",
      "Epoch 8586: train loss: 1.6020121620385908e-05, val loss: 0.12844029068946838\n",
      "Epoch 8587: train loss: 2.4323244360857643e-05, val loss: 0.1285886913537979\n",
      "Epoch 8588: train loss: 2.235415558970999e-05, val loss: 0.12829124927520752\n",
      "Epoch 8589: train loss: 1.7732681953930296e-05, val loss: 0.12914781272411346\n",
      "Epoch 8590: train loss: 1.5390538464998826e-05, val loss: 0.1290968805551529\n",
      "Epoch 8591: train loss: 1.355958102067234e-05, val loss: 0.12882591784000397\n",
      "Epoch 8592: train loss: 8.104656444629654e-06, val loss: 0.1288173943758011\n",
      "Epoch 8593: train loss: 1.0770928383863065e-05, val loss: 0.12960441410541534\n",
      "Epoch 8594: train loss: 9.325383871328086e-06, val loss: 0.13021698594093323\n",
      "Epoch 8595: train loss: 8.959441402112134e-06, val loss: 0.1292492002248764\n",
      "Epoch 8596: train loss: 1.0654398465703707e-05, val loss: 0.12942062318325043\n",
      "Epoch 8597: train loss: 7.587163054267876e-06, val loss: 0.1299264132976532\n",
      "Epoch 8598: train loss: 8.76610374689335e-06, val loss: 0.1299787163734436\n",
      "Epoch 8599: train loss: 5.136268555361312e-06, val loss: 0.12974019348621368\n",
      "Epoch 8600: train loss: 5.457868155644974e-06, val loss: 0.1304011046886444\n",
      "Epoch 8601: train loss: 4.6524473873432726e-06, val loss: 0.13041061162948608\n",
      "Epoch 8602: train loss: 3.564815642675967e-06, val loss: 0.13003657758235931\n",
      "Epoch 8603: train loss: 4.229697879054584e-06, val loss: 0.13056686520576477\n",
      "Epoch 8604: train loss: 4.806292508874321e-06, val loss: 0.13075487315654755\n",
      "Epoch 8605: train loss: 3.8046255212975666e-06, val loss: 0.13127456605434418\n",
      "Epoch 8606: train loss: 4.009696112916572e-06, val loss: 0.13125169277191162\n",
      "Epoch 8607: train loss: 3.2970340271276655e-06, val loss: 0.13118217885494232\n",
      "Epoch 8608: train loss: 3.4642355331016006e-06, val loss: 0.13145309686660767\n",
      "Epoch 8609: train loss: 2.1059713617432863e-06, val loss: 0.1321323662996292\n",
      "Epoch 8610: train loss: 2.0489469534368254e-06, val loss: 0.13212202489376068\n",
      "Epoch 8611: train loss: 1.8967244841405773e-06, val loss: 0.13229644298553467\n",
      "Epoch 8612: train loss: 1.8698658550420078e-06, val loss: 0.13238762319087982\n",
      "Epoch 8613: train loss: 2.1297048533597263e-06, val loss: 0.1325087547302246\n",
      "Epoch 8614: train loss: 2.3599027372256387e-06, val loss: 0.13345834612846375\n",
      "Epoch 8615: train loss: 2.5969984562834725e-06, val loss: 0.13317926228046417\n",
      "Epoch 8616: train loss: 2.4579028377047507e-06, val loss: 0.13396354019641876\n",
      "Epoch 8617: train loss: 3.0595329008065164e-06, val loss: 0.13346625864505768\n",
      "Epoch 8618: train loss: 3.6973540318285814e-06, val loss: 0.13427522778511047\n",
      "Epoch 8619: train loss: 6.261253474804107e-06, val loss: 0.1340339183807373\n",
      "Epoch 8620: train loss: 1.567375147715211e-05, val loss: 0.1343628168106079\n",
      "Epoch 8621: train loss: 4.695015377365053e-05, val loss: 0.13420799374580383\n",
      "Epoch 8622: train loss: 0.00015943024482112378, val loss: 0.1338183432817459\n",
      "Epoch 8623: train loss: 0.000490141159389168, val loss: 0.13478437066078186\n",
      "Epoch 8624: train loss: 0.001213438343256712, val loss: 0.12844927608966827\n",
      "Epoch 8625: train loss: 0.0015200088964775205, val loss: 0.1387716829776764\n",
      "Epoch 8626: train loss: 0.0008527643512934446, val loss: 0.12047834694385529\n",
      "Epoch 8627: train loss: 0.0008082861895672977, val loss: 0.12687230110168457\n",
      "Epoch 8628: train loss: 0.0007087068515829742, val loss: 0.12773601710796356\n",
      "Epoch 8629: train loss: 0.0005878782831132412, val loss: 0.12723301351070404\n",
      "Epoch 8630: train loss: 0.0005955850938335061, val loss: 0.11541014164686203\n",
      "Epoch 8631: train loss: 0.0005180371226742864, val loss: 0.1218428984284401\n",
      "Epoch 8632: train loss: 0.000353208597516641, val loss: 0.12525136768817902\n",
      "Epoch 8633: train loss: 0.00030095878173597157, val loss: 0.1219353899359703\n",
      "Epoch 8634: train loss: 0.00017702368495520204, val loss: 0.1175236627459526\n",
      "Epoch 8635: train loss: 0.00012636861356440932, val loss: 0.11778489500284195\n",
      "Epoch 8636: train loss: 0.00018119295418728143, val loss: 0.11967017501592636\n",
      "Epoch 8637: train loss: 0.0001580746320541948, val loss: 0.11913090199232101\n",
      "Epoch 8638: train loss: 0.00020090870384592563, val loss: 0.11933866888284683\n",
      "Epoch 8639: train loss: 0.00018073672254104167, val loss: 0.11758377403020859\n",
      "Epoch 8640: train loss: 0.00011076348164351657, val loss: 0.11738767474889755\n",
      "Epoch 8641: train loss: 9.480351582169533e-05, val loss: 0.11813115328550339\n",
      "Epoch 8642: train loss: 5.467578739626333e-05, val loss: 0.11899721622467041\n",
      "Epoch 8643: train loss: 5.152359153726138e-05, val loss: 0.11918210238218307\n",
      "Epoch 8644: train loss: 8.250013343058527e-05, val loss: 0.11896168440580368\n",
      "Epoch 8645: train loss: 7.285615720320493e-05, val loss: 0.11891000717878342\n",
      "Epoch 8646: train loss: 8.342543878825381e-05, val loss: 0.11674334853887558\n",
      "Epoch 8647: train loss: 5.504153887159191e-05, val loss: 0.11759883165359497\n",
      "Epoch 8648: train loss: 4.1996139771072194e-05, val loss: 0.11885323375463486\n",
      "Epoch 8649: train loss: 2.1284880858729593e-05, val loss: 0.11871285736560822\n",
      "Epoch 8650: train loss: 2.832902828231454e-05, val loss: 0.1182548776268959\n",
      "Epoch 8651: train loss: 3.280800592619926e-05, val loss: 0.11829739063978195\n",
      "Epoch 8652: train loss: 3.3998392609646544e-05, val loss: 0.11943487077951431\n",
      "Epoch 8653: train loss: 4.2462423152755946e-05, val loss: 0.11845393478870392\n",
      "Epoch 8654: train loss: 2.566076182120014e-05, val loss: 0.11898501217365265\n",
      "Epoch 8655: train loss: 2.2790245566284284e-05, val loss: 0.11941466480493546\n",
      "Epoch 8656: train loss: 1.1345885468472261e-05, val loss: 0.11917014420032501\n",
      "Epoch 8657: train loss: 1.2631520803552121e-05, val loss: 0.11892717331647873\n",
      "Epoch 8658: train loss: 1.3647032574226614e-05, val loss: 0.11898701637983322\n",
      "Epoch 8659: train loss: 1.867845821834635e-05, val loss: 0.11980868875980377\n",
      "Epoch 8660: train loss: 1.7381966245011427e-05, val loss: 0.11939828842878342\n",
      "Epoch 8661: train loss: 1.661988972045947e-05, val loss: 0.11983340233564377\n",
      "Epoch 8662: train loss: 1.0777585885080043e-05, val loss: 0.11968352645635605\n",
      "Epoch 8663: train loss: 7.005601219134405e-06, val loss: 0.11997032165527344\n",
      "Epoch 8664: train loss: 5.860536930413218e-06, val loss: 0.12044697999954224\n",
      "Epoch 8665: train loss: 5.6538892749813385e-06, val loss: 0.12021259218454361\n",
      "Epoch 8666: train loss: 7.786322385072708e-06, val loss: 0.12042595446109772\n",
      "Epoch 8667: train loss: 8.940051884565037e-06, val loss: 0.12060805410146713\n",
      "Epoch 8668: train loss: 7.888895197538659e-06, val loss: 0.1210385337471962\n",
      "Epoch 8669: train loss: 7.382142030110117e-06, val loss: 0.12040350586175919\n",
      "Epoch 8670: train loss: 4.509641712502344e-06, val loss: 0.12103326618671417\n",
      "Epoch 8671: train loss: 3.426304829190485e-06, val loss: 0.12172650545835495\n",
      "Epoch 8672: train loss: 2.4855319225025596e-06, val loss: 0.12143564224243164\n",
      "Epoch 8673: train loss: 3.3574883673281875e-06, val loss: 0.1212330088019371\n",
      "Epoch 8674: train loss: 3.3281996820733184e-06, val loss: 0.12132173776626587\n",
      "Epoch 8675: train loss: 4.514702595770359e-06, val loss: 0.12218789011240005\n",
      "Epoch 8676: train loss: 4.053329575981479e-06, val loss: 0.12192197144031525\n",
      "Epoch 8677: train loss: 3.6639539757743478e-06, val loss: 0.12194972485303879\n",
      "Epoch 8678: train loss: 2.924147338490002e-06, val loss: 0.12221329659223557\n",
      "Epoch 8679: train loss: 1.827388814490405e-06, val loss: 0.12303674221038818\n",
      "Epoch 8680: train loss: 2.274115558975609e-06, val loss: 0.12251734733581543\n",
      "Epoch 8681: train loss: 1.8385003386356402e-06, val loss: 0.12252847105264664\n",
      "Epoch 8682: train loss: 1.5679013358749216e-06, val loss: 0.12299312651157379\n",
      "Epoch 8683: train loss: 1.9179262835677946e-06, val loss: 0.12328926473855972\n",
      "Epoch 8684: train loss: 1.7506362155472743e-06, val loss: 0.12324831634759903\n",
      "Epoch 8685: train loss: 1.8334737887926167e-06, val loss: 0.12354137748479843\n",
      "Epoch 8686: train loss: 3.3237363368243678e-06, val loss: 0.12342426925897598\n",
      "Epoch 8687: train loss: 8.881441317498684e-06, val loss: 0.12434454262256622\n",
      "Epoch 8688: train loss: 2.701917946978938e-05, val loss: 0.12312974780797958\n",
      "Epoch 8689: train loss: 9.335856884717941e-05, val loss: 0.1250733584165573\n",
      "Epoch 8690: train loss: 0.00027371273608878255, val loss: 0.12291038036346436\n",
      "Epoch 8691: train loss: 0.0006473858375102282, val loss: 0.1246589943766594\n",
      "Epoch 8692: train loss: 0.0010100557701662183, val loss: 0.12476266920566559\n",
      "Epoch 8693: train loss: 0.0009737125947140157, val loss: 0.12903325259685516\n",
      "Epoch 8694: train loss: 0.0005358502385206521, val loss: 0.12810198962688446\n",
      "Epoch 8695: train loss: 0.0005814910982735455, val loss: 0.13132499158382416\n",
      "Epoch 8696: train loss: 0.0005508498870767653, val loss: 0.1256486475467682\n",
      "Epoch 8697: train loss: 0.00015477885608561337, val loss: 0.12694962322711945\n",
      "Epoch 8698: train loss: 0.00015022458683233708, val loss: 0.13004134595394135\n",
      "Epoch 8699: train loss: 0.0002938915276899934, val loss: 0.12748806178569794\n",
      "Epoch 8700: train loss: 0.00020933595078531653, val loss: 0.12780146300792694\n",
      "Epoch 8701: train loss: 0.0003227923298254609, val loss: 0.12712334096431732\n",
      "Epoch 8702: train loss: 0.00021110099623911083, val loss: 0.12991946935653687\n",
      "Epoch 8703: train loss: 6.93040419719182e-05, val loss: 0.12919266521930695\n",
      "Epoch 8704: train loss: 0.00012185994273750111, val loss: 0.12715940177440643\n",
      "Epoch 8705: train loss: 8.031507604755461e-05, val loss: 0.12888453900814056\n",
      "Epoch 8706: train loss: 0.00016076499014161527, val loss: 0.12831442058086395\n",
      "Epoch 8707: train loss: 0.0001565368438605219, val loss: 0.12825261056423187\n",
      "Epoch 8708: train loss: 6.942629988770932e-05, val loss: 0.12697246670722961\n",
      "Epoch 8709: train loss: 7.144927076296881e-05, val loss: 0.1275946944952011\n",
      "Epoch 8710: train loss: 3.0455274099949747e-05, val loss: 0.12845346331596375\n",
      "Epoch 8711: train loss: 7.444511720677838e-05, val loss: 0.12638109922409058\n",
      "Epoch 8712: train loss: 9.577174932928756e-05, val loss: 0.1260429471731186\n",
      "Epoch 8713: train loss: 5.6376618886133656e-05, val loss: 0.1252155900001526\n",
      "Epoch 8714: train loss: 5.26175026607234e-05, val loss: 0.1258396953344345\n",
      "Epoch 8715: train loss: 1.70866751432186e-05, val loss: 0.1264047771692276\n",
      "Epoch 8716: train loss: 3.138615647912957e-05, val loss: 0.1257326453924179\n",
      "Epoch 8717: train loss: 5.0675989768933505e-05, val loss: 0.12603606283664703\n",
      "Epoch 8718: train loss: 4.152212932240218e-05, val loss: 0.1252317577600479\n",
      "Epoch 8719: train loss: 3.597286558942869e-05, val loss: 0.1255846470594406\n",
      "Epoch 8720: train loss: 1.578916453581769e-05, val loss: 0.12505871057510376\n",
      "Epoch 8721: train loss: 1.2155863259977195e-05, val loss: 0.12440772354602814\n",
      "Epoch 8722: train loss: 2.6206615075352602e-05, val loss: 0.12525252997875214\n",
      "Epoch 8723: train loss: 2.3517044610343874e-05, val loss: 0.12492813169956207\n",
      "Epoch 8724: train loss: 2.6206473194179125e-05, val loss: 0.12514711916446686\n",
      "Epoch 8725: train loss: 1.5301046005333774e-05, val loss: 0.12488558143377304\n",
      "Epoch 8726: train loss: 4.578404059429886e-06, val loss: 0.12515050172805786\n",
      "Epoch 8727: train loss: 1.331709609075915e-05, val loss: 0.12556982040405273\n",
      "Epoch 8728: train loss: 1.3331049558473751e-05, val loss: 0.12476807087659836\n",
      "Epoch 8729: train loss: 1.387582415190991e-05, val loss: 0.1251680701971054\n",
      "Epoch 8730: train loss: 1.4461091268458404e-05, val loss: 0.1251913607120514\n",
      "Epoch 8731: train loss: 6.75257115290151e-06, val loss: 0.12515191733837128\n",
      "Epoch 8732: train loss: 3.07480650008074e-06, val loss: 0.1251996010541916\n",
      "Epoch 8733: train loss: 8.749746484681964e-06, val loss: 0.12547798454761505\n",
      "Epoch 8734: train loss: 8.248646736319643e-06, val loss: 0.12583544850349426\n",
      "Epoch 8735: train loss: 7.2830448516469914e-06, val loss: 0.12503008544445038\n",
      "Epoch 8736: train loss: 9.700604095996823e-06, val loss: 0.12583692371845245\n",
      "Epoch 8737: train loss: 4.5566566768684424e-06, val loss: 0.12583844363689423\n",
      "Epoch 8738: train loss: 1.6037680552472011e-06, val loss: 0.12535178661346436\n",
      "Epoch 8739: train loss: 4.078256552020321e-06, val loss: 0.12599419057369232\n",
      "Epoch 8740: train loss: 5.867342679266585e-06, val loss: 0.12557999789714813\n",
      "Epoch 8741: train loss: 5.6243543440359645e-06, val loss: 0.12576891481876373\n",
      "Epoch 8742: train loss: 3.3030273698386736e-06, val loss: 0.1261068880558014\n",
      "Epoch 8743: train loss: 3.49282413480978e-06, val loss: 0.1262899488210678\n",
      "Epoch 8744: train loss: 6.383998425008031e-06, val loss: 0.12669941782951355\n",
      "Epoch 8745: train loss: 9.868701454252005e-06, val loss: 0.12619592249393463\n",
      "Epoch 8746: train loss: 1.4820082469668705e-05, val loss: 0.12668809294700623\n",
      "Epoch 8747: train loss: 2.5950592316803522e-05, val loss: 0.12673453986644745\n",
      "Epoch 8748: train loss: 6.073311305954121e-05, val loss: 0.1271279901266098\n",
      "Epoch 8749: train loss: 0.0001624469005037099, val loss: 0.12698359787464142\n",
      "Epoch 8750: train loss: 0.0004565318231470883, val loss: 0.12650127708911896\n",
      "Epoch 8751: train loss: 0.0010622249683365226, val loss: 0.12706004083156586\n",
      "Epoch 8752: train loss: 0.001425929949618876, val loss: 0.1227746233344078\n",
      "Epoch 8753: train loss: 0.0005099894478917122, val loss: 0.12759317457675934\n",
      "Epoch 8754: train loss: 0.0006924115004949272, val loss: 0.1254352331161499\n",
      "Epoch 8755: train loss: 0.0005012609763070941, val loss: 0.11732778698205948\n",
      "Epoch 8756: train loss: 0.0006950318929739296, val loss: 0.12242048978805542\n",
      "Epoch 8757: train loss: 0.000273756479145959, val loss: 0.12975269556045532\n",
      "Epoch 8758: train loss: 0.0002485640870872885, val loss: 0.12312629073858261\n",
      "Epoch 8759: train loss: 0.000277700019069016, val loss: 0.11632479727268219\n",
      "Epoch 8760: train loss: 0.00015133171109482646, val loss: 0.1196836456656456\n",
      "Epoch 8761: train loss: 0.00019547970441635698, val loss: 0.11893194168806076\n",
      "Epoch 8762: train loss: 0.00014304526848718524, val loss: 0.11859535425901413\n",
      "Epoch 8763: train loss: 0.0001122297762776725, val loss: 0.11943484842777252\n",
      "Epoch 8764: train loss: 0.0001417488674633205, val loss: 0.11807078123092651\n",
      "Epoch 8765: train loss: 5.479301398736425e-05, val loss: 0.11633660644292831\n",
      "Epoch 8766: train loss: 0.00010667073365766555, val loss: 0.11726533621549606\n",
      "Epoch 8767: train loss: 6.110910180723295e-05, val loss: 0.11775362491607666\n",
      "Epoch 8768: train loss: 8.254032582044601e-05, val loss: 0.11680646240711212\n",
      "Epoch 8769: train loss: 5.1508115575416014e-05, val loss: 0.11775628477334976\n",
      "Epoch 8770: train loss: 6.380995182553306e-05, val loss: 0.11578967422246933\n",
      "Epoch 8771: train loss: 3.7117300962563604e-05, val loss: 0.11521174758672714\n",
      "Epoch 8772: train loss: 3.640619615907781e-05, val loss: 0.11626918613910675\n",
      "Epoch 8773: train loss: 4.3687981815310195e-05, val loss: 0.11533989012241364\n",
      "Epoch 8774: train loss: 2.7520034564076923e-05, val loss: 0.11604118347167969\n",
      "Epoch 8775: train loss: 3.571713386918418e-05, val loss: 0.117100290954113\n",
      "Epoch 8776: train loss: 2.4150069293682463e-05, val loss: 0.1166306734085083\n",
      "Epoch 8777: train loss: 2.765163480944466e-05, val loss: 0.11579666286706924\n",
      "Epoch 8778: train loss: 1.466194589738734e-05, val loss: 0.11668449640274048\n",
      "Epoch 8779: train loss: 2.3776014131726697e-05, val loss: 0.11701419204473495\n",
      "Epoch 8780: train loss: 1.426903145329561e-05, val loss: 0.11646880954504013\n",
      "Epoch 8781: train loss: 1.947340388142038e-05, val loss: 0.11705310642719269\n",
      "Epoch 8782: train loss: 1.2798474017472472e-05, val loss: 0.11675035953521729\n",
      "Epoch 8783: train loss: 1.4940814253350254e-05, val loss: 0.11661574989557266\n",
      "Epoch 8784: train loss: 9.181517270917539e-06, val loss: 0.1171477809548378\n",
      "Epoch 8785: train loss: 1.1067405466747005e-05, val loss: 0.11741416901350021\n",
      "Epoch 8786: train loss: 8.920572327042464e-06, val loss: 0.11814018338918686\n",
      "Epoch 8787: train loss: 9.42764563660603e-06, val loss: 0.11857038736343384\n",
      "Epoch 8788: train loss: 7.96198128227843e-06, val loss: 0.1182994395494461\n",
      "Epoch 8789: train loss: 7.622858447575709e-06, val loss: 0.11808174103498459\n",
      "Epoch 8790: train loss: 6.819834197813179e-06, val loss: 0.11888813972473145\n",
      "Epoch 8791: train loss: 4.5961246541992296e-06, val loss: 0.11920684576034546\n",
      "Epoch 8792: train loss: 6.088994268793613e-06, val loss: 0.11892211437225342\n",
      "Epoch 8793: train loss: 3.989829565398395e-06, val loss: 0.11959626525640488\n",
      "Epoch 8794: train loss: 5.669190159096615e-06, val loss: 0.12022532522678375\n",
      "Epoch 8795: train loss: 3.491121560728061e-06, val loss: 0.12103648483753204\n",
      "Epoch 8796: train loss: 4.633007847587578e-06, val loss: 0.12090964615345001\n",
      "Epoch 8797: train loss: 2.4528876565454993e-06, val loss: 0.1206570640206337\n",
      "Epoch 8798: train loss: 3.1035383472044487e-06, val loss: 0.12119865417480469\n",
      "Epoch 8799: train loss: 2.4613932509964798e-06, val loss: 0.1219477429986\n",
      "Epoch 8800: train loss: 2.3782686184858903e-06, val loss: 0.12213387340307236\n",
      "Epoch 8801: train loss: 2.5880397060973337e-06, val loss: 0.12212236225605011\n",
      "Epoch 8802: train loss: 2.0436534668988315e-06, val loss: 0.1228233352303505\n",
      "Epoch 8803: train loss: 2.088129349431256e-06, val loss: 0.12286198139190674\n",
      "Epoch 8804: train loss: 1.5820209000594332e-06, val loss: 0.12293612957000732\n",
      "Epoch 8805: train loss: 1.8652627886694972e-06, val loss: 0.1233954057097435\n",
      "Epoch 8806: train loss: 1.437507876289601e-06, val loss: 0.12418685108423233\n",
      "Epoch 8807: train loss: 1.8115066495738574e-06, val loss: 0.12333405017852783\n",
      "Epoch 8808: train loss: 1.6684930415067356e-06, val loss: 0.12415307760238647\n",
      "Epoch 8809: train loss: 1.951007106981706e-06, val loss: 0.1240767240524292\n",
      "Epoch 8810: train loss: 3.6463397918851115e-06, val loss: 0.12514536082744598\n",
      "Epoch 8811: train loss: 7.73075589677319e-06, val loss: 0.12346508353948593\n",
      "Epoch 8812: train loss: 2.1169404135434888e-05, val loss: 0.1273147612810135\n",
      "Epoch 8813: train loss: 6.513301923405379e-05, val loss: 0.1201343759894371\n",
      "Epoch 8814: train loss: 0.00019925377273466438, val loss: 0.1317598968744278\n",
      "Epoch 8815: train loss: 0.0005174586549401283, val loss: 0.11548703908920288\n",
      "Epoch 8816: train loss: 0.001026809448376298, val loss: 0.13559067249298096\n",
      "Epoch 8817: train loss: 0.001143322791904211, val loss: 0.12156569957733154\n",
      "Epoch 8818: train loss: 0.0005910216132178903, val loss: 0.12789605557918549\n",
      "Epoch 8819: train loss: 0.0004230944032315165, val loss: 0.13411344587802887\n",
      "Epoch 8820: train loss: 0.0006935984129086137, val loss: 0.1282918006181717\n",
      "Epoch 8821: train loss: 0.0002235882420791313, val loss: 0.12397650629281998\n",
      "Epoch 8822: train loss: 0.00017141392163466662, val loss: 0.12431752681732178\n",
      "Epoch 8823: train loss: 0.0003057455469388515, val loss: 0.12304437160491943\n",
      "Epoch 8824: train loss: 0.00019033050921279937, val loss: 0.1205182820558548\n",
      "Epoch 8825: train loss: 0.00034743623109534383, val loss: 0.12320055067539215\n",
      "Epoch 8826: train loss: 0.00016207782027777284, val loss: 0.12469746172428131\n",
      "Epoch 8827: train loss: 0.00015593221178278327, val loss: 0.12451789528131485\n",
      "Epoch 8828: train loss: 9.014926763484254e-05, val loss: 0.12228532135486603\n",
      "Epoch 8829: train loss: 0.00010840005415957421, val loss: 0.11972504109144211\n",
      "Epoch 8830: train loss: 0.00014459629892371595, val loss: 0.12106483429670334\n",
      "Epoch 8831: train loss: 0.00013554719043895602, val loss: 0.1213718056678772\n",
      "Epoch 8832: train loss: 9.90773260127753e-05, val loss: 0.12186437845230103\n",
      "Epoch 8833: train loss: 6.701797246932983e-05, val loss: 0.1219276413321495\n",
      "Epoch 8834: train loss: 3.511127215460874e-05, val loss: 0.12163519114255905\n",
      "Epoch 8835: train loss: 8.087985042948276e-05, val loss: 0.12155814468860626\n",
      "Epoch 8836: train loss: 6.24334134045057e-05, val loss: 0.12055831402540207\n",
      "Epoch 8837: train loss: 8.710434485692531e-05, val loss: 0.1215745136141777\n",
      "Epoch 8838: train loss: 2.708077590796165e-05, val loss: 0.12135709822177887\n",
      "Epoch 8839: train loss: 4.114615512662567e-05, val loss: 0.12062704563140869\n",
      "Epoch 8840: train loss: 2.394081457168795e-05, val loss: 0.12140756845474243\n",
      "Epoch 8841: train loss: 5.35619656147901e-05, val loss: 0.12236952781677246\n",
      "Epoch 8842: train loss: 4.4985128624830395e-05, val loss: 0.12231092900037766\n",
      "Epoch 8843: train loss: 3.522876068018377e-05, val loss: 0.12041723728179932\n",
      "Epoch 8844: train loss: 2.336176112294197e-05, val loss: 0.12121381610631943\n",
      "Epoch 8845: train loss: 1.647032877372112e-05, val loss: 0.12257955223321915\n",
      "Epoch 8846: train loss: 2.270818549732212e-05, val loss: 0.12228772789239883\n",
      "Epoch 8847: train loss: 2.478621172485873e-05, val loss: 0.12250029295682907\n",
      "Epoch 8848: train loss: 2.3728396627120674e-05, val loss: 0.12243111431598663\n",
      "Epoch 8849: train loss: 2.043718814093154e-05, val loss: 0.12263812869787216\n",
      "Epoch 8850: train loss: 9.997639608627651e-06, val loss: 0.12252853065729141\n",
      "Epoch 8851: train loss: 1.3029420188104268e-05, val loss: 0.12311206012964249\n",
      "Epoch 8852: train loss: 1.1837100828415714e-05, val loss: 0.1233508512377739\n",
      "Epoch 8853: train loss: 1.8319275113753974e-05, val loss: 0.12251710146665573\n",
      "Epoch 8854: train loss: 1.6153493561432697e-05, val loss: 0.12367186695337296\n",
      "Epoch 8855: train loss: 9.4187043941929e-06, val loss: 0.1241774782538414\n",
      "Epoch 8856: train loss: 8.539134796592407e-06, val loss: 0.12335330247879028\n",
      "Epoch 8857: train loss: 4.64398181065917e-06, val loss: 0.12368621677160263\n",
      "Epoch 8858: train loss: 9.056576345756184e-06, val loss: 0.12434899806976318\n",
      "Epoch 8859: train loss: 1.1961526070081163e-05, val loss: 0.12457817792892456\n",
      "Epoch 8860: train loss: 1.3547624803322833e-05, val loss: 0.12423820793628693\n",
      "Epoch 8861: train loss: 2.0234636394889094e-05, val loss: 0.12530136108398438\n",
      "Epoch 8862: train loss: 3.304888014099561e-05, val loss: 0.12502005696296692\n",
      "Epoch 8863: train loss: 4.979544610250741e-05, val loss: 0.1248321533203125\n",
      "Epoch 8864: train loss: 6.546068470925093e-05, val loss: 0.12481731176376343\n",
      "Epoch 8865: train loss: 7.83038412919268e-05, val loss: 0.12570175528526306\n",
      "Epoch 8866: train loss: 2.530132042011246e-05, val loss: 0.12650588154792786\n",
      "Epoch 8867: train loss: 1.9964931198046543e-05, val loss: 0.12519463896751404\n",
      "Epoch 8868: train loss: 4.6765584556851536e-05, val loss: 0.12647835910320282\n",
      "Epoch 8869: train loss: 1.9045450244448148e-05, val loss: 0.12747232615947723\n",
      "Epoch 8870: train loss: 2.149721512978431e-05, val loss: 0.12462534755468369\n",
      "Epoch 8871: train loss: 2.5575531253707595e-05, val loss: 0.12636259198188782\n",
      "Epoch 8872: train loss: 2.4010194465517998e-05, val loss: 0.1272062212228775\n",
      "Epoch 8873: train loss: 2.2821401216788217e-05, val loss: 0.12596164643764496\n",
      "Epoch 8874: train loss: 1.529850487713702e-05, val loss: 0.12571080029010773\n",
      "Epoch 8875: train loss: 2.1762634787592106e-05, val loss: 0.12726931273937225\n",
      "Epoch 8876: train loss: 2.012156073760707e-05, val loss: 0.12532739341259003\n",
      "Epoch 8877: train loss: 1.6408022929681465e-05, val loss: 0.1264161616563797\n",
      "Epoch 8878: train loss: 1.9704750229720958e-05, val loss: 0.12554357945919037\n",
      "Epoch 8879: train loss: 2.5967379770008847e-05, val loss: 0.12778913974761963\n",
      "Epoch 8880: train loss: 4.2612195102265105e-05, val loss: 0.12276642769575119\n",
      "Epoch 8881: train loss: 8.695124415680766e-05, val loss: 0.1310572624206543\n",
      "Epoch 8882: train loss: 0.00018153611745219678, val loss: 0.1190946102142334\n",
      "Epoch 8883: train loss: 0.0004734505491796881, val loss: 0.13409440219402313\n",
      "Epoch 8884: train loss: 0.000564573158044368, val loss: 0.12058205902576447\n",
      "Epoch 8885: train loss: 0.00045450704055838287, val loss: 0.12700505554676056\n",
      "Epoch 8886: train loss: 0.00012800788681488484, val loss: 0.13049718737602234\n",
      "Epoch 8887: train loss: 0.00024328517611138523, val loss: 0.1223018690943718\n",
      "Epoch 8888: train loss: 0.00017193829989992082, val loss: 0.12029599398374557\n",
      "Epoch 8889: train loss: 0.00013109718565829098, val loss: 0.12598846852779388\n",
      "Epoch 8890: train loss: 0.00012017684639431536, val loss: 0.12598156929016113\n",
      "Epoch 8891: train loss: 0.00011580692080315202, val loss: 0.12062176316976547\n",
      "Epoch 8892: train loss: 0.00010664645378710702, val loss: 0.12022409588098526\n",
      "Epoch 8893: train loss: 9.334180504083633e-05, val loss: 0.12271972745656967\n",
      "Epoch 8894: train loss: 6.778590613976121e-05, val loss: 0.12335269898176193\n",
      "Epoch 8895: train loss: 6.607975228689611e-05, val loss: 0.12131088227033615\n",
      "Epoch 8896: train loss: 5.5407082982128486e-05, val loss: 0.12058035284280777\n",
      "Epoch 8897: train loss: 4.4091586460126564e-05, val loss: 0.12244334071874619\n",
      "Epoch 8898: train loss: 6.985485379118472e-05, val loss: 0.1226993128657341\n",
      "Epoch 8899: train loss: 2.8148124329163693e-05, val loss: 0.1218215823173523\n",
      "Epoch 8900: train loss: 4.7372410335810855e-05, val loss: 0.12108501046895981\n",
      "Epoch 8901: train loss: 3.322583870613016e-05, val loss: 0.12259461730718613\n",
      "Epoch 8902: train loss: 2.2500838895211928e-05, val loss: 0.12341039627790451\n",
      "Epoch 8903: train loss: 3.1238178053172305e-05, val loss: 0.12204287201166153\n",
      "Epoch 8904: train loss: 2.4754110199864954e-05, val loss: 0.12197160720825195\n",
      "Epoch 8905: train loss: 2.656299875525292e-05, val loss: 0.12292511761188507\n",
      "Epoch 8906: train loss: 2.0462795873754658e-05, val loss: 0.12274272739887238\n",
      "Epoch 8907: train loss: 1.9295972379040904e-05, val loss: 0.12095936387777328\n",
      "Epoch 8908: train loss: 1.4260446732805576e-05, val loss: 0.12163107842206955\n",
      "Epoch 8909: train loss: 1.5474428437300958e-05, val loss: 0.12360220402479172\n",
      "Epoch 8910: train loss: 1.1874922165588941e-05, val loss: 0.12324857711791992\n",
      "Epoch 8911: train loss: 1.3877081983082462e-05, val loss: 0.12197591364383698\n",
      "Epoch 8912: train loss: 1.3331830814422574e-05, val loss: 0.12210917472839355\n",
      "Epoch 8913: train loss: 9.553361451253295e-06, val loss: 0.12339217960834503\n",
      "Epoch 8914: train loss: 8.73487806529738e-06, val loss: 0.12319134920835495\n",
      "Epoch 8915: train loss: 9.742289876157884e-06, val loss: 0.12253047525882721\n",
      "Epoch 8916: train loss: 6.343759650917491e-06, val loss: 0.12281601876020432\n",
      "Epoch 8917: train loss: 6.110869435360655e-06, val loss: 0.12345901876688004\n",
      "Epoch 8918: train loss: 8.315119885082822e-06, val loss: 0.12322535365819931\n",
      "Epoch 8919: train loss: 5.315856014931342e-06, val loss: 0.12248920649290085\n",
      "Epoch 8920: train loss: 6.625265996262897e-06, val loss: 0.12355747073888779\n",
      "Epoch 8921: train loss: 5.983069968351629e-06, val loss: 0.1239781528711319\n",
      "Epoch 8922: train loss: 7.016413746896433e-06, val loss: 0.12345290184020996\n",
      "Epoch 8923: train loss: 5.545841304410715e-06, val loss: 0.1232505589723587\n",
      "Epoch 8924: train loss: 4.751468168251449e-06, val loss: 0.12382151186466217\n",
      "Epoch 8925: train loss: 5.667131517839152e-06, val loss: 0.12364840507507324\n",
      "Epoch 8926: train loss: 6.240398306545103e-06, val loss: 0.12366809695959091\n",
      "Epoch 8927: train loss: 7.332372661039699e-06, val loss: 0.12361810356378555\n",
      "Epoch 8928: train loss: 9.061069249582943e-06, val loss: 0.12407735735177994\n",
      "Epoch 8929: train loss: 1.70827261172235e-05, val loss: 0.12370697408914566\n",
      "Epoch 8930: train loss: 3.892910171998665e-05, val loss: 0.12420814484357834\n",
      "Epoch 8931: train loss: 9.814647637540475e-05, val loss: 0.12328123301267624\n",
      "Epoch 8932: train loss: 0.0002585975453257561, val loss: 0.12594972550868988\n",
      "Epoch 8933: train loss: 0.0006226574769243598, val loss: 0.12251061201095581\n",
      "Epoch 8934: train loss: 0.0013767749769613147, val loss: 0.12907056510448456\n",
      "Epoch 8935: train loss: 0.002140764379873872, val loss: 0.11794930696487427\n",
      "Epoch 8936: train loss: 0.0017317351885139942, val loss: 0.12321650981903076\n",
      "Epoch 8937: train loss: 0.0004569948941934854, val loss: 0.12920749187469482\n",
      "Epoch 8938: train loss: 0.0008880143868736923, val loss: 0.12507231533527374\n",
      "Epoch 8939: train loss: 0.0006836973479948938, val loss: 0.11678923666477203\n",
      "Epoch 8940: train loss: 0.00020052290346939117, val loss: 0.12014489620923996\n",
      "Epoch 8941: train loss: 0.0005583643796853721, val loss: 0.1266520619392395\n",
      "Epoch 8942: train loss: 0.00020015434711240232, val loss: 0.12535224854946136\n",
      "Epoch 8943: train loss: 0.00044371490366756916, val loss: 0.11970029026269913\n",
      "Epoch 8944: train loss: 0.00021169382671359926, val loss: 0.11855621635913849\n",
      "Epoch 8945: train loss: 0.00023467592836823314, val loss: 0.12204065173864365\n",
      "Epoch 8946: train loss: 0.00014569921768270433, val loss: 0.12337630987167358\n",
      "Epoch 8947: train loss: 0.00014348475087899715, val loss: 0.11869511753320694\n",
      "Epoch 8948: train loss: 0.00016607643919996917, val loss: 0.11754167079925537\n",
      "Epoch 8949: train loss: 0.0001576186768943444, val loss: 0.11861729621887207\n",
      "Epoch 8950: train loss: 0.00012895117106381804, val loss: 0.11965253204107285\n",
      "Epoch 8951: train loss: 9.850823698798195e-05, val loss: 0.11751969903707504\n",
      "Epoch 8952: train loss: 6.506322824861854e-05, val loss: 0.11698074638843536\n",
      "Epoch 8953: train loss: 8.052906196098775e-05, val loss: 0.11983151733875275\n",
      "Epoch 8954: train loss: 6.731150642735884e-05, val loss: 0.12076961994171143\n",
      "Epoch 8955: train loss: 9.215838508680463e-05, val loss: 0.11909544467926025\n",
      "Epoch 8956: train loss: 5.427370342658833e-05, val loss: 0.11653497070074081\n",
      "Epoch 8957: train loss: 5.10372847202234e-05, val loss: 0.11716127395629883\n",
      "Epoch 8958: train loss: 3.420576103962958e-05, val loss: 0.11931609362363815\n",
      "Epoch 8959: train loss: 3.246636697440408e-05, val loss: 0.11940989643335342\n",
      "Epoch 8960: train loss: 4.7643017751397565e-05, val loss: 0.11828001588582993\n",
      "Epoch 8961: train loss: 3.4545959351817146e-05, val loss: 0.11728020757436752\n",
      "Epoch 8962: train loss: 3.735120117198676e-05, val loss: 0.11850886791944504\n",
      "Epoch 8963: train loss: 2.1713673049816862e-05, val loss: 0.11976522207260132\n",
      "Epoch 8964: train loss: 1.812035588955041e-05, val loss: 0.11980005353689194\n",
      "Epoch 8965: train loss: 1.9665489162434824e-05, val loss: 0.11914441734552383\n",
      "Epoch 8966: train loss: 2.192635838582646e-05, val loss: 0.11841268837451935\n",
      "Epoch 8967: train loss: 2.1996587747707963e-05, val loss: 0.11918788403272629\n",
      "Epoch 8968: train loss: 1.8061353330267593e-05, val loss: 0.11982700973749161\n",
      "Epoch 8969: train loss: 1.3089185813441873e-05, val loss: 0.11997528374195099\n",
      "Epoch 8970: train loss: 9.04874559637392e-06, val loss: 0.11957397311925888\n",
      "Epoch 8971: train loss: 1.0816507710842416e-05, val loss: 0.11928432434797287\n",
      "Epoch 8972: train loss: 1.1618719327088911e-05, val loss: 0.11987096071243286\n",
      "Epoch 8973: train loss: 1.2534019333543256e-05, val loss: 0.11987774819135666\n",
      "Epoch 8974: train loss: 9.365998266730458e-06, val loss: 0.11978775262832642\n",
      "Epoch 8975: train loss: 8.242143849201966e-06, val loss: 0.11958213150501251\n",
      "Epoch 8976: train loss: 4.535728294285946e-06, val loss: 0.119644396007061\n",
      "Epoch 8977: train loss: 6.418169505195692e-06, val loss: 0.11951346695423126\n",
      "Epoch 8978: train loss: 5.193980541662313e-06, val loss: 0.11858921498060226\n",
      "Epoch 8979: train loss: 7.387167897832114e-06, val loss: 0.11812248080968857\n",
      "Epoch 8980: train loss: 4.983160124538699e-06, val loss: 0.11783506721258163\n",
      "Epoch 8981: train loss: 4.631346200767439e-06, val loss: 0.11805951595306396\n",
      "Epoch 8982: train loss: 3.1574263630318455e-06, val loss: 0.1177394762635231\n",
      "Epoch 8983: train loss: 2.703555992411566e-06, val loss: 0.11748595535755157\n",
      "Epoch 8984: train loss: 3.604816583901993e-06, val loss: 0.11794223636388779\n",
      "Epoch 8985: train loss: 3.292228711870848e-06, val loss: 0.11806163936853409\n",
      "Epoch 8986: train loss: 3.57686985807959e-06, val loss: 0.11789287626743317\n",
      "Epoch 8987: train loss: 2.5483382160018664e-06, val loss: 0.11758750677108765\n",
      "Epoch 8988: train loss: 2.1247899439913454e-06, val loss: 0.11781249195337296\n",
      "Epoch 8989: train loss: 2.055298637060332e-06, val loss: 0.11800571531057358\n",
      "Epoch 8990: train loss: 1.721240664664947e-06, val loss: 0.11802858114242554\n",
      "Epoch 8991: train loss: 1.8859295778383967e-06, val loss: 0.11828988045454025\n",
      "Epoch 8992: train loss: 2.262944235553732e-06, val loss: 0.11811889708042145\n",
      "Epoch 8993: train loss: 1.8316258092454518e-06, val loss: 0.11838686466217041\n",
      "Epoch 8994: train loss: 1.4526424365612911e-06, val loss: 0.11832910031080246\n",
      "Epoch 8995: train loss: 1.2296857221372193e-06, val loss: 0.11820993572473526\n",
      "Epoch 8996: train loss: 1.0381539823356434e-06, val loss: 0.11834223568439484\n",
      "Epoch 8997: train loss: 9.49892694279697e-07, val loss: 0.1187184676527977\n",
      "Epoch 8998: train loss: 1.245517978532007e-06, val loss: 0.1186399832367897\n",
      "Epoch 8999: train loss: 1.146049726230558e-06, val loss: 0.11816071718931198\n",
      "Epoch 9000: train loss: 2.122031446560868e-06, val loss: 0.11917612701654434\n",
      "Epoch 9001: train loss: 3.789010861510178e-06, val loss: 0.11848270893096924\n",
      "Epoch 9002: train loss: 1.2483555110520683e-05, val loss: 0.11957339197397232\n",
      "Epoch 9003: train loss: 4.232407809467986e-05, val loss: 0.11687115579843521\n",
      "Epoch 9004: train loss: 7.228881440823898e-05, val loss: 0.11821752041578293\n",
      "Epoch 9005: train loss: 3.190966890542768e-05, val loss: 0.1183779239654541\n",
      "Epoch 9006: train loss: 3.235143594793044e-05, val loss: 0.1176508218050003\n",
      "Epoch 9007: train loss: 3.101075708400458e-05, val loss: 0.11907611042261124\n",
      "Epoch 9008: train loss: 2.9490123779396527e-05, val loss: 0.1182563528418541\n",
      "Epoch 9009: train loss: 5.383380994317122e-05, val loss: 0.11999652534723282\n",
      "Epoch 9010: train loss: 0.00011559460108401254, val loss: 0.11716782301664352\n",
      "Epoch 9011: train loss: 0.0003289568703621626, val loss: 0.12455116957426071\n",
      "Epoch 9012: train loss: 0.00034194919862784445, val loss: 0.12117638438940048\n",
      "Epoch 9013: train loss: 0.00017521617701277137, val loss: 0.12385022640228271\n",
      "Epoch 9014: train loss: 5.0592378102010116e-05, val loss: 0.12765522301197052\n",
      "Epoch 9015: train loss: 0.00014898199879098684, val loss: 0.12686121463775635\n",
      "Epoch 9016: train loss: 4.630026887753047e-05, val loss: 0.12721692025661469\n",
      "Epoch 9017: train loss: 9.409475023858249e-05, val loss: 0.13168983161449432\n",
      "Epoch 9018: train loss: 2.6245104891131632e-05, val loss: 0.1335342675447464\n",
      "Epoch 9019: train loss: 7.892368012107909e-05, val loss: 0.13089492917060852\n",
      "Epoch 9020: train loss: 3.54496187355835e-05, val loss: 0.13260474801063538\n",
      "Epoch 9021: train loss: 4.2419029341544956e-05, val loss: 0.13642865419387817\n",
      "Epoch 9022: train loss: 4.72349020128604e-05, val loss: 0.1355072259902954\n",
      "Epoch 9023: train loss: 2.9250206353026442e-05, val loss: 0.13270801305770874\n",
      "Epoch 9024: train loss: 3.0096858608885668e-05, val loss: 0.13357985019683838\n",
      "Epoch 9025: train loss: 2.7787464205175638e-05, val loss: 0.13509352505207062\n",
      "Epoch 9026: train loss: 2.148878229490947e-05, val loss: 0.13612255454063416\n",
      "Epoch 9027: train loss: 2.286546805407852e-05, val loss: 0.1363210529088974\n",
      "Epoch 9028: train loss: 2.3961776605574414e-05, val loss: 0.1349775344133377\n",
      "Epoch 9029: train loss: 1.476473335060291e-05, val loss: 0.13607047498226166\n",
      "Epoch 9030: train loss: 1.9161914678988978e-05, val loss: 0.1370927393436432\n",
      "Epoch 9031: train loss: 1.65635283337906e-05, val loss: 0.1364663690328598\n",
      "Epoch 9032: train loss: 1.1463105693110265e-05, val loss: 0.13611666858196259\n",
      "Epoch 9033: train loss: 1.1121699571958743e-05, val loss: 0.13669274747371674\n",
      "Epoch 9034: train loss: 1.2778496056853328e-05, val loss: 0.13630807399749756\n",
      "Epoch 9035: train loss: 8.072318451013416e-06, val loss: 0.13632002472877502\n",
      "Epoch 9036: train loss: 1.1330975212331396e-05, val loss: 0.13708458840847015\n",
      "Epoch 9037: train loss: 8.976748176792171e-06, val loss: 0.13627853989601135\n",
      "Epoch 9038: train loss: 8.02692102297442e-06, val loss: 0.1363605111837387\n",
      "Epoch 9039: train loss: 5.8079581322090235e-06, val loss: 0.1365831345319748\n",
      "Epoch 9040: train loss: 6.395030140993185e-06, val loss: 0.13614331185817719\n",
      "Epoch 9041: train loss: 8.300213266920764e-06, val loss: 0.13643109798431396\n",
      "Epoch 9042: train loss: 7.149080829549348e-06, val loss: 0.13638679683208466\n",
      "Epoch 9043: train loss: 8.45915110403439e-06, val loss: 0.1370013952255249\n",
      "Epoch 9044: train loss: 8.843358045851346e-06, val loss: 0.1367301195859909\n",
      "Epoch 9045: train loss: 1.3151980965631083e-05, val loss: 0.13579213619232178\n",
      "Epoch 9046: train loss: 2.0977799067622982e-05, val loss: 0.1357783079147339\n",
      "Epoch 9047: train loss: 3.757226295419969e-05, val loss: 0.13605038821697235\n",
      "Epoch 9048: train loss: 7.139469380490482e-05, val loss: 0.13581793010234833\n",
      "Epoch 9049: train loss: 0.000142081335070543, val loss: 0.13491252064704895\n",
      "Epoch 9050: train loss: 0.00025788770290091634, val loss: 0.1365143060684204\n",
      "Epoch 9051: train loss: 0.00043513919808901846, val loss: 0.13354693353176117\n",
      "Epoch 9052: train loss: 0.0005828752182424068, val loss: 0.13731972873210907\n",
      "Epoch 9053: train loss: 0.00020007022249046713, val loss: 0.13811148703098297\n",
      "Epoch 9054: train loss: 0.00020520269754342735, val loss: 0.13871513307094574\n",
      "Epoch 9055: train loss: 0.00015379968681372702, val loss: 0.13664059340953827\n",
      "Epoch 9056: train loss: 0.00021559179003816098, val loss: 0.13482306897640228\n",
      "Epoch 9057: train loss: 9.830242197494954e-05, val loss: 0.13705508410930634\n",
      "Epoch 9058: train loss: 0.00012926649651490152, val loss: 0.13906410336494446\n",
      "Epoch 9059: train loss: 9.296346979681402e-05, val loss: 0.13809697329998016\n",
      "Epoch 9060: train loss: 0.00010021778143709525, val loss: 0.13604572415351868\n",
      "Epoch 9061: train loss: 7.217095844680443e-05, val loss: 0.1352616846561432\n",
      "Epoch 9062: train loss: 7.923664816189557e-05, val loss: 0.13438932597637177\n",
      "Epoch 9063: train loss: 6.093406773288734e-05, val loss: 0.13391298055648804\n",
      "Epoch 9064: train loss: 6.514790584333241e-05, val loss: 0.1338062733411789\n",
      "Epoch 9065: train loss: 4.564727714750916e-05, val loss: 0.13665905594825745\n",
      "Epoch 9066: train loss: 4.174304922344163e-05, val loss: 0.1372288316488266\n",
      "Epoch 9067: train loss: 3.661809387267567e-05, val loss: 0.13494090735912323\n",
      "Epoch 9068: train loss: 3.4426160709699616e-05, val loss: 0.13476136326789856\n",
      "Epoch 9069: train loss: 2.6429101126268506e-05, val loss: 0.13648991286754608\n",
      "Epoch 9070: train loss: 2.9251737942104228e-05, val loss: 0.13569322228431702\n",
      "Epoch 9071: train loss: 2.0690798919531517e-05, val loss: 0.1336524933576584\n",
      "Epoch 9072: train loss: 2.7994423362542875e-05, val loss: 0.13391195237636566\n",
      "Epoch 9073: train loss: 1.4006848687131424e-05, val loss: 0.13537505269050598\n",
      "Epoch 9074: train loss: 2.1142799596418627e-05, val loss: 0.13643507659435272\n",
      "Epoch 9075: train loss: 1.6232193956966512e-05, val loss: 0.13580016791820526\n",
      "Epoch 9076: train loss: 1.2960043022758327e-05, val loss: 0.13563309609889984\n",
      "Epoch 9077: train loss: 1.3302525985636748e-05, val loss: 0.13592064380645752\n",
      "Epoch 9078: train loss: 1.0455253686814103e-05, val loss: 0.13600941002368927\n",
      "Epoch 9079: train loss: 1.2168601642770227e-05, val loss: 0.13602106273174286\n",
      "Epoch 9080: train loss: 7.687325705774128e-06, val loss: 0.13563354313373566\n",
      "Epoch 9081: train loss: 9.131750630331226e-06, val loss: 0.13591299951076508\n",
      "Epoch 9082: train loss: 8.226801583077759e-06, val loss: 0.13663585484027863\n",
      "Epoch 9083: train loss: 8.356885700777639e-06, val loss: 0.13702356815338135\n",
      "Epoch 9084: train loss: 5.288018655846827e-06, val loss: 0.13697631657123566\n",
      "Epoch 9085: train loss: 7.375745099125197e-06, val loss: 0.1370362639427185\n",
      "Epoch 9086: train loss: 5.122471065988066e-06, val loss: 0.1366996020078659\n",
      "Epoch 9087: train loss: 5.982618404232198e-06, val loss: 0.13709859549999237\n",
      "Epoch 9088: train loss: 4.590703611029312e-06, val loss: 0.13747131824493408\n",
      "Epoch 9089: train loss: 3.678056145872688e-06, val loss: 0.13789017498493195\n",
      "Epoch 9090: train loss: 4.035762685816735e-06, val loss: 0.13802284002304077\n",
      "Epoch 9091: train loss: 3.93393656850094e-06, val loss: 0.13776026666164398\n",
      "Epoch 9092: train loss: 4.842234829993686e-06, val loss: 0.13807831704616547\n",
      "Epoch 9093: train loss: 6.220785508048721e-06, val loss: 0.13859513401985168\n",
      "Epoch 9094: train loss: 9.489270269114058e-06, val loss: 0.1386481523513794\n",
      "Epoch 9095: train loss: 2.4004533770494163e-05, val loss: 0.1387724131345749\n",
      "Epoch 9096: train loss: 7.545261178165674e-05, val loss: 0.13906724750995636\n",
      "Epoch 9097: train loss: 0.00025604863185435534, val loss: 0.13834837079048157\n",
      "Epoch 9098: train loss: 0.0007583884871564806, val loss: 0.13994543254375458\n",
      "Epoch 9099: train loss: 0.001620935625396669, val loss: 0.13062003254890442\n",
      "Epoch 9100: train loss: 0.002330709481611848, val loss: 0.13487230241298676\n",
      "Epoch 9101: train loss: 0.0011304192012175918, val loss: 0.14164811372756958\n",
      "Epoch 9102: train loss: 0.0008604473550803959, val loss: 0.13678234815597534\n",
      "Epoch 9103: train loss: 0.0013294846285134554, val loss: 0.1365440934896469\n",
      "Epoch 9104: train loss: 0.0006742645055055618, val loss: 0.1343849152326584\n",
      "Epoch 9105: train loss: 0.000624725129455328, val loss: 0.135553240776062\n",
      "Epoch 9106: train loss: 0.0002503377036191523, val loss: 0.13723993301391602\n",
      "Epoch 9107: train loss: 0.00036290253046900034, val loss: 0.13341961801052094\n",
      "Epoch 9108: train loss: 0.00026522757252678275, val loss: 0.12971486151218414\n",
      "Epoch 9109: train loss: 0.000492808292619884, val loss: 0.12888281047344208\n",
      "Epoch 9110: train loss: 0.0002837070787791163, val loss: 0.13235153257846832\n",
      "Epoch 9111: train loss: 0.00031514468719251454, val loss: 0.12935221195220947\n",
      "Epoch 9112: train loss: 0.0001016997848637402, val loss: 0.12477908283472061\n",
      "Epoch 9113: train loss: 0.00013237430539447814, val loss: 0.12458653748035431\n",
      "Epoch 9114: train loss: 0.00016452303680125624, val loss: 0.12602964043617249\n",
      "Epoch 9115: train loss: 0.00016831424727570266, val loss: 0.12673674523830414\n",
      "Epoch 9116: train loss: 0.00020713702542707324, val loss: 0.12340018898248672\n",
      "Epoch 9117: train loss: 7.570815796498209e-05, val loss: 0.12247537821531296\n",
      "Epoch 9118: train loss: 9.257128112949431e-05, val loss: 0.12417266517877579\n",
      "Epoch 9119: train loss: 4.4589905883185565e-05, val loss: 0.12456794083118439\n",
      "Epoch 9120: train loss: 0.00010352497338317335, val loss: 0.12341281026601791\n",
      "Epoch 9121: train loss: 0.00010318820568500087, val loss: 0.12166967242956161\n",
      "Epoch 9122: train loss: 7.138936780393124e-05, val loss: 0.12301146984100342\n",
      "Epoch 9123: train loss: 5.8647568948799744e-05, val loss: 0.12385769188404083\n",
      "Epoch 9124: train loss: 2.088274595735129e-05, val loss: 0.12312519550323486\n",
      "Epoch 9125: train loss: 4.981480014976114e-05, val loss: 0.1226574182510376\n",
      "Epoch 9126: train loss: 5.319366027833894e-05, val loss: 0.12240226566791534\n",
      "Epoch 9127: train loss: 4.925971734337509e-05, val loss: 0.12385854870080948\n",
      "Epoch 9128: train loss: 3.98408155888319e-05, val loss: 0.12375696748495102\n",
      "Epoch 9129: train loss: 1.1680974239425268e-05, val loss: 0.12323404848575592\n",
      "Epoch 9130: train loss: 2.47108891926473e-05, val loss: 0.1233552098274231\n",
      "Epoch 9131: train loss: 2.504390067770146e-05, val loss: 0.12302861362695694\n",
      "Epoch 9132: train loss: 3.261518577346578e-05, val loss: 0.12304222583770752\n",
      "Epoch 9133: train loss: 2.5505307348794304e-05, val loss: 0.12235286086797714\n",
      "Epoch 9134: train loss: 1.1775508028222248e-05, val loss: 0.12251228094100952\n",
      "Epoch 9135: train loss: 1.237540527654346e-05, val loss: 0.12279780209064484\n",
      "Epoch 9136: train loss: 1.1440142770879902e-05, val loss: 0.12240024656057358\n",
      "Epoch 9137: train loss: 1.8720660591498017e-05, val loss: 0.12268205732107162\n",
      "Epoch 9138: train loss: 1.7010488591040485e-05, val loss: 0.12299201637506485\n",
      "Epoch 9139: train loss: 1.0242837561236229e-05, val loss: 0.12369270622730255\n",
      "Epoch 9140: train loss: 8.089840775937773e-06, val loss: 0.12348683178424835\n",
      "Epoch 9141: train loss: 4.188442744634813e-06, val loss: 0.12310867756605148\n",
      "Epoch 9142: train loss: 1.0923415175057016e-05, val loss: 0.1236918717622757\n",
      "Epoch 9143: train loss: 8.246548532042652e-06, val loss: 0.12362120300531387\n",
      "Epoch 9144: train loss: 1.0139534424524754e-05, val loss: 0.1239921823143959\n",
      "Epoch 9145: train loss: 4.422249276103685e-06, val loss: 0.12422940880060196\n",
      "Epoch 9146: train loss: 4.411278041516198e-06, val loss: 0.12464144080877304\n",
      "Epoch 9147: train loss: 3.7714971767854877e-06, val loss: 0.12502525746822357\n",
      "Epoch 9148: train loss: 5.655879249388818e-06, val loss: 0.12481226772069931\n",
      "Epoch 9149: train loss: 5.7871466196957044e-06, val loss: 0.12499973922967911\n",
      "Epoch 9150: train loss: 4.585889200825477e-06, val loss: 0.1251138299703598\n",
      "Epoch 9151: train loss: 2.5800832190725487e-06, val loss: 0.1253911852836609\n",
      "Epoch 9152: train loss: 2.387367430856102e-06, val loss: 0.125475212931633\n",
      "Epoch 9153: train loss: 2.5497806745988782e-06, val loss: 0.12541912496089935\n",
      "Epoch 9154: train loss: 3.4311758554395055e-06, val loss: 0.12593035399913788\n",
      "Epoch 9155: train loss: 2.9867894681956386e-06, val loss: 0.12596558034420013\n",
      "Epoch 9156: train loss: 2.9397062917269068e-06, val loss: 0.12632067501544952\n",
      "Epoch 9157: train loss: 1.6335952750523575e-06, val loss: 0.1268148571252823\n",
      "Epoch 9158: train loss: 1.2397472346492577e-06, val loss: 0.12655995786190033\n",
      "Epoch 9159: train loss: 1.7869616613097605e-06, val loss: 0.12645407021045685\n",
      "Epoch 9160: train loss: 1.5077356465553748e-06, val loss: 0.12693028151988983\n",
      "Epoch 9161: train loss: 2.3470481664844556e-06, val loss: 0.12704350054264069\n",
      "Epoch 9162: train loss: 2.4745027076278348e-06, val loss: 0.12649624049663544\n",
      "Epoch 9163: train loss: 3.7213042105577188e-06, val loss: 0.1264323592185974\n",
      "Epoch 9164: train loss: 9.286201020586304e-06, val loss: 0.12738296389579773\n",
      "Epoch 9165: train loss: 2.2364336473401636e-05, val loss: 0.12480073422193527\n",
      "Epoch 9166: train loss: 5.5178956245072186e-05, val loss: 0.12750573456287384\n",
      "Epoch 9167: train loss: 7.858689059503376e-05, val loss: 0.12499196827411652\n",
      "Epoch 9168: train loss: 4.708558481070213e-05, val loss: 0.1251329779624939\n",
      "Epoch 9169: train loss: 1.5856092431931756e-05, val loss: 0.12601052224636078\n",
      "Epoch 9170: train loss: 3.1241554097505286e-05, val loss: 0.1262439340353012\n",
      "Epoch 9171: train loss: 2.580255204520654e-05, val loss: 0.12511801719665527\n",
      "Epoch 9172: train loss: 1.937841261678841e-05, val loss: 0.12620413303375244\n",
      "Epoch 9173: train loss: 1.489518672315171e-05, val loss: 0.12746655941009521\n",
      "Epoch 9174: train loss: 2.245865107397549e-05, val loss: 0.12382598221302032\n",
      "Epoch 9175: train loss: 2.158070492441766e-05, val loss: 0.1262151151895523\n",
      "Epoch 9176: train loss: 1.7236578059964813e-05, val loss: 0.12598000466823578\n",
      "Epoch 9177: train loss: 1.5082039681146853e-05, val loss: 0.12562258541584015\n",
      "Epoch 9178: train loss: 1.3433013918984216e-05, val loss: 0.12689009308815002\n",
      "Epoch 9179: train loss: 1.2537430848169606e-05, val loss: 0.12728573381900787\n",
      "Epoch 9180: train loss: 1.1797316801676061e-05, val loss: 0.1284959763288498\n",
      "Epoch 9181: train loss: 1.1752385944419075e-05, val loss: 0.1277012825012207\n",
      "Epoch 9182: train loss: 1.4383025700226426e-05, val loss: 0.12975560128688812\n",
      "Epoch 9183: train loss: 1.3922786820330657e-05, val loss: 0.12939158082008362\n",
      "Epoch 9184: train loss: 1.0730936082836706e-05, val loss: 0.13035109639167786\n",
      "Epoch 9185: train loss: 1.4542426470143255e-05, val loss: 0.12895050644874573\n",
      "Epoch 9186: train loss: 2.4204360670410097e-05, val loss: 0.13188143074512482\n",
      "Epoch 9187: train loss: 5.219564263825305e-05, val loss: 0.12720470130443573\n",
      "Epoch 9188: train loss: 0.00012114504352211952, val loss: 0.1346225142478943\n",
      "Epoch 9189: train loss: 0.0001971038873307407, val loss: 0.12464606016874313\n",
      "Epoch 9190: train loss: 0.0002615411067381501, val loss: 0.13139818608760834\n",
      "Epoch 9191: train loss: 0.0001386644726153463, val loss: 0.1302434653043747\n",
      "Epoch 9192: train loss: 5.3881860367255285e-05, val loss: 0.12758119404315948\n",
      "Epoch 9193: train loss: 9.552474512020126e-05, val loss: 0.12892885506153107\n",
      "Epoch 9194: train loss: 9.376309753861278e-05, val loss: 0.12943275272846222\n",
      "Epoch 9195: train loss: 3.368287434568629e-05, val loss: 0.12839610874652863\n",
      "Epoch 9196: train loss: 4.7335695853689685e-05, val loss: 0.12916012108325958\n",
      "Epoch 9197: train loss: 5.1571973017416894e-05, val loss: 0.1303480714559555\n",
      "Epoch 9198: train loss: 4.518616697168909e-05, val loss: 0.12885849177837372\n",
      "Epoch 9199: train loss: 4.580071254167706e-05, val loss: 0.12884187698364258\n",
      "Epoch 9200: train loss: 3.122043199255131e-05, val loss: 0.13056454062461853\n",
      "Epoch 9201: train loss: 3.272985486546531e-05, val loss: 0.13096983730793\n",
      "Epoch 9202: train loss: 2.5974377422244288e-05, val loss: 0.1297692358493805\n",
      "Epoch 9203: train loss: 1.7008585928124376e-05, val loss: 0.13039323687553406\n",
      "Epoch 9204: train loss: 2.4670735001564026e-05, val loss: 0.13118939101696014\n",
      "Epoch 9205: train loss: 2.180740557378158e-05, val loss: 0.12952736020088196\n",
      "Epoch 9206: train loss: 2.4661494535394013e-05, val loss: 0.13057756423950195\n",
      "Epoch 9207: train loss: 1.3906656931794714e-05, val loss: 0.13154450058937073\n",
      "Epoch 9208: train loss: 1.2717375284410082e-05, val loss: 0.13032835721969604\n",
      "Epoch 9209: train loss: 1.4537516108248383e-05, val loss: 0.13077567517757416\n",
      "Epoch 9210: train loss: 1.1263760825386271e-05, val loss: 0.13157396018505096\n",
      "Epoch 9211: train loss: 1.196340235765092e-05, val loss: 0.1306917667388916\n",
      "Epoch 9212: train loss: 1.0947818736894988e-05, val loss: 0.1309429258108139\n",
      "Epoch 9213: train loss: 9.321423021901865e-06, val loss: 0.13189929723739624\n",
      "Epoch 9214: train loss: 7.670287232031114e-06, val loss: 0.13105331361293793\n",
      "Epoch 9215: train loss: 1.0005804142565466e-05, val loss: 0.1310763657093048\n",
      "Epoch 9216: train loss: 8.861415153660346e-06, val loss: 0.13196814060211182\n",
      "Epoch 9217: train loss: 7.318422831303906e-06, val loss: 0.1315806359052658\n",
      "Epoch 9218: train loss: 1.1809199349954724e-05, val loss: 0.13144265115261078\n",
      "Epoch 9219: train loss: 1.1495659236970823e-05, val loss: 0.13241985440254211\n",
      "Epoch 9220: train loss: 7.705539246671833e-06, val loss: 0.1326989382505417\n",
      "Epoch 9221: train loss: 1.0725831998570357e-05, val loss: 0.1315954476594925\n",
      "Epoch 9222: train loss: 1.6744190361350775e-05, val loss: 0.13245244324207306\n",
      "Epoch 9223: train loss: 2.5571323931217194e-05, val loss: 0.13307206332683563\n",
      "Epoch 9224: train loss: 4.591183824231848e-05, val loss: 0.13257090747356415\n",
      "Epoch 9225: train loss: 9.576264710631222e-05, val loss: 0.13247863948345184\n",
      "Epoch 9226: train loss: 0.0002039473911281675, val loss: 0.13312633335590363\n",
      "Epoch 9227: train loss: 0.0004506717959884554, val loss: 0.13004934787750244\n",
      "Epoch 9228: train loss: 0.000872153730597347, val loss: 0.13448739051818848\n",
      "Epoch 9229: train loss: 0.001277764793485403, val loss: 0.1253616213798523\n",
      "Epoch 9230: train loss: 0.0009501559543423355, val loss: 0.1293979287147522\n",
      "Epoch 9231: train loss: 0.00020791632414329797, val loss: 0.13009947538375854\n",
      "Epoch 9232: train loss: 0.00030885013984516263, val loss: 0.12373693287372589\n",
      "Epoch 9233: train loss: 0.00046505991485901177, val loss: 0.12451787292957306\n",
      "Epoch 9234: train loss: 6.187888357089832e-05, val loss: 0.12571638822555542\n",
      "Epoch 9235: train loss: 0.0002622411120682955, val loss: 0.1232481524348259\n",
      "Epoch 9236: train loss: 0.00014472802286036313, val loss: 0.12461759895086288\n",
      "Epoch 9237: train loss: 0.0001459888298995793, val loss: 0.12573359906673431\n",
      "Epoch 9238: train loss: 0.00019020106992684305, val loss: 0.12331781536340714\n",
      "Epoch 9239: train loss: 8.253194391727448e-05, val loss: 0.12209074944257736\n",
      "Epoch 9240: train loss: 0.0001385050272801891, val loss: 0.12472867965698242\n",
      "Epoch 9241: train loss: 3.425032991799526e-05, val loss: 0.1253058761358261\n",
      "Epoch 9242: train loss: 0.00011603318125708029, val loss: 0.12373775243759155\n",
      "Epoch 9243: train loss: 3.8815771404188126e-05, val loss: 0.1243027076125145\n",
      "Epoch 9244: train loss: 9.349943138659e-05, val loss: 0.1259932667016983\n",
      "Epoch 9245: train loss: 4.331295713200234e-05, val loss: 0.1279318779706955\n",
      "Epoch 9246: train loss: 7.380265014944598e-05, val loss: 0.12709544599056244\n",
      "Epoch 9247: train loss: 2.3124794097384438e-05, val loss: 0.12715384364128113\n",
      "Epoch 9248: train loss: 4.701273064711131e-05, val loss: 0.12813787162303925\n",
      "Epoch 9249: train loss: 1.4258979717851616e-05, val loss: 0.12911169230937958\n",
      "Epoch 9250: train loss: 4.1341667383676395e-05, val loss: 0.12956944108009338\n",
      "Epoch 9251: train loss: 2.3137603420764208e-05, val loss: 0.1286313831806183\n",
      "Epoch 9252: train loss: 3.5632405342767015e-05, val loss: 0.12967358529567719\n",
      "Epoch 9253: train loss: 2.1836516680195928e-05, val loss: 0.13121259212493896\n",
      "Epoch 9254: train loss: 2.0376821339596063e-05, val loss: 0.13195380568504333\n",
      "Epoch 9255: train loss: 1.4335142623167485e-05, val loss: 0.13114897906780243\n",
      "Epoch 9256: train loss: 1.3024950021645054e-05, val loss: 0.1313857138156891\n",
      "Epoch 9257: train loss: 1.4591335457225796e-05, val loss: 0.13315121829509735\n",
      "Epoch 9258: train loss: 1.4586887118639424e-05, val loss: 0.13311170041561127\n",
      "Epoch 9259: train loss: 1.5030654139991384e-05, val loss: 0.1327429562807083\n",
      "Epoch 9260: train loss: 1.2401103958836757e-05, val loss: 0.1330304741859436\n",
      "Epoch 9261: train loss: 1.0076858416141476e-05, val loss: 0.13444377481937408\n",
      "Epoch 9262: train loss: 7.368902060989058e-06, val loss: 0.13471463322639465\n",
      "Epoch 9263: train loss: 6.850467343610944e-06, val loss: 0.1340770274400711\n",
      "Epoch 9264: train loss: 6.534636668220628e-06, val loss: 0.13403676450252533\n",
      "Epoch 9265: train loss: 7.5311099863029085e-06, val loss: 0.1346459686756134\n",
      "Epoch 9266: train loss: 7.090682174748508e-06, val loss: 0.13543979823589325\n",
      "Epoch 9267: train loss: 6.43921111986856e-06, val loss: 0.13492366671562195\n",
      "Epoch 9268: train loss: 5.903835244680522e-06, val loss: 0.13545094430446625\n",
      "Epoch 9269: train loss: 4.029313458886463e-06, val loss: 0.1362636387348175\n",
      "Epoch 9270: train loss: 3.3126582366094226e-06, val loss: 0.13609793782234192\n",
      "Epoch 9271: train loss: 6.0427778407756705e-06, val loss: 0.1362772434949875\n",
      "Epoch 9272: train loss: 9.527321708446834e-06, val loss: 0.13653793931007385\n",
      "Epoch 9273: train loss: 4.0332174648938235e-06, val loss: 0.1369912028312683\n",
      "Epoch 9274: train loss: 6.874471182527486e-06, val loss: 0.1371511071920395\n",
      "Epoch 9275: train loss: 4.457333943719277e-06, val loss: 0.13738714158535004\n",
      "Epoch 9276: train loss: 5.167273684492102e-06, val loss: 0.1375090777873993\n",
      "Epoch 9277: train loss: 4.681482550950022e-06, val loss: 0.1377779245376587\n",
      "Epoch 9278: train loss: 4.67109839519253e-06, val loss: 0.13847915828227997\n",
      "Epoch 9279: train loss: 6.7201854108134285e-06, val loss: 0.13828273117542267\n",
      "Epoch 9280: train loss: 9.529113413009327e-06, val loss: 0.1387667953968048\n",
      "Epoch 9281: train loss: 2.918236168625299e-05, val loss: 0.13955092430114746\n",
      "Epoch 9282: train loss: 0.0001070455982699059, val loss: 0.13944898545742035\n",
      "Epoch 9283: train loss: 0.00040101524791680276, val loss: 0.14177657663822174\n",
      "Epoch 9284: train loss: 0.0006395268137566745, val loss: 0.1407010406255722\n",
      "Epoch 9285: train loss: 0.0003960066824220121, val loss: 0.14118023216724396\n",
      "Epoch 9286: train loss: 0.00016615405911579728, val loss: 0.14262700080871582\n",
      "Epoch 9287: train loss: 0.000269527779892087, val loss: 0.14135758578777313\n",
      "Epoch 9288: train loss: 0.00012099490413675085, val loss: 0.14192363619804382\n",
      "Epoch 9289: train loss: 0.00020301427866797894, val loss: 0.1423589587211609\n",
      "Epoch 9290: train loss: 9.626420796848834e-05, val loss: 0.13929855823516846\n",
      "Epoch 9291: train loss: 0.0001225530868396163, val loss: 0.13817821443080902\n",
      "Epoch 9292: train loss: 9.347469313070178e-05, val loss: 0.1414976567029953\n",
      "Epoch 9293: train loss: 8.318250911543146e-05, val loss: 0.1425301432609558\n",
      "Epoch 9294: train loss: 8.706144581083208e-05, val loss: 0.13955286145210266\n",
      "Epoch 9295: train loss: 6.776598456781358e-05, val loss: 0.13818302750587463\n",
      "Epoch 9296: train loss: 5.862465695827268e-05, val loss: 0.14018797874450684\n",
      "Epoch 9297: train loss: 5.9040688938694075e-05, val loss: 0.14106084406375885\n",
      "Epoch 9298: train loss: 4.085639011464082e-05, val loss: 0.14089031517505646\n",
      "Epoch 9299: train loss: 4.846871888730675e-05, val loss: 0.14182396233081818\n",
      "Epoch 9300: train loss: 3.223611565772444e-05, val loss: 0.14188645780086517\n",
      "Epoch 9301: train loss: 3.926305362256244e-05, val loss: 0.14065496623516083\n",
      "Epoch 9302: train loss: 2.7549296646611765e-05, val loss: 0.1401149034500122\n",
      "Epoch 9303: train loss: 3.180721978424117e-05, val loss: 0.14069722592830658\n",
      "Epoch 9304: train loss: 2.3585595045005903e-05, val loss: 0.14069390296936035\n",
      "Epoch 9305: train loss: 2.2735921447747387e-05, val loss: 0.1404447704553604\n",
      "Epoch 9306: train loss: 2.0263989426894113e-05, val loss: 0.14031803607940674\n",
      "Epoch 9307: train loss: 1.8818676835508086e-05, val loss: 0.14104139804840088\n",
      "Epoch 9308: train loss: 1.5114708730834536e-05, val loss: 0.14163020253181458\n",
      "Epoch 9309: train loss: 1.8208704204880632e-05, val loss: 0.1410938948392868\n",
      "Epoch 9310: train loss: 1.0217463568551466e-05, val loss: 0.1401498168706894\n",
      "Epoch 9311: train loss: 1.5451367289642803e-05, val loss: 0.13993211090564728\n",
      "Epoch 9312: train loss: 1.0245596058666706e-05, val loss: 0.14054512977600098\n",
      "Epoch 9313: train loss: 1.1175860890944023e-05, val loss: 0.1400950700044632\n",
      "Epoch 9314: train loss: 8.68981715029804e-06, val loss: 0.13946115970611572\n",
      "Epoch 9315: train loss: 8.734025868761819e-06, val loss: 0.14008204638957977\n",
      "Epoch 9316: train loss: 7.416507742163958e-06, val loss: 0.1411970853805542\n",
      "Epoch 9317: train loss: 7.583981641801074e-06, val loss: 0.14109604060649872\n",
      "Epoch 9318: train loss: 4.691447884397348e-06, val loss: 0.14021101593971252\n",
      "Epoch 9319: train loss: 7.775797712383792e-06, val loss: 0.1403321623802185\n",
      "Epoch 9320: train loss: 3.868099156534299e-06, val loss: 0.14112135767936707\n",
      "Epoch 9321: train loss: 5.79576271775295e-06, val loss: 0.14114020764827728\n",
      "Epoch 9322: train loss: 4.503502623265376e-06, val loss: 0.1406920850276947\n",
      "Epoch 9323: train loss: 4.51978166893241e-06, val loss: 0.14123769104480743\n",
      "Epoch 9324: train loss: 3.684708644868806e-06, val loss: 0.1422659456729889\n",
      "Epoch 9325: train loss: 3.3018930025718873e-06, val loss: 0.14206500351428986\n",
      "Epoch 9326: train loss: 3.3772621463867836e-06, val loss: 0.14174123108386993\n",
      "Epoch 9327: train loss: 3.6288063256506575e-06, val loss: 0.14211563766002655\n",
      "Epoch 9328: train loss: 7.249893769767368e-06, val loss: 0.14301300048828125\n",
      "Epoch 9329: train loss: 1.5332636394305155e-05, val loss: 0.14240148663520813\n",
      "Epoch 9330: train loss: 4.266773248673417e-05, val loss: 0.14290617406368256\n",
      "Epoch 9331: train loss: 0.00010379417653894052, val loss: 0.14215660095214844\n",
      "Epoch 9332: train loss: 0.00021467321494128555, val loss: 0.14367961883544922\n",
      "Epoch 9333: train loss: 0.0003332749765831977, val loss: 0.1430140733718872\n",
      "Epoch 9334: train loss: 0.00040800354327075183, val loss: 0.14500780403614044\n",
      "Epoch 9335: train loss: 0.00039345340337604284, val loss: 0.14100484549999237\n",
      "Epoch 9336: train loss: 0.00046778752584941685, val loss: 0.14045827090740204\n",
      "Epoch 9337: train loss: 0.00048168658395297825, val loss: 0.134720578789711\n",
      "Epoch 9338: train loss: 0.0003217081248294562, val loss: 0.13890913128852844\n",
      "Epoch 9339: train loss: 0.0001949909928953275, val loss: 0.1350562870502472\n",
      "Epoch 9340: train loss: 0.00027219828916713595, val loss: 0.13234210014343262\n",
      "Epoch 9341: train loss: 0.00028801505686715245, val loss: 0.13314424455165863\n",
      "Epoch 9342: train loss: 0.0001811847323551774, val loss: 0.13346228003501892\n",
      "Epoch 9343: train loss: 7.264083251357079e-05, val loss: 0.1291254609823227\n",
      "Epoch 9344: train loss: 0.00012477872951421887, val loss: 0.1297513246536255\n",
      "Epoch 9345: train loss: 8.857306238496676e-05, val loss: 0.12935082614421844\n",
      "Epoch 9346: train loss: 3.295642454759218e-05, val loss: 0.12610791623592377\n",
      "Epoch 9347: train loss: 0.00011912040645256639, val loss: 0.12709949910640717\n",
      "Epoch 9348: train loss: 9.886461339192465e-05, val loss: 0.1261824518442154\n",
      "Epoch 9349: train loss: 6.970517279114574e-05, val loss: 0.12555482983589172\n",
      "Epoch 9350: train loss: 0.00010534266766626388, val loss: 0.12432783842086792\n",
      "Epoch 9351: train loss: 4.734644971904345e-05, val loss: 0.12400765717029572\n",
      "Epoch 9352: train loss: 3.1586179829901084e-05, val loss: 0.12334097921848297\n",
      "Epoch 9353: train loss: 4.6235178160713986e-05, val loss: 0.12399286031723022\n",
      "Epoch 9354: train loss: 1.1021781574527267e-05, val loss: 0.12332339584827423\n",
      "Epoch 9355: train loss: 3.731130709638819e-05, val loss: 0.12167668342590332\n",
      "Epoch 9356: train loss: 4.5583896280732006e-05, val loss: 0.12363632768392563\n",
      "Epoch 9357: train loss: 2.694168324524071e-05, val loss: 0.12359154224395752\n",
      "Epoch 9358: train loss: 4.610099131241441e-05, val loss: 0.1226232573390007\n",
      "Epoch 9359: train loss: 2.6646681362763047e-05, val loss: 0.12242070585489273\n",
      "Epoch 9360: train loss: 1.5586434528813697e-05, val loss: 0.12347348779439926\n",
      "Epoch 9361: train loss: 1.793205774447415e-05, val loss: 0.12243225425481796\n",
      "Epoch 9362: train loss: 9.247242815035861e-06, val loss: 0.1220182552933693\n",
      "Epoch 9363: train loss: 1.229929239343619e-05, val loss: 0.12334759533405304\n",
      "Epoch 9364: train loss: 1.712342054815963e-05, val loss: 0.12255177646875381\n",
      "Epoch 9365: train loss: 1.7247290088562295e-05, val loss: 0.12251883000135422\n",
      "Epoch 9366: train loss: 1.543010148452595e-05, val loss: 0.12269418686628342\n",
      "Epoch 9367: train loss: 1.7238429791177623e-05, val loss: 0.12318410724401474\n",
      "Epoch 9368: train loss: 1.2850930033891927e-05, val loss: 0.12298949062824249\n",
      "Epoch 9369: train loss: 6.3490083448414225e-06, val loss: 0.12390508502721786\n",
      "Epoch 9370: train loss: 9.090864296013024e-06, val loss: 0.12342040985822678\n",
      "Epoch 9371: train loss: 1.0452797141624615e-05, val loss: 0.12360761314630508\n",
      "Epoch 9372: train loss: 5.82781922275899e-06, val loss: 0.12424943596124649\n",
      "Epoch 9373: train loss: 6.524504442495527e-06, val loss: 0.12393637001514435\n",
      "Epoch 9374: train loss: 1.0932408258668147e-05, val loss: 0.12436192482709885\n",
      "Epoch 9375: train loss: 9.96519793261541e-06, val loss: 0.12438297271728516\n",
      "Epoch 9376: train loss: 6.3092797972785775e-06, val loss: 0.12456656992435455\n",
      "Epoch 9377: train loss: 5.029271051171236e-06, val loss: 0.12401027977466583\n",
      "Epoch 9378: train loss: 6.657995527348248e-06, val loss: 0.1251690834760666\n",
      "Epoch 9379: train loss: 7.703448318352457e-06, val loss: 0.1242644190788269\n",
      "Epoch 9380: train loss: 1.3041906640864909e-05, val loss: 0.12554442882537842\n",
      "Epoch 9381: train loss: 3.290034510428086e-05, val loss: 0.12451320886611938\n",
      "Epoch 9382: train loss: 8.98034923011437e-05, val loss: 0.1280660480260849\n",
      "Epoch 9383: train loss: 0.0001868650724645704, val loss: 0.12710212171077728\n",
      "Epoch 9384: train loss: 0.0002913956413976848, val loss: 0.12630999088287354\n",
      "Epoch 9385: train loss: 0.00036060786806046963, val loss: 0.13161318004131317\n",
      "Epoch 9386: train loss: 0.00024600655888207257, val loss: 0.12910833954811096\n",
      "Epoch 9387: train loss: 0.0001452811702620238, val loss: 0.1304270476102829\n",
      "Epoch 9388: train loss: 0.00013519468484446406, val loss: 0.13387107849121094\n",
      "Epoch 9389: train loss: 0.00016316950495820493, val loss: 0.1304445117712021\n",
      "Epoch 9390: train loss: 0.00010043873044196516, val loss: 0.1317240446805954\n",
      "Epoch 9391: train loss: 7.588472362840548e-05, val loss: 0.13608644902706146\n",
      "Epoch 9392: train loss: 0.00010401741019450128, val loss: 0.13250266015529633\n",
      "Epoch 9393: train loss: 8.408376015722752e-05, val loss: 0.1307847797870636\n",
      "Epoch 9394: train loss: 6.142674828879535e-05, val loss: 0.13406997919082642\n",
      "Epoch 9395: train loss: 6.185511301737279e-05, val loss: 0.13352851569652557\n",
      "Epoch 9396: train loss: 6.227999983821064e-05, val loss: 0.13207054138183594\n",
      "Epoch 9397: train loss: 4.0511131373932585e-05, val loss: 0.1341811865568161\n",
      "Epoch 9398: train loss: 4.0573489968664944e-05, val loss: 0.13452480733394623\n",
      "Epoch 9399: train loss: 4.86431599711068e-05, val loss: 0.13322651386260986\n",
      "Epoch 9400: train loss: 3.542955528246239e-05, val loss: 0.13315246999263763\n",
      "Epoch 9401: train loss: 2.396202035015449e-05, val loss: 0.1335071176290512\n",
      "Epoch 9402: train loss: 3.5196568205719814e-05, val loss: 0.13482478260993958\n",
      "Epoch 9403: train loss: 2.828870856319554e-05, val loss: 0.13458259403705597\n",
      "Epoch 9404: train loss: 1.4505191757052671e-05, val loss: 0.1331287920475006\n",
      "Epoch 9405: train loss: 2.625377419462893e-05, val loss: 0.1342453509569168\n",
      "Epoch 9406: train loss: 2.2034984795027412e-05, val loss: 0.1350395530462265\n",
      "Epoch 9407: train loss: 1.1810226169473026e-05, val loss: 0.133440300822258\n",
      "Epoch 9408: train loss: 1.7211323211085983e-05, val loss: 0.13376133143901825\n",
      "Epoch 9409: train loss: 1.5313906260416843e-05, val loss: 0.13532084226608276\n",
      "Epoch 9410: train loss: 1.3028999092057347e-05, val loss: 0.1344381421804428\n",
      "Epoch 9411: train loss: 9.2485452114488e-06, val loss: 0.1336405724287033\n",
      "Epoch 9412: train loss: 1.12060643004952e-05, val loss: 0.13509778678417206\n",
      "Epoch 9413: train loss: 1.2387888091325294e-05, val loss: 0.13516171276569366\n",
      "Epoch 9414: train loss: 6.86511111780419e-06, val loss: 0.1340387612581253\n",
      "Epoch 9415: train loss: 7.965313670865726e-06, val loss: 0.1348177194595337\n",
      "Epoch 9416: train loss: 9.237298399966676e-06, val loss: 0.13498632609844208\n",
      "Epoch 9417: train loss: 7.317235940718092e-06, val loss: 0.1350793093442917\n",
      "Epoch 9418: train loss: 5.483367658598581e-06, val loss: 0.13493011891841888\n",
      "Epoch 9419: train loss: 5.532999239221681e-06, val loss: 0.13478213548660278\n",
      "Epoch 9420: train loss: 6.151621619210346e-06, val loss: 0.13535486161708832\n",
      "Epoch 9421: train loss: 7.0977866926114075e-06, val loss: 0.13521598279476166\n",
      "Epoch 9422: train loss: 5.360674549592659e-06, val loss: 0.13537172973155975\n",
      "Epoch 9423: train loss: 2.1983421447657747e-06, val loss: 0.13543741405010223\n",
      "Epoch 9424: train loss: 3.4540853448561393e-06, val loss: 0.13512422144412994\n",
      "Epoch 9425: train loss: 5.930146016908111e-06, val loss: 0.13581815361976624\n",
      "Epoch 9426: train loss: 6.007901902194135e-06, val loss: 0.13493208587169647\n",
      "Epoch 9427: train loss: 6.5226031438214704e-06, val loss: 0.13576273620128632\n",
      "Epoch 9428: train loss: 1.0615360224619508e-05, val loss: 0.13487866520881653\n",
      "Epoch 9429: train loss: 2.4193039280362427e-05, val loss: 0.13803203403949738\n",
      "Epoch 9430: train loss: 7.635091606061906e-05, val loss: 0.13353972136974335\n",
      "Epoch 9431: train loss: 0.000248858705163002, val loss: 0.13652974367141724\n",
      "Epoch 9432: train loss: 0.0005598017596639693, val loss: 0.13399182260036469\n",
      "Epoch 9433: train loss: 0.0011057869996875525, val loss: 0.13702274858951569\n",
      "Epoch 9434: train loss: 0.0014587666373699903, val loss: 0.1359459012746811\n",
      "Epoch 9435: train loss: 0.0013139030197635293, val loss: 0.1346265822649002\n",
      "Epoch 9436: train loss: 0.0008790012798272073, val loss: 0.12615711987018585\n",
      "Epoch 9437: train loss: 0.0007567069842480123, val loss: 0.12772002816200256\n",
      "Epoch 9438: train loss: 0.0006919752340763807, val loss: 0.12399859726428986\n",
      "Epoch 9439: train loss: 0.0004154803173150867, val loss: 0.11754816025495529\n",
      "Epoch 9440: train loss: 0.0002017854421865195, val loss: 0.113826684653759\n",
      "Epoch 9441: train loss: 0.00023911012976896018, val loss: 0.11793167889118195\n",
      "Epoch 9442: train loss: 0.00022104235540609807, val loss: 0.11992361396551132\n",
      "Epoch 9443: train loss: 0.0003275041817687452, val loss: 0.1151246577501297\n",
      "Epoch 9444: train loss: 0.00026592935319058597, val loss: 0.11670231074094772\n",
      "Epoch 9445: train loss: 0.00022136101324576885, val loss: 0.11996450275182724\n",
      "Epoch 9446: train loss: 9.9566976132337e-05, val loss: 0.12074390798807144\n",
      "Epoch 9447: train loss: 8.53895180625841e-05, val loss: 0.11777641624212265\n",
      "Epoch 9448: train loss: 9.143719216808677e-05, val loss: 0.11568666994571686\n",
      "Epoch 9449: train loss: 0.00014466849097516388, val loss: 0.11852288246154785\n",
      "Epoch 9450: train loss: 0.00013432135165203363, val loss: 0.11890323460102081\n",
      "Epoch 9451: train loss: 0.00010704576561693102, val loss: 0.11862712353467941\n",
      "Epoch 9452: train loss: 5.09760357090272e-05, val loss: 0.11848729848861694\n",
      "Epoch 9453: train loss: 3.620526331360452e-05, val loss: 0.11990859359502792\n",
      "Epoch 9454: train loss: 5.109029734740034e-05, val loss: 0.12110882252454758\n",
      "Epoch 9455: train loss: 6.498000584542751e-05, val loss: 0.11916472762823105\n",
      "Epoch 9456: train loss: 7.444805669365451e-05, val loss: 0.1197114959359169\n",
      "Epoch 9457: train loss: 4.391517359181307e-05, val loss: 0.12065800279378891\n",
      "Epoch 9458: train loss: 2.6540086764725856e-05, val loss: 0.12117015570402145\n",
      "Epoch 9459: train loss: 1.5015402823337354e-05, val loss: 0.12056296318769455\n",
      "Epoch 9460: train loss: 2.6576441086945124e-05, val loss: 0.11978679150342941\n",
      "Epoch 9461: train loss: 3.7547921238001436e-05, val loss: 0.12082236260175705\n",
      "Epoch 9462: train loss: 3.6302164517110214e-05, val loss: 0.12060370296239853\n",
      "Epoch 9463: train loss: 2.4006089006434195e-05, val loss: 0.12050876766443253\n",
      "Epoch 9464: train loss: 1.085082931240322e-05, val loss: 0.12026558071374893\n",
      "Epoch 9465: train loss: 1.0206054867012426e-05, val loss: 0.12010584026575089\n",
      "Epoch 9466: train loss: 1.529093242425006e-05, val loss: 0.12040426582098007\n",
      "Epoch 9467: train loss: 2.1088397261337377e-05, val loss: 0.11993993818759918\n",
      "Epoch 9468: train loss: 1.955115658347495e-05, val loss: 0.12056930363178253\n",
      "Epoch 9469: train loss: 1.2039985449519008e-05, val loss: 0.12079881876707077\n",
      "Epoch 9470: train loss: 6.446323368436424e-06, val loss: 0.12060680240392685\n",
      "Epoch 9471: train loss: 5.845597115694545e-06, val loss: 0.1206522211432457\n",
      "Epoch 9472: train loss: 9.061155651579611e-06, val loss: 0.12114199250936508\n",
      "Epoch 9473: train loss: 1.2032307495246641e-05, val loss: 0.12221767753362656\n",
      "Epoch 9474: train loss: 1.0860045222216286e-05, val loss: 0.12158119678497314\n",
      "Epoch 9475: train loss: 7.437901786033763e-06, val loss: 0.1216525062918663\n",
      "Epoch 9476: train loss: 3.7881927710259333e-06, val loss: 0.12222017347812653\n",
      "Epoch 9477: train loss: 2.5476381324551767e-06, val loss: 0.1223755031824112\n",
      "Epoch 9478: train loss: 4.886491296929307e-06, val loss: 0.12223158031702042\n",
      "Epoch 9479: train loss: 5.8548121160129085e-06, val loss: 0.12206234782934189\n",
      "Epoch 9480: train loss: 6.991293957980815e-06, val loss: 0.12308122962713242\n",
      "Epoch 9481: train loss: 6.19300681137247e-06, val loss: 0.12249050289392471\n",
      "Epoch 9482: train loss: 5.356168458092725e-06, val loss: 0.12262232601642609\n",
      "Epoch 9483: train loss: 5.8393356994201895e-06, val loss: 0.12285488098859787\n",
      "Epoch 9484: train loss: 7.12045721229515e-06, val loss: 0.12278349697589874\n",
      "Epoch 9485: train loss: 1.1553609510883689e-05, val loss: 0.12320748716592789\n",
      "Epoch 9486: train loss: 1.894098568300251e-05, val loss: 0.12280833721160889\n",
      "Epoch 9487: train loss: 2.9017406632192433e-05, val loss: 0.12397056072950363\n",
      "Epoch 9488: train loss: 4.103536775801331e-05, val loss: 0.12397713959217072\n",
      "Epoch 9489: train loss: 4.979547156835906e-05, val loss: 0.12569598853588104\n",
      "Epoch 9490: train loss: 5.0014052249025553e-05, val loss: 0.12832723557949066\n",
      "Epoch 9491: train loss: 3.349183680256829e-05, val loss: 0.12890921533107758\n",
      "Epoch 9492: train loss: 2.2487782189273275e-05, val loss: 0.13043734431266785\n",
      "Epoch 9493: train loss: 1.652721402933821e-05, val loss: 0.1303851455450058\n",
      "Epoch 9494: train loss: 2.4911823857109994e-05, val loss: 0.12964867055416107\n",
      "Epoch 9495: train loss: 3.576510425773449e-05, val loss: 0.13141889870166779\n",
      "Epoch 9496: train loss: 2.3374999727820978e-05, val loss: 0.13170647621154785\n",
      "Epoch 9497: train loss: 9.334592505183537e-06, val loss: 0.13127866387367249\n",
      "Epoch 9498: train loss: 1.6109846910694614e-05, val loss: 0.1330314427614212\n",
      "Epoch 9499: train loss: 1.933497878781054e-05, val loss: 0.13272105157375336\n",
      "Epoch 9500: train loss: 1.2799469004676212e-05, val loss: 0.1329561322927475\n",
      "Epoch 9501: train loss: 7.254321189975599e-06, val loss: 0.1338016539812088\n",
      "Epoch 9502: train loss: 9.171008059638552e-06, val loss: 0.13308097422122955\n",
      "Epoch 9503: train loss: 1.4510422261082567e-05, val loss: 0.13417701423168182\n",
      "Epoch 9504: train loss: 1.4874665794195607e-05, val loss: 0.1339501142501831\n",
      "Epoch 9505: train loss: 8.633286597614642e-06, val loss: 0.13388271629810333\n",
      "Epoch 9506: train loss: 6.7556684371083975e-06, val loss: 0.13394306600093842\n",
      "Epoch 9507: train loss: 1.135913043981418e-05, val loss: 0.13401566445827484\n",
      "Epoch 9508: train loss: 1.5083794096426573e-05, val loss: 0.1348326951265335\n",
      "Epoch 9509: train loss: 1.6343725292244926e-05, val loss: 0.1343589425086975\n",
      "Epoch 9510: train loss: 2.0196988771203905e-05, val loss: 0.1338474005460739\n",
      "Epoch 9511: train loss: 3.486992500256747e-05, val loss: 0.13524752855300903\n",
      "Epoch 9512: train loss: 5.051281186752021e-05, val loss: 0.13297522068023682\n",
      "Epoch 9513: train loss: 6.770114850951359e-05, val loss: 0.135630801320076\n",
      "Epoch 9514: train loss: 8.83201792021282e-05, val loss: 0.13346317410469055\n",
      "Epoch 9515: train loss: 0.00012406791211105883, val loss: 0.13414578139781952\n",
      "Epoch 9516: train loss: 0.00015463186718989164, val loss: 0.1328108161687851\n",
      "Epoch 9517: train loss: 0.00017833960009738803, val loss: 0.13178592920303345\n",
      "Epoch 9518: train loss: 0.00016357062850147486, val loss: 0.1333974450826645\n",
      "Epoch 9519: train loss: 0.00011068758612964302, val loss: 0.12956644594669342\n",
      "Epoch 9520: train loss: 5.409031655290164e-05, val loss: 0.12870696187019348\n",
      "Epoch 9521: train loss: 4.4939144572708756e-05, val loss: 0.1296261101961136\n",
      "Epoch 9522: train loss: 6.935063720447943e-05, val loss: 0.12782377004623413\n",
      "Epoch 9523: train loss: 7.85297088441439e-05, val loss: 0.1284579038619995\n",
      "Epoch 9524: train loss: 5.121113281347789e-05, val loss: 0.1279623806476593\n",
      "Epoch 9525: train loss: 2.8821106752729975e-05, val loss: 0.12772522866725922\n",
      "Epoch 9526: train loss: 3.687586649903096e-05, val loss: 0.12669925391674042\n",
      "Epoch 9527: train loss: 4.048248229082674e-05, val loss: 0.12575678527355194\n",
      "Epoch 9528: train loss: 2.8018199373036623e-05, val loss: 0.1262291818857193\n",
      "Epoch 9529: train loss: 2.1593394194496796e-05, val loss: 0.12512081861495972\n",
      "Epoch 9530: train loss: 2.2675527361570857e-05, val loss: 0.1253434270620346\n",
      "Epoch 9531: train loss: 2.2493097276310436e-05, val loss: 0.12614397704601288\n",
      "Epoch 9532: train loss: 2.3475186026189476e-05, val loss: 0.12405053526163101\n",
      "Epoch 9533: train loss: 1.3667857274413109e-05, val loss: 0.12429329007863998\n",
      "Epoch 9534: train loss: 1.629110920475796e-05, val loss: 0.12549063563346863\n",
      "Epoch 9535: train loss: 1.937742308655288e-05, val loss: 0.12369741499423981\n",
      "Epoch 9536: train loss: 1.349158446828369e-05, val loss: 0.12386345863342285\n",
      "Epoch 9537: train loss: 1.1257946425757837e-05, val loss: 0.12417993694543839\n",
      "Epoch 9538: train loss: 1.1283490493951831e-05, val loss: 0.12432537227869034\n",
      "Epoch 9539: train loss: 9.70163364399923e-06, val loss: 0.12420902401208878\n",
      "Epoch 9540: train loss: 1.1507419003464747e-05, val loss: 0.1233944445848465\n",
      "Epoch 9541: train loss: 1.1393759450584184e-05, val loss: 0.1241263672709465\n",
      "Epoch 9542: train loss: 8.526725650881417e-06, val loss: 0.12389621883630753\n",
      "Epoch 9543: train loss: 7.802633263054304e-06, val loss: 0.12422628700733185\n",
      "Epoch 9544: train loss: 9.533154297969304e-06, val loss: 0.12427004426717758\n",
      "Epoch 9545: train loss: 1.0917146937572397e-05, val loss: 0.12454690784215927\n",
      "Epoch 9546: train loss: 1.3054705959802959e-05, val loss: 0.12504951655864716\n",
      "Epoch 9547: train loss: 1.9882403648807667e-05, val loss: 0.1254476010799408\n",
      "Epoch 9548: train loss: 3.216339246137068e-05, val loss: 0.12418153136968613\n",
      "Epoch 9549: train loss: 6.0910333559149876e-05, val loss: 0.1265036165714264\n",
      "Epoch 9550: train loss: 0.00012303379480727017, val loss: 0.12379822880029678\n",
      "Epoch 9551: train loss: 0.0002641691535245627, val loss: 0.12487746775150299\n",
      "Epoch 9552: train loss: 0.0005116155953146517, val loss: 0.12434817850589752\n",
      "Epoch 9553: train loss: 0.0008450191817246377, val loss: 0.12708918750286102\n",
      "Epoch 9554: train loss: 0.0009404172888025641, val loss: 0.12965624034404755\n",
      "Epoch 9555: train loss: 0.000607003050390631, val loss: 0.12940672039985657\n",
      "Epoch 9556: train loss: 0.00011518969404278323, val loss: 0.1285705864429474\n",
      "Epoch 9557: train loss: 0.0002899533719755709, val loss: 0.1290203332901001\n",
      "Epoch 9558: train loss: 0.00040134097798727453, val loss: 0.12872980535030365\n",
      "Epoch 9559: train loss: 0.000146534395753406, val loss: 0.12786802649497986\n",
      "Epoch 9560: train loss: 0.00015473358507733792, val loss: 0.12765364348888397\n",
      "Epoch 9561: train loss: 0.0001655664382269606, val loss: 0.12944869697093964\n",
      "Epoch 9562: train loss: 9.972249245038256e-05, val loss: 0.1282278299331665\n",
      "Epoch 9563: train loss: 0.00011835253826575354, val loss: 0.12433930486440659\n",
      "Epoch 9564: train loss: 0.00010531108273426071, val loss: 0.12645283341407776\n",
      "Epoch 9565: train loss: 6.606168608414009e-05, val loss: 0.12838171422481537\n",
      "Epoch 9566: train loss: 9.151433187071234e-05, val loss: 0.12696711719036102\n",
      "Epoch 9567: train loss: 3.888293940690346e-05, val loss: 0.1257762610912323\n",
      "Epoch 9568: train loss: 7.405894575640559e-05, val loss: 0.12842847406864166\n",
      "Epoch 9569: train loss: 2.757471338554751e-05, val loss: 0.129801407456398\n",
      "Epoch 9570: train loss: 5.892520857742056e-05, val loss: 0.1267005056142807\n",
      "Epoch 9571: train loss: 2.782470801321324e-05, val loss: 0.12579773366451263\n",
      "Epoch 9572: train loss: 4.6030956582399085e-05, val loss: 0.12832863628864288\n",
      "Epoch 9573: train loss: 2.3492477339459583e-05, val loss: 0.12949000298976898\n",
      "Epoch 9574: train loss: 3.3239419281017035e-05, val loss: 0.12710005044937134\n",
      "Epoch 9575: train loss: 1.8225244275527075e-05, val loss: 0.12667135894298553\n",
      "Epoch 9576: train loss: 2.4201119231292978e-05, val loss: 0.12880533933639526\n",
      "Epoch 9577: train loss: 1.4773019756830763e-05, val loss: 0.12942613661289215\n",
      "Epoch 9578: train loss: 1.951227204699535e-05, val loss: 0.12809766829013824\n",
      "Epoch 9579: train loss: 1.4605759133701213e-05, val loss: 0.1273907572031021\n",
      "Epoch 9580: train loss: 1.527517451904714e-05, val loss: 0.12842132151126862\n",
      "Epoch 9581: train loss: 1.4058469787414651e-05, val loss: 0.12899228930473328\n",
      "Epoch 9582: train loss: 1.0361681233916897e-05, val loss: 0.12905189394950867\n",
      "Epoch 9583: train loss: 1.2898220120405313e-05, val loss: 0.12878616154193878\n",
      "Epoch 9584: train loss: 6.530136033688905e-06, val loss: 0.1289125382900238\n",
      "Epoch 9585: train loss: 1.1126469871669542e-05, val loss: 0.12884049117565155\n",
      "Epoch 9586: train loss: 4.777028152602725e-06, val loss: 0.12894205749034882\n",
      "Epoch 9587: train loss: 9.539819075143896e-06, val loss: 0.12958137691020966\n",
      "Epoch 9588: train loss: 4.967292625224218e-06, val loss: 0.12973742187023163\n",
      "Epoch 9589: train loss: 6.83559346725815e-06, val loss: 0.12948468327522278\n",
      "Epoch 9590: train loss: 6.3831939769443125e-06, val loss: 0.1292845606803894\n",
      "Epoch 9591: train loss: 3.958232809964102e-06, val loss: 0.1298796385526657\n",
      "Epoch 9592: train loss: 6.446640782087343e-06, val loss: 0.1300288885831833\n",
      "Epoch 9593: train loss: 2.8134013518865686e-06, val loss: 0.12964358925819397\n",
      "Epoch 9594: train loss: 4.484661985770799e-06, val loss: 0.1296789050102234\n",
      "Epoch 9595: train loss: 4.396973963594064e-06, val loss: 0.13046202063560486\n",
      "Epoch 9596: train loss: 2.027251866820734e-06, val loss: 0.1304156631231308\n",
      "Epoch 9597: train loss: 4.452287157619139e-06, val loss: 0.12997637689113617\n",
      "Epoch 9598: train loss: 2.9706216082558967e-06, val loss: 0.13025204837322235\n",
      "Epoch 9599: train loss: 4.035101937915897e-06, val loss: 0.13074885308742523\n",
      "Epoch 9600: train loss: 8.976269782579038e-06, val loss: 0.13055290281772614\n",
      "Epoch 9601: train loss: 2.8091246349504218e-05, val loss: 0.13208125531673431\n",
      "Epoch 9602: train loss: 6.011774894432165e-05, val loss: 0.13091084361076355\n",
      "Epoch 9603: train loss: 7.497261685784906e-05, val loss: 0.13124971091747284\n",
      "Epoch 9604: train loss: 9.332355693913996e-05, val loss: 0.1316951960325241\n",
      "Epoch 9605: train loss: 0.00027348630828782916, val loss: 0.13075779378414154\n",
      "Epoch 9606: train loss: 0.0006160713965073228, val loss: 0.12711408734321594\n",
      "Epoch 9607: train loss: 0.00042240318725816905, val loss: 0.12835188210010529\n",
      "Epoch 9608: train loss: 0.00012463638267945498, val loss: 0.1265190690755844\n",
      "Epoch 9609: train loss: 0.00019882565538864583, val loss: 0.12316911667585373\n",
      "Epoch 9610: train loss: 0.0002049125760095194, val loss: 0.12612828612327576\n",
      "Epoch 9611: train loss: 0.00014167165500111878, val loss: 0.13014034926891327\n",
      "Epoch 9612: train loss: 0.00011589409405132756, val loss: 0.12833625078201294\n",
      "Epoch 9613: train loss: 0.00012010068167001009, val loss: 0.12599749863147736\n",
      "Epoch 9614: train loss: 0.0001277045375900343, val loss: 0.12394233793020248\n",
      "Epoch 9615: train loss: 5.5005246395012364e-05, val loss: 0.12613508105278015\n",
      "Epoch 9616: train loss: 8.881133544491604e-05, val loss: 0.1273379623889923\n",
      "Epoch 9617: train loss: 7.927620754344389e-05, val loss: 0.12423039972782135\n",
      "Epoch 9618: train loss: 3.786285742535256e-05, val loss: 0.1236082911491394\n",
      "Epoch 9619: train loss: 5.723754657083191e-05, val loss: 0.12622493505477905\n",
      "Epoch 9620: train loss: 5.541359496419318e-05, val loss: 0.12651203572750092\n",
      "Epoch 9621: train loss: 4.053504017065279e-05, val loss: 0.12481186538934708\n",
      "Epoch 9622: train loss: 3.991602352471091e-05, val loss: 0.12456362694501877\n",
      "Epoch 9623: train loss: 4.891187199973501e-05, val loss: 0.12494782358407974\n",
      "Epoch 9624: train loss: 1.8903996533481404e-05, val loss: 0.1255892515182495\n",
      "Epoch 9625: train loss: 3.325498983031139e-05, val loss: 0.12485187500715256\n",
      "Epoch 9626: train loss: 3.326137084513903e-05, val loss: 0.12350336462259293\n",
      "Epoch 9627: train loss: 1.4198324606695678e-05, val loss: 0.12321662902832031\n",
      "Epoch 9628: train loss: 2.3209180653793737e-05, val loss: 0.1238778606057167\n",
      "Epoch 9629: train loss: 1.8330960301682353e-05, val loss: 0.12393682450056076\n",
      "Epoch 9630: train loss: 1.640529626456555e-05, val loss: 0.12433674186468124\n",
      "Epoch 9631: train loss: 1.4626413758378476e-05, val loss: 0.12490835040807724\n",
      "Epoch 9632: train loss: 1.531194538983982e-05, val loss: 0.12442143261432648\n",
      "Epoch 9633: train loss: 9.64090577326715e-06, val loss: 0.12424164265394211\n",
      "Epoch 9634: train loss: 1.5282847016351297e-05, val loss: 0.12431211769580841\n",
      "Epoch 9635: train loss: 1.2004291420453228e-05, val loss: 0.12424428761005402\n",
      "Epoch 9636: train loss: 1.0165498679270968e-05, val loss: 0.12341292202472687\n",
      "Epoch 9637: train loss: 8.448601874988526e-06, val loss: 0.1234118714928627\n",
      "Epoch 9638: train loss: 1.8477127014193684e-05, val loss: 0.12350714206695557\n",
      "Epoch 9639: train loss: 3.600347918109037e-05, val loss: 0.12472615391016006\n",
      "Epoch 9640: train loss: 9.13858093554154e-05, val loss: 0.1231239065527916\n",
      "Epoch 9641: train loss: 0.0001355244021397084, val loss: 0.12400298565626144\n",
      "Epoch 9642: train loss: 0.0001110570301534608, val loss: 0.12389719486236572\n",
      "Epoch 9643: train loss: 4.560373054118827e-05, val loss: 0.12389584630727768\n",
      "Epoch 9644: train loss: 1.6186992070288397e-05, val loss: 0.12448437511920929\n",
      "Epoch 9645: train loss: 4.131036621402018e-05, val loss: 0.12282109260559082\n",
      "Epoch 9646: train loss: 6.76447743899189e-05, val loss: 0.12393207848072052\n",
      "Epoch 9647: train loss: 4.840224210056476e-05, val loss: 0.12434935569763184\n",
      "Epoch 9648: train loss: 1.755225457600318e-05, val loss: 0.12384887039661407\n",
      "Epoch 9649: train loss: 2.3694412448094226e-05, val loss: 0.12399423122406006\n",
      "Epoch 9650: train loss: 4.250676283845678e-05, val loss: 0.12488260120153427\n",
      "Epoch 9651: train loss: 3.394104351173155e-05, val loss: 0.12450667470693588\n",
      "Epoch 9652: train loss: 1.542724930914119e-05, val loss: 0.1237897202372551\n",
      "Epoch 9653: train loss: 1.2698905266006477e-05, val loss: 0.1239173635840416\n",
      "Epoch 9654: train loss: 2.3642975065740757e-05, val loss: 0.12436287850141525\n",
      "Epoch 9655: train loss: 2.1975130948703736e-05, val loss: 0.1237877607345581\n",
      "Epoch 9656: train loss: 1.225473897648044e-05, val loss: 0.12396194785833359\n",
      "Epoch 9657: train loss: 8.647609320178162e-06, val loss: 0.12450232356786728\n",
      "Epoch 9658: train loss: 1.4532233763020486e-05, val loss: 0.12411307543516159\n",
      "Epoch 9659: train loss: 1.703751513559837e-05, val loss: 0.1242716833949089\n",
      "Epoch 9660: train loss: 1.1508653187775053e-05, val loss: 0.12452533096075058\n",
      "Epoch 9661: train loss: 6.9805987550353166e-06, val loss: 0.12440755218267441\n",
      "Epoch 9662: train loss: 7.729328899586108e-06, val loss: 0.12409894913434982\n",
      "Epoch 9663: train loss: 1.2465056897781324e-05, val loss: 0.12452235072851181\n",
      "Epoch 9664: train loss: 1.616447843844071e-05, val loss: 0.12383633106946945\n",
      "Epoch 9665: train loss: 1.6913858416955918e-05, val loss: 0.12462802231311798\n",
      "Epoch 9666: train loss: 1.6036736269597895e-05, val loss: 0.12421079725027084\n",
      "Epoch 9667: train loss: 1.8871411157306284e-05, val loss: 0.1246328130364418\n",
      "Epoch 9668: train loss: 2.6801757485372946e-05, val loss: 0.12358037382364273\n",
      "Epoch 9669: train loss: 4.398876626510173e-05, val loss: 0.12484899908304214\n",
      "Epoch 9670: train loss: 7.467067916877568e-05, val loss: 0.12218178808689117\n",
      "Epoch 9671: train loss: 0.00014278815069701523, val loss: 0.12502525746822357\n",
      "Epoch 9672: train loss: 0.000272483128355816, val loss: 0.1211075410246849\n",
      "Epoch 9673: train loss: 0.0005172482342459261, val loss: 0.1259755790233612\n",
      "Epoch 9674: train loss: 0.0008848462020978332, val loss: 0.12349765747785568\n",
      "Epoch 9675: train loss: 0.0013049666304141283, val loss: 0.12678682804107666\n",
      "Epoch 9676: train loss: 0.0012765086721628904, val loss: 0.12158029526472092\n",
      "Epoch 9677: train loss: 0.0009145483491010964, val loss: 0.12848182022571564\n",
      "Epoch 9678: train loss: 0.0007405588985420763, val loss: 0.12322080135345459\n",
      "Epoch 9679: train loss: 0.0006346277077682316, val loss: 0.12320413440465927\n",
      "Epoch 9680: train loss: 0.00019868448725901544, val loss: 0.1277541220188141\n",
      "Epoch 9681: train loss: 0.00027122232131659985, val loss: 0.12881921231746674\n",
      "Epoch 9682: train loss: 0.00021959978039376438, val loss: 0.1273752897977829\n",
      "Epoch 9683: train loss: 0.00029132128111086786, val loss: 0.12748733162879944\n",
      "Epoch 9684: train loss: 0.00033178451121784747, val loss: 0.13451515138149261\n",
      "Epoch 9685: train loss: 0.00023099429381545633, val loss: 0.1356029510498047\n",
      "Epoch 9686: train loss: 0.00015983340563252568, val loss: 0.13382859528064728\n",
      "Epoch 9687: train loss: 7.89643163443543e-05, val loss: 0.1350293904542923\n",
      "Epoch 9688: train loss: 9.81262419372797e-05, val loss: 0.1375742107629776\n",
      "Epoch 9689: train loss: 0.0001258036500075832, val loss: 0.13833527266979218\n",
      "Epoch 9690: train loss: 0.0001587713195476681, val loss: 0.13491234183311462\n",
      "Epoch 9691: train loss: 0.00011206157796550542, val loss: 0.13544118404388428\n",
      "Epoch 9692: train loss: 7.515436300309375e-05, val loss: 0.13796591758728027\n",
      "Epoch 9693: train loss: 3.433093661442399e-05, val loss: 0.13942143321037292\n",
      "Epoch 9694: train loss: 5.2881016017636284e-05, val loss: 0.13931933045387268\n",
      "Epoch 9695: train loss: 7.17793736839667e-05, val loss: 0.13828423619270325\n",
      "Epoch 9696: train loss: 7.914462184999138e-05, val loss: 0.14012478291988373\n",
      "Epoch 9697: train loss: 5.656576104229316e-05, val loss: 0.14072714745998383\n",
      "Epoch 9698: train loss: 3.200696664862335e-05, val loss: 0.14072659611701965\n",
      "Epoch 9699: train loss: 1.8358494344283827e-05, val loss: 0.14078915119171143\n",
      "Epoch 9700: train loss: 3.2098523661261424e-05, val loss: 0.14076955616474152\n",
      "Epoch 9701: train loss: 4.089540379936807e-05, val loss: 0.14111550152301788\n",
      "Epoch 9702: train loss: 4.051102223456837e-05, val loss: 0.13999101519584656\n",
      "Epoch 9703: train loss: 2.75723486993229e-05, val loss: 0.14067566394805908\n",
      "Epoch 9704: train loss: 1.1411004379624501e-05, val loss: 0.14178220927715302\n",
      "Epoch 9705: train loss: 1.4962835848564282e-05, val loss: 0.14158545434474945\n",
      "Epoch 9706: train loss: 1.6628880985081196e-05, val loss: 0.14123743772506714\n",
      "Epoch 9707: train loss: 2.612632124510128e-05, val loss: 0.14100535213947296\n",
      "Epoch 9708: train loss: 1.960515874088742e-05, val loss: 0.14213596284389496\n",
      "Epoch 9709: train loss: 1.3053802831564099e-05, val loss: 0.14193038642406464\n",
      "Epoch 9710: train loss: 6.644950644840719e-06, val loss: 0.14116017520427704\n",
      "Epoch 9711: train loss: 7.911823558970354e-06, val loss: 0.14108334481716156\n",
      "Epoch 9712: train loss: 1.1791767974500544e-05, val loss: 0.1410856693983078\n",
      "Epoch 9713: train loss: 1.4277143236540724e-05, val loss: 0.14173740148544312\n",
      "Epoch 9714: train loss: 1.0791976819746196e-05, val loss: 0.14128927886486053\n",
      "Epoch 9715: train loss: 5.915792371524731e-06, val loss: 0.14112751185894012\n",
      "Epoch 9716: train loss: 4.594004167302046e-06, val loss: 0.14116553962230682\n",
      "Epoch 9717: train loss: 4.963458650308894e-06, val loss: 0.14119850099086761\n",
      "Epoch 9718: train loss: 8.097664249362424e-06, val loss: 0.1417243331670761\n",
      "Epoch 9719: train loss: 7.145872586988844e-06, val loss: 0.1414317935705185\n",
      "Epoch 9720: train loss: 6.801924882893218e-06, val loss: 0.14151126146316528\n",
      "Epoch 9721: train loss: 2.9061407076369505e-06, val loss: 0.14175613224506378\n",
      "Epoch 9722: train loss: 3.047880682061077e-06, val loss: 0.14181742072105408\n",
      "Epoch 9723: train loss: 4.354154498287244e-06, val loss: 0.14210276305675507\n",
      "Epoch 9724: train loss: 5.425734798336634e-06, val loss: 0.14183567464351654\n",
      "Epoch 9725: train loss: 8.62603701534681e-06, val loss: 0.14237737655639648\n",
      "Epoch 9726: train loss: 1.0119588296220172e-05, val loss: 0.14287109673023224\n",
      "Epoch 9727: train loss: 1.3457156455842778e-05, val loss: 0.14305277168750763\n",
      "Epoch 9728: train loss: 1.7726426449371502e-05, val loss: 0.1432584673166275\n",
      "Epoch 9729: train loss: 2.0379580746521242e-05, val loss: 0.14420945942401886\n",
      "Epoch 9730: train loss: 2.1680049030692317e-05, val loss: 0.14354653656482697\n",
      "Epoch 9731: train loss: 2.1820855181431398e-05, val loss: 0.14495457708835602\n",
      "Epoch 9732: train loss: 1.2528279512480367e-05, val loss: 0.14493632316589355\n",
      "Epoch 9733: train loss: 3.1722811399959028e-06, val loss: 0.1453622281551361\n",
      "Epoch 9734: train loss: 4.114389867027057e-06, val loss: 0.1463717222213745\n",
      "Epoch 9735: train loss: 1.0356287020840682e-05, val loss: 0.14501404762268066\n",
      "Epoch 9736: train loss: 1.572382461745292e-05, val loss: 0.14585614204406738\n",
      "Epoch 9737: train loss: 1.776844510459341e-05, val loss: 0.14557035267353058\n",
      "Epoch 9738: train loss: 1.6765674445196055e-05, val loss: 0.14630921185016632\n",
      "Epoch 9739: train loss: 1.3044740626355633e-05, val loss: 0.1453327238559723\n",
      "Epoch 9740: train loss: 8.552386134397238e-06, val loss: 0.1463942974805832\n",
      "Epoch 9741: train loss: 4.102921593585052e-06, val loss: 0.14544938504695892\n",
      "Epoch 9742: train loss: 2.4024182039283914e-06, val loss: 0.14563412964344025\n",
      "Epoch 9743: train loss: 2.597214233901468e-06, val loss: 0.14558930695056915\n",
      "Epoch 9744: train loss: 3.912294687324902e-06, val loss: 0.14509797096252441\n",
      "Epoch 9745: train loss: 4.264049493940547e-06, val loss: 0.14543958008289337\n",
      "Epoch 9746: train loss: 5.680003141605994e-06, val loss: 0.14464040100574493\n",
      "Epoch 9747: train loss: 1.247656109626405e-05, val loss: 0.1466231793165207\n",
      "Epoch 9748: train loss: 3.4278804378118366e-05, val loss: 0.14271506667137146\n",
      "Epoch 9749: train loss: 0.00010877041495405138, val loss: 0.14719711244106293\n",
      "Epoch 9750: train loss: 0.00027908518677577376, val loss: 0.13796944916248322\n",
      "Epoch 9751: train loss: 0.0006724310806021094, val loss: 0.14590442180633545\n",
      "Epoch 9752: train loss: 0.0006701413076370955, val loss: 0.143711119890213\n",
      "Epoch 9753: train loss: 0.00031036967993713915, val loss: 0.13889680802822113\n",
      "Epoch 9754: train loss: 0.00011727293895091861, val loss: 0.14381448924541473\n",
      "Epoch 9755: train loss: 0.00024552285321988165, val loss: 0.1403912454843521\n",
      "Epoch 9756: train loss: 0.00021390455367509276, val loss: 0.1354770064353943\n",
      "Epoch 9757: train loss: 0.00015840651758480817, val loss: 0.13853780925273895\n",
      "Epoch 9758: train loss: 0.00012303466792218387, val loss: 0.1364525407552719\n",
      "Epoch 9759: train loss: 0.00010076401667902246, val loss: 0.13061456382274628\n",
      "Epoch 9760: train loss: 0.00013300370483193547, val loss: 0.13128118216991425\n",
      "Epoch 9761: train loss: 6.525615754071623e-05, val loss: 0.1333201825618744\n",
      "Epoch 9762: train loss: 7.964820542838424e-05, val loss: 0.12991365790367126\n",
      "Epoch 9763: train loss: 9.94706861092709e-05, val loss: 0.1283082515001297\n",
      "Epoch 9764: train loss: 2.3907574359327555e-05, val loss: 0.12854082882404327\n",
      "Epoch 9765: train loss: 7.940716750454158e-05, val loss: 0.12626217305660248\n",
      "Epoch 9766: train loss: 4.3566433305386454e-05, val loss: 0.1264982521533966\n",
      "Epoch 9767: train loss: 3.375932283233851e-05, val loss: 0.12738044559955597\n",
      "Epoch 9768: train loss: 5.172512464923784e-05, val loss: 0.1252361238002777\n",
      "Epoch 9769: train loss: 3.0494116799673066e-05, val loss: 0.12335195392370224\n",
      "Epoch 9770: train loss: 2.6587742468109354e-05, val loss: 0.12418635934591293\n",
      "Epoch 9771: train loss: 3.688021388370544e-05, val loss: 0.12445919960737228\n",
      "Epoch 9772: train loss: 1.6219059034483507e-05, val loss: 0.12349430471658707\n",
      "Epoch 9773: train loss: 2.6002107915701345e-05, val loss: 0.12302837520837784\n",
      "Epoch 9774: train loss: 2.2428750526160002e-05, val loss: 0.12271914631128311\n",
      "Epoch 9775: train loss: 1.3655521797772963e-05, val loss: 0.12260022014379501\n",
      "Epoch 9776: train loss: 2.1015224774600938e-05, val loss: 0.12295901775360107\n",
      "Epoch 9777: train loss: 1.2822611097362824e-05, val loss: 0.12280704826116562\n",
      "Epoch 9778: train loss: 1.3537744962377474e-05, val loss: 0.12172675132751465\n",
      "Epoch 9779: train loss: 1.2065662303939462e-05, val loss: 0.12152262032032013\n",
      "Epoch 9780: train loss: 1.1923561942239758e-05, val loss: 0.12236606329679489\n",
      "Epoch 9781: train loss: 9.67343930824427e-06, val loss: 0.12261495739221573\n",
      "Epoch 9782: train loss: 8.861549758876208e-06, val loss: 0.1221546158194542\n",
      "Epoch 9783: train loss: 1.0128826943400782e-05, val loss: 0.12177407741546631\n",
      "Epoch 9784: train loss: 6.787255188100971e-06, val loss: 0.12217290699481964\n",
      "Epoch 9785: train loss: 7.2635389187780675e-06, val loss: 0.1224636435508728\n",
      "Epoch 9786: train loss: 7.134085080906516e-06, val loss: 0.12197624891996384\n",
      "Epoch 9787: train loss: 7.656135494471528e-06, val loss: 0.12198305130004883\n",
      "Epoch 9788: train loss: 4.212753538013203e-06, val loss: 0.1219601035118103\n",
      "Epoch 9789: train loss: 6.5518215706106275e-06, val loss: 0.12223024666309357\n",
      "Epoch 9790: train loss: 4.932157935400028e-06, val loss: 0.12299349159002304\n",
      "Epoch 9791: train loss: 3.1293416213884484e-06, val loss: 0.12333505600690842\n",
      "Epoch 9792: train loss: 5.738441359426361e-06, val loss: 0.12303296476602554\n",
      "Epoch 9793: train loss: 3.4571808100736234e-06, val loss: 0.12302128225564957\n",
      "Epoch 9794: train loss: 3.12844736072293e-06, val loss: 0.1234704852104187\n",
      "Epoch 9795: train loss: 4.671495844377205e-06, val loss: 0.123398557305336\n",
      "Epoch 9796: train loss: 4.407018423080444e-06, val loss: 0.12333955615758896\n",
      "Epoch 9797: train loss: 6.0708348428306635e-06, val loss: 0.12315143644809723\n",
      "Epoch 9798: train loss: 1.1719720532710198e-05, val loss: 0.12433631718158722\n",
      "Epoch 9799: train loss: 1.951380545506254e-05, val loss: 0.1236143484711647\n",
      "Epoch 9800: train loss: 3.763640779652633e-05, val loss: 0.12452931702136993\n",
      "Epoch 9801: train loss: 7.058202754706144e-05, val loss: 0.12360576540231705\n",
      "Epoch 9802: train loss: 0.00011191991507075727, val loss: 0.12556558847427368\n",
      "Epoch 9803: train loss: 0.00011921330587938428, val loss: 0.1251799613237381\n",
      "Epoch 9804: train loss: 9.126075747190043e-05, val loss: 0.1262301653623581\n",
      "Epoch 9805: train loss: 4.7237474063877016e-05, val loss: 0.1269376426935196\n",
      "Epoch 9806: train loss: 4.157123839831911e-05, val loss: 0.12699423730373383\n",
      "Epoch 9807: train loss: 8.923517452785745e-05, val loss: 0.12980887293815613\n",
      "Epoch 9808: train loss: 0.00014150307106319815, val loss: 0.12690101563930511\n",
      "Epoch 9809: train loss: 0.0001521661033621058, val loss: 0.132566899061203\n",
      "Epoch 9810: train loss: 0.0001595684007043019, val loss: 0.13188162446022034\n",
      "Epoch 9811: train loss: 0.00022664600692223758, val loss: 0.133930042386055\n",
      "Epoch 9812: train loss: 9.915209375321865e-05, val loss: 0.13117538392543793\n",
      "Epoch 9813: train loss: 0.00013082323130220175, val loss: 0.13213935494422913\n",
      "Epoch 9814: train loss: 0.00011337658361298963, val loss: 0.1364392340183258\n",
      "Epoch 9815: train loss: 7.835169526515529e-05, val loss: 0.1339193880558014\n",
      "Epoch 9816: train loss: 5.951956336502917e-05, val loss: 0.13051660358905792\n",
      "Epoch 9817: train loss: 6.556815060321242e-05, val loss: 0.132588729262352\n",
      "Epoch 9818: train loss: 7.545748667325824e-05, val loss: 0.13460707664489746\n",
      "Epoch 9819: train loss: 6.310544995358214e-05, val loss: 0.13273702561855316\n",
      "Epoch 9820: train loss: 4.991714740754105e-05, val loss: 0.13295717537403107\n",
      "Epoch 9821: train loss: 4.0595892642159015e-05, val loss: 0.13378310203552246\n",
      "Epoch 9822: train loss: 4.642639396479353e-05, val loss: 0.13491635024547577\n",
      "Epoch 9823: train loss: 3.7291600165190175e-05, val loss: 0.13349811732769012\n",
      "Epoch 9824: train loss: 2.756718276941683e-05, val loss: 0.1315465271472931\n",
      "Epoch 9825: train loss: 1.933022213052027e-05, val loss: 0.1330019235610962\n",
      "Epoch 9826: train loss: 2.217959990957752e-05, val loss: 0.13586494326591492\n",
      "Epoch 9827: train loss: 2.8410569939296693e-05, val loss: 0.1361035406589508\n",
      "Epoch 9828: train loss: 2.060148835880682e-05, val loss: 0.13351015746593475\n",
      "Epoch 9829: train loss: 1.8335971617489122e-05, val loss: 0.13519854843616486\n",
      "Epoch 9830: train loss: 1.6804133338155225e-05, val loss: 0.13690514862537384\n",
      "Epoch 9831: train loss: 2.132862755388487e-05, val loss: 0.1370759755373001\n",
      "Epoch 9832: train loss: 1.6951325960690156e-05, val loss: 0.1353323757648468\n",
      "Epoch 9833: train loss: 1.3332381968211848e-05, val loss: 0.13633422553539276\n",
      "Epoch 9834: train loss: 1.2132256415497977e-05, val loss: 0.136867955327034\n",
      "Epoch 9835: train loss: 1.2683610293606762e-05, val loss: 0.13818295300006866\n",
      "Epoch 9836: train loss: 1.4383276720764115e-05, val loss: 0.13641393184661865\n",
      "Epoch 9837: train loss: 1.2826501915697008e-05, val loss: 0.13738775253295898\n",
      "Epoch 9838: train loss: 1.3436447261483409e-05, val loss: 0.1378277987241745\n",
      "Epoch 9839: train loss: 1.4114628356765024e-05, val loss: 0.13848458230495453\n",
      "Epoch 9840: train loss: 1.9585590052884072e-05, val loss: 0.1366134136915207\n",
      "Epoch 9841: train loss: 3.1320123525802046e-05, val loss: 0.13998691737651825\n",
      "Epoch 9842: train loss: 5.6551260058768094e-05, val loss: 0.13664479553699493\n",
      "Epoch 9843: train loss: 0.00011625088518485427, val loss: 0.14035086333751678\n",
      "Epoch 9844: train loss: 0.0002677294542081654, val loss: 0.1366318315267563\n",
      "Epoch 9845: train loss: 0.0006117661250755191, val loss: 0.14288602769374847\n",
      "Epoch 9846: train loss: 0.0011568950721994042, val loss: 0.13945548236370087\n",
      "Epoch 9847: train loss: 0.0014266789657995105, val loss: 0.1436641663312912\n",
      "Epoch 9848: train loss: 0.0009255254408344626, val loss: 0.1429343819618225\n",
      "Epoch 9849: train loss: 0.00032627605833113194, val loss: 0.14744462072849274\n",
      "Epoch 9850: train loss: 0.0004970048321411014, val loss: 0.14389923214912415\n",
      "Epoch 9851: train loss: 0.00045706634409725666, val loss: 0.13867907226085663\n",
      "Epoch 9852: train loss: 0.00014548296167049557, val loss: 0.14113852381706238\n",
      "Epoch 9853: train loss: 0.0002646511420607567, val loss: 0.14558853209018707\n",
      "Epoch 9854: train loss: 0.0002379742218181491, val loss: 0.14203372597694397\n",
      "Epoch 9855: train loss: 0.00017313726129941642, val loss: 0.1372857242822647\n",
      "Epoch 9856: train loss: 0.00024487069458700716, val loss: 0.14200854301452637\n",
      "Epoch 9857: train loss: 0.00014556458336301148, val loss: 0.14323659241199493\n",
      "Epoch 9858: train loss: 0.00014916088548488915, val loss: 0.14249257743358612\n",
      "Epoch 9859: train loss: 9.061245509656146e-05, val loss: 0.14081144332885742\n",
      "Epoch 9860: train loss: 8.238391455961391e-05, val loss: 0.14203695952892303\n",
      "Epoch 9861: train loss: 7.905827806098387e-05, val loss: 0.1423957496881485\n",
      "Epoch 9862: train loss: 8.373494347324595e-05, val loss: 0.13996608555316925\n",
      "Epoch 9863: train loss: 8.70050280354917e-05, val loss: 0.14002560079097748\n",
      "Epoch 9864: train loss: 7.098590140230954e-05, val loss: 0.1413232386112213\n",
      "Epoch 9865: train loss: 6.0943246353417635e-05, val loss: 0.14160245656967163\n",
      "Epoch 9866: train loss: 3.8411868445109576e-05, val loss: 0.13927797973155975\n",
      "Epoch 9867: train loss: 3.6426314181881025e-05, val loss: 0.1388183832168579\n",
      "Epoch 9868: train loss: 3.259768709540367e-05, val loss: 0.1415206342935562\n",
      "Epoch 9869: train loss: 3.742029002751224e-05, val loss: 0.14192995429039001\n",
      "Epoch 9870: train loss: 4.006882954854518e-05, val loss: 0.1398141235113144\n",
      "Epoch 9871: train loss: 3.194026794517413e-05, val loss: 0.1378791779279709\n",
      "Epoch 9872: train loss: 3.0381223041331396e-05, val loss: 0.1398707926273346\n",
      "Epoch 9873: train loss: 1.595411595189944e-05, val loss: 0.14180560410022736\n",
      "Epoch 9874: train loss: 1.871515814855229e-05, val loss: 0.1406300663948059\n",
      "Epoch 9875: train loss: 1.2263841199455783e-05, val loss: 0.13876916468143463\n",
      "Epoch 9876: train loss: 2.0211720766383223e-05, val loss: 0.13913214206695557\n",
      "Epoch 9877: train loss: 1.588058876222931e-05, val loss: 0.14125756919384003\n",
      "Epoch 9878: train loss: 1.936197077156976e-05, val loss: 0.14069277048110962\n",
      "Epoch 9879: train loss: 1.1369796084181871e-05, val loss: 0.13919952511787415\n",
      "Epoch 9880: train loss: 1.2741206774080638e-05, val loss: 0.13929496705532074\n",
      "Epoch 9881: train loss: 4.971689122612588e-06, val loss: 0.14088833332061768\n",
      "Epoch 9882: train loss: 1.0576577551546507e-05, val loss: 0.14111338555812836\n",
      "Epoch 9883: train loss: 4.997719770472031e-06, val loss: 0.1398547738790512\n",
      "Epoch 9884: train loss: 1.1660206837404985e-05, val loss: 0.1405504047870636\n",
      "Epoch 9885: train loss: 6.315741757134674e-06, val loss: 0.1417597532272339\n",
      "Epoch 9886: train loss: 9.025704457599204e-06, val loss: 0.14147460460662842\n",
      "Epoch 9887: train loss: 4.431692559592193e-06, val loss: 0.1401403397321701\n",
      "Epoch 9888: train loss: 5.198374310566578e-06, val loss: 0.14075568318367004\n",
      "Epoch 9889: train loss: 2.8143128929514205e-06, val loss: 0.14190033078193665\n",
      "Epoch 9890: train loss: 4.564837126963539e-06, val loss: 0.14108356833457947\n",
      "Epoch 9891: train loss: 4.042326963826781e-06, val loss: 0.14054527878761292\n",
      "Epoch 9892: train loss: 4.515024556894787e-06, val loss: 0.1411716789007187\n",
      "Epoch 9893: train loss: 3.921019015251659e-06, val loss: 0.1416347175836563\n",
      "Epoch 9894: train loss: 3.890239895554259e-06, val loss: 0.14086522161960602\n",
      "Epoch 9895: train loss: 2.490054612280801e-06, val loss: 0.14077536761760712\n",
      "Epoch 9896: train loss: 1.9179831269866554e-06, val loss: 0.14122374355793\n",
      "Epoch 9897: train loss: 1.9093160972261103e-06, val loss: 0.14110545814037323\n",
      "Epoch 9898: train loss: 1.7622728591959458e-06, val loss: 0.1409018486738205\n",
      "Epoch 9899: train loss: 1.9283918391010957e-06, val loss: 0.1408705711364746\n",
      "Epoch 9900: train loss: 2.334870032427716e-06, val loss: 0.14144228398799896\n",
      "Epoch 9901: train loss: 1.789910015759233e-06, val loss: 0.14134781062602997\n",
      "Epoch 9902: train loss: 2.061764917016262e-06, val loss: 0.14134733378887177\n",
      "Epoch 9903: train loss: 1.315980057370325e-06, val loss: 0.14120332896709442\n",
      "Epoch 9904: train loss: 1.2752933571391623e-06, val loss: 0.14157941937446594\n",
      "Epoch 9905: train loss: 1.482007519371109e-06, val loss: 0.1415114849805832\n",
      "Epoch 9906: train loss: 2.3694344690738944e-06, val loss: 0.1419069916009903\n",
      "Epoch 9907: train loss: 8.225110832427163e-06, val loss: 0.13930140435695648\n",
      "Epoch 9908: train loss: 2.9712749892496504e-05, val loss: 0.14232276380062103\n",
      "Epoch 9909: train loss: 0.00010548175487201661, val loss: 0.13813185691833496\n",
      "Epoch 9910: train loss: 0.00026276768767274916, val loss: 0.14041487872600555\n",
      "Epoch 9911: train loss: 0.0002009644958889112, val loss: 0.1457822620868683\n",
      "Epoch 9912: train loss: 0.00015621069178450853, val loss: 0.14249198138713837\n",
      "Epoch 9913: train loss: 9.608486288925633e-05, val loss: 0.14285863935947418\n",
      "Epoch 9914: train loss: 7.45091019780375e-05, val loss: 0.14517919719219208\n",
      "Epoch 9915: train loss: 9.070103260455653e-05, val loss: 0.14268091320991516\n",
      "Epoch 9916: train loss: 8.338156476384029e-05, val loss: 0.14350822567939758\n",
      "Epoch 9917: train loss: 4.2524581658653915e-05, val loss: 0.14450962841510773\n",
      "Epoch 9918: train loss: 4.82106588606257e-05, val loss: 0.14264047145843506\n",
      "Epoch 9919: train loss: 5.4915391956456006e-05, val loss: 0.1420082300901413\n",
      "Epoch 9920: train loss: 4.327181522967294e-05, val loss: 0.1426478624343872\n",
      "Epoch 9921: train loss: 3.9096223190426826e-05, val loss: 0.1426451951265335\n",
      "Epoch 9922: train loss: 3.523699342622422e-05, val loss: 0.14237020909786224\n",
      "Epoch 9923: train loss: 3.343658318044618e-05, val loss: 0.1426166146993637\n",
      "Epoch 9924: train loss: 2.8129954444011673e-05, val loss: 0.14308059215545654\n",
      "Epoch 9925: train loss: 2.5376253688591532e-05, val loss: 0.14336863160133362\n",
      "Epoch 9926: train loss: 2.1605634174193256e-05, val loss: 0.1410607248544693\n",
      "Epoch 9927: train loss: 1.9733393855858594e-05, val loss: 0.13938279449939728\n",
      "Epoch 9928: train loss: 1.6966223483905196e-05, val loss: 0.14090080559253693\n",
      "Epoch 9929: train loss: 1.9520888599799946e-05, val loss: 0.14157941937446594\n",
      "Epoch 9930: train loss: 1.7367288819514215e-05, val loss: 0.14094436168670654\n",
      "Epoch 9931: train loss: 1.3274218872538768e-05, val loss: 0.14055006206035614\n",
      "Epoch 9932: train loss: 1.0764862963696942e-05, val loss: 0.1404331773519516\n",
      "Epoch 9933: train loss: 1.2372043784125708e-05, val loss: 0.14039777219295502\n",
      "Epoch 9934: train loss: 1.2733073162962683e-05, val loss: 0.14072872698307037\n",
      "Epoch 9935: train loss: 8.814737157081254e-06, val loss: 0.14041297137737274\n",
      "Epoch 9936: train loss: 6.751427918061381e-06, val loss: 0.13930976390838623\n",
      "Epoch 9937: train loss: 7.67570236348547e-06, val loss: 0.13912513852119446\n",
      "Epoch 9938: train loss: 8.760205673752353e-06, val loss: 0.1398257464170456\n",
      "Epoch 9939: train loss: 5.861366844328586e-06, val loss: 0.1400327980518341\n",
      "Epoch 9940: train loss: 6.560978818015428e-06, val loss: 0.13986870646476746\n",
      "Epoch 9941: train loss: 7.38677090339479e-06, val loss: 0.13957928121089935\n",
      "Epoch 9942: train loss: 7.4407657848496456e-06, val loss: 0.13962781429290771\n",
      "Epoch 9943: train loss: 7.264348369062645e-06, val loss: 0.1400330364704132\n",
      "Epoch 9944: train loss: 6.0500192375911865e-06, val loss: 0.13978657126426697\n",
      "Epoch 9945: train loss: 7.716715117567219e-06, val loss: 0.1396872103214264\n",
      "Epoch 9946: train loss: 8.503831850248389e-06, val loss: 0.13994787633419037\n",
      "Epoch 9947: train loss: 1.1198396350664552e-05, val loss: 0.13985805213451385\n",
      "Epoch 9948: train loss: 2.094165211019572e-05, val loss: 0.14087910950183868\n",
      "Epoch 9949: train loss: 4.0259961679112166e-05, val loss: 0.1402311623096466\n",
      "Epoch 9950: train loss: 9.516697900835425e-05, val loss: 0.14053325355052948\n",
      "Epoch 9951: train loss: 0.00021812559862155467, val loss: 0.13876593112945557\n",
      "Epoch 9952: train loss: 0.00045002781553193927, val loss: 0.14093036949634552\n",
      "Epoch 9953: train loss: 0.0006148121901787817, val loss: 0.14138707518577576\n",
      "Epoch 9954: train loss: 0.0006309021846391261, val loss: 0.1380699872970581\n",
      "Epoch 9955: train loss: 0.0003065553610213101, val loss: 0.1445513814687729\n",
      "Epoch 9956: train loss: 0.00019219008390791714, val loss: 0.1387214958667755\n",
      "Epoch 9957: train loss: 0.00031743882573209703, val loss: 0.14143556356430054\n",
      "Epoch 9958: train loss: 0.0003660497604869306, val loss: 0.14108510315418243\n",
      "Epoch 9959: train loss: 0.0003020545409526676, val loss: 0.14170792698860168\n",
      "Epoch 9960: train loss: 0.0002482979034539312, val loss: 0.1384894698858261\n",
      "Epoch 9961: train loss: 0.00026908967993222177, val loss: 0.14093467593193054\n",
      "Epoch 9962: train loss: 0.0001587770093465224, val loss: 0.1401996910572052\n",
      "Epoch 9963: train loss: 0.00013215863145887852, val loss: 0.139784574508667\n",
      "Epoch 9964: train loss: 0.00011347510007908568, val loss: 0.1401689201593399\n",
      "Epoch 9965: train loss: 6.52326489216648e-05, val loss: 0.1380547285079956\n",
      "Epoch 9966: train loss: 6.597165338462219e-05, val loss: 0.1366352140903473\n",
      "Epoch 9967: train loss: 8.986533794086426e-05, val loss: 0.1375948190689087\n",
      "Epoch 9968: train loss: 7.676036329939961e-05, val loss: 0.13850899040699005\n",
      "Epoch 9969: train loss: 8.594883547630161e-05, val loss: 0.13606801629066467\n",
      "Epoch 9970: train loss: 8.392416930291802e-05, val loss: 0.13781289756298065\n",
      "Epoch 9971: train loss: 4.7508845455013216e-05, val loss: 0.136897474527359\n",
      "Epoch 9972: train loss: 5.7505760196363553e-05, val loss: 0.1363397091627121\n",
      "Epoch 9973: train loss: 1.6276795577141456e-05, val loss: 0.1365097016096115\n",
      "Epoch 9974: train loss: 3.363002178957686e-05, val loss: 0.1367102712392807\n",
      "Epoch 9975: train loss: 2.3653705284232274e-05, val loss: 0.13730064034461975\n",
      "Epoch 9976: train loss: 2.989651693496853e-05, val loss: 0.13667051494121552\n",
      "Epoch 9977: train loss: 3.5311488318257034e-05, val loss: 0.1374286413192749\n",
      "Epoch 9978: train loss: 3.143242429359816e-05, val loss: 0.13723865151405334\n",
      "Epoch 9979: train loss: 2.7076124752056785e-05, val loss: 0.13708318769931793\n",
      "Epoch 9980: train loss: 2.139220669050701e-05, val loss: 0.1363934576511383\n",
      "Epoch 9981: train loss: 1.3563973880081903e-05, val loss: 0.1367805153131485\n",
      "Epoch 9982: train loss: 1.0458458746143151e-05, val loss: 0.13659659028053284\n",
      "Epoch 9983: train loss: 1.1750380508601665e-05, val loss: 0.13629697263240814\n",
      "Epoch 9984: train loss: 8.555724889447447e-06, val loss: 0.13665573298931122\n",
      "Epoch 9985: train loss: 1.573508052388206e-05, val loss: 0.1362602263689041\n",
      "Epoch 9986: train loss: 1.2575151231430937e-05, val loss: 0.13656066358089447\n",
      "Epoch 9987: train loss: 1.2561826224555261e-05, val loss: 0.13600508868694305\n",
      "Epoch 9988: train loss: 1.2531452739494853e-05, val loss: 0.13602371513843536\n",
      "Epoch 9989: train loss: 5.815698386868462e-06, val loss: 0.13606272637844086\n",
      "Epoch 9990: train loss: 7.2386815190839116e-06, val loss: 0.13653795421123505\n",
      "Epoch 9991: train loss: 4.059719231008785e-06, val loss: 0.1365872621536255\n",
      "Epoch 9992: train loss: 3.2894593005039496e-06, val loss: 0.13606077432632446\n",
      "Epoch 9993: train loss: 5.3262424444255885e-06, val loss: 0.13599565625190735\n",
      "Epoch 9994: train loss: 4.905942205368774e-06, val loss: 0.13613606989383698\n",
      "Epoch 9995: train loss: 5.8151154007646255e-06, val loss: 0.13683485984802246\n",
      "Epoch 9996: train loss: 5.737033916375367e-06, val loss: 0.13620294630527496\n",
      "Epoch 9997: train loss: 6.3460006458626594e-06, val loss: 0.13609129190444946\n",
      "Epoch 9998: train loss: 5.644058092002524e-06, val loss: 0.1363036185503006\n",
      "Epoch 9999: train loss: 6.905612735863542e-06, val loss: 0.1364777684211731\n",
      "Epoch 10000: train loss: 1.0679787010303698e-05, val loss: 0.1361924707889557\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "            \"\"\"\n",
    "            The model class, which defines our feature extractor used in pretraining.\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                \"\"\"\n",
    "                The constructor of the model.\n",
    "                \"\"\"\n",
    "                super().__init__()\n",
    "                # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "                # and then used to extract features from the training and test data.\n",
    "                self.seq = nn.Sequential(\n",
    "                    nn.Linear(500, 100),\n",
    "                    nn.LeakyReLU(0.01),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.Linear(100, 1)\n",
    "                )\n",
    "\n",
    "                for m in self.modules():\n",
    "                    if isinstance(m, nn.Linear):    \n",
    "                        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "            def forward(self, x):\n",
    "                \"\"\"\n",
    "                The forward pass of the model.\n",
    "\n",
    "                input: x: torch.Tensor, the input to the model\n",
    "\n",
    "                output: x: torch.Tensor, the output of the model\n",
    "                \"\"\"\n",
    "                # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "                # defined in the constructor.\n",
    "                x = self.seq(x)\n",
    "                return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=10, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.4, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr = loss.item()\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val = loss.item()\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-8):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "one_model = get_regression_model(featured_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae-1000-500-100-1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-ae-1000-500-100-1.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = one_model(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
