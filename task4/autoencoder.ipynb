{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "#     scaler = StandardScaler()\n",
    "#     x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "#     x_train = scaler.transform(x_train)\n",
    "#     x_test_transed = scaler.transform(x_test)\n",
    "#     x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1030d800957344fcb62462baa68bb175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.04049326893413553, val loss: 0.023283341720700262\n",
      "Epoch 2: train loss: 0.020222239522605528, val loss: 0.018128526717424393\n",
      "Epoch 3: train loss: 0.017175616049949004, val loss: 0.015975482270121574\n",
      "Epoch 4: train loss: 0.014432156973499425, val loss: 0.013527899652719498\n",
      "Epoch 5: train loss: 0.012760671442108495, val loss: 0.012341900207102298\n",
      "Epoch 6: train loss: 0.011686629722921216, val loss: 0.011293500296771525\n",
      "Epoch 7: train loss: 0.010882926372241001, val loss: 0.010569950111210346\n",
      "Epoch 8: train loss: 0.010122431041938919, val loss: 0.009909167237579822\n",
      "Epoch 9: train loss: 0.009508812655447698, val loss: 0.009305797331035138\n",
      "Epoch 10: train loss: 0.008998560582496682, val loss: 0.008889973744750023\n",
      "Epoch 11: train loss: 0.008699497373888688, val loss: 0.008684891782701015\n",
      "Epoch 12: train loss: 0.008271224187953131, val loss: 0.008154441125690937\n",
      "Epoch 13: train loss: 0.007907005592572446, val loss: 0.007867228966206312\n",
      "Epoch 14: train loss: 0.007623113148674673, val loss: 0.007720700703561306\n",
      "Epoch 15: train loss: 0.00738680045276272, val loss: 0.007494148090481758\n",
      "Epoch 16: train loss: 0.007179250511511856, val loss: 0.007387556754052639\n",
      "Epoch 17: train loss: 0.006995812567140983, val loss: 0.007135195463895798\n",
      "Epoch 18: train loss: 0.006815913858766459, val loss: 0.006823674820363522\n",
      "Epoch 19: train loss: 0.0066319068233881675, val loss: 0.006655523743480444\n",
      "Epoch 20: train loss: 0.006460098972810166, val loss: 0.006630007416009903\n",
      "Epoch 21: train loss: 0.006327100559688953, val loss: 0.006397621572017669\n",
      "Epoch 22: train loss: 0.006153104922449102, val loss: 0.006305410664528608\n",
      "Epoch 23: train loss: 0.006039671433139212, val loss: 0.006242908991873264\n",
      "Epoch 24: train loss: 0.005890320198496385, val loss: 0.006051147550344467\n",
      "Epoch 25: train loss: 0.0057721304789337576, val loss: 0.006055135779082775\n",
      "Epoch 26: train loss: 0.005666968264826098, val loss: 0.005864746734499931\n",
      "Epoch 27: train loss: 0.005541547905957821, val loss: 0.005763066679239273\n",
      "Epoch 28: train loss: 0.005447672446999623, val loss: 0.005529767539352179\n",
      "Epoch 29: train loss: 0.0053228534190174264, val loss: 0.0054761538915336135\n",
      "Epoch 30: train loss: 0.005220373636301683, val loss: 0.005387028627097607\n",
      "Epoch 31: train loss: 0.005116992774438493, val loss: 0.005359040334820747\n",
      "Epoch 32: train loss: 0.005043725133930542, val loss: 0.0051714607104659084\n",
      "Epoch 33: train loss: 0.004935650072079532, val loss: 0.005093769319355488\n",
      "Epoch 34: train loss: 0.004865737225784331, val loss: 0.004900462795048952\n",
      "Epoch 35: train loss: 0.004790031747930512, val loss: 0.004979433145374059\n",
      "Epoch 36: train loss: 0.004712583633709927, val loss: 0.004954981852322817\n",
      "Epoch 37: train loss: 0.004630995789504781, val loss: 0.004846510257571936\n",
      "Epoch 38: train loss: 0.004558601203165492, val loss: 0.004850636046379805\n",
      "Epoch 39: train loss: 0.004507934860428985, val loss: 0.004684213440865278\n",
      "Epoch 40: train loss: 0.0044340974170790646, val loss: 0.004665709748864174\n",
      "Epoch 41: train loss: 0.004381913251946775, val loss: 0.004585451368242502\n",
      "Epoch 42: train loss: 0.004340788129945191, val loss: 0.004522313967347145\n",
      "Epoch 43: train loss: 0.0042761188097754305, val loss: 0.004425850450992584\n",
      "Epoch 44: train loss: 0.004217451747886989, val loss: 0.004450922228395939\n",
      "Epoch 45: train loss: 0.004149592601371055, val loss: 0.0043847961649298665\n",
      "Epoch 46: train loss: 0.004099600930907288, val loss: 0.004324792690575123\n",
      "Epoch 47: train loss: 0.004056663542529758, val loss: 0.00417376185208559\n",
      "Epoch 48: train loss: 0.004023217417907958, val loss: 0.004253584455698728\n",
      "Epoch 49: train loss: 0.003964421438927553, val loss: 0.004180867753922939\n",
      "Epoch 50: train loss: 0.003933937463909388, val loss: 0.004101778585463762\n",
      "Epoch 51: train loss: 0.0038958143584278166, val loss: 0.0041726122573018075\n",
      "Epoch 52: train loss: 0.003860600716225347, val loss: 0.004011754672974348\n",
      "Epoch 53: train loss: 0.003819671271026743, val loss: 0.004017515011131763\n",
      "Epoch 54: train loss: 0.003772809400242202, val loss: 0.003913528367877007\n",
      "Epoch 55: train loss: 0.003755974372278671, val loss: 0.003977086823433638\n",
      "Epoch 56: train loss: 0.0037266697000940237, val loss: 0.003958606140688062\n",
      "Epoch 57: train loss: 0.003692804622786994, val loss: 0.003933289214968681\n",
      "Epoch 58: train loss: 0.0036481036752158283, val loss: 0.003840998001396656\n",
      "Epoch 59: train loss: 0.003656583735909389, val loss: 0.003841391932219267\n",
      "Epoch 60: train loss: 0.0036152240233305764, val loss: 0.003861891757696867\n",
      "Epoch 61: train loss: 0.003575390280205376, val loss: 0.0038940452206879853\n",
      "Epoch 62: train loss: 0.0035685173000912277, val loss: 0.0038235019147396087\n",
      "Epoch 63: train loss: 0.00353859127160846, val loss: 0.0038810466155409814\n",
      "Epoch 64: train loss: 0.003506996188692901, val loss: 0.003713737279176712\n",
      "Epoch 65: train loss: 0.0034708493272108693, val loss: 0.0037306653149425982\n",
      "Epoch 66: train loss: 0.0034671051937098405, val loss: 0.003653388451784849\n",
      "Epoch 67: train loss: 0.003436878728927398, val loss: 0.0036899134162813427\n",
      "Epoch 68: train loss: 0.003411983076026853, val loss: 0.003729713138192892\n",
      "Epoch 69: train loss: 0.003418827472900858, val loss: 0.0035554228778928517\n",
      "Epoch 70: train loss: 0.0033836373698194417, val loss: 0.0036178564727306368\n",
      "Epoch 71: train loss: 0.003362255851833188, val loss: 0.0037615130711346864\n",
      "Epoch 72: train loss: 0.0033456471199739954, val loss: 0.0035606970246881246\n",
      "Epoch 73: train loss: 0.003339095236315411, val loss: 0.0034771453887224198\n",
      "Epoch 74: train loss: 0.0033212908396915513, val loss: 0.0034316431488841774\n",
      "Epoch 75: train loss: 0.0033028005519691777, val loss: 0.0034797433968633412\n",
      "Epoch 76: train loss: 0.0032749956248670207, val loss: 0.0035286051724106073\n",
      "Epoch 77: train loss: 0.0032347973238068574, val loss: 0.0035595542285591364\n",
      "Epoch 78: train loss: 0.003225531755889557, val loss: 0.0035492750890552998\n",
      "Epoch 79: train loss: 0.003220192704577835, val loss: 0.003495746048167348\n",
      "Epoch 80: train loss: 0.0032117841131985188, val loss: 0.003418400241062045\n",
      "Epoch 81: train loss: 0.0032247691409928456, val loss: 0.003436060192063451\n",
      "Epoch 82: train loss: 0.0031916283989439206, val loss: 0.0033665763437747956\n",
      "Epoch 83: train loss: 0.003169932895549098, val loss: 0.0033928739670664072\n",
      "Epoch 84: train loss: 0.003158779340967232, val loss: 0.0034624351300299168\n",
      "Epoch 85: train loss: 0.003133585002045242, val loss: 0.003416128760203719\n",
      "Epoch 86: train loss: 0.0031288600286506875, val loss: 0.003397347580641508\n",
      "Epoch 87: train loss: 0.0031044597670587958, val loss: 0.003499112818390131\n",
      "Epoch 88: train loss: 0.0031083368962364537, val loss: 0.0033531475067138674\n",
      "Epoch 89: train loss: 0.0031053836360117612, val loss: 0.0033200735859572885\n",
      "Epoch 90: train loss: 0.003079544334159214, val loss: 0.0033091000765562055\n",
      "Epoch 91: train loss: 0.003088424048177442, val loss: 0.0033412912506610155\n",
      "Epoch 92: train loss: 0.003083295651692517, val loss: 0.00337191691249609\n",
      "Epoch 93: train loss: 0.003049059730365264, val loss: 0.003261841010302305\n",
      "Epoch 94: train loss: 0.0030378594707165447, val loss: 0.003307479403913021\n",
      "Epoch 95: train loss: 0.0030233834838228565, val loss: 0.0034023903384804727\n",
      "Epoch 96: train loss: 0.0030065553970254804, val loss: 0.003218908404931426\n",
      "Epoch 97: train loss: 0.003005911876808624, val loss: 0.003380210539326072\n",
      "Epoch 98: train loss: 0.0030020353800560137, val loss: 0.0032624130267649887\n",
      "Epoch 99: train loss: 0.0029922548125745083, val loss: 0.003259280411526561\n",
      "Epoch 100: train loss: 0.002984950916879639, val loss: 0.003274497825652361\n",
      "Epoch 101: train loss: 0.0029663981413658785, val loss: 0.003148735849186778\n",
      "Epoch 102: train loss: 0.002949572086942439, val loss: 0.0030814332347363234\n",
      "Epoch 103: train loss: 0.0029544586982502012, val loss: 0.0032051453590393065\n",
      "Epoch 104: train loss: 0.0029509619639388153, val loss: 0.0031400997173041106\n",
      "Epoch 105: train loss: 0.002929959001969926, val loss: 0.0032135642431676387\n",
      "Epoch 106: train loss: 0.0029350785058828033, val loss: 0.0032383780088275672\n",
      "Epoch 107: train loss: 0.0029099210633383114, val loss: 0.003279903277754784\n",
      "Epoch 00108: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 108: train loss: 0.0028925928700031067, val loss: 0.0032160943504422903\n",
      "Epoch 109: train loss: 0.002765653003197239, val loss: 0.00305325335636735\n",
      "Epoch 110: train loss: 0.0027154275572725704, val loss: 0.0029700001515448092\n",
      "Epoch 111: train loss: 0.002698569668038767, val loss: 0.0029304028749465943\n",
      "Epoch 112: train loss: 0.0026719152877981564, val loss: 0.002896845869719982\n",
      "Epoch 113: train loss: 0.0026663759358379305, val loss: 0.0029202438686043025\n",
      "Epoch 114: train loss: 0.0026546457565332555, val loss: 0.0029853862673044204\n",
      "Epoch 115: train loss: 0.0026410526571681303, val loss: 0.0029173911418765783\n",
      "Epoch 116: train loss: 0.0026254869776264744, val loss: 0.0029541997089982033\n",
      "Epoch 117: train loss: 0.0026262573114767367, val loss: 0.0029110658075660466\n",
      "Epoch 118: train loss: 0.0026000245231374793, val loss: 0.0028332022354006768\n",
      "Epoch 119: train loss: 0.0026184434604507928, val loss: 0.0028307994026690723\n",
      "Epoch 120: train loss: 0.002604642858804793, val loss: 0.0027753416299819947\n",
      "Epoch 121: train loss: 0.0025995577594683486, val loss: 0.0028359570279717443\n",
      "Epoch 122: train loss: 0.002591526442986666, val loss: 0.0028470237348228694\n",
      "Epoch 123: train loss: 0.002569226729550532, val loss: 0.002870864622294903\n",
      "Epoch 124: train loss: 0.0025690524247760065, val loss: 0.0028104797676205633\n",
      "Epoch 125: train loss: 0.002570482872920681, val loss: 0.002846180893480778\n",
      "Epoch 00126: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 126: train loss: 0.0025733613222457315, val loss: 0.002898325655609369\n",
      "Epoch 127: train loss: 0.002546635574452123, val loss: 0.002752035228535533\n",
      "Epoch 128: train loss: 0.0025317944640635835, val loss: 0.0027665641233325005\n",
      "Epoch 129: train loss: 0.0025291317246702253, val loss: 0.0027569940704852344\n",
      "Epoch 130: train loss: 0.002509638082122012, val loss: 0.002786007948219776\n",
      "Epoch 131: train loss: 0.00251293372841818, val loss: 0.002749808058142662\n",
      "Epoch 132: train loss: 0.0025234318668668977, val loss: 0.0027521934397518634\n",
      "Epoch 133: train loss: 0.002503838435081499, val loss: 0.0027958900034427644\n",
      "Epoch 134: train loss: 0.0025004256830303646, val loss: 0.002770269053056836\n",
      "Epoch 135: train loss: 0.002509068247310969, val loss: 0.002707470912486315\n",
      "Epoch 136: train loss: 0.0024993858454482897, val loss: 0.002746550749987364\n",
      "Epoch 137: train loss: 0.002492218655819187, val loss: 0.002750174066051841\n",
      "Epoch 138: train loss: 0.002497154374169756, val loss: 0.0027032977491617204\n",
      "Epoch 139: train loss: 0.002486591771967253, val loss: 0.002751260906457901\n",
      "Epoch 140: train loss: 0.002480385896084564, val loss: 0.0027721075750887394\n",
      "Epoch 141: train loss: 0.0024910866866002276, val loss: 0.002770323084667325\n",
      "Epoch 142: train loss: 0.002481124698811648, val loss: 0.002784360945224762\n",
      "Epoch 143: train loss: 0.002479904102047487, val loss: 0.002796862153336406\n",
      "Epoch 00144: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 144: train loss: 0.0024781531626928825, val loss: 0.0027613741271197796\n",
      "Epoch 145: train loss: 0.002483233832963267, val loss: 0.002845800654962659\n",
      "Epoch 146: train loss: 0.0024632082930480946, val loss: 0.002792763067409396\n",
      "Epoch 147: train loss: 0.0024512529571217544, val loss: 0.0026991926431655885\n",
      "Epoch 148: train loss: 0.0024563239349090324, val loss: 0.002769920364022255\n",
      "Epoch 149: train loss: 0.002457494745677223, val loss: 0.0027322657462209464\n",
      "Epoch 150: train loss: 0.002452908912787632, val loss: 0.0027169485967606308\n",
      "Epoch 151: train loss: 0.0024601175665703353, val loss: 0.002726625293493271\n",
      "Epoch 152: train loss: 0.0024605726921284685, val loss: 0.002731514310464263\n",
      "Epoch 00153: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 153: train loss: 0.0024511717384186936, val loss: 0.0027946408428251744\n",
      "Epoch 154: train loss: 0.0024618126853768315, val loss: 0.002712633300572634\n",
      "Epoch 155: train loss: 0.002446646525011379, val loss: 0.0028520785700529815\n",
      "Epoch 156: train loss: 0.002446579325183922, val loss: 0.00265052673779428\n",
      "Epoch 157: train loss: 0.0024412912420776426, val loss: 0.002711731432005763\n",
      "Epoch 158: train loss: 0.0024449634207222536, val loss: 0.002760281961411238\n",
      "Epoch 159: train loss: 0.0024475911303366324, val loss: 0.0026410573180764914\n",
      "Epoch 160: train loss: 0.0024612901601940395, val loss: 0.0027161737382411957\n",
      "Epoch 161: train loss: 0.002444708926641211, val loss: 0.002805815977975726\n",
      "Epoch 162: train loss: 0.002452850959890959, val loss: 0.002765750903636217\n",
      "Epoch 163: train loss: 0.0024441049686652057, val loss: 0.002741139868274331\n",
      "Epoch 164: train loss: 0.0024354571013198217, val loss: 0.0027058170698583124\n",
      "Epoch 00165: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 165: train loss: 0.002448381201231054, val loss: 0.002730232145637274\n",
      "Epoch 166: train loss: 0.002445499579821314, val loss: 0.0027493378594517707\n",
      "Epoch 167: train loss: 0.0024502068669060057, val loss: 0.0026785096526145934\n",
      "Epoch 168: train loss: 0.002437926699160313, val loss: 0.002755450127646327\n",
      "Epoch 169: train loss: 0.0024447291016350594, val loss: 0.0027240244690328835\n",
      "Epoch 170: train loss: 0.0024571940849478146, val loss: 0.0027008469961583613\n",
      "Epoch 00171: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 171: train loss: 0.002461714176309048, val loss: 0.002744960557669401\n",
      "Epoch 172: train loss: 0.0024445311264039912, val loss: 0.002640713669359684\n",
      "Epoch 173: train loss: 0.002456621601797488, val loss: 0.002746227264404297\n",
      "Epoch 174: train loss: 0.002465313977694937, val loss: 0.002674128232523799\n",
      "Epoch 175: train loss: 0.0024349314463990076, val loss: 0.00270327727496624\n",
      "Epoch 176: train loss: 0.0024545637311756004, val loss: 0.0026265654135495422\n",
      "Epoch 177: train loss: 0.002454239788598248, val loss: 0.0027322671096771957\n",
      "Epoch 178: train loss: 0.002448564857092439, val loss: 0.0027587288822978734\n",
      "Epoch 179: train loss: 0.002446714495593796, val loss: 0.002752800539135933\n",
      "Epoch 180: train loss: 0.0024446945200693243, val loss: 0.002683275008574128\n",
      "Epoch 181: train loss: 0.0024420356801806054, val loss: 0.002715865759178996\n",
      "Epoch 00182: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 182: train loss: 0.0024396627222624968, val loss: 0.002719343373551965\n",
      "Epoch 183: train loss: 0.0024373490033405168, val loss: 0.002736251790076494\n",
      "Epoch 184: train loss: 0.002450070435827484, val loss: 0.0027313834466040133\n",
      "Epoch 185: train loss: 0.002442346136408801, val loss: 0.0027227403689175844\n",
      "Epoch 186: train loss: 0.002450848974059431, val loss: 0.0027602171041071414\n",
      "Epoch 187: train loss: 0.0024458956763680493, val loss: 0.0026342573780566455\n",
      "Epoch 00188: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 188: train loss: 0.0024438396265768275, val loss: 0.002725616244599223\n",
      "Early stop at epoch 188\n"
     ]
    }
   ],
   "source": [
    "eval_size = 1000\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(ae_model.parameters(), lr=learning_rate)\n",
    "    # optimizer = torch.optim.SGD(ae_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 1000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        \n",
    "        # if(epoch % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1000]) torch.Size([50000, 1000])\n"
     ]
    }
   ],
   "source": [
    "featured_x_train = ae_model.encode(torch.tensor(x_train, dtype=torch.float).to(device))\n",
    "featured_x_pretrain = ae_model.encode(torch.tensor(x_pretrain, dtype=torch.float).to(device))\n",
    "print(featured_x_train.shape, featured_x_pretrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.seq = nn.Sequential(\n",
    "                nn.Linear(1000, 100),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Dropout(0.6),\n",
    "                nn.Linear(100, 1)\n",
    "            )\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):    \n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.seq(x)\n",
    "            return x\n",
    "        \n",
    "        def encode(self, x):\n",
    "            # not use lest layer\n",
    "            x = self.seq[:-4](x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linan\\AppData\\Local\\Temp\\ipykernel_18832\\267134906.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bf3f8fc1364221843088064756040a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.7016918918453917, val loss: 0.2668460011482239\n",
      "Epoch 11: train loss: 0.052476061578916045, val loss: 0.04919317737221718\n",
      "Epoch 00037: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 21: train loss: 0.046659563320023675, val loss: 0.04504149407148361\n",
      "Epoch 00055: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 31: train loss: 0.04546674976604326, val loss: 0.04361594095826149\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 41: train loss: 0.04546062464130168, val loss: 0.04165210947394371\n",
      "Epoch 00087: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 00093: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 00099: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 51: train loss: 0.04578845234671418, val loss: 0.04667235165834427\n",
      "Epoch 00105: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.9683e-07.\n",
      "Epoch 00117: reducing learning rate of group 0 to 5.9049e-08.\n",
      "Epoch 61: train loss: 0.045670673010300616, val loss: 0.04782582446932793\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.7715e-08.\n",
      "Epoch 00129: reducing learning rate of group 0 to 5.3144e-09.\n",
      "Early stop at epoch 65  train loss: 0.04574608755111694, val loss: 0.045891933143138885\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=1000, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "    \n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.4, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze()\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(y)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze()\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(y)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "            \n",
    "        scheduler.step(loss_val)\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-8):\n",
    "            print(f\"Early stop at epoch {epoch+1}  train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "one_model = get_regression_model(featured_x_pretrain, y_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1000])\n"
     ]
    }
   ],
   "source": [
    "featured_x_train = ae_model.encode(torch.tensor(x_train, dtype=torch.float).to(device))\n",
    "# featured_x_train = one_model.encode(featured_x_train).cpu().detach().numpy()\n",
    "print(featured_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linan\\AppData\\Local\\Temp\\ipykernel_18832\\901375131.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tr = torch.tensor(X, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187139f8c47142d298a4d2e521fd5b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 2.7356514620780943\n",
      "Epoch 20: train loss: 0.18647895528003575\n",
      "Epoch 30: train loss: 0.13487567819654941\n",
      "Epoch 00034: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 00040: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 40: train loss: 0.13999808656517415\n",
      "Epoch 50: train loss: 0.1256117253098637\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 00057: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 60: train loss: 0.09152740573859773\n",
      "Epoch 00066: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 70: train loss: 0.1299008280877024\n",
      "Epoch 00072: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 00078: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 80: train loss: 0.11183915830217302\n",
      "Epoch 00084: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.9683e-07.\n",
      "Epoch 90: train loss: 0.1262991946018883\n",
      "Epoch 00096: reducing learning rate of group 0 to 5.9049e-08.\n",
      "Epoch 100: train loss: 0.13074480797629803\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.7715e-08.\n",
      "Epoch 00108: reducing learning rate of group 0 to 5.3144e-09.\n",
      "Epoch 110: train loss: 0.14215186118381098\n",
      "Epoch 120: train loss: 0.14154193052090705\n",
      "Epoch 130: train loss: 0.13175351467449217\n",
      "Epoch 140: train loss: 0.12748441725037993\n",
      "Epoch 150: train loss: 0.12371202787850052\n",
      "Epoch 160: train loss: 0.12609212619718163\n",
      "Epoch 170: train loss: 0.13496029132977128\n",
      "Epoch 180: train loss: 0.1282144912239164\n",
      "Epoch 190: train loss: 0.1371518981299596\n",
      "Epoch 200: train loss: 0.11229465804994106\n",
      "Epoch 210: train loss: 0.1068054423230933\n",
      "Epoch 220: train loss: 0.11283347469754518\n",
      "Epoch 230: train loss: 0.158286940054968\n",
      "Epoch 240: train loss: 0.11422615827992559\n",
      "Epoch 250: train loss: 0.11570424512960016\n",
      "Epoch 260: train loss: 0.12116916573606432\n",
      "Epoch 270: train loss: 0.13104569440707564\n",
      "Epoch 280: train loss: 0.1447554954024963\n",
      "Epoch 290: train loss: 0.15525421394733713\n",
      "Epoch 300: train loss: 0.1352010611910373\n",
      "Epoch 310: train loss: 0.14115116316825152\n",
      "Epoch 320: train loss: 0.11585675266222097\n",
      "Epoch 330: train loss: 0.11769175781868398\n",
      "Epoch 340: train loss: 0.10927527652122081\n",
      "Epoch 350: train loss: 0.1326929045841098\n",
      "Epoch 360: train loss: 0.1224013795913197\n",
      "Epoch 370: train loss: 0.12371279003797099\n",
      "Epoch 380: train loss: 0.15814436156128067\n",
      "Epoch 390: train loss: 0.12589285870315506\n",
      "Epoch 400: train loss: 0.11467041567899287\n",
      "Epoch 410: train loss: 0.13510046659925137\n",
      "Epoch 420: train loss: 0.08472956120036543\n",
      "Epoch 430: train loss: 0.14451030322257793\n",
      "Epoch 440: train loss: 0.11895146671682597\n",
      "Epoch 450: train loss: 0.11543670010054484\n",
      "Epoch 460: train loss: 0.13688557228073478\n",
      "Epoch 470: train loss: 0.13038372654933483\n",
      "Epoch 480: train loss: 0.10929273091780488\n",
      "Epoch 490: train loss: 0.13141724705696106\n",
      "Epoch 500: train loss: 0.12513942278921605\n",
      "Epoch 510: train loss: 0.12066367834806442\n",
      "Epoch 520: train loss: 0.1153372445446439\n",
      "Epoch 530: train loss: 0.12226634255814134\n",
      "Epoch 540: train loss: 0.1234929110086523\n",
      "Epoch 550: train loss: 0.13636814892175608\n",
      "Epoch 560: train loss: 0.12680832505458967\n",
      "Epoch 570: train loss: 0.12364084547385573\n",
      "Epoch 580: train loss: 0.12718310759024462\n",
      "Epoch 590: train loss: 0.14077430807170457\n",
      "Epoch 600: train loss: 0.13850572201190517\n",
      "Epoch 610: train loss: 0.15415183413773775\n",
      "Epoch 620: train loss: 0.12803949234134052\n",
      "Epoch 630: train loss: 0.12999499682802707\n",
      "Epoch 640: train loss: 0.1300942200841382\n",
      "Epoch 650: train loss: 0.10662492590025067\n",
      "Epoch 660: train loss: 0.14414112715050578\n",
      "Epoch 670: train loss: 0.11377555931685493\n",
      "Epoch 680: train loss: 0.11371751287719235\n",
      "Epoch 690: train loss: 0.1358513669995591\n",
      "Epoch 700: train loss: 0.10826360145249055\n",
      "Epoch 710: train loss: 0.14230425366011332\n",
      "Epoch 720: train loss: 0.11333245075889863\n",
      "Epoch 730: train loss: 0.11862095226766542\n",
      "Epoch 740: train loss: 0.13297731921076775\n",
      "Epoch 750: train loss: 0.13826858517753635\n",
      "Epoch 760: train loss: 0.12965253472328186\n",
      "Epoch 770: train loss: 0.12058796518016607\n",
      "Epoch 780: train loss: 0.13385955514386297\n",
      "Epoch 790: train loss: 0.12413001397624612\n",
      "Epoch 800: train loss: 0.13201525799930094\n",
      "Epoch 810: train loss: 0.12430821596179158\n",
      "Epoch 820: train loss: 0.12299072594381869\n",
      "Epoch 830: train loss: 0.13565749097615482\n",
      "Epoch 840: train loss: 0.12138324767351151\n",
      "Epoch 850: train loss: 0.13247205588500946\n",
      "Epoch 860: train loss: 0.12110221160342917\n",
      "Epoch 870: train loss: 0.13922734631225467\n",
      "Epoch 880: train loss: 0.14553827529307456\n",
      "Epoch 890: train loss: 0.12629713071510196\n",
      "Epoch 900: train loss: 0.13484406779054553\n",
      "Epoch 910: train loss: 0.10728401096072047\n",
      "Epoch 920: train loss: 0.15516715337871573\n",
      "Epoch 930: train loss: 0.14555751766078173\n",
      "Epoch 940: train loss: 0.10992994411382824\n",
      "Epoch 950: train loss: 0.12722278021275998\n",
      "Epoch 960: train loss: 0.13545672750973609\n",
      "Epoch 970: train loss: 0.1258149085578043\n",
      "Epoch 980: train loss: 0.1381738083786331\n",
      "Epoch 990: train loss: 0.128077080771327\n",
      "Epoch 1000: train loss: 0.10697751771600451\n",
      "Epoch 1010: train loss: 0.1171523408126086\n",
      "Epoch 1020: train loss: 0.12058125869603828\n",
      "Epoch 1030: train loss: 0.10375087718013674\n",
      "Epoch 1040: train loss: 0.12296115074306727\n",
      "Epoch 1050: train loss: 0.11400713080540299\n",
      "Epoch 1060: train loss: 0.154131312390964\n",
      "Epoch 1070: train loss: 0.12589937288314104\n",
      "Epoch 1080: train loss: 0.1206706223078072\n",
      "Epoch 1090: train loss: 0.13753912969623344\n",
      "Epoch 1100: train loss: 0.1404528587544337\n",
      "Epoch 1110: train loss: 0.1292724185192492\n",
      "Epoch 1120: train loss: 0.11678767339442857\n",
      "Epoch 1130: train loss: 0.14499168241862206\n",
      "Epoch 1140: train loss: 0.1251257831649855\n",
      "Epoch 1150: train loss: 0.13136352178640665\n",
      "Epoch 1160: train loss: 0.12426779860456008\n",
      "Epoch 1170: train loss: 0.12863100809860042\n",
      "Epoch 1180: train loss: 0.12450986397918314\n",
      "Epoch 1190: train loss: 0.11930697923758998\n",
      "Epoch 1200: train loss: 0.1324916490353644\n",
      "Epoch 1210: train loss: 0.13877697693184018\n",
      "Epoch 1220: train loss: 0.1156046647124458\n",
      "Epoch 1230: train loss: 0.10730518939904869\n",
      "Epoch 1240: train loss: 0.11973119206493721\n",
      "Epoch 1250: train loss: 0.1462821129336953\n",
      "Epoch 1260: train loss: 0.14505737130763008\n",
      "Epoch 1270: train loss: 0.10909026530454867\n",
      "Epoch 1280: train loss: 0.0992357324436307\n",
      "Epoch 1290: train loss: 0.12459594643674791\n",
      "Epoch 1300: train loss: 0.12000674926035572\n",
      "Epoch 1310: train loss: 0.11368849765975028\n",
      "Epoch 1320: train loss: 0.13806273862952367\n",
      "Epoch 1330: train loss: 0.12374641255883034\n",
      "Epoch 1340: train loss: 0.11401723624672741\n",
      "Epoch 1350: train loss: 0.1300480978039559\n",
      "Epoch 1360: train loss: 0.1185638630669564\n",
      "Epoch 1370: train loss: 0.14389698705635964\n",
      "Epoch 1380: train loss: 0.13240059692412615\n",
      "Epoch 1390: train loss: 0.1257492537412327\n",
      "Epoch 1400: train loss: 0.13646799965528772\n",
      "Epoch 1410: train loss: 0.11073222025006543\n",
      "Epoch 1420: train loss: 0.14560154207982123\n",
      "Epoch 1430: train loss: 0.14701037063612601\n",
      "Epoch 1440: train loss: 0.11241458817152307\n",
      "Epoch 1450: train loss: 0.1416588585730642\n",
      "Epoch 1460: train loss: 0.12560654254164547\n",
      "Epoch 1470: train loss: 0.13797041992656886\n",
      "Epoch 1480: train loss: 0.14943111732602118\n",
      "Epoch 1490: train loss: 0.12760821483447216\n",
      "Epoch 1500: train loss: 0.13358006438938902\n",
      "Epoch 1510: train loss: 0.1271189951710403\n",
      "Epoch 1520: train loss: 0.12062977339577628\n",
      "Epoch 1530: train loss: 0.1273766130930744\n",
      "Epoch 1540: train loss: 0.13278468242147937\n",
      "Epoch 1550: train loss: 0.12864413681789302\n",
      "Epoch 1560: train loss: 0.1296427910021157\n",
      "Epoch 1570: train loss: 0.12377482294104994\n",
      "Epoch 1580: train loss: 0.11438667928799987\n",
      "Epoch 1590: train loss: 0.13866593879647554\n",
      "Epoch 1600: train loss: 0.137219684649026\n",
      "Epoch 1610: train loss: 0.1339295145869255\n",
      "Epoch 1620: train loss: 0.13133922633714973\n",
      "Epoch 1630: train loss: 0.1292157697456423\n",
      "Epoch 1640: train loss: 0.1295394934155047\n",
      "Epoch 1650: train loss: 0.13140971672488377\n",
      "Epoch 1660: train loss: 0.11469606109661981\n",
      "Epoch 1670: train loss: 0.11931578459509183\n",
      "Epoch 1680: train loss: 0.1352818321227096\n",
      "Epoch 1690: train loss: 0.0896109092165716\n",
      "Epoch 1700: train loss: 0.12117262371815742\n",
      "Epoch 1710: train loss: 0.10332798722200096\n",
      "Epoch 1720: train loss: 0.12226727645844221\n",
      "Epoch 1730: train loss: 0.13380290988832713\n",
      "Epoch 1740: train loss: 0.14382188652700278\n",
      "Epoch 1750: train loss: 0.12149585992097854\n",
      "Epoch 1760: train loss: 0.1309922500140965\n",
      "Epoch 1770: train loss: 0.1452886102022603\n",
      "Epoch 1780: train loss: 0.13488818712532522\n",
      "Epoch 1790: train loss: 0.12043522974592634\n",
      "Epoch 1800: train loss: 0.14752560887485744\n",
      "Epoch 1810: train loss: 0.14582301415503024\n",
      "Epoch 1820: train loss: 0.1385159207507968\n",
      "Epoch 1830: train loss: 0.1235767549148295\n",
      "Epoch 1840: train loss: 0.10968197939218953\n",
      "Epoch 1850: train loss: 0.11077860034871265\n",
      "Epoch 1860: train loss: 0.12804312822408975\n",
      "Epoch 1870: train loss: 0.13149892146451747\n",
      "Epoch 1880: train loss: 0.13992224681627705\n",
      "Epoch 1890: train loss: 0.1405348687316291\n",
      "Epoch 1900: train loss: 0.12895289922016673\n",
      "Epoch 1910: train loss: 0.10527564041782171\n",
      "Epoch 1920: train loss: 0.1223066810425371\n",
      "Epoch 1930: train loss: 0.14525643134955316\n",
      "Epoch 1940: train loss: 0.14225655340822413\n",
      "Epoch 1950: train loss: 0.11912855139002204\n",
      "Epoch 1960: train loss: 0.11906826559454203\n",
      "Epoch 1970: train loss: 0.13361471049371176\n",
      "Epoch 1980: train loss: 0.12713726955233143\n",
      "Epoch 1990: train loss: 0.12807586762588472\n",
      "Epoch 2000: train loss: 0.15092211950570344\n",
      "Epoch 2010: train loss: 0.1250391434156336\n",
      "Epoch 2020: train loss: 0.10378885162994266\n",
      "Epoch 2030: train loss: 0.11537164540728555\n",
      "Epoch 2040: train loss: 0.1495560466288589\n",
      "Epoch 2050: train loss: 0.10549604051746428\n",
      "Epoch 2060: train loss: 0.13136126737110318\n",
      "Epoch 2070: train loss: 0.13231359532102943\n",
      "Epoch 2080: train loss: 0.11301951245695818\n",
      "Epoch 2090: train loss: 0.11367971575818955\n",
      "Epoch 2100: train loss: 0.12434047022950835\n",
      "Epoch 2110: train loss: 0.10725089386687614\n",
      "Epoch 2120: train loss: 0.12632618226110937\n",
      "Epoch 2130: train loss: 0.10531919518951327\n",
      "Epoch 2140: train loss: 0.12757742000278086\n",
      "Epoch 2150: train loss: 0.09687960061361083\n",
      "Epoch 2160: train loss: 0.12564522852189838\n",
      "Epoch 2170: train loss: 0.13316076159477233\n",
      "Epoch 2180: train loss: 0.14442715446464718\n",
      "Epoch 2190: train loss: 0.13178371635265648\n",
      "Epoch 2200: train loss: 0.13987115603871644\n",
      "Epoch 2210: train loss: 0.10222804147051648\n",
      "Epoch 2220: train loss: 0.12929229962523095\n",
      "Epoch 2230: train loss: 0.13646389733883552\n",
      "Epoch 2240: train loss: 0.1117198577011004\n",
      "Epoch 2250: train loss: 0.12199944702908397\n",
      "Epoch 2260: train loss: 0.12503212897572666\n",
      "Epoch 2270: train loss: 0.1302308445284143\n",
      "Epoch 2280: train loss: 0.12711664608214052\n",
      "Epoch 2290: train loss: 0.11912368818302639\n",
      "Epoch 2300: train loss: 0.14098562268540263\n",
      "Epoch 2310: train loss: 0.10514055380139326\n",
      "Epoch 2320: train loss: 0.12059392777271569\n",
      "Epoch 2330: train loss: 0.1498776179959532\n",
      "Epoch 2340: train loss: 0.11674009173642844\n",
      "Epoch 2350: train loss: 0.1327888751588762\n",
      "Epoch 2360: train loss: 0.13070019765174948\n",
      "Epoch 2370: train loss: 0.11262644974980503\n",
      "Epoch 2380: train loss: 0.13260987002402544\n",
      "Epoch 2390: train loss: 0.14096613369882108\n",
      "Epoch 2400: train loss: 0.13505615681409835\n",
      "Epoch 2410: train loss: 0.11988820064812898\n",
      "Epoch 2420: train loss: 0.10659603262145538\n",
      "Epoch 2430: train loss: 0.1273050755506847\n",
      "Epoch 2440: train loss: 0.12336100987973624\n",
      "Epoch 2450: train loss: 0.14230800108984112\n",
      "Epoch 2460: train loss: 0.1307014310872546\n",
      "Epoch 2470: train loss: 0.11561996204312891\n",
      "Epoch 2480: train loss: 0.11157335195573978\n",
      "Epoch 2490: train loss: 0.12777202059980483\n",
      "Epoch 2500: train loss: 0.1276218860805966\n",
      "Epoch 2510: train loss: 0.10846167453273665\n",
      "Epoch 2520: train loss: 0.11420552839554148\n",
      "Epoch 2530: train loss: 0.14893898162525146\n",
      "Epoch 2540: train loss: 0.10904777336632833\n",
      "Epoch 2550: train loss: 0.13558205442037433\n",
      "Epoch 2560: train loss: 0.12891091496683657\n",
      "Epoch 2570: train loss: 0.1505108967667911\n",
      "Epoch 2580: train loss: 0.11531978876329958\n",
      "Epoch 2590: train loss: 0.13511423236457631\n",
      "Epoch 2600: train loss: 0.15584476718679072\n",
      "Epoch 2610: train loss: 0.1464126570895314\n",
      "Epoch 2620: train loss: 0.1192747429665178\n",
      "Epoch 2630: train loss: 0.09911086110398173\n",
      "Epoch 2640: train loss: 0.1222811605699826\n",
      "Epoch 2650: train loss: 0.1249992723017931\n",
      "Epoch 2660: train loss: 0.13640340992249547\n",
      "Epoch 2670: train loss: 0.11390040993690491\n",
      "Epoch 2680: train loss: 0.1245166994642932\n",
      "Epoch 2690: train loss: 0.12806193046271802\n",
      "Epoch 2700: train loss: 0.1402433710359037\n",
      "Epoch 2710: train loss: 0.13115858879638836\n",
      "Epoch 2720: train loss: 0.15776907531311735\n",
      "Epoch 2730: train loss: 0.12192375103652012\n",
      "Epoch 2740: train loss: 0.16791227283189072\n",
      "Epoch 2750: train loss: 0.13137832644046285\n",
      "Epoch 2760: train loss: 0.13360872303601354\n",
      "Epoch 2770: train loss: 0.12476081900997088\n",
      "Epoch 2780: train loss: 0.11368093349970877\n",
      "Epoch 2790: train loss: 0.12298677144572139\n",
      "Epoch 2800: train loss: 0.14030297413468362\n",
      "Epoch 2810: train loss: 0.11420981101226062\n",
      "Epoch 2820: train loss: 0.11619687230559066\n",
      "Epoch 2830: train loss: 0.1366524591567577\n",
      "Epoch 2840: train loss: 0.12171330046374351\n",
      "Epoch 2850: train loss: 0.1361269862158224\n",
      "Epoch 2860: train loss: 0.13861501389648764\n",
      "Epoch 2870: train loss: 0.13728258722927422\n",
      "Epoch 2880: train loss: 0.13117594451643527\n",
      "Epoch 2890: train loss: 0.13977362751960754\n",
      "Epoch 2900: train loss: 0.12439292490656953\n",
      "Epoch 2910: train loss: 0.11854836605489254\n",
      "Epoch 2920: train loss: 0.1172206694772467\n",
      "Epoch 2930: train loss: 0.14504404736682772\n",
      "Epoch 2940: train loss: 0.10476108596660197\n",
      "Epoch 2950: train loss: 0.12962857725098728\n",
      "Epoch 2960: train loss: 0.13534606636269017\n",
      "Epoch 2970: train loss: 0.13989922794979065\n",
      "Epoch 2980: train loss: 0.13149703226983547\n",
      "Epoch 2990: train loss: 0.11607306118501583\n",
      "Epoch 3000: train loss: 0.13601945846807212\n",
      "Epoch 3010: train loss: 0.13269158996175975\n",
      "Epoch 3020: train loss: 0.13466325946181315\n",
      "Epoch 3030: train loss: 0.12776808633934705\n",
      "Epoch 3040: train loss: 0.12515625214204193\n",
      "Epoch 3050: train loss: 0.11318919632816687\n",
      "Epoch 3060: train loss: 0.1337571519613266\n",
      "Epoch 3070: train loss: 0.14702421781490557\n",
      "Epoch 3080: train loss: 0.1193262988445349\n",
      "Epoch 3090: train loss: 0.13468162919627502\n",
      "Epoch 3100: train loss: 0.13630736269056798\n",
      "Epoch 3110: train loss: 0.1181200994597748\n",
      "Epoch 3120: train loss: 0.1212778246216476\n",
      "Epoch 3130: train loss: 0.12630000641802325\n",
      "Epoch 3140: train loss: 0.12424891016446055\n",
      "Epoch 3150: train loss: 0.14531280547496864\n",
      "Epoch 3160: train loss: 0.14096562058548442\n",
      "Epoch 3170: train loss: 0.14288838072679938\n",
      "Epoch 3180: train loss: 0.11326364621985703\n",
      "Epoch 3190: train loss: 0.15515554047655314\n",
      "Epoch 3200: train loss: 0.13825430222786964\n",
      "Epoch 3210: train loss: 0.11704198128543794\n",
      "Epoch 3220: train loss: 0.12213501437450759\n",
      "Epoch 3230: train loss: 0.13742561955703422\n",
      "Epoch 3240: train loss: 0.1371794565487653\n",
      "Epoch 3250: train loss: 0.12784423098899425\n",
      "Epoch 3260: train loss: 0.11727436545770616\n",
      "Epoch 3270: train loss: 0.14087901489343493\n",
      "Epoch 3280: train loss: 0.13347295458661393\n",
      "Epoch 3290: train loss: 0.11574859107844532\n",
      "Epoch 3300: train loss: 0.12763456834480166\n",
      "Epoch 3310: train loss: 0.13233015611767768\n",
      "Epoch 3320: train loss: 0.13096489602699876\n",
      "Epoch 3330: train loss: 0.14111965449992567\n",
      "Epoch 3340: train loss: 0.11223717604763805\n",
      "Epoch 3350: train loss: 0.12782981169642882\n",
      "Epoch 3360: train loss: 0.12482846681494265\n",
      "Epoch 3370: train loss: 0.1304236169345677\n",
      "Epoch 3380: train loss: 0.14686007566284387\n",
      "Epoch 3390: train loss: 0.10959326473996044\n",
      "Epoch 3400: train loss: 0.11863652675703634\n",
      "Epoch 3410: train loss: 0.1400179221108556\n",
      "Epoch 3420: train loss: 0.11331140565685928\n",
      "Epoch 3430: train loss: 0.12276884923921898\n",
      "Epoch 3440: train loss: 0.11236280641052872\n",
      "Epoch 3450: train loss: 0.13651611940469593\n",
      "Epoch 3460: train loss: 0.14016572708031164\n",
      "Epoch 3470: train loss: 0.1235485150385648\n",
      "Epoch 3480: train loss: 0.11646719765849411\n",
      "Epoch 3490: train loss: 0.14299082215875386\n",
      "Epoch 3500: train loss: 0.12185279726283625\n",
      "Epoch 3510: train loss: 0.10624014531262219\n",
      "Epoch 3520: train loss: 0.14283332305960358\n",
      "Epoch 3530: train loss: 0.09860771621199092\n",
      "Epoch 3540: train loss: 0.13072618626058102\n",
      "Epoch 3550: train loss: 0.11874585352954455\n",
      "Epoch 3560: train loss: 0.11648917763959617\n",
      "Epoch 3570: train loss: 0.12246696749702096\n",
      "Epoch 3580: train loss: 0.1326744229043834\n",
      "Epoch 3590: train loss: 0.13108395252842456\n",
      "Epoch 3600: train loss: 0.12139913283288478\n",
      "Epoch 3610: train loss: 0.11986921439878642\n",
      "Epoch 3620: train loss: 0.09757818602083716\n",
      "Epoch 3630: train loss: 0.12131078184582293\n",
      "Epoch 3640: train loss: 0.10893978293868713\n",
      "Epoch 3650: train loss: 0.11705440785270184\n",
      "Epoch 3660: train loss: 0.14103673250734572\n",
      "Epoch 3670: train loss: 0.1049221003241837\n",
      "Epoch 3680: train loss: 0.10561868889257311\n",
      "Epoch 3690: train loss: 0.15248266385868192\n",
      "Epoch 3700: train loss: 0.11424061534926296\n",
      "Epoch 3710: train loss: 0.15229633333627135\n",
      "Epoch 3720: train loss: 0.1328171656676568\n",
      "Epoch 3730: train loss: 0.12116591502912342\n",
      "Epoch 3740: train loss: 0.10038210426457227\n",
      "Epoch 3750: train loss: 0.1345574216172099\n",
      "Epoch 3760: train loss: 0.1305368899484165\n",
      "Epoch 3770: train loss: 0.11264653785619885\n",
      "Epoch 3780: train loss: 0.12683984574861826\n",
      "Epoch 3790: train loss: 0.12299043961800635\n",
      "Epoch 3800: train loss: 0.13231427463237197\n",
      "Epoch 3810: train loss: 0.1534907892718911\n",
      "Epoch 3820: train loss: 0.13882254563272\n",
      "Epoch 3830: train loss: 0.09906493022106588\n",
      "Epoch 3840: train loss: 0.11299167557619512\n",
      "Epoch 3850: train loss: 0.1346058276388794\n",
      "Epoch 3860: train loss: 0.1277787446975708\n",
      "Epoch 3870: train loss: 0.12186767450883053\n",
      "Epoch 3880: train loss: 0.1271750037604943\n",
      "Epoch 3890: train loss: 0.12447065777145326\n",
      "Epoch 3900: train loss: 0.12199841514695436\n",
      "Epoch 3910: train loss: 0.1503900488652289\n",
      "Epoch 3920: train loss: 0.1120085939684941\n",
      "Epoch 3930: train loss: 0.12998234754893928\n",
      "Epoch 3940: train loss: 0.11978819568175822\n",
      "Epoch 3950: train loss: 0.11942152079660445\n",
      "Epoch 3960: train loss: 0.13337032644078137\n",
      "Epoch 3970: train loss: 0.1347691950411536\n",
      "Epoch 3980: train loss: 0.11234678319655358\n",
      "Epoch 3990: train loss: 0.1337616980727762\n",
      "Epoch 4000: train loss: 0.12019331406801939\n",
      "Epoch 4010: train loss: 0.10607200071215629\n",
      "Epoch 4020: train loss: 0.12329944001452532\n",
      "Epoch 4030: train loss: 0.11922199568711221\n",
      "Epoch 4040: train loss: 0.12583390671759845\n",
      "Epoch 4050: train loss: 0.12581963081029243\n",
      "Epoch 4060: train loss: 0.13141350100049748\n",
      "Epoch 4070: train loss: 0.14285092439502478\n",
      "Epoch 4080: train loss: 0.12707496748771518\n",
      "Epoch 4090: train loss: 0.1104435827722773\n",
      "Epoch 4100: train loss: 0.1472701978124678\n",
      "Epoch 4110: train loss: 0.12537953340448438\n",
      "Epoch 4120: train loss: 0.131262474746909\n",
      "Epoch 4130: train loss: 0.11318767565768212\n",
      "Epoch 4140: train loss: 0.11770995078608394\n",
      "Epoch 4150: train loss: 0.11845339685794898\n",
      "Epoch 4160: train loss: 0.10073039595037699\n",
      "Epoch 4170: train loss: 0.12127558047883212\n",
      "Epoch 4180: train loss: 0.1475378849520348\n",
      "Epoch 4190: train loss: 0.12788525651441887\n",
      "Epoch 4200: train loss: 0.12352329600602388\n",
      "Epoch 4210: train loss: 0.11856012352043763\n",
      "Epoch 4220: train loss: 0.13730441368184984\n",
      "Epoch 4230: train loss: 0.11283374232705683\n",
      "Epoch 4240: train loss: 0.12437673861772054\n",
      "Epoch 4250: train loss: 0.12029300812166184\n",
      "Epoch 4260: train loss: 0.12056912271073088\n",
      "Epoch 4270: train loss: 0.1293479037564248\n",
      "Epoch 4280: train loss: 0.1126723180920817\n",
      "Epoch 4290: train loss: 0.1207369068695698\n",
      "Epoch 4300: train loss: 0.1342381931375712\n",
      "Epoch 4310: train loss: 0.14183516407385469\n",
      "Epoch 4320: train loss: 0.11107973791193217\n",
      "Epoch 4330: train loss: 0.12992332182824612\n",
      "Epoch 4340: train loss: 0.13898045435140374\n",
      "Epoch 4350: train loss: 0.12345461939927191\n",
      "Epoch 4360: train loss: 0.1307890885323286\n",
      "Epoch 4370: train loss: 0.11412684150505811\n",
      "Epoch 4380: train loss: 0.14211410951451398\n",
      "Epoch 4390: train loss: 0.10441494668833912\n",
      "Epoch 4400: train loss: 0.12337836945429444\n",
      "Epoch 4410: train loss: 0.11363029836677015\n",
      "Epoch 4420: train loss: 0.12206785426475107\n",
      "Epoch 4430: train loss: 0.14144399013370276\n",
      "Epoch 4440: train loss: 0.12476588767487556\n",
      "Epoch 4450: train loss: 0.1111346104554832\n",
      "Epoch 4460: train loss: 0.1255910619162023\n",
      "Epoch 4470: train loss: 0.12604359079618008\n",
      "Epoch 4480: train loss: 0.12210842821048573\n",
      "Epoch 4490: train loss: 0.13529020412359385\n",
      "Epoch 4500: train loss: 0.11386568865273147\n",
      "Epoch 4510: train loss: 0.13695604127831756\n",
      "Epoch 4520: train loss: 0.1217408514872659\n",
      "Epoch 4530: train loss: 0.14177087073214353\n",
      "Epoch 4540: train loss: 0.13520447459071874\n",
      "Epoch 4550: train loss: 0.10246977697592229\n",
      "Epoch 4560: train loss: 0.12808947413926944\n",
      "Epoch 4570: train loss: 0.12991449553053827\n",
      "Epoch 4580: train loss: 0.15444946320727468\n",
      "Epoch 4590: train loss: 0.13675492235459388\n",
      "Epoch 4600: train loss: 0.11185749888885767\n",
      "Epoch 4610: train loss: 0.1025008044205606\n",
      "Epoch 4620: train loss: 0.13060689259087666\n",
      "Epoch 4630: train loss: 0.14787771912291647\n",
      "Epoch 4640: train loss: 0.13231719192815944\n",
      "Epoch 4650: train loss: 0.11825015619397164\n",
      "Epoch 4660: train loss: 0.09830448647961021\n",
      "Epoch 4670: train loss: 0.13155789657961578\n",
      "Epoch 4680: train loss: 0.13212605792097748\n",
      "Epoch 4690: train loss: 0.1135906758857891\n",
      "Epoch 4700: train loss: 0.12696997478138655\n",
      "Epoch 4710: train loss: 0.13247411146759988\n",
      "Epoch 4720: train loss: 0.12291038308292628\n",
      "Epoch 4730: train loss: 0.12953542921924963\n",
      "Epoch 4740: train loss: 0.1404092665016651\n",
      "Epoch 4750: train loss: 0.10317861150950193\n",
      "Epoch 4760: train loss: 0.1253263850545045\n",
      "Epoch 4770: train loss: 0.11665577423293144\n",
      "Epoch 4780: train loss: 0.1172625606780639\n",
      "Epoch 4790: train loss: 0.12623577972408384\n",
      "Epoch 4800: train loss: 0.11909903699997812\n",
      "Epoch 4810: train loss: 0.11113393649808131\n",
      "Epoch 4820: train loss: 0.121582992519252\n",
      "Epoch 4830: train loss: 0.11225219062296674\n",
      "Epoch 4840: train loss: 0.1442034724354744\n",
      "Epoch 4850: train loss: 0.11977913612499833\n",
      "Epoch 4860: train loss: 0.13390665276907385\n",
      "Epoch 4870: train loss: 0.1511313288542442\n",
      "Epoch 4880: train loss: 0.13768282398814335\n",
      "Epoch 4890: train loss: 0.139596717574168\n",
      "Epoch 4900: train loss: 0.14338492122013122\n",
      "Epoch 4910: train loss: 0.14581541161285713\n",
      "Epoch 4920: train loss: 0.12931967447511852\n",
      "Epoch 4930: train loss: 0.12619960138690658\n",
      "Epoch 4940: train loss: 0.15270528082037343\n",
      "Epoch 4950: train loss: 0.11146756827365607\n",
      "Epoch 4960: train loss: 0.16198483239393682\n",
      "Epoch 4970: train loss: 0.11998599960468709\n",
      "Epoch 4980: train loss: 0.12342040620744228\n",
      "Epoch 4990: train loss: 0.12365497546677943\n",
      "Epoch 5000: train loss: 0.12317360769957304\n",
      "Epoch 5010: train loss: 0.12394619166501798\n",
      "Epoch 5020: train loss: 0.152403379175812\n",
      "Epoch 5030: train loss: 0.15198584398254752\n",
      "Epoch 5040: train loss: 0.1154051473783329\n",
      "Epoch 5050: train loss: 0.11991363595705479\n",
      "Epoch 5060: train loss: 0.1238117545703426\n",
      "Epoch 5070: train loss: 0.12057230610400438\n",
      "Epoch 5080: train loss: 0.14123375912662595\n",
      "Epoch 5090: train loss: 0.14010107765672727\n",
      "Epoch 5100: train loss: 0.14548567285062744\n",
      "Epoch 5110: train loss: 0.10807411114335991\n",
      "Epoch 5120: train loss: 0.09988376235589386\n",
      "Epoch 5130: train loss: 0.1378418782679364\n",
      "Epoch 5140: train loss: 0.132973397327587\n",
      "Epoch 5150: train loss: 0.13492207612842322\n",
      "Epoch 5160: train loss: 0.1015665693115443\n",
      "Epoch 5170: train loss: 0.14399014175753108\n",
      "Epoch 5180: train loss: 0.13408783461898566\n",
      "Epoch 5190: train loss: 0.12276069437852129\n",
      "Epoch 5200: train loss: 0.12285473426803946\n",
      "Epoch 5210: train loss: 0.10564720839262008\n",
      "Epoch 5220: train loss: 0.12617393742781133\n",
      "Epoch 5230: train loss: 0.12881106901564635\n",
      "Epoch 5240: train loss: 0.12438348729745485\n",
      "Epoch 5250: train loss: 0.14088983136229216\n",
      "Epoch 5260: train loss: 0.14188935404643416\n",
      "Epoch 5270: train loss: 0.13282849399372934\n",
      "Epoch 5280: train loss: 0.13473433317732997\n",
      "Epoch 5290: train loss: 0.12862525558448396\n",
      "Epoch 5300: train loss: 0.11520836004288867\n",
      "Epoch 5310: train loss: 0.13384609604254366\n",
      "Epoch 5320: train loss: 0.14556981328641996\n",
      "Epoch 5330: train loss: 0.1378481578733772\n",
      "Epoch 5340: train loss: 0.1250184687273577\n",
      "Epoch 5350: train loss: 0.12352126577869058\n",
      "Epoch 5360: train loss: 0.1308796954760328\n",
      "Epoch 5370: train loss: 0.14752280526794492\n",
      "Epoch 5380: train loss: 0.1123797186370939\n",
      "Epoch 5390: train loss: 0.10878851529210806\n",
      "Epoch 5400: train loss: 0.11709244677360403\n",
      "Epoch 5410: train loss: 0.11590801260375884\n",
      "Epoch 5420: train loss: 0.129482211493887\n",
      "Epoch 5430: train loss: 0.12669282038375967\n",
      "Epoch 5440: train loss: 0.1084471934766043\n",
      "Epoch 5450: train loss: 0.1349510765937157\n",
      "Epoch 5460: train loss: 0.12365091474261135\n",
      "Epoch 5470: train loss: 0.14031476131407544\n",
      "Epoch 5480: train loss: 0.13349140518344937\n",
      "Epoch 5490: train loss: 0.13418123322539033\n",
      "Epoch 5500: train loss: 0.1542326067434624\n",
      "Epoch 5510: train loss: 0.1328204286377877\n",
      "Epoch 5520: train loss: 0.124741748040542\n",
      "Epoch 5530: train loss: 0.11553712157998235\n",
      "Epoch 5540: train loss: 0.10410108340904117\n",
      "Epoch 5550: train loss: 0.11725894319592044\n",
      "Epoch 5560: train loss: 0.140301915705204\n",
      "Epoch 5570: train loss: 0.10049562303349376\n",
      "Epoch 5580: train loss: 0.13620989006944\n",
      "Epoch 5590: train loss: 0.12944677207618951\n",
      "Epoch 5600: train loss: 0.11705964126973413\n",
      "Epoch 5610: train loss: 0.13042558052344247\n",
      "Epoch 5620: train loss: 0.13193051081892918\n",
      "Epoch 5630: train loss: 0.10284653095761315\n",
      "Epoch 5640: train loss: 0.12310990414116532\n",
      "Epoch 5650: train loss: 0.10513397910632193\n",
      "Epoch 5660: train loss: 0.1116175761539489\n",
      "Epoch 5670: train loss: 0.13458315949479582\n",
      "Epoch 5680: train loss: 0.12425994832068682\n",
      "Epoch 5690: train loss: 0.12917250589001925\n",
      "Epoch 5700: train loss: 0.11536432210588828\n",
      "Epoch 5710: train loss: 0.1276546666707145\n",
      "Epoch 5720: train loss: 0.09955767961917444\n",
      "Epoch 5730: train loss: 0.12188214301597328\n",
      "Epoch 5740: train loss: 0.14198163905646652\n",
      "Epoch 5750: train loss: 0.1437371236877516\n",
      "Epoch 5760: train loss: 0.12317819507792592\n",
      "Epoch 5770: train loss: 0.14083303415216505\n",
      "Epoch 5780: train loss: 0.13844781074672938\n",
      "Epoch 5790: train loss: 0.10795515455887653\n",
      "Epoch 5800: train loss: 0.11090078051740421\n",
      "Epoch 5810: train loss: 0.11730985347065143\n",
      "Epoch 5820: train loss: 0.13456229330739006\n",
      "Epoch 5830: train loss: 0.11865109780803323\n",
      "Epoch 5840: train loss: 0.1499356891773641\n",
      "Epoch 5850: train loss: 0.13743740137666463\n",
      "Epoch 5860: train loss: 0.12064257642254234\n",
      "Epoch 5870: train loss: 0.13731235572515288\n",
      "Epoch 5880: train loss: 0.13466184020042418\n",
      "Epoch 5890: train loss: 0.11870171885966556\n",
      "Epoch 5900: train loss: 0.11367070315405726\n",
      "Epoch 5910: train loss: 0.10582676227884803\n",
      "Epoch 5920: train loss: 0.14553738120710477\n",
      "Epoch 5930: train loss: 0.12686353881494142\n",
      "Epoch 5940: train loss: 0.1499262463336345\n",
      "Epoch 5950: train loss: 0.13336575680412352\n",
      "Epoch 5960: train loss: 0.13449626096291467\n",
      "Epoch 5970: train loss: 0.1357548436615616\n",
      "Epoch 5980: train loss: 0.13251129350275734\n",
      "Epoch 5990: train loss: 0.11071458897087723\n",
      "Epoch 6000: train loss: 0.11395864751888439\n",
      "Epoch 6010: train loss: 0.1507673371967394\n",
      "Epoch 6020: train loss: 0.14420218509156257\n",
      "Epoch 6030: train loss: 0.11582097000908106\n",
      "Epoch 6040: train loss: 0.1228299137018621\n",
      "Epoch 6050: train loss: 0.15606806511059404\n",
      "Epoch 6060: train loss: 0.11567522917408496\n",
      "Epoch 6070: train loss: 0.09756673605181276\n",
      "Epoch 6080: train loss: 0.11384871606482193\n",
      "Epoch 6090: train loss: 0.13072316196747125\n",
      "Epoch 6100: train loss: 0.12972337079234422\n",
      "Epoch 6110: train loss: 0.09321363237919286\n",
      "Epoch 6120: train loss: 0.13548748837085442\n",
      "Epoch 6130: train loss: 0.1717147887405008\n",
      "Epoch 6140: train loss: 0.12959117674268783\n",
      "Epoch 6150: train loss: 0.11174597658973653\n",
      "Epoch 6160: train loss: 0.1417797047458589\n",
      "Epoch 6170: train loss: 0.13970194768859073\n",
      "Epoch 6180: train loss: 0.11865542987827211\n",
      "Epoch 6190: train loss: 0.12429000409319997\n",
      "Epoch 6200: train loss: 0.13247863416559993\n",
      "Epoch 6210: train loss: 0.13750468574464322\n",
      "Epoch 6220: train loss: 0.13759965146426112\n",
      "Epoch 6230: train loss: 0.1429282543552108\n",
      "Epoch 6240: train loss: 0.10232310049468651\n",
      "Epoch 6250: train loss: 0.14911029201000928\n",
      "Epoch 6260: train loss: 0.14153390601277352\n",
      "Epoch 6270: train loss: 0.13266856086440384\n",
      "Epoch 6280: train loss: 0.11668121611175593\n",
      "Epoch 6290: train loss: 0.1433621121896431\n",
      "Epoch 6300: train loss: 0.11752383611397818\n",
      "Epoch 6310: train loss: 0.10742369145620614\n",
      "Epoch 6320: train loss: 0.12822358140721918\n",
      "Epoch 6330: train loss: 0.13670688085258006\n",
      "Epoch 6340: train loss: 0.13540147458203136\n",
      "Epoch 6350: train loss: 0.11064979382790625\n",
      "Epoch 6360: train loss: 0.13816975659690797\n",
      "Epoch 6370: train loss: 0.11895122364046984\n",
      "Epoch 6380: train loss: 0.1264413139258977\n",
      "Epoch 6390: train loss: 0.12158618237357587\n",
      "Epoch 6400: train loss: 0.1337722327098163\n",
      "Epoch 6410: train loss: 0.13710758903063833\n",
      "Epoch 6420: train loss: 0.1222201883397065\n",
      "Epoch 6430: train loss: 0.12140538346488029\n",
      "Epoch 6440: train loss: 0.12068288584705442\n",
      "Epoch 6450: train loss: 0.11623987851897255\n",
      "Epoch 6460: train loss: 0.12992407481418922\n",
      "Epoch 6470: train loss: 0.12106142876204103\n",
      "Epoch 6480: train loss: 0.11993398307822645\n",
      "Epoch 6490: train loss: 0.1332708639651537\n",
      "Epoch 6500: train loss: 0.12321570153348148\n",
      "Epoch 6510: train loss: 0.14476226451457477\n",
      "Epoch 6520: train loss: 0.13699554303893818\n",
      "Epoch 6530: train loss: 0.12713564573670738\n",
      "Epoch 6540: train loss: 0.1405181835498661\n",
      "Epoch 6550: train loss: 0.09945891549286898\n",
      "Epoch 6560: train loss: 0.13149326926097274\n",
      "Epoch 6570: train loss: 0.10744408329366706\n",
      "Epoch 6580: train loss: 0.12358643304090947\n",
      "Epoch 6590: train loss: 0.1281272180681117\n",
      "Epoch 6600: train loss: 0.12172429332509636\n",
      "Epoch 6610: train loss: 0.12906040638219565\n",
      "Epoch 6620: train loss: 0.1246667162241647\n",
      "Epoch 6630: train loss: 0.1346262255869806\n",
      "Epoch 6640: train loss: 0.13416399292647838\n",
      "Epoch 6650: train loss: 0.11495020838454366\n",
      "Epoch 6660: train loss: 0.13216677672462537\n",
      "Epoch 6670: train loss: 0.11943345371517353\n",
      "Epoch 6680: train loss: 0.13334447817876935\n",
      "Epoch 6690: train loss: 0.11308245371794329\n",
      "Epoch 6700: train loss: 0.12478173648240044\n",
      "Epoch 6710: train loss: 0.12909192211925982\n",
      "Epoch 6720: train loss: 0.12753635043278336\n",
      "Epoch 6730: train loss: 0.13035463171079756\n",
      "Epoch 6740: train loss: 0.11774765995563939\n",
      "Epoch 6750: train loss: 0.1454113876540214\n",
      "Epoch 6760: train loss: 0.11251411098986865\n",
      "Epoch 6770: train loss: 0.12094264529645443\n",
      "Epoch 6780: train loss: 0.1339247810607776\n",
      "Epoch 6790: train loss: 0.1346646691299975\n",
      "Epoch 6800: train loss: 0.11470069136586972\n",
      "Epoch 6810: train loss: 0.12366644122288563\n",
      "Epoch 6820: train loss: 0.13081483386922627\n",
      "Epoch 6830: train loss: 0.12035561800235882\n",
      "Epoch 6840: train loss: 0.10841553107835353\n",
      "Epoch 6850: train loss: 0.14780026970081964\n",
      "Epoch 6860: train loss: 0.12493959952145815\n",
      "Epoch 6870: train loss: 0.10928243281319737\n",
      "Epoch 6880: train loss: 0.11883335147053004\n",
      "Epoch 6890: train loss: 0.12101930681965314\n",
      "Epoch 6900: train loss: 0.12888330090325326\n",
      "Epoch 6910: train loss: 0.10674316654913127\n",
      "Epoch 6920: train loss: 0.13065888594603167\n",
      "Epoch 6930: train loss: 0.12315911376965233\n",
      "Epoch 6940: train loss: 0.1191543060541153\n",
      "Epoch 6950: train loss: 0.1036904338374734\n",
      "Epoch 6960: train loss: 0.1415529200527817\n",
      "Epoch 6970: train loss: 0.10976717823417857\n",
      "Epoch 6980: train loss: 0.1163739640172571\n",
      "Epoch 6990: train loss: 0.13816877749282866\n",
      "Epoch 7000: train loss: 0.12992717035114765\n",
      "Epoch 7010: train loss: 0.1502037449926138\n",
      "Epoch 7020: train loss: 0.13955668931594117\n",
      "Epoch 7030: train loss: 0.12284661496058107\n",
      "Epoch 7040: train loss: 0.1294772361870855\n",
      "Epoch 7050: train loss: 0.1380537298717536\n",
      "Epoch 7060: train loss: 0.12777301658352372\n",
      "Epoch 7070: train loss: 0.12313211861997843\n",
      "Epoch 7080: train loss: 0.13431930942228065\n",
      "Epoch 7090: train loss: 0.1589076398406178\n",
      "Epoch 7100: train loss: 0.10472571424674243\n",
      "Epoch 7110: train loss: 0.13118032891128678\n",
      "Epoch 7120: train loss: 0.10744214687496424\n",
      "Epoch 7130: train loss: 0.11758843957912177\n",
      "Epoch 7140: train loss: 0.12632458995562046\n",
      "Epoch 7150: train loss: 0.13991336325183512\n",
      "Epoch 7160: train loss: 0.1256255163764581\n",
      "Epoch 7170: train loss: 0.12682616390753537\n",
      "Epoch 7180: train loss: 0.12333998440997675\n",
      "Epoch 7190: train loss: 0.12851734871743248\n",
      "Epoch 7200: train loss: 0.14294671865529382\n",
      "Epoch 7210: train loss: 0.13688503365498036\n",
      "Epoch 7220: train loss: 0.14629285778384657\n",
      "Epoch 7230: train loss: 0.1145419297972694\n",
      "Epoch 7240: train loss: 0.11800717260688544\n",
      "Epoch 7250: train loss: 0.14374936681590042\n",
      "Epoch 7260: train loss: 0.13033233198686503\n",
      "Epoch 7270: train loss: 0.10756637586513534\n",
      "Epoch 7280: train loss: 0.12136246808920986\n",
      "Epoch 7290: train loss: 0.13023587023839356\n",
      "Epoch 7300: train loss: 0.12673417769372464\n",
      "Epoch 7310: train loss: 0.11789032578933983\n",
      "Epoch 7320: train loss: 0.12560754836536944\n",
      "Epoch 7330: train loss: 0.13376858185627497\n",
      "Epoch 7340: train loss: 0.14659140437026508\n",
      "Epoch 7350: train loss: 0.1487918113009073\n",
      "Epoch 7360: train loss: 0.12541895225411281\n",
      "Epoch 7370: train loss: 0.13255344822537155\n",
      "Epoch 7380: train loss: 0.12777031057514251\n",
      "Epoch 7390: train loss: 0.14194682667963207\n",
      "Epoch 7400: train loss: 0.13091014281148092\n",
      "Epoch 7410: train loss: 0.12781776321819052\n",
      "Epoch 7420: train loss: 0.1284913856931962\n",
      "Epoch 7430: train loss: 0.14038539313711226\n",
      "Epoch 7440: train loss: 0.12495301416376606\n",
      "Epoch 7450: train loss: 0.13401469305856153\n",
      "Epoch 7460: train loss: 0.1278751602419652\n",
      "Epoch 7470: train loss: 0.11497101343935355\n",
      "Epoch 7480: train loss: 0.12976586101343857\n",
      "Epoch 7490: train loss: 0.14232700062682851\n",
      "Epoch 7500: train loss: 0.12108909512404352\n",
      "Epoch 7510: train loss: 0.1392119908099994\n",
      "Epoch 7520: train loss: 0.1345068035647273\n",
      "Epoch 7530: train loss: 0.1123034485115204\n",
      "Epoch 7540: train loss: 0.14978067709133028\n",
      "Epoch 7550: train loss: 0.15602185775991528\n",
      "Epoch 7560: train loss: 0.13821842145174743\n",
      "Epoch 7570: train loss: 0.1358478049794212\n",
      "Epoch 7580: train loss: 0.13170666057150812\n",
      "Epoch 7590: train loss: 0.12112677880330011\n",
      "Epoch 7600: train loss: 0.12307145918020979\n",
      "Epoch 7610: train loss: 0.11705436640855624\n",
      "Epoch 7620: train loss: 0.1142287354823202\n",
      "Epoch 7630: train loss: 0.11748180150985718\n",
      "Epoch 7640: train loss: 0.147782798219705\n",
      "Epoch 7650: train loss: 0.1411819873645436\n",
      "Epoch 7660: train loss: 0.1308201994560659\n",
      "Epoch 7670: train loss: 0.12499933955143205\n",
      "Epoch 7680: train loss: 0.13288806591648608\n",
      "Epoch 7690: train loss: 0.10939156719279708\n",
      "Epoch 7700: train loss: 0.1225446536875097\n",
      "Epoch 7710: train loss: 0.12655543953413143\n",
      "Epoch 7720: train loss: 0.11281503790523857\n",
      "Epoch 7730: train loss: 0.11553603865206241\n",
      "Epoch 7740: train loss: 0.13813209984218702\n",
      "Epoch 7750: train loss: 0.10585421660915018\n",
      "Epoch 7760: train loss: 0.10686216783768032\n",
      "Epoch 7770: train loss: 0.14460693386849016\n",
      "Epoch 7780: train loss: 0.10585802652523853\n",
      "Epoch 7790: train loss: 0.1412295668805018\n",
      "Epoch 7800: train loss: 0.090909239014145\n",
      "Epoch 7810: train loss: 0.10193306309636682\n",
      "Epoch 7820: train loss: 0.1281651571765542\n",
      "Epoch 7830: train loss: 0.11339690664201044\n",
      "Epoch 7840: train loss: 0.13610299537423998\n",
      "Epoch 7850: train loss: 0.1365530329151079\n",
      "Epoch 7860: train loss: 0.12305396290495992\n",
      "Epoch 7870: train loss: 0.1431838095653802\n",
      "Epoch 7880: train loss: 0.14643459424376487\n",
      "Epoch 7890: train loss: 0.12036642616672907\n",
      "Epoch 7900: train loss: 0.1346384057402611\n",
      "Epoch 7910: train loss: 0.11689630933571607\n",
      "Epoch 7920: train loss: 0.13825074668042361\n",
      "Epoch 7930: train loss: 0.1284444516338408\n",
      "Epoch 7940: train loss: 0.12747875385510268\n",
      "Epoch 7950: train loss: 0.12650301516674517\n",
      "Epoch 7960: train loss: 0.13310471897479148\n",
      "Epoch 7970: train loss: 0.1333935552276671\n",
      "Epoch 7980: train loss: 0.12462608657777309\n",
      "Epoch 7990: train loss: 0.14671487269457428\n",
      "Epoch 8000: train loss: 0.10655285100452602\n",
      "Epoch 8010: train loss: 0.10481488440185785\n",
      "Epoch 8020: train loss: 0.12055279487743974\n",
      "Epoch 8030: train loss: 0.1154730436950922\n",
      "Epoch 8040: train loss: 0.13300886333570816\n",
      "Epoch 8050: train loss: 0.14121004262706266\n",
      "Epoch 8060: train loss: 0.13228615263476967\n",
      "Epoch 8070: train loss: 0.12525847893673928\n",
      "Epoch 8080: train loss: 0.1161297821183689\n",
      "Epoch 8090: train loss: 0.11753527732333169\n",
      "Epoch 8100: train loss: 0.12988543038722128\n",
      "Epoch 8110: train loss: 0.11589051160612143\n",
      "Epoch 8120: train loss: 0.126897757796105\n",
      "Epoch 8130: train loss: 0.1097598862182349\n",
      "Epoch 8140: train loss: 0.10700576925417409\n",
      "Epoch 8150: train loss: 0.10710765387164428\n",
      "Epoch 8160: train loss: 0.1251744605624117\n",
      "Epoch 8170: train loss: 0.11705759985779877\n",
      "Epoch 8180: train loss: 0.10034186432138086\n",
      "Epoch 8190: train loss: 0.10239587521413342\n",
      "Epoch 8200: train loss: 0.13382012410555036\n",
      "Epoch 8210: train loss: 0.12014453335665166\n",
      "Epoch 8220: train loss: 0.13910281975753605\n",
      "Epoch 8230: train loss: 0.1112757819565013\n",
      "Epoch 8240: train loss: 0.12995580635790246\n",
      "Epoch 8250: train loss: 0.1146955633093603\n",
      "Epoch 8260: train loss: 0.1178033147379756\n",
      "Epoch 8270: train loss: 0.11162240725476295\n",
      "Epoch 8280: train loss: 0.15341813787352293\n",
      "Epoch 8290: train loss: 0.15240499794483184\n",
      "Epoch 8300: train loss: 0.1236863120389171\n",
      "Epoch 8310: train loss: 0.10906308216217439\n",
      "Epoch 8320: train loss: 0.14677583553013393\n",
      "Epoch 8330: train loss: 0.12109854656166136\n",
      "Epoch 8340: train loss: 0.09523651762865484\n",
      "Epoch 8350: train loss: 0.10573899927083402\n",
      "Epoch 8360: train loss: 0.14579395788256078\n",
      "Epoch 8370: train loss: 0.12974043981172145\n",
      "Epoch 8380: train loss: 0.1521439252840355\n",
      "Epoch 8390: train loss: 0.11866825237753802\n",
      "Epoch 8400: train loss: 0.12338311013299971\n",
      "Epoch 8410: train loss: 0.1597146132029593\n",
      "Epoch 8420: train loss: 0.12389766091480851\n",
      "Epoch 8430: train loss: 0.11639492374408292\n",
      "Epoch 8440: train loss: 0.11163145917773364\n",
      "Epoch 8450: train loss: 0.13411694544833153\n",
      "Epoch 8460: train loss: 0.11964612731942907\n",
      "Epoch 8470: train loss: 0.12219734997721389\n",
      "Epoch 8480: train loss: 0.108086226885207\n",
      "Epoch 8490: train loss: 0.13154300658497958\n",
      "Epoch 8500: train loss: 0.13017290318617597\n",
      "Epoch 8510: train loss: 0.11010728911845945\n",
      "Epoch 8520: train loss: 0.1199216775665991\n",
      "Epoch 8530: train loss: 0.11377174533903599\n",
      "Epoch 8540: train loss: 0.11439103979151696\n",
      "Epoch 8550: train loss: 0.12176673973663128\n",
      "Epoch 8560: train loss: 0.11881769153056666\n",
      "Epoch 8570: train loss: 0.12669423092156648\n",
      "Epoch 8580: train loss: 0.1304794739524368\n",
      "Epoch 8590: train loss: 0.128018148036208\n",
      "Epoch 8600: train loss: 0.13370682997163386\n",
      "Epoch 8610: train loss: 0.11767260811291635\n",
      "Epoch 8620: train loss: 0.11741035622544586\n",
      "Epoch 8630: train loss: 0.1475490765599534\n",
      "Epoch 8640: train loss: 0.11447866685863119\n",
      "Epoch 8650: train loss: 0.16666173581033944\n",
      "Epoch 8660: train loss: 0.13278053515357896\n",
      "Epoch 8670: train loss: 0.146013177887653\n",
      "Epoch 8680: train loss: 0.11556146833114327\n",
      "Epoch 8690: train loss: 0.13931914370507\n",
      "Epoch 8700: train loss: 0.13035047645680606\n",
      "Epoch 8710: train loss: 0.12484331987798214\n",
      "Epoch 8720: train loss: 0.14680041106796124\n",
      "Epoch 8730: train loss: 0.11202194252051413\n",
      "Epoch 8740: train loss: 0.12999737197184003\n",
      "Epoch 8750: train loss: 0.12488523511216044\n",
      "Epoch 8760: train loss: 0.1221605286654085\n",
      "Epoch 8770: train loss: 0.10456181939691306\n",
      "Epoch 8780: train loss: 0.12324472512118519\n",
      "Epoch 8790: train loss: 0.11686046811752021\n",
      "Epoch 8800: train loss: 0.13607363210059703\n",
      "Epoch 8810: train loss: 0.13657956949435174\n",
      "Epoch 8820: train loss: 0.12667336548678576\n",
      "Epoch 8830: train loss: 0.12126423868350684\n",
      "Epoch 8840: train loss: 0.12297458865796215\n",
      "Epoch 8850: train loss: 0.13306075476109983\n",
      "Epoch 8860: train loss: 0.10133338002720847\n",
      "Epoch 8870: train loss: 0.10944740778766572\n",
      "Epoch 8880: train loss: 0.1084171073930338\n",
      "Epoch 8890: train loss: 0.10311257015098818\n",
      "Epoch 8900: train loss: 0.10672853986732661\n",
      "Epoch 8910: train loss: 0.13496905760955996\n",
      "Epoch 8920: train loss: 0.1330213800747879\n",
      "Epoch 8930: train loss: 0.12246900491416454\n",
      "Epoch 8940: train loss: 0.14948318838141858\n",
      "Epoch 8950: train loss: 0.11020457223392441\n",
      "Epoch 8960: train loss: 0.11622945984709077\n",
      "Epoch 8970: train loss: 0.1435885231755674\n",
      "Epoch 8980: train loss: 0.11492384167853743\n",
      "Epoch 8990: train loss: 0.11856928687178879\n",
      "Epoch 9000: train loss: 0.12410618001886178\n",
      "Epoch 9010: train loss: 0.12167407953180373\n",
      "Epoch 9020: train loss: 0.12423356419894845\n",
      "Epoch 9030: train loss: 0.12570315051823855\n",
      "Epoch 9040: train loss: 0.13607837577466853\n",
      "Epoch 9050: train loss: 0.13091815999534448\n",
      "Epoch 9060: train loss: 0.13630203865701332\n",
      "Epoch 9070: train loss: 0.12932588120922447\n",
      "Epoch 9080: train loss: 0.1082947824965231\n",
      "Epoch 9090: train loss: 0.13435309667605908\n",
      "Epoch 9100: train loss: 0.1173006759909913\n",
      "Epoch 9110: train loss: 0.13614642439875752\n",
      "Epoch 9120: train loss: 0.12928808110067622\n",
      "Epoch 9130: train loss: 0.1285292925219983\n",
      "Epoch 9140: train loss: 0.11732897733803839\n",
      "Epoch 9150: train loss: 0.12495525473263115\n",
      "Epoch 9160: train loss: 0.12766447565634734\n",
      "Epoch 9170: train loss: 0.11650291951373219\n",
      "Epoch 9180: train loss: 0.1543103663623333\n",
      "Epoch 9190: train loss: 0.10083823696128093\n",
      "Epoch 9200: train loss: 0.10972420633072033\n",
      "Epoch 9210: train loss: 0.1322858162643388\n",
      "Epoch 9220: train loss: 0.11257995804422535\n",
      "Epoch 9230: train loss: 0.13521479880902917\n",
      "Epoch 9240: train loss: 0.12248865811270662\n",
      "Epoch 9250: train loss: 0.12782954456983134\n",
      "Epoch 9260: train loss: 0.10695266983006149\n",
      "Epoch 9270: train loss: 0.13165046186652035\n",
      "Epoch 9280: train loss: 0.12823159261140973\n",
      "Epoch 9290: train loss: 0.1212202206812799\n",
      "Epoch 9300: train loss: 0.10893254519323818\n",
      "Epoch 9310: train loss: 0.10597161842044443\n",
      "Epoch 9320: train loss: 0.123392368638888\n",
      "Epoch 9330: train loss: 0.1219388675247319\n",
      "Epoch 9340: train loss: 0.1274485962279141\n",
      "Epoch 9350: train loss: 0.14766314272768796\n",
      "Epoch 9360: train loss: 0.1268029558681883\n",
      "Epoch 9370: train loss: 0.13936367240734399\n",
      "Epoch 9380: train loss: 0.13200835431460292\n",
      "Epoch 9390: train loss: 0.12219508544250857\n",
      "Epoch 9400: train loss: 0.1324607672356069\n",
      "Epoch 9410: train loss: 0.11698547098785639\n",
      "Epoch 9420: train loss: 0.12645446546375752\n",
      "Epoch 9430: train loss: 0.10982189394067973\n",
      "Epoch 9440: train loss: 0.15006693275645375\n",
      "Epoch 9450: train loss: 0.1326459217071533\n",
      "Epoch 9460: train loss: 0.12730751488823444\n",
      "Epoch 9470: train loss: 0.12427776963915677\n",
      "Epoch 9480: train loss: 0.13969110735110007\n",
      "Epoch 9490: train loss: 0.12072836646286306\n",
      "Epoch 9500: train loss: 0.12762908946489915\n",
      "Epoch 9510: train loss: 0.11732899209368043\n",
      "Epoch 9520: train loss: 0.14314188783057033\n",
      "Epoch 9530: train loss: 0.13631978924619034\n",
      "Epoch 9540: train loss: 0.1270468739385251\n",
      "Epoch 9550: train loss: 0.12195382415258792\n",
      "Epoch 9560: train loss: 0.1295808752346784\n",
      "Epoch 9570: train loss: 0.12559488477185368\n",
      "Epoch 9580: train loss: 0.12326640488114209\n",
      "Epoch 9590: train loss: 0.10457996163517237\n",
      "Epoch 9600: train loss: 0.10189290427137167\n",
      "Epoch 9610: train loss: 0.11613050456624478\n",
      "Epoch 9620: train loss: 0.11517788492841646\n",
      "Epoch 9630: train loss: 0.1148609160585329\n",
      "Epoch 9640: train loss: 0.11899615587084554\n",
      "Epoch 9650: train loss: 0.14519484772346913\n",
      "Epoch 9660: train loss: 0.1277854007575661\n",
      "Epoch 9670: train loss: 0.13297636731760576\n",
      "Epoch 9680: train loss: 0.13754696917720138\n",
      "Epoch 9690: train loss: 0.12978218856966123\n",
      "Epoch 9700: train loss: 0.1220737132942304\n",
      "Epoch 9710: train loss: 0.13773385955020787\n",
      "Epoch 9720: train loss: 0.135705718039535\n",
      "Epoch 9730: train loss: 0.08731611607596278\n",
      "Epoch 9740: train loss: 0.12643367901444436\n",
      "Epoch 9750: train loss: 0.1406378343841061\n",
      "Epoch 9760: train loss: 0.13057370241731406\n",
      "Epoch 9770: train loss: 0.13717905358411372\n",
      "Epoch 9780: train loss: 0.1296520335879177\n",
      "Epoch 9790: train loss: 0.1456467426859308\n",
      "Epoch 9800: train loss: 0.12464862221619115\n",
      "Epoch 9810: train loss: 0.1327604628505651\n",
      "Epoch 9820: train loss: 0.1305569123942405\n",
      "Epoch 9830: train loss: 0.12760720307880546\n",
      "Epoch 9840: train loss: 0.1413560150261037\n",
      "Epoch 9850: train loss: 0.13187895839568228\n",
      "Epoch 9860: train loss: 0.1072863208828494\n",
      "Epoch 9870: train loss: 0.12782777911052107\n",
      "Epoch 9880: train loss: 0.14749872367596253\n",
      "Epoch 9890: train loss: 0.12643304518889636\n",
      "Epoch 9900: train loss: 0.13672756851650775\n",
      "Epoch 9910: train loss: 0.1637940936302766\n",
      "Epoch 9920: train loss: 0.15749960945453495\n",
      "Epoch 9930: train loss: 0.12266977186547592\n",
      "Epoch 9940: train loss: 0.15764812717679888\n",
      "Epoch 9950: train loss: 0.13527122305007652\n",
      "Epoch 9960: train loss: 0.12058634291111957\n",
      "Epoch 9970: train loss: 0.1269446531147696\n",
      "Epoch 9980: train loss: 0.1331529642920941\n",
      "Epoch 9990: train loss: 0.15633170011918993\n",
      "Epoch 10000: train loss: 0.11271763848140835\n"
     ]
    }
   ],
   "source": [
    "def finetune(old_model, X, y, tune_layers=1):\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    model.load_state_dict(old_model.state_dict())\n",
    "    \n",
    "    # frozen layers and not frozen last n layers\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        if i < len(list(model.parameters())) - tune_layers:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "   \n",
    "    x_tr = torch.tensor(X, dtype=torch.float)\n",
    "    y_tr = torch.tensor(y, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze()\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(y)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        scheduler.step(loss_tr)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss_tr}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-9):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss_tr}\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "finetune_model = finetune(one_model, featured_x_train, y_train, tune_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae-1000-100-fintune-1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1000])\n",
      "Predictions saved to results-ae-1000-100-fintune-1.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "featured_x_test = ae_model.encode(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))\n",
    "print(featured_x_test.shape)\n",
    "y_pred = finetune_model(featured_x_test).squeeze(-1).cpu().detach().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
