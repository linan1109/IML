{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 1000)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb0553610f54d9a90a4b6afd8fb5111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.4051969674863445, val loss: 1.1628666818141937\n",
      "Epoch 2: train loss: 0.991415945932936, val loss: 1.0502719581127167\n",
      "Epoch 3: train loss: 0.9317291708445868, val loss: 1.0165606439113617\n",
      "Epoch 4: train loss: 0.9062194476054408, val loss: 0.9983953535556793\n",
      "Epoch 5: train loss: 0.8856553137789206, val loss: 0.9777085185050964\n",
      "Epoch 6: train loss: 0.8663969801283086, val loss: 0.958357185125351\n",
      "Epoch 7: train loss: 0.8476379026349407, val loss: 0.9405882358551025\n",
      "Epoch 8: train loss: 0.8293201430332268, val loss: 0.9255602359771729\n",
      "Epoch 9: train loss: 0.8117932967016488, val loss: 0.9102109670639038\n",
      "Epoch 10: train loss: 0.7948995638384379, val loss: 0.8917213380336761\n",
      "Epoch 11: train loss: 0.778454465887119, val loss: 0.8790036141872406\n",
      "Epoch 12: train loss: 0.7625459319078545, val loss: 0.8614891320466995\n",
      "Epoch 13: train loss: 0.7477716118126514, val loss: 0.8473160117864609\n",
      "Epoch 14: train loss: 0.7332883733076739, val loss: 0.8343290388584137\n",
      "Epoch 15: train loss: 0.7194974375916088, val loss: 0.8206227719783783\n",
      "Epoch 16: train loss: 0.7062625308240874, val loss: 0.8097594529390335\n",
      "Epoch 17: train loss: 0.6936893797473789, val loss: 0.7976999431848526\n",
      "Epoch 18: train loss: 0.68164547710238, val loss: 0.7856031656265259\n",
      "Epoch 19: train loss: 0.6699385935284273, val loss: 0.7747862488031387\n",
      "Epoch 20: train loss: 0.6587153842775544, val loss: 0.7648332267999649\n",
      "Epoch 21: train loss: 0.6485170124208799, val loss: 0.7553848326206207\n",
      "Epoch 22: train loss: 0.6380107251112463, val loss: 0.7474150657653809\n",
      "Epoch 23: train loss: 0.6285055383908441, val loss: 0.7350076884031296\n",
      "Epoch 24: train loss: 0.6192679855606206, val loss: 0.7262491136789322\n",
      "Epoch 25: train loss: 0.6101640008002939, val loss: 0.721275195479393\n",
      "Epoch 26: train loss: 0.6015779041496222, val loss: 0.7110760360956192\n",
      "Epoch 27: train loss: 0.5930963005555373, val loss: 0.7023608386516571\n",
      "Epoch 28: train loss: 0.5850990538268415, val loss: 0.6941415071487427\n",
      "Epoch 29: train loss: 0.5776186805463078, val loss: 0.6869513094425201\n",
      "Epoch 30: train loss: 0.5701760309110939, val loss: 0.68044713139534\n",
      "Epoch 31: train loss: 0.5628138240238141, val loss: 0.6718849539756775\n",
      "Epoch 32: train loss: 0.5557625430621808, val loss: 0.6669540554285049\n",
      "Epoch 33: train loss: 0.5491699898838802, val loss: 0.6618811786174774\n",
      "Epoch 34: train loss: 0.542888158464385, val loss: 0.6523930132389069\n",
      "Epoch 35: train loss: 0.5360790190257422, val loss: 0.6469870954751968\n",
      "Epoch 36: train loss: 0.5302253939948881, val loss: 0.6417636424303055\n",
      "Epoch 37: train loss: 0.5241879970760215, val loss: 0.6350894123315811\n",
      "Epoch 38: train loss: 0.5181935099615298, val loss: 0.6317606866359711\n",
      "Epoch 39: train loss: 0.5124875281866529, val loss: 0.6263820677995682\n",
      "Epoch 40: train loss: 0.5069415680686854, val loss: 0.6198993921279907\n",
      "Epoch 41: train loss: 0.5016660959998676, val loss: 0.614718534052372\n",
      "Epoch 42: train loss: 0.49627903825276354, val loss: 0.6090531647205353\n",
      "Epoch 43: train loss: 0.49134455181655834, val loss: 0.6047410666942596\n",
      "Epoch 44: train loss: 0.48635951807124284, val loss: 0.6006042510271072\n",
      "Epoch 45: train loss: 0.481686077060282, val loss: 0.5951884165406227\n",
      "Epoch 46: train loss: 0.4766460915516888, val loss: 0.5904167294502258\n",
      "Epoch 47: train loss: 0.4721093137682516, val loss: 0.5856974646449089\n",
      "Epoch 48: train loss: 0.46810461881113224, val loss: 0.5802512019872665\n",
      "Epoch 49: train loss: 0.4633417837820735, val loss: 0.5762521922588348\n",
      "Epoch 50: train loss: 0.4590228656194289, val loss: 0.5744306147098541\n",
      "Epoch 51: train loss: 0.45460214334662075, val loss: 0.5695426613092422\n",
      "Epoch 52: train loss: 0.4506331896984433, val loss: 0.5635979026556015\n",
      "Epoch 53: train loss: 0.4467105861389337, val loss: 0.5596641302108765\n",
      "Epoch 54: train loss: 0.44257716095085353, val loss: 0.5572041943669319\n",
      "Epoch 55: train loss: 0.43901224798020566, val loss: 0.552384153008461\n",
      "Epoch 56: train loss: 0.43468905257578966, val loss: 0.5505364462733269\n",
      "Epoch 57: train loss: 0.43149829348953905, val loss: 0.5462630838155746\n",
      "Epoch 58: train loss: 0.42766119965693794, val loss: 0.5404011979699135\n",
      "Epoch 59: train loss: 0.42427498207338415, val loss: 0.5386788323521614\n",
      "Epoch 60: train loss: 0.4204492647929816, val loss: 0.5343946740031242\n",
      "Epoch 61: train loss: 0.4170650598933517, val loss: 0.5326199755072594\n",
      "Epoch 62: train loss: 0.4138158899979901, val loss: 0.5276975184679031\n",
      "Epoch 63: train loss: 0.4107589051054413, val loss: 0.5234918594360352\n",
      "Epoch 64: train loss: 0.40703792285389545, val loss: 0.52194494754076\n",
      "Epoch 65: train loss: 0.40386693915172717, val loss: 0.518105685710907\n",
      "Epoch 66: train loss: 0.400822603188477, val loss: 0.5151756256818771\n",
      "Epoch 67: train loss: 0.397460836360985, val loss: 0.5137119218707085\n",
      "Epoch 68: train loss: 0.3950511253238386, val loss: 0.5093236863613129\n",
      "Epoch 69: train loss: 0.3912889581905086, val loss: 0.5047490447759628\n",
      "Epoch 70: train loss: 0.38848688097336603, val loss: 0.5040207430720329\n",
      "Epoch 71: train loss: 0.38572441532657803, val loss: 0.49928679317235947\n",
      "Epoch 72: train loss: 0.382687829388625, val loss: 0.49546103179454803\n",
      "Epoch 73: train loss: 0.379999096352304, val loss: 0.49544519931077957\n",
      "Epoch 74: train loss: 0.37705636330189873, val loss: 0.4906744956970215\n",
      "Epoch 75: train loss: 0.3745622959587941, val loss: 0.48999327421188354\n",
      "Epoch 76: train loss: 0.3715975276398059, val loss: 0.48714040219783783\n",
      "Epoch 77: train loss: 0.36912017061756314, val loss: 0.4831104725599289\n",
      "Epoch 78: train loss: 0.36659546327411796, val loss: 0.4812120795249939\n",
      "Epoch 79: train loss: 0.3637096136857076, val loss: 0.4783227890729904\n",
      "Epoch 80: train loss: 0.36113493249175677, val loss: 0.47571758180856705\n",
      "Epoch 81: train loss: 0.35887969144805903, val loss: 0.4734485223889351\n",
      "Epoch 82: train loss: 0.3560013169753166, val loss: 0.4707479178905487\n",
      "Epoch 83: train loss: 0.3534971604702403, val loss: 0.46869242936372757\n",
      "Epoch 84: train loss: 0.35128006508482007, val loss: 0.4654078856110573\n",
      "Epoch 85: train loss: 0.34893351125390026, val loss: 0.4629553183913231\n",
      "Epoch 86: train loss: 0.3466314346581571, val loss: 0.4602445289492607\n",
      "Epoch 87: train loss: 0.34405968898809336, val loss: 0.4589751362800598\n",
      "Epoch 88: train loss: 0.34222567085345873, val loss: 0.4548320472240448\n",
      "Epoch 89: train loss: 0.3398937999092259, val loss: 0.45491479337215424\n",
      "Epoch 90: train loss: 0.33774620731723576, val loss: 0.45112618803977966\n",
      "Epoch 91: train loss: 0.33518480189444, val loss: 0.450051911175251\n",
      "Epoch 92: train loss: 0.3328155705213002, val loss: 0.44673532247543335\n",
      "Epoch 93: train loss: 0.33076851571825194, val loss: 0.445116326212883\n",
      "Epoch 94: train loss: 0.32857514757691003, val loss: 0.44328423589468\n",
      "Epoch 95: train loss: 0.32677762532148513, val loss: 0.4406069293618202\n",
      "Epoch 96: train loss: 0.3242416894833117, val loss: 0.43779802322387695\n",
      "Epoch 97: train loss: 0.3223485202230063, val loss: 0.4369214028120041\n",
      "Epoch 98: train loss: 0.3204288568538764, val loss: 0.4335874393582344\n",
      "Epoch 99: train loss: 0.3185309198743103, val loss: 0.43287964910268784\n",
      "Epoch 100: train loss: 0.31621171079870536, val loss: 0.4292277470231056\n",
      "Epoch 101: train loss: 0.3142331964398708, val loss: 0.42872773855924606\n",
      "Epoch 102: train loss: 0.31205657790471275, val loss: 0.42399879544973373\n",
      "Epoch 103: train loss: 0.3102778401572655, val loss: 0.42240384966135025\n",
      "Epoch 104: train loss: 0.3082934069948935, val loss: 0.4233675003051758\n",
      "Epoch 105: train loss: 0.3065795579092131, val loss: 0.42081432044506073\n",
      "Epoch 106: train loss: 0.30481048149830453, val loss: 0.41815611720085144\n",
      "Epoch 107: train loss: 0.30292840923643005, val loss: 0.41521143168210983\n",
      "Epoch 108: train loss: 0.30097691208971206, val loss: 0.412591852247715\n",
      "Epoch 109: train loss: 0.2987520509356574, val loss: 0.41179440915584564\n",
      "Epoch 110: train loss: 0.29702879572685775, val loss: 0.4097457751631737\n",
      "Epoch 111: train loss: 0.29559263547079817, val loss: 0.4098891243338585\n",
      "Epoch 112: train loss: 0.29354279229587216, val loss: 0.4062199518084526\n",
      "Epoch 113: train loss: 0.2917478904152415, val loss: 0.40437448024749756\n",
      "Epoch 114: train loss: 0.29017004723137635, val loss: 0.4013622850179672\n",
      "Epoch 115: train loss: 0.28895955958501923, val loss: 0.4015416279435158\n",
      "Epoch 116: train loss: 0.2867375544203297, val loss: 0.397707499563694\n",
      "Epoch 117: train loss: 0.2850712488208554, val loss: 0.3979472294449806\n",
      "Epoch 118: train loss: 0.2832028513950135, val loss: 0.3949371427297592\n",
      "Epoch 119: train loss: 0.2815667458002725, val loss: 0.3932251110672951\n",
      "Epoch 120: train loss: 0.27978560401504626, val loss: 0.3920377641916275\n",
      "Epoch 121: train loss: 0.2783674567071335, val loss: 0.38939882814884186\n",
      "Epoch 122: train loss: 0.2765339368583719, val loss: 0.3879087418317795\n",
      "Epoch 123: train loss: 0.27516577347786897, val loss: 0.3852724954485893\n",
      "Epoch 124: train loss: 0.2732167855094983, val loss: 0.38511793315410614\n",
      "Epoch 125: train loss: 0.27202830599710387, val loss: 0.38465654104948044\n",
      "Epoch 126: train loss: 0.27023879056738626, val loss: 0.38250893354415894\n",
      "Epoch 127: train loss: 0.2685888829534251, val loss: 0.38031119853258133\n",
      "Epoch 128: train loss: 0.26742402499931694, val loss: 0.3770856559276581\n",
      "Epoch 129: train loss: 0.26613103413613004, val loss: 0.3770561218261719\n",
      "Epoch 130: train loss: 0.2641303209999892, val loss: 0.3746512681245804\n",
      "Epoch 131: train loss: 0.26280110502936255, val loss: 0.3720680996775627\n",
      "Epoch 132: train loss: 0.2612489290055945, val loss: 0.3718215674161911\n",
      "Epoch 133: train loss: 0.26000444717128385, val loss: 0.36968184262514114\n",
      "Epoch 134: train loss: 0.25853526084028167, val loss: 0.36725620925426483\n",
      "Epoch 135: train loss: 0.25677396081039927, val loss: 0.36585555225610733\n",
      "Epoch 136: train loss: 0.2552539175318488, val loss: 0.36534368246793747\n",
      "Epoch 137: train loss: 0.253557011431551, val loss: 0.3628939390182495\n",
      "Epoch 138: train loss: 0.2528568612072679, val loss: 0.36187809705734253\n",
      "Epoch 139: train loss: 0.25129178657090884, val loss: 0.3618135154247284\n",
      "Epoch 140: train loss: 0.24997616003090087, val loss: 0.35919344425201416\n",
      "Epoch 141: train loss: 0.24883731179321486, val loss: 0.35917768627405167\n",
      "Epoch 142: train loss: 0.24713673314328521, val loss: 0.3567691370844841\n",
      "Epoch 143: train loss: 0.24546207557196556, val loss: 0.3552941679954529\n",
      "Epoch 144: train loss: 0.24451657982676375, val loss: 0.35401467233896255\n",
      "Epoch 145: train loss: 0.24296596251021643, val loss: 0.3509996756911278\n",
      "Epoch 146: train loss: 0.2414185753136591, val loss: 0.3498469442129135\n",
      "Epoch 147: train loss: 0.24053269319968923, val loss: 0.3501383475959301\n",
      "Epoch 148: train loss: 0.23889006689380718, val loss: 0.3477695509791374\n",
      "Epoch 149: train loss: 0.23778395990030862, val loss: 0.34646737948060036\n",
      "Epoch 150: train loss: 0.23671470134135986, val loss: 0.346428707242012\n",
      "Epoch 151: train loss: 0.2351449389097076, val loss: 0.34455788880586624\n",
      "Epoch 152: train loss: 0.23377939900495928, val loss: 0.34285710752010345\n",
      "Epoch 153: train loss: 0.23261363852534253, val loss: 0.3407306671142578\n",
      "Epoch 154: train loss: 0.23115279769165303, val loss: 0.3401186093688011\n",
      "Epoch 155: train loss: 0.23021602321006004, val loss: 0.3385342210531235\n",
      "Epoch 156: train loss: 0.22902167043005162, val loss: 0.3383597992360592\n",
      "Epoch 157: train loss: 0.22859824052319655, val loss: 0.33501260355114937\n",
      "Epoch 158: train loss: 0.2266851939706388, val loss: 0.3365224562585354\n",
      "Epoch 159: train loss: 0.22549465820996523, val loss: 0.335661593824625\n",
      "Epoch 160: train loss: 0.22429564026631937, val loss: 0.33062422275543213\n",
      "Epoch 161: train loss: 0.22315513473326612, val loss: 0.3301471509039402\n",
      "Epoch 162: train loss: 0.221950420267011, val loss: 0.3278740160167217\n",
      "Epoch 163: train loss: 0.22092052481214192, val loss: 0.3269454762339592\n",
      "Epoch 164: train loss: 0.21978482413198464, val loss: 0.32688871026039124\n",
      "Epoch 165: train loss: 0.21834550860737398, val loss: 0.3252132795751095\n",
      "Epoch 166: train loss: 0.2175163953805118, val loss: 0.32706087455153465\n",
      "Epoch 167: train loss: 0.2161377398008149, val loss: 0.3218093179166317\n",
      "Epoch 168: train loss: 0.21545191022042004, val loss: 0.3233041800558567\n",
      "Epoch 169: train loss: 0.21381177415122632, val loss: 0.32122788950800896\n",
      "Epoch 170: train loss: 0.21288683440941528, val loss: 0.3207356482744217\n",
      "Epoch 171: train loss: 0.2118162533273809, val loss: 0.31790104880928993\n",
      "Epoch 172: train loss: 0.21096393132533017, val loss: 0.3156191147863865\n",
      "Epoch 173: train loss: 0.20950190753611192, val loss: 0.31462206318974495\n",
      "Epoch 174: train loss: 0.2086818196422249, val loss: 0.3138270042836666\n",
      "Epoch 175: train loss: 0.20758444328336925, val loss: 0.311971303075552\n",
      "Epoch 176: train loss: 0.20639190754930165, val loss: 0.31143391132354736\n",
      "Epoch 177: train loss: 0.20538701050346173, val loss: 0.3102467730641365\n",
      "Epoch 178: train loss: 0.20431960843155408, val loss: 0.31288348138332367\n",
      "Epoch 179: train loss: 0.20329017472769537, val loss: 0.30978596583008766\n",
      "Epoch 180: train loss: 0.2024587963442131, val loss: 0.30686599016189575\n",
      "Epoch 181: train loss: 0.20172309689041212, val loss: 0.3058469668030739\n",
      "Epoch 182: train loss: 0.2003170530056494, val loss: 0.30504172667860985\n",
      "Epoch 183: train loss: 0.19941974288885284, val loss: 0.3030752055346966\n",
      "Epoch 184: train loss: 0.19849525623775627, val loss: 0.3031977377831936\n",
      "Epoch 185: train loss: 0.19730212962226282, val loss: 0.3028009906411171\n",
      "Epoch 186: train loss: 0.19667917410655958, val loss: 0.30334610119462013\n",
      "Epoch 187: train loss: 0.1956483331557311, val loss: 0.30038947612047195\n",
      "Epoch 188: train loss: 0.19422313398646593, val loss: 0.29784898832440376\n",
      "Epoch 189: train loss: 0.1934915040490363, val loss: 0.29984112083911896\n",
      "Epoch 190: train loss: 0.1924494386391093, val loss: 0.29582905396819115\n",
      "Epoch 191: train loss: 0.19166570085982254, val loss: 0.29485488682985306\n",
      "Epoch 192: train loss: 0.19038903564263387, val loss: 0.2949082851409912\n",
      "Epoch 193: train loss: 0.18967991528928532, val loss: 0.2925114408135414\n",
      "Epoch 194: train loss: 0.18853705466202528, val loss: 0.29139329120516777\n",
      "Epoch 195: train loss: 0.18770726717830832, val loss: 0.2899961993098259\n",
      "Epoch 196: train loss: 0.18684235017543446, val loss: 0.2895711176097393\n",
      "Epoch 197: train loss: 0.18606043100960695, val loss: 0.28950533643364906\n",
      "Epoch 198: train loss: 0.18513865940265506, val loss: 0.28974156081676483\n",
      "Epoch 199: train loss: 0.18407700008541414, val loss: 0.29022227227687836\n",
      "Epoch 200: train loss: 0.18302151126551575, val loss: 0.2869562804698944\n",
      "Epoch 201: train loss: 0.18254322577870308, val loss: 0.28610146790742874\n",
      "Epoch 202: train loss: 0.18145261839767252, val loss: 0.28309595957398415\n",
      "Epoch 203: train loss: 0.18049923408191137, val loss: 0.2835547551512718\n",
      "Epoch 204: train loss: 0.17970425418703745, val loss: 0.2811846286058426\n",
      "Epoch 205: train loss: 0.17878940062307136, val loss: 0.28116248175501823\n",
      "Epoch 206: train loss: 0.1779060295324472, val loss: 0.28279516473412514\n",
      "Epoch 207: train loss: 0.17711033020016415, val loss: 0.28266526013612747\n",
      "Epoch 208: train loss: 0.17635488170827382, val loss: 0.27986467629671097\n",
      "Epoch 209: train loss: 0.17544956651340704, val loss: 0.2772625498473644\n",
      "Epoch 210: train loss: 0.17453165678991986, val loss: 0.27690352126955986\n",
      "Epoch 211: train loss: 0.17389141280193104, val loss: 0.27393631264567375\n",
      "Epoch 212: train loss: 0.1728884677157142, val loss: 0.2770838811993599\n",
      "Epoch 213: train loss: 0.17217047069536942, val loss: 0.2727426365017891\n",
      "Epoch 214: train loss: 0.17119490063412443, val loss: 0.2736544907093048\n",
      "Epoch 215: train loss: 0.17064635994016872, val loss: 0.2715328298509121\n",
      "Epoch 216: train loss: 0.1698715786690885, val loss: 0.2705399617552757\n",
      "Epoch 217: train loss: 0.16875668156287205, val loss: 0.2700585760176182\n",
      "Epoch 218: train loss: 0.16800084313860286, val loss: 0.26957646384835243\n",
      "Epoch 219: train loss: 0.16705919920482964, val loss: 0.26859185472130775\n",
      "Epoch 220: train loss: 0.16656617203478485, val loss: 0.27026892080903053\n",
      "Epoch 221: train loss: 0.16568870310863612, val loss: 0.26742854341864586\n",
      "Epoch 222: train loss: 0.16472576457820892, val loss: 0.26405182108283043\n",
      "Epoch 223: train loss: 0.16419806813091595, val loss: 0.2642117887735367\n",
      "Epoch 224: train loss: 0.16334439537097567, val loss: 0.26165036484599113\n",
      "Epoch 225: train loss: 0.16282771408986282, val loss: 0.262912780046463\n",
      "Epoch 226: train loss: 0.16149711036888534, val loss: 0.2616698443889618\n",
      "Epoch 227: train loss: 0.16097700770132606, val loss: 0.2633998468518257\n",
      "Epoch 228: train loss: 0.15985064630082368, val loss: 0.25926172733306885\n",
      "Epoch 229: train loss: 0.15946552595528307, val loss: 0.25993504747748375\n",
      "Epoch 230: train loss: 0.15897570129840524, val loss: 0.2604978643357754\n",
      "Epoch 231: train loss: 0.15755953356667463, val loss: 0.25877754017710686\n",
      "Epoch 232: train loss: 0.1573753333810263, val loss: 0.25853264331817627\n",
      "Epoch 233: train loss: 0.15627327757561685, val loss: 0.2576645463705063\n",
      "Epoch 234: train loss: 0.15555710129120973, val loss: 0.2546031214296818\n",
      "Epoch 235: train loss: 0.15513039466720202, val loss: 0.25429948046803474\n",
      "Epoch 236: train loss: 0.15393503027311223, val loss: 0.2530701234936714\n",
      "Epoch 237: train loss: 0.1534931578937211, val loss: 0.25226036086678505\n",
      "Epoch 238: train loss: 0.15250107457107362, val loss: 0.25060316547751427\n",
      "Epoch 239: train loss: 0.1520996320553365, val loss: 0.2503886818885803\n",
      "Epoch 240: train loss: 0.1513350886754918, val loss: 0.25123681873083115\n",
      "Epoch 241: train loss: 0.15033772346806737, val loss: 0.24883097037672997\n",
      "Epoch 242: train loss: 0.14971713840533066, val loss: 0.2485448271036148\n",
      "Epoch 243: train loss: 0.1490218188882614, val loss: 0.24787301942706108\n",
      "Epoch 244: train loss: 0.14847075488395217, val loss: 0.24774431064724922\n",
      "Epoch 245: train loss: 0.14744808303106458, val loss: 0.24558642879128456\n",
      "Epoch 246: train loss: 0.1468930337571973, val loss: 0.24318234622478485\n",
      "Epoch 247: train loss: 0.14643542932961665, val loss: 0.24396910890936852\n",
      "Epoch 248: train loss: 0.1453290943692845, val loss: 0.24377285316586494\n",
      "Epoch 249: train loss: 0.14505586827436096, val loss: 0.24209674075245857\n",
      "Epoch 250: train loss: 0.1442891788790328, val loss: 0.24258754402399063\n",
      "Epoch 251: train loss: 0.1434079826277325, val loss: 0.24129166826605797\n",
      "Epoch 252: train loss: 0.14269678715746686, val loss: 0.24072078987956047\n",
      "Epoch 253: train loss: 0.14196111832382396, val loss: 0.23993614315986633\n",
      "Epoch 254: train loss: 0.14142373037256473, val loss: 0.23845167458057404\n",
      "Epoch 255: train loss: 0.14082806079129073, val loss: 0.23721370100975037\n",
      "Epoch 256: train loss: 0.1402034691563232, val loss: 0.23699401691555977\n",
      "Epoch 257: train loss: 0.13970989460105485, val loss: 0.2357098124921322\n",
      "Epoch 258: train loss: 0.13864263425812695, val loss: 0.24043015018105507\n",
      "Epoch 259: train loss: 0.13810620796481834, val loss: 0.23792139068245888\n",
      "Epoch 260: train loss: 0.13753780416416522, val loss: 0.23317666724324226\n",
      "Epoch 261: train loss: 0.1367157578409906, val loss: 0.2332690991461277\n",
      "Epoch 262: train loss: 0.13596606235105357, val loss: 0.23271707072854042\n",
      "Epoch 263: train loss: 0.13539990143560188, val loss: 0.23044100031256676\n",
      "Epoch 264: train loss: 0.1343727196520818, val loss: 0.23035334050655365\n",
      "Epoch 265: train loss: 0.13413943360538041, val loss: 0.22990506142377853\n",
      "Epoch 266: train loss: 0.13353954418661235, val loss: 0.22778765857219696\n",
      "Epoch 267: train loss: 0.13233600411781957, val loss: 0.22763121128082275\n",
      "Epoch 268: train loss: 0.13225263338663812, val loss: 0.22763453423976898\n",
      "Epoch 269: train loss: 0.13159975941100957, val loss: 0.2257130779325962\n",
      "Epoch 270: train loss: 0.13071181591781517, val loss: 0.22763941064476967\n",
      "Epoch 271: train loss: 0.13032276471799903, val loss: 0.22317277640104294\n",
      "Epoch 272: train loss: 0.12965223264729264, val loss: 0.2238324098289013\n",
      "Epoch 273: train loss: 0.1289304855094942, val loss: 0.22404953092336655\n",
      "Epoch 274: train loss: 0.12825674448029661, val loss: 0.2222524769604206\n",
      "Epoch 275: train loss: 0.12761066152024447, val loss: 0.22173228487372398\n",
      "Epoch 276: train loss: 0.12715251797342253, val loss: 0.22257978841662407\n",
      "Epoch 277: train loss: 0.12637953594088439, val loss: 0.22064568102359772\n",
      "Epoch 278: train loss: 0.1256980496424937, val loss: 0.2204807810485363\n",
      "Epoch 279: train loss: 0.12501316862007403, val loss: 0.21925842389464378\n",
      "Epoch 280: train loss: 0.12449696676593246, val loss: 0.2180306389927864\n",
      "Epoch 281: train loss: 0.12392327064336106, val loss: 0.2170817032456398\n",
      "Epoch 282: train loss: 0.12331606723716235, val loss: 0.21632138639688492\n",
      "Epoch 283: train loss: 0.12260699914254149, val loss: 0.21646737679839134\n",
      "Epoch 284: train loss: 0.12210467496229052, val loss: 0.2160625457763672\n",
      "Epoch 285: train loss: 0.1216974273124422, val loss: 0.2136557586491108\n",
      "Epoch 286: train loss: 0.12078290160809885, val loss: 0.21476532146334648\n",
      "Epoch 287: train loss: 0.11996389955661921, val loss: 0.21291512623429298\n",
      "Epoch 288: train loss: 0.11959781086121615, val loss: 0.21214715018868446\n",
      "Epoch 289: train loss: 0.11892387954891215, val loss: 0.21181315928697586\n",
      "Epoch 290: train loss: 0.11846966739672143, val loss: 0.21190369129180908\n",
      "Epoch 291: train loss: 0.11755522248773005, val loss: 0.20891131833195686\n",
      "Epoch 292: train loss: 0.11683470299920784, val loss: 0.21132025495171547\n",
      "Epoch 293: train loss: 0.11678747338179239, val loss: 0.2100025750696659\n",
      "Epoch 294: train loss: 0.11596192790962676, val loss: 0.20735527575016022\n",
      "Epoch 295: train loss: 0.11547259362314076, val loss: 0.20624171197414398\n",
      "Epoch 296: train loss: 0.1148642066626407, val loss: 0.20723809860646725\n",
      "Epoch 297: train loss: 0.11451577590653726, val loss: 0.206187991425395\n",
      "Epoch 298: train loss: 0.11378993545724302, val loss: 0.20571904629468918\n",
      "Epoch 299: train loss: 0.1131008015577406, val loss: 0.2046334594488144\n",
      "Epoch 300: train loss: 0.11292800107196202, val loss: 0.205560103058815\n",
      "Epoch 301: train loss: 0.11228140529971697, val loss: 0.20585596188902855\n",
      "Epoch 302: train loss: 0.11110250480480344, val loss: 0.20460600592195988\n",
      "Epoch 303: train loss: 0.11068740157043572, val loss: 0.20185752026736736\n",
      "Epoch 304: train loss: 0.1100125527048921, val loss: 0.20061862096190453\n",
      "Epoch 305: train loss: 0.11008614298760677, val loss: 0.20197118632495403\n",
      "Epoch 306: train loss: 0.10897169416732003, val loss: 0.20173345506191254\n",
      "Epoch 307: train loss: 0.10887278304871918, val loss: 0.1988582070916891\n",
      "Epoch 308: train loss: 0.1079364643596193, val loss: 0.19874232448637486\n",
      "Epoch 309: train loss: 0.10726721320460723, val loss: 0.19734627939760685\n",
      "Epoch 310: train loss: 0.10678784009854181, val loss: 0.19696294143795967\n",
      "Epoch 311: train loss: 0.10650769836545887, val loss: 0.19824383780360222\n",
      "Epoch 312: train loss: 0.10600108799105647, val loss: 0.19604083336889744\n",
      "Epoch 313: train loss: 0.10564594619202482, val loss: 0.197235107421875\n",
      "Epoch 314: train loss: 0.10457762881105766, val loss: 0.1967543177306652\n",
      "Epoch 315: train loss: 0.10408702942036757, val loss: 0.19472195021808147\n",
      "Epoch 316: train loss: 0.10357110562549623, val loss: 0.19333773478865623\n",
      "Epoch 317: train loss: 0.10312542941437092, val loss: 0.1919486578553915\n",
      "Epoch 318: train loss: 0.10257287045508107, val loss: 0.19311288185417652\n",
      "Epoch 319: train loss: 0.10221203138862549, val loss: 0.19233892112970352\n",
      "Epoch 320: train loss: 0.10170718445298096, val loss: 0.1906319446861744\n",
      "Epoch 321: train loss: 0.10104930377481654, val loss: 0.19040012545883656\n",
      "Epoch 322: train loss: 0.10027114124965605, val loss: 0.1901009902358055\n",
      "Epoch 323: train loss: 0.09977194547107975, val loss: 0.188227703794837\n",
      "Epoch 324: train loss: 0.09944474089091138, val loss: 0.18919508904218674\n",
      "Epoch 325: train loss: 0.09901352563117774, val loss: 0.18969976529479027\n",
      "Epoch 326: train loss: 0.09830578984933054, val loss: 0.1890158262103796\n",
      "Epoch 327: train loss: 0.0978824127707673, val loss: 0.18860010243952274\n",
      "Epoch 328: train loss: 0.09710661401958179, val loss: 0.18540609255433083\n",
      "Epoch 329: train loss: 0.09670456780965948, val loss: 0.1827061027288437\n",
      "Epoch 330: train loss: 0.09635748010042328, val loss: 0.18447575718164444\n",
      "Epoch 331: train loss: 0.09582505759480965, val loss: 0.1822713352739811\n",
      "Epoch 332: train loss: 0.0952979747652618, val loss: 0.18560361303389072\n",
      "Epoch 333: train loss: 0.09490771814377547, val loss: 0.18374551460146904\n",
      "Epoch 334: train loss: 0.09441471914768375, val loss: 0.18351680040359497\n",
      "Epoch 335: train loss: 0.09363575937388886, val loss: 0.18126505054533482\n",
      "Epoch 336: train loss: 0.09346249180656366, val loss: 0.1810952927917242\n",
      "Epoch 337: train loss: 0.09292032480074354, val loss: 0.18015114963054657\n",
      "Epoch 338: train loss: 0.09241485835212152, val loss: 0.17918619699776173\n",
      "Epoch 339: train loss: 0.0917421161047737, val loss: 0.17800846695899963\n",
      "Epoch 340: train loss: 0.09136944867083933, val loss: 0.17683801800012589\n",
      "Epoch 341: train loss: 0.09096471793956906, val loss: 0.17725953832268715\n",
      "Epoch 342: train loss: 0.09033229992363197, val loss: 0.18100799433887005\n",
      "Epoch 343: train loss: 0.09000919158066928, val loss: 0.17631622962653637\n",
      "Epoch 344: train loss: 0.08922834450982046, val loss: 0.17731250077486038\n",
      "Epoch 345: train loss: 0.08893679728873295, val loss: 0.17550907284021378\n",
      "Epoch 346: train loss: 0.0887559242315325, val loss: 0.17548968456685543\n",
      "Epoch 347: train loss: 0.0880823614321009, val loss: 0.17661868408322334\n",
      "Epoch 348: train loss: 0.08736983009293671, val loss: 0.17414217814803123\n",
      "Epoch 349: train loss: 0.0869353178162, val loss: 0.17460176534950733\n",
      "Epoch 350: train loss: 0.08668243763581945, val loss: 0.171880966052413\n",
      "Epoch 351: train loss: 0.08595031785861763, val loss: 0.17281155847012997\n",
      "Epoch 352: train loss: 0.08560759629914279, val loss: 0.17117416858673096\n",
      "Epoch 353: train loss: 0.08561824996247085, val loss: 0.17297499254345894\n",
      "Epoch 354: train loss: 0.08477337337599514, val loss: 0.1734130010008812\n",
      "Epoch 355: train loss: 0.08442598618311416, val loss: 0.1701321378350258\n",
      "Epoch 356: train loss: 0.08376061226059042, val loss: 0.1699846126139164\n",
      "Epoch 357: train loss: 0.0832874951595109, val loss: 0.1682630106806755\n",
      "Epoch 358: train loss: 0.08283347995968202, val loss: 0.16899549774825573\n",
      "Epoch 359: train loss: 0.08254469339168372, val loss: 0.1663307026028633\n",
      "Epoch 360: train loss: 0.08198432763323446, val loss: 0.17051006108522415\n",
      "Epoch 361: train loss: 0.08164730307343972, val loss: 0.16710285656154156\n",
      "Epoch 362: train loss: 0.08112843779616401, val loss: 0.16598597913980484\n",
      "Epoch 363: train loss: 0.08064244656158172, val loss: 0.16470455192029476\n",
      "Epoch 364: train loss: 0.08015845536367833, val loss: 0.16515850648283958\n",
      "Epoch 365: train loss: 0.07987490475752197, val loss: 0.16367272101342678\n",
      "Epoch 366: train loss: 0.07941168543025584, val loss: 0.16362980008125305\n",
      "Epoch 367: train loss: 0.07906795028756546, val loss: 0.16288109682500362\n",
      "Epoch 368: train loss: 0.07865949328994876, val loss: 0.162046043202281\n",
      "Epoch 369: train loss: 0.07827362483893684, val loss: 0.16383340395987034\n",
      "Epoch 370: train loss: 0.07770165120978949, val loss: 0.16259055398404598\n",
      "Epoch 371: train loss: 0.07731879578476752, val loss: 0.1614200510084629\n",
      "Epoch 372: train loss: 0.07682450481029322, val loss: 0.16063429974019527\n",
      "Epoch 373: train loss: 0.07644060718997636, val loss: 0.162012854591012\n",
      "Epoch 374: train loss: 0.07616931320755593, val loss: 0.15788636729121208\n",
      "Epoch 375: train loss: 0.07553793780556772, val loss: 0.15895582921802998\n",
      "Epoch 376: train loss: 0.07540690451763067, val loss: 0.16080179251730442\n",
      "Epoch 377: train loss: 0.0750191610273642, val loss: 0.15873200446367264\n",
      "Epoch 378: train loss: 0.07426103067452594, val loss: 0.1578608714044094\n",
      "Epoch 379: train loss: 0.0739554836518505, val loss: 0.15865521878004074\n",
      "Epoch 380: train loss: 0.07363008986702155, val loss: 0.15641057305037975\n",
      "Epoch 381: train loss: 0.07315760103657681, val loss: 0.15711928345263004\n",
      "Epoch 382: train loss: 0.07297654892447258, val loss: 0.15596738830208778\n",
      "Epoch 383: train loss: 0.07241786486745777, val loss: 0.15631126426160336\n",
      "Epoch 384: train loss: 0.07212531909331976, val loss: 0.15511232241988182\n",
      "Epoch 385: train loss: 0.07182098083181032, val loss: 0.15604924224317074\n",
      "Epoch 386: train loss: 0.07149920644216076, val loss: 0.15511015988886356\n",
      "Epoch 387: train loss: 0.0709808806370614, val loss: 0.15424517169594765\n",
      "Epoch 388: train loss: 0.07059863176378149, val loss: 0.15396937914192677\n",
      "Epoch 389: train loss: 0.07016111494366618, val loss: 0.15183295868337154\n",
      "Epoch 390: train loss: 0.06989013709368873, val loss: 0.1529554482549429\n",
      "Epoch 391: train loss: 0.06954887237220915, val loss: 0.15210794657468796\n",
      "Epoch 392: train loss: 0.06916653283113204, val loss: 0.1529393307864666\n",
      "Epoch 393: train loss: 0.06903958586901085, val loss: 0.15140260010957718\n",
      "Epoch 394: train loss: 0.06845323595150511, val loss: 0.15033837780356407\n",
      "Epoch 395: train loss: 0.06801853467926641, val loss: 0.14947832748293877\n",
      "Epoch 396: train loss: 0.06780186581265027, val loss: 0.1500297375023365\n",
      "Epoch 397: train loss: 0.06720267619349839, val loss: 0.1503338012844324\n",
      "Epoch 398: train loss: 0.06712265691650883, val loss: 0.14819058030843735\n",
      "Epoch 399: train loss: 0.06670030639456674, val loss: 0.14789396151900291\n",
      "Epoch 400: train loss: 0.0663030720110502, val loss: 0.1482536718249321\n",
      "Epoch 401: train loss: 0.06578830523950062, val loss: 0.146677166223526\n",
      "Epoch 402: train loss: 0.06550348008265101, val loss: 0.14688052982091904\n",
      "Epoch 403: train loss: 0.06544694630140108, val loss: 0.14719839207828045\n",
      "Epoch 404: train loss: 0.06493999326601016, val loss: 0.14616931788623333\n",
      "Epoch 405: train loss: 0.06463150691261715, val loss: 0.14672213979065418\n",
      "Epoch 406: train loss: 0.06424076142984059, val loss: 0.14792440086603165\n",
      "Epoch 407: train loss: 0.06411854453002733, val loss: 0.1441892683506012\n",
      "Epoch 408: train loss: 0.06384038487288034, val loss: 0.14484509453177452\n",
      "Epoch 409: train loss: 0.06338272155139307, val loss: 0.14471695572137833\n",
      "Epoch 410: train loss: 0.06299017745562138, val loss: 0.1434667445719242\n",
      "Epoch 411: train loss: 0.06265258412760716, val loss: 0.14555938355624676\n",
      "Epoch 412: train loss: 0.062323140388744716, val loss: 0.14354793820530176\n",
      "Epoch 413: train loss: 0.06182618495910552, val loss: 0.14449558220803738\n",
      "Epoch 414: train loss: 0.06175603632130914, val loss: 0.14303833059966564\n",
      "Epoch 415: train loss: 0.061724352894052106, val loss: 0.1439509503543377\n",
      "Epoch 416: train loss: 0.06130967941530699, val loss: 0.14215006865561008\n",
      "Epoch 417: train loss: 0.06088974785907952, val loss: 0.14074543863534927\n",
      "Epoch 418: train loss: 0.06064726381384053, val loss: 0.1417882889509201\n",
      "Epoch 419: train loss: 0.060291741112699235, val loss: 0.1411195332184434\n",
      "Epoch 420: train loss: 0.06009261333710693, val loss: 0.14129782654345036\n",
      "Epoch 421: train loss: 0.059682746797225635, val loss: 0.14150439761579037\n",
      "Epoch 422: train loss: 0.05948971207285582, val loss: 0.14018752425909042\n",
      "Epoch 423: train loss: 0.05895158434052438, val loss: 0.14033574238419533\n",
      "Epoch 424: train loss: 0.05887399930281796, val loss: 0.14008646458387375\n",
      "Epoch 425: train loss: 0.05851367912634024, val loss: 0.13911197613924742\n",
      "Epoch 426: train loss: 0.058169936194738425, val loss: 0.13884884864091873\n",
      "Epoch 427: train loss: 0.05811233365313752, val loss: 0.1400427334010601\n",
      "Epoch 428: train loss: 0.05762805894129185, val loss: 0.137882387265563\n",
      "Epoch 429: train loss: 0.05761756396729819, val loss: 0.13813934475183487\n",
      "Epoch 430: train loss: 0.057205695784274986, val loss: 0.13586863689124584\n",
      "Epoch 431: train loss: 0.056976052351011164, val loss: 0.13632455468177795\n",
      "Epoch 432: train loss: 0.05668251592469971, val loss: 0.13609503768384457\n",
      "Epoch 433: train loss: 0.056572812694869995, val loss: 0.13692679069936275\n",
      "Epoch 434: train loss: 0.056326920504334, val loss: 0.13601899333298206\n",
      "Epoch 435: train loss: 0.05632176062798663, val loss: 0.1354493871331215\n",
      "Epoch 436: train loss: 0.05576954787364002, val loss: 0.1361271757632494\n",
      "Epoch 437: train loss: 0.05555807438863722, val loss: 0.13502374105155468\n",
      "Epoch 438: train loss: 0.0552735239320898, val loss: 0.1366743529215455\n",
      "Epoch 439: train loss: 0.05494876857512296, val loss: 0.13458316028118134\n",
      "Epoch 440: train loss: 0.05482768646615738, val loss: 0.1342830639332533\n",
      "Epoch 441: train loss: 0.054415565696038365, val loss: 0.13366875052452087\n",
      "Epoch 442: train loss: 0.05443529545900597, val loss: 0.13345368951559067\n",
      "Epoch 443: train loss: 0.05424519145617256, val loss: 0.133458212018013\n",
      "Epoch 444: train loss: 0.05401367818762064, val loss: 0.13291852176189423\n",
      "Epoch 445: train loss: 0.05368481032679339, val loss: 0.1329189632087946\n",
      "Epoch 446: train loss: 0.053533228084430395, val loss: 0.1337465327233076\n",
      "Epoch 447: train loss: 0.053123985090722296, val loss: 0.13359471037983894\n",
      "Epoch 448: train loss: 0.0528899289640832, val loss: 0.13228503428399563\n",
      "Epoch 449: train loss: 0.052859402620998794, val loss: 0.13226777873933315\n",
      "Epoch 450: train loss: 0.05239718532258488, val loss: 0.13426796160638332\n",
      "Epoch 451: train loss: 0.05227854840605915, val loss: 0.13207082264125347\n",
      "Epoch 452: train loss: 0.05212284593177519, val loss: 0.13153024204075336\n",
      "Epoch 453: train loss: 0.05214436063089623, val loss: 0.13111039623618126\n",
      "Epoch 454: train loss: 0.05152823327414081, val loss: 0.1313132494688034\n",
      "Epoch 455: train loss: 0.0515152203544064, val loss: 0.1308599654585123\n",
      "Epoch 456: train loss: 0.051270842922158354, val loss: 0.13095033820718527\n",
      "Epoch 457: train loss: 0.05088047784981639, val loss: 0.1291667688637972\n",
      "Epoch 458: train loss: 0.05099207003671178, val loss: 0.13147500529885292\n",
      "Epoch 459: train loss: 0.05063182771283791, val loss: 0.12893720157444477\n",
      "Epoch 460: train loss: 0.05061731693431935, val loss: 0.12957895454019308\n",
      "Epoch 461: train loss: 0.05020816864403818, val loss: 0.1295281983911991\n",
      "Epoch 462: train loss: 0.050226356000516274, val loss: 0.12897659931331873\n",
      "Epoch 463: train loss: 0.0499597520040204, val loss: 0.12925026565790176\n",
      "Epoch 464: train loss: 0.04966596954060084, val loss: 0.1285521239042282\n",
      "Epoch 465: train loss: 0.04945413507606344, val loss: 0.1286522066220641\n",
      "Epoch 466: train loss: 0.04935252312321322, val loss: 0.1304391212761402\n",
      "Epoch 467: train loss: 0.04940208111156712, val loss: 0.1317919734865427\n",
      "Epoch 468: train loss: 0.04930683691024157, val loss: 0.12827908992767334\n",
      "Epoch 469: train loss: 0.04876884940995846, val loss: 0.12738963216543198\n",
      "Epoch 470: train loss: 0.048735841429630784, val loss: 0.12819227669388056\n",
      "Epoch 471: train loss: 0.04840116840791796, val loss: 0.12699919007718563\n",
      "Epoch 472: train loss: 0.048191700516495586, val loss: 0.12675348948687315\n",
      "Epoch 473: train loss: 0.048398532348203335, val loss: 0.12761742994189262\n",
      "Epoch 474: train loss: 0.04783486895450616, val loss: 0.12763558328151703\n",
      "Epoch 475: train loss: 0.04779751927673096, val loss: 0.12751084566116333\n",
      "Epoch 476: train loss: 0.04758327518343653, val loss: 0.1273807818070054\n",
      "Epoch 477: train loss: 0.0474424371042504, val loss: 0.12638654559850693\n",
      "Epoch 478: train loss: 0.047292924215010045, val loss: 0.12682691682130098\n",
      "Epoch 479: train loss: 0.04717946584631282, val loss: 0.12531561125069857\n",
      "Epoch 480: train loss: 0.04699108810235534, val loss: 0.12731398083269596\n",
      "Epoch 481: train loss: 0.046956727119958, val loss: 0.12479899823665619\n",
      "Epoch 482: train loss: 0.046689418893902566, val loss: 0.12504300009459257\n",
      "Epoch 483: train loss: 0.04637122463114859, val loss: 0.1255081258714199\n",
      "Epoch 484: train loss: 0.046053496106354015, val loss: 0.1248658886179328\n",
      "Epoch 485: train loss: 0.04616035513484918, val loss: 0.12519339937716722\n",
      "Epoch 486: train loss: 0.045961621517120174, val loss: 0.12478481978178024\n",
      "Epoch 487: train loss: 0.045952901835633976, val loss: 0.12531568575650454\n",
      "Epoch 488: train loss: 0.045851523012205186, val loss: 0.12483128439635038\n",
      "Epoch 489: train loss: 0.04541064503096708, val loss: 0.12530684284865856\n",
      "Epoch 490: train loss: 0.04546070558179324, val loss: 0.12374590337276459\n",
      "Epoch 491: train loss: 0.04518485342860416, val loss: 0.12406828440725803\n",
      "Epoch 492: train loss: 0.04507013324651793, val loss: 0.1239001015201211\n",
      "Epoch 493: train loss: 0.04482984256604185, val loss: 0.12337494641542435\n",
      "Epoch 494: train loss: 0.044834867755072214, val loss: 0.12385258451104164\n",
      "Epoch 495: train loss: 0.044438696411924786, val loss: 0.12248361203819513\n",
      "Epoch 496: train loss: 0.04439380248459053, val loss: 0.12440199963748455\n",
      "Epoch 497: train loss: 0.044285988119333174, val loss: 0.12274061515927315\n",
      "Epoch 498: train loss: 0.044235538144841456, val loss: 0.12286632787436247\n",
      "Epoch 499: train loss: 0.044352166060138004, val loss: 0.1236424669623375\n",
      "Epoch 500: train loss: 0.0437874177903453, val loss: 0.12312733568251133\n",
      "Epoch 501: train loss: 0.043583540910990634, val loss: 0.12211044412106276\n",
      "Epoch 502: train loss: 0.04365188763280664, val loss: 0.12218169122934341\n",
      "Epoch 503: train loss: 0.0436263139274766, val loss: 0.12343840673565865\n",
      "Epoch 504: train loss: 0.04360042817401403, val loss: 0.12082781828939915\n",
      "Epoch 505: train loss: 0.04341898392916349, val loss: 0.12196662276983261\n",
      "Epoch 506: train loss: 0.04318035795812473, val loss: 0.12207785062491894\n",
      "Epoch 507: train loss: 0.043159420738633804, val loss: 0.12179969903081656\n",
      "Epoch 508: train loss: 0.042992364927975424, val loss: 0.12211415264755487\n",
      "Epoch 509: train loss: 0.042629760324623955, val loss: 0.12129659298807383\n",
      "Epoch 00510: reducing learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 510: train loss: 0.042557419273772296, val loss: 0.1210062988102436\n",
      "Epoch 511: train loss: 0.04189642100104784, val loss: 0.11971763614565134\n",
      "Epoch 512: train loss: 0.04173834338828031, val loss: 0.11940181627869606\n",
      "Epoch 513: train loss: 0.04181813194554291, val loss: 0.12040982116013765\n",
      "Epoch 514: train loss: 0.04172231407385175, val loss: 0.11961491964757442\n",
      "Epoch 515: train loss: 0.041635718718730945, val loss: 0.11948393378406763\n",
      "Epoch 516: train loss: 0.041650166144191106, val loss: 0.11941886320710182\n",
      "Epoch 517: train loss: 0.04158742909618781, val loss: 0.119250881485641\n",
      "Epoch 518: train loss: 0.04154130957146868, val loss: 0.12008710112422705\n",
      "Epoch 519: train loss: 0.04132162635474764, val loss: 0.12066859379410744\n",
      "Epoch 520: train loss: 0.041367550837822266, val loss: 0.11943662259727716\n",
      "Epoch 521: train loss: 0.04136864306470297, val loss: 0.11841138172894716\n",
      "Epoch 522: train loss: 0.04136584036974726, val loss: 0.12026271689683199\n",
      "Epoch 523: train loss: 0.041195869891229156, val loss: 0.11928243841975927\n",
      "Epoch 524: train loss: 0.041190354918409354, val loss: 0.11904272437095642\n",
      "Epoch 525: train loss: 0.041259505711459366, val loss: 0.11937357392162085\n",
      "Epoch 526: train loss: 0.041185807228594655, val loss: 0.11927005555480719\n",
      "Epoch 00527: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 527: train loss: 0.04114900978950845, val loss: 0.12030290998518467\n",
      "Epoch 528: train loss: 0.0410027415611821, val loss: 0.11820174101740122\n",
      "Epoch 529: train loss: 0.04079551477568168, val loss: 0.11879350058734417\n",
      "Epoch 530: train loss: 0.040943067565886504, val loss: 0.11937748454511166\n",
      "Epoch 531: train loss: 0.04090253912869017, val loss: 0.12000748701393604\n",
      "Epoch 532: train loss: 0.04095195741074726, val loss: 0.11813555099070072\n",
      "Epoch 533: train loss: 0.04080339925462847, val loss: 0.11831422336399555\n",
      "Epoch 534: train loss: 0.04066293239009337, val loss: 0.11851738020777702\n",
      "Epoch 535: train loss: 0.040909884498131814, val loss: 0.11830003187060356\n",
      "Epoch 536: train loss: 0.04073708343305295, val loss: 0.11819059494882822\n",
      "Epoch 537: train loss: 0.040646153648512616, val loss: 0.11776636261492968\n",
      "Epoch 538: train loss: 0.040880451181508556, val loss: 0.11863725446164608\n",
      "Epoch 539: train loss: 0.04076636994938491, val loss: 0.1183239808306098\n",
      "Epoch 540: train loss: 0.04075239639156046, val loss: 0.11918239295482635\n",
      "Epoch 541: train loss: 0.040782743606720046, val loss: 0.11909706145524979\n",
      "Epoch 542: train loss: 0.040761560931855345, val loss: 0.11825025733560324\n",
      "Epoch 00543: reducing learning rate of group 0 to 2.7000e-03.\n",
      "Epoch 543: train loss: 0.04104352668682293, val loss: 0.11842265632003546\n",
      "Epoch 544: train loss: 0.04070823679184337, val loss: 0.11913926061242819\n",
      "Epoch 545: train loss: 0.040368246855941405, val loss: 0.11803989484906197\n",
      "Epoch 546: train loss: 0.04070270142789683, val loss: 0.11928112525492907\n",
      "Epoch 547: train loss: 0.04063978285687844, val loss: 0.11837598029524088\n",
      "Epoch 548: train loss: 0.040477268015664605, val loss: 0.11898107454180717\n",
      "Epoch 00549: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 549: train loss: 0.040440411932160365, val loss: 0.11828813049942255\n",
      "Epoch 550: train loss: 0.04058040333354991, val loss: 0.11876284424215555\n",
      "Epoch 551: train loss: 0.040541337524917696, val loss: 0.1178819052875042\n",
      "Epoch 552: train loss: 0.04046140079578517, val loss: 0.11867923103272915\n",
      "Epoch 553: train loss: 0.04051792250909045, val loss: 0.11808187793940306\n",
      "Epoch 554: train loss: 0.04056964450531324, val loss: 0.11939002573490143\n",
      "Epoch 00555: reducing learning rate of group 0 to 2.4300e-04.\n",
      "Epoch 555: train loss: 0.040865015750887815, val loss: 0.11861933954060078\n",
      "Epoch 556: train loss: 0.04059192568686849, val loss: 0.11933345627039671\n",
      "Epoch 557: train loss: 0.04057498703893592, val loss: 0.11944747995585203\n",
      "Epoch 558: train loss: 0.04057441626020367, val loss: 0.1181158721446991\n",
      "Epoch 559: train loss: 0.040384369958599814, val loss: 0.11890895571559668\n",
      "Epoch 560: train loss: 0.04064141423951096, val loss: 0.11806195881217718\n",
      "Epoch 00561: reducing learning rate of group 0 to 7.2900e-05.\n",
      "Epoch 561: train loss: 0.04043032342859516, val loss: 0.11877902317792177\n",
      "Epoch 562: train loss: 0.0405456085647683, val loss: 0.11793037597090006\n",
      "Epoch 563: train loss: 0.04058844296761254, val loss: 0.11832328327000141\n",
      "Epoch 564: train loss: 0.04048184826799718, val loss: 0.12003919202834368\n",
      "Epoch 565: train loss: 0.04056919066004798, val loss: 0.11829530727118254\n",
      "Epoch 566: train loss: 0.04059573034374046, val loss: 0.11974155902862549\n",
      "Epoch 00567: reducing learning rate of group 0 to 2.1870e-05.\n",
      "Epoch 567: train loss: 0.04049970655270318, val loss: 0.11851769126951694\n",
      "Epoch 568: train loss: 0.0406273823257016, val loss: 0.11927791405469179\n",
      "Epoch 569: train loss: 0.04046449316887636, val loss: 0.11898899171501398\n",
      "Epoch 570: train loss: 0.040501517535463265, val loss: 0.11830707732588053\n",
      "Epoch 571: train loss: 0.040691614082848, val loss: 0.11857702489942312\n",
      "Epoch 572: train loss: 0.04049367051144435, val loss: 0.1180798914283514\n",
      "Epoch 00573: reducing learning rate of group 0 to 6.5610e-06.\n",
      "Epoch 573: train loss: 0.040657060597504485, val loss: 0.1185618257150054\n",
      "Epoch 574: train loss: 0.0407767265338946, val loss: 0.11775981821119785\n",
      "Epoch 575: train loss: 0.0406212643207541, val loss: 0.11876564379781485\n",
      "Epoch 576: train loss: 0.04065637756352018, val loss: 0.11838351562619209\n",
      "Epoch 577: train loss: 0.04060641568116369, val loss: 0.11837449204176664\n",
      "Epoch 578: train loss: 0.04054247564552501, val loss: 0.11832838226109743\n",
      "Epoch 00579: reducing learning rate of group 0 to 1.9683e-06.\n",
      "Epoch 579: train loss: 0.04052683002747807, val loss: 0.11881619598716497\n",
      "Epoch 580: train loss: 0.040733806703730134, val loss: 0.11862035002559423\n",
      "Epoch 581: train loss: 0.040556466743004393, val loss: 0.11898545455187559\n",
      "Epoch 582: train loss: 0.04033380062877645, val loss: 0.11909723281860352\n",
      "Epoch 583: train loss: 0.040775499347085, val loss: 0.11847451701760292\n",
      "Epoch 584: train loss: 0.04062086253462567, val loss: 0.11854251008480787\n",
      "Epoch 00585: reducing learning rate of group 0 to 5.9049e-07.\n",
      "Epoch 585: train loss: 0.04066839947571205, val loss: 0.11832885351032019\n",
      "Early stop at epoch 585\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(1000, 40),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.Linear(40, 1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = self.seq(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_x_train = ae_model.encoder(torch.tensor(x_train, dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linan\\AppData\\Local\\Temp\\ipykernel_18016\\784483399.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b7206ec0a54881bb9ea83c68ffa171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 5.1358819007873535, val loss: 6.634143829345703\n",
      "Epoch 2: train loss: 9.775274276733398, val loss: 3.2889747619628906\n",
      "Epoch 3: train loss: 5.008784294128418, val loss: 3.526519536972046\n",
      "Epoch 4: train loss: 3.6139533519744873, val loss: 3.464247226715088\n",
      "Epoch 5: train loss: 2.789952278137207, val loss: 3.165626287460327\n",
      "Epoch 6: train loss: 2.292762041091919, val loss: 2.820711851119995\n",
      "Epoch 7: train loss: 2.0040178298950195, val loss: 2.513446807861328\n",
      "Epoch 8: train loss: 1.82310950756073, val loss: 2.2671782970428467\n",
      "Epoch 9: train loss: 1.6883230209350586, val loss: 2.079256057739258\n",
      "Epoch 10: train loss: 1.570068359375, val loss: 1.9184671640396118\n",
      "Epoch 11: train loss: 1.453373908996582, val loss: 1.7668430805206299\n",
      "Epoch 12: train loss: 1.333536148071289, val loss: 1.624039888381958\n",
      "Epoch 13: train loss: 1.210031270980835, val loss: 1.479267954826355\n",
      "Epoch 14: train loss: 1.0839898586273193, val loss: 1.3588405847549438\n",
      "Epoch 15: train loss: 0.9586302042007446, val loss: 1.2486164569854736\n",
      "Epoch 16: train loss: 0.835930585861206, val loss: 1.14897620677948\n",
      "Epoch 17: train loss: 0.7183400392532349, val loss: 1.0588133335113525\n",
      "Epoch 18: train loss: 0.6073899269104004, val loss: 0.9853948950767517\n",
      "Epoch 19: train loss: 0.5048075318336487, val loss: 0.9702350497245789\n",
      "Epoch 20: train loss: 0.4115843176841736, val loss: 0.9412890672683716\n",
      "Epoch 21: train loss: 0.32842710614204407, val loss: 0.9125623106956482\n",
      "Epoch 22: train loss: 0.255758136510849, val loss: 0.888011634349823\n",
      "Epoch 23: train loss: 0.19358915090560913, val loss: 0.8690962791442871\n",
      "Epoch 24: train loss: 0.14189070463180542, val loss: 0.8562378883361816\n",
      "Epoch 25: train loss: 0.10035651922225952, val loss: 0.8485453724861145\n",
      "Epoch 26: train loss: 0.06841195374727249, val loss: 0.8439896702766418\n",
      "Epoch 27: train loss: 0.04532962664961815, val loss: 0.8413287997245789\n",
      "Epoch 28: train loss: 0.03024684078991413, val loss: 0.8397457003593445\n",
      "Epoch 29: train loss: 0.02211317978799343, val loss: 0.8384116291999817\n",
      "Epoch 30: train loss: 0.019735736772418022, val loss: 0.8362292647361755\n",
      "Epoch 31: train loss: 0.021858682855963707, val loss: 0.8320528268814087\n",
      "Epoch 32: train loss: 0.02714626118540764, val loss: 0.8264477849006653\n",
      "Epoch 33: train loss: 0.034295618534088135, val loss: 0.8198170065879822\n",
      "Epoch 34: train loss: 0.04212837666273117, val loss: 0.8121436238288879\n",
      "Epoch 35: train loss: 0.049639664590358734, val loss: 0.8034667372703552\n",
      "Epoch 36: train loss: 0.05600178986787796, val loss: 0.7938467264175415\n",
      "Epoch 37: train loss: 0.06059407442808151, val loss: 0.7832695841789246\n",
      "Epoch 38: train loss: 0.0630127340555191, val loss: 0.7718830704689026\n",
      "Epoch 39: train loss: 0.06308816373348236, val loss: 0.7599130868911743\n",
      "Epoch 40: train loss: 0.0608723983168602, val loss: 0.7476282119750977\n",
      "Epoch 41: train loss: 0.05661752074956894, val loss: 0.7354086637496948\n",
      "Epoch 42: train loss: 0.05075249820947647, val loss: 0.7236908078193665\n",
      "Epoch 43: train loss: 0.043814897537231445, val loss: 0.7126844525337219\n",
      "Epoch 44: train loss: 0.03637688234448433, val loss: 0.7026106119155884\n",
      "Epoch 45: train loss: 0.028986144810914993, val loss: 0.693602979183197\n",
      "Epoch 46: train loss: 0.022103140130639076, val loss: 0.6857330203056335\n",
      "Epoch 47: train loss: 0.01607298105955124, val loss: 0.6791623830795288\n",
      "Epoch 48: train loss: 0.011110187508165836, val loss: 0.6736966371536255\n",
      "Epoch 49: train loss: 0.007305318955332041, val loss: 0.6691423654556274\n",
      "Epoch 50: train loss: 0.004644815344363451, val loss: 0.6652479767799377\n",
      "Epoch 51: train loss: 0.0030195077415555716, val loss: 0.6617383360862732\n",
      "Epoch 52: train loss: 0.00227557634934783, val loss: 0.6583454608917236\n",
      "Epoch 53: train loss: 0.002225599018856883, val loss: 0.6548210382461548\n",
      "Epoch 54: train loss: 0.0026714603882282972, val loss: 0.6509767770767212\n",
      "Epoch 55: train loss: 0.0034197040367871523, val loss: 0.6466657519340515\n",
      "Epoch 56: train loss: 0.004294180311262608, val loss: 0.6418390274047852\n",
      "Epoch 57: train loss: 0.005145784467458725, val loss: 0.6364615559577942\n",
      "Epoch 58: train loss: 0.00585915008559823, val loss: 0.6305791139602661\n",
      "Epoch 59: train loss: 0.006355782970786095, val loss: 0.6243099570274353\n",
      "Epoch 60: train loss: 0.006594465114176273, val loss: 0.6178191304206848\n",
      "Epoch 61: train loss: 0.006568270269781351, val loss: 0.6113057732582092\n",
      "Epoch 62: train loss: 0.006298534106463194, val loss: 0.6049871444702148\n",
      "Epoch 63: train loss: 0.005827340297400951, val loss: 0.5990849733352661\n",
      "Epoch 64: train loss: 0.005208977963775396, val loss: 0.5938073992729187\n",
      "Epoch 65: train loss: 0.0045019760727882385, val loss: 0.5893388390541077\n",
      "Epoch 66: train loss: 0.0037625194527208805, val loss: 0.5858290195465088\n",
      "Epoch 67: train loss: 0.003039612667635083, val loss: 0.5833827257156372\n",
      "Epoch 68: train loss: 0.002371956128627062, val loss: 0.5820416212081909\n",
      "Epoch 69: train loss: 0.0017882795073091984, val loss: 0.581810474395752\n",
      "Epoch 70: train loss: 0.0013064933009445667, val loss: 0.5826420187950134\n",
      "Epoch 71: train loss: 0.0009345684084109962, val loss: 0.5844448208808899\n",
      "Epoch 72: train loss: 0.0006717175128869712, val loss: 0.5870880484580994\n",
      "Epoch 73: train loss: 0.0005096581298857927, val loss: 0.5904098153114319\n",
      "Epoch 74: train loss: 0.00043407955672591925, val loss: 0.5942251086235046\n",
      "Epoch 75: train loss: 0.0004263962327968329, val loss: 0.5983363389968872\n",
      "Epoch 76: train loss: 0.00046581588685512543, val loss: 0.6025426983833313\n",
      "Epoch 77: train loss: 0.0005314877489581704, val loss: 0.6066522002220154\n",
      "Epoch 78: train loss: 0.0006044820765964687, val loss: 0.6104888319969177\n",
      "Epoch 79: train loss: 0.0006694201729260385, val loss: 0.6139013767242432\n",
      "Epoch 80: train loss: 0.0007153511396609247, val loss: 0.6167700886726379\n",
      "Epoch 81: train loss: 0.000735906301997602, val loss: 0.6190074682235718\n",
      "Epoch 82: train loss: 0.0007289263303391635, val loss: 0.6205642819404602\n",
      "Epoch 83: train loss: 0.0006958349258638918, val loss: 0.6214233636856079\n",
      "Epoch 84: train loss: 0.0006402317085303366, val loss: 0.6216008067131042\n",
      "Epoch 85: train loss: 0.0005676706787198782, val loss: 0.6211388111114502\n",
      "Epoch 86: train loss: 0.0004847362288273871, val loss: 0.6201215982437134\n",
      "Epoch 87: train loss: 0.0003983337082900107, val loss: 0.6186445355415344\n",
      "Epoch 88: train loss: 0.00031502966885454953, val loss: 0.6167657971382141\n",
      "Epoch 89: train loss: 0.0002404192928224802, val loss: 0.6145820021629333\n",
      "Epoch 90: train loss: 0.00017857213970273733, val loss: 0.6121911406517029\n",
      "Epoch 91: train loss: 0.00013169689918868244, val loss: 0.6096877455711365\n",
      "Epoch 92: train loss: 0.00010009644029196352, val loss: 0.6071614623069763\n",
      "Epoch 93: train loss: 8.239559247158468e-05, val loss: 0.6046924591064453\n",
      "Epoch 94: train loss: 7.601782272104174e-05, val loss: 0.602350652217865\n",
      "Epoch 95: train loss: 7.774504774715751e-05, val loss: 0.6001952290534973\n",
      "Epoch 96: train loss: 8.426627755397931e-05, val loss: 0.5982722640037537\n",
      "Epoch 97: train loss: 9.259730722988024e-05, val loss: 0.596615195274353\n",
      "Epoch 98: train loss: 0.00010033977741841227, val loss: 0.5952436327934265\n",
      "Epoch 99: train loss: 0.0001057794361258857, val loss: 0.5941650867462158\n",
      "Epoch 100: train loss: 0.00010788217332446948, val loss: 0.5933747291564941\n",
      "Epoch 101: train loss: 0.00010623934213072062, val loss: 0.5928559303283691\n",
      "Epoch 102: train loss: 0.0001009674379019998, val loss: 0.5925835967063904\n",
      "Epoch 103: train loss: 9.26149805309251e-05, val loss: 0.5925244688987732\n",
      "Epoch 104: train loss: 8.202894969144836e-05, val loss: 0.5926405787467957\n",
      "Epoch 105: train loss: 7.022218051133677e-05, val loss: 0.592890739440918\n",
      "Epoch 106: train loss: 5.821931335958652e-05, val loss: 0.5932333469390869\n",
      "Epoch 107: train loss: 4.69245424028486e-05, val loss: 0.5936424136161804\n",
      "Epoch 108: train loss: 3.701955210999586e-05, val loss: 0.5940811038017273\n",
      "Epoch 109: train loss: 2.8923470381414518e-05, val loss: 0.594504714012146\n",
      "Epoch 110: train loss: 2.2790904040448368e-05, val loss: 0.5948891043663025\n",
      "Epoch 111: train loss: 1.8563910998636857e-05, val loss: 0.5952172875404358\n",
      "Epoch 112: train loss: 1.602537668077275e-05, val loss: 0.5954789519309998\n",
      "Epoch 113: train loss: 1.4859928342048079e-05, val loss: 0.5956692099571228\n",
      "Epoch 114: train loss: 1.4694505807710811e-05, val loss: 0.5957887768745422\n",
      "Epoch 115: train loss: 1.513843108114088e-05, val loss: 0.5958418250083923\n",
      "Epoch 116: train loss: 1.5814421203685924e-05, val loss: 0.595833957195282\n",
      "Epoch 117: train loss: 1.6391704775742255e-05, val loss: 0.5957720279693604\n",
      "Epoch 118: train loss: 1.6618861991446465e-05, val loss: 0.595663845539093\n",
      "Epoch 119: train loss: 1.6349607903975993e-05, val loss: 0.5955142378807068\n",
      "Epoch 120: train loss: 1.554664777358994e-05, val loss: 0.595329225063324\n",
      "Epoch 121: train loss: 1.4275090506998822e-05, val loss: 0.5951107740402222\n",
      "Epoch 122: train loss: 1.266386425413657e-05, val loss: 0.594862163066864\n",
      "Epoch 123: train loss: 1.088362478185445e-05, val loss: 0.5945842862129211\n",
      "Epoch 124: train loss: 9.099441740545444e-06, val loss: 0.5942781567573547\n",
      "Epoch 125: train loss: 7.447989446518477e-06, val loss: 0.5939444899559021\n",
      "Epoch 126: train loss: 6.025421953381738e-06, val loss: 0.5935844779014587\n",
      "Epoch 127: train loss: 4.8872711886360776e-06, val loss: 0.5932003259658813\n",
      "Epoch 128: train loss: 4.049496055813506e-06, val loss: 0.5927950143814087\n",
      "Epoch 129: train loss: 3.4966242310474627e-06, val loss: 0.5923721194267273\n",
      "Epoch 130: train loss: 3.188187520208885e-06, val loss: 0.5919362902641296\n",
      "Epoch 131: train loss: 3.066459157707868e-06, val loss: 0.5914925336837769\n",
      "Epoch 132: train loss: 3.0639796477771597e-06, val loss: 0.5910465717315674\n",
      "Epoch 133: train loss: 3.115769231953891e-06, val loss: 0.5906041264533997\n",
      "Epoch 134: train loss: 3.165291218465427e-06, val loss: 0.590170681476593\n",
      "Epoch 135: train loss: 3.1726622182759456e-06, val loss: 0.5897510647773743\n",
      "Epoch 136: train loss: 3.114902256129426e-06, val loss: 0.5893504023551941\n",
      "Epoch 137: train loss: 2.9841517061868217e-06, val loss: 0.5889719128608704\n",
      "Epoch 138: train loss: 2.784922344289953e-06, val loss: 0.5886193513870239\n",
      "Epoch 139: train loss: 2.5293636554124532e-06, val loss: 0.5882944464683533\n",
      "Epoch 140: train loss: 2.2335541416396154e-06, val loss: 0.5879986882209778\n",
      "Epoch 141: train loss: 1.9176184196112445e-06, val loss: 0.5877333283424377\n",
      "Epoch 142: train loss: 1.6020754856072017e-06, val loss: 0.5874966382980347\n",
      "Epoch 143: train loss: 1.307502088820911e-06, val loss: 0.5872883796691895\n",
      "Epoch 144: train loss: 1.051519234351872e-06, val loss: 0.5871053338050842\n",
      "Epoch 145: train loss: 8.459054470222327e-07, val loss: 0.5869435667991638\n",
      "Epoch 146: train loss: 6.949053954485862e-07, val loss: 0.5868001580238342\n",
      "Epoch 147: train loss: 5.953181698714616e-07, val loss: 0.586668848991394\n",
      "Epoch 148: train loss: 5.382883045967901e-07, val loss: 0.5865440368652344\n",
      "Epoch 149: train loss: 5.115028898217133e-07, val loss: 0.5864205360412598\n",
      "Epoch 150: train loss: 5.022611162530666e-07, val loss: 0.5862924456596375\n",
      "Epoch 151: train loss: 4.993963784727384e-07, val loss: 0.586154580116272\n",
      "Epoch 152: train loss: 4.94380799409555e-07, val loss: 0.5860021710395813\n",
      "Epoch 153: train loss: 4.818340357815032e-07, val loss: 0.5858325362205505\n",
      "Epoch 154: train loss: 4.5953962057865283e-07, val loss: 0.5856426954269409\n",
      "Epoch 155: train loss: 4.280997814021248e-07, val loss: 0.5854324102401733\n",
      "Epoch 156: train loss: 3.898002205460216e-07, val loss: 0.5852006077766418\n",
      "Epoch 157: train loss: 3.485179718154541e-07, val loss: 0.5849497318267822\n",
      "Epoch 158: train loss: 3.081216561895417e-07, val loss: 0.5846810936927795\n",
      "Epoch 159: train loss: 2.7191958906769287e-07, val loss: 0.5843979716300964\n",
      "Epoch 160: train loss: 2.4192985392801347e-07, val loss: 0.5841028094291687\n",
      "Epoch 161: train loss: 2.187600784964161e-07, val loss: 0.5837993621826172\n",
      "Epoch 162: train loss: 2.0201279937737127e-07, val loss: 0.5834904909133911\n",
      "Epoch 163: train loss: 1.9058457212395297e-07, val loss: 0.5831789970397949\n",
      "Epoch 164: train loss: 1.832348175412335e-07, val loss: 0.5828675627708435\n",
      "Epoch 165: train loss: 1.7877654556741618e-07, val loss: 0.5825584530830383\n",
      "Epoch 166: train loss: 1.7622440395825834e-07, val loss: 0.5822520852088928\n",
      "Epoch 167: train loss: 1.7453254486099468e-07, val loss: 0.5819514393806458\n",
      "Epoch 168: train loss: 1.7274753361107287e-07, val loss: 0.5816561579704285\n",
      "Epoch 169: train loss: 1.699238083574528e-07, val loss: 0.5813671946525574\n",
      "Epoch 170: train loss: 1.6538548663902475e-07, val loss: 0.5810849070549011\n",
      "Epoch 171: train loss: 1.5880870307682926e-07, val loss: 0.5808089375495911\n",
      "Epoch 172: train loss: 1.5010529352821322e-07, val loss: 0.5805399417877197\n",
      "Epoch 173: train loss: 1.3954806377114437e-07, val loss: 0.580277144908905\n",
      "Epoch 174: train loss: 1.279503720752473e-07, val loss: 0.5800197124481201\n",
      "Epoch 175: train loss: 1.1591152571099883e-07, val loss: 0.579767644405365\n",
      "Epoch 176: train loss: 1.0409515027731686e-07, val loss: 0.5795194506645203\n",
      "Epoch 177: train loss: 9.319789739947737e-08, val loss: 0.5792746543884277\n",
      "Epoch 178: train loss: 8.377888605082262e-08, val loss: 0.5790321230888367\n",
      "Epoch 179: train loss: 7.607240348761479e-08, val loss: 0.5787906050682068\n",
      "Epoch 180: train loss: 7.0125722118064e-08, val loss: 0.5785490274429321\n",
      "Epoch 181: train loss: 6.567360344433837e-08, val loss: 0.5783063769340515\n",
      "Epoch 182: train loss: 6.251448070315746e-08, val loss: 0.5780627131462097\n",
      "Epoch 183: train loss: 6.024512089197742e-08, val loss: 0.5778164267539978\n",
      "Epoch 184: train loss: 5.862508345444439e-08, val loss: 0.5775673985481262\n",
      "Epoch 185: train loss: 5.746117892613256e-08, val loss: 0.5773155093193054\n",
      "Epoch 186: train loss: 5.667279978638362e-08, val loss: 0.5770604014396667\n",
      "Epoch 187: train loss: 5.6284878979795394e-08, val loss: 0.5768020153045654\n",
      "Epoch 188: train loss: 5.634063526827049e-08, val loss: 0.5765406489372253\n",
      "Epoch 189: train loss: 5.679362047317227e-08, val loss: 0.5762758851051331\n",
      "Epoch 190: train loss: 5.7504678352415795e-08, val loss: 0.5760082006454468\n",
      "Epoch 191: train loss: 5.832274752037847e-08, val loss: 0.5757374167442322\n",
      "Epoch 192: train loss: 5.909382494451165e-08, val loss: 0.5754644274711609\n",
      "Epoch 193: train loss: 5.974081318527169e-08, val loss: 0.575188934803009\n",
      "Epoch 194: train loss: 6.017479137199189e-08, val loss: 0.5749116539955139\n",
      "Epoch 195: train loss: 6.039134348156949e-08, val loss: 0.5746315121650696\n",
      "Epoch 196: train loss: 6.04355676614432e-08, val loss: 0.5743502974510193\n",
      "Epoch 197: train loss: 6.024792043035632e-08, val loss: 0.5740687251091003\n",
      "Epoch 198: train loss: 5.990423090906916e-08, val loss: 0.5737872123718262\n",
      "Epoch 199: train loss: 5.947796211103196e-08, val loss: 0.5735062956809998\n",
      "Epoch 200: train loss: 5.889712895168486e-08, val loss: 0.5732261538505554\n",
      "Epoch 201: train loss: 5.825589610708448e-08, val loss: 0.5729475021362305\n",
      "Epoch 202: train loss: 5.75127714341761e-08, val loss: 0.5726699829101562\n",
      "Epoch 203: train loss: 5.6433794526355996e-08, val loss: 0.5723940134048462\n",
      "Epoch 204: train loss: 5.495114407949586e-08, val loss: 0.572118878364563\n",
      "Epoch 205: train loss: 5.362439381428885e-08, val loss: 0.5718452334403992\n",
      "Epoch 206: train loss: 5.244144674065865e-08, val loss: 0.5715723633766174\n",
      "Epoch 207: train loss: 5.14623579306317e-08, val loss: 0.5713004469871521\n",
      "Epoch 208: train loss: 5.065518848823558e-08, val loss: 0.5710282325744629\n",
      "Epoch 209: train loss: 5.0042071819689227e-08, val loss: 0.5707566738128662\n",
      "Epoch 210: train loss: 4.955028742870127e-08, val loss: 0.5704846382141113\n",
      "Epoch 211: train loss: 4.91633791455115e-08, val loss: 0.5702126622200012\n",
      "Epoch 212: train loss: 4.883457549453851e-08, val loss: 0.5699397325515747\n",
      "Epoch 213: train loss: 4.86080864448013e-08, val loss: 0.5696666836738586\n",
      "Epoch 214: train loss: 4.84782418652685e-08, val loss: 0.5693925619125366\n",
      "Epoch 215: train loss: 4.8386912254727577e-08, val loss: 0.5691177248954773\n",
      "Epoch 216: train loss: 4.835429834315619e-08, val loss: 0.5688420534133911\n",
      "Epoch 217: train loss: 4.8372168492960554e-08, val loss: 0.568565309047699\n",
      "Epoch 218: train loss: 4.8430067067783966e-08, val loss: 0.568287193775177\n",
      "Epoch 219: train loss: 4.8527155627198226e-08, val loss: 0.5680078864097595\n",
      "Epoch 220: train loss: 4.8631203952709257e-08, val loss: 0.56772780418396\n",
      "Epoch 221: train loss: 4.8726160883916236e-08, val loss: 0.5674461126327515\n",
      "Epoch 222: train loss: 4.884326187948318e-08, val loss: 0.5671631693840027\n",
      "Epoch 223: train loss: 4.8957989662312684e-08, val loss: 0.5668789744377136\n",
      "Epoch 224: train loss: 4.908444495299591e-08, val loss: 0.5665944218635559\n",
      "Epoch 225: train loss: 4.917205842502881e-08, val loss: 0.5663082599639893\n",
      "Epoch 226: train loss: 4.92111240646409e-08, val loss: 0.5660219192504883\n",
      "Epoch 227: train loss: 4.900840622212854e-08, val loss: 0.56573486328125\n",
      "Epoch 228: train loss: 4.878527093410412e-08, val loss: 0.5654462575912476\n",
      "Epoch 229: train loss: 4.854704371837215e-08, val loss: 0.5651575922966003\n",
      "Epoch 230: train loss: 4.829993471844318e-08, val loss: 0.5648671984672546\n",
      "Epoch 231: train loss: 4.805485787073849e-08, val loss: 0.5645790100097656\n",
      "Epoch 232: train loss: 4.7779739276165856e-08, val loss: 0.56429123878479\n",
      "Epoch 233: train loss: 4.751754190124302e-08, val loss: 0.5640024542808533\n",
      "Epoch 234: train loss: 4.7211962339588354e-08, val loss: 0.5637133121490479\n",
      "Epoch 235: train loss: 4.6918497531578396e-08, val loss: 0.5634234547615051\n",
      "Epoch 236: train loss: 4.661329100486e-08, val loss: 0.5631328821182251\n",
      "Epoch 237: train loss: 4.6353086702310975e-08, val loss: 0.562841534614563\n",
      "Epoch 238: train loss: 4.6094790207007463e-08, val loss: 0.5625500082969666\n",
      "Epoch 239: train loss: 4.590548741134626e-08, val loss: 0.5622574687004089\n",
      "Epoch 240: train loss: 4.5793978387109746e-08, val loss: 0.5619649291038513\n",
      "Epoch 241: train loss: 4.5718429930730053e-08, val loss: 0.5616710782051086\n",
      "Epoch 242: train loss: 4.56940725257482e-08, val loss: 0.561376690864563\n",
      "Epoch 243: train loss: 4.571159806232572e-08, val loss: 0.5610814690589905\n",
      "Epoch 244: train loss: 4.5743419718746736e-08, val loss: 0.5607854127883911\n",
      "Epoch 245: train loss: 4.5811571425247166e-08, val loss: 0.5604881644248962\n",
      "Epoch 246: train loss: 4.587801782918177e-08, val loss: 0.5601896643638611\n",
      "Epoch 247: train loss: 4.5943824744654194e-08, val loss: 0.5598902702331543\n",
      "Epoch 248: train loss: 4.6192997871230546e-08, val loss: 0.559589684009552\n",
      "Epoch 249: train loss: 4.68756624627531e-08, val loss: 0.559287965297699\n",
      "Epoch 250: train loss: 4.7566519612018965e-08, val loss: 0.5589848756790161\n",
      "Epoch 251: train loss: 4.8264773511164094e-08, val loss: 0.5586807727813721\n",
      "Epoch 252: train loss: 4.8944247765803084e-08, val loss: 0.5583757162094116\n",
      "Epoch 253: train loss: 4.9597890239283515e-08, val loss: 0.5580695271492004\n",
      "Epoch 254: train loss: 5.015741777469884e-08, val loss: 0.5577625632286072\n",
      "Epoch 255: train loss: 5.066295827305112e-08, val loss: 0.5574540495872498\n",
      "Epoch 256: train loss: 5.108975287271278e-08, val loss: 0.5571446418762207\n",
      "Epoch 257: train loss: 5.141612291481579e-08, val loss: 0.5568333864212036\n",
      "Epoch 258: train loss: 5.161730243230522e-08, val loss: 0.5565208792686462\n",
      "Epoch 259: train loss: 5.15894704733455e-08, val loss: 0.5562067031860352\n",
      "Epoch 260: train loss: 5.146391046650933e-08, val loss: 0.555891215801239\n",
      "Epoch 261: train loss: 5.131268210334383e-08, val loss: 0.5555740594863892\n",
      "Epoch 262: train loss: 5.107302314399931e-08, val loss: 0.5552545785903931\n",
      "Epoch 263: train loss: 5.077830778077441e-08, val loss: 0.5549333691596985\n",
      "Epoch 264: train loss: 5.044472217718976e-08, val loss: 0.5546109080314636\n",
      "Epoch 265: train loss: 5.007894188224782e-08, val loss: 0.5542861819267273\n",
      "Epoch 266: train loss: 4.9606924790168705e-08, val loss: 0.5539600253105164\n",
      "Epoch 267: train loss: 4.9009074132300157e-08, val loss: 0.5536319613456726\n",
      "Epoch 268: train loss: 4.856899948890714e-08, val loss: 0.5533024668693542\n",
      "Epoch 269: train loss: 4.821707477731252e-08, val loss: 0.5529731512069702\n",
      "Epoch 270: train loss: 4.797995600824834e-08, val loss: 0.5526416301727295\n",
      "Epoch 271: train loss: 4.7806398839611575e-08, val loss: 0.5523087382316589\n",
      "Epoch 272: train loss: 4.757974281233146e-08, val loss: 0.5519746541976929\n",
      "Epoch 273: train loss: 4.731480984787595e-08, val loss: 0.5516387224197388\n",
      "Epoch 274: train loss: 4.7061025298944514e-08, val loss: 0.5513014197349548\n",
      "Epoch 275: train loss: 4.6998053448987775e-08, val loss: 0.5509631037712097\n",
      "Epoch 276: train loss: 4.764229899478778e-08, val loss: 0.5506247282028198\n",
      "Epoch 277: train loss: 4.872999070926198e-08, val loss: 0.5502867698669434\n",
      "Epoch 278: train loss: 4.942706510746575e-08, val loss: 0.549949586391449\n",
      "Epoch 279: train loss: 4.961799504599185e-08, val loss: 0.5496129393577576\n",
      "Epoch 280: train loss: 4.928570263018628e-08, val loss: 0.5492761135101318\n",
      "Epoch 281: train loss: 4.8506809235959736e-08, val loss: 0.5489387512207031\n",
      "Epoch 282: train loss: 4.738210890309347e-08, val loss: 0.5485998392105103\n",
      "Epoch 283: train loss: 4.602803826969648e-08, val loss: 0.5482588410377502\n",
      "Epoch 284: train loss: 4.458225433268126e-08, val loss: 0.5479148626327515\n",
      "Epoch 285: train loss: 4.319867130675448e-08, val loss: 0.547568142414093\n",
      "Epoch 286: train loss: 4.198937730848229e-08, val loss: 0.5472177863121033\n",
      "Epoch 287: train loss: 4.134475872774601e-08, val loss: 0.5468632578849792\n",
      "Epoch 288: train loss: 4.0946499524352475e-08, val loss: 0.5465051531791687\n",
      "Epoch 289: train loss: 4.064044745177853e-08, val loss: 0.5461435914039612\n",
      "Epoch 290: train loss: 4.0506428433673136e-08, val loss: 0.5457797646522522\n",
      "Epoch 291: train loss: 4.053404367709845e-08, val loss: 0.5454132556915283\n",
      "Epoch 292: train loss: 4.0711540805205004e-08, val loss: 0.5450451374053955\n",
      "Epoch 293: train loss: 4.1034883935253674e-08, val loss: 0.5446752905845642\n",
      "Epoch 294: train loss: 4.146626153556099e-08, val loss: 0.5443036556243896\n",
      "Epoch 295: train loss: 4.1965638075680545e-08, val loss: 0.54393070936203\n",
      "Epoch 296: train loss: 4.246776441618749e-08, val loss: 0.5435553193092346\n",
      "Epoch 297: train loss: 4.295333511095123e-08, val loss: 0.5431777238845825\n",
      "Epoch 298: train loss: 4.360191141472569e-08, val loss: 0.5427989959716797\n",
      "Epoch 299: train loss: 4.4012502087298344e-08, val loss: 0.5424184799194336\n",
      "Epoch 300: train loss: 4.396130748318683e-08, val loss: 0.542035698890686\n",
      "Epoch 301: train loss: 4.3539316152418905e-08, val loss: 0.5416475534439087\n",
      "Epoch 302: train loss: 4.340435211247495e-08, val loss: 0.5412548780441284\n",
      "Epoch 303: train loss: 4.3169091412664784e-08, val loss: 0.5408574938774109\n",
      "Epoch 304: train loss: 4.287999999519343e-08, val loss: 0.5404549837112427\n",
      "Epoch 305: train loss: 4.260902386477028e-08, val loss: 0.5400502681732178\n",
      "Epoch 306: train loss: 4.232110484281293e-08, val loss: 0.5396437644958496\n",
      "Epoch 307: train loss: 4.200783365604366e-08, val loss: 0.5392335653305054\n",
      "Epoch 308: train loss: 4.179545953775232e-08, val loss: 0.5388198494911194\n",
      "Epoch 309: train loss: 4.165841716030627e-08, val loss: 0.5384027361869812\n",
      "Epoch 310: train loss: 4.1608558376537985e-08, val loss: 0.5379840135574341\n",
      "Epoch 311: train loss: 4.1472162592981476e-08, val loss: 0.5375638604164124\n",
      "Epoch 312: train loss: 4.1347796297941386e-08, val loss: 0.5371385812759399\n",
      "Epoch 313: train loss: 4.124714081399361e-08, val loss: 0.5367090106010437\n",
      "Epoch 314: train loss: 4.130382791345255e-08, val loss: 0.5362753868103027\n",
      "Epoch 315: train loss: 4.158982491730967e-08, val loss: 0.5358418822288513\n",
      "Epoch 316: train loss: 4.175840828679611e-08, val loss: 0.5354037284851074\n",
      "Epoch 317: train loss: 4.204962422704739e-08, val loss: 0.5349609851837158\n",
      "Epoch 318: train loss: 4.230933470239506e-08, val loss: 0.5345134139060974\n",
      "Epoch 319: train loss: 4.242802376097643e-08, val loss: 0.5340603590011597\n",
      "Epoch 320: train loss: 4.254308194617806e-08, val loss: 0.5336055159568787\n",
      "Epoch 321: train loss: 4.211665327602532e-08, val loss: 0.5331478118896484\n",
      "Epoch 322: train loss: 4.1697528985196186e-08, val loss: 0.5326836109161377\n",
      "Epoch 323: train loss: 4.127854325020053e-08, val loss: 0.5322128534317017\n",
      "Epoch 324: train loss: 4.0851102056649324e-08, val loss: 0.5317355990409851\n",
      "Epoch 325: train loss: 4.045061530177918e-08, val loss: 0.5312517285346985\n",
      "Epoch 326: train loss: 4.01533668537013e-08, val loss: 0.5307616591453552\n",
      "Epoch 327: train loss: 3.9892920966622114e-08, val loss: 0.5302652716636658\n",
      "Epoch 328: train loss: 3.978388463110605e-08, val loss: 0.5297672152519226\n",
      "Epoch 329: train loss: 3.969413597815219e-08, val loss: 0.5292686223983765\n",
      "Epoch 330: train loss: 3.94216250754198e-08, val loss: 0.528763473033905\n",
      "Epoch 331: train loss: 3.938563608585355e-08, val loss: 0.528251051902771\n",
      "Epoch 332: train loss: 3.941515203109702e-08, val loss: 0.5277366042137146\n",
      "Epoch 333: train loss: 3.9399534301765016e-08, val loss: 0.5272142291069031\n",
      "Epoch 334: train loss: 3.943950233065152e-08, val loss: 0.5266843438148499\n",
      "Epoch 335: train loss: 3.952590432731995e-08, val loss: 0.5261471271514893\n",
      "Epoch 336: train loss: 3.9617646052647615e-08, val loss: 0.5256037712097168\n",
      "Epoch 337: train loss: 3.979881313398437e-08, val loss: 0.5250589847564697\n",
      "Epoch 338: train loss: 3.959423366950432e-08, val loss: 0.5245072841644287\n",
      "Epoch 339: train loss: 3.950255589302287e-08, val loss: 0.5239471793174744\n",
      "Epoch 340: train loss: 3.9400962492663893e-08, val loss: 0.5233780741691589\n",
      "Epoch 341: train loss: 3.927350178400957e-08, val loss: 0.5228009223937988\n",
      "Epoch 342: train loss: 3.915185331493376e-08, val loss: 0.5222199559211731\n",
      "Epoch 343: train loss: 3.8993203332893245e-08, val loss: 0.5216295123100281\n",
      "Epoch 344: train loss: 3.88584560084837e-08, val loss: 0.5210296511650085\n",
      "Epoch 345: train loss: 3.8766774679288574e-08, val loss: 0.5204259753227234\n",
      "Epoch 346: train loss: 3.8589654138831975e-08, val loss: 0.519812285900116\n",
      "Epoch 347: train loss: 3.845199714191949e-08, val loss: 0.5191879272460938\n",
      "Epoch 348: train loss: 3.83096327993826e-08, val loss: 0.5185536742210388\n",
      "Epoch 349: train loss: 3.832861850128211e-08, val loss: 0.5179169774055481\n",
      "Epoch 350: train loss: 3.801228132260803e-08, val loss: 0.5172691345214844\n",
      "Epoch 351: train loss: 3.788675684290865e-08, val loss: 0.5166100859642029\n",
      "Epoch 352: train loss: 3.7802340813186674e-08, val loss: 0.5159398913383484\n",
      "Epoch 353: train loss: 3.795388536786959e-08, val loss: 0.5152677893638611\n",
      "Epoch 354: train loss: 3.7724209533962494e-08, val loss: 0.5145830512046814\n",
      "Epoch 355: train loss: 3.763697264957955e-08, val loss: 0.5138862133026123\n",
      "Epoch 356: train loss: 3.753051558419429e-08, val loss: 0.513177216053009\n",
      "Epoch 357: train loss: 3.743630827557354e-08, val loss: 0.5124582648277283\n",
      "Epoch 358: train loss: 3.738127318797524e-08, val loss: 0.5117374062538147\n",
      "Epoch 359: train loss: 3.70422554851757e-08, val loss: 0.511002242565155\n",
      "Epoch 360: train loss: 3.687770089300102e-08, val loss: 0.5102534294128418\n",
      "Epoch 361: train loss: 3.671735271382204e-08, val loss: 0.5094908475875854\n",
      "Epoch 362: train loss: 3.6591007557262856e-08, val loss: 0.508714497089386\n",
      "Epoch 363: train loss: 3.6502306954844244e-08, val loss: 0.5079266428947449\n",
      "Epoch 364: train loss: 3.667520331873675e-08, val loss: 0.5071377754211426\n",
      "Epoch 365: train loss: 3.6315913831685975e-08, val loss: 0.5063340067863464\n",
      "Epoch 366: train loss: 3.625778788318712e-08, val loss: 0.5055135488510132\n",
      "Epoch 367: train loss: 3.62056056246729e-08, val loss: 0.5046781897544861\n",
      "Epoch 368: train loss: 3.6164951922046384e-08, val loss: 0.5038248896598816\n",
      "Epoch 369: train loss: 3.624813516012182e-08, val loss: 0.5029631853103638\n",
      "Epoch 370: train loss: 3.604635168130699e-08, val loss: 0.5020838379859924\n",
      "Epoch 371: train loss: 3.598207598543013e-08, val loss: 0.5011891722679138\n",
      "Epoch 372: train loss: 3.5937272713226776e-08, val loss: 0.5002872347831726\n",
      "Epoch 373: train loss: 3.580258578494977e-08, val loss: 0.4993666708469391\n",
      "Epoch 374: train loss: 3.5695592259799014e-08, val loss: 0.49842730164527893\n",
      "Epoch 375: train loss: 3.558694317007394e-08, val loss: 0.49746251106262207\n",
      "Epoch 376: train loss: 3.551480887153957e-08, val loss: 0.49646469950675964\n",
      "Epoch 377: train loss: 3.544251470088966e-08, val loss: 0.4954516589641571\n",
      "Epoch 378: train loss: 3.5352531568833e-08, val loss: 0.49442312121391296\n",
      "Epoch 379: train loss: 3.522042746340048e-08, val loss: 0.4933778941631317\n",
      "Epoch 380: train loss: 3.51540272447437e-08, val loss: 0.4923110902309418\n",
      "Epoch 381: train loss: 3.5158969069470913e-08, val loss: 0.4912358224391937\n",
      "Epoch 382: train loss: 3.508150214770467e-08, val loss: 0.4901375472545624\n",
      "Epoch 383: train loss: 3.512897706059448e-08, val loss: 0.4890177845954895\n",
      "Epoch 384: train loss: 3.5522052854730646e-08, val loss: 0.48787593841552734\n",
      "Epoch 385: train loss: 3.584062469030869e-08, val loss: 0.48671475052833557\n",
      "Epoch 386: train loss: 3.605988041499586e-08, val loss: 0.4855494201183319\n",
      "Epoch 387: train loss: 3.5880315607528246e-08, val loss: 0.48436832427978516\n",
      "Epoch 388: train loss: 3.572208839841551e-08, val loss: 0.4831707179546356\n",
      "Epoch 389: train loss: 3.546316307279085e-08, val loss: 0.4819526672363281\n",
      "Epoch 390: train loss: 3.521620328683639e-08, val loss: 0.48071011900901794\n",
      "Epoch 391: train loss: 3.5458022296097624e-08, val loss: 0.4794403612613678\n",
      "Epoch 392: train loss: 3.608946386179923e-08, val loss: 0.47814011573791504\n",
      "Epoch 393: train loss: 3.679897986330616e-08, val loss: 0.476813942193985\n",
      "Epoch 394: train loss: 3.74393316349142e-08, val loss: 0.4754633903503418\n",
      "Epoch 395: train loss: 3.783218716080228e-08, val loss: 0.47409066557884216\n",
      "Epoch 396: train loss: 3.786641045167016e-08, val loss: 0.47269654273986816\n",
      "Epoch 397: train loss: 3.7542111641641895e-08, val loss: 0.47129565477371216\n",
      "Epoch 398: train loss: 3.6926024904460064e-08, val loss: 0.46986618638038635\n",
      "Epoch 399: train loss: 3.6304868444858585e-08, val loss: 0.46840620040893555\n",
      "Epoch 400: train loss: 3.575917162379483e-08, val loss: 0.4669155776500702\n",
      "Epoch 401: train loss: 3.535959081091278e-08, val loss: 0.4653949439525604\n",
      "Epoch 402: train loss: 3.504900902839836e-08, val loss: 0.46384334564208984\n",
      "Epoch 403: train loss: 3.473753551475056e-08, val loss: 0.4622627794742584\n",
      "Epoch 404: train loss: 3.4438414786563953e-08, val loss: 0.4606513977050781\n",
      "Epoch 405: train loss: 3.410960403016361e-08, val loss: 0.459012895822525\n",
      "Epoch 406: train loss: 3.386877267530508e-08, val loss: 0.45734354853630066\n",
      "Epoch 407: train loss: 3.387684088806964e-08, val loss: 0.45566198229789734\n",
      "Epoch 408: train loss: 3.3499098606171174e-08, val loss: 0.45394715666770935\n",
      "Epoch 409: train loss: 3.337234133482525e-08, val loss: 0.4521965980529785\n",
      "Epoch 410: train loss: 3.315550500815334e-08, val loss: 0.45041003823280334\n",
      "Epoch 411: train loss: 3.296269213137748e-08, val loss: 0.44858860969543457\n",
      "Epoch 412: train loss: 3.3272339550194374e-08, val loss: 0.44673439860343933\n",
      "Epoch 413: train loss: 3.371733470203253e-08, val loss: 0.44484710693359375\n",
      "Epoch 414: train loss: 3.431267359133017e-08, val loss: 0.44294866919517517\n",
      "Epoch 415: train loss: 3.399050996222286e-08, val loss: 0.44100871682167053\n",
      "Epoch 416: train loss: 3.387384950315209e-08, val loss: 0.4390251636505127\n",
      "Epoch 417: train loss: 3.3690433554056654e-08, val loss: 0.4370051324367523\n",
      "Epoch 418: train loss: 3.354963595825211e-08, val loss: 0.4349496066570282\n",
      "Epoch 419: train loss: 3.3437725477369895e-08, val loss: 0.4328606128692627\n",
      "Epoch 420: train loss: 3.3341322591695643e-08, val loss: 0.43073520064353943\n",
      "Epoch 421: train loss: 3.32403438108031e-08, val loss: 0.43024882674217224\n",
      "Epoch 422: train loss: 3.3016963385534837e-08, val loss: 0.4301168620586395\n",
      "Epoch 423: train loss: 3.2757128565208404e-08, val loss: 0.4299868047237396\n",
      "Epoch 424: train loss: 3.2504544833500404e-08, val loss: 0.4298580586910248\n",
      "Epoch 425: train loss: 3.233456880025187e-08, val loss: 0.42972898483276367\n",
      "Epoch 426: train loss: 3.219029665046946e-08, val loss: 0.42960473895072937\n",
      "Epoch 427: train loss: 3.2069600308659574e-08, val loss: 0.4294773042201996\n",
      "Epoch 428: train loss: 3.202428899840015e-08, val loss: 0.42934709787368774\n",
      "Epoch 429: train loss: 3.2185582199417695e-08, val loss: 0.4292336106300354\n",
      "Epoch 430: train loss: 3.199514253537927e-08, val loss: 0.42911335825920105\n",
      "Epoch 431: train loss: 3.1983351078679334e-08, val loss: 0.4289863705635071\n",
      "Epoch 432: train loss: 3.193343545149219e-08, val loss: 0.4288548529148102\n",
      "Epoch 433: train loss: 3.188103647744356e-08, val loss: 0.428723007440567\n",
      "Epoch 434: train loss: 3.187516739444618e-08, val loss: 0.42859306931495667\n",
      "Epoch 435: train loss: 3.190775288430814e-08, val loss: 0.42846614122390747\n",
      "Epoch 436: train loss: 3.195724573856751e-08, val loss: 0.4283458888530731\n",
      "Epoch 437: train loss: 3.181041918765004e-08, val loss: 0.42822352051734924\n",
      "Epoch 438: train loss: 3.1722613869078486e-08, val loss: 0.42809757590293884\n",
      "Epoch 439: train loss: 3.159612660397215e-08, val loss: 0.42797303199768066\n",
      "Epoch 440: train loss: 3.139090054560256e-08, val loss: 0.42784592509269714\n",
      "Epoch 441: train loss: 3.122134017985445e-08, val loss: 0.4277298152446747\n",
      "Epoch 442: train loss: 3.09969188094783e-08, val loss: 0.4276105463504791\n",
      "Epoch 443: train loss: 3.081670740812115e-08, val loss: 0.4274882972240448\n",
      "Epoch 444: train loss: 3.06866354549129e-08, val loss: 0.4273798167705536\n",
      "Epoch 445: train loss: 3.060948472466407e-08, val loss: 0.4273296296596527\n",
      "Epoch 446: train loss: 3.060575437530133e-08, val loss: 0.42728015780448914\n",
      "Epoch 447: train loss: 3.061475339904973e-08, val loss: 0.4272312819957733\n",
      "Epoch 448: train loss: 3.078989152527356e-08, val loss: 0.427182674407959\n",
      "Epoch 449: train loss: 3.072133125670007e-08, val loss: 0.42713379859924316\n",
      "Epoch 450: train loss: 3.0707063558566006e-08, val loss: 0.427083820104599\n",
      "Epoch 451: train loss: 3.066000786589029e-08, val loss: 0.42703303694725037\n",
      "Epoch 452: train loss: 3.060400999288504e-08, val loss: 0.42698144912719727\n",
      "Epoch 453: train loss: 3.054459440932078e-08, val loss: 0.42692992091178894\n",
      "Epoch 454: train loss: 3.0439046838637296e-08, val loss: 0.4268783628940582\n",
      "Epoch 455: train loss: 3.033572326671674e-08, val loss: 0.4268265664577484\n",
      "Epoch 456: train loss: 3.015058780420077e-08, val loss: 0.42677465081214905\n",
      "Epoch 457: train loss: 3.003353299391165e-08, val loss: 0.42672258615493774\n",
      "Epoch 458: train loss: 2.987590264069695e-08, val loss: 0.4266710877418518\n",
      "Epoch 459: train loss: 2.9925470101943574e-08, val loss: 0.42662081122398376\n",
      "Epoch 460: train loss: 2.9581235239106718e-08, val loss: 0.4265720844268799\n",
      "Epoch 461: train loss: 2.954684141798225e-08, val loss: 0.42652416229248047\n",
      "Epoch 462: train loss: 2.9374321641739698e-08, val loss: 0.4264768660068512\n",
      "Epoch 463: train loss: 2.9324739969638358e-08, val loss: 0.42643043398857117\n",
      "Epoch 464: train loss: 2.9336975515548147e-08, val loss: 0.42638421058654785\n",
      "Epoch 465: train loss: 2.9355076591741636e-08, val loss: 0.4263381063938141\n",
      "Epoch 466: train loss: 2.950063660023261e-08, val loss: 0.42629167437553406\n",
      "Epoch 467: train loss: 2.952919153642597e-08, val loss: 0.4262452721595764\n",
      "Epoch 468: train loss: 2.938027243715169e-08, val loss: 0.42619916796684265\n",
      "Epoch 469: train loss: 2.9151548730510513e-08, val loss: 0.42615434527397156\n",
      "Epoch 470: train loss: 2.880759630841112e-08, val loss: 0.42611002922058105\n",
      "Epoch 471: train loss: 2.866510051546811e-08, val loss: 0.42606598138809204\n",
      "Epoch 472: train loss: 2.842252833090697e-08, val loss: 0.4260222613811493\n",
      "Epoch 473: train loss: 2.8340199520471288e-08, val loss: 0.4259783923625946\n",
      "Epoch 474: train loss: 2.822823041981337e-08, val loss: 0.42593565583229065\n",
      "Epoch 475: train loss: 2.8188980039089984e-08, val loss: 0.4258953034877777\n",
      "Epoch 476: train loss: 2.8151747599736154e-08, val loss: 0.4258565902709961\n",
      "Epoch 477: train loss: 2.808097754325445e-08, val loss: 0.42602941393852234\n",
      "Epoch 478: train loss: 2.8027443477185443e-08, val loss: 0.42644044756889343\n",
      "Epoch 479: train loss: 2.7938767743762583e-08, val loss: 0.42687034606933594\n",
      "Epoch 480: train loss: 2.7997762330755904e-08, val loss: 0.42728686332702637\n",
      "Epoch 481: train loss: 2.796545750527457e-08, val loss: 0.4276794493198395\n",
      "Epoch 482: train loss: 2.7982713035612505e-08, val loss: 0.4280564785003662\n",
      "Epoch 483: train loss: 2.7808432889742107e-08, val loss: 0.42843562364578247\n",
      "Epoch 484: train loss: 2.760050143990611e-08, val loss: 0.42880359292030334\n",
      "Epoch 485: train loss: 2.738283733094704e-08, val loss: 0.42913728952407837\n",
      "Epoch 486: train loss: 2.7263640234309605e-08, val loss: 0.4294310510158539\n",
      "Epoch 487: train loss: 2.708164714704253e-08, val loss: 0.4297010898590088\n",
      "Epoch 488: train loss: 2.679447419495773e-08, val loss: 0.4299621284008026\n",
      "Epoch 489: train loss: 2.6790408114152342e-08, val loss: 0.4302101135253906\n",
      "Epoch 490: train loss: 2.642915575279403e-08, val loss: 0.43043994903564453\n",
      "Epoch 491: train loss: 2.6256827823090134e-08, val loss: 0.4306431710720062\n",
      "Epoch 492: train loss: 2.6254610929754563e-08, val loss: 0.4308246672153473\n",
      "Epoch 493: train loss: 2.6116694584743527e-08, val loss: 0.43099427223205566\n",
      "Epoch 494: train loss: 2.6281139042794166e-08, val loss: 0.4311552047729492\n",
      "Epoch 495: train loss: 2.6355861493243538e-08, val loss: 0.4313044548034668\n",
      "Epoch 496: train loss: 2.6421089316386315e-08, val loss: 0.43143749237060547\n",
      "Epoch 497: train loss: 2.6641796324611278e-08, val loss: 0.43155574798583984\n",
      "Epoch 498: train loss: 2.6687091647659145e-08, val loss: 0.43166419863700867\n",
      "Epoch 499: train loss: 2.6972026390126302e-08, val loss: 0.4317595660686493\n",
      "Epoch 500: train loss: 2.7194564822252687e-08, val loss: 0.4318457245826721\n",
      "Epoch 501: train loss: 2.7097602384174024e-08, val loss: 0.4319259822368622\n",
      "Epoch 502: train loss: 2.6878655745576907e-08, val loss: 0.4319995939731598\n",
      "Epoch 503: train loss: 2.644503460658143e-08, val loss: 0.43206921219825745\n",
      "Epoch 504: train loss: 2.63897952379466e-08, val loss: 0.4321373403072357\n",
      "Epoch 505: train loss: 2.6203959890835904e-08, val loss: 0.43219929933547974\n",
      "Epoch 506: train loss: 2.6172475742214374e-08, val loss: 0.4322536587715149\n",
      "Epoch 507: train loss: 2.6009931985981893e-08, val loss: 0.43230515718460083\n",
      "Epoch 508: train loss: 2.598462245373412e-08, val loss: 0.43235549330711365\n",
      "Epoch 509: train loss: 2.601112747413481e-08, val loss: 0.43239927291870117\n",
      "Epoch 510: train loss: 2.6124288510231963e-08, val loss: 0.4324333369731903\n",
      "Epoch 511: train loss: 2.6070342329376217e-08, val loss: 0.432462602853775\n",
      "Epoch 512: train loss: 2.594786607801325e-08, val loss: 0.4324929714202881\n",
      "Epoch 513: train loss: 2.59592027873623e-08, val loss: 0.4325244128704071\n",
      "Epoch 514: train loss: 2.5843894135846313e-08, val loss: 0.43255430459976196\n",
      "Epoch 515: train loss: 2.5972159534148886e-08, val loss: 0.43258044123649597\n",
      "Epoch 516: train loss: 2.5702449946152228e-08, val loss: 0.4326044023036957\n",
      "Epoch 517: train loss: 2.5279211612883046e-08, val loss: 0.4326228201389313\n",
      "Epoch 518: train loss: 2.5120908020426214e-08, val loss: 0.43263545632362366\n",
      "Epoch 519: train loss: 2.4843505030958113e-08, val loss: 0.4326494634151459\n",
      "Epoch 520: train loss: 2.476297922271442e-08, val loss: 0.432667076587677\n",
      "Epoch 521: train loss: 2.4633305173438202e-08, val loss: 0.4326837956905365\n",
      "Epoch 522: train loss: 2.435715273918504e-08, val loss: 0.43269893527030945\n",
      "Epoch 523: train loss: 2.419777089812669e-08, val loss: 0.4327120780944824\n",
      "Epoch 524: train loss: 2.3965368356471117e-08, val loss: 0.4327232837677002\n",
      "Epoch 525: train loss: 2.404014587398251e-08, val loss: 0.43273383378982544\n",
      "Epoch 526: train loss: 2.3727537268314336e-08, val loss: 0.4327560365200043\n",
      "Epoch 527: train loss: 2.1372491332272148e-08, val loss: 0.4327811300754547\n",
      "Epoch 528: train loss: 1.998587606522051e-08, val loss: 0.43280163407325745\n",
      "Epoch 529: train loss: 2.0027524527677087e-08, val loss: 0.43281394243240356\n",
      "Epoch 530: train loss: 2.1058632171389036e-08, val loss: 0.4328227937221527\n",
      "Epoch 531: train loss: 2.2293072277079773e-08, val loss: 0.4328315258026123\n",
      "Epoch 532: train loss: 2.2629036422472382e-08, val loss: 0.43283793330192566\n",
      "Epoch 533: train loss: 2.238202867488326e-08, val loss: 0.43283993005752563\n",
      "Epoch 534: train loss: 2.1762479818221436e-08, val loss: 0.4328420162200928\n",
      "Epoch 535: train loss: 2.1179660691927893e-08, val loss: 0.4328463673591614\n",
      "Epoch 536: train loss: 2.1082982470943534e-08, val loss: 0.4328542649745941\n",
      "Epoch 537: train loss: 2.1325758936541206e-08, val loss: 0.43286821246147156\n",
      "Epoch 538: train loss: 2.154279243882229e-08, val loss: 0.43288499116897583\n",
      "Epoch 539: train loss: 2.145843325251917e-08, val loss: 0.43290024995803833\n",
      "Epoch 540: train loss: 2.1178433229351867e-08, val loss: 0.43291163444519043\n",
      "Epoch 541: train loss: 2.0682392687376705e-08, val loss: 0.4329279363155365\n",
      "Epoch 542: train loss: 2.0232819863963414e-08, val loss: 0.4329488277435303\n",
      "Epoch 543: train loss: 1.9931873040945902e-08, val loss: 0.4329698979854584\n",
      "Epoch 544: train loss: 1.993634946018119e-08, val loss: 0.43298885226249695\n",
      "Epoch 545: train loss: 2.0055432869980905e-08, val loss: 0.4330046772956848\n",
      "Epoch 546: train loss: 2.0061923677872073e-08, val loss: 0.43302473425865173\n",
      "Epoch 547: train loss: 1.9897386849265786e-08, val loss: 0.4330507218837738\n",
      "Epoch 548: train loss: 1.9630260084113615e-08, val loss: 0.4330751597881317\n",
      "Epoch 549: train loss: 1.9451890764798918e-08, val loss: 0.4331014156341553\n",
      "Epoch 550: train loss: 1.936711946370906e-08, val loss: 0.43312200903892517\n",
      "Epoch 551: train loss: 1.9588000554904283e-08, val loss: 0.43315479159355164\n",
      "Epoch 552: train loss: 1.9794171635112434e-08, val loss: 0.4330737590789795\n",
      "Epoch 553: train loss: 2.0023835034521653e-08, val loss: 0.43297791481018066\n",
      "Epoch 554: train loss: 2.0063522399027534e-08, val loss: 0.43290072679519653\n",
      "Epoch 555: train loss: 1.999890386628067e-08, val loss: 0.4327988624572754\n",
      "Epoch 556: train loss: 1.9706082099446576e-08, val loss: 0.4327412247657776\n",
      "Epoch 557: train loss: 1.930806092786952e-08, val loss: 0.43268442153930664\n",
      "Epoch 558: train loss: 1.9052533772878633e-08, val loss: 0.43259626626968384\n",
      "Epoch 559: train loss: 1.8187190420348998e-08, val loss: 0.4325481355190277\n",
      "Epoch 560: train loss: 1.8294025849741047e-08, val loss: 0.4324983060359955\n",
      "Epoch 561: train loss: 1.8642042576288986e-08, val loss: 0.4325083792209625\n",
      "Epoch 562: train loss: 1.858522580278077e-08, val loss: 0.4324577748775482\n",
      "Epoch 563: train loss: 1.901285529015695e-08, val loss: 0.4324844479560852\n",
      "Epoch 564: train loss: 2.129212717250084e-08, val loss: 0.43230733275413513\n",
      "Epoch 565: train loss: 2.4858016089979174e-08, val loss: 0.43251487612724304\n",
      "Epoch 566: train loss: 3.2078183664907556e-08, val loss: 0.4322678744792938\n",
      "Epoch 567: train loss: 4.484853022290736e-08, val loss: 0.4326189160346985\n",
      "Epoch 568: train loss: 6.797103679900829e-08, val loss: 0.4321719706058502\n",
      "Epoch 569: train loss: 1.1881521544410134e-07, val loss: 0.43296295404434204\n",
      "Epoch 570: train loss: 2.674788674994488e-07, val loss: 0.43176576495170593\n",
      "Epoch 571: train loss: 6.63352295759978e-07, val loss: 0.4342794418334961\n",
      "Epoch 572: train loss: 1.9534031707735267e-06, val loss: 0.4305535852909088\n",
      "Epoch 573: train loss: 3.904108325514244e-06, val loss: 0.4338860511779785\n",
      "Epoch 574: train loss: 8.87934129423229e-06, val loss: 0.43009838461875916\n",
      "Epoch 575: train loss: 1.1073664609284606e-05, val loss: 0.4305829107761383\n",
      "Epoch 576: train loss: 1.1134423402836546e-05, val loss: 0.4341649115085602\n",
      "Epoch 577: train loss: 2.7511457574291853e-06, val loss: 0.43347206711769104\n",
      "Epoch 578: train loss: 1.6826347746246029e-06, val loss: 0.43145275115966797\n",
      "Epoch 579: train loss: 5.382086328609148e-06, val loss: 0.4320860803127289\n",
      "Epoch 580: train loss: 1.0530847021072987e-06, val loss: 0.4336315095424652\n",
      "Epoch 581: train loss: 2.4298271910083713e-06, val loss: 0.4350832998752594\n",
      "Epoch 582: train loss: 1.97204690266517e-06, val loss: 0.43680301308631897\n",
      "Epoch 583: train loss: 9.792503306016442e-07, val loss: 0.4384000897407532\n",
      "Epoch 584: train loss: 2.0355619199108332e-06, val loss: 0.43834590911865234\n",
      "Epoch 585: train loss: 4.580387553687615e-07, val loss: 0.43827396631240845\n",
      "Epoch 586: train loss: 1.6615039157841238e-06, val loss: 0.44098979234695435\n",
      "Epoch 587: train loss: 3.1957537771631905e-07, val loss: 0.4431382715702057\n",
      "Epoch 588: train loss: 1.2593659448612016e-06, val loss: 0.4421657621860504\n",
      "Epoch 589: train loss: 3.539656461271079e-07, val loss: 0.4430321753025055\n",
      "Epoch 590: train loss: 9.510495146969333e-07, val loss: 0.44275251030921936\n",
      "Epoch 591: train loss: 3.282662817127857e-07, val loss: 0.44167062640190125\n",
      "Epoch 592: train loss: 6.612505103475996e-07, val loss: 0.4432266354560852\n",
      "Epoch 593: train loss: 3.843982199214224e-07, val loss: 0.4441809356212616\n",
      "Epoch 594: train loss: 4.5226164502309985e-07, val loss: 0.4428693354129791\n",
      "Epoch 595: train loss: 4.055425790738809e-07, val loss: 0.4440164566040039\n",
      "Epoch 596: train loss: 2.896935598073469e-07, val loss: 0.44397374987602234\n",
      "Epoch 597: train loss: 3.665095675842167e-07, val loss: 0.4438897669315338\n",
      "Epoch 598: train loss: 2.0320135263318662e-07, val loss: 0.44395890831947327\n",
      "Epoch 599: train loss: 3.2667060168023454e-07, val loss: 0.4439900517463684\n",
      "Epoch 600: train loss: 1.5860003088619123e-07, val loss: 0.4438535273075104\n",
      "Epoch 601: train loss: 2.7124809776069014e-07, val loss: 0.44375038146972656\n",
      "Epoch 602: train loss: 1.3364321205244778e-07, val loss: 0.44363996386528015\n",
      "Epoch 603: train loss: 2.1245703862859955e-07, val loss: 0.4435346722602844\n",
      "Epoch 604: train loss: 1.434437848502057e-07, val loss: 0.44352027773857117\n",
      "Epoch 605: train loss: 1.621649516891921e-07, val loss: 0.44343051314353943\n",
      "Epoch 606: train loss: 1.2356208856090234e-07, val loss: 0.4432964026927948\n",
      "Epoch 607: train loss: 1.3272733667690773e-07, val loss: 0.4432266652584076\n",
      "Epoch 608: train loss: 1.0925728588517813e-07, val loss: 0.44310078024864197\n",
      "Epoch 609: train loss: 1.1579915337733837e-07, val loss: 0.44300898909568787\n",
      "Epoch 610: train loss: 9.059641570274835e-08, val loss: 0.44295597076416016\n",
      "Epoch 611: train loss: 9.518946342268464e-08, val loss: 0.4428585171699524\n",
      "Epoch 612: train loss: 8.66570744051387e-08, val loss: 0.44279155135154724\n",
      "Epoch 613: train loss: 7.498173459907775e-08, val loss: 0.4426555335521698\n",
      "Epoch 614: train loss: 8.430093600964028e-08, val loss: 0.4425323009490967\n",
      "Epoch 615: train loss: 5.0543164320515643e-08, val loss: 0.44248300790786743\n",
      "Epoch 616: train loss: 8.347345925585614e-08, val loss: 0.44236665964126587\n",
      "Epoch 617: train loss: 4.090308536319753e-08, val loss: 0.4422995150089264\n",
      "Epoch 618: train loss: 7.336460328133398e-08, val loss: 0.44212841987609863\n",
      "Epoch 619: train loss: 4.5075115195913895e-08, val loss: 0.4420705735683441\n",
      "Epoch 620: train loss: 5.075933628972962e-08, val loss: 0.4419388771057129\n",
      "Epoch 621: train loss: 5.2521308191444405e-08, val loss: 0.44190406799316406\n",
      "Epoch 622: train loss: 4.203173631367463e-08, val loss: 0.4417569637298584\n",
      "Epoch 623: train loss: 5.275802550386288e-08, val loss: 0.4417262077331543\n",
      "Epoch 624: train loss: 5.012591586250892e-08, val loss: 0.4415174126625061\n",
      "Epoch 625: train loss: 6.802741836509085e-08, val loss: 0.44154998660087585\n",
      "Epoch 626: train loss: 1.0285505425144947e-07, val loss: 0.4412347376346588\n",
      "Epoch 627: train loss: 2.488097550212842e-07, val loss: 0.4415597915649414\n",
      "Epoch 628: train loss: 6.475471536759869e-07, val loss: 0.44076162576675415\n",
      "Epoch 629: train loss: 2.1521816506719915e-06, val loss: 0.4419209063053131\n",
      "Epoch 630: train loss: 7.0331088863895275e-06, val loss: 0.4300973415374756\n",
      "Epoch 631: train loss: 2.3681288439547643e-05, val loss: 0.4453131854534149\n",
      "Epoch 632: train loss: 6.227350968401879e-05, val loss: 0.4280509054660797\n",
      "Epoch 633: train loss: 0.00015438898117281497, val loss: 0.43174996972084045\n",
      "Epoch 634: train loss: 7.565432315459475e-05, val loss: 0.43001610040664673\n",
      "Epoch 635: train loss: 1.194448304886464e-05, val loss: 0.43301263451576233\n",
      "Epoch 636: train loss: 5.633374166791327e-05, val loss: 0.4290964603424072\n",
      "Epoch 637: train loss: 7.511860985687235e-06, val loss: 0.43042778968811035\n",
      "Epoch 638: train loss: 3.4317694371566176e-05, val loss: 0.43105053901672363\n",
      "Epoch 639: train loss: 1.3574571312346961e-05, val loss: 0.43102002143859863\n",
      "Epoch 640: train loss: 1.5313906260416843e-05, val loss: 0.43120869994163513\n",
      "Epoch 641: train loss: 1.4956337508920114e-05, val loss: 0.43191003799438477\n",
      "Epoch 642: train loss: 6.534561634907732e-06, val loss: 0.4325566291809082\n",
      "Epoch 643: train loss: 1.2949175470566843e-05, val loss: 0.43273216485977173\n",
      "Epoch 644: train loss: 8.328730473294854e-06, val loss: 0.43247079849243164\n",
      "Epoch 645: train loss: 6.610659056605073e-06, val loss: 0.43222832679748535\n",
      "Epoch 646: train loss: 6.188039606058737e-06, val loss: 0.4323233664035797\n",
      "Epoch 647: train loss: 7.04123613104457e-06, val loss: 0.4326638877391815\n",
      "Epoch 648: train loss: 4.672078830481041e-06, val loss: 0.4329081177711487\n",
      "Epoch 649: train loss: 4.243277089699404e-06, val loss: 0.4328901767730713\n",
      "Epoch 650: train loss: 6.000503617542563e-06, val loss: 0.43278008699417114\n",
      "Epoch 651: train loss: 1.9272974896011874e-06, val loss: 0.4328511357307434\n",
      "Epoch 652: train loss: 3.6756537156179547e-06, val loss: 0.43314844369888306\n",
      "Epoch 653: train loss: 3.916858076991048e-06, val loss: 0.43346309661865234\n",
      "Epoch 654: train loss: 1.787673681974411e-06, val loss: 0.4335644841194153\n",
      "Epoch 655: train loss: 2.4800951905490365e-06, val loss: 0.43341922760009766\n",
      "Epoch 656: train loss: 2.3048598905006656e-06, val loss: 0.43320009112358093\n",
      "Epoch 657: train loss: 2.017170118051581e-06, val loss: 0.43308597803115845\n",
      "Epoch 658: train loss: 1.395030494677485e-06, val loss: 0.4330982267856598\n",
      "Epoch 659: train loss: 1.955088009708561e-06, val loss: 0.43313953280448914\n",
      "Epoch 660: train loss: 1.6718806818971643e-06, val loss: 0.43311578035354614\n",
      "Epoch 661: train loss: 6.978849569350132e-07, val loss: 0.4330112040042877\n",
      "Epoch 662: train loss: 1.6542029470656416e-06, val loss: 0.43287602066993713\n",
      "Epoch 663: train loss: 1.1153858849866083e-06, val loss: 0.432723730802536\n",
      "Epoch 664: train loss: 6.329064490273595e-07, val loss: 0.43255510926246643\n",
      "Epoch 665: train loss: 1.1915819868590916e-06, val loss: 0.43241173028945923\n",
      "Epoch 666: train loss: 7.100434231688268e-07, val loss: 0.43233558535575867\n",
      "Epoch 667: train loss: 6.65231254970422e-07, val loss: 0.4323063790798187\n",
      "Epoch 668: train loss: 7.313978471756855e-07, val loss: 0.43222466111183167\n",
      "Epoch 669: train loss: 5.567306402554095e-07, val loss: 0.4320037066936493\n",
      "Epoch 670: train loss: 5.457722522805852e-07, val loss: 0.4316849410533905\n",
      "Epoch 671: train loss: 3.5995739722238795e-07, val loss: 0.4314132332801819\n",
      "Epoch 672: train loss: 5.643656209031178e-07, val loss: 0.4312867820262909\n",
      "Epoch 673: train loss: 3.8624764897576824e-07, val loss: 0.431246280670166\n",
      "Epoch 674: train loss: 2.3392739478822477e-07, val loss: 0.43114471435546875\n",
      "Epoch 675: train loss: 5.108970526634948e-07, val loss: 0.43092161417007446\n",
      "Epoch 676: train loss: 1.4201815190517664e-07, val loss: 0.43067237734794617\n",
      "Epoch 677: train loss: 2.8155025688647584e-07, val loss: 0.43051767349243164\n",
      "Epoch 678: train loss: 3.410694944250281e-07, val loss: 0.430452823638916\n",
      "Epoch 679: train loss: 1.0831774233110991e-07, val loss: 0.43036437034606934\n",
      "Epoch 680: train loss: 2.701982566577499e-07, val loss: 0.4301753044128418\n",
      "Epoch 681: train loss: 1.51317976815335e-07, val loss: 0.42993760108947754\n",
      "Epoch 682: train loss: 1.7927972351117205e-07, val loss: 0.4297437369823456\n",
      "Epoch 683: train loss: 1.5151638876886864e-07, val loss: 0.42960429191589355\n",
      "Epoch 684: train loss: 1.7650168615546136e-07, val loss: 0.42945700883865356\n",
      "Epoch 685: train loss: 1.0000144357036334e-07, val loss: 0.42926833033561707\n",
      "Epoch 686: train loss: 1.3128880027579726e-07, val loss: 0.4290710985660553\n",
      "Epoch 687: train loss: 1.2986927799829573e-07, val loss: 0.42890664935112\n",
      "Epoch 688: train loss: 7.512203126225359e-08, val loss: 0.4287705421447754\n",
      "Epoch 689: train loss: 1.3501424689366104e-07, val loss: 0.428634375333786\n",
      "Epoch 690: train loss: 5.6359073852263464e-08, val loss: 0.4284854829311371\n",
      "Epoch 691: train loss: 9.480078944079651e-08, val loss: 0.42832469940185547\n",
      "Epoch 692: train loss: 7.419973968580962e-08, val loss: 0.4281449019908905\n",
      "Epoch 693: train loss: 7.03869034168747e-08, val loss: 0.4279499053955078\n",
      "Epoch 694: train loss: 5.4715080466394284e-08, val loss: 0.4277644157409668\n",
      "Epoch 695: train loss: 7.014732972265847e-08, val loss: 0.427603542804718\n",
      "Epoch 696: train loss: 3.271259174653096e-08, val loss: 0.42744770646095276\n",
      "Epoch 697: train loss: 6.908052796461561e-08, val loss: 0.42727571725845337\n",
      "Epoch 698: train loss: 2.9717487137759235e-08, val loss: 0.4270989000797272\n",
      "Epoch 699: train loss: 4.902153705188539e-08, val loss: 0.4269386827945709\n",
      "Epoch 700: train loss: 3.8965595194895286e-08, val loss: 0.42679062485694885\n",
      "Epoch 701: train loss: 3.4978047125378e-08, val loss: 0.4266378879547119\n",
      "Epoch 702: train loss: 4.1899294700442624e-08, val loss: 0.4264819324016571\n",
      "Epoch 703: train loss: 2.9878165719310346e-08, val loss: 0.42633548378944397\n",
      "Epoch 704: train loss: 3.310737284323295e-08, val loss: 0.4261960983276367\n",
      "Epoch 705: train loss: 3.204437959425377e-08, val loss: 0.4260527789592743\n",
      "Epoch 706: train loss: 2.371425189551246e-08, val loss: 0.4259040355682373\n",
      "Epoch 707: train loss: 3.3190442394470665e-08, val loss: 0.42575129866600037\n",
      "Epoch 708: train loss: 2.0273651202273868e-08, val loss: 0.42559322714805603\n",
      "Epoch 709: train loss: 3.1216142559742366e-08, val loss: 0.42544037103652954\n",
      "Epoch 710: train loss: 1.9696285491477283e-08, val loss: 0.4253050982952118\n",
      "Epoch 711: train loss: 2.6604453751133406e-08, val loss: 0.42516955733299255\n",
      "Epoch 712: train loss: 2.137200993956867e-08, val loss: 0.42501091957092285\n",
      "Epoch 713: train loss: 2.1588885346091047e-08, val loss: 0.42484912276268005\n",
      "Epoch 714: train loss: 2.0857292781784054e-08, val loss: 0.42472153902053833\n",
      "Epoch 715: train loss: 1.7777193050960705e-08, val loss: 0.4246048033237457\n",
      "Epoch 716: train loss: 2.0797244815184968e-08, val loss: 0.42446693778038025\n",
      "Epoch 717: train loss: 1.553115680508199e-08, val loss: 0.4243309497833252\n",
      "Epoch 718: train loss: 1.9800364015054583e-08, val loss: 0.42421838641166687\n",
      "Epoch 719: train loss: 1.5798150343471207e-08, val loss: 0.42410311102867126\n",
      "Epoch 720: train loss: 1.5517882090421153e-08, val loss: 0.42396846413612366\n",
      "Epoch 721: train loss: 1.659715032076292e-08, val loss: 0.4238419532775879\n",
      "Epoch 722: train loss: 1.504397317830808e-08, val loss: 0.42373552918434143\n",
      "Epoch 723: train loss: 1.3710222290796992e-08, val loss: 0.423627108335495\n",
      "Epoch 724: train loss: 1.673511107469494e-08, val loss: 0.42350563406944275\n",
      "Epoch 725: train loss: 1.2714645336586727e-08, val loss: 0.4233846366405487\n",
      "Epoch 726: train loss: 1.590091613934419e-08, val loss: 0.4232800602912903\n",
      "Epoch 727: train loss: 1.3494226180910118e-08, val loss: 0.4231758713722229\n",
      "Epoch 728: train loss: 1.3519233732495195e-08, val loss: 0.4230555593967438\n",
      "Epoch 729: train loss: 1.3510635277214078e-08, val loss: 0.4229401648044586\n",
      "Epoch 730: train loss: 1.4751337040763701e-08, val loss: 0.4228442311286926\n",
      "Epoch 731: train loss: 1.3477924554194942e-08, val loss: 0.42273950576782227\n",
      "Epoch 732: train loss: 1.3628113748609394e-08, val loss: 0.42262744903564453\n",
      "Epoch 733: train loss: 1.4470892040208128e-08, val loss: 0.42253533005714417\n",
      "Epoch 734: train loss: 1.2608462718333158e-08, val loss: 0.42244455218315125\n",
      "Epoch 735: train loss: 1.3144502375439515e-08, val loss: 0.4223342835903168\n",
      "Epoch 736: train loss: 1.3027448453328816e-08, val loss: 0.42223426699638367\n",
      "Epoch 737: train loss: 1.3015870159449605e-08, val loss: 0.4221526086330414\n",
      "Epoch 738: train loss: 1.2733388565777659e-08, val loss: 0.42206498980522156\n",
      "Epoch 739: train loss: 1.3270081034022496e-08, val loss: 0.4219686985015869\n",
      "Epoch 740: train loss: 1.654736259126821e-08, val loss: 0.4218871295452118\n",
      "Epoch 741: train loss: 2.8128564366625142e-08, val loss: 0.4218026101589203\n",
      "Epoch 742: train loss: 7.247562194834245e-08, val loss: 0.4217248857021332\n",
      "Epoch 743: train loss: 2.8022341780342686e-07, val loss: 0.4216403067111969\n",
      "Epoch 744: train loss: 1.2746946822517202e-06, val loss: 0.42158088088035583\n",
      "Epoch 745: train loss: 6.5179765442735516e-06, val loss: 0.4214065670967102\n",
      "Epoch 746: train loss: 1.8471962903277017e-05, val loss: 0.42109328508377075\n",
      "Epoch 747: train loss: 9.76802948571276e-06, val loss: 0.4210704267024994\n",
      "Epoch 748: train loss: 4.146846549701877e-06, val loss: 0.4205474555492401\n",
      "Epoch 749: train loss: 9.142188901023474e-06, val loss: 0.41996821761131287\n",
      "Epoch 750: train loss: 9.70663495536428e-06, val loss: 0.42030858993530273\n",
      "Epoch 751: train loss: 3.6309402275946923e-06, val loss: 0.4200915992259979\n",
      "Epoch 752: train loss: 3.99462351197144e-06, val loss: 0.41999855637550354\n",
      "Epoch 753: train loss: 9.513003533356823e-06, val loss: 0.41994473338127136\n",
      "Epoch 754: train loss: 6.796203706471715e-06, val loss: 0.4201976954936981\n",
      "Epoch 755: train loss: 1.6639804698570515e-06, val loss: 0.4200321137905121\n",
      "Epoch 756: train loss: 3.5081075111520477e-06, val loss: 0.41956862807273865\n",
      "Epoch 757: train loss: 6.3261172726925e-06, val loss: 0.4193890690803528\n",
      "Epoch 758: train loss: 3.808390601989231e-06, val loss: 0.4191664755344391\n",
      "Epoch 759: train loss: 9.76482397163636e-07, val loss: 0.4189518988132477\n",
      "Epoch 760: train loss: 2.8651086267927894e-06, val loss: 0.419058233499527\n",
      "Epoch 761: train loss: 3.9670535443292465e-06, val loss: 0.4191957116127014\n",
      "Epoch 762: train loss: 1.9697756670211675e-06, val loss: 0.4190882742404938\n",
      "Epoch 763: train loss: 1.1106359352197614e-06, val loss: 0.41925016045570374\n",
      "Epoch 764: train loss: 1.984123855436337e-06, val loss: 0.41884586215019226\n",
      "Epoch 765: train loss: 2.4150490389729384e-06, val loss: 0.4187667965888977\n",
      "Epoch 766: train loss: 1.1978640941379126e-06, val loss: 0.4187600612640381\n",
      "Epoch 767: train loss: 6.961490726098418e-07, val loss: 0.4185672402381897\n",
      "Epoch 768: train loss: 1.579830382070213e-06, val loss: 0.41858264803886414\n",
      "Epoch 769: train loss: 1.5251584954967257e-06, val loss: 0.4185124933719635\n",
      "Epoch 770: train loss: 4.973987302037131e-07, val loss: 0.4185224175453186\n",
      "Epoch 771: train loss: 8.223641430049611e-07, val loss: 0.4186061918735504\n",
      "Epoch 772: train loss: 1.1523165994731244e-06, val loss: 0.41831278800964355\n",
      "Epoch 773: train loss: 7.516649702665745e-07, val loss: 0.41837573051452637\n",
      "Epoch 774: train loss: 3.93850172031307e-07, val loss: 0.4182557165622711\n",
      "Epoch 775: train loss: 6.574449002982874e-07, val loss: 0.417994886636734\n",
      "Epoch 776: train loss: 9.518540764474892e-07, val loss: 0.4179804027080536\n",
      "Epoch 777: train loss: 4.945554223922954e-07, val loss: 0.41793057322502136\n",
      "Epoch 778: train loss: 1.9886579138983507e-07, val loss: 0.4179251790046692\n",
      "Epoch 779: train loss: 4.570594569486275e-07, val loss: 0.41798773407936096\n",
      "Epoch 780: train loss: 6.261323051148793e-07, val loss: 0.4177829325199127\n",
      "Epoch 781: train loss: 3.213595789475221e-07, val loss: 0.4177696704864502\n",
      "Epoch 782: train loss: 2.536355339088914e-07, val loss: 0.4176057279109955\n",
      "Epoch 783: train loss: 4.412460157254827e-07, val loss: 0.4175301194190979\n",
      "Epoch 784: train loss: 3.9195276713144267e-07, val loss: 0.41748857498168945\n",
      "Epoch 785: train loss: 4.180639621154114e-07, val loss: 0.41740766167640686\n",
      "Epoch 786: train loss: 8.69122231961228e-07, val loss: 0.41738197207450867\n",
      "Epoch 787: train loss: 1.7970821772905765e-06, val loss: 0.41729551553726196\n",
      "Epoch 788: train loss: 3.871960871038027e-06, val loss: 0.4163667857646942\n",
      "Epoch 789: train loss: 1.0223174285783898e-05, val loss: 0.41728517413139343\n",
      "Epoch 790: train loss: 2.7853186111315154e-05, val loss: 0.41589123010635376\n",
      "Epoch 791: train loss: 7.48587553971447e-05, val loss: 0.4178003966808319\n",
      "Epoch 792: train loss: 8.753169822739437e-05, val loss: 0.41603729128837585\n",
      "Epoch 793: train loss: 4.920820356346667e-05, val loss: 0.41823574900627136\n",
      "Epoch 794: train loss: 2.3095059077604674e-05, val loss: 0.41912347078323364\n",
      "Epoch 795: train loss: 3.847931293421425e-05, val loss: 0.4143246114253998\n",
      "Epoch 796: train loss: 2.1659665435436182e-05, val loss: 0.4104776084423065\n",
      "Epoch 797: train loss: 1.993084151763469e-05, val loss: 0.4094148278236389\n",
      "Epoch 798: train loss: 1.7184367607114837e-05, val loss: 0.407865434885025\n",
      "Epoch 799: train loss: 1.6480353224324062e-05, val loss: 0.40513452887535095\n",
      "Epoch 800: train loss: 1.0488545740372501e-05, val loss: 0.40346288681030273\n",
      "Epoch 801: train loss: 1.1986210665781982e-05, val loss: 0.40304192900657654\n",
      "Epoch 802: train loss: 9.132443665293977e-06, val loss: 0.4030833840370178\n",
      "Epoch 803: train loss: 8.567532859160565e-06, val loss: 0.40287619829177856\n",
      "Epoch 804: train loss: 6.635679255850846e-06, val loss: 0.40208908915519714\n",
      "Epoch 805: train loss: 6.12758003626368e-06, val loss: 0.4014225900173187\n",
      "Epoch 806: train loss: 6.0846696214866824e-06, val loss: 0.40133410692214966\n",
      "Epoch 807: train loss: 3.907139216607902e-06, val loss: 0.40170541405677795\n",
      "Epoch 808: train loss: 5.659873750119004e-06, val loss: 0.40215978026390076\n",
      "Epoch 809: train loss: 2.7730104648071574e-06, val loss: 0.40235263109207153\n",
      "Epoch 810: train loss: 3.956434284191346e-06, val loss: 0.40193554759025574\n",
      "Epoch 811: train loss: 3.1520528409600956e-06, val loss: 0.40204140543937683\n",
      "Epoch 812: train loss: 2.430169615763589e-06, val loss: 0.40222102403640747\n",
      "Epoch 813: train loss: 2.639540980453603e-06, val loss: 0.4025229513645172\n",
      "Epoch 814: train loss: 2.362713985348819e-06, val loss: 0.40193596482276917\n",
      "Epoch 815: train loss: 1.4069786402615136e-06, val loss: 0.40237709879875183\n",
      "Epoch 816: train loss: 2.1001415007049218e-06, val loss: 0.4024359881877899\n",
      "Epoch 817: train loss: 1.8190127093475894e-06, val loss: 0.4045751690864563\n",
      "Epoch 818: train loss: 7.36743345441937e-07, val loss: 0.40600988268852234\n",
      "Epoch 819: train loss: 1.9153392258886015e-06, val loss: 0.4052111804485321\n",
      "Epoch 820: train loss: 8.216210289901937e-07, val loss: 0.40460607409477234\n",
      "Epoch 821: train loss: 8.513824809597281e-07, val loss: 0.40614691376686096\n",
      "Epoch 822: train loss: 1.3641512168760528e-06, val loss: 0.4088159501552582\n",
      "Epoch 823: train loss: 4.376799154215405e-07, val loss: 0.4101872444152832\n",
      "Epoch 824: train loss: 8.414676813117694e-07, val loss: 0.4096863865852356\n",
      "Epoch 825: train loss: 7.594783824060869e-07, val loss: 0.4081502854824066\n",
      "Epoch 826: train loss: 4.3748758571382496e-07, val loss: 0.4072590470314026\n",
      "Epoch 827: train loss: 6.610358695979812e-07, val loss: 0.40759333968162537\n",
      "Epoch 828: train loss: 5.25488871971902e-07, val loss: 0.40847212076187134\n",
      "Epoch 829: train loss: 3.4395779380247404e-07, val loss: 0.40875712037086487\n",
      "Epoch 830: train loss: 5.190778438191046e-07, val loss: 0.40857353806495667\n",
      "Epoch 831: train loss: 3.2674952876732277e-07, val loss: 0.4087725579738617\n",
      "Epoch 832: train loss: 3.1375640219266643e-07, val loss: 0.408905953168869\n",
      "Epoch 833: train loss: 3.781295845328714e-07, val loss: 0.4092850387096405\n",
      "Epoch 834: train loss: 2.0882124829313398e-07, val loss: 0.40988245606422424\n",
      "Epoch 835: train loss: 2.977318445118726e-07, val loss: 0.4103558659553528\n",
      "Epoch 836: train loss: 2.2870133875585452e-07, val loss: 0.41062673926353455\n",
      "Epoch 837: train loss: 2.0233120778812008e-07, val loss: 0.4110212028026581\n",
      "Epoch 838: train loss: 2.1143439710158418e-07, val loss: 0.41167908906936646\n",
      "Epoch 839: train loss: 1.6196614183172642e-07, val loss: 0.41224464774131775\n",
      "Epoch 840: train loss: 1.616911049495684e-07, val loss: 0.4124513566493988\n",
      "Epoch 841: train loss: 1.465842416337182e-07, val loss: 0.4125935137271881\n",
      "Epoch 842: train loss: 1.2552621342365455e-07, val loss: 0.41302400827407837\n",
      "Epoch 843: train loss: 1.1128256005576986e-07, val loss: 0.41319963335990906\n",
      "Epoch 844: train loss: 1.1129602484061252e-07, val loss: 0.4131782650947571\n",
      "Epoch 845: train loss: 1.0044598042213693e-07, val loss: 0.41308316588401794\n",
      "Epoch 846: train loss: 9.175568038699566e-08, val loss: 0.413056343793869\n",
      "Epoch 847: train loss: 6.3884428413985e-08, val loss: 0.41308942437171936\n",
      "Epoch 848: train loss: 9.882854357101678e-08, val loss: 0.4130810797214508\n",
      "Epoch 849: train loss: 4.9011003255827745e-08, val loss: 0.41303712129592896\n",
      "Epoch 850: train loss: 8.598453860031441e-08, val loss: 0.4130233824253082\n",
      "Epoch 851: train loss: 4.272275333505604e-08, val loss: 0.4130316376686096\n",
      "Epoch 852: train loss: 5.902994715256682e-08, val loss: 0.4129926860332489\n",
      "Epoch 853: train loss: 4.9639801602552325e-08, val loss: 0.4129365086555481\n",
      "Epoch 854: train loss: 4.030068723182012e-08, val loss: 0.41291937232017517\n",
      "Epoch 855: train loss: 5.106468847770884e-08, val loss: 0.4129186272621155\n",
      "Epoch 856: train loss: 2.673310639522697e-08, val loss: 0.41291186213493347\n",
      "Epoch 857: train loss: 4.71332235463251e-08, val loss: 0.4129337966442108\n",
      "Epoch 858: train loss: 2.496894957459972e-08, val loss: 0.41295185685157776\n",
      "Epoch 859: train loss: 3.726931296910152e-08, val loss: 0.41291290521621704\n",
      "Epoch 860: train loss: 2.545637300954695e-08, val loss: 0.41287872195243835\n",
      "Epoch 861: train loss: 2.869356841017634e-08, val loss: 0.41293883323669434\n",
      "Epoch 862: train loss: 2.3166576212929613e-08, val loss: 0.41299954056739807\n",
      "Epoch 863: train loss: 2.6117824347693386e-08, val loss: 0.41295304894447327\n",
      "Epoch 864: train loss: 1.6645028466655276e-08, val loss: 0.41291341185569763\n",
      "Epoch 865: train loss: 2.501132634336045e-08, val loss: 0.41299009323120117\n",
      "Epoch 866: train loss: 1.2812577665499703e-08, val loss: 0.413053423166275\n",
      "Epoch 867: train loss: 2.0547266998960367e-08, val loss: 0.4130081236362457\n",
      "Epoch 868: train loss: 1.433647689452755e-08, val loss: 0.4129999577999115\n",
      "Epoch 869: train loss: 1.629258683522039e-08, val loss: 0.4130805432796478\n",
      "Epoch 870: train loss: 1.4480100674063578e-08, val loss: 0.41309306025505066\n",
      "Epoch 871: train loss: 1.337364707865163e-08, val loss: 0.4130667746067047\n",
      "Epoch 872: train loss: 1.4871250897385835e-08, val loss: 0.41313859820365906\n",
      "Epoch 873: train loss: 9.001500878014213e-09, val loss: 0.41318750381469727\n",
      "Epoch 874: train loss: 1.410119043754321e-08, val loss: 0.4131492078304291\n",
      "Epoch 875: train loss: 1.015851580632443e-08, val loss: 0.4130212962627411\n",
      "Epoch 876: train loss: 1.1325156812347359e-08, val loss: 0.4129776060581207\n",
      "Epoch 877: train loss: 1.1258463494812077e-08, val loss: 0.41285258531570435\n",
      "Epoch 878: train loss: 9.375241916131927e-09, val loss: 0.4127836227416992\n",
      "Epoch 879: train loss: 9.663510880386639e-09, val loss: 0.41285863518714905\n",
      "Epoch 880: train loss: 9.84382531044048e-09, val loss: 0.41284018754959106\n",
      "Epoch 881: train loss: 9.41625266648316e-09, val loss: 0.41282206773757935\n",
      "Epoch 882: train loss: 1.0225875257674488e-08, val loss: 0.41288453340530396\n",
      "Epoch 883: train loss: 1.0345583056903251e-08, val loss: 0.41290122270584106\n",
      "Epoch 884: train loss: 1.0472390954419097e-08, val loss: 0.4129514694213867\n",
      "Epoch 885: train loss: 1.4003669335238556e-08, val loss: 0.41300222277641296\n",
      "Epoch 886: train loss: 1.7560978449182585e-08, val loss: 0.413055419921875\n",
      "Epoch 887: train loss: 3.415298621689544e-08, val loss: 0.41309863328933716\n",
      "Epoch 888: train loss: 8.629082515199116e-08, val loss: 0.4131966531276703\n",
      "Epoch 889: train loss: 2.7135737923345005e-07, val loss: 0.41323065757751465\n",
      "Epoch 890: train loss: 1.000457245936559e-06, val loss: 0.4133739173412323\n",
      "Epoch 891: train loss: 4.15969861933263e-06, val loss: 0.4133738577365875\n",
      "Epoch 892: train loss: 1.889469422167167e-05, val loss: 0.41385212540626526\n",
      "Epoch 893: train loss: 7.713869126746431e-05, val loss: 0.40973177552223206\n",
      "Epoch 894: train loss: 8.551133214496076e-05, val loss: 0.4100988805294037\n",
      "Epoch 895: train loss: 1.7783251678338274e-05, val loss: 0.41740402579307556\n",
      "Epoch 896: train loss: 4.970376176061109e-05, val loss: 0.41378793120384216\n",
      "Epoch 897: train loss: 3.103167182416655e-05, val loss: 0.4229559004306793\n",
      "Epoch 898: train loss: 3.27606285281945e-05, val loss: 0.42753908038139343\n",
      "Epoch 899: train loss: 2.396089257672429e-05, val loss: 0.4348401725292206\n",
      "Epoch 900: train loss: 2.1507976271095686e-05, val loss: 0.4361424148082733\n",
      "Epoch 901: train loss: 1.1192112651770003e-05, val loss: 0.431033194065094\n",
      "Epoch 902: train loss: 2.1212936189840548e-05, val loss: 0.430824339389801\n",
      "Epoch 903: train loss: 7.260579423018498e-06, val loss: 0.4317033886909485\n",
      "Epoch 904: train loss: 1.2582866474986076e-05, val loss: 0.4312015473842621\n",
      "Epoch 905: train loss: 1.0728654160629958e-05, val loss: 0.43040433526039124\n",
      "Epoch 906: train loss: 5.7956258388003334e-06, val loss: 0.43008366227149963\n",
      "Epoch 907: train loss: 8.64562298374949e-06, val loss: 0.4303087294101715\n",
      "Epoch 908: train loss: 5.557139047596138e-06, val loss: 0.43065720796585083\n",
      "Epoch 909: train loss: 5.753746791015146e-06, val loss: 0.43064966797828674\n",
      "Epoch 910: train loss: 4.872189492743928e-06, val loss: 0.4304240345954895\n",
      "Epoch 911: train loss: 3.998772626800928e-06, val loss: 0.4302034378051758\n",
      "Epoch 912: train loss: 4.492637344810646e-06, val loss: 0.4304233491420746\n",
      "Epoch 913: train loss: 2.7344096906745108e-06, val loss: 0.43134331703186035\n",
      "Epoch 914: train loss: 3.4059696645272197e-06, val loss: 0.4323018491268158\n",
      "Epoch 915: train loss: 2.607200030979584e-06, val loss: 0.43282848596572876\n",
      "Epoch 916: train loss: 2.475775545462966e-06, val loss: 0.4328414499759674\n",
      "Epoch 917: train loss: 2.4234327611338813e-06, val loss: 0.4325573146343231\n",
      "Epoch 918: train loss: 1.6445590063085547e-06, val loss: 0.4323689043521881\n",
      "Epoch 919: train loss: 2.051709770967136e-06, val loss: 0.43257448077201843\n",
      "Epoch 920: train loss: 1.4007691788719967e-06, val loss: 0.4331187307834625\n",
      "Epoch 921: train loss: 1.3442764839055599e-06, val loss: 0.4336867034435272\n",
      "Epoch 922: train loss: 1.5580968693029718e-06, val loss: 0.4339326322078705\n",
      "Epoch 923: train loss: 9.406005574419396e-07, val loss: 0.433709055185318\n",
      "Epoch 924: train loss: 1.0456828931637574e-06, val loss: 0.433359295129776\n",
      "Epoch 925: train loss: 1.168711264654121e-06, val loss: 0.43349137902259827\n",
      "Epoch 926: train loss: 5.679488026544277e-07, val loss: 0.43416547775268555\n",
      "Epoch 927: train loss: 9.164468792732805e-07, val loss: 0.43470311164855957\n",
      "Epoch 928: train loss: 7.395970555990061e-07, val loss: 0.43456679582595825\n",
      "Epoch 929: train loss: 5.133738909535168e-07, val loss: 0.43401041626930237\n",
      "Epoch 930: train loss: 6.559956204910122e-07, val loss: 0.4336976110935211\n",
      "Epoch 931: train loss: 4.910360189569474e-07, val loss: 0.43385010957717896\n",
      "Epoch 932: train loss: 4.3797507487397525e-07, val loss: 0.43426552414894104\n",
      "Epoch 933: train loss: 4.7447693418689596e-07, val loss: 0.43468496203422546\n",
      "Epoch 934: train loss: 3.387919491615321e-07, val loss: 0.43478748202323914\n",
      "Epoch 935: train loss: 3.962572634463868e-07, val loss: 0.43447980284690857\n",
      "Epoch 936: train loss: 2.6275461095792707e-07, val loss: 0.43419337272644043\n",
      "Epoch 937: train loss: 3.2637390745549055e-07, val loss: 0.43424445390701294\n",
      "Epoch 938: train loss: 2.3162334628068493e-07, val loss: 0.4343368113040924\n",
      "Epoch 939: train loss: 2.439782349483721e-07, val loss: 0.43420788645744324\n",
      "Epoch 940: train loss: 2.200021498310889e-07, val loss: 0.4341491758823395\n",
      "Epoch 941: train loss: 1.6036166528010654e-07, val loss: 0.4342976212501526\n",
      "Epoch 942: train loss: 2.0736374040097871e-07, val loss: 0.4343642294406891\n",
      "Epoch 943: train loss: 1.396122257801835e-07, val loss: 0.43422070145606995\n",
      "Epoch 944: train loss: 1.5549461807040643e-07, val loss: 0.4339991509914398\n",
      "Epoch 945: train loss: 1.3439669999115722e-07, val loss: 0.43375363945961\n",
      "Epoch 946: train loss: 1.2060388598911231e-07, val loss: 0.4337775409221649\n",
      "Epoch 947: train loss: 1.0912246750649501e-07, val loss: 0.43412524461746216\n",
      "Epoch 948: train loss: 1.0318044019186345e-07, val loss: 0.43402576446533203\n",
      "Epoch 949: train loss: 9.356989494335721e-08, val loss: 0.43340322375297546\n",
      "Epoch 950: train loss: 7.735842189049436e-08, val loss: 0.4331730008125305\n",
      "Epoch 951: train loss: 8.86859226056913e-08, val loss: 0.43349942564964294\n",
      "Epoch 952: train loss: 5.591293472662073e-08, val loss: 0.4335795044898987\n",
      "Epoch 953: train loss: 7.905838117494568e-08, val loss: 0.43329378962516785\n",
      "Epoch 954: train loss: 4.9326882134437255e-08, val loss: 0.43282565474510193\n",
      "Epoch 955: train loss: 6.790170203885282e-08, val loss: 0.4324292242527008\n",
      "Epoch 956: train loss: 3.968656159258899e-08, val loss: 0.43254444003105164\n",
      "Epoch 957: train loss: 6.004782449053891e-08, val loss: 0.4324532449245453\n",
      "Epoch 958: train loss: 2.8201975865727036e-08, val loss: 0.43207016587257385\n",
      "Epoch 959: train loss: 5.133357561248886e-08, val loss: 0.43190017342567444\n",
      "Epoch 960: train loss: 3.01971034843973e-08, val loss: 0.43153879046440125\n",
      "Epoch 961: train loss: 3.694605865689482e-08, val loss: 0.43108582496643066\n",
      "Epoch 962: train loss: 3.1179393289448853e-08, val loss: 0.43060263991355896\n",
      "Epoch 963: train loss: 2.775742480309873e-08, val loss: 0.43028971552848816\n",
      "Epoch 964: train loss: 2.989566283417844e-08, val loss: 0.4300564229488373\n",
      "Epoch 965: train loss: 2.4461709102752138e-08, val loss: 0.4292996823787689\n",
      "Epoch 966: train loss: 2.5049285312661596e-08, val loss: 0.4289465546607971\n",
      "Epoch 967: train loss: 1.8019145286984894e-08, val loss: 0.42868661880493164\n",
      "Epoch 968: train loss: 2.377559127353379e-08, val loss: 0.42834940552711487\n",
      "Epoch 969: train loss: 1.5066207836866852e-08, val loss: 0.4276459217071533\n",
      "Epoch 970: train loss: 2.2698429802403552e-08, val loss: 0.4272150695323944\n",
      "Epoch 971: train loss: 1.585210185339747e-08, val loss: 0.42661818861961365\n",
      "Epoch 972: train loss: 1.836282415013102e-08, val loss: 0.4261960983276367\n",
      "Epoch 973: train loss: 1.7124696327641686e-08, val loss: 0.42563554644584656\n",
      "Epoch 974: train loss: 1.9476392054684766e-08, val loss: 0.42582592368125916\n",
      "Epoch 975: train loss: 3.432042561257731e-08, val loss: 0.4253489673137665\n",
      "Epoch 976: train loss: 8.927685257731355e-08, val loss: 0.4258786141872406\n",
      "Epoch 977: train loss: 3.3253536457777955e-07, val loss: 0.4242885112762451\n",
      "Epoch 978: train loss: 1.5228214351736824e-06, val loss: 0.42813625931739807\n",
      "Epoch 979: train loss: 8.101507773972116e-06, val loss: 0.42139625549316406\n",
      "Epoch 980: train loss: 4.4170628825668246e-05, val loss: 0.4483869671821594\n",
      "Epoch 981: train loss: 0.00017299952742177993, val loss: 0.4187777638435364\n",
      "Epoch 982: train loss: 0.00014825786638539284, val loss: 0.44595637917518616\n",
      "Epoch 983: train loss: 6.307811418082565e-05, val loss: 0.4490818977355957\n",
      "Epoch 984: train loss: 7.23916818969883e-05, val loss: 0.440630167722702\n",
      "Epoch 985: train loss: 3.409684723010287e-05, val loss: 0.4388551414012909\n",
      "Epoch 986: train loss: 4.615716534317471e-05, val loss: 0.4412696063518524\n",
      "Epoch 987: train loss: 2.7410931579652242e-05, val loss: 0.4432927072048187\n",
      "Epoch 988: train loss: 3.2915188057813793e-05, val loss: 0.4402402937412262\n",
      "Epoch 989: train loss: 2.102487997035496e-05, val loss: 0.4392409324645996\n",
      "Epoch 990: train loss: 1.8846525563276373e-05, val loss: 0.44003936648368835\n",
      "Epoch 991: train loss: 1.9202567273168825e-05, val loss: 0.44171810150146484\n",
      "Epoch 992: train loss: 1.524512845207937e-05, val loss: 0.4428578317165375\n",
      "Epoch 993: train loss: 1.0076250873680692e-05, val loss: 0.44245991110801697\n",
      "Epoch 994: train loss: 1.3931675312051084e-05, val loss: 0.44117996096611023\n",
      "Epoch 995: train loss: 1.0117840247403365e-05, val loss: 0.44016414880752563\n",
      "Epoch 996: train loss: 7.751626071694773e-06, val loss: 0.44003477692604065\n",
      "Epoch 997: train loss: 8.49763546284521e-06, val loss: 0.44059592485427856\n",
      "Epoch 998: train loss: 7.392833140329458e-06, val loss: 0.4412912428379059\n",
      "Epoch 999: train loss: 4.803185674973065e-06, val loss: 0.4418511390686035\n",
      "Epoch 1000: train loss: 5.563738795899553e-06, val loss: 0.4422161281108856\n",
      "Epoch 1001: train loss: 6.1314890444919e-06, val loss: 0.4424510896205902\n",
      "Epoch 1002: train loss: 3.9877409108157735e-06, val loss: 0.4424956440925598\n",
      "Epoch 1003: train loss: 3.696206704262295e-06, val loss: 0.44224196672439575\n",
      "Epoch 1004: train loss: 3.810528141912073e-06, val loss: 0.44168534874916077\n",
      "Epoch 1005: train loss: 3.089638084929902e-06, val loss: 0.4410264194011688\n",
      "Epoch 1006: train loss: 2.6035918381239753e-06, val loss: 0.44054803252220154\n",
      "Epoch 1007: train loss: 3.1733227388031082e-06, val loss: 0.44041958451271057\n",
      "Epoch 1008: train loss: 2.34944104704482e-06, val loss: 0.4406077563762665\n",
      "Epoch 1009: train loss: 1.4733346915818402e-06, val loss: 0.44094061851501465\n",
      "Epoch 1010: train loss: 2.236002501376788e-06, val loss: 0.4412246644496918\n",
      "Epoch 1011: train loss: 1.8493489051252254e-06, val loss: 0.44134655594825745\n",
      "Epoch 1012: train loss: 1.3291032701090444e-06, val loss: 0.4412820041179657\n",
      "Epoch 1013: train loss: 1.4175124078974477e-06, val loss: 0.44105228781700134\n",
      "Epoch 1014: train loss: 1.3746780496148858e-06, val loss: 0.4407211244106293\n",
      "Epoch 1015: train loss: 1.024851940201188e-06, val loss: 0.4403810501098633\n",
      "Epoch 1016: train loss: 1.0166694437430124e-06, val loss: 0.4401397705078125\n",
      "Epoch 1017: train loss: 1.1403913049434777e-06, val loss: 0.4400629997253418\n",
      "Epoch 1018: train loss: 6.659544169451692e-07, val loss: 0.44012051820755005\n",
      "Epoch 1019: train loss: 6.563138867932139e-07, val loss: 0.44124871492385864\n",
      "Epoch 1020: train loss: 7.496813623220078e-07, val loss: 0.4423027038574219\n",
      "Epoch 1021: train loss: 7.195668558779289e-07, val loss: 0.44237184524536133\n",
      "Epoch 1022: train loss: 5.593653895630268e-07, val loss: 0.44204193353652954\n",
      "Epoch 1023: train loss: 4.313090755658777e-07, val loss: 0.4420265853404999\n",
      "Epoch 1024: train loss: 5.125390316607081e-07, val loss: 0.4424533545970917\n",
      "Epoch 1025: train loss: 4.0147259028344706e-07, val loss: 0.44290074706077576\n",
      "Epoch 1026: train loss: 3.6696306437988824e-07, val loss: 0.4431029260158539\n",
      "Epoch 1027: train loss: 4.054869577885256e-07, val loss: 0.44300180673599243\n",
      "Epoch 1028: train loss: 3.0815976970188785e-07, val loss: 0.4427713453769684\n",
      "Epoch 1029: train loss: 2.0437097703052132e-07, val loss: 0.44263267517089844\n",
      "Epoch 1030: train loss: 3.5029256650886964e-07, val loss: 0.44260460138320923\n",
      "Epoch 1031: train loss: 2.304204258507525e-07, val loss: 0.44255995750427246\n",
      "Epoch 1032: train loss: 1.5168785694186226e-07, val loss: 0.442399263381958\n",
      "Epoch 1033: train loss: 2.3658937209347641e-07, val loss: 0.4421822130680084\n",
      "Epoch 1034: train loss: 1.763471004778694e-07, val loss: 0.44207945466041565\n",
      "Epoch 1035: train loss: 1.4467740072632296e-07, val loss: 0.4421537518501282\n",
      "Epoch 1036: train loss: 1.7006755115289707e-07, val loss: 0.4422566592693329\n",
      "Epoch 1037: train loss: 1.18797743198229e-07, val loss: 0.4421907365322113\n",
      "Epoch 1038: train loss: 1.0829984375959611e-07, val loss: 0.4419344365596771\n",
      "Epoch 1039: train loss: 1.2146936967383226e-07, val loss: 0.44164106249809265\n",
      "Epoch 1040: train loss: 8.952263641504032e-08, val loss: 0.4414498507976532\n",
      "Epoch 1041: train loss: 9.383018806374821e-08, val loss: 0.44136306643486023\n",
      "Epoch 1042: train loss: 9.789312116481597e-08, val loss: 0.44130444526672363\n",
      "Epoch 1043: train loss: 8.217711666702598e-08, val loss: 0.44122645258903503\n",
      "Epoch 1044: train loss: 6.311739042530462e-08, val loss: 0.4411173462867737\n",
      "Epoch 1045: train loss: 7.523374279116979e-08, val loss: 0.4409765303134918\n",
      "Epoch 1046: train loss: 4.9119375233885876e-08, val loss: 0.44081616401672363\n",
      "Epoch 1047: train loss: 5.3207831030022135e-08, val loss: 0.4406697750091553\n",
      "Epoch 1048: train loss: 5.743289932524931e-08, val loss: 0.44054651260375977\n",
      "Epoch 1049: train loss: 4.0707572424025784e-08, val loss: 0.4404188096523285\n",
      "Epoch 1050: train loss: 4.7842210193493884e-08, val loss: 0.4402790665626526\n",
      "Epoch 1051: train loss: 3.3252078424084175e-08, val loss: 0.44016534090042114\n",
      "Epoch 1052: train loss: 3.845733331786505e-08, val loss: 0.440096378326416\n",
      "Epoch 1053: train loss: 2.8618082126286026e-08, val loss: 0.4400451183319092\n",
      "Epoch 1054: train loss: 2.972259771638619e-08, val loss: 0.4399562478065491\n",
      "Epoch 1055: train loss: 2.6308061507052116e-08, val loss: 0.4397903084754944\n",
      "Epoch 1056: train loss: 2.4188254954538024e-08, val loss: 0.43958044052124023\n",
      "Epoch 1057: train loss: 2.2088631368433198e-08, val loss: 0.4394167959690094\n",
      "Epoch 1058: train loss: 2.2118969766893315e-08, val loss: 0.439324289560318\n",
      "Epoch 1059: train loss: 1.785338810122994e-08, val loss: 0.439216673374176\n",
      "Epoch 1060: train loss: 2.087604933365128e-08, val loss: 0.4390433728694916\n",
      "Epoch 1061: train loss: 1.3795549591577583e-08, val loss: 0.438866525888443\n",
      "Epoch 1062: train loss: 2.0699511438238005e-08, val loss: 0.43873801827430725\n",
      "Epoch 1063: train loss: 1.1957017598263064e-08, val loss: 0.4385990798473358\n",
      "Epoch 1064: train loss: 1.6228350219194e-08, val loss: 0.43840208649635315\n",
      "Epoch 1065: train loss: 9.651846433200717e-09, val loss: 0.4381842315196991\n",
      "Epoch 1066: train loss: 1.4730256125972119e-08, val loss: 0.43801021575927734\n",
      "Epoch 1067: train loss: 1.2752670031090929e-08, val loss: 0.43784967064857483\n",
      "Epoch 1068: train loss: 1.4410939996878369e-08, val loss: 0.4376091957092285\n",
      "Epoch 1069: train loss: 1.218049927587117e-08, val loss: 0.43730631470680237\n",
      "Epoch 1070: train loss: 1.1855963322204843e-08, val loss: 0.43703871965408325\n",
      "Epoch 1071: train loss: 1.0208835554692541e-08, val loss: 0.43674740195274353\n",
      "Epoch 1072: train loss: 9.135781020574996e-09, val loss: 0.4363936483860016\n",
      "Epoch 1073: train loss: 9.201480466458634e-09, val loss: 0.43604776263237\n",
      "Epoch 1074: train loss: 9.847522797201691e-09, val loss: 0.4356595575809479\n",
      "Epoch 1075: train loss: 7.290247516777981e-09, val loss: 0.43519145250320435\n",
      "Epoch 1076: train loss: 9.877328288609988e-09, val loss: 0.4346331059932709\n",
      "Epoch 1077: train loss: 6.739119484677758e-09, val loss: 0.43391963839530945\n",
      "Epoch 1078: train loss: 9.17243347942076e-09, val loss: 0.4331355690956116\n",
      "Epoch 1079: train loss: 7.636351107009887e-09, val loss: 0.4323531687259674\n",
      "Epoch 1080: train loss: 7.381123268146439e-09, val loss: 0.4313203990459442\n",
      "Epoch 1081: train loss: 6.739628855001456e-09, val loss: 0.4300411343574524\n",
      "Epoch 1082: train loss: 8.369255510842777e-09, val loss: 0.42894354462623596\n",
      "Epoch 1083: train loss: 9.329504280231049e-09, val loss: 0.42734628915786743\n",
      "Epoch 1084: train loss: 1.569962648773071e-08, val loss: 0.42701467871665955\n",
      "Epoch 1085: train loss: 3.4267067405835405e-08, val loss: 0.42679300904273987\n",
      "Epoch 1086: train loss: 1.0112373871606906e-07, val loss: 0.426776647567749\n",
      "Epoch 1087: train loss: 3.6868172514914477e-07, val loss: 0.42627593874931335\n",
      "Epoch 1088: train loss: 1.551291916257469e-06, val loss: 0.4268200397491455\n",
      "Epoch 1089: train loss: 7.2592451942909975e-06, val loss: 0.424955815076828\n",
      "Epoch 1090: train loss: 2.4643608412588947e-05, val loss: 0.4268774092197418\n",
      "Epoch 1091: train loss: 6.550347461597994e-05, val loss: 0.41036149859428406\n",
      "Epoch 1092: train loss: 0.00020176613179501146, val loss: 0.43520107865333557\n",
      "Epoch 1093: train loss: 0.0002012432087212801, val loss: 0.4239262044429779\n",
      "Epoch 1094: train loss: 2.2381125745596364e-05, val loss: 0.4168151021003723\n",
      "Epoch 1095: train loss: 9.433756349608302e-05, val loss: 0.4165251851081848\n",
      "Epoch 1096: train loss: 3.5863609809894115e-05, val loss: 0.41715508699417114\n",
      "Epoch 1097: train loss: 4.152168185100891e-05, val loss: 0.4174244999885559\n",
      "Epoch 1098: train loss: 2.98569502774626e-05, val loss: 0.41697144508361816\n",
      "Epoch 1099: train loss: 3.9500453567598015e-05, val loss: 0.4159751832485199\n",
      "Epoch 1100: train loss: 1.6661912013660185e-05, val loss: 0.41539832949638367\n",
      "Epoch 1101: train loss: 1.894577144412324e-05, val loss: 0.41532501578330994\n",
      "Epoch 1102: train loss: 3.1744966690894216e-05, val loss: 0.41524767875671387\n",
      "Epoch 1103: train loss: 9.199650776281487e-06, val loss: 0.41539207100868225\n",
      "Epoch 1104: train loss: 1.4614156498282682e-05, val loss: 0.4150258004665375\n",
      "Epoch 1105: train loss: 1.4688575902255252e-05, val loss: 0.4139617085456848\n",
      "Epoch 1106: train loss: 1.119596345233731e-05, val loss: 0.4134134352207184\n",
      "Epoch 1107: train loss: 1.192556919704657e-05, val loss: 0.4139793813228607\n",
      "Epoch 1108: train loss: 6.395654963853303e-06, val loss: 0.4144582748413086\n",
      "Epoch 1109: train loss: 9.578343451721594e-06, val loss: 0.4143458306789398\n",
      "Epoch 1110: train loss: 8.269660611404106e-06, val loss: 0.414130836725235\n",
      "Epoch 1111: train loss: 4.707641892309766e-06, val loss: 0.41444191336631775\n",
      "Epoch 1112: train loss: 8.000336492841598e-06, val loss: 0.4151691496372223\n",
      "Epoch 1113: train loss: 4.614862064045155e-06, val loss: 0.4162738025188446\n",
      "Epoch 1114: train loss: 3.1759284411236877e-06, val loss: 0.41742077469825745\n",
      "Epoch 1115: train loss: 5.659121598000638e-06, val loss: 0.41787177324295044\n",
      "Epoch 1116: train loss: 3.487482445052592e-06, val loss: 0.4172610938549042\n",
      "Epoch 1117: train loss: 3.4623999454197474e-06, val loss: 0.4163160026073456\n",
      "Epoch 1118: train loss: 2.9899613309680717e-06, val loss: 0.4162577688694\n",
      "Epoch 1119: train loss: 1.932593477249611e-06, val loss: 0.41719523072242737\n",
      "Epoch 1120: train loss: 3.3872577205329435e-06, val loss: 0.4180912971496582\n",
      "Epoch 1121: train loss: 2.539778733989806e-06, val loss: 0.4183163642883301\n",
      "Epoch 1122: train loss: 1.3420105915429303e-06, val loss: 0.4180055558681488\n",
      "Epoch 1123: train loss: 2.151661647076253e-06, val loss: 0.4176247715950012\n",
      "Epoch 1124: train loss: 1.388310579386598e-06, val loss: 0.4172845780849457\n",
      "Epoch 1125: train loss: 1.4404666899281438e-06, val loss: 0.41704612970352173\n",
      "Epoch 1126: train loss: 1.4668976291432045e-06, val loss: 0.41707396507263184\n",
      "Epoch 1127: train loss: 8.253525720647303e-07, val loss: 0.4173729121685028\n",
      "Epoch 1128: train loss: 1.1719090480255545e-06, val loss: 0.4176945686340332\n",
      "Epoch 1129: train loss: 9.625421171222115e-07, val loss: 0.4178428649902344\n",
      "Epoch 1130: train loss: 7.970601814122347e-07, val loss: 0.41785407066345215\n",
      "Epoch 1131: train loss: 8.641985118629236e-07, val loss: 0.41775521636009216\n",
      "Epoch 1132: train loss: 5.486274403665448e-07, val loss: 0.4175490438938141\n",
      "Epoch 1133: train loss: 7.018743986009213e-07, val loss: 0.41741085052490234\n",
      "Epoch 1134: train loss: 5.572817940446839e-07, val loss: 0.4175930917263031\n",
      "Epoch 1135: train loss: 3.72503052403772e-07, val loss: 0.4179352819919586\n",
      "Epoch 1136: train loss: 5.85381258133566e-07, val loss: 0.41797953844070435\n",
      "Epoch 1137: train loss: 3.880358008245821e-07, val loss: 0.4176325500011444\n",
      "Epoch 1138: train loss: 3.388127822745446e-07, val loss: 0.41725292801856995\n",
      "Epoch 1139: train loss: 4.0147745039575966e-07, val loss: 0.4173438549041748\n",
      "Epoch 1140: train loss: 2.8186005351926724e-07, val loss: 0.4178738594055176\n",
      "Epoch 1141: train loss: 3.101717140907567e-07, val loss: 0.41825899481773376\n",
      "Epoch 1142: train loss: 2.281602462517185e-07, val loss: 0.41795873641967773\n",
      "Epoch 1143: train loss: 2.1522744475532818e-07, val loss: 0.41724488139152527\n",
      "Epoch 1144: train loss: 2.470814024491119e-07, val loss: 0.41686373949050903\n",
      "Epoch 1145: train loss: 1.5016392751476815e-07, val loss: 0.41703853011131287\n",
      "Epoch 1146: train loss: 2.072283677989617e-07, val loss: 0.41746383905410767\n",
      "Epoch 1147: train loss: 1.4269268433508842e-07, val loss: 0.4177291989326477\n",
      "Epoch 1148: train loss: 1.357944796609445e-07, val loss: 0.41779500246047974\n",
      "Epoch 1149: train loss: 1.4219725130715233e-07, val loss: 0.41770392656326294\n",
      "Epoch 1150: train loss: 1.0424750485071854e-07, val loss: 0.41739875078201294\n",
      "Epoch 1151: train loss: 1.3200809689806192e-07, val loss: 0.4169835150241852\n",
      "Epoch 1152: train loss: 7.479633268303587e-08, val loss: 0.4166775345802307\n",
      "Epoch 1153: train loss: 9.363664332795452e-08, val loss: 0.41652747988700867\n",
      "Epoch 1154: train loss: 9.463955308319782e-08, val loss: 0.41646119952201843\n",
      "Epoch 1155: train loss: 7.134085677762414e-08, val loss: 0.416485995054245\n",
      "Epoch 1156: train loss: 6.834348909023902e-08, val loss: 0.4165080189704895\n",
      "Epoch 1157: train loss: 4.807206011037124e-08, val loss: 0.41635170578956604\n",
      "Epoch 1158: train loss: 7.127920298444224e-08, val loss: 0.4160272777080536\n",
      "Epoch 1159: train loss: 4.4138314336805706e-08, val loss: 0.41570231318473816\n",
      "Epoch 1160: train loss: 4.498879491166008e-08, val loss: 0.4153887927532196\n",
      "Epoch 1161: train loss: 4.312677504003659e-08, val loss: 0.41506242752075195\n",
      "Epoch 1162: train loss: 4.199171854679662e-08, val loss: 0.4148528277873993\n",
      "Epoch 1163: train loss: 4.021241650775664e-08, val loss: 0.41478559374809265\n",
      "Epoch 1164: train loss: 2.7913140243640555e-08, val loss: 0.4147457182407379\n",
      "Epoch 1165: train loss: 3.3964639101213834e-08, val loss: 0.41463276743888855\n",
      "Epoch 1166: train loss: 2.890518757681093e-08, val loss: 0.4143769443035126\n",
      "Epoch 1167: train loss: 2.722725866988185e-08, val loss: 0.414038747549057\n",
      "Epoch 1168: train loss: 2.5824880012237372e-08, val loss: 0.41373759508132935\n",
      "Epoch 1169: train loss: 2.5875481313164528e-08, val loss: 0.4135846793651581\n",
      "Epoch 1170: train loss: 2.437871238214484e-08, val loss: 0.41344648599624634\n",
      "Epoch 1171: train loss: 1.5475016823529586e-08, val loss: 0.4133654534816742\n",
      "Epoch 1172: train loss: 1.916059133577619e-08, val loss: 0.4132741093635559\n",
      "Epoch 1173: train loss: 1.3835059320399523e-08, val loss: 0.4131661355495453\n",
      "Epoch 1174: train loss: 1.6015929915624838e-08, val loss: 0.4130294919013977\n",
      "Epoch 1175: train loss: 1.3272996923774372e-08, val loss: 0.41286301612854004\n",
      "Epoch 1176: train loss: 1.7631817783581027e-08, val loss: 0.4127471148967743\n",
      "Epoch 1177: train loss: 1.0606123090894926e-08, val loss: 0.41262325644493103\n",
      "Epoch 1178: train loss: 1.3210855520640052e-08, val loss: 0.4125351011753082\n",
      "Epoch 1179: train loss: 9.139105472399933e-09, val loss: 0.4124546945095062\n",
      "Epoch 1180: train loss: 1.1257114351792552e-08, val loss: 0.4123167097568512\n",
      "Epoch 1181: train loss: 1.0151178564399288e-08, val loss: 0.4121948778629303\n",
      "Epoch 1182: train loss: 1.0718208542925822e-08, val loss: 0.4121486246585846\n",
      "Epoch 1183: train loss: 9.925343213978977e-09, val loss: 0.41218072175979614\n",
      "Epoch 1184: train loss: 1.1164869029300917e-08, val loss: 0.41210100054740906\n",
      "Epoch 1185: train loss: 1.2447941344362334e-08, val loss: 0.4119417369365692\n",
      "Epoch 1186: train loss: 2.6650713635945067e-08, val loss: 0.41184988617897034\n",
      "Epoch 1187: train loss: 2.4892456096381466e-08, val loss: 0.41181474924087524\n",
      "Epoch 1188: train loss: 1.6214894316135542e-08, val loss: 0.4116944968700409\n",
      "Epoch 1189: train loss: 3.211235721778394e-08, val loss: 0.41175174713134766\n",
      "Epoch 1190: train loss: 5.024442017997899e-08, val loss: 0.411494642496109\n",
      "Epoch 1191: train loss: 1.0726865440346955e-07, val loss: 0.41188913583755493\n",
      "Epoch 1192: train loss: 3.4439162277521973e-07, val loss: 0.41108494997024536\n",
      "Epoch 1193: train loss: 1.3327994565770496e-06, val loss: 0.41264763474464417\n",
      "Epoch 1194: train loss: 4.975637693860335e-06, val loss: 0.40926629304885864\n",
      "Epoch 1195: train loss: 2.028657945629675e-05, val loss: 0.41755351424217224\n",
      "Epoch 1196: train loss: 3.951091639464721e-05, val loss: 0.40918636322021484\n",
      "Epoch 1197: train loss: 3.9985141484066844e-05, val loss: 0.41459202766418457\n",
      "Epoch 1198: train loss: 1.3952226254332345e-05, val loss: 0.41470518708229065\n",
      "Epoch 1199: train loss: 1.6832307665026747e-05, val loss: 0.41348353028297424\n",
      "Epoch 1200: train loss: 1.5391844499390572e-05, val loss: 0.41416874527931213\n",
      "Epoch 1201: train loss: 1.1841257219202816e-05, val loss: 0.41169872879981995\n",
      "Epoch 1202: train loss: 7.866784471843857e-06, val loss: 0.4108586311340332\n",
      "Epoch 1203: train loss: 7.6921332947677e-06, val loss: 0.41062355041503906\n",
      "Epoch 1204: train loss: 7.456669209204847e-06, val loss: 0.4072946608066559\n",
      "Epoch 1205: train loss: 5.098046131024603e-06, val loss: 0.40604373812675476\n",
      "Epoch 1206: train loss: 5.271140253171325e-06, val loss: 0.40986281633377075\n",
      "Epoch 1207: train loss: 5.221318133408204e-06, val loss: 0.4085555076599121\n",
      "Epoch 1208: train loss: 3.016810978806461e-06, val loss: 0.4044437110424042\n",
      "Epoch 1209: train loss: 5.05471734868479e-06, val loss: 0.4056510925292969\n",
      "Epoch 1210: train loss: 1.858412701949419e-06, val loss: 0.4107297956943512\n",
      "Epoch 1211: train loss: 4.543983322946588e-06, val loss: 0.41296058893203735\n",
      "Epoch 1212: train loss: 1.1126228400826221e-06, val loss: 0.41302162408828735\n",
      "Epoch 1213: train loss: 3.5186037621315336e-06, val loss: 0.4135971963405609\n",
      "Epoch 1214: train loss: 1.6673211575835012e-06, val loss: 0.414083331823349\n",
      "Epoch 1215: train loss: 1.9638844150904333e-06, val loss: 0.4143650233745575\n",
      "Epoch 1216: train loss: 1.8773073406919139e-06, val loss: 0.41499072313308716\n",
      "Epoch 1217: train loss: 1.3848052731191274e-06, val loss: 0.4148326516151428\n",
      "Epoch 1218: train loss: 1.5356542917288607e-06, val loss: 0.4137600064277649\n",
      "Epoch 1219: train loss: 1.3065107395959785e-06, val loss: 0.41331344842910767\n",
      "Epoch 1220: train loss: 1.0507993692954187e-06, val loss: 0.4133683741092682\n",
      "Epoch 1221: train loss: 1.1372574135748437e-06, val loss: 0.412884384393692\n",
      "Epoch 1222: train loss: 8.379379323741887e-07, val loss: 0.41292428970336914\n",
      "Epoch 1223: train loss: 9.95967525341257e-07, val loss: 0.41425690054893494\n",
      "Epoch 1224: train loss: 6.051983518773341e-07, val loss: 0.4147956967353821\n",
      "Epoch 1225: train loss: 8.288521939903148e-07, val loss: 0.41397905349731445\n",
      "Epoch 1226: train loss: 6.441899813580676e-07, val loss: 0.41385728120803833\n",
      "Epoch 1227: train loss: 4.3339323951840925e-07, val loss: 0.4147215783596039\n",
      "Epoch 1228: train loss: 7.604423899465473e-07, val loss: 0.4151071608066559\n",
      "Epoch 1229: train loss: 2.271475949555679e-07, val loss: 0.4152649939060211\n",
      "Epoch 1230: train loss: 6.162699719425291e-07, val loss: 0.41563230752944946\n",
      "Epoch 1231: train loss: 3.108246460215014e-07, val loss: 0.41540828347206116\n",
      "Epoch 1232: train loss: 3.778378641072777e-07, val loss: 0.4153349995613098\n",
      "Epoch 1233: train loss: 3.69979062497805e-07, val loss: 0.41629835963249207\n",
      "Epoch 1234: train loss: 2.4963941314126714e-07, val loss: 0.41640177369117737\n",
      "Epoch 1235: train loss: 3.151955922930938e-07, val loss: 0.41540905833244324\n",
      "Epoch 1236: train loss: 2.359743547231119e-07, val loss: 0.41562190651893616\n",
      "Epoch 1237: train loss: 2.66978929630568e-07, val loss: 0.41635653376579285\n",
      "Epoch 1238: train loss: 1.317106210763086e-07, val loss: 0.4163212776184082\n",
      "Epoch 1239: train loss: 2.8113419148212415e-07, val loss: 0.41638660430908203\n",
      "Epoch 1240: train loss: 1.4666073866465013e-07, val loss: 0.416431725025177\n",
      "Epoch 1241: train loss: 1.5081145932072104e-07, val loss: 0.41634830832481384\n",
      "Epoch 1242: train loss: 1.6194077545605978e-07, val loss: 0.41668349504470825\n",
      "Epoch 1243: train loss: 1.4862517616620607e-07, val loss: 0.41675668954849243\n",
      "Epoch 1244: train loss: 1.2537513782717724e-07, val loss: 0.41661763191223145\n",
      "Epoch 1245: train loss: 1.2220797884765489e-07, val loss: 0.41662126779556274\n",
      "Epoch 1246: train loss: 1.4077974697102036e-07, val loss: 0.4166610836982727\n",
      "Epoch 1247: train loss: 9.646156939879802e-08, val loss: 0.4168228805065155\n",
      "Epoch 1248: train loss: 6.78185330116321e-08, val loss: 0.4167299270629883\n",
      "Epoch 1249: train loss: 9.42752791388557e-08, val loss: 0.4167267382144928\n",
      "Epoch 1250: train loss: 8.411424801124667e-08, val loss: 0.41692885756492615\n",
      "Epoch 1251: train loss: 6.911704275580632e-08, val loss: 0.41673365235328674\n",
      "Epoch 1252: train loss: 7.581540728551772e-08, val loss: 0.4168170094490051\n",
      "Epoch 1253: train loss: 1.0023032359640638e-07, val loss: 0.41665658354759216\n",
      "Epoch 1254: train loss: 9.303334991273005e-08, val loss: 0.4170759618282318\n",
      "Epoch 1255: train loss: 8.786937399918315e-08, val loss: 0.416576623916626\n",
      "Epoch 1256: train loss: 1.8570572990483925e-07, val loss: 0.41686269640922546\n",
      "Epoch 1257: train loss: 4.247070251039986e-07, val loss: 0.4163019359111786\n",
      "Epoch 1258: train loss: 1.2044183677062392e-06, val loss: 0.41747456789016724\n",
      "Epoch 1259: train loss: 2.3983645860425895e-06, val loss: 0.41556239128112793\n",
      "Epoch 1260: train loss: 4.096312295587268e-06, val loss: 0.4174940586090088\n",
      "Epoch 1261: train loss: 7.521347924921429e-06, val loss: 0.4143548011779785\n",
      "Epoch 1262: train loss: 1.8263897800352424e-05, val loss: 0.4186650216579437\n",
      "Epoch 1263: train loss: 4.120999437873252e-05, val loss: 0.4101594388484955\n",
      "Epoch 1264: train loss: 8.32624064059928e-05, val loss: 0.4226045608520508\n",
      "Epoch 1265: train loss: 5.453215635498054e-05, val loss: 0.4206876754760742\n",
      "Epoch 1266: train loss: 1.2641437024285551e-05, val loss: 0.4136393964290619\n",
      "Epoch 1267: train loss: 3.166199167026207e-05, val loss: 0.4150519371032715\n",
      "Epoch 1268: train loss: 1.6068295735749416e-05, val loss: 0.4130581319332123\n",
      "Epoch 1269: train loss: 1.969071126950439e-05, val loss: 0.409589022397995\n",
      "Epoch 1270: train loss: 8.934909601521213e-06, val loss: 0.40839052200317383\n",
      "Epoch 1271: train loss: 1.6135285477503203e-05, val loss: 0.4123879373073578\n",
      "Epoch 1272: train loss: 9.260976185032632e-06, val loss: 0.41443273425102234\n",
      "Epoch 1273: train loss: 8.107972462312318e-06, val loss: 0.41100284457206726\n",
      "Epoch 1274: train loss: 5.84514282309101e-06, val loss: 0.40752801299095154\n",
      "Epoch 1275: train loss: 8.511252417520154e-06, val loss: 0.40885433554649353\n",
      "Epoch 1276: train loss: 5.496250651049195e-06, val loss: 0.41298389434814453\n",
      "Epoch 1277: train loss: 5.0208527682116255e-06, val loss: 0.41487303376197815\n",
      "Epoch 1278: train loss: 4.6994000513223e-06, val loss: 0.41319534182548523\n",
      "Epoch 1279: train loss: 3.260616040279274e-06, val loss: 0.4113365709781647\n",
      "Epoch 1280: train loss: 4.866366452915827e-06, val loss: 0.4118759334087372\n",
      "Epoch 1281: train loss: 3.0869589409121545e-06, val loss: 0.4135129153728485\n",
      "Epoch 1282: train loss: 2.2994950086285826e-06, val loss: 0.41371574997901917\n",
      "Epoch 1283: train loss: 3.154146497763577e-06, val loss: 0.4124462306499481\n",
      "Epoch 1284: train loss: 2.379930492679705e-06, val loss: 0.4118471145629883\n",
      "Epoch 1285: train loss: 2.3254578991327435e-06, val loss: 0.4132552146911621\n",
      "Epoch 1286: train loss: 1.5660008330087294e-06, val loss: 0.41565951704978943\n",
      "Epoch 1287: train loss: 1.963100885404856e-06, val loss: 0.4169580638408661\n",
      "Epoch 1288: train loss: 1.3202932223066455e-06, val loss: 0.4166111946105957\n",
      "Epoch 1289: train loss: 1.771841994013812e-06, val loss: 0.4157295823097229\n",
      "Epoch 1290: train loss: 1.1580440286707017e-06, val loss: 0.4155656397342682\n",
      "Epoch 1291: train loss: 9.864337471299223e-07, val loss: 0.41628655791282654\n",
      "Epoch 1292: train loss: 1.1863221516250633e-06, val loss: 0.4171324372291565\n",
      "Epoch 1293: train loss: 9.890239880405716e-07, val loss: 0.41779962182044983\n",
      "Epoch 1294: train loss: 8.290700748148083e-07, val loss: 0.41843485832214355\n",
      "Epoch 1295: train loss: 6.861588985884737e-07, val loss: 0.4188590943813324\n",
      "Epoch 1296: train loss: 6.734908879479917e-07, val loss: 0.41885247826576233\n",
      "Epoch 1297: train loss: 6.971804964450712e-07, val loss: 0.4189988076686859\n",
      "Epoch 1298: train loss: 5.534226943382237e-07, val loss: 0.41987189650535583\n",
      "Epoch 1299: train loss: 5.613130724668736e-07, val loss: 0.4208779036998749\n",
      "Epoch 1300: train loss: 3.840676470190374e-07, val loss: 0.42093944549560547\n",
      "Epoch 1301: train loss: 4.4375784113981354e-07, val loss: 0.420123815536499\n",
      "Epoch 1302: train loss: 3.951320763917465e-07, val loss: 0.41983142495155334\n",
      "Epoch 1303: train loss: 3.980267422321049e-07, val loss: 0.4205758273601532\n",
      "Epoch 1304: train loss: 2.4138597609635326e-07, val loss: 0.4213624894618988\n",
      "Epoch 1305: train loss: 2.8187184852868086e-07, val loss: 0.4213801324367523\n",
      "Epoch 1306: train loss: 2.880601073229627e-07, val loss: 0.4209054112434387\n",
      "Epoch 1307: train loss: 2.511086734102719e-07, val loss: 0.42073318362236023\n",
      "Epoch 1308: train loss: 2.1366294333802216e-07, val loss: 0.4210832715034485\n",
      "Epoch 1309: train loss: 1.5438529032962833e-07, val loss: 0.4215312898159027\n",
      "Epoch 1310: train loss: 2.086039359028291e-07, val loss: 0.42155614495277405\n",
      "Epoch 1311: train loss: 1.6263426516616164e-07, val loss: 0.42107078433036804\n",
      "Epoch 1312: train loss: 1.429621647730528e-07, val loss: 0.42055025696754456\n",
      "Epoch 1313: train loss: 1.2890646416963136e-07, val loss: 0.42047080397605896\n",
      "Epoch 1314: train loss: 1.1188012649654411e-07, val loss: 0.4207856357097626\n",
      "Epoch 1315: train loss: 1.4248702484565e-07, val loss: 0.42103472352027893\n",
      "Epoch 1316: train loss: 8.548545338271651e-08, val loss: 0.42087021470069885\n",
      "Epoch 1317: train loss: 9.784843513216401e-08, val loss: 0.4201779067516327\n",
      "Epoch 1318: train loss: 8.354930969289853e-08, val loss: 0.4193890690803528\n",
      "Epoch 1319: train loss: 8.805841389403213e-08, val loss: 0.41928672790527344\n",
      "Epoch 1320: train loss: 4.847943557706458e-08, val loss: 0.4197719693183899\n",
      "Epoch 1321: train loss: 8.195024747692514e-08, val loss: 0.4199119508266449\n",
      "Epoch 1322: train loss: 4.919767349065296e-08, val loss: 0.41944804787635803\n",
      "Epoch 1323: train loss: 6.771822569362485e-08, val loss: 0.4189237058162689\n",
      "Epoch 1324: train loss: 4.0468989936925936e-08, val loss: 0.4186162054538727\n",
      "Epoch 1325: train loss: 5.230869959405027e-08, val loss: 0.4185575544834137\n",
      "Epoch 1326: train loss: 4.391165830952559e-08, val loss: 0.41873112320899963\n",
      "Epoch 1327: train loss: 2.9193856221354508e-08, val loss: 0.4184335172176361\n",
      "Epoch 1328: train loss: 4.171267420360891e-08, val loss: 0.4174794852733612\n",
      "Epoch 1329: train loss: 3.304653262148349e-08, val loss: 0.4170350134372711\n",
      "Epoch 1330: train loss: 3.021552785753556e-08, val loss: 0.41718122363090515\n",
      "Epoch 1331: train loss: 3.090519484771903e-08, val loss: 0.4166719615459442\n",
      "Epoch 1332: train loss: 2.97577553709516e-08, val loss: 0.4157474637031555\n",
      "Epoch 1333: train loss: 2.7729928575581653e-08, val loss: 0.4149415194988251\n",
      "Epoch 1334: train loss: 3.476528576129567e-08, val loss: 0.4139747619628906\n",
      "Epoch 1335: train loss: 4.9275968194706365e-08, val loss: 0.4127667546272278\n",
      "Epoch 1336: train loss: 1.4218146304756374e-07, val loss: 0.4126628339290619\n",
      "Epoch 1337: train loss: 5.315116027304612e-07, val loss: 0.41207247972488403\n",
      "Epoch 1338: train loss: 2.4986545668070903e-06, val loss: 0.412092924118042\n",
      "Epoch 1339: train loss: 8.763181540416554e-06, val loss: 0.4127114713191986\n",
      "Epoch 1340: train loss: 9.19568174140295e-06, val loss: 0.4130120873451233\n",
      "Epoch 1341: train loss: 1.0745779945864342e-06, val loss: 0.41214123368263245\n",
      "Epoch 1342: train loss: 3.5162527183274506e-06, val loss: 0.4121656119823456\n",
      "Epoch 1343: train loss: 2.9654311219928786e-06, val loss: 0.412249892950058\n",
      "Epoch 1344: train loss: 6.756704920007905e-07, val loss: 0.4118458926677704\n",
      "Epoch 1345: train loss: 2.2724645987182157e-06, val loss: 0.412381649017334\n",
      "Epoch 1346: train loss: 2.280408807564527e-06, val loss: 0.41201305389404297\n",
      "Epoch 1347: train loss: 6.162075010252011e-07, val loss: 0.4117595851421356\n",
      "Epoch 1348: train loss: 1.56043518018123e-06, val loss: 0.4117416441440582\n",
      "Epoch 1349: train loss: 1.699965650914237e-06, val loss: 0.41188374161720276\n",
      "Epoch 1350: train loss: 4.444690944183094e-07, val loss: 0.4120445251464844\n",
      "Epoch 1351: train loss: 1.2475751418605796e-06, val loss: 0.4113636612892151\n",
      "Epoch 1352: train loss: 1.3433707408694318e-06, val loss: 0.4114907383918762\n",
      "Epoch 1353: train loss: 6.635734735027654e-07, val loss: 0.4102117121219635\n",
      "Epoch 1354: train loss: 1.3487473324858001e-06, val loss: 0.4110542833805084\n",
      "Epoch 1355: train loss: 2.4746420876908815e-06, val loss: 0.40794530510902405\n",
      "Epoch 1356: train loss: 6.003599082760047e-06, val loss: 0.4116400182247162\n",
      "Epoch 1357: train loss: 2.0158888219157234e-05, val loss: 0.39805468916893005\n",
      "Epoch 1358: train loss: 5.554402741836384e-05, val loss: 0.4122222065925598\n",
      "Epoch 1359: train loss: 9.552962728776038e-05, val loss: 0.39947211742401123\n",
      "Epoch 1360: train loss: 5.97210819250904e-05, val loss: 0.40090566873550415\n",
      "Epoch 1361: train loss: 1.5333263945649378e-05, val loss: 0.4030980169773102\n",
      "Epoch 1362: train loss: 3.1485920771956444e-05, val loss: 0.40221619606018066\n",
      "Epoch 1363: train loss: 2.0951571059413254e-05, val loss: 0.40255212783813477\n",
      "Epoch 1364: train loss: 1.5578538295812905e-05, val loss: 0.4024926722049713\n",
      "Epoch 1365: train loss: 1.7596581528778188e-05, val loss: 0.4035049080848694\n",
      "Epoch 1366: train loss: 1.0426003427710384e-05, val loss: 0.4030604362487793\n",
      "Epoch 1367: train loss: 1.3457924978865776e-05, val loss: 0.40176182985305786\n",
      "Epoch 1368: train loss: 8.530108971172012e-06, val loss: 0.4016321301460266\n",
      "Epoch 1369: train loss: 9.205117748933844e-06, val loss: 0.40238237380981445\n",
      "Epoch 1370: train loss: 7.534379165008431e-06, val loss: 0.4024498462677002\n",
      "Epoch 1371: train loss: 6.607368504774058e-06, val loss: 0.40220770239830017\n",
      "Epoch 1372: train loss: 6.127719643700402e-06, val loss: 0.4024125039577484\n",
      "Epoch 1373: train loss: 4.747283128381241e-06, val loss: 0.4026985764503479\n",
      "Epoch 1374: train loss: 5.328654879122041e-06, val loss: 0.4021436274051666\n",
      "Epoch 1375: train loss: 3.0669011721329298e-06, val loss: 0.4015253186225891\n",
      "Epoch 1376: train loss: 5.147284355189186e-06, val loss: 0.4017520844936371\n",
      "Epoch 1377: train loss: 1.7024709677571082e-06, val loss: 0.40258416533470154\n",
      "Epoch 1378: train loss: 4.468011866265442e-06, val loss: 0.40305328369140625\n",
      "Epoch 1379: train loss: 1.3758252634943346e-06, val loss: 0.4028969407081604\n",
      "Epoch 1380: train loss: 3.3750525290088262e-06, val loss: 0.40262371301651\n",
      "Epoch 1381: train loss: 1.3987644251756137e-06, val loss: 0.4024864733219147\n",
      "Epoch 1382: train loss: 2.378828185101156e-06, val loss: 0.4023975431919098\n",
      "Epoch 1383: train loss: 1.3875678632757626e-06, val loss: 0.40248972177505493\n",
      "Epoch 1384: train loss: 1.7607384279472171e-06, val loss: 0.4029001295566559\n",
      "Epoch 1385: train loss: 1.2883128874818794e-06, val loss: 0.4033033847808838\n",
      "Epoch 1386: train loss: 1.2842924661526922e-06, val loss: 0.4031994938850403\n",
      "Epoch 1387: train loss: 1.0883893537538825e-06, val loss: 0.4026199281215668\n",
      "Epoch 1388: train loss: 1.0946680504275719e-06, val loss: 0.4022391438484192\n",
      "Epoch 1389: train loss: 8.047488790907664e-07, val loss: 0.4025050103664398\n",
      "Epoch 1390: train loss: 9.52311950186413e-07, val loss: 0.4030291736125946\n",
      "Epoch 1391: train loss: 6.663689191555022e-07, val loss: 0.40311190485954285\n",
      "Epoch 1392: train loss: 7.041037974886422e-07, val loss: 0.4026709496974945\n",
      "Epoch 1393: train loss: 5.996151344334066e-07, val loss: 0.4023100435733795\n",
      "Epoch 1394: train loss: 6.059036081751401e-07, val loss: 0.40249234437942505\n",
      "Epoch 1395: train loss: 4.498610337577702e-07, val loss: 0.4027162194252014\n",
      "Epoch 1396: train loss: 4.980776111551677e-07, val loss: 0.4023594856262207\n",
      "Epoch 1397: train loss: 3.91711694192054e-07, val loss: 0.4017369747161865\n",
      "Epoch 1398: train loss: 4.103414710243669e-07, val loss: 0.40165454149246216\n",
      "Epoch 1399: train loss: 3.1475309469897184e-07, val loss: 0.4019485414028168\n",
      "Epoch 1400: train loss: 3.260636844970577e-07, val loss: 0.40183302760124207\n",
      "Epoch 1401: train loss: 2.7813982228508394e-07, val loss: 0.4012947976589203\n",
      "Epoch 1402: train loss: 2.721734233546158e-07, val loss: 0.40107011795043945\n",
      "Epoch 1403: train loss: 2.143556798728241e-07, val loss: 0.4011397063732147\n",
      "Epoch 1404: train loss: 2.4938131559792964e-07, val loss: 0.40091535449028015\n",
      "Epoch 1405: train loss: 1.5432311784024932e-07, val loss: 0.40047845244407654\n",
      "Epoch 1406: train loss: 2.146172306538574e-07, val loss: 0.4003233015537262\n",
      "Epoch 1407: train loss: 1.255811241662741e-07, val loss: 0.4002411961555481\n",
      "Epoch 1408: train loss: 1.7497835358426528e-07, val loss: 0.399780809879303\n",
      "Epoch 1409: train loss: 1.185614664223067e-07, val loss: 0.399306058883667\n",
      "Epoch 1410: train loss: 1.2256666082066658e-07, val loss: 0.39925912022590637\n",
      "Epoch 1411: train loss: 1.2082556111181475e-07, val loss: 0.3991400897502899\n",
      "Epoch 1412: train loss: 8.894132719206027e-08, val loss: 0.398629367351532\n",
      "Epoch 1413: train loss: 1.1535325938893948e-07, val loss: 0.39821746945381165\n",
      "Epoch 1414: train loss: 6.3483398093922e-08, val loss: 0.39804425835609436\n",
      "Epoch 1415: train loss: 9.849841120512792e-08, val loss: 0.3977397084236145\n",
      "Epoch 1416: train loss: 6.703636756810738e-08, val loss: 0.3974425494670868\n",
      "Epoch 1417: train loss: 5.8839336958271815e-08, val loss: 0.39724284410476685\n",
      "Epoch 1418: train loss: 8.886291169574179e-08, val loss: 0.3968118131160736\n",
      "Epoch 1419: train loss: 4.072538217769761e-08, val loss: 0.3964496850967407\n",
      "Epoch 1420: train loss: 5.6958693761544055e-08, val loss: 0.3961900770664215\n",
      "Epoch 1421: train loss: 5.961234705864626e-08, val loss: 0.39570656418800354\n",
      "Epoch 1422: train loss: 4.167916145547679e-08, val loss: 0.3954951763153076\n",
      "Epoch 1423: train loss: 3.77793440975438e-08, val loss: 0.39524462819099426\n",
      "Epoch 1424: train loss: 5.3587388748610465e-08, val loss: 0.3947087228298187\n",
      "Epoch 1425: train loss: 3.8591757345329825e-08, val loss: 0.39458000659942627\n",
      "Epoch 1426: train loss: 3.235302159509956e-08, val loss: 0.394288569688797\n",
      "Epoch 1427: train loss: 4.944922693539411e-08, val loss: 0.3939891755580902\n",
      "Epoch 1428: train loss: 2.6960877974602226e-08, val loss: 0.3935339152812958\n",
      "Epoch 1429: train loss: 2.8534941520774737e-08, val loss: 0.3933534324169159\n",
      "Epoch 1430: train loss: 1.7576535782382052e-08, val loss: 0.3931640386581421\n",
      "Epoch 1431: train loss: 2.4806306342384232e-08, val loss: 0.39279207587242126\n",
      "Epoch 1432: train loss: 3.783840440974018e-08, val loss: 0.39270511269569397\n",
      "Epoch 1433: train loss: 4.981402312864702e-08, val loss: 0.39216041564941406\n",
      "Epoch 1434: train loss: 9.21023399769183e-08, val loss: 0.3927370607852936\n",
      "Epoch 1435: train loss: 3.5451648727757856e-07, val loss: 0.3912825286388397\n",
      "Epoch 1436: train loss: 1.8719063064054353e-06, val loss: 0.39487528800964355\n",
      "Epoch 1437: train loss: 1.352001891063992e-05, val loss: 0.38711467385292053\n",
      "Epoch 1438: train loss: 9.079754090635106e-05, val loss: 0.4072802662849426\n",
      "Epoch 1439: train loss: 0.00025286662275902927, val loss: 0.39574041962623596\n",
      "Epoch 1440: train loss: 0.00021763496624771506, val loss: 0.3958907127380371\n",
      "Epoch 1441: train loss: 0.0001366335345664993, val loss: 0.401872843503952\n",
      "Epoch 1442: train loss: 8.501878619426861e-05, val loss: 0.40867167711257935\n",
      "Epoch 1443: train loss: 7.421653572237119e-05, val loss: 0.413255512714386\n",
      "Epoch 1444: train loss: 9.011571819428355e-05, val loss: 0.41449594497680664\n",
      "Epoch 1445: train loss: 6.665439286734909e-05, val loss: 0.4160033166408539\n",
      "Epoch 1446: train loss: 5.908968159928918e-05, val loss: 0.4160453975200653\n",
      "Epoch 1447: train loss: 3.6280598578741774e-05, val loss: 0.4155433177947998\n",
      "Epoch 1448: train loss: 3.0261926440289244e-05, val loss: 0.414999395608902\n",
      "Epoch 1449: train loss: 3.592957727960311e-05, val loss: 0.4148510992527008\n",
      "Epoch 1450: train loss: 3.750193354790099e-05, val loss: 0.41518741846084595\n",
      "Epoch 1451: train loss: 2.793947896861937e-05, val loss: 0.41563501954078674\n",
      "Epoch 1452: train loss: 2.1431244022096507e-05, val loss: 0.4156532287597656\n",
      "Epoch 1453: train loss: 2.048224450845737e-05, val loss: 0.4153135418891907\n",
      "Epoch 1454: train loss: 1.7502128685009666e-05, val loss: 0.4153061807155609\n",
      "Epoch 1455: train loss: 1.5110737876966596e-05, val loss: 0.4160427749156952\n",
      "Epoch 1456: train loss: 1.4854628716420848e-05, val loss: 0.4170820415019989\n",
      "Epoch 1457: train loss: 1.64462071552407e-05, val loss: 0.4176200032234192\n",
      "Epoch 1458: train loss: 1.4150000424706377e-05, val loss: 0.41739264130592346\n",
      "Epoch 1459: train loss: 9.788495844986755e-06, val loss: 0.41686826944351196\n",
      "Epoch 1460: train loss: 8.973101103038061e-06, val loss: 0.4165547490119934\n",
      "Epoch 1461: train loss: 7.40215000405442e-06, val loss: 0.41650304198265076\n",
      "Epoch 1462: train loss: 7.066666512400843e-06, val loss: 0.4165317118167877\n",
      "Epoch 1463: train loss: 8.993720257421955e-06, val loss: 0.41647306084632874\n",
      "Epoch 1464: train loss: 7.956145964271855e-06, val loss: 0.4162469804286957\n",
      "Epoch 1465: train loss: 5.5978271120693535e-06, val loss: 0.41597792506217957\n",
      "Epoch 1466: train loss: 4.885178441327298e-06, val loss: 0.4158376157283783\n",
      "Epoch 1467: train loss: 4.1513180804031435e-06, val loss: 0.41591188311576843\n",
      "Epoch 1468: train loss: 3.281615363448509e-06, val loss: 0.41601285338401794\n",
      "Epoch 1469: train loss: 4.80994958707015e-06, val loss: 0.4159214198589325\n",
      "Epoch 1470: train loss: 4.181468284514267e-06, val loss: 0.4156803786754608\n",
      "Epoch 1471: train loss: 2.8707618184853345e-06, val loss: 0.415496826171875\n",
      "Epoch 1472: train loss: 2.3897423488961067e-06, val loss: 0.4154432415962219\n",
      "Epoch 1473: train loss: 2.6423817871545907e-06, val loss: 0.41540881991386414\n",
      "Epoch 1474: train loss: 1.8872776763600996e-06, val loss: 0.415274053812027\n",
      "Epoch 1475: train loss: 2.3404829789797077e-06, val loss: 0.41501593589782715\n",
      "Epoch 1476: train loss: 2.0965469502698397e-06, val loss: 0.4147054851055145\n",
      "Epoch 1477: train loss: 1.86994907380722e-06, val loss: 0.41447100043296814\n",
      "Epoch 1478: train loss: 1.3222543202573434e-06, val loss: 0.4143885672092438\n",
      "Epoch 1479: train loss: 1.6075698567874497e-06, val loss: 0.41436466574668884\n",
      "Epoch 1480: train loss: 1.1691118970702519e-06, val loss: 0.4142272174358368\n",
      "Epoch 1481: train loss: 1.0633126521497616e-06, val loss: 0.4139258563518524\n",
      "Epoch 1482: train loss: 9.091952506423695e-07, val loss: 0.41359663009643555\n",
      "Epoch 1483: train loss: 1.089344323190744e-06, val loss: 0.4133780002593994\n",
      "Epoch 1484: train loss: 7.330220341827953e-07, val loss: 0.4132765233516693\n",
      "Epoch 1485: train loss: 8.291816584460321e-07, val loss: 0.4132244288921356\n",
      "Epoch 1486: train loss: 6.482109142780246e-07, val loss: 0.4131593704223633\n",
      "Epoch 1487: train loss: 5.141817496223666e-07, val loss: 0.41303735971450806\n",
      "Epoch 1488: train loss: 5.424739697446057e-07, val loss: 0.4128565788269043\n",
      "Epoch 1489: train loss: 6.432200621020456e-07, val loss: 0.41265377402305603\n",
      "Epoch 1490: train loss: 5.421801461125142e-07, val loss: 0.4124249517917633\n",
      "Epoch 1491: train loss: 4.110225404474477e-07, val loss: 0.41212090849876404\n",
      "Epoch 1492: train loss: 3.422469774250203e-07, val loss: 0.4118315875530243\n",
      "Epoch 1493: train loss: 2.9966784609314345e-07, val loss: 0.4115775227546692\n",
      "Epoch 1494: train loss: 3.6599948316506925e-07, val loss: 0.4114396572113037\n",
      "Epoch 1495: train loss: 2.591072245650139e-07, val loss: 0.41132497787475586\n",
      "Epoch 1496: train loss: 2.4584889501966245e-07, val loss: 0.4111505150794983\n",
      "Epoch 1497: train loss: 2.1642486558448581e-07, val loss: 0.41095447540283203\n",
      "Epoch 1498: train loss: 2.2135222366159724e-07, val loss: 0.41077953577041626\n",
      "Epoch 1499: train loss: 1.7763980508789246e-07, val loss: 0.410640686750412\n",
      "Epoch 1500: train loss: 2.0036338810314192e-07, val loss: 0.41057252883911133\n",
      "Epoch 1501: train loss: 1.6418744053225964e-07, val loss: 0.41054126620292664\n",
      "Epoch 1502: train loss: 1.2201491017549415e-07, val loss: 0.41040942072868347\n",
      "Epoch 1503: train loss: 1.6775184974449076e-07, val loss: 0.41010528802871704\n",
      "Epoch 1504: train loss: 9.56930961137914e-08, val loss: 0.4096854329109192\n",
      "Epoch 1505: train loss: 9.709761883414103e-08, val loss: 0.4092028737068176\n",
      "Epoch 1506: train loss: 9.535915523883887e-08, val loss: 0.4086388051509857\n",
      "Epoch 1507: train loss: 8.414527030708996e-08, val loss: 0.407943457365036\n",
      "Epoch 1508: train loss: 1.0539899619743665e-07, val loss: 0.4071086049079895\n",
      "Epoch 1509: train loss: 6.293138454793734e-08, val loss: 0.406238853931427\n",
      "Epoch 1510: train loss: 6.46840945250915e-08, val loss: 0.40535351634025574\n",
      "Epoch 1511: train loss: 6.483801229251185e-08, val loss: 0.4043005108833313\n",
      "Epoch 1512: train loss: 5.849595652307471e-08, val loss: 0.40344539284706116\n",
      "Epoch 1513: train loss: 5.505303590780386e-08, val loss: 0.4031820297241211\n",
      "Epoch 1514: train loss: 4.2981845638223604e-08, val loss: 0.40284404158592224\n",
      "Epoch 1515: train loss: 4.0302893467014655e-08, val loss: 0.40249893069267273\n",
      "Epoch 1516: train loss: 3.1489612695168034e-08, val loss: 0.4021768271923065\n",
      "Epoch 1517: train loss: 3.190128694541272e-08, val loss: 0.40189066529273987\n",
      "Epoch 1518: train loss: 3.644225898824516e-08, val loss: 0.40163707733154297\n",
      "Epoch 1519: train loss: 2.1921147563830345e-08, val loss: 0.4013870358467102\n",
      "Epoch 1520: train loss: 3.3078247696494145e-08, val loss: 0.4011082351207733\n",
      "Epoch 1521: train loss: 2.937943754943717e-08, val loss: 0.4007929265499115\n",
      "Epoch 1522: train loss: 2.3403664783927525e-08, val loss: 0.4004846215248108\n",
      "Epoch 1523: train loss: 2.5332191455618158e-08, val loss: 0.40018540620803833\n",
      "Epoch 1524: train loss: 2.1349224610389683e-08, val loss: 0.3997825086116791\n",
      "Epoch 1525: train loss: 1.8202442220172088e-08, val loss: 0.3992471992969513\n",
      "Epoch 1526: train loss: 1.669607740950596e-08, val loss: 0.3987461030483246\n",
      "Epoch 1527: train loss: 1.192767840052511e-08, val loss: 0.398355096578598\n",
      "Epoch 1528: train loss: 1.4168497486366505e-08, val loss: 0.39792636036872864\n",
      "Epoch 1529: train loss: 1.6366064059525343e-08, val loss: 0.3973860740661621\n",
      "Epoch 1530: train loss: 1.291944595749328e-08, val loss: 0.3968145549297333\n",
      "Epoch 1531: train loss: 1.596172083395686e-08, val loss: 0.39627596735954285\n",
      "Epoch 1532: train loss: 1.4319517127603376e-08, val loss: 0.3957013189792633\n",
      "Epoch 1533: train loss: 1.3079454852515937e-08, val loss: 0.3950420916080475\n",
      "Epoch 1534: train loss: 1.1639452957012963e-08, val loss: 0.39438751339912415\n",
      "Epoch 1535: train loss: 7.610342578345808e-09, val loss: 0.3937808573246002\n",
      "Epoch 1536: train loss: 9.219887076028499e-09, val loss: 0.3931144177913666\n",
      "Epoch 1537: train loss: 9.284799595832283e-09, val loss: 0.392609179019928\n",
      "Epoch 1538: train loss: 9.194457639694065e-09, val loss: 0.3922722637653351\n",
      "Epoch 1539: train loss: 1.0008790241045062e-08, val loss: 0.3919207751750946\n",
      "Epoch 1540: train loss: 9.24532805868239e-09, val loss: 0.3915664851665497\n",
      "Epoch 1541: train loss: 8.08194933199502e-09, val loss: 0.3912167549133301\n",
      "Epoch 1542: train loss: 6.78712464008413e-09, val loss: 0.39100801944732666\n",
      "Epoch 1543: train loss: 7.044307803738548e-09, val loss: 0.3908953368663788\n",
      "Epoch 1544: train loss: 6.770698224300986e-09, val loss: 0.3907620906829834\n",
      "Epoch 1545: train loss: 5.669143376962893e-09, val loss: 0.39063331484794617\n",
      "Epoch 1546: train loss: 6.7222738486805156e-09, val loss: 0.39053162932395935\n",
      "Epoch 1547: train loss: 8.675982599015697e-09, val loss: 0.3904317021369934\n",
      "Epoch 1548: train loss: 1.0949745110622189e-08, val loss: 0.3903278410434723\n",
      "Epoch 1549: train loss: 1.774682090172064e-08, val loss: 0.39021745324134827\n",
      "Epoch 1550: train loss: 4.3193676191322083e-08, val loss: 0.39013731479644775\n",
      "Epoch 1551: train loss: 1.296394742666962e-07, val loss: 0.39000198245048523\n",
      "Epoch 1552: train loss: 4.7182621187857876e-07, val loss: 0.38991811871528625\n",
      "Epoch 1553: train loss: 1.9040704728467972e-06, val loss: 0.389735609292984\n",
      "Epoch 1554: train loss: 7.484542493330082e-06, val loss: 0.3888119161128998\n",
      "Epoch 1555: train loss: 1.607204285392072e-05, val loss: 0.3891182839870453\n",
      "Epoch 1556: train loss: 1.4408301467483398e-05, val loss: 0.386516809463501\n",
      "Epoch 1557: train loss: 3.115971776423976e-05, val loss: 0.38840562105178833\n",
      "Epoch 1558: train loss: 2.593081444501877e-05, val loss: 0.38494086265563965\n",
      "Epoch 1559: train loss: 8.658557817398105e-06, val loss: 0.3813726305961609\n",
      "Epoch 1560: train loss: 1.2681533917202614e-05, val loss: 0.3832668960094452\n",
      "Epoch 1561: train loss: 8.663896551297512e-06, val loss: 0.38482746481895447\n",
      "Epoch 1562: train loss: 9.826061614148784e-06, val loss: 0.3826819956302643\n",
      "Epoch 1563: train loss: 6.500353720184648e-06, val loss: 0.3809615969657898\n",
      "Epoch 1564: train loss: 1.102504575101193e-05, val loss: 0.3814433217048645\n",
      "Epoch 1565: train loss: 9.557827979733702e-06, val loss: 0.38380709290504456\n",
      "Epoch 1566: train loss: 1.7507993106846698e-05, val loss: 0.3820475935935974\n",
      "Epoch 1567: train loss: 3.874824415106559e-06, val loss: 0.3800341486930847\n",
      "Epoch 1568: train loss: 1.2309455996728502e-05, val loss: 0.3815000057220459\n",
      "Epoch 1569: train loss: 5.17970738656004e-06, val loss: 0.3842606246471405\n",
      "Epoch 1570: train loss: 7.523844942625146e-06, val loss: 0.38503217697143555\n",
      "Epoch 1571: train loss: 6.176656825118698e-06, val loss: 0.3839426636695862\n",
      "Epoch 1572: train loss: 4.572232683131006e-06, val loss: 0.3832922875881195\n",
      "Epoch 1573: train loss: 5.986236374155851e-06, val loss: 0.3838459849357605\n",
      "Epoch 1574: train loss: 2.8769970867870143e-06, val loss: 0.38484659790992737\n",
      "Epoch 1575: train loss: 5.3752355597680435e-06, val loss: 0.3851380944252014\n",
      "Epoch 1576: train loss: 1.9915012217097683e-06, val loss: 0.38484176993370056\n",
      "Epoch 1577: train loss: 4.5452770791598596e-06, val loss: 0.38467463850975037\n",
      "Epoch 1578: train loss: 1.428303335160308e-06, val loss: 0.38463103771209717\n",
      "Epoch 1579: train loss: 3.943556293961592e-06, val loss: 0.38466596603393555\n",
      "Epoch 1580: train loss: 8.083911779976916e-07, val loss: 0.3849336504936218\n",
      "Epoch 1581: train loss: 3.5189837035431992e-06, val loss: 0.3852735459804535\n",
      "Epoch 1582: train loss: 4.984379984307452e-07, val loss: 0.3851904273033142\n",
      "Epoch 1583: train loss: 2.822395799739752e-06, val loss: 0.38470548391342163\n",
      "Epoch 1584: train loss: 5.652389063470764e-07, val loss: 0.3846381902694702\n",
      "Epoch 1585: train loss: 2.1139203454367816e-06, val loss: 0.3850766122341156\n",
      "Epoch 1586: train loss: 6.710595812364772e-07, val loss: 0.3852609694004059\n",
      "Epoch 1587: train loss: 1.5063784530866542e-06, val loss: 0.3848712742328644\n",
      "Epoch 1588: train loss: 7.417572760459734e-07, val loss: 0.38461729884147644\n",
      "Epoch 1589: train loss: 1.0466711728440714e-06, val loss: 0.38480621576309204\n",
      "Epoch 1590: train loss: 8.027824378586956e-07, val loss: 0.3848517835140228\n",
      "Epoch 1591: train loss: 6.865481623208325e-07, val loss: 0.3845382332801819\n",
      "Epoch 1592: train loss: 7.911935426818673e-07, val loss: 0.3845230042934418\n",
      "Epoch 1593: train loss: 4.5968434392307245e-07, val loss: 0.3848951458930969\n",
      "Epoch 1594: train loss: 6.999663924034394e-07, val loss: 0.3848536014556885\n",
      "Epoch 1595: train loss: 3.853116936625156e-07, val loss: 0.3842930495738983\n",
      "Epoch 1596: train loss: 5.486855343406205e-07, val loss: 0.38407936692237854\n",
      "Epoch 1597: train loss: 3.3417782674405316e-07, val loss: 0.38439875841140747\n",
      "Epoch 1598: train loss: 4.345674256001075e-07, val loss: 0.3845047950744629\n",
      "Epoch 1599: train loss: 2.895459658702748e-07, val loss: 0.38415899872779846\n",
      "Epoch 1600: train loss: 3.5501585671227076e-07, val loss: 0.38386449217796326\n",
      "Epoch 1601: train loss: 2.3695297102221957e-07, val loss: 0.3838174045085907\n",
      "Epoch 1602: train loss: 2.8506249805104744e-07, val loss: 0.3837358057498932\n",
      "Epoch 1603: train loss: 2.0283543733512488e-07, val loss: 0.38354989886283875\n",
      "Epoch 1604: train loss: 2.4115854557749117e-07, val loss: 0.383473664522171\n",
      "Epoch 1605: train loss: 1.5179485046701302e-07, val loss: 0.38351088762283325\n",
      "Epoch 1606: train loss: 2.0908802866870246e-07, val loss: 0.3833997845649719\n",
      "Epoch 1607: train loss: 1.2790069092716294e-07, val loss: 0.3831097185611725\n",
      "Epoch 1608: train loss: 1.6279309988931345e-07, val loss: 0.3829915523529053\n",
      "Epoch 1609: train loss: 1.2018075778996717e-07, val loss: 0.38315925002098083\n",
      "Epoch 1610: train loss: 1.1509791164598937e-07, val loss: 0.38317355513572693\n",
      "Epoch 1611: train loss: 1.248177596835376e-07, val loss: 0.3829343020915985\n",
      "Epoch 1612: train loss: 8.463994305429878e-08, val loss: 0.38297006487846375\n",
      "Epoch 1613: train loss: 1.0067622469023263e-07, val loss: 0.3832648694515228\n",
      "Epoch 1614: train loss: 8.008583307628214e-08, val loss: 0.3831976354122162\n",
      "Epoch 1615: train loss: 8.620807534498454e-08, val loss: 0.3831371068954468\n",
      "Epoch 1616: train loss: 5.7446239765113205e-08, val loss: 0.3834270238876343\n",
      "Epoch 1617: train loss: 7.429859039120856e-08, val loss: 0.38341501355171204\n",
      "Epoch 1618: train loss: 6.554301990036038e-08, val loss: 0.3832767903804779\n",
      "Epoch 1619: train loss: 4.722524238331971e-08, val loss: 0.3834676444530487\n",
      "Epoch 1620: train loss: 6.224715320968244e-08, val loss: 0.3835948705673218\n",
      "Epoch 1621: train loss: 5.608378472743425e-08, val loss: 0.38352319598197937\n",
      "Epoch 1622: train loss: 4.343687720620437e-08, val loss: 0.3834894001483917\n",
      "Epoch 1623: train loss: 5.4714277553102875e-08, val loss: 0.3838363587856293\n",
      "Epoch 1624: train loss: 6.659734452796329e-08, val loss: 0.3835711181163788\n",
      "Epoch 1625: train loss: 1.32951015530125e-07, val loss: 0.3836612403392792\n",
      "Epoch 1626: train loss: 5.743702331528766e-07, val loss: 0.3836887776851654\n",
      "Epoch 1627: train loss: 3.251882390031824e-06, val loss: 0.3843156397342682\n",
      "Epoch 1628: train loss: 1.8508739231037907e-05, val loss: 0.3829837739467621\n",
      "Epoch 1629: train loss: 6.339990795822814e-05, val loss: 0.39833003282546997\n",
      "Epoch 1630: train loss: 4.0283019188791513e-05, val loss: 0.39428436756134033\n",
      "Epoch 1631: train loss: 2.275090264447499e-05, val loss: 0.3895474374294281\n",
      "Epoch 1632: train loss: 1.9881477783201262e-05, val loss: 0.3904957175254822\n",
      "Epoch 1633: train loss: 1.9852166587952524e-05, val loss: 0.3931640684604645\n",
      "Epoch 1634: train loss: 1.1317230018903501e-05, val loss: 0.3948810398578644\n",
      "Epoch 1635: train loss: 1.5801195331732742e-05, val loss: 0.39477404952049255\n",
      "Epoch 1636: train loss: 9.729425073601305e-06, val loss: 0.3934193253517151\n",
      "Epoch 1637: train loss: 7.991351594682783e-06, val loss: 0.39089569449424744\n",
      "Epoch 1638: train loss: 8.934797733672895e-06, val loss: 0.38928020000457764\n",
      "Epoch 1639: train loss: 7.52386222302448e-06, val loss: 0.39057689905166626\n",
      "Epoch 1640: train loss: 6.755591584806098e-06, val loss: 0.3936607837677002\n",
      "Epoch 1641: train loss: 4.393035851535387e-06, val loss: 0.39520367980003357\n",
      "Epoch 1642: train loss: 5.600018994300626e-06, val loss: 0.3941028118133545\n",
      "Epoch 1643: train loss: 3.6008709685120266e-06, val loss: 0.3926096558570862\n",
      "Epoch 1644: train loss: 5.4686111070623156e-06, val loss: 0.39270997047424316\n",
      "Epoch 1645: train loss: 2.492347675797646e-06, val loss: 0.3936161994934082\n",
      "Epoch 1646: train loss: 3.0250544114096556e-06, val loss: 0.3936605155467987\n",
      "Epoch 1647: train loss: 3.205177790732705e-06, val loss: 0.3929634988307953\n",
      "Epoch 1648: train loss: 2.117296617143438e-06, val loss: 0.3926942050457001\n",
      "Epoch 1649: train loss: 2.531579184505972e-06, val loss: 0.39318040013313293\n",
      "Epoch 1650: train loss: 2.1008534076827345e-06, val loss: 0.3938537538051605\n",
      "Epoch 1651: train loss: 1.7593757775102858e-06, val loss: 0.3940737843513489\n",
      "Epoch 1652: train loss: 1.3253113593236776e-06, val loss: 0.39375612139701843\n",
      "Epoch 1653: train loss: 1.6662172583892243e-06, val loss: 0.39328765869140625\n",
      "Epoch 1654: train loss: 1.5205487216007896e-06, val loss: 0.39311084151268005\n",
      "Epoch 1655: train loss: 1.1218887721042847e-06, val loss: 0.39319828152656555\n",
      "Epoch 1656: train loss: 1.0300689154973952e-06, val loss: 0.393300861120224\n",
      "Epoch 1657: train loss: 9.922996468958445e-07, val loss: 0.3933184742927551\n",
      "Epoch 1658: train loss: 8.088536560535431e-07, val loss: 0.39324191212654114\n",
      "Epoch 1659: train loss: 8.970058047452767e-07, val loss: 0.39295223355293274\n",
      "Epoch 1660: train loss: 7.155333605624037e-07, val loss: 0.392649382352829\n",
      "Epoch 1661: train loss: 6.350775265673292e-07, val loss: 0.3926412761211395\n",
      "Epoch 1662: train loss: 5.21813831255713e-07, val loss: 0.39287519454956055\n",
      "Epoch 1663: train loss: 5.548160402213398e-07, val loss: 0.3929900825023651\n",
      "Epoch 1664: train loss: 5.833693990098254e-07, val loss: 0.3927769362926483\n",
      "Epoch 1665: train loss: 4.284562464818009e-07, val loss: 0.3922772705554962\n",
      "Epoch 1666: train loss: 3.708836686655559e-07, val loss: 0.3919104039669037\n",
      "Epoch 1667: train loss: 3.956710372676753e-07, val loss: 0.3918136656284332\n",
      "Epoch 1668: train loss: 3.148012126530375e-07, val loss: 0.39172425866127014\n",
      "Epoch 1669: train loss: 3.3626747608650476e-07, val loss: 0.3917011320590973\n",
      "Epoch 1670: train loss: 2.4388882025050407e-07, val loss: 0.3919534683227539\n",
      "Epoch 1671: train loss: 2.5155770799756283e-07, val loss: 0.39205893874168396\n",
      "Epoch 1672: train loss: 2.681668433979212e-07, val loss: 0.39174729585647583\n",
      "Epoch 1673: train loss: 1.829565405842004e-07, val loss: 0.39135321974754333\n",
      "Epoch 1674: train loss: 2.018618658894411e-07, val loss: 0.3914218842983246\n",
      "Epoch 1675: train loss: 1.5372985728845379e-07, val loss: 0.39156612753868103\n",
      "Epoch 1676: train loss: 1.5040662049159437e-07, val loss: 0.39131632447242737\n",
      "Epoch 1677: train loss: 1.5284030041584629e-07, val loss: 0.39100727438926697\n",
      "Epoch 1678: train loss: 1.388858805739801e-07, val loss: 0.3909939229488373\n",
      "Epoch 1679: train loss: 1.0428806973550309e-07, val loss: 0.391220360994339\n",
      "Epoch 1680: train loss: 1.1776756281278722e-07, val loss: 0.39143654704093933\n",
      "Epoch 1681: train loss: 1.0048941589957394e-07, val loss: 0.39128896594047546\n",
      "Epoch 1682: train loss: 8.600951417747638e-08, val loss: 0.39096808433532715\n",
      "Epoch 1683: train loss: 8.145423180394573e-08, val loss: 0.39094042778015137\n",
      "Epoch 1684: train loss: 7.423153647323488e-08, val loss: 0.3908822536468506\n",
      "Epoch 1685: train loss: 7.827790682313207e-08, val loss: 0.3906165361404419\n",
      "Epoch 1686: train loss: 5.474371178593174e-08, val loss: 0.3906872868537903\n",
      "Epoch 1687: train loss: 5.246353751431343e-08, val loss: 0.390800803899765\n",
      "Epoch 1688: train loss: 6.025896936989739e-08, val loss: 0.3904891610145569\n",
      "Epoch 1689: train loss: 5.387168044990176e-08, val loss: 0.39022350311279297\n",
      "Epoch 1690: train loss: 4.094007977073488e-08, val loss: 0.3901298940181732\n",
      "Epoch 1691: train loss: 3.6951551152242246e-08, val loss: 0.39009571075439453\n",
      "Epoch 1692: train loss: 3.8427820925335254e-08, val loss: 0.3900183439254761\n",
      "Epoch 1693: train loss: 4.030333400351083e-08, val loss: 0.3896849751472473\n",
      "Epoch 1694: train loss: 4.0475775620052445e-08, val loss: 0.38979288935661316\n",
      "Epoch 1695: train loss: 3.461728326215052e-08, val loss: 0.3894495964050293\n",
      "Epoch 1696: train loss: 3.4442440011162034e-08, val loss: 0.3890922963619232\n",
      "Epoch 1697: train loss: 4.8824471576836004e-08, val loss: 0.3888165056705475\n",
      "Epoch 1698: train loss: 6.975934496722402e-08, val loss: 0.38879403471946716\n",
      "Epoch 1699: train loss: 1.7789356832054182e-07, val loss: 0.3878149092197418\n",
      "Epoch 1700: train loss: 6.451533636209206e-07, val loss: 0.3882061541080475\n",
      "Epoch 1701: train loss: 2.116874838975491e-06, val loss: 0.3863229751586914\n",
      "Epoch 1702: train loss: 2.7239584596827626e-06, val loss: 0.38691455125808716\n",
      "Epoch 1703: train loss: 1.195819550048327e-06, val loss: 0.3860473334789276\n",
      "Epoch 1704: train loss: 5.179053346182627e-07, val loss: 0.3850399851799011\n",
      "Epoch 1705: train loss: 1.4986032965680351e-06, val loss: 0.38581007719039917\n",
      "Epoch 1706: train loss: 1.7248353287868667e-06, val loss: 0.3841376006603241\n",
      "Epoch 1707: train loss: 8.675622211740119e-07, val loss: 0.3838397264480591\n",
      "Epoch 1708: train loss: 9.051933602677309e-07, val loss: 0.3840599060058594\n",
      "Epoch 1709: train loss: 1.7852629525805241e-06, val loss: 0.3833138942718506\n",
      "Epoch 1710: train loss: 1.2278001122467685e-06, val loss: 0.3833853304386139\n",
      "Epoch 1711: train loss: 1.677648810982646e-07, val loss: 0.38352665305137634\n",
      "Epoch 1712: train loss: 5.72530723275122e-07, val loss: 0.38330820202827454\n",
      "Epoch 1713: train loss: 1.1847226915051579e-06, val loss: 0.38332399725914\n",
      "Epoch 1714: train loss: 8.872485750544001e-07, val loss: 0.3834594488143921\n",
      "Epoch 1715: train loss: 5.507525315806561e-07, val loss: 0.3832319676876068\n",
      "Epoch 1716: train loss: 7.306012435037701e-07, val loss: 0.3835362493991852\n",
      "Epoch 1717: train loss: 8.467266638945148e-07, val loss: 0.38326045870780945\n",
      "Epoch 1718: train loss: 6.330374162644148e-07, val loss: 0.383462518453598\n",
      "Epoch 1719: train loss: 3.8804060409347585e-07, val loss: 0.38352933526039124\n",
      "Epoch 1720: train loss: 2.854561103049491e-07, val loss: 0.38332873582839966\n",
      "Epoch 1721: train loss: 3.731953484020778e-07, val loss: 0.38375169038772583\n",
      "Epoch 1722: train loss: 5.175471642360208e-07, val loss: 0.3830927610397339\n",
      "Epoch 1723: train loss: 4.4721568315253535e-07, val loss: 0.3839772641658783\n",
      "Epoch 1724: train loss: 2.3598457232765213e-07, val loss: 0.38316795229911804\n",
      "Epoch 1725: train loss: 1.5581174750423088e-07, val loss: 0.3836499750614166\n",
      "Epoch 1726: train loss: 3.1578110792906955e-07, val loss: 0.38390451669692993\n",
      "Epoch 1727: train loss: 6.033811246197729e-07, val loss: 0.3830586373806\n",
      "Epoch 1728: train loss: 1.172923248304869e-06, val loss: 0.3842115104198456\n",
      "Epoch 1729: train loss: 3.172367996739922e-06, val loss: 0.38271161913871765\n",
      "Epoch 1730: train loss: 1.0707147339417133e-05, val loss: 0.3847436308860779\n",
      "Epoch 1731: train loss: 3.4669887099880725e-05, val loss: 0.38345280289649963\n",
      "Epoch 1732: train loss: 4.436946983332746e-05, val loss: 0.38252389430999756\n",
      "Epoch 1733: train loss: 3.348588870721869e-05, val loss: 0.38663554191589355\n",
      "Epoch 1734: train loss: 1.316237830906175e-05, val loss: 0.3871091902256012\n",
      "Epoch 1735: train loss: 1.1757624633901287e-05, val loss: 0.3874915838241577\n",
      "Epoch 1736: train loss: 1.599448296474293e-05, val loss: 0.38908594846725464\n",
      "Epoch 1737: train loss: 6.4278733589162584e-06, val loss: 0.3893943428993225\n",
      "Epoch 1738: train loss: 1.0075632417283487e-05, val loss: 0.38835883140563965\n",
      "Epoch 1739: train loss: 7.356813966907794e-06, val loss: 0.38819050788879395\n",
      "Epoch 1740: train loss: 5.345792942534899e-06, val loss: 0.38913437724113464\n",
      "Epoch 1741: train loss: 6.220004252099898e-06, val loss: 0.38954854011535645\n",
      "Epoch 1742: train loss: 4.458622697711689e-06, val loss: 0.3890056610107422\n",
      "Epoch 1743: train loss: 5.046764727012487e-06, val loss: 0.38889357447624207\n",
      "Epoch 1744: train loss: 2.7830051294586156e-06, val loss: 0.38937509059906006\n",
      "Epoch 1745: train loss: 4.579258074954851e-06, val loss: 0.389237642288208\n",
      "Epoch 1746: train loss: 2.3030524971545674e-06, val loss: 0.38877740502357483\n",
      "Epoch 1747: train loss: 3.26094755109807e-06, val loss: 0.38881632685661316\n",
      "Epoch 1748: train loss: 1.940751417350839e-06, val loss: 0.3891395032405853\n",
      "Epoch 1749: train loss: 2.926168690464692e-06, val loss: 0.38928818702697754\n",
      "Epoch 1750: train loss: 1.2648450820051949e-06, val loss: 0.3892899453639984\n",
      "Epoch 1751: train loss: 2.324204388060025e-06, val loss: 0.3890337646007538\n",
      "Epoch 1752: train loss: 1.2856145303885569e-06, val loss: 0.3887391686439514\n",
      "Epoch 1753: train loss: 1.754575578161166e-06, val loss: 0.3887776732444763\n",
      "Epoch 1754: train loss: 9.578019444234087e-07, val loss: 0.38952022790908813\n",
      "Epoch 1755: train loss: 1.5956472907419084e-06, val loss: 0.39002034068107605\n",
      "Epoch 1756: train loss: 7.434068152178952e-07, val loss: 0.3895590007305145\n",
      "Epoch 1757: train loss: 1.129412680711539e-06, val loss: 0.38853272795677185\n",
      "Epoch 1758: train loss: 8.043826937864651e-07, val loss: 0.3884207606315613\n",
      "Epoch 1759: train loss: 8.410907526013034e-07, val loss: 0.3894328773021698\n",
      "Epoch 1760: train loss: 6.330462269943382e-07, val loss: 0.38994932174682617\n",
      "Epoch 1761: train loss: 7.259614562826755e-07, val loss: 0.38921061158180237\n",
      "Epoch 1762: train loss: 5.194620484871848e-07, val loss: 0.38851961493492126\n",
      "Epoch 1763: train loss: 5.675257170878467e-07, val loss: 0.3888363540172577\n",
      "Epoch 1764: train loss: 4.5186922648099426e-07, val loss: 0.38934996724128723\n",
      "Epoch 1765: train loss: 4.70979529154647e-07, val loss: 0.3891327381134033\n",
      "Epoch 1766: train loss: 3.719231074228446e-07, val loss: 0.38855987787246704\n",
      "Epoch 1767: train loss: 3.121708687103819e-07, val loss: 0.3883865773677826\n",
      "Epoch 1768: train loss: 4.1278948970102647e-07, val loss: 0.38842278718948364\n",
      "Epoch 1769: train loss: 1.968246863270906e-07, val loss: 0.38838720321655273\n",
      "Epoch 1770: train loss: 3.1809821621209267e-07, val loss: 0.38819560408592224\n",
      "Epoch 1771: train loss: 2.413721063021512e-07, val loss: 0.3876844346523285\n",
      "Epoch 1772: train loss: 1.9549270291463472e-07, val loss: 0.38715070486068726\n",
      "Epoch 1773: train loss: 2.2016033085492381e-07, val loss: 0.3870893120765686\n",
      "Epoch 1774: train loss: 1.7335933932827174e-07, val loss: 0.3870871961116791\n",
      "Epoch 1775: train loss: 1.8591519790334132e-07, val loss: 0.38663554191589355\n",
      "Epoch 1776: train loss: 1.1684607414963466e-07, val loss: 0.3860255181789398\n",
      "Epoch 1777: train loss: 1.8274428725817415e-07, val loss: 0.3857536017894745\n",
      "Epoch 1778: train loss: 9.597791006399348e-08, val loss: 0.38561078906059265\n",
      "Epoch 1779: train loss: 1.2532346715943277e-07, val loss: 0.38513046503067017\n",
      "Epoch 1780: train loss: 1.0921067428171227e-07, val loss: 0.3845195174217224\n",
      "Epoch 1781: train loss: 8.931180417448559e-08, val loss: 0.3840901553630829\n",
      "Epoch 1782: train loss: 8.931283446145244e-08, val loss: 0.38343918323516846\n",
      "Epoch 1783: train loss: 9.614541340852156e-08, val loss: 0.3831438720226288\n",
      "Epoch 1784: train loss: 4.993617253035154e-08, val loss: 0.3830283582210541\n",
      "Epoch 1785: train loss: 8.47864924935493e-08, val loss: 0.38266053795814514\n",
      "Epoch 1786: train loss: 6.323079304593193e-08, val loss: 0.38233503699302673\n",
      "Epoch 1787: train loss: 4.578982526481923e-08, val loss: 0.38220226764678955\n",
      "Epoch 1788: train loss: 5.731572016998143e-08, val loss: 0.3820153772830963\n",
      "Epoch 1789: train loss: 5.659155988269049e-08, val loss: 0.3818005621433258\n",
      "Epoch 1790: train loss: 3.9421276909479275e-08, val loss: 0.3815569579601288\n",
      "Epoch 1791: train loss: 3.573639162368636e-08, val loss: 0.38148441910743713\n",
      "Epoch 1792: train loss: 5.12704900756944e-08, val loss: 0.38147440552711487\n",
      "Epoch 1793: train loss: 3.792192160290142e-08, val loss: 0.3811734616756439\n",
      "Epoch 1794: train loss: 3.051971120271446e-08, val loss: 0.38114142417907715\n",
      "Epoch 1795: train loss: 2.8327592715982064e-08, val loss: 0.38122135400772095\n",
      "Epoch 1796: train loss: 3.050387320513437e-08, val loss: 0.38106653094291687\n",
      "Epoch 1797: train loss: 3.41205996789995e-08, val loss: 0.3811046779155731\n",
      "Epoch 1798: train loss: 4.579843704277664e-08, val loss: 0.381041556596756\n",
      "Epoch 1799: train loss: 4.854930324427187e-08, val loss: 0.38108763098716736\n",
      "Epoch 1800: train loss: 5.8796654656134706e-08, val loss: 0.38090676069259644\n",
      "Epoch 1801: train loss: 9.16255942229327e-08, val loss: 0.3813866674900055\n",
      "Epoch 1802: train loss: 1.8185609462761931e-07, val loss: 0.38074925541877747\n",
      "Epoch 1803: train loss: 5.302342174218211e-07, val loss: 0.38160398602485657\n",
      "Epoch 1804: train loss: 1.8039027054328471e-06, val loss: 0.3804958462715149\n",
      "Epoch 1805: train loss: 7.141625701478915e-06, val loss: 0.38316676020622253\n",
      "Epoch 1806: train loss: 2.6552337658358738e-05, val loss: 0.3793160021305084\n",
      "Epoch 1807: train loss: 7.01358585502021e-05, val loss: 0.3894154131412506\n",
      "Epoch 1808: train loss: 5.72599601582624e-05, val loss: 0.3850345313549042\n",
      "Epoch 1809: train loss: 1.162118587672012e-05, val loss: 0.3852190375328064\n",
      "Epoch 1810: train loss: 3.819834819296375e-05, val loss: 0.39079737663269043\n",
      "Epoch 1811: train loss: 9.527333531877957e-06, val loss: 0.39330634474754333\n",
      "Epoch 1812: train loss: 2.4019534976105206e-05, val loss: 0.3922935426235199\n",
      "Epoch 1813: train loss: 1.007994342216989e-05, val loss: 0.39297041296958923\n",
      "Epoch 1814: train loss: 1.681003595876973e-05, val loss: 0.39492642879486084\n",
      "Epoch 1815: train loss: 9.852958100964315e-06, val loss: 0.3951970338821411\n",
      "Epoch 1816: train loss: 1.1309163710393477e-05, val loss: 0.3956036865711212\n",
      "Epoch 1817: train loss: 8.402161256526597e-06, val loss: 0.3956686556339264\n",
      "Epoch 1818: train loss: 7.921637006802484e-06, val loss: 0.3961809277534485\n",
      "Epoch 1819: train loss: 7.5624493547366e-06, val loss: 0.39675888419151306\n",
      "Epoch 1820: train loss: 6.891792509122752e-06, val loss: 0.3964235782623291\n",
      "Epoch 1821: train loss: 5.370388407754945e-06, val loss: 0.3959342837333679\n",
      "Epoch 1822: train loss: 5.032190983911278e-06, val loss: 0.3965202867984772\n",
      "Epoch 1823: train loss: 4.537740551313618e-06, val loss: 0.39744487404823303\n",
      "Epoch 1824: train loss: 4.32282422480057e-06, val loss: 0.3973146378993988\n",
      "Epoch 1825: train loss: 3.605948222684674e-06, val loss: 0.3965848982334137\n",
      "Epoch 1826: train loss: 3.084059244429227e-06, val loss: 0.39677074551582336\n",
      "Epoch 1827: train loss: 3.107039674432599e-06, val loss: 0.3977374732494354\n",
      "Epoch 1828: train loss: 2.667432227099198e-06, val loss: 0.39776501059532166\n",
      "Epoch 1829: train loss: 2.205254304499249e-06, val loss: 0.39698097109794617\n",
      "Epoch 1830: train loss: 2.2578128664463293e-06, val loss: 0.3972459137439728\n",
      "Epoch 1831: train loss: 2.0033160126331495e-06, val loss: 0.3979208171367645\n",
      "Epoch 1832: train loss: 1.4908703178662108e-06, val loss: 0.3973159193992615\n",
      "Epoch 1833: train loss: 1.804728526622057e-06, val loss: 0.3964320719242096\n",
      "Epoch 1834: train loss: 1.1788052916017477e-06, val loss: 0.3966708481311798\n",
      "Epoch 1835: train loss: 1.4455665677814977e-06, val loss: 0.39709755778312683\n",
      "Epoch 1836: train loss: 1.0219937394140288e-06, val loss: 0.39639759063720703\n",
      "Epoch 1837: train loss: 1.1390234249120113e-06, val loss: 0.3952568769454956\n",
      "Epoch 1838: train loss: 8.158773425748223e-07, val loss: 0.395522803068161\n",
      "Epoch 1839: train loss: 9.036485835167696e-07, val loss: 0.39621707797050476\n",
      "Epoch 1840: train loss: 7.490497750950453e-07, val loss: 0.3954998850822449\n",
      "Epoch 1841: train loss: 6.933316853974247e-07, val loss: 0.39511895179748535\n",
      "Epoch 1842: train loss: 5.889894509891747e-07, val loss: 0.3955303132534027\n",
      "Epoch 1843: train loss: 5.950864192527661e-07, val loss: 0.395892858505249\n",
      "Epoch 1844: train loss: 4.929208898829529e-07, val loss: 0.39576682448387146\n",
      "Epoch 1845: train loss: 4.388492413909262e-07, val loss: 0.39562317728996277\n",
      "Epoch 1846: train loss: 4.457629643184191e-07, val loss: 0.3958117663860321\n",
      "Epoch 1847: train loss: 3.5844431067744154e-07, val loss: 0.3960953652858734\n",
      "Epoch 1848: train loss: 3.713079763656424e-07, val loss: 0.3960162103176117\n",
      "Epoch 1849: train loss: 2.5537937631270324e-07, val loss: 0.3960140645503998\n",
      "Epoch 1850: train loss: 3.5569271972235583e-07, val loss: 0.3963332176208496\n",
      "Epoch 1851: train loss: 2.1288920493134356e-07, val loss: 0.39633551239967346\n",
      "Epoch 1852: train loss: 2.514718460133736e-07, val loss: 0.3962276875972748\n",
      "Epoch 1853: train loss: 2.0505144959770405e-07, val loss: 0.3964398503303528\n",
      "Epoch 1854: train loss: 2.0167813374882826e-07, val loss: 0.39656156301498413\n",
      "Epoch 1855: train loss: 1.8297237147635315e-07, val loss: 0.3964851498603821\n",
      "Epoch 1856: train loss: 1.5634486771887168e-07, val loss: 0.3965066075325012\n",
      "Epoch 1857: train loss: 1.312334916292457e-07, val loss: 0.39669644832611084\n",
      "Epoch 1858: train loss: 1.6264857549685985e-07, val loss: 0.39676913619041443\n",
      "Epoch 1859: train loss: 1.0428959029695761e-07, val loss: 0.39671313762664795\n",
      "Epoch 1860: train loss: 1.2671874571879016e-07, val loss: 0.39687058329582214\n",
      "Epoch 1861: train loss: 8.617529090315657e-08, val loss: 0.39701777696609497\n",
      "Epoch 1862: train loss: 1.221145993213213e-07, val loss: 0.39697518944740295\n",
      "Epoch 1863: train loss: 5.154623039516082e-08, val loss: 0.39699432253837585\n",
      "Epoch 1864: train loss: 1.1174683578474287e-07, val loss: 0.39710086584091187\n",
      "Epoch 1865: train loss: 5.3779015019017606e-08, val loss: 0.3972591161727905\n",
      "Epoch 1866: train loss: 8.801236361932752e-08, val loss: 0.3972470164299011\n",
      "Epoch 1867: train loss: 5.4327486509464507e-08, val loss: 0.39715442061424255\n",
      "Epoch 1868: train loss: 5.050656071148296e-08, val loss: 0.3973138630390167\n",
      "Epoch 1869: train loss: 6.951169950752956e-08, val loss: 0.39744216203689575\n",
      "Epoch 1870: train loss: 4.7092090227351946e-08, val loss: 0.39741790294647217\n",
      "Epoch 1871: train loss: 6.562640209040183e-08, val loss: 0.39748644828796387\n",
      "Epoch 1872: train loss: 2.0539226852633874e-07, val loss: 0.39746659994125366\n",
      "Epoch 1873: train loss: 3.968177963997732e-07, val loss: 0.3970271050930023\n",
      "Epoch 1874: train loss: 4.0901707620832894e-07, val loss: 0.3973143994808197\n",
      "Epoch 1875: train loss: 2.505705367639166e-07, val loss: 0.39613088965415955\n",
      "Epoch 1876: train loss: 2.953766795599222e-07, val loss: 0.39770644903182983\n",
      "Epoch 1877: train loss: 9.255362556359614e-07, val loss: 0.3951394259929657\n",
      "Epoch 1878: train loss: 3.786819434026256e-06, val loss: 0.39897313714027405\n",
      "Epoch 1879: train loss: 1.3807858522341121e-05, val loss: 0.39271166920661926\n",
      "Epoch 1880: train loss: 5.627260179608129e-05, val loss: 0.4002546966075897\n",
      "Epoch 1881: train loss: 2.11647929972969e-05, val loss: 0.3989775776863098\n",
      "Epoch 1882: train loss: 3.1819923606235534e-05, val loss: 0.3920162618160248\n",
      "Epoch 1883: train loss: 2.7982921892544255e-05, val loss: 0.39812302589416504\n",
      "Epoch 1884: train loss: 1.5282250387826934e-05, val loss: 0.40161076188087463\n",
      "Epoch 1885: train loss: 1.3666293853020761e-05, val loss: 0.39730212092399597\n",
      "Epoch 1886: train loss: 1.027150938170962e-05, val loss: 0.39592429995536804\n",
      "Epoch 1887: train loss: 9.880573998088948e-06, val loss: 0.3980729877948761\n",
      "Epoch 1888: train loss: 9.502807188255247e-06, val loss: 0.39856359362602234\n",
      "Epoch 1889: train loss: 1.1095503396063577e-05, val loss: 0.3972559869289398\n",
      "Epoch 1890: train loss: 8.818389687803574e-06, val loss: 0.3969179093837738\n",
      "Epoch 1891: train loss: 4.381087819638196e-06, val loss: 0.39884164929389954\n",
      "Epoch 1892: train loss: 5.801602128485683e-06, val loss: 0.3990590572357178\n",
      "Epoch 1893: train loss: 5.2869222599838395e-06, val loss: 0.3980274796485901\n",
      "Epoch 1894: train loss: 4.742704732052516e-06, val loss: 0.3979974687099457\n",
      "Epoch 1895: train loss: 5.415603027358884e-06, val loss: 0.3976331055164337\n",
      "Epoch 1896: train loss: 3.7357140172389336e-06, val loss: 0.39828968048095703\n",
      "Epoch 1897: train loss: 2.5909214400599012e-06, val loss: 0.3989847004413605\n",
      "Epoch 1898: train loss: 3.1314609714172548e-06, val loss: 0.3992372155189514\n",
      "Epoch 1899: train loss: 2.8135948468843708e-06, val loss: 0.3988710045814514\n",
      "Epoch 1900: train loss: 2.4710111574677285e-06, val loss: 0.397990345954895\n",
      "Epoch 1901: train loss: 2.7751307243306655e-06, val loss: 0.3987937867641449\n",
      "Epoch 1902: train loss: 2.337215391889913e-06, val loss: 0.3993225693702698\n",
      "Epoch 1903: train loss: 1.326733581663575e-06, val loss: 0.39919111132621765\n",
      "Epoch 1904: train loss: 1.3717256024392555e-06, val loss: 0.3990557789802551\n",
      "Epoch 1905: train loss: 1.5862457303228439e-06, val loss: 0.3991660177707672\n",
      "Epoch 1906: train loss: 1.5634814189979807e-06, val loss: 0.39955049753189087\n",
      "Epoch 1907: train loss: 1.450412696613057e-06, val loss: 0.3991113603115082\n",
      "Epoch 1908: train loss: 1.3674216461367905e-06, val loss: 0.399528831243515\n",
      "Epoch 1909: train loss: 6.246635280149349e-07, val loss: 0.3997736871242523\n",
      "Epoch 1910: train loss: 6.126028893049806e-07, val loss: 0.3995010554790497\n",
      "Epoch 1911: train loss: 9.168973065243335e-07, val loss: 0.39971524477005005\n",
      "Epoch 1912: train loss: 1.029850068334781e-06, val loss: 0.39971429109573364\n",
      "Epoch 1913: train loss: 7.035731641735765e-07, val loss: 0.39996537566185\n",
      "Epoch 1914: train loss: 8.287444188681548e-07, val loss: 0.3997039198875427\n",
      "Epoch 1915: train loss: 4.5542864768322033e-07, val loss: 0.3996123671531677\n",
      "Epoch 1916: train loss: 3.7407130548672285e-07, val loss: 0.3996584415435791\n",
      "Epoch 1917: train loss: 3.5687966715158836e-07, val loss: 0.39966461062431335\n",
      "Epoch 1918: train loss: 5.21478966675204e-07, val loss: 0.39979663491249084\n",
      "Epoch 1919: train loss: 3.9524869066553947e-07, val loss: 0.39939361810684204\n",
      "Epoch 1920: train loss: 4.22156318791167e-07, val loss: 0.39917251467704773\n",
      "Epoch 1921: train loss: 4.567075961858791e-07, val loss: 0.39881953597068787\n",
      "Epoch 1922: train loss: 3.4724735087365843e-07, val loss: 0.3987462520599365\n",
      "Epoch 1923: train loss: 2.8823791353715933e-07, val loss: 0.39843055605888367\n",
      "Epoch 1924: train loss: 2.2606876370900864e-07, val loss: 0.3980396091938019\n",
      "Epoch 1925: train loss: 2.2660019283193833e-07, val loss: 0.3975927531719208\n",
      "Epoch 1926: train loss: 1.682438721672952e-07, val loss: 0.3974592387676239\n",
      "Epoch 1927: train loss: 1.1689567713801807e-07, val loss: 0.397533655166626\n",
      "Epoch 1928: train loss: 1.273460270567739e-07, val loss: 0.39782366156578064\n",
      "Epoch 1929: train loss: 1.9893559510819614e-07, val loss: 0.3980036675930023\n",
      "Epoch 1930: train loss: 2.0496653974078072e-07, val loss: 0.39777326583862305\n",
      "Epoch 1931: train loss: 2.0585596871569578e-07, val loss: 0.39780211448669434\n",
      "Epoch 1932: train loss: 2.6131687036468065e-07, val loss: 0.3979673385620117\n",
      "Epoch 1933: train loss: 3.182840941917675e-07, val loss: 0.39814096689224243\n",
      "Epoch 1934: train loss: 4.0252646726912644e-07, val loss: 0.3980245292186737\n",
      "Epoch 1935: train loss: 4.908627033728408e-07, val loss: 0.3984214663505554\n",
      "Epoch 1936: train loss: 6.63950572743488e-07, val loss: 0.3982606530189514\n",
      "Epoch 1937: train loss: 9.753321137395687e-07, val loss: 0.3985055983066559\n",
      "Epoch 1938: train loss: 1.6061488850027672e-06, val loss: 0.3982434570789337\n",
      "Epoch 1939: train loss: 2.8660253974521765e-06, val loss: 0.3987623155117035\n",
      "Epoch 1940: train loss: 5.599087671726011e-06, val loss: 0.39822161197662354\n",
      "Epoch 1941: train loss: 1.0664673027349636e-05, val loss: 0.39937421679496765\n",
      "Epoch 1942: train loss: 1.874639383458998e-05, val loss: 0.398124635219574\n",
      "Epoch 1943: train loss: 3.7332116335164756e-05, val loss: 0.4003526270389557\n",
      "Epoch 1944: train loss: 7.273652590811253e-05, val loss: 0.3979516923427582\n",
      "Epoch 1945: train loss: 0.00013714644592255354, val loss: 0.4015170633792877\n",
      "Epoch 1946: train loss: 0.00023256363056134433, val loss: 0.3989171087741852\n",
      "Epoch 1947: train loss: 0.0003239217330701649, val loss: 0.4053914248943329\n",
      "Epoch 1948: train loss: 0.0003209352435078472, val loss: 0.4004271626472473\n",
      "Epoch 1949: train loss: 0.00016981280350591987, val loss: 0.40587082505226135\n",
      "Epoch 1950: train loss: 2.9262888347147964e-05, val loss: 0.4066692292690277\n",
      "Epoch 1951: train loss: 3.216471668565646e-05, val loss: 0.4025181829929352\n",
      "Epoch 1952: train loss: 0.00011286138033028692, val loss: 0.4087805449962616\n",
      "Epoch 1953: train loss: 0.00011655397975118831, val loss: 0.40712347626686096\n",
      "Epoch 1954: train loss: 5.010906170355156e-05, val loss: 0.407839834690094\n",
      "Epoch 1955: train loss: 3.081495378864929e-05, val loss: 0.4116460382938385\n",
      "Epoch 1956: train loss: 9.367198072141036e-05, val loss: 0.4096255302429199\n",
      "Epoch 1957: train loss: 0.00017561060667503625, val loss: 0.41496849060058594\n",
      "Epoch 1958: train loss: 0.00011511379125295207, val loss: 0.4121755063533783\n",
      "Epoch 1959: train loss: 1.793514638848137e-05, val loss: 0.4126470983028412\n",
      "Epoch 1960: train loss: 6.929311348358169e-05, val loss: 0.4171048104763031\n",
      "Epoch 1961: train loss: 0.00011629044456640258, val loss: 0.41276928782463074\n",
      "Epoch 1962: train loss: 4.30919935752172e-05, val loss: 0.4143381118774414\n",
      "Epoch 1963: train loss: 9.324574421043508e-06, val loss: 0.41754934191703796\n",
      "Epoch 1964: train loss: 6.616561586270109e-05, val loss: 0.41479578614234924\n",
      "Epoch 1965: train loss: 7.633152563357726e-05, val loss: 0.41661712527275085\n",
      "Epoch 1966: train loss: 2.6479399821255356e-05, val loss: 0.41764822602272034\n",
      "Epoch 1967: train loss: 3.079973248532042e-05, val loss: 0.41582512855529785\n",
      "Epoch 1968: train loss: 7.61989940656349e-05, val loss: 0.4184841215610504\n",
      "Epoch 1969: train loss: 6.735022907378152e-05, val loss: 0.41638994216918945\n",
      "Epoch 1970: train loss: 3.607553298934363e-05, val loss: 0.4165056347846985\n",
      "Epoch 1971: train loss: 4.651391282095574e-05, val loss: 0.4181205928325653\n",
      "Epoch 1972: train loss: 5.437006984720938e-05, val loss: 0.41498786211013794\n",
      "Epoch 1973: train loss: 1.952410275407601e-05, val loss: 0.41549068689346313\n",
      "Epoch 1974: train loss: 1.1935594557144213e-06, val loss: 0.4170145094394684\n",
      "Epoch 1975: train loss: 2.2813626856077462e-05, val loss: 0.4142580032348633\n",
      "Epoch 1976: train loss: 3.161611675750464e-05, val loss: 0.4139626920223236\n",
      "Epoch 1977: train loss: 1.6015430446714163e-05, val loss: 0.4142409861087799\n",
      "Epoch 1978: train loss: 1.2029380741296336e-05, val loss: 0.41334423422813416\n",
      "Epoch 1979: train loss: 1.8665665265871212e-05, val loss: 0.4144689738750458\n",
      "Epoch 1980: train loss: 1.128068379330216e-05, val loss: 0.41366368532180786\n",
      "Epoch 1981: train loss: 3.1055567433213582e-06, val loss: 0.4135499596595764\n",
      "Epoch 1982: train loss: 1.148976298281923e-05, val loss: 0.4145244061946869\n",
      "Epoch 1983: train loss: 1.7971980923903175e-05, val loss: 0.4134543836116791\n",
      "Epoch 1984: train loss: 8.750569577387068e-06, val loss: 0.41398268938064575\n",
      "Epoch 1985: train loss: 9.80714503384661e-07, val loss: 0.41421642899513245\n",
      "Epoch 1986: train loss: 5.5822815738793e-06, val loss: 0.4129720628261566\n",
      "Epoch 1987: train loss: 9.3677645054413e-06, val loss: 0.41415151953697205\n",
      "Epoch 1988: train loss: 5.703238457499538e-06, val loss: 0.41407233476638794\n",
      "Epoch 1989: train loss: 3.860706783598289e-06, val loss: 0.41284599900245667\n",
      "Epoch 1990: train loss: 6.239165031729499e-06, val loss: 0.41401633620262146\n",
      "Epoch 1991: train loss: 5.411568508861819e-06, val loss: 0.41365471482276917\n",
      "Epoch 1992: train loss: 1.4683264453196898e-06, val loss: 0.4135754108428955\n",
      "Epoch 1993: train loss: 1.3054368537268601e-06, val loss: 0.41395458579063416\n",
      "Epoch 1994: train loss: 4.62493972008815e-06, val loss: 0.4134694039821625\n",
      "Epoch 1995: train loss: 5.183951998333214e-06, val loss: 0.41400694847106934\n",
      "Epoch 1996: train loss: 2.487135816409136e-06, val loss: 0.41325852274894714\n",
      "Epoch 1997: train loss: 1.2809463214580319e-06, val loss: 0.41369104385375977\n",
      "Epoch 1998: train loss: 2.657916866155574e-06, val loss: 0.4134434759616852\n",
      "Epoch 1999: train loss: 5.158542990102433e-06, val loss: 0.4143257141113281\n",
      "Epoch 2000: train loss: 7.9333922258229e-06, val loss: 0.41470977663993835\n",
      "Epoch 2001: train loss: 1.1703033123922069e-05, val loss: 0.41555628180503845\n",
      "Epoch 2002: train loss: 1.1258726772211958e-05, val loss: 0.41496381163597107\n",
      "Epoch 2003: train loss: 6.99480006005615e-06, val loss: 0.41525617241859436\n",
      "Epoch 2004: train loss: 2.830169705703156e-06, val loss: 0.4159579277038574\n",
      "Epoch 2005: train loss: 3.1946156013873406e-06, val loss: 0.41568222641944885\n",
      "Epoch 2006: train loss: 5.48273101230734e-06, val loss: 0.41624870896339417\n",
      "Epoch 2007: train loss: 4.072779574926244e-06, val loss: 0.417894572019577\n",
      "Epoch 2008: train loss: 3.828857188636903e-06, val loss: 0.4183032214641571\n",
      "Epoch 2009: train loss: 4.682798135036137e-06, val loss: 0.41897058486938477\n",
      "Epoch 2010: train loss: 2.2349017854139674e-06, val loss: 0.41887155175209045\n",
      "Epoch 2011: train loss: 1.4642797623309889e-06, val loss: 0.41907811164855957\n",
      "Epoch 2012: train loss: 2.4038347419264028e-06, val loss: 0.4195795953273773\n",
      "Epoch 2013: train loss: 2.0712282093882095e-06, val loss: 0.41916561126708984\n",
      "Epoch 2014: train loss: 2.6335053462389624e-06, val loss: 0.41965022683143616\n",
      "Epoch 2015: train loss: 2.6337727376812836e-06, val loss: 0.4201158583164215\n",
      "Epoch 2016: train loss: 3.1884258078207495e-06, val loss: 0.4198150336742401\n",
      "Epoch 2017: train loss: 4.767121481563663e-06, val loss: 0.4190652072429657\n",
      "Epoch 2018: train loss: 7.796867066645063e-06, val loss: 0.4199000298976898\n",
      "Epoch 2019: train loss: 1.6204561688937247e-05, val loss: 0.4200497567653656\n",
      "Epoch 2020: train loss: 3.577622555894777e-05, val loss: 0.41871586441993713\n",
      "Epoch 2021: train loss: 7.189164170995355e-05, val loss: 0.41949397325515747\n",
      "Epoch 2022: train loss: 0.00014243564510252327, val loss: 0.42005807161331177\n",
      "Epoch 2023: train loss: 0.00023870443692430854, val loss: 0.4167180061340332\n",
      "Epoch 2024: train loss: 0.0003250399313401431, val loss: 0.41858768463134766\n",
      "Epoch 2025: train loss: 0.00037831466761417687, val loss: 0.4165247082710266\n",
      "Epoch 2026: train loss: 0.0003507264773361385, val loss: 0.4174751341342926\n",
      "Epoch 2027: train loss: 0.0002633120457176119, val loss: 0.4151054322719574\n",
      "Epoch 2028: train loss: 0.00013508858683053404, val loss: 0.4166615605354309\n",
      "Epoch 2029: train loss: 3.3523072488605976e-05, val loss: 0.41511964797973633\n",
      "Epoch 2030: train loss: 7.870389140407497e-07, val loss: 0.4137284457683563\n",
      "Epoch 2031: train loss: 3.690504308906384e-05, val loss: 0.4136411249637604\n",
      "Epoch 2032: train loss: 9.53176204347983e-05, val loss: 0.4114062488079071\n",
      "Epoch 2033: train loss: 0.00011805816029664129, val loss: 0.4115687906742096\n",
      "Epoch 2034: train loss: 8.777518814895302e-05, val loss: 0.41057321429252625\n",
      "Epoch 2035: train loss: 3.210673457942903e-05, val loss: 0.4097435176372528\n",
      "Epoch 2036: train loss: 1.1148670182592468e-06, val loss: 0.40883398056030273\n",
      "Epoch 2037: train loss: 1.3803165529679973e-05, val loss: 0.4077070355415344\n",
      "Epoch 2038: train loss: 4.4832344428868964e-05, val loss: 0.4081737697124481\n",
      "Epoch 2039: train loss: 5.685828364221379e-05, val loss: 0.40632709860801697\n",
      "Epoch 2040: train loss: 3.759096216526814e-05, val loss: 0.4063923954963684\n",
      "Epoch 2041: train loss: 9.552208211971447e-06, val loss: 0.4060976207256317\n",
      "Epoch 2042: train loss: 7.889393032201042e-07, val loss: 0.4053656756877899\n",
      "Epoch 2043: train loss: 1.4806144463364035e-05, val loss: 0.4060388505458832\n",
      "Epoch 2044: train loss: 3.0172563128871843e-05, val loss: 0.405017226934433\n",
      "Epoch 2045: train loss: 2.76829487120267e-05, val loss: 0.4055130183696747\n",
      "Epoch 2046: train loss: 1.1909301065315958e-05, val loss: 0.40525737404823303\n",
      "Epoch 2047: train loss: 6.508866476906405e-07, val loss: 0.4052034914493561\n",
      "Epoch 2048: train loss: 3.881367319991114e-06, val loss: 0.4057938754558563\n",
      "Epoch 2049: train loss: 1.4090530385146849e-05, val loss: 0.4051319658756256\n",
      "Epoch 2050: train loss: 1.7913591364049353e-05, val loss: 0.4056874215602875\n",
      "Epoch 2051: train loss: 1.1649735824903473e-05, val loss: 0.40556445717811584\n",
      "Epoch 2052: train loss: 2.818698249029694e-06, val loss: 0.40565356612205505\n",
      "Epoch 2053: train loss: 2.9980995464029547e-07, val loss: 0.4058937132358551\n",
      "Epoch 2054: train loss: 4.690265086537693e-06, val loss: 0.40561220049858093\n",
      "Epoch 2055: train loss: 9.647478691476863e-06, val loss: 0.40627118945121765\n",
      "Epoch 2056: train loss: 9.828349902818445e-06, val loss: 0.40608111023902893\n",
      "Epoch 2057: train loss: 5.692045760952169e-06, val loss: 0.40627995133399963\n",
      "Epoch 2058: train loss: 1.5871393088673358e-06, val loss: 0.4064883291721344\n",
      "Epoch 2059: train loss: 6.95148457907635e-07, val loss: 0.4063780903816223\n",
      "Epoch 2060: train loss: 2.5118604298768332e-06, val loss: 0.40681296586990356\n",
      "Epoch 2061: train loss: 4.562233243632363e-06, val loss: 0.4066266119480133\n",
      "Epoch 2062: train loss: 5.137500920682214e-06, val loss: 0.40695545077323914\n",
      "Epoch 2063: train loss: 4.498935140873073e-06, val loss: 0.40696650743484497\n",
      "Epoch 2064: train loss: 3.830592049780535e-06, val loss: 0.4068821966648102\n",
      "Epoch 2065: train loss: 3.5597515761764953e-06, val loss: 0.40719595551490784\n",
      "Epoch 2066: train loss: 3.441301032580668e-06, val loss: 0.4069409966468811\n",
      "Epoch 2067: train loss: 2.7995699838356813e-06, val loss: 0.4071807861328125\n",
      "Epoch 2068: train loss: 1.6876830386536312e-06, val loss: 0.40714484453201294\n",
      "Epoch 2069: train loss: 5.896839070373971e-07, val loss: 0.4071190357208252\n",
      "Epoch 2070: train loss: 6.820090447945404e-08, val loss: 0.4072561264038086\n",
      "Epoch 2071: train loss: 2.1981054487696383e-07, val loss: 0.40713056921958923\n",
      "Epoch 2072: train loss: 7.494601845792204e-07, val loss: 0.4072383940219879\n",
      "Epoch 2073: train loss: 1.3109691963109071e-06, val loss: 0.4071573317050934\n",
      "Epoch 2074: train loss: 1.77494075614959e-06, val loss: 0.4071994423866272\n",
      "Epoch 2075: train loss: 2.3955631149874534e-06, val loss: 0.40728655457496643\n",
      "Epoch 2076: train loss: 1.9325009361637058e-06, val loss: 0.4068458676338196\n",
      "Epoch 2077: train loss: 1.1451044201749028e-06, val loss: 0.40770530700683594\n",
      "Epoch 2078: train loss: 1.4711524727317737e-06, val loss: 0.40694233775138855\n",
      "Epoch 2079: train loss: 1.9779574813583167e-06, val loss: 0.4072449207305908\n",
      "Epoch 2080: train loss: 1.757300765348191e-06, val loss: 0.4074331223964691\n",
      "Epoch 2081: train loss: 1.5756055518068024e-06, val loss: 0.4065375030040741\n",
      "Epoch 2082: train loss: 2.05872424885456e-06, val loss: 0.4078519344329834\n",
      "Epoch 2083: train loss: 3.0819562653050525e-06, val loss: 0.40608856081962585\n",
      "Epoch 2084: train loss: 3.888908395310864e-06, val loss: 0.40743979811668396\n",
      "Epoch 2085: train loss: 3.8723678699170705e-06, val loss: 0.40649858117103577\n",
      "Epoch 2086: train loss: 5.964433057670249e-06, val loss: 0.4057788550853729\n",
      "Epoch 2087: train loss: 8.670512215758208e-06, val loss: 0.4068307876586914\n",
      "Epoch 2088: train loss: 1.07129271782469e-05, val loss: 0.40476685762405396\n",
      "Epoch 2089: train loss: 1.1669299055938609e-05, val loss: 0.4057336449623108\n",
      "Epoch 2090: train loss: 1.0621744877425954e-05, val loss: 0.40458938479423523\n",
      "Epoch 2091: train loss: 1.172537486127112e-05, val loss: 0.40564462542533875\n",
      "Epoch 2092: train loss: 1.2895959116576705e-05, val loss: 0.4057950973510742\n",
      "Epoch 2093: train loss: 1.8929522411781363e-05, val loss: 0.4043702185153961\n",
      "Epoch 2094: train loss: 2.1324578483472578e-05, val loss: 0.40483322739601135\n",
      "Epoch 2095: train loss: 1.5432675354531966e-05, val loss: 0.40357571840286255\n",
      "Epoch 2096: train loss: 1.1176737643836532e-05, val loss: 0.4046788215637207\n",
      "Epoch 2097: train loss: 2.0316711015766487e-05, val loss: 0.40378889441490173\n",
      "Epoch 2098: train loss: 6.026616392773576e-05, val loss: 0.40423107147216797\n",
      "Epoch 2099: train loss: 0.0001412472629453987, val loss: 0.4046599566936493\n",
      "Epoch 2100: train loss: 7.711076614214107e-05, val loss: 0.4040810167789459\n",
      "Epoch 2101: train loss: 4.049759081681259e-05, val loss: 0.4028739631175995\n",
      "Epoch 2102: train loss: 0.00011752185673685744, val loss: 0.40249061584472656\n",
      "Epoch 2103: train loss: 7.770611409796402e-05, val loss: 0.4055808484554291\n",
      "Epoch 2104: train loss: 9.203567606164142e-05, val loss: 0.4074483811855316\n",
      "Epoch 2105: train loss: 0.00010107846173923463, val loss: 0.40337371826171875\n",
      "Epoch 2106: train loss: 4.540092049865052e-05, val loss: 0.4078766405582428\n",
      "Epoch 2107: train loss: 7.873854337958619e-05, val loss: 0.4068383276462555\n",
      "Epoch 2108: train loss: 0.00010433149873279035, val loss: 0.405277818441391\n",
      "Epoch 2109: train loss: 0.00011467215517768636, val loss: 0.4049191474914551\n",
      "Epoch 2110: train loss: 0.0001707957126200199, val loss: 0.40857553482055664\n",
      "Epoch 2111: train loss: 0.0002426584396744147, val loss: 0.4008125960826874\n",
      "Epoch 2112: train loss: 0.0003646555996965617, val loss: 0.40776950120925903\n",
      "Epoch 2113: train loss: 0.0003027395869139582, val loss: 0.4005596339702606\n",
      "Epoch 2114: train loss: 6.331840995699167e-05, val loss: 0.40295955538749695\n",
      "Epoch 2115: train loss: 8.070253534242511e-05, val loss: 0.4055860638618469\n",
      "Epoch 2116: train loss: 0.0002168118953704834, val loss: 0.3980095386505127\n",
      "Epoch 2117: train loss: 0.00019397848518565297, val loss: 0.40264707803726196\n",
      "Epoch 2118: train loss: 4.7863315558061004e-05, val loss: 0.40377840399742126\n",
      "Epoch 2119: train loss: 4.3756805098382756e-05, val loss: 0.3964259922504425\n",
      "Epoch 2120: train loss: 0.0001438832696294412, val loss: 0.40218067169189453\n",
      "Epoch 2121: train loss: 9.694209438748658e-05, val loss: 0.4006032943725586\n",
      "Epoch 2122: train loss: 1.223495837621158e-05, val loss: 0.39819446206092834\n",
      "Epoch 2123: train loss: 4.324359542806633e-05, val loss: 0.40034371614456177\n",
      "Epoch 2124: train loss: 8.968699694378302e-05, val loss: 0.39718708395957947\n",
      "Epoch 2125: train loss: 4.5400465751299635e-05, val loss: 0.399335116147995\n",
      "Epoch 2126: train loss: 6.327833034447394e-06, val loss: 0.39874011278152466\n",
      "Epoch 2127: train loss: 5.30955549038481e-05, val loss: 0.39530086517333984\n",
      "Epoch 2128: train loss: 5.6041539210127667e-05, val loss: 0.39925119280815125\n",
      "Epoch 2129: train loss: 8.459662240056787e-06, val loss: 0.39822426438331604\n",
      "Epoch 2130: train loss: 3.1043902708915994e-05, val loss: 0.39243030548095703\n",
      "Epoch 2131: train loss: 3.4169937862316146e-05, val loss: 0.39889103174209595\n",
      "Epoch 2132: train loss: 6.28939233138226e-06, val loss: 0.3989458978176117\n",
      "Epoch 2133: train loss: 2.108138687617611e-05, val loss: 0.39123719930648804\n",
      "Epoch 2134: train loss: 2.343572850804776e-05, val loss: 0.3965630531311035\n",
      "Epoch 2135: train loss: 4.791750598087674e-06, val loss: 0.3986293375492096\n",
      "Epoch 2136: train loss: 1.668875484028831e-05, val loss: 0.3914638161659241\n",
      "Epoch 2137: train loss: 1.8459179045748897e-05, val loss: 0.3947910964488983\n",
      "Epoch 2138: train loss: 3.4939200759254163e-06, val loss: 0.3982352614402771\n",
      "Epoch 2139: train loss: 1.4753757568541914e-05, val loss: 0.3917427957057953\n",
      "Epoch 2140: train loss: 1.2353395504760556e-05, val loss: 0.3937898576259613\n",
      "Epoch 2141: train loss: 2.558428604970686e-06, val loss: 0.3969840109348297\n",
      "Epoch 2142: train loss: 1.1145797543576919e-05, val loss: 0.3925923705101013\n",
      "Epoch 2143: train loss: 7.160571385611547e-06, val loss: 0.3935219943523407\n",
      "Epoch 2144: train loss: 2.4901655706344172e-06, val loss: 0.39606955647468567\n",
      "Epoch 2145: train loss: 7.574819392175414e-06, val loss: 0.3927840292453766\n",
      "Epoch 2146: train loss: 4.752854692924302e-06, val loss: 0.3933880031108856\n",
      "Epoch 2147: train loss: 2.7764572223532014e-06, val loss: 0.3954760730266571\n",
      "Epoch 2148: train loss: 5.2938635235477705e-06, val loss: 0.3930610120296478\n",
      "Epoch 2149: train loss: 3.923048552678665e-06, val loss: 0.3937384784221649\n",
      "Epoch 2150: train loss: 2.82979726762278e-06, val loss: 0.3947422206401825\n",
      "Epoch 2151: train loss: 4.169710337009747e-06, val loss: 0.393276572227478\n",
      "Epoch 2152: train loss: 3.418080041228677e-06, val loss: 0.3937589228153229\n",
      "Epoch 2153: train loss: 2.826503305186634e-06, val loss: 0.3947433829307556\n",
      "Epoch 2154: train loss: 3.998324245912954e-06, val loss: 0.39335325360298157\n",
      "Epoch 2155: train loss: 3.23518770528608e-06, val loss: 0.3942456543445587\n",
      "Epoch 2156: train loss: 3.4903034702438163e-06, val loss: 0.394264280796051\n",
      "Epoch 2157: train loss: 4.951124992658151e-06, val loss: 0.3938632011413574\n",
      "Epoch 2158: train loss: 5.008947482565418e-06, val loss: 0.39392805099487305\n",
      "Epoch 2159: train loss: 6.582399691978935e-06, val loss: 0.3947499394416809\n",
      "Epoch 2160: train loss: 9.893718925013673e-06, val loss: 0.3934547007083893\n",
      "Epoch 2161: train loss: 1.3967963241157122e-05, val loss: 0.3945571482181549\n",
      "Epoch 2162: train loss: 2.2116275431471877e-05, val loss: 0.39370933175086975\n",
      "Epoch 2163: train loss: 3.653797466540709e-05, val loss: 0.3945215046405792\n",
      "Epoch 2164: train loss: 6.23690357315354e-05, val loss: 0.39293792843818665\n",
      "Epoch 2165: train loss: 0.00010979623039020225, val loss: 0.3957809507846832\n",
      "Epoch 2166: train loss: 0.000202985480427742, val loss: 0.3914795517921448\n",
      "Epoch 2167: train loss: 0.0003703589318320155, val loss: 0.3982402980327606\n",
      "Epoch 2168: train loss: 0.0006949746166355908, val loss: 0.38966771960258484\n",
      "Epoch 2169: train loss: 0.0012613472063094378, val loss: 0.397483229637146\n",
      "Epoch 2170: train loss: 0.002203363925218582, val loss: 0.38432368636131287\n",
      "Epoch 2171: train loss: 0.002990822773426771, val loss: 0.39119425415992737\n",
      "Epoch 2172: train loss: 0.002351116854697466, val loss: 0.3688843846321106\n",
      "Epoch 2173: train loss: 0.000413031637435779, val loss: 0.37243759632110596\n",
      "Epoch 2174: train loss: 0.0002989594067912549, val loss: 0.3774694502353668\n",
      "Epoch 2175: train loss: 0.0014118228573352098, val loss: 0.3559097945690155\n",
      "Epoch 2176: train loss: 0.0008872610051184893, val loss: 0.36688530445098877\n",
      "Epoch 2177: train loss: 8.391602023039013e-05, val loss: 0.365123450756073\n",
      "Epoch 2178: train loss: 0.0006550654070451856, val loss: 0.34379008412361145\n",
      "Epoch 2179: train loss: 0.0006660561775788665, val loss: 0.3556373715400696\n",
      "Epoch 2180: train loss: 8.872633770806715e-05, val loss: 0.35962435603141785\n",
      "Epoch 2181: train loss: 0.00040599179919809103, val loss: 0.3370850682258606\n",
      "Epoch 2182: train loss: 0.0004197588423267007, val loss: 0.3413166105747223\n",
      "Epoch 2183: train loss: 7.148908480303362e-05, val loss: 0.35129377245903015\n",
      "Epoch 2184: train loss: 0.000316162797389552, val loss: 0.333682119846344\n",
      "Epoch 2185: train loss: 0.0002317485777894035, val loss: 0.3302396237850189\n",
      "Epoch 2186: train loss: 7.181079854490235e-05, val loss: 0.3442832827568054\n",
      "Epoch 2187: train loss: 0.00025225887657143176, val loss: 0.33527252078056335\n",
      "Epoch 2188: train loss: 0.00010758198914118111, val loss: 0.3238995671272278\n",
      "Epoch 2189: train loss: 9.239860810339451e-05, val loss: 0.33319076895713806\n",
      "Epoch 2190: train loss: 0.00017478829249739647, val loss: 0.3333331048488617\n",
      "Epoch 2191: train loss: 4.859123509959318e-05, val loss: 0.3225056827068329\n",
      "Epoch 2192: train loss: 0.00010709506022976711, val loss: 0.325692743062973\n",
      "Epoch 2193: train loss: 9.722161485115066e-05, val loss: 0.32972538471221924\n",
      "Epoch 2194: train loss: 3.850423308904283e-05, val loss: 0.321409672498703\n",
      "Epoch 2195: train loss: 9.396106179337949e-05, val loss: 0.31948423385620117\n",
      "Epoch 2196: train loss: 4.568884469335899e-05, val loss: 0.3255274295806885\n",
      "Epoch 2197: train loss: 4.440567863639444e-05, val loss: 0.32181844115257263\n",
      "Epoch 2198: train loss: 6.636472244281322e-05, val loss: 0.31587493419647217\n",
      "Epoch 2199: train loss: 2.249218050565105e-05, val loss: 0.3208172023296356\n",
      "Epoch 2200: train loss: 4.651578638004139e-05, val loss: 0.32204464077949524\n",
      "Epoch 2201: train loss: 3.884821126121096e-05, val loss: 0.3152455985546112\n",
      "Epoch 2202: train loss: 1.6055171727202833e-05, val loss: 0.316862016916275\n",
      "Epoch 2203: train loss: 4.235418600728735e-05, val loss: 0.32117387652397156\n",
      "Epoch 2204: train loss: 1.6336322005372494e-05, val loss: 0.3161388039588928\n",
      "Epoch 2205: train loss: 2.082825631077867e-05, val loss: 0.3142174780368805\n",
      "Epoch 2206: train loss: 2.8993184969294816e-05, val loss: 0.3190706670284271\n",
      "Epoch 2207: train loss: 6.091734121582704e-06, val loss: 0.31723102927207947\n",
      "Epoch 2208: train loss: 2.5494018700555898e-05, val loss: 0.3130633234977722\n",
      "Epoch 2209: train loss: 1.1726268894562963e-05, val loss: 0.3162330687046051\n",
      "Epoch 2210: train loss: 9.20379170565866e-06, val loss: 0.31798630952835083\n",
      "Epoch 2211: train loss: 2.0361396309453994e-05, val loss: 0.31402167677879333\n",
      "Epoch 2212: train loss: 2.312244532731711e-06, val loss: 0.3139413297176361\n",
      "Epoch 2213: train loss: 1.4297139387053903e-05, val loss: 0.3171926438808441\n",
      "Epoch 2214: train loss: 9.543588930682745e-06, val loss: 0.31557390093803406\n",
      "Epoch 2215: train loss: 2.8719550755340606e-06, val loss: 0.3133719265460968\n",
      "Epoch 2216: train loss: 1.332010197074851e-05, val loss: 0.31593987345695496\n",
      "Epoch 2217: train loss: 2.133931729986216e-06, val loss: 0.316427081823349\n",
      "Epoch 2218: train loss: 6.464104899350787e-06, val loss: 0.3134940564632416\n",
      "Epoch 2219: train loss: 7.996010026545264e-06, val loss: 0.31461164355278015\n",
      "Epoch 2220: train loss: 5.731978944822913e-07, val loss: 0.3167787492275238\n",
      "Epoch 2221: train loss: 7.450703833455918e-06, val loss: 0.3146684467792511\n",
      "Epoch 2222: train loss: 2.9883090064686257e-06, val loss: 0.31401631236076355\n",
      "Epoch 2223: train loss: 1.988379608519608e-06, val loss: 0.3162424564361572\n",
      "Epoch 2224: train loss: 5.64667743674363e-06, val loss: 0.3156615197658539\n",
      "Epoch 2225: train loss: 9.298784675593197e-07, val loss: 0.3142479956150055\n",
      "Epoch 2226: train loss: 2.996174316649558e-06, val loss: 0.3157225549221039\n",
      "Epoch 2227: train loss: 3.1882698294793954e-06, val loss: 0.3164178431034088\n",
      "Epoch 2228: train loss: 8.444785635219887e-07, val loss: 0.314897745847702\n",
      "Epoch 2229: train loss: 2.715686605370138e-06, val loss: 0.31531140208244324\n",
      "Epoch 2230: train loss: 1.6725548448448535e-06, val loss: 0.3168201446533203\n",
      "Epoch 2231: train loss: 1.15002899292449e-06, val loss: 0.31579551100730896\n",
      "Epoch 2232: train loss: 1.85487738235679e-06, val loss: 0.31528040766716003\n",
      "Epoch 2233: train loss: 1.1123585181849194e-06, val loss: 0.3171079456806183\n",
      "Epoch 2234: train loss: 1.18853279218456e-06, val loss: 0.3169732987880707\n",
      "Epoch 2235: train loss: 1.061084731190931e-06, val loss: 0.3156624734401703\n",
      "Epoch 2236: train loss: 1.0156471716982196e-06, val loss: 0.31774482131004333\n",
      "Epoch 2237: train loss: 1.0072512850456405e-06, val loss: 0.3179461658000946\n",
      "Epoch 2238: train loss: 5.137978860147996e-07, val loss: 0.3163047730922699\n",
      "Epoch 2239: train loss: 9.444520401302725e-07, val loss: 0.3183784782886505\n",
      "Epoch 2240: train loss: 8.288937465295021e-07, val loss: 0.3187476694583893\n",
      "Epoch 2241: train loss: 2.4926009700720897e-07, val loss: 0.31710147857666016\n",
      "Epoch 2242: train loss: 7.213510571091319e-07, val loss: 0.319028377532959\n",
      "Epoch 2243: train loss: 6.852915817034955e-07, val loss: 0.31920167803764343\n",
      "Epoch 2244: train loss: 2.71496645609659e-07, val loss: 0.317917138338089\n",
      "Epoch 2245: train loss: 4.3658204162966285e-07, val loss: 0.31982773542404175\n",
      "Epoch 2246: train loss: 4.6361893168977986e-07, val loss: 0.31961461901664734\n",
      "Epoch 2247: train loss: 4.124568988572719e-07, val loss: 0.3189595639705658\n",
      "Epoch 2248: train loss: 3.358987612500641e-07, val loss: 0.3204309046268463\n",
      "Epoch 2249: train loss: 1.8990937178386957e-07, val loss: 0.3199774920940399\n",
      "Epoch 2250: train loss: 3.8236524346757506e-07, val loss: 0.320250928401947\n",
      "Epoch 2251: train loss: 4.1521360572005506e-07, val loss: 0.32069888710975647\n",
      "Epoch 2252: train loss: 1.2748819244734477e-07, val loss: 0.3208799958229065\n",
      "Epoch 2253: train loss: 1.3542539534228126e-07, val loss: 0.3210338354110718\n",
      "Epoch 2254: train loss: 2.5652414592514106e-07, val loss: 0.3210163712501526\n",
      "Epoch 2255: train loss: 2.5789762503336533e-07, val loss: 0.32203349471092224\n",
      "Epoch 2256: train loss: 2.920758959135128e-07, val loss: 0.32086339592933655\n",
      "Epoch 2257: train loss: 3.6905768752149015e-07, val loss: 0.3228864371776581\n",
      "Epoch 2258: train loss: 6.946289090592472e-07, val loss: 0.32086634635925293\n",
      "Epoch 2259: train loss: 1.8517351918490021e-06, val loss: 0.32446980476379395\n",
      "Epoch 2260: train loss: 5.552036782319192e-06, val loss: 0.3186165988445282\n",
      "Epoch 2261: train loss: 1.8105902199749835e-05, val loss: 0.32856711745262146\n",
      "Epoch 2262: train loss: 4.388617162476294e-05, val loss: 0.3162815272808075\n",
      "Epoch 2263: train loss: 8.717793389223516e-05, val loss: 0.3269389569759369\n",
      "Epoch 2264: train loss: 8.89747534529306e-05, val loss: 0.3256705403327942\n",
      "Epoch 2265: train loss: 6.895840488141403e-05, val loss: 0.32227131724357605\n",
      "Epoch 2266: train loss: 2.6072895707329735e-05, val loss: 0.3217046856880188\n",
      "Epoch 2267: train loss: 9.499985026195645e-06, val loss: 0.32584714889526367\n",
      "Epoch 2268: train loss: 2.447555380058475e-05, val loss: 0.3264743983745575\n",
      "Epoch 2269: train loss: 4.22135490225628e-05, val loss: 0.3204197585582733\n",
      "Epoch 2270: train loss: 4.311289012548514e-05, val loss: 0.32680410146713257\n",
      "Epoch 2271: train loss: 2.2264923245529644e-05, val loss: 0.32473477721214294\n",
      "Epoch 2272: train loss: 5.2435607358347625e-06, val loss: 0.3221609592437744\n",
      "Epoch 2273: train loss: 1.3101817785354797e-05, val loss: 0.325419157743454\n",
      "Epoch 2274: train loss: 2.3428558051818982e-05, val loss: 0.3235807418823242\n",
      "Epoch 2275: train loss: 2.1246891265036538e-05, val loss: 0.3255915641784668\n",
      "Epoch 2276: train loss: 1.1577203622437082e-05, val loss: 0.3243862986564636\n",
      "Epoch 2277: train loss: 5.855006747879088e-06, val loss: 0.3240712285041809\n",
      "Epoch 2278: train loss: 7.554732292192057e-06, val loss: 0.32566171884536743\n",
      "Epoch 2279: train loss: 1.3091187611280475e-05, val loss: 0.32401758432388306\n",
      "Epoch 2280: train loss: 1.2344711649348028e-05, val loss: 0.32626044750213623\n",
      "Epoch 2281: train loss: 7.231277777464129e-06, val loss: 0.32500919699668884\n",
      "Epoch 2282: train loss: 4.639415237761568e-06, val loss: 0.3245128095149994\n",
      "Epoch 2283: train loss: 4.579349479172379e-06, val loss: 0.32622653245925903\n",
      "Epoch 2284: train loss: 6.307432613539277e-06, val loss: 0.32451650500297546\n",
      "Epoch 2285: train loss: 7.765948794258293e-06, val loss: 0.32586488127708435\n",
      "Epoch 2286: train loss: 5.3658709475712385e-06, val loss: 0.32615169882774353\n",
      "Epoch 2287: train loss: 3.0127473564789398e-06, val loss: 0.3252080976963043\n",
      "Epoch 2288: train loss: 3.1948957257554866e-06, val loss: 0.32644763588905334\n",
      "Epoch 2289: train loss: 5.314547706802841e-06, val loss: 0.3256710171699524\n",
      "Epoch 2290: train loss: 6.087439032853581e-06, val loss: 0.32699471712112427\n",
      "Epoch 2291: train loss: 5.804807642562082e-06, val loss: 0.32583189010620117\n",
      "Epoch 2292: train loss: 4.465403890208108e-06, val loss: 0.32613345980644226\n",
      "Epoch 2293: train loss: 4.034835001220927e-06, val loss: 0.32689544558525085\n",
      "Epoch 2294: train loss: 5.898313247598708e-06, val loss: 0.32712358236312866\n",
      "Epoch 2295: train loss: 8.254287422460038e-06, val loss: 0.32688164710998535\n",
      "Epoch 2296: train loss: 9.36042306420859e-06, val loss: 0.32678431272506714\n",
      "Epoch 2297: train loss: 1.0469158041814808e-05, val loss: 0.3269159495830536\n",
      "Epoch 2298: train loss: 1.3298399608174805e-05, val loss: 0.32833340764045715\n",
      "Epoch 2299: train loss: 1.885576239146758e-05, val loss: 0.3259446918964386\n",
      "Epoch 2300: train loss: 2.832987593137659e-05, val loss: 0.3287743330001831\n",
      "Epoch 2301: train loss: 4.213302236166783e-05, val loss: 0.3264428675174713\n",
      "Epoch 2302: train loss: 6.222983938641846e-05, val loss: 0.3288678526878357\n",
      "Epoch 2303: train loss: 9.196373139275238e-05, val loss: 0.3258019685745239\n",
      "Epoch 2304: train loss: 0.00013424539065454155, val loss: 0.3299992084503174\n",
      "Epoch 2305: train loss: 0.00017478295194450766, val loss: 0.3256154954433441\n",
      "Epoch 2306: train loss: 0.00018974310660269111, val loss: 0.32609492540359497\n",
      "Epoch 2307: train loss: 0.00011770948185585439, val loss: 0.32172781229019165\n",
      "Epoch 2308: train loss: 3.264518454670906e-05, val loss: 0.31785279512405396\n",
      "Epoch 2309: train loss: 1.4755662050447427e-05, val loss: 0.32010945677757263\n",
      "Epoch 2310: train loss: 5.231476097833365e-05, val loss: 0.31501978635787964\n",
      "Epoch 2311: train loss: 9.012025111587718e-05, val loss: 0.3112838864326477\n",
      "Epoch 2312: train loss: 6.708590808557346e-05, val loss: 0.3145151138305664\n",
      "Epoch 2313: train loss: 2.8321219360805117e-05, val loss: 0.3072679936885834\n",
      "Epoch 2314: train loss: 1.80430433829315e-05, val loss: 0.3056747615337372\n",
      "Epoch 2315: train loss: 2.789720929285977e-05, val loss: 0.3056151270866394\n",
      "Epoch 2316: train loss: 3.550219116732478e-05, val loss: 0.3014882504940033\n",
      "Epoch 2317: train loss: 3.430515789659694e-05, val loss: 0.30131182074546814\n",
      "Epoch 2318: train loss: 2.423959449515678e-05, val loss: 0.2996973693370819\n",
      "Epoch 2319: train loss: 1.5039725440146867e-05, val loss: 0.29973259568214417\n",
      "Epoch 2320: train loss: 1.2663103007071186e-05, val loss: 0.29821428656578064\n",
      "Epoch 2321: train loss: 1.659306690271478e-05, val loss: 0.29713502526283264\n",
      "Epoch 2322: train loss: 2.1405639927252196e-05, val loss: 0.2984461486339569\n",
      "Epoch 2323: train loss: 1.6825057173264213e-05, val loss: 0.2959515154361725\n",
      "Epoch 2324: train loss: 7.208514489320805e-06, val loss: 0.2962644696235657\n",
      "Epoch 2325: train loss: 4.795774657395668e-06, val loss: 0.2973800599575043\n",
      "Epoch 2326: train loss: 1.133335808845004e-05, val loss: 0.2945155203342438\n",
      "Epoch 2327: train loss: 1.4624014511355199e-05, val loss: 0.29648709297180176\n",
      "Epoch 2328: train loss: 8.60306499816943e-06, val loss: 0.29668450355529785\n",
      "Epoch 2329: train loss: 2.0261311419744743e-06, val loss: 0.29407596588134766\n",
      "Epoch 2330: train loss: 3.839545570372138e-06, val loss: 0.29677921533584595\n",
      "Epoch 2331: train loss: 8.41639302961994e-06, val loss: 0.29590103030204773\n",
      "Epoch 2332: train loss: 7.766384442220442e-06, val loss: 0.2945399284362793\n",
      "Epoch 2333: train loss: 4.1367106859979685e-06, val loss: 0.29699426889419556\n",
      "Epoch 2334: train loss: 2.1840676254214486e-06, val loss: 0.2957479953765869\n",
      "Epoch 2335: train loss: 3.3886715300468495e-06, val loss: 0.2953557074069977\n",
      "Epoch 2336: train loss: 4.276277195458533e-06, val loss: 0.2968461215496063\n",
      "Epoch 2337: train loss: 3.4122440411010757e-06, val loss: 0.29607221484184265\n",
      "Epoch 2338: train loss: 2.8121671675762627e-06, val loss: 0.296071857213974\n",
      "Epoch 2339: train loss: 3.21916695611435e-06, val loss: 0.29707562923431396\n",
      "Epoch 2340: train loss: 2.752875388978282e-06, val loss: 0.29649659991264343\n",
      "Epoch 2341: train loss: 1.0666630032574176e-06, val loss: 0.29674169421195984\n",
      "Epoch 2342: train loss: 9.818719490795047e-07, val loss: 0.29745227098464966\n",
      "Epoch 2343: train loss: 2.3151037567004096e-06, val loss: 0.29697272181510925\n",
      "Epoch 2344: train loss: 2.9057769097562414e-06, val loss: 0.29736781120300293\n",
      "Epoch 2345: train loss: 2.1445277980092214e-06, val loss: 0.298043817281723\n",
      "Epoch 2346: train loss: 8.832259368318773e-07, val loss: 0.2977195382118225\n",
      "Epoch 2347: train loss: 4.207485915230791e-07, val loss: 0.297837495803833\n",
      "Epoch 2348: train loss: 8.873663546182797e-07, val loss: 0.29887378215789795\n",
      "Epoch 2349: train loss: 1.4155019698591786e-06, val loss: 0.2979515492916107\n",
      "Epoch 2350: train loss: 1.2588279787451029e-06, val loss: 0.2990005910396576\n",
      "Epoch 2351: train loss: 9.98479436020716e-07, val loss: 0.29916319251060486\n",
      "Epoch 2352: train loss: 1.123460947383137e-06, val loss: 0.2987503111362457\n",
      "Epoch 2353: train loss: 1.315289068770653e-06, val loss: 0.3000001013278961\n",
      "Epoch 2354: train loss: 1.5381100411104853e-06, val loss: 0.2990870475769043\n",
      "Epoch 2355: train loss: 1.921597913678852e-06, val loss: 0.3006487786769867\n",
      "Epoch 2356: train loss: 3.562335450624232e-06, val loss: 0.2992097735404968\n",
      "Epoch 2357: train loss: 9.370814950671047e-06, val loss: 0.3026115596294403\n",
      "Epoch 2358: train loss: 2.5076478777918965e-05, val loss: 0.299067884683609\n",
      "Epoch 2359: train loss: 4.20902251789812e-05, val loss: 0.30338966846466064\n",
      "Epoch 2360: train loss: 5.031203181715682e-05, val loss: 0.30022984743118286\n",
      "Epoch 2361: train loss: 5.8625049859983847e-05, val loss: 0.30180102586746216\n",
      "Epoch 2362: train loss: 8.573068043915555e-05, val loss: 0.30000802874565125\n",
      "Epoch 2363: train loss: 0.00012011499347863719, val loss: 0.30400317907333374\n",
      "Epoch 2364: train loss: 0.00013332387607079, val loss: 0.299249529838562\n",
      "Epoch 2365: train loss: 0.00011830301082227379, val loss: 0.3013486862182617\n",
      "Epoch 2366: train loss: 0.00010355191625421867, val loss: 0.3038678467273712\n",
      "Epoch 2367: train loss: 0.00012294304906390607, val loss: 0.29710468649864197\n",
      "Epoch 2368: train loss: 0.00017998127441387624, val loss: 0.312183678150177\n",
      "Epoch 2369: train loss: 0.00024305532861035317, val loss: 0.2963480055332184\n",
      "Epoch 2370: train loss: 0.000295002362690866, val loss: 0.31440314650535583\n",
      "Epoch 2371: train loss: 0.00032873128657229245, val loss: 0.3006531298160553\n",
      "Epoch 2372: train loss: 0.00031381435110233724, val loss: 0.3152175545692444\n",
      "Epoch 2373: train loss: 0.00010945912799797952, val loss: 0.3183549642562866\n",
      "Epoch 2374: train loss: 5.269491521175951e-05, val loss: 0.30672603845596313\n",
      "Epoch 2375: train loss: 0.0001640892878640443, val loss: 0.32129764556884766\n",
      "Epoch 2376: train loss: 8.346039976458997e-05, val loss: 0.3128642141819\n",
      "Epoch 2377: train loss: 6.568059325218201e-05, val loss: 0.31706342101097107\n",
      "Epoch 2378: train loss: 0.0001207571622217074, val loss: 0.32121825218200684\n",
      "Epoch 2379: train loss: 6.48327695671469e-05, val loss: 0.30908671021461487\n",
      "Epoch 2380: train loss: 5.4648964578518644e-05, val loss: 0.31883111596107483\n",
      "Epoch 2381: train loss: 5.7074987125815824e-05, val loss: 0.31825003027915955\n",
      "Epoch 2382: train loss: 3.483014734229073e-05, val loss: 0.3099628984928131\n",
      "Epoch 2383: train loss: 5.984479867038317e-05, val loss: 0.3177652060985565\n",
      "Epoch 2384: train loss: 4.604693822329864e-05, val loss: 0.31548556685447693\n",
      "Epoch 2385: train loss: 2.8221806132933125e-05, val loss: 0.309885710477829\n",
      "Epoch 2386: train loss: 3.893548637279309e-05, val loss: 0.3169271647930145\n",
      "Epoch 2387: train loss: 2.1247238692012616e-05, val loss: 0.31265756487846375\n",
      "Epoch 2388: train loss: 2.1191024643485434e-05, val loss: 0.3107786476612091\n",
      "Epoch 2389: train loss: 3.345592267578468e-05, val loss: 0.3176955580711365\n",
      "Epoch 2390: train loss: 2.4362636395380832e-05, val loss: 0.31248971819877625\n",
      "Epoch 2391: train loss: 1.7654881958151236e-05, val loss: 0.3121320903301239\n",
      "Epoch 2392: train loss: 1.7020767700159922e-05, val loss: 0.3165301978588104\n",
      "Epoch 2393: train loss: 1.2348605196166318e-05, val loss: 0.3121814429759979\n",
      "Epoch 2394: train loss: 1.28343463074998e-05, val loss: 0.3136657476425171\n",
      "Epoch 2395: train loss: 1.618927126401104e-05, val loss: 0.3166252672672272\n",
      "Epoch 2396: train loss: 1.3952673725725617e-05, val loss: 0.3126818835735321\n",
      "Epoch 2397: train loss: 1.0721690159698483e-05, val loss: 0.3139188885688782\n",
      "Epoch 2398: train loss: 9.318605407315772e-06, val loss: 0.31539034843444824\n",
      "Epoch 2399: train loss: 7.188742074504262e-06, val loss: 0.31361013650894165\n",
      "Epoch 2400: train loss: 6.7565674726211e-06, val loss: 0.316033273935318\n",
      "Epoch 2401: train loss: 7.538298632425722e-06, val loss: 0.31447502970695496\n",
      "Epoch 2402: train loss: 6.7894929998146836e-06, val loss: 0.3151702880859375\n",
      "Epoch 2403: train loss: 7.143698439904256e-06, val loss: 0.3162388801574707\n",
      "Epoch 2404: train loss: 5.404620878834976e-06, val loss: 0.31439924240112305\n",
      "Epoch 2405: train loss: 4.387140961625846e-06, val loss: 0.31755781173706055\n",
      "Epoch 2406: train loss: 5.965756827208679e-06, val loss: 0.3153317868709564\n",
      "Epoch 2407: train loss: 4.2050178308272734e-06, val loss: 0.31606248021125793\n",
      "Epoch 2408: train loss: 2.819972451106878e-06, val loss: 0.317579984664917\n",
      "Epoch 2409: train loss: 3.970599664171459e-06, val loss: 0.31672340631484985\n",
      "Epoch 2410: train loss: 3.2221400942944456e-06, val loss: 0.31711816787719727\n",
      "Epoch 2411: train loss: 1.7046314724211697e-06, val loss: 0.3178892135620117\n",
      "Epoch 2412: train loss: 1.9333558611833723e-06, val loss: 0.31771281361579895\n",
      "Epoch 2413: train loss: 2.5558551897120196e-06, val loss: 0.3178758919239044\n",
      "Epoch 2414: train loss: 2.7242051601206185e-06, val loss: 0.3194703161716461\n",
      "Epoch 2415: train loss: 2.5127869776042644e-06, val loss: 0.3179232180118561\n",
      "Epoch 2416: train loss: 2.572069888628903e-06, val loss: 0.3199470639228821\n",
      "Epoch 2417: train loss: 3.423769612709293e-06, val loss: 0.3187600076198578\n",
      "Epoch 2418: train loss: 3.7022766719019273e-06, val loss: 0.32030776143074036\n",
      "Epoch 2419: train loss: 4.013338184449822e-06, val loss: 0.3185248076915741\n",
      "Epoch 2420: train loss: 5.47376885151607e-06, val loss: 0.32142505049705505\n",
      "Epoch 2421: train loss: 8.146074833348393e-06, val loss: 0.31748542189598083\n",
      "Epoch 2422: train loss: 1.2713515388895757e-05, val loss: 0.32208868861198425\n",
      "Epoch 2423: train loss: 2.1380350517574698e-05, val loss: 0.3163966238498688\n",
      "Epoch 2424: train loss: 3.728500450961292e-05, val loss: 0.3221670687198639\n",
      "Epoch 2425: train loss: 6.784081051591784e-05, val loss: 0.3161468505859375\n",
      "Epoch 2426: train loss: 0.00012537244765553623, val loss: 0.32245001196861267\n",
      "Epoch 2427: train loss: 0.00022184559202287346, val loss: 0.3159816861152649\n",
      "Epoch 2428: train loss: 0.00037024501943960786, val loss: 0.32184910774230957\n",
      "Epoch 2429: train loss: 0.0005920124240219593, val loss: 0.31334492564201355\n",
      "Epoch 2430: train loss: 0.0009093394037336111, val loss: 0.3230021893978119\n",
      "Epoch 2431: train loss: 0.0013241996057331562, val loss: 0.3024255037307739\n",
      "Epoch 2432: train loss: 0.0017344086663797498, val loss: 0.3259357810020447\n",
      "Epoch 2433: train loss: 0.0019784029573202133, val loss: 0.2903997004032135\n",
      "Epoch 2434: train loss: 0.0017177470726892352, val loss: 0.320841521024704\n",
      "Epoch 2435: train loss: 0.001176095800474286, val loss: 0.2777566611766815\n",
      "Epoch 2436: train loss: 0.0006627945695072412, val loss: 0.3158348500728607\n",
      "Epoch 2437: train loss: 0.0003748074814211577, val loss: 0.2922573983669281\n",
      "Epoch 2438: train loss: 0.0002887299342546612, val loss: 0.28554829955101013\n",
      "Epoch 2439: train loss: 0.0004429914988577366, val loss: 0.31210970878601074\n",
      "Epoch 2440: train loss: 0.0005614283145405352, val loss: 0.27574262022972107\n",
      "Epoch 2441: train loss: 0.0003482713073026389, val loss: 0.28945818543434143\n",
      "Epoch 2442: train loss: 8.862302638590336e-05, val loss: 0.30180737376213074\n",
      "Epoch 2443: train loss: 0.0002397816424490884, val loss: 0.2710920572280884\n",
      "Epoch 2444: train loss: 0.0003544188803061843, val loss: 0.2900853753089905\n",
      "Epoch 2445: train loss: 0.00011390787403797731, val loss: 0.29100319743156433\n",
      "Epoch 2446: train loss: 8.116825483739376e-05, val loss: 0.2712866961956024\n",
      "Epoch 2447: train loss: 0.00022037079907022417, val loss: 0.288495272397995\n",
      "Epoch 2448: train loss: 0.0001317633577855304, val loss: 0.28525134921073914\n",
      "Epoch 2449: train loss: 6.451766967074946e-05, val loss: 0.27216482162475586\n",
      "Epoch 2450: train loss: 0.00011053214257117361, val loss: 0.2840815484523773\n",
      "Epoch 2451: train loss: 9.950216190190986e-05, val loss: 0.2808733880519867\n",
      "Epoch 2452: train loss: 8.380479266634211e-05, val loss: 0.2735016644001007\n",
      "Epoch 2453: train loss: 5.2833904192084447e-05, val loss: 0.28014516830444336\n",
      "Epoch 2454: train loss: 5.6461856729583815e-05, val loss: 0.2775643765926361\n",
      "Epoch 2455: train loss: 8.435951167484745e-05, val loss: 0.2733595073223114\n",
      "Epoch 2456: train loss: 3.317449227324687e-05, val loss: 0.2765289843082428\n",
      "Epoch 2457: train loss: 3.311805267003365e-05, val loss: 0.27446967363357544\n",
      "Epoch 2458: train loss: 6.525597564177588e-05, val loss: 0.273089736700058\n",
      "Epoch 2459: train loss: 2.362126724619884e-05, val loss: 0.2742997109889984\n",
      "Epoch 2460: train loss: 2.3217633497552015e-05, val loss: 0.27206483483314514\n",
      "Epoch 2461: train loss: 4.678456753026694e-05, val loss: 0.27220964431762695\n",
      "Epoch 2462: train loss: 1.789532689144835e-05, val loss: 0.27332448959350586\n",
      "Epoch 2463: train loss: 1.7526110241306014e-05, val loss: 0.2714161276817322\n",
      "Epoch 2464: train loss: 3.320920586702414e-05, val loss: 0.27141985297203064\n",
      "Epoch 2465: train loss: 1.3623008271679282e-05, val loss: 0.2718730866909027\n",
      "Epoch 2466: train loss: 1.253775099030463e-05, val loss: 0.2705560326576233\n",
      "Epoch 2467: train loss: 2.3972846975084394e-05, val loss: 0.2708718180656433\n",
      "Epoch 2468: train loss: 1.1664027624647133e-05, val loss: 0.2711174488067627\n",
      "Epoch 2469: train loss: 8.080334737314843e-06, val loss: 0.2702237069606781\n",
      "Epoch 2470: train loss: 1.6326617696904577e-05, val loss: 0.2703046500682831\n",
      "Epoch 2471: train loss: 1.1333318070683163e-05, val loss: 0.2703554630279541\n",
      "Epoch 2472: train loss: 4.7385096877405886e-06, val loss: 0.27024081349372864\n",
      "Epoch 2473: train loss: 1.0926348295470234e-05, val loss: 0.2699865996837616\n",
      "Epoch 2474: train loss: 9.730317287903745e-06, val loss: 0.2697160840034485\n",
      "Epoch 2475: train loss: 3.19537207360554e-06, val loss: 0.2703627645969391\n",
      "Epoch 2476: train loss: 7.724904207861982e-06, val loss: 0.2697688043117523\n",
      "Epoch 2477: train loss: 7.490717507607769e-06, val loss: 0.26924043893814087\n",
      "Epoch 2478: train loss: 2.3121128833736293e-06, val loss: 0.2706209123134613\n",
      "Epoch 2479: train loss: 5.461445198307047e-06, val loss: 0.26953551173210144\n",
      "Epoch 2480: train loss: 5.859484645043267e-06, val loss: 0.2686777114868164\n",
      "Epoch 2481: train loss: 1.9619021713879192e-06, val loss: 0.27081403136253357\n",
      "Epoch 2482: train loss: 3.5157181628164835e-06, val loss: 0.2695655822753906\n",
      "Epoch 2483: train loss: 4.228860689181602e-06, val loss: 0.2686765491962433\n",
      "Epoch 2484: train loss: 2.1435876078612637e-06, val loss: 0.271013081073761\n",
      "Epoch 2485: train loss: 2.4023083824431524e-06, val loss: 0.2694079577922821\n",
      "Epoch 2486: train loss: 2.697363697734545e-06, val loss: 0.2688223421573639\n",
      "Epoch 2487: train loss: 1.8795418554873322e-06, val loss: 0.27117714285850525\n",
      "Epoch 2488: train loss: 1.8927216842712369e-06, val loss: 0.26937535405158997\n",
      "Epoch 2489: train loss: 1.9068759229412535e-06, val loss: 0.26939573884010315\n",
      "Epoch 2490: train loss: 1.4521599496220006e-06, val loss: 0.27136778831481934\n",
      "Epoch 2491: train loss: 1.427582560609153e-06, val loss: 0.2693955898284912\n",
      "Epoch 2492: train loss: 1.3718414493268938e-06, val loss: 0.27017247676849365\n",
      "Epoch 2493: train loss: 1.0988940175593598e-06, val loss: 0.27154967188835144\n",
      "Epoch 2494: train loss: 1.1106803867733106e-06, val loss: 0.26972559094429016\n",
      "Epoch 2495: train loss: 1.1558967116798158e-06, val loss: 0.2710592448711395\n",
      "Epoch 2496: train loss: 8.79332958447776e-07, val loss: 0.27139991521835327\n",
      "Epoch 2497: train loss: 5.887952170269273e-07, val loss: 0.2701244056224823\n",
      "Epoch 2498: train loss: 7.885652166805812e-07, val loss: 0.2718130350112915\n",
      "Epoch 2499: train loss: 9.755519840837223e-07, val loss: 0.27125537395477295\n",
      "Epoch 2500: train loss: 5.513770702236798e-07, val loss: 0.27083733677864075\n",
      "Epoch 2501: train loss: 3.41504261314185e-07, val loss: 0.2722507417201996\n",
      "Epoch 2502: train loss: 7.026348498584412e-07, val loss: 0.2712301015853882\n",
      "Epoch 2503: train loss: 7.180245233939786e-07, val loss: 0.2717844843864441\n",
      "Epoch 2504: train loss: 3.188862081060506e-07, val loss: 0.272339791059494\n",
      "Epoch 2505: train loss: 2.2532954346843326e-07, val loss: 0.2714923024177551\n",
      "Epoch 2506: train loss: 4.3689490780707274e-07, val loss: 0.27254173159599304\n",
      "Epoch 2507: train loss: 5.528440851776395e-07, val loss: 0.27232879400253296\n",
      "Epoch 2508: train loss: 4.758596503506851e-07, val loss: 0.2722301185131073\n",
      "Epoch 2509: train loss: 3.059753908019047e-07, val loss: 0.27307507395744324\n",
      "Epoch 2510: train loss: 1.7686335240796325e-07, val loss: 0.27245429158210754\n",
      "Epoch 2511: train loss: 2.0185528626370797e-07, val loss: 0.27302032709121704\n",
      "Epoch 2512: train loss: 3.5492283245730505e-07, val loss: 0.2732747197151184\n",
      "Epoch 2513: train loss: 5.121014510223176e-07, val loss: 0.2729245126247406\n",
      "Epoch 2514: train loss: 6.593454600078985e-07, val loss: 0.2737713158130646\n",
      "Epoch 2515: train loss: 7.357186859735521e-07, val loss: 0.2734043598175049\n",
      "Epoch 2516: train loss: 9.122051096710493e-07, val loss: 0.27368924021720886\n",
      "Epoch 2517: train loss: 1.2702167850875412e-06, val loss: 0.27408847212791443\n",
      "Epoch 2518: train loss: 2.048745727734058e-06, val loss: 0.27379509806632996\n",
      "Epoch 2519: train loss: 3.475261337371194e-06, val loss: 0.2744765877723694\n",
      "Epoch 2520: train loss: 6.029073119862005e-06, val loss: 0.27417829632759094\n",
      "Epoch 2521: train loss: 1.0132045645150356e-05, val loss: 0.27487191557884216\n",
      "Epoch 2522: train loss: 1.6689686162862927e-05, val loss: 0.27429598569869995\n",
      "Epoch 2523: train loss: 2.5005281713674776e-05, val loss: 0.27553626894950867\n",
      "Epoch 2524: train loss: 3.502667459542863e-05, val loss: 0.2740767300128937\n",
      "Epoch 2525: train loss: 4.611920303432271e-05, val loss: 0.2766159176826477\n",
      "Epoch 2526: train loss: 6.223193486221135e-05, val loss: 0.27505776286125183\n",
      "Epoch 2527: train loss: 8.38119667605497e-05, val loss: 0.27542850375175476\n",
      "Epoch 2528: train loss: 9.802122804103419e-05, val loss: 0.27699336409568787\n",
      "Epoch 2529: train loss: 9.230207797372714e-05, val loss: 0.27405238151550293\n",
      "Epoch 2530: train loss: 7.230493793031201e-05, val loss: 0.27743902802467346\n",
      "Epoch 2531: train loss: 4.59057955595199e-05, val loss: 0.2765001058578491\n",
      "Epoch 2532: train loss: 2.132442568836268e-05, val loss: 0.2759110629558563\n",
      "Epoch 2533: train loss: 4.905044079350773e-06, val loss: 0.2788679301738739\n",
      "Epoch 2534: train loss: 1.5007778983999742e-06, val loss: 0.2759701609611511\n",
      "Epoch 2535: train loss: 8.40793381939875e-06, val loss: 0.27802544832229614\n",
      "Epoch 2536: train loss: 1.8748765796772204e-05, val loss: 0.2788223326206207\n",
      "Epoch 2537: train loss: 2.829011827998329e-05, val loss: 0.276615709066391\n",
      "Epoch 2538: train loss: 3.2567786547588184e-05, val loss: 0.27921512722969055\n",
      "Epoch 2539: train loss: 2.92075037577888e-05, val loss: 0.27834707498550415\n",
      "Epoch 2540: train loss: 2.0574840164044872e-05, val loss: 0.27783775329589844\n",
      "Epoch 2541: train loss: 1.1450491911091376e-05, val loss: 0.279539555311203\n",
      "Epoch 2542: train loss: 4.29934971180046e-06, val loss: 0.2781413495540619\n",
      "Epoch 2543: train loss: 8.767121926211985e-07, val loss: 0.27897945046424866\n",
      "Epoch 2544: train loss: 1.3509496739061433e-06, val loss: 0.27929210662841797\n",
      "Epoch 2545: train loss: 4.434202764969086e-06, val loss: 0.2789863646030426\n",
      "Epoch 2546: train loss: 8.587898264522664e-06, val loss: 0.2797388732433319\n",
      "Epoch 2547: train loss: 1.2062179848726373e-05, val loss: 0.27944865822792053\n",
      "Epoch 2548: train loss: 1.3473927538143471e-05, val loss: 0.27984920144081116\n",
      "Epoch 2549: train loss: 1.2501847777457442e-05, val loss: 0.27986717224121094\n",
      "Epoch 2550: train loss: 1.028795577440178e-05, val loss: 0.2800970673561096\n",
      "Epoch 2551: train loss: 7.56930285206181e-06, val loss: 0.2805332839488983\n",
      "Epoch 2552: train loss: 4.8313172555936035e-06, val loss: 0.28018155694007874\n",
      "Epoch 2553: train loss: 2.369105459365528e-06, val loss: 0.2810649573802948\n",
      "Epoch 2554: train loss: 8.191177016669826e-07, val loss: 0.28050974011421204\n",
      "Epoch 2555: train loss: 2.3030577267491026e-07, val loss: 0.2811322808265686\n",
      "Epoch 2556: train loss: 2.8462650902838504e-07, val loss: 0.2814565598964691\n",
      "Epoch 2557: train loss: 7.982377496773552e-07, val loss: 0.2808728516101837\n",
      "Epoch 2558: train loss: 1.7197290844706004e-06, val loss: 0.282123327255249\n",
      "Epoch 2559: train loss: 2.9353702757362043e-06, val loss: 0.2812395989894867\n",
      "Epoch 2560: train loss: 4.212926796753891e-06, val loss: 0.2820069193840027\n",
      "Epoch 2561: train loss: 5.715186034649378e-06, val loss: 0.2821708619594574\n",
      "Epoch 2562: train loss: 7.4697363743325695e-06, val loss: 0.281760036945343\n",
      "Epoch 2563: train loss: 9.757233783602715e-06, val loss: 0.2826596200466156\n",
      "Epoch 2564: train loss: 1.2791221706720535e-05, val loss: 0.2822662889957428\n",
      "Epoch 2565: train loss: 1.732949931465555e-05, val loss: 0.28217563033103943\n",
      "Epoch 2566: train loss: 2.446622784191277e-05, val loss: 0.2835802733898163\n",
      "Epoch 2567: train loss: 3.6724468372995034e-05, val loss: 0.2808094620704651\n",
      "Epoch 2568: train loss: 5.728475298383273e-05, val loss: 0.2851673364639282\n",
      "Epoch 2569: train loss: 9.190744458464906e-05, val loss: 0.2781582772731781\n",
      "Epoch 2570: train loss: 0.00014519733667839319, val loss: 0.2871285378932953\n",
      "Epoch 2571: train loss: 0.00022396606800612062, val loss: 0.27395254373550415\n",
      "Epoch 2572: train loss: 0.00033736697514541447, val loss: 0.28906846046447754\n",
      "Epoch 2573: train loss: 0.0004961656522937119, val loss: 0.27050623297691345\n",
      "Epoch 2574: train loss: 0.0006944512715563178, val loss: 0.287624329328537\n",
      "Epoch 2575: train loss: 0.0009021753212437034, val loss: 0.27188751101493835\n",
      "Epoch 2576: train loss: 0.001077141030691564, val loss: 0.2760486304759979\n",
      "Epoch 2577: train loss: 0.0009926469065248966, val loss: 0.27099889516830444\n",
      "Epoch 2578: train loss: 0.0005993387312628329, val loss: 0.26755163073539734\n",
      "Epoch 2579: train loss: 0.00016637546650599688, val loss: 0.2753928601741791\n",
      "Epoch 2580: train loss: 6.174758163979277e-05, val loss: 0.25430911779403687\n",
      "Epoch 2581: train loss: 0.00026415337924845517, val loss: 0.2744815945625305\n",
      "Epoch 2582: train loss: 0.0003448437200859189, val loss: 0.2612406313419342\n",
      "Epoch 2583: train loss: 0.0001637670793570578, val loss: 0.2614787518978119\n",
      "Epoch 2584: train loss: 6.413162191165611e-05, val loss: 0.26394444704055786\n",
      "Epoch 2585: train loss: 0.00015417371469084173, val loss: 0.25461944937705994\n",
      "Epoch 2586: train loss: 0.00015204327064566314, val loss: 0.25484099984169006\n",
      "Epoch 2587: train loss: 5.4230738896876574e-05, val loss: 0.2588416039943695\n",
      "Epoch 2588: train loss: 7.270806236192584e-05, val loss: 0.25162288546562195\n",
      "Epoch 2589: train loss: 0.00011060687393182889, val loss: 0.25037893652915955\n",
      "Epoch 2590: train loss: 5.89257906540297e-05, val loss: 0.25651153922080994\n",
      "Epoch 2591: train loss: 4.471171268960461e-05, val loss: 0.24465428292751312\n",
      "Epoch 2592: train loss: 7.500615174649283e-05, val loss: 0.2521999776363373\n",
      "Epoch 2593: train loss: 4.543044269667007e-05, val loss: 0.2511577308177948\n",
      "Epoch 2594: train loss: 2.1495070541277528e-05, val loss: 0.24371002614498138\n",
      "Epoch 2595: train loss: 5.0530161388451234e-05, val loss: 0.2520696222782135\n",
      "Epoch 2596: train loss: 4.025270027341321e-05, val loss: 0.24640364944934845\n",
      "Epoch 2597: train loss: 1.6802767277113162e-05, val loss: 0.24551860988140106\n",
      "Epoch 2598: train loss: 3.598925468395464e-05, val loss: 0.24945545196533203\n",
      "Epoch 2599: train loss: 3.2755095162428916e-05, val loss: 0.24544008076190948\n",
      "Epoch 2600: train loss: 9.727312317409087e-06, val loss: 0.24446316063404083\n",
      "Epoch 2601: train loss: 2.045204928435851e-05, val loss: 0.25038719177246094\n",
      "Epoch 2602: train loss: 2.6362369681010023e-05, val loss: 0.24402356147766113\n",
      "Epoch 2603: train loss: 8.443537808489054e-06, val loss: 0.24344873428344727\n",
      "Epoch 2604: train loss: 1.2393711585900746e-05, val loss: 0.25130632519721985\n",
      "Epoch 2605: train loss: 2.260277688037604e-05, val loss: 0.24241267144680023\n",
      "Epoch 2606: train loss: 1.0096717232954688e-05, val loss: 0.24650660157203674\n",
      "Epoch 2607: train loss: 5.884845904802205e-06, val loss: 0.24910227954387665\n",
      "Epoch 2608: train loss: 1.4834441572020296e-05, val loss: 0.24308185279369354\n",
      "Epoch 2609: train loss: 9.726213647809345e-06, val loss: 0.24785521626472473\n",
      "Epoch 2610: train loss: 3.083810952375643e-06, val loss: 0.2484951764345169\n",
      "Epoch 2611: train loss: 8.21634239400737e-06, val loss: 0.24370542168617249\n",
      "Epoch 2612: train loss: 8.732888090889901e-06, val loss: 0.24851588904857635\n",
      "Epoch 2613: train loss: 3.996434770670021e-06, val loss: 0.24845090508460999\n",
      "Epoch 2614: train loss: 5.378610694606323e-06, val loss: 0.24473078548908234\n",
      "Epoch 2615: train loss: 7.523955446231412e-06, val loss: 0.25004246830940247\n",
      "Epoch 2616: train loss: 4.760021965921624e-06, val loss: 0.24615421891212463\n",
      "Epoch 2617: train loss: 3.866905899485573e-06, val loss: 0.24839451909065247\n",
      "Epoch 2618: train loss: 6.168830168462591e-06, val loss: 0.248650461435318\n",
      "Epoch 2619: train loss: 5.577830506808823e-06, val loss: 0.24793016910552979\n",
      "Epoch 2620: train loss: 4.1373677959199995e-06, val loss: 0.24813203513622284\n",
      "Epoch 2621: train loss: 5.917286671319744e-06, val loss: 0.2505122125148773\n",
      "Epoch 2622: train loss: 7.69147663959302e-06, val loss: 0.24644863605499268\n",
      "Epoch 2623: train loss: 8.655890269437805e-06, val loss: 0.25224509835243225\n",
      "Epoch 2624: train loss: 1.3184928320697509e-05, val loss: 0.24685971438884735\n",
      "Epoch 2625: train loss: 2.1777304937131703e-05, val loss: 0.2525639235973358\n",
      "Epoch 2626: train loss: 3.56060809281189e-05, val loss: 0.24648304283618927\n",
      "Epoch 2627: train loss: 6.117513839853927e-05, val loss: 0.2562134265899658\n",
      "Epoch 2628: train loss: 0.0001020193230942823, val loss: 0.24137723445892334\n",
      "Epoch 2629: train loss: 0.00015622655337210745, val loss: 0.2626760005950928\n",
      "Epoch 2630: train loss: 0.00022691251069772989, val loss: 0.23579740524291992\n",
      "Epoch 2631: train loss: 0.00031049104291014373, val loss: 0.26741790771484375\n",
      "Epoch 2632: train loss: 0.0004089298308826983, val loss: 0.2321406900882721\n",
      "Epoch 2633: train loss: 0.0005008305888622999, val loss: 0.27049049735069275\n",
      "Epoch 2634: train loss: 0.0005918701644986868, val loss: 0.22825105488300323\n",
      "Epoch 2635: train loss: 0.0006355362711474299, val loss: 0.2695857584476471\n",
      "Epoch 2636: train loss: 0.000628396519459784, val loss: 0.2258284091949463\n",
      "Epoch 2637: train loss: 0.0005302887875586748, val loss: 0.2600460648536682\n",
      "Epoch 2638: train loss: 0.0003704869595821947, val loss: 0.22966282069683075\n",
      "Epoch 2639: train loss: 0.00018265201651956886, val loss: 0.24538536369800568\n",
      "Epoch 2640: train loss: 4.313177851145156e-05, val loss: 0.23828153312206268\n",
      "Epoch 2641: train loss: 1.4944102986191865e-06, val loss: 0.2316921204328537\n",
      "Epoch 2642: train loss: 5.2470295486273244e-05, val loss: 0.24561229348182678\n",
      "Epoch 2643: train loss: 0.00013384362682700157, val loss: 0.22480428218841553\n",
      "Epoch 2644: train loss: 0.0001695485261734575, val loss: 0.24477258324623108\n",
      "Epoch 2645: train loss: 0.00013237621169537306, val loss: 0.2273620367050171\n",
      "Epoch 2646: train loss: 5.6513643357902765e-05, val loss: 0.2369808703660965\n",
      "Epoch 2647: train loss: 5.903759301872924e-06, val loss: 0.23460793495178223\n",
      "Epoch 2648: train loss: 1.2449278983694967e-05, val loss: 0.2280796319246292\n",
      "Epoch 2649: train loss: 5.295160735840909e-05, val loss: 0.24050556123256683\n",
      "Epoch 2650: train loss: 7.882546196924523e-05, val loss: 0.22485437989234924\n",
      "Epoch 2651: train loss: 6.412391667254269e-05, val loss: 0.2380315065383911\n",
      "Epoch 2652: train loss: 2.7166322979610413e-05, val loss: 0.2295845001935959\n",
      "Epoch 2653: train loss: 3.6090541470912285e-06, val loss: 0.2308012992143631\n",
      "Epoch 2654: train loss: 1.008618619380286e-05, val loss: 0.23572857677936554\n",
      "Epoch 2655: train loss: 3.109377939836122e-05, val loss: 0.22632741928100586\n",
      "Epoch 2656: train loss: 4.0788218029774725e-05, val loss: 0.23761744797229767\n",
      "Epoch 2657: train loss: 3.0454999432549812e-05, val loss: 0.22709155082702637\n",
      "Epoch 2658: train loss: 1.2156479897384997e-05, val loss: 0.23406635224819183\n",
      "Epoch 2659: train loss: 3.321294570923783e-06, val loss: 0.23211093246936798\n",
      "Epoch 2660: train loss: 8.362665539607406e-06, val loss: 0.2291419506072998\n",
      "Epoch 2661: train loss: 1.7873389879241586e-05, val loss: 0.23623786866664886\n",
      "Epoch 2662: train loss: 2.1042656953795813e-05, val loss: 0.22738538682460785\n",
      "Epoch 2663: train loss: 1.5557547158095986e-05, val loss: 0.2359907180070877\n",
      "Epoch 2664: train loss: 7.824827662261669e-06, val loss: 0.22978539764881134\n",
      "Epoch 2665: train loss: 4.167801307630725e-06, val loss: 0.23306791484355927\n",
      "Epoch 2666: train loss: 5.816268185299123e-06, val loss: 0.23362715542316437\n",
      "Epoch 2667: train loss: 9.041576959134545e-06, val loss: 0.23035047948360443\n",
      "Epoch 2668: train loss: 1.7087217202060856e-05, val loss: 0.23529990017414093\n",
      "Epoch 2669: train loss: 3.5425280657364056e-05, val loss: 0.2291460484266281\n",
      "Epoch 2670: train loss: 6.59680154058151e-05, val loss: 0.23704767227172852\n",
      "Epoch 2671: train loss: 0.0001111230521928519, val loss: 0.22750873863697052\n",
      "Epoch 2672: train loss: 0.00014905301213730127, val loss: 0.23993010818958282\n",
      "Epoch 2673: train loss: 0.00016111755394376814, val loss: 0.22563229501247406\n",
      "Epoch 2674: train loss: 0.00013172192848287523, val loss: 0.23949003219604492\n",
      "Epoch 2675: train loss: 7.496860052924603e-05, val loss: 0.22899079322814941\n",
      "Epoch 2676: train loss: 2.68231251538964e-05, val loss: 0.23572707176208496\n",
      "Epoch 2677: train loss: 8.778934898145963e-06, val loss: 0.23240987956523895\n",
      "Epoch 2678: train loss: 2.018552549998276e-05, val loss: 0.2326204627752304\n",
      "Epoch 2679: train loss: 4.151219764025882e-05, val loss: 0.23672866821289062\n",
      "Epoch 2680: train loss: 5.398862776928581e-05, val loss: 0.22844234108924866\n",
      "Epoch 2681: train loss: 5.174939360585995e-05, val loss: 0.23994489014148712\n",
      "Epoch 2682: train loss: 4.079686914337799e-05, val loss: 0.2272048443555832\n",
      "Epoch 2683: train loss: 3.071871469728649e-05, val loss: 0.24023187160491943\n",
      "Epoch 2684: train loss: 2.5269322577514686e-05, val loss: 0.22927974164485931\n",
      "Epoch 2685: train loss: 2.412241701676976e-05, val loss: 0.2388303279876709\n",
      "Epoch 2686: train loss: 2.2243901184992865e-05, val loss: 0.23326964676380157\n",
      "Epoch 2687: train loss: 1.9014838471775874e-05, val loss: 0.2354942411184311\n",
      "Epoch 2688: train loss: 1.5063013961480465e-05, val loss: 0.2375515252351761\n",
      "Epoch 2689: train loss: 1.2335799510765355e-05, val loss: 0.2326381653547287\n",
      "Epoch 2690: train loss: 1.2578839232446626e-05, val loss: 0.24100008606910706\n",
      "Epoch 2691: train loss: 1.4812900190008804e-05, val loss: 0.2309155911207199\n",
      "Epoch 2692: train loss: 1.8190252376371063e-05, val loss: 0.24271252751350403\n",
      "Epoch 2693: train loss: 2.1308611394488253e-05, val loss: 0.23175306618213654\n",
      "Epoch 2694: train loss: 2.164415127481334e-05, val loss: 0.24301278591156006\n",
      "Epoch 2695: train loss: 1.8769655071082525e-05, val loss: 0.23314766585826874\n",
      "Epoch 2696: train loss: 1.359927591693122e-05, val loss: 0.24188537895679474\n",
      "Epoch 2697: train loss: 8.151488145813346e-06, val loss: 0.23635642230510712\n",
      "Epoch 2698: train loss: 4.0229115256806836e-06, val loss: 0.23926137387752533\n",
      "Epoch 2699: train loss: 2.5432448182982625e-06, val loss: 0.23913979530334473\n",
      "Epoch 2700: train loss: 3.280848432041239e-06, val loss: 0.23782360553741455\n",
      "Epoch 2701: train loss: 5.513138148671715e-06, val loss: 0.24155429005622864\n",
      "Epoch 2702: train loss: 8.074491233855952e-06, val loss: 0.23624110221862793\n",
      "Epoch 2703: train loss: 1.0440593541716225e-05, val loss: 0.2434348315000534\n",
      "Epoch 2704: train loss: 1.2434151358320378e-05, val loss: 0.23570704460144043\n",
      "Epoch 2705: train loss: 1.4693368029838894e-05, val loss: 0.24422860145568848\n",
      "Epoch 2706: train loss: 1.8199360056314617e-05, val loss: 0.23541061580181122\n",
      "Epoch 2707: train loss: 2.4522598323528655e-05, val loss: 0.2451867163181305\n",
      "Epoch 2708: train loss: 3.614158049458638e-05, val loss: 0.23441119492053986\n",
      "Epoch 2709: train loss: 5.733425132348202e-05, val loss: 0.24704723060131073\n",
      "Epoch 2710: train loss: 9.730401507113129e-05, val loss: 0.2321917861700058\n",
      "Epoch 2711: train loss: 0.0001667262549744919, val loss: 0.2497027963399887\n",
      "Epoch 2712: train loss: 0.00029309006640687585, val loss: 0.22801534831523895\n",
      "Epoch 2713: train loss: 0.000509388861246407, val loss: 0.2548162639141083\n",
      "Epoch 2714: train loss: 0.0008825837867334485, val loss: 0.22179067134857178\n",
      "Epoch 2715: train loss: 0.001352317864075303, val loss: 0.2565652132034302\n",
      "Epoch 2716: train loss: 0.0017416276969015598, val loss: 0.21393238008022308\n",
      "Epoch 2717: train loss: 0.0015170302940532565, val loss: 0.25125861167907715\n",
      "Epoch 2718: train loss: 0.0008264373755082488, val loss: 0.2078341543674469\n",
      "Epoch 2719: train loss: 0.00039277347968891263, val loss: 0.2376260757446289\n",
      "Epoch 2720: train loss: 0.0007009004475548863, val loss: 0.21108463406562805\n",
      "Epoch 2721: train loss: 0.0008526837918907404, val loss: 0.22127270698547363\n",
      "Epoch 2722: train loss: 0.00036926052416674793, val loss: 0.21291284263134003\n",
      "Epoch 2723: train loss: 0.0002178894355893135, val loss: 0.21029499173164368\n",
      "Epoch 2724: train loss: 0.00038088412838988006, val loss: 0.21299877762794495\n",
      "Epoch 2725: train loss: 0.00014039243978913873, val loss: 0.20201735198497772\n",
      "Epoch 2726: train loss: 0.00014170064241625369, val loss: 0.21042338013648987\n",
      "Epoch 2727: train loss: 0.00031961375498212874, val loss: 0.20203819870948792\n",
      "Epoch 2728: train loss: 0.00014911867037881166, val loss: 0.20042946934700012\n",
      "Epoch 2729: train loss: 0.00015751027967780828, val loss: 0.20162616670131683\n",
      "Epoch 2730: train loss: 0.00013381624012254179, val loss: 0.19714275002479553\n",
      "Epoch 2731: train loss: 5.281357152853161e-05, val loss: 0.1990983933210373\n",
      "Epoch 2732: train loss: 0.0001383660128340125, val loss: 0.1947159618139267\n",
      "Epoch 2733: train loss: 0.00010713574738474563, val loss: 0.19473890960216522\n",
      "Epoch 2734: train loss: 8.059288666117936e-05, val loss: 0.19614000618457794\n",
      "Epoch 2735: train loss: 7.157921209000051e-05, val loss: 0.19120262563228607\n",
      "Epoch 2736: train loss: 4.382817496662028e-05, val loss: 0.1917416900396347\n",
      "Epoch 2737: train loss: 5.6715263781370595e-05, val loss: 0.1927909404039383\n",
      "Epoch 2738: train loss: 7.284710591193289e-05, val loss: 0.18862420320510864\n",
      "Epoch 2739: train loss: 5.1592385716503486e-05, val loss: 0.19007663428783417\n",
      "Epoch 2740: train loss: 2.4166147341020405e-05, val loss: 0.1915467083454132\n",
      "Epoch 2741: train loss: 3.665926124085672e-05, val loss: 0.18588562309741974\n",
      "Epoch 2742: train loss: 3.603123695938848e-05, val loss: 0.18863077461719513\n",
      "Epoch 2743: train loss: 3.7916826840955764e-05, val loss: 0.1906955987215042\n",
      "Epoch 2744: train loss: 2.9407921829260886e-05, val loss: 0.18412569165229797\n",
      "Epoch 2745: train loss: 1.753963624651078e-05, val loss: 0.18747912347316742\n",
      "Epoch 2746: train loss: 2.045240944426041e-05, val loss: 0.18891184031963348\n",
      "Epoch 2747: train loss: 2.1637648387695663e-05, val loss: 0.18497471511363983\n",
      "Epoch 2748: train loss: 2.1290319637046196e-05, val loss: 0.18729102611541748\n",
      "Epoch 2749: train loss: 2.0310530089773238e-05, val loss: 0.18688362836837769\n",
      "Epoch 2750: train loss: 8.447371328657027e-06, val loss: 0.18659229576587677\n",
      "Epoch 2751: train loss: 1.1387029189791065e-05, val loss: 0.18669798970222473\n",
      "Epoch 2752: train loss: 1.4706214642501436e-05, val loss: 0.1850121021270752\n",
      "Epoch 2753: train loss: 1.141807842941489e-05, val loss: 0.1878611296415329\n",
      "Epoch 2754: train loss: 1.3941446923126932e-05, val loss: 0.18652288615703583\n",
      "Epoch 2755: train loss: 4.659433670894941e-06, val loss: 0.18519750237464905\n",
      "Epoch 2756: train loss: 5.245812644716352e-06, val loss: 0.18795494735240936\n",
      "Epoch 2757: train loss: 1.0658844985300675e-05, val loss: 0.18643920123577118\n",
      "Epoch 2758: train loss: 5.721782144973986e-06, val loss: 0.18594062328338623\n",
      "Epoch 2759: train loss: 8.179429642041214e-06, val loss: 0.1872844249010086\n",
      "Epoch 2760: train loss: 4.867615643888712e-06, val loss: 0.1870238035917282\n",
      "Epoch 2761: train loss: 2.7992980449198512e-06, val loss: 0.18650493025779724\n",
      "Epoch 2762: train loss: 5.029919066146249e-06, val loss: 0.18658077716827393\n",
      "Epoch 2763: train loss: 3.4248928386659827e-06, val loss: 0.18767009675502777\n",
      "Epoch 2764: train loss: 5.199913630349329e-06, val loss: 0.18693891167640686\n",
      "Epoch 2765: train loss: 4.496613655646797e-06, val loss: 0.18664215505123138\n",
      "Epoch 2766: train loss: 1.7729884120853967e-06, val loss: 0.18800599873065948\n",
      "Epoch 2767: train loss: 2.1725161332142306e-06, val loss: 0.18729637563228607\n",
      "Epoch 2768: train loss: 2.128176447513397e-06, val loss: 0.18674997985363007\n",
      "Epoch 2769: train loss: 2.2535330117534613e-06, val loss: 0.1881260722875595\n",
      "Epoch 2770: train loss: 3.098507022514241e-06, val loss: 0.18764916062355042\n",
      "Epoch 2771: train loss: 2.2712220015819184e-06, val loss: 0.18724146485328674\n",
      "Epoch 2772: train loss: 1.8089880313709727e-06, val loss: 0.18845511972904205\n",
      "Epoch 2773: train loss: 1.3157143712305697e-06, val loss: 0.1876385658979416\n",
      "Epoch 2774: train loss: 1.0850473017853801e-06, val loss: 0.18810400366783142\n",
      "Epoch 2775: train loss: 9.431195735487563e-07, val loss: 0.18823450803756714\n",
      "Epoch 2776: train loss: 9.175066679745214e-07, val loss: 0.1880670040845871\n",
      "Epoch 2777: train loss: 1.5481045920751058e-06, val loss: 0.18889419734477997\n",
      "Epoch 2778: train loss: 1.4298267387857777e-06, val loss: 0.18793891370296478\n",
      "Epoch 2779: train loss: 1.4719070122737321e-06, val loss: 0.18898075819015503\n",
      "Epoch 2780: train loss: 1.2822503094866988e-06, val loss: 0.1887647956609726\n",
      "Epoch 2781: train loss: 9.042010447046778e-07, val loss: 0.18871597945690155\n",
      "Epoch 2782: train loss: 9.672643273006543e-07, val loss: 0.18923801183700562\n",
      "Epoch 2783: train loss: 6.416670998987684e-07, val loss: 0.1890624612569809\n",
      "Epoch 2784: train loss: 4.2485618223508936e-07, val loss: 0.18919049203395844\n",
      "Epoch 2785: train loss: 4.6990771807031706e-07, val loss: 0.18954773247241974\n",
      "Epoch 2786: train loss: 3.0560701702597726e-07, val loss: 0.1894538402557373\n",
      "Epoch 2787: train loss: 2.048119114306246e-07, val loss: 0.18969325721263885\n",
      "Epoch 2788: train loss: 3.061869051634858e-07, val loss: 0.18983402848243713\n",
      "Epoch 2789: train loss: 2.418059352748969e-07, val loss: 0.18996469676494598\n",
      "Epoch 2790: train loss: 1.6654125545301213e-07, val loss: 0.18996642529964447\n",
      "Epoch 2791: train loss: 2.2260373100380093e-07, val loss: 0.1903383582830429\n",
      "Epoch 2792: train loss: 1.9126711947592412e-07, val loss: 0.19026724994182587\n",
      "Epoch 2793: train loss: 2.5310555429314263e-07, val loss: 0.1904633492231369\n",
      "Epoch 2794: train loss: 3.4973470519616967e-07, val loss: 0.1908336579799652\n",
      "Epoch 2795: train loss: 5.512637244464713e-07, val loss: 0.1904323548078537\n",
      "Epoch 2796: train loss: 1.2027952607240877e-06, val loss: 0.19135956466197968\n",
      "Epoch 2797: train loss: 2.787499624901102e-06, val loss: 0.1905478686094284\n",
      "Epoch 2798: train loss: 6.995456260483479e-06, val loss: 0.19185307621955872\n",
      "Epoch 2799: train loss: 1.7204669347847812e-05, val loss: 0.19045789539813995\n",
      "Epoch 2800: train loss: 3.4454868000466377e-05, val loss: 0.19282518327236176\n",
      "Epoch 2801: train loss: 5.763363515143283e-05, val loss: 0.1897287219762802\n",
      "Epoch 2802: train loss: 8.813440217636526e-05, val loss: 0.19343189895153046\n",
      "Epoch 2803: train loss: 0.00013023304927628487, val loss: 0.18915782868862152\n",
      "Epoch 2804: train loss: 0.00018374415230937302, val loss: 0.19397573173046112\n",
      "Epoch 2805: train loss: 0.000254693441092968, val loss: 0.18798093497753143\n",
      "Epoch 2806: train loss: 0.00034297301317565143, val loss: 0.19439348578453064\n",
      "Epoch 2807: train loss: 0.0004576625069603324, val loss: 0.18619370460510254\n",
      "Epoch 2808: train loss: 0.0005873903864994645, val loss: 0.19526521861553192\n",
      "Epoch 2809: train loss: 0.0007336647249758244, val loss: 0.1833006888628006\n",
      "Epoch 2810: train loss: 0.000848442199639976, val loss: 0.19544784724712372\n",
      "Epoch 2811: train loss: 0.0009123529889620841, val loss: 0.1795070469379425\n",
      "Epoch 2812: train loss: 0.0008434998453594744, val loss: 0.1943814605474472\n",
      "Epoch 2813: train loss: 0.0006484417826868594, val loss: 0.1764381378889084\n",
      "Epoch 2814: train loss: 0.0003597253526095301, val loss: 0.19135551154613495\n",
      "Epoch 2815: train loss: 0.00012192335998406634, val loss: 0.17559050023555756\n",
      "Epoch 2816: train loss: 5.311879067448899e-05, val loss: 0.1868649423122406\n",
      "Epoch 2817: train loss: 0.00014929547614883631, val loss: 0.17674432694911957\n",
      "Epoch 2818: train loss: 0.00026721120229922235, val loss: 0.18264202773571014\n",
      "Epoch 2819: train loss: 0.00025696796365082264, val loss: 0.17743919789791107\n",
      "Epoch 2820: train loss: 0.00012692082964349538, val loss: 0.17959994077682495\n",
      "Epoch 2821: train loss: 1.862562567112036e-05, val loss: 0.17656321823596954\n",
      "Epoch 2822: train loss: 3.458467108430341e-05, val loss: 0.17715983092784882\n",
      "Epoch 2823: train loss: 0.00011401019582990557, val loss: 0.17554815113544464\n",
      "Epoch 2824: train loss: 0.00012863508891314268, val loss: 0.1740388423204422\n",
      "Epoch 2825: train loss: 6.260840746108443e-05, val loss: 0.1756340116262436\n",
      "Epoch 2826: train loss: 1.1333250768075231e-05, val loss: 0.1709490269422531\n",
      "Epoch 2827: train loss: 3.578751056920737e-05, val loss: 0.17554424703121185\n",
      "Epoch 2828: train loss: 8.14357481431216e-05, val loss: 0.16890478134155273\n",
      "Epoch 2829: train loss: 7.384218042716384e-05, val loss: 0.17520873248577118\n",
      "Epoch 2830: train loss: 3.172132346662693e-05, val loss: 0.16817474365234375\n",
      "Epoch 2831: train loss: 2.1432273570098914e-05, val loss: 0.1738666594028473\n",
      "Epoch 2832: train loss: 5.626923302770592e-05, val loss: 0.16887255012989044\n",
      "Epoch 2833: train loss: 9.521270840195939e-05, val loss: 0.1731964349746704\n",
      "Epoch 2834: train loss: 0.00011778771295212209, val loss: 0.1677340716123581\n",
      "Epoch 2835: train loss: 0.0001424712681910023, val loss: 0.17526543140411377\n",
      "Epoch 2836: train loss: 0.0002324942615814507, val loss: 0.16850052773952484\n",
      "Epoch 2837: train loss: 0.00030586295179091394, val loss: 0.17560134828090668\n",
      "Epoch 2838: train loss: 0.0002836305648088455, val loss: 0.1692458540201187\n",
      "Epoch 2839: train loss: 0.00020507065346464515, val loss: 0.18027566373348236\n",
      "Epoch 2840: train loss: 0.0001222826394950971, val loss: 0.16621147096157074\n",
      "Epoch 2841: train loss: 9.346447041025385e-05, val loss: 0.18066082894802094\n",
      "Epoch 2842: train loss: 8.509397594025359e-05, val loss: 0.16960211098194122\n",
      "Epoch 2843: train loss: 7.286821346497163e-05, val loss: 0.17705132067203522\n",
      "Epoch 2844: train loss: 6.0313890571706e-05, val loss: 0.17270950973033905\n",
      "Epoch 2845: train loss: 5.9700974816223606e-05, val loss: 0.17467977106571198\n",
      "Epoch 2846: train loss: 7.148004078771919e-05, val loss: 0.1760794073343277\n",
      "Epoch 2847: train loss: 6.910604133736342e-05, val loss: 0.1717957705259323\n",
      "Epoch 2848: train loss: 4.44725519628264e-05, val loss: 0.17785950005054474\n",
      "Epoch 2849: train loss: 1.6361664165742695e-05, val loss: 0.17338019609451294\n",
      "Epoch 2850: train loss: 1.586332291481085e-05, val loss: 0.17489540576934814\n",
      "Epoch 2851: train loss: 3.5970711905974895e-05, val loss: 0.17535346746444702\n",
      "Epoch 2852: train loss: 4.5995442633284256e-05, val loss: 0.1748702973127365\n",
      "Epoch 2853: train loss: 3.395454041310586e-05, val loss: 0.17442761361598969\n",
      "Epoch 2854: train loss: 1.5466192053281702e-05, val loss: 0.1764739602804184\n",
      "Epoch 2855: train loss: 1.0412313713459298e-05, val loss: 0.17286106944084167\n",
      "Epoch 2856: train loss: 3.0489531127386726e-05, val loss: 0.17868401110172272\n",
      "Epoch 2857: train loss: 9.212652366841212e-05, val loss: 0.17334884405136108\n",
      "Epoch 2858: train loss: 0.00012709862494375557, val loss: 0.17409740388393402\n",
      "Epoch 2859: train loss: 7.542312232544646e-05, val loss: 0.17702026665210724\n",
      "Epoch 2860: train loss: 1.7324729924439453e-05, val loss: 0.17254053056240082\n",
      "Epoch 2861: train loss: 2.0897026843158528e-05, val loss: 0.17731092870235443\n",
      "Epoch 2862: train loss: 6.119009049143642e-05, val loss: 0.17225873470306396\n",
      "Epoch 2863: train loss: 7.229438051581383e-05, val loss: 0.17676128447055817\n",
      "Epoch 2864: train loss: 4.571653698803857e-05, val loss: 0.1729103922843933\n",
      "Epoch 2865: train loss: 2.3762926502968185e-05, val loss: 0.1767023354768753\n",
      "Epoch 2866: train loss: 3.1023984774947166e-05, val loss: 0.17200301587581635\n",
      "Epoch 2867: train loss: 4.903246008325368e-05, val loss: 0.1786166876554489\n",
      "Epoch 2868: train loss: 5.260623220237903e-05, val loss: 0.16966953873634338\n",
      "Epoch 2869: train loss: 4.551011443254538e-05, val loss: 0.18224979937076569\n",
      "Epoch 2870: train loss: 4.885780072072521e-05, val loss: 0.16715314984321594\n",
      "Epoch 2871: train loss: 7.178862870205194e-05, val loss: 0.18622586131095886\n",
      "Epoch 2872: train loss: 0.00010527329868637025, val loss: 0.16387009620666504\n",
      "Epoch 2873: train loss: 0.00014628397184424102, val loss: 0.19040028750896454\n",
      "Epoch 2874: train loss: 0.00019876389706041664, val loss: 0.1598244160413742\n",
      "Epoch 2875: train loss: 0.0002911062038037926, val loss: 0.1972728967666626\n",
      "Epoch 2876: train loss: 0.00044020969653502107, val loss: 0.15295293927192688\n",
      "Epoch 2877: train loss: 0.0006624548695981503, val loss: 0.20803387463092804\n",
      "Epoch 2878: train loss: 0.0009208897827193141, val loss: 0.1447669267654419\n",
      "Epoch 2879: train loss: 0.001179663697257638, val loss: 0.21497602760791779\n",
      "Epoch 2880: train loss: 0.0012461551232263446, val loss: 0.14201372861862183\n",
      "Epoch 2881: train loss: 0.0010081661166623235, val loss: 0.19122476875782013\n",
      "Epoch 2882: train loss: 0.0004829980607610196, val loss: 0.1652052402496338\n",
      "Epoch 2883: train loss: 0.00010514330642763525, val loss: 0.1521732658147812\n",
      "Epoch 2884: train loss: 0.00015283898392226547, val loss: 0.18017882108688354\n",
      "Epoch 2885: train loss: 0.000368005596101284, val loss: 0.14599749445915222\n",
      "Epoch 2886: train loss: 0.000344335800036788, val loss: 0.16344691812992096\n",
      "Epoch 2887: train loss: 0.00017150877101812512, val loss: 0.1622420847415924\n",
      "Epoch 2888: train loss: 0.00017590868810657412, val loss: 0.14709220826625824\n",
      "Epoch 2889: train loss: 0.0002649912203196436, val loss: 0.1643109768629074\n",
      "Epoch 2890: train loss: 0.00017320325423497707, val loss: 0.1546899527311325\n",
      "Epoch 2891: train loss: 5.375881664804183e-05, val loss: 0.14850711822509766\n",
      "Epoch 2892: train loss: 0.00011419544171076268, val loss: 0.16161435842514038\n",
      "Epoch 2893: train loss: 0.00016540421347599477, val loss: 0.14962263405323029\n",
      "Epoch 2894: train loss: 6.961196777410805e-05, val loss: 0.1487470418214798\n",
      "Epoch 2895: train loss: 2.498471803846769e-05, val loss: 0.1585521697998047\n",
      "Epoch 2896: train loss: 9.164781658910215e-05, val loss: 0.14797194302082062\n",
      "Epoch 2897: train loss: 9.853962546912953e-05, val loss: 0.15005242824554443\n",
      "Epoch 2898: train loss: 4.875336162513122e-05, val loss: 0.1553710550069809\n",
      "Epoch 2899: train loss: 4.511099905357696e-05, val loss: 0.14646832644939423\n",
      "Epoch 2900: train loss: 5.1488925237208605e-05, val loss: 0.15055599808692932\n",
      "Epoch 2901: train loss: 3.3221906051039696e-05, val loss: 0.15305200219154358\n",
      "Epoch 2902: train loss: 3.342367199365981e-05, val loss: 0.14558684825897217\n",
      "Epoch 2903: train loss: 4.0416201954940334e-05, val loss: 0.1512603908777237\n",
      "Epoch 2904: train loss: 2.794289139274042e-05, val loss: 0.15104138851165771\n",
      "Epoch 2905: train loss: 2.3201149815577082e-05, val loss: 0.14530479907989502\n",
      "Epoch 2906: train loss: 3.140097396681085e-05, val loss: 0.15189099311828613\n",
      "Epoch 2907: train loss: 2.7720036086975597e-05, val loss: 0.14942921698093414\n",
      "Epoch 2908: train loss: 1.572306609887164e-05, val loss: 0.14649216830730438\n",
      "Epoch 2909: train loss: 1.1807785085693467e-05, val loss: 0.15120114386081696\n",
      "Epoch 2910: train loss: 1.5245557733578607e-05, val loss: 0.14841340482234955\n",
      "Epoch 2911: train loss: 1.7903830666909926e-05, val loss: 0.147687628865242\n",
      "Epoch 2912: train loss: 1.5203661860141438e-05, val loss: 0.1505948305130005\n",
      "Epoch 2913: train loss: 9.863809282251168e-06, val loss: 0.1484392136335373\n",
      "Epoch 2914: train loss: 1.0899689186771866e-05, val loss: 0.14831098914146423\n",
      "Epoch 2915: train loss: 1.379489003738854e-05, val loss: 0.1505904644727707\n",
      "Epoch 2916: train loss: 9.380071787745692e-06, val loss: 0.14841656386852264\n",
      "Epoch 2917: train loss: 5.4525457926501986e-06, val loss: 0.1488088220357895\n",
      "Epoch 2918: train loss: 6.849239980510902e-06, val loss: 0.15065057575702667\n",
      "Epoch 2919: train loss: 6.267218395805685e-06, val loss: 0.14888954162597656\n",
      "Epoch 2920: train loss: 4.4141070247860625e-06, val loss: 0.14960582554340363\n",
      "Epoch 2921: train loss: 6.481765467469813e-06, val loss: 0.150483176112175\n",
      "Epoch 2922: train loss: 7.143268703657668e-06, val loss: 0.14902693033218384\n",
      "Epoch 2923: train loss: 3.609186705944012e-06, val loss: 0.15024910867214203\n",
      "Epoch 2924: train loss: 3.3253345463890582e-06, val loss: 0.15080682933330536\n",
      "Epoch 2925: train loss: 6.617547569476301e-06, val loss: 0.14925456047058105\n",
      "Epoch 2926: train loss: 5.76813272346044e-06, val loss: 0.15110789239406586\n",
      "Epoch 2927: train loss: 2.1955775082460605e-06, val loss: 0.15037448704242706\n",
      "Epoch 2928: train loss: 2.3977688670129282e-06, val loss: 0.14998005330562592\n",
      "Epoch 2929: train loss: 4.1880512071656995e-06, val loss: 0.15139220654964447\n",
      "Epoch 2930: train loss: 3.0337125735968584e-06, val loss: 0.15052619576454163\n",
      "Epoch 2931: train loss: 1.6000227560653002e-06, val loss: 0.1506180614233017\n",
      "Epoch 2932: train loss: 2.373106326558627e-06, val loss: 0.15168459713459015\n",
      "Epoch 2933: train loss: 2.69861789092829e-06, val loss: 0.15066055953502655\n",
      "Epoch 2934: train loss: 1.3234720199761796e-06, val loss: 0.15141987800598145\n",
      "Epoch 2935: train loss: 1.0172848305955995e-06, val loss: 0.15175139904022217\n",
      "Epoch 2936: train loss: 2.33553646467044e-06, val loss: 0.15130533277988434\n",
      "Epoch 2937: train loss: 2.7546782348508714e-06, val loss: 0.15178607404232025\n",
      "Epoch 2938: train loss: 1.9352703475306043e-06, val loss: 0.152126744389534\n",
      "Epoch 2939: train loss: 2.014118763327133e-06, val loss: 0.15158100426197052\n",
      "Epoch 2940: train loss: 3.7795830394316e-06, val loss: 0.15263298153877258\n",
      "Epoch 2941: train loss: 6.077446414565202e-06, val loss: 0.15200339257717133\n",
      "Epoch 2942: train loss: 9.104813216254115e-06, val loss: 0.15257035195827484\n",
      "Epoch 2943: train loss: 1.5356285075540654e-05, val loss: 0.15255038440227509\n",
      "Epoch 2944: train loss: 2.8066457161912695e-05, val loss: 0.15291474759578705\n",
      "Epoch 2945: train loss: 5.246321961749345e-05, val loss: 0.15226015448570251\n",
      "Epoch 2946: train loss: 0.00010035086597781628, val loss: 0.15415629744529724\n",
      "Epoch 2947: train loss: 0.0001980080414796248, val loss: 0.15133816003799438\n",
      "Epoch 2948: train loss: 0.0003979122848249972, val loss: 0.1558845490217209\n",
      "Epoch 2949: train loss: 0.000810382014606148, val loss: 0.15021690726280212\n",
      "Epoch 2950: train loss: 0.0016199109377339482, val loss: 0.15808962285518646\n",
      "Epoch 2951: train loss: 0.0031353337690234184, val loss: 0.14741896092891693\n",
      "Epoch 2952: train loss: 0.00542483851313591, val loss: 0.15900404751300812\n",
      "Epoch 2953: train loss: 0.007637367118149996, val loss: 0.140808567404747\n",
      "Epoch 2954: train loss: 0.006802862975746393, val loss: 0.15156550705432892\n",
      "Epoch 2955: train loss: 0.002242045709863305, val loss: 0.13883079588413239\n",
      "Epoch 2956: train loss: 0.0001664370793150738, val loss: 0.13548456132411957\n",
      "Epoch 2957: train loss: 0.0026984193827956915, val loss: 0.14473700523376465\n",
      "Epoch 2958: train loss: 0.0018664976814761758, val loss: 0.13170157372951508\n",
      "Epoch 2959: train loss: 0.00014496561198029667, val loss: 0.13123653829097748\n",
      "Epoch 2960: train loss: 0.001792675000615418, val loss: 0.1377721130847931\n",
      "Epoch 2961: train loss: 0.0006270408630371094, val loss: 0.12818467617034912\n",
      "Epoch 2962: train loss: 0.0005768212722614408, val loss: 0.13908927142620087\n",
      "Epoch 2963: train loss: 0.0010519860079512, val loss: 0.1480519026517868\n",
      "Epoch 2964: train loss: 0.00014916560030542314, val loss: 0.14144276082515717\n",
      "Epoch 2965: train loss: 0.0008803358068689704, val loss: 0.1416209638118744\n",
      "Epoch 2966: train loss: 0.00017643884348217398, val loss: 0.14858338236808777\n",
      "Epoch 2967: train loss: 0.0005592338857240975, val loss: 0.14558424055576324\n",
      "Epoch 2968: train loss: 0.0002745084057096392, val loss: 0.1414995938539505\n",
      "Epoch 2969: train loss: 0.0003161614004056901, val loss: 0.1451474279165268\n",
      "Epoch 2970: train loss: 0.00030557787977159023, val loss: 0.14730189740657806\n",
      "Epoch 2971: train loss: 0.0002025751891778782, val loss: 0.1455075442790985\n",
      "Epoch 2972: train loss: 0.00026645141770131886, val loss: 0.14390012621879578\n",
      "Epoch 2973: train loss: 0.00014183174062054604, val loss: 0.14449754357337952\n",
      "Epoch 2974: train loss: 0.00022382676252163947, val loss: 0.14710192382335663\n",
      "Epoch 2975: train loss: 0.00011043424456147477, val loss: 0.1465926468372345\n",
      "Epoch 2976: train loss: 0.00017579003178980201, val loss: 0.14278067648410797\n",
      "Epoch 2977: train loss: 9.462389425607398e-05, val loss: 0.14399369060993195\n",
      "Epoch 2978: train loss: 0.00013103846868034452, val loss: 0.1483643800020218\n",
      "Epoch 2979: train loss: 8.745664672460407e-05, val loss: 0.14489375054836273\n",
      "Epoch 2980: train loss: 9.339976531919092e-05, val loss: 0.14156420528888702\n",
      "Epoch 2981: train loss: 8.193439862225205e-05, val loss: 0.1459723711013794\n",
      "Epoch 2982: train loss: 6.498685979750007e-05, val loss: 0.14825618267059326\n",
      "Epoch 2983: train loss: 7.19161907909438e-05, val loss: 0.1427038013935089\n",
      "Epoch 2984: train loss: 5.0910293794004247e-05, val loss: 0.14187876880168915\n",
      "Epoch 2985: train loss: 5.6967270211316645e-05, val loss: 0.14689695835113525\n",
      "Epoch 2986: train loss: 4.242804061505012e-05, val loss: 0.14673957228660583\n",
      "Epoch 2987: train loss: 4.566732968669385e-05, val loss: 0.14281071722507477\n",
      "Epoch 2988: train loss: 3.328319871798158e-05, val loss: 0.14324428141117096\n",
      "Epoch 2989: train loss: 3.799873957177624e-05, val loss: 0.14572732150554657\n",
      "Epoch 2990: train loss: 2.6038906071335077e-05, val loss: 0.14548838138580322\n",
      "Epoch 2991: train loss: 3.196555553586222e-05, val loss: 0.14392390847206116\n",
      "Epoch 2992: train loss: 1.94740805454785e-05, val loss: 0.1437036246061325\n",
      "Epoch 2993: train loss: 2.7711597795132548e-05, val loss: 0.14496387541294098\n",
      "Epoch 2994: train loss: 1.3919500815973151e-05, val loss: 0.14565743505954742\n",
      "Epoch 2995: train loss: 2.4206621674238704e-05, val loss: 0.14398065209388733\n",
      "Epoch 2996: train loss: 1.0407230547571089e-05, val loss: 0.14276349544525146\n",
      "Epoch 2997: train loss: 1.9734839952434413e-05, val loss: 0.14467990398406982\n",
      "Epoch 2998: train loss: 8.72802411322482e-06, val loss: 0.14611421525478363\n",
      "Epoch 2999: train loss: 1.531909583718516e-05, val loss: 0.14412014186382294\n",
      "Epoch 3000: train loss: 8.066157533903606e-06, val loss: 0.14280612766742706\n",
      "Epoch 3001: train loss: 1.1453639672254212e-05, val loss: 0.1446642130613327\n",
      "Epoch 3002: train loss: 7.421954251185525e-06, val loss: 0.14587585628032684\n",
      "Epoch 3003: train loss: 8.647414688311983e-06, val loss: 0.14418816566467285\n",
      "Epoch 3004: train loss: 6.536883574881358e-06, val loss: 0.14292222261428833\n",
      "Epoch 3005: train loss: 6.745414339093259e-06, val loss: 0.14443397521972656\n",
      "Epoch 3006: train loss: 5.579483058681944e-06, val loss: 0.14558905363082886\n",
      "Epoch 3007: train loss: 5.261609203444095e-06, val loss: 0.1441427320241928\n",
      "Epoch 3008: train loss: 4.683046881837072e-06, val loss: 0.14321093261241913\n",
      "Epoch 3009: train loss: 4.0859872569853906e-06, val loss: 0.14464478194713593\n",
      "Epoch 3010: train loss: 4.063535925524775e-06, val loss: 0.14526225626468658\n",
      "Epoch 3011: train loss: 3.0819628591416404e-06, val loss: 0.14384068548679352\n",
      "Epoch 3012: train loss: 3.4871375191869447e-06, val loss: 0.1435607671737671\n",
      "Epoch 3013: train loss: 2.4323023808392463e-06, val loss: 0.14493165910243988\n",
      "Epoch 3014: train loss: 2.9059115149721038e-06, val loss: 0.14498722553253174\n",
      "Epoch 3015: train loss: 1.9508329387463164e-06, val loss: 0.1437731385231018\n",
      "Epoch 3016: train loss: 2.4184671474358765e-06, val loss: 0.14390838146209717\n",
      "Epoch 3017: train loss: 1.5755870208522538e-06, val loss: 0.14494597911834717\n",
      "Epoch 3018: train loss: 1.8583323253551498e-06, val loss: 0.14471332728862762\n",
      "Epoch 3019: train loss: 1.5022068282632972e-06, val loss: 0.143961563706398\n",
      "Epoch 3020: train loss: 1.2550975725389435e-06, val loss: 0.1443617194890976\n",
      "Epoch 3021: train loss: 1.4275332205215818e-06, val loss: 0.14497630298137665\n",
      "Epoch 3022: train loss: 9.257145165975089e-07, val loss: 0.144632950425148\n",
      "Epoch 3023: train loss: 1.2012757224511006e-06, val loss: 0.1442328542470932\n",
      "Epoch 3024: train loss: 7.867764679758693e-07, val loss: 0.14459393918514252\n",
      "Epoch 3025: train loss: 9.111083727475489e-07, val loss: 0.14501480758190155\n",
      "Epoch 3026: train loss: 7.572262461508217e-07, val loss: 0.14474204182624817\n",
      "Epoch 3027: train loss: 6.291332965702168e-07, val loss: 0.14440707862377167\n",
      "Epoch 3028: train loss: 6.843403070888598e-07, val loss: 0.14484965801239014\n",
      "Epoch 3029: train loss: 4.6794602326372114e-07, val loss: 0.1451888382434845\n",
      "Epoch 3030: train loss: 6.010952802171232e-07, val loss: 0.1446911096572876\n",
      "Epoch 3031: train loss: 3.328906359456596e-07, val loss: 0.14461718499660492\n",
      "Epoch 3032: train loss: 5.430662781691353e-07, val loss: 0.1451507806777954\n",
      "Epoch 3033: train loss: 2.390403039953526e-07, val loss: 0.14504161477088928\n",
      "Epoch 3034: train loss: 4.6943347342676134e-07, val loss: 0.14476655423641205\n",
      "Epoch 3035: train loss: 1.9471572443308105e-07, val loss: 0.1450367569923401\n",
      "Epoch 3036: train loss: 3.6143364923191257e-07, val loss: 0.1451660841703415\n",
      "Epoch 3037: train loss: 2.2331315108203853e-07, val loss: 0.14505742490291595\n",
      "Epoch 3038: train loss: 1.9611222512594395e-07, val loss: 0.1450464427471161\n",
      "Epoch 3039: train loss: 2.592263399492367e-07, val loss: 0.1451176255941391\n",
      "Epoch 3040: train loss: 1.1589208526174843e-07, val loss: 0.14523649215698242\n",
      "Epoch 3041: train loss: 2.2560651302683254e-07, val loss: 0.14520488679409027\n",
      "Epoch 3042: train loss: 1.0568670205657327e-07, val loss: 0.14514325559139252\n",
      "Epoch 3043: train loss: 1.673682561431633e-07, val loss: 0.14530065655708313\n",
      "Epoch 3044: train loss: 1.1562333668280189e-07, val loss: 0.14534926414489746\n",
      "Epoch 3045: train loss: 1.1444000591609438e-07, val loss: 0.14527130126953125\n",
      "Epoch 3046: train loss: 1.1571883362648805e-07, val loss: 0.145325168967247\n",
      "Epoch 3047: train loss: 7.44471151392645e-08, val loss: 0.14539925754070282\n",
      "Epoch 3048: train loss: 1.0645672432474385e-07, val loss: 0.14541314542293549\n",
      "Epoch 3049: train loss: 5.3959695378580363e-08, val loss: 0.1454048603773117\n",
      "Epoch 3050: train loss: 8.384613892076231e-08, val loss: 0.14544445276260376\n",
      "Epoch 3051: train loss: 6.204311375768157e-08, val loss: 0.145524263381958\n",
      "Epoch 3052: train loss: 4.9956270231632516e-08, val loss: 0.14551161229610443\n",
      "Epoch 3053: train loss: 6.735897528642454e-08, val loss: 0.1454961597919464\n",
      "Epoch 3054: train loss: 3.759415889703632e-08, val loss: 0.1455877274274826\n",
      "Epoch 3055: train loss: 5.29689110351228e-08, val loss: 0.14562064409255981\n",
      "Epoch 3056: train loss: 3.945066850974399e-08, val loss: 0.14559324085712433\n",
      "Epoch 3057: train loss: 3.447769358899677e-08, val loss: 0.14563898742198944\n",
      "Epoch 3058: train loss: 3.741858733974368e-08, val loss: 0.14569775760173798\n",
      "Epoch 3059: train loss: 2.2514234032655622e-08, val loss: 0.14570853114128113\n",
      "Epoch 3060: train loss: 3.2501901614523376e-08, val loss: 0.14569781720638275\n",
      "Epoch 3061: train loss: 2.3213951649836417e-08, val loss: 0.14576832950115204\n",
      "Epoch 3062: train loss: 2.3625187139941772e-08, val loss: 0.1458088755607605\n",
      "Epoch 3063: train loss: 2.682793720509835e-08, val loss: 0.1457950323820114\n",
      "Epoch 3064: train loss: 1.4680688664725494e-08, val loss: 0.14585919678211212\n",
      "Epoch 3065: train loss: 2.3673374371924183e-08, val loss: 0.14588384330272675\n",
      "Epoch 3066: train loss: 1.52214418847052e-08, val loss: 0.14593671262264252\n",
      "Epoch 3067: train loss: 1.3540972787495775e-08, val loss: 0.1459408849477768\n",
      "Epoch 3068: train loss: 1.9331714895542973e-08, val loss: 0.1460369974374771\n",
      "Epoch 3069: train loss: 1.0150155382859793e-08, val loss: 0.1461365520954132\n",
      "Epoch 3070: train loss: 1.8953747016325906e-08, val loss: 0.14649274945259094\n",
      "Epoch 3071: train loss: 2.162308554432002e-08, val loss: 0.14637911319732666\n",
      "Epoch 3072: train loss: 2.9220638353422146e-08, val loss: 0.1469690352678299\n",
      "Epoch 3073: train loss: 5.651698131714511e-08, val loss: 0.14661496877670288\n",
      "Epoch 3074: train loss: 9.315374427387724e-08, val loss: 0.14746688306331635\n",
      "Epoch 3075: train loss: 1.5421851173869072e-07, val loss: 0.14675071835517883\n",
      "Epoch 3076: train loss: 2.1589326593129954e-07, val loss: 0.1480179727077484\n",
      "Epoch 3077: train loss: 2.4435385626020434e-07, val loss: 0.14722342789173126\n",
      "Epoch 3078: train loss: 2.2671351018743735e-07, val loss: 0.14807943999767303\n",
      "Epoch 3079: train loss: 1.8885349106767535e-07, val loss: 0.1479518860578537\n",
      "Epoch 3080: train loss: 1.3310911128883163e-07, val loss: 0.14822836220264435\n",
      "Epoch 3081: train loss: 9.582698368149067e-08, val loss: 0.1483619511127472\n",
      "Epoch 3082: train loss: 7.858873374289033e-08, val loss: 0.14850151538848877\n",
      "Epoch 3083: train loss: 8.586673061472538e-08, val loss: 0.14879703521728516\n",
      "Epoch 3084: train loss: 8.590093614202488e-08, val loss: 0.14868111908435822\n",
      "Epoch 3085: train loss: 7.577146732273832e-08, val loss: 0.14913783967494965\n",
      "Epoch 3086: train loss: 6.445821298939336e-08, val loss: 0.14902116358280182\n",
      "Epoch 3087: train loss: 5.030353733559423e-08, val loss: 0.14928776025772095\n",
      "Epoch 3088: train loss: 3.320624486491397e-08, val loss: 0.14939706027507782\n",
      "Epoch 3089: train loss: 2.012048661015342e-08, val loss: 0.14955833554267883\n",
      "Epoch 3090: train loss: 1.4243017432136185e-08, val loss: 0.14974696934223175\n",
      "Epoch 3091: train loss: 2.0333496664193262e-08, val loss: 0.149959996342659\n",
      "Epoch 3092: train loss: 3.772339596253005e-08, val loss: 0.150083988904953\n",
      "Epoch 3093: train loss: 6.246286687883185e-08, val loss: 0.1503359079360962\n",
      "Epoch 3094: train loss: 1.0846082432180992e-07, val loss: 0.15040385723114014\n",
      "Epoch 3095: train loss: 1.781413629942108e-07, val loss: 0.15068171918392181\n",
      "Epoch 3096: train loss: 3.1261532740245457e-07, val loss: 0.15067553520202637\n",
      "Epoch 3097: train loss: 5.708769208467857e-07, val loss: 0.15111304819583893\n",
      "Epoch 3098: train loss: 1.1428829793658224e-06, val loss: 0.15074865520000458\n",
      "Epoch 3099: train loss: 2.399051709289779e-06, val loss: 0.15179704129695892\n",
      "Epoch 3100: train loss: 5.328376118995948e-06, val loss: 0.15041786432266235\n",
      "Epoch 3101: train loss: 1.2264754332136363e-05, val loss: 0.15308748185634613\n",
      "Epoch 3102: train loss: 2.9129030735930428e-05, val loss: 0.14932085573673248\n",
      "Epoch 3103: train loss: 6.851616490166634e-05, val loss: 0.15550445020198822\n",
      "Epoch 3104: train loss: 0.00015724485274404287, val loss: 0.14663128554821014\n",
      "Epoch 3105: train loss: 0.00031550199491903186, val loss: 0.1605219542980194\n",
      "Epoch 3106: train loss: 0.0005515587981790304, val loss: 0.1399325579404831\n",
      "Epoch 3107: train loss: 0.000882317079231143, val loss: 0.16537562012672424\n",
      "Epoch 3108: train loss: 0.001166371046565473, val loss: 0.13710258901119232\n",
      "Epoch 3109: train loss: 0.0008436340722255409, val loss: 0.16131660342216492\n",
      "Epoch 3110: train loss: 0.00030749934376217425, val loss: 0.1436700075864792\n",
      "Epoch 3111: train loss: 0.0002958618861157447, val loss: 0.1497841626405716\n",
      "Epoch 3112: train loss: 0.000351046648574993, val loss: 0.1535898745059967\n",
      "Epoch 3113: train loss: 0.00027776509523391724, val loss: 0.13750307261943817\n",
      "Epoch 3114: train loss: 0.0003599111514631659, val loss: 0.1559278964996338\n",
      "Epoch 3115: train loss: 0.0002586313639767468, val loss: 0.14496944844722748\n",
      "Epoch 3116: train loss: 7.699401612626389e-05, val loss: 0.1409669667482376\n",
      "Epoch 3117: train loss: 0.0002290888805873692, val loss: 0.15857215225696564\n",
      "Epoch 3118: train loss: 0.0002326633402844891, val loss: 0.1436297446489334\n",
      "Epoch 3119: train loss: 4.731371154775843e-05, val loss: 0.14275072515010834\n",
      "Epoch 3120: train loss: 0.0001262243458768353, val loss: 0.157705619931221\n",
      "Epoch 3121: train loss: 0.00016581486852373928, val loss: 0.1445913165807724\n",
      "Epoch 3122: train loss: 5.057408634456806e-05, val loss: 0.1443149298429489\n",
      "Epoch 3123: train loss: 7.66172815929167e-05, val loss: 0.15604166686534882\n",
      "Epoch 3124: train loss: 0.00010789709631353617, val loss: 0.1442808210849762\n",
      "Epoch 3125: train loss: 4.637613892555237e-05, val loss: 0.1444951742887497\n",
      "Epoch 3126: train loss: 6.060885061742738e-05, val loss: 0.1556195467710495\n",
      "Epoch 3127: train loss: 6.526411743834615e-05, val loss: 0.14582155644893646\n",
      "Epoch 3128: train loss: 3.581999408197589e-05, val loss: 0.14425750076770782\n",
      "Epoch 3129: train loss: 5.167331255506724e-05, val loss: 0.15377037227153778\n",
      "Epoch 3130: train loss: 4.024981171824038e-05, val loss: 0.14669132232666016\n",
      "Epoch 3131: train loss: 2.8435850254027173e-05, val loss: 0.14434616267681122\n",
      "Epoch 3132: train loss: 4.049141352879815e-05, val loss: 0.1515619307756424\n",
      "Epoch 3133: train loss: 2.4121003662003204e-05, val loss: 0.147042915225029\n",
      "Epoch 3134: train loss: 2.5936165911844e-05, val loss: 0.14494989812374115\n",
      "Epoch 3135: train loss: 2.5937401005649008e-05, val loss: 0.1495431512594223\n",
      "Epoch 3136: train loss: 1.684303060756065e-05, val loss: 0.1470712125301361\n",
      "Epoch 3137: train loss: 2.342714105907362e-05, val loss: 0.14571984112262726\n",
      "Epoch 3138: train loss: 1.5132490261748899e-05, val loss: 0.1477901190519333\n",
      "Epoch 3139: train loss: 1.2648836673179176e-05, val loss: 0.14638976752758026\n",
      "Epoch 3140: train loss: 1.9406192222959362e-05, val loss: 0.14631931483745575\n",
      "Epoch 3141: train loss: 9.717787179397419e-06, val loss: 0.14699937403202057\n",
      "Epoch 3142: train loss: 8.794794666755479e-06, val loss: 0.14614544808864594\n",
      "Epoch 3143: train loss: 1.5059469660627656e-05, val loss: 0.14657308161258698\n",
      "Epoch 3144: train loss: 7.306285169761395e-06, val loss: 0.14613191783428192\n",
      "Epoch 3145: train loss: 6.104970907472307e-06, val loss: 0.1455785483121872\n",
      "Epoch 3146: train loss: 1.0884279618039727e-05, val loss: 0.14646364748477936\n",
      "Epoch 3147: train loss: 5.703129772882676e-06, val loss: 0.14570550620555878\n",
      "Epoch 3148: train loss: 4.9112195483758114e-06, val loss: 0.14530479907989502\n",
      "Epoch 3149: train loss: 7.356082733167568e-06, val loss: 0.14612610638141632\n",
      "Epoch 3150: train loss: 4.134328719374025e-06, val loss: 0.14540672302246094\n",
      "Epoch 3151: train loss: 4.572185389406513e-06, val loss: 0.14528708159923553\n",
      "Epoch 3152: train loss: 4.668427664000774e-06, val loss: 0.14559465646743774\n",
      "Epoch 3153: train loss: 2.8225567803019658e-06, val loss: 0.14466127753257751\n",
      "Epoch 3154: train loss: 4.132226422370877e-06, val loss: 0.1451614946126938\n",
      "Epoch 3155: train loss: 3.1033705454319715e-06, val loss: 0.14561913907527924\n",
      "Epoch 3156: train loss: 1.90645346265228e-06, val loss: 0.14448103308677673\n",
      "Epoch 3157: train loss: 3.3222233923879685e-06, val loss: 0.14507564902305603\n",
      "Epoch 3158: train loss: 2.2513395379064605e-06, val loss: 0.14550630748271942\n",
      "Epoch 3159: train loss: 1.3700504268854274e-06, val loss: 0.14421667158603668\n",
      "Epoch 3160: train loss: 2.5729582375788596e-06, val loss: 0.14520302414894104\n",
      "Epoch 3161: train loss: 1.5601698351019877e-06, val loss: 0.14542245864868164\n",
      "Epoch 3162: train loss: 9.494787036601338e-07, val loss: 0.1441199630498886\n",
      "Epoch 3163: train loss: 2.0640704860852566e-06, val loss: 0.14528034627437592\n",
      "Epoch 3164: train loss: 1.2894678320662933e-06, val loss: 0.14519309997558594\n",
      "Epoch 3165: train loss: 4.413242606915446e-07, val loss: 0.14415225386619568\n",
      "Epoch 3166: train loss: 1.4361966123033199e-06, val loss: 0.14526286721229553\n",
      "Epoch 3167: train loss: 1.3127836382409441e-06, val loss: 0.14483293890953064\n",
      "Epoch 3168: train loss: 3.694439101309399e-07, val loss: 0.14428582787513733\n",
      "Epoch 3169: train loss: 7.537338433394325e-07, val loss: 0.14518040418624878\n",
      "Epoch 3170: train loss: 9.69818074736395e-07, val loss: 0.1445557177066803\n",
      "Epoch 3171: train loss: 5.086482133265235e-07, val loss: 0.14437235891819\n",
      "Epoch 3172: train loss: 5.888812211196637e-07, val loss: 0.14489403367042542\n",
      "Epoch 3173: train loss: 5.849697686244326e-07, val loss: 0.1443716585636139\n",
      "Epoch 3174: train loss: 3.255396165968705e-07, val loss: 0.14439009130001068\n",
      "Epoch 3175: train loss: 5.087341037324222e-07, val loss: 0.14459574222564697\n",
      "Epoch 3176: train loss: 5.274605996419268e-07, val loss: 0.14424003660678864\n",
      "Epoch 3177: train loss: 2.0738077921578224e-07, val loss: 0.14432018995285034\n",
      "Epoch 3178: train loss: 3.0503764492095797e-07, val loss: 0.14437662065029144\n",
      "Epoch 3179: train loss: 4.5764852529828204e-07, val loss: 0.1442289650440216\n",
      "Epoch 3180: train loss: 1.9413363361309166e-07, val loss: 0.14427857100963593\n",
      "Epoch 3181: train loss: 1.2688474271271843e-07, val loss: 0.14420883357524872\n",
      "Epoch 3182: train loss: 3.5071499837613374e-07, val loss: 0.14419226348400116\n",
      "Epoch 3183: train loss: 2.777950101062743e-07, val loss: 0.1441606730222702\n",
      "Epoch 3184: train loss: 6.413834796603624e-08, val loss: 0.14414729177951813\n",
      "Epoch 3185: train loss: 1.4114972657353064e-07, val loss: 0.14421860873699188\n",
      "Epoch 3186: train loss: 2.4321161617990583e-07, val loss: 0.14414072036743164\n",
      "Epoch 3187: train loss: 1.5109304740690277e-07, val loss: 0.14427991211414337\n",
      "Epoch 3188: train loss: 9.199062134257474e-08, val loss: 0.14421828091144562\n",
      "Epoch 3189: train loss: 1.1998112370292802e-07, val loss: 0.14431887865066528\n",
      "Epoch 3190: train loss: 1.056779481700687e-07, val loss: 0.1444665640592575\n",
      "Epoch 3191: train loss: 8.455720745814688e-08, val loss: 0.14441215991973877\n",
      "Epoch 3192: train loss: 1.207382069878804e-07, val loss: 0.14482830464839935\n",
      "Epoch 3193: train loss: 1.208485116421798e-07, val loss: 0.14476656913757324\n",
      "Epoch 3194: train loss: 4.926001295757487e-08, val loss: 0.14504051208496094\n",
      "Epoch 3195: train loss: 2.956777933604826e-08, val loss: 0.1453961580991745\n",
      "Epoch 3196: train loss: 8.32633659797466e-08, val loss: 0.14540180563926697\n",
      "Epoch 3197: train loss: 9.13258020318608e-08, val loss: 0.1457727998495102\n",
      "Epoch 3198: train loss: 3.901123335481316e-08, val loss: 0.1458469182252884\n",
      "Epoch 3199: train loss: 2.1601676891691568e-08, val loss: 0.146071195602417\n",
      "Epoch 3200: train loss: 5.272425696034588e-08, val loss: 0.14629267156124115\n",
      "Epoch 3201: train loss: 6.64169874653453e-08, val loss: 0.14643415808677673\n",
      "Epoch 3202: train loss: 5.0458748290793665e-08, val loss: 0.14667470753192902\n",
      "Epoch 3203: train loss: 4.0727172745391726e-08, val loss: 0.1468450129032135\n",
      "Epoch 3204: train loss: 4.5761968436863754e-08, val loss: 0.1470511257648468\n",
      "Epoch 3205: train loss: 4.529946195930279e-08, val loss: 0.14726398885250092\n",
      "Epoch 3206: train loss: 2.684127053953489e-08, val loss: 0.14741377532482147\n",
      "Epoch 3207: train loss: 2.0434695713333895e-08, val loss: 0.14770939946174622\n",
      "Epoch 3208: train loss: 3.82291744926988e-08, val loss: 0.14774967730045319\n",
      "Epoch 3209: train loss: 5.928958657364092e-08, val loss: 0.1481601744890213\n",
      "Epoch 3210: train loss: 8.123997474740463e-08, val loss: 0.14808636903762817\n",
      "Epoch 3211: train loss: 1.256921819958734e-07, val loss: 0.14862994849681854\n",
      "Epoch 3212: train loss: 2.509050034404936e-07, val loss: 0.14840605854988098\n",
      "Epoch 3213: train loss: 5.493233175002388e-07, val loss: 0.14914816617965698\n",
      "Epoch 3214: train loss: 1.2519191159299226e-06, val loss: 0.1485840380191803\n",
      "Epoch 3215: train loss: 2.8290075988479657e-06, val loss: 0.1499459445476532\n",
      "Epoch 3216: train loss: 6.400729489541845e-06, val loss: 0.14836333692073822\n",
      "Epoch 3217: train loss: 1.4472296243184246e-05, val loss: 0.15146763622760773\n",
      "Epoch 3218: train loss: 2.5141920559690334e-05, val loss: 0.14681437611579895\n",
      "Epoch 3219: train loss: 4.032590732094832e-05, val loss: 0.1538822501897812\n",
      "Epoch 3220: train loss: 7.832863775547594e-05, val loss: 0.1445249617099762\n",
      "Epoch 3221: train loss: 0.00015949210501275957, val loss: 0.15804091095924377\n",
      "Epoch 3222: train loss: 0.000305441819364205, val loss: 0.14006909728050232\n",
      "Epoch 3223: train loss: 0.0005498166428878903, val loss: 0.16617971658706665\n",
      "Epoch 3224: train loss: 0.000925748550798744, val loss: 0.1351790875196457\n",
      "Epoch 3225: train loss: 0.0014599247369915247, val loss: 0.17240796983242035\n",
      "Epoch 3226: train loss: 0.001952907769009471, val loss: 0.13387885689735413\n",
      "Epoch 3227: train loss: 0.0020221704617142677, val loss: 0.15968552231788635\n",
      "Epoch 3228: train loss: 0.0013276005629450083, val loss: 0.13992038369178772\n",
      "Epoch 3229: train loss: 0.00032572561758570373, val loss: 0.13637974858283997\n",
      "Epoch 3230: train loss: 0.00010318783461116254, val loss: 0.146444171667099\n",
      "Epoch 3231: train loss: 0.0005671442486345768, val loss: 0.13033632934093475\n",
      "Epoch 3232: train loss: 0.0006431574001908302, val loss: 0.13512296974658966\n",
      "Epoch 3233: train loss: 0.0002042569249169901, val loss: 0.1374661773443222\n",
      "Epoch 3234: train loss: 0.00014538608957082033, val loss: 0.1291705220937729\n",
      "Epoch 3235: train loss: 0.0003533442213665694, val loss: 0.1323985904455185\n",
      "Epoch 3236: train loss: 0.00023889090516604483, val loss: 0.13137191534042358\n",
      "Epoch 3237: train loss: 0.0001217929893755354, val loss: 0.1282695084810257\n",
      "Epoch 3238: train loss: 0.0001809154637157917, val loss: 0.13274988532066345\n",
      "Epoch 3239: train loss: 0.0001673647202551365, val loss: 0.13023002445697784\n",
      "Epoch 3240: train loss: 0.00011849329166579992, val loss: 0.12912103533744812\n",
      "Epoch 3241: train loss: 0.00011408475984353572, val loss: 0.13194526731967926\n",
      "Epoch 3242: train loss: 9.93418216239661e-05, val loss: 0.12884604930877686\n",
      "Epoch 3243: train loss: 0.0001094090475817211, val loss: 0.13029609620571136\n",
      "Epoch 3244: train loss: 7.66933080740273e-05, val loss: 0.13300083577632904\n",
      "Epoch 3245: train loss: 5.784956010757014e-05, val loss: 0.12930327653884888\n",
      "Epoch 3246: train loss: 8.908938616514206e-05, val loss: 0.13121241331100464\n",
      "Epoch 3247: train loss: 5.3247666073730215e-05, val loss: 0.13437369465827942\n",
      "Epoch 3248: train loss: 3.812223076238297e-05, val loss: 0.1296987682580948\n",
      "Epoch 3249: train loss: 6.605798989767209e-05, val loss: 0.12999732792377472\n",
      "Epoch 3250: train loss: 3.7069814425194636e-05, val loss: 0.13411925733089447\n",
      "Epoch 3251: train loss: 3.0249271731008776e-05, val loss: 0.13082711398601532\n",
      "Epoch 3252: train loss: 4.168478699284606e-05, val loss: 0.12975452840328217\n",
      "Epoch 3253: train loss: 3.167325849062763e-05, val loss: 0.13331742584705353\n",
      "Epoch 3254: train loss: 2.268419848405756e-05, val loss: 0.131434828042984\n",
      "Epoch 3255: train loss: 2.200453309342265e-05, val loss: 0.12984099984169006\n",
      "Epoch 3256: train loss: 2.9678563805646263e-05, val loss: 0.1328892707824707\n",
      "Epoch 3257: train loss: 1.357045130134793e-05, val loss: 0.13199564814567566\n",
      "Epoch 3258: train loss: 1.4281647963798605e-05, val loss: 0.12942788004875183\n",
      "Epoch 3259: train loss: 2.550241151766386e-05, val loss: 0.13172946870326996\n",
      "Epoch 3260: train loss: 6.677806595689617e-06, val loss: 0.13235916197299957\n",
      "Epoch 3261: train loss: 1.1704282769642305e-05, val loss: 0.12984707951545715\n",
      "Epoch 3262: train loss: 1.8995915525010787e-05, val loss: 0.1309312880039215\n",
      "Epoch 3263: train loss: 3.7574170619336655e-06, val loss: 0.13210026919841766\n",
      "Epoch 3264: train loss: 1.0192558875132818e-05, val loss: 0.12990702688694\n",
      "Epoch 3265: train loss: 1.2477244126785081e-05, val loss: 0.13021132349967957\n",
      "Epoch 3266: train loss: 3.417137122596614e-06, val loss: 0.13178372383117676\n",
      "Epoch 3267: train loss: 8.368797352886759e-06, val loss: 0.13026626408100128\n",
      "Epoch 3268: train loss: 7.388650374196004e-06, val loss: 0.13014450669288635\n",
      "Epoch 3269: train loss: 3.823369752353756e-06, val loss: 0.13150177896022797\n",
      "Epoch 3270: train loss: 6.20729360889527e-06, val loss: 0.1304105967283249\n",
      "Epoch 3271: train loss: 4.458212060853839e-06, val loss: 0.13021719455718994\n",
      "Epoch 3272: train loss: 3.7731169868493453e-06, val loss: 0.13141010701656342\n",
      "Epoch 3273: train loss: 4.380082827992737e-06, val loss: 0.1307409107685089\n",
      "Epoch 3274: train loss: 2.996038347191643e-06, val loss: 0.1306804120540619\n",
      "Epoch 3275: train loss: 3.1156766908679856e-06, val loss: 0.13150480389595032\n",
      "Epoch 3276: train loss: 3.13923032990715e-06, val loss: 0.13103581964969635\n",
      "Epoch 3277: train loss: 2.27245141104504e-06, val loss: 0.13109427690505981\n",
      "Epoch 3278: train loss: 2.2448471099778544e-06, val loss: 0.13174860179424286\n",
      "Epoch 3279: train loss: 2.327716401850921e-06, val loss: 0.1311149299144745\n",
      "Epoch 3280: train loss: 1.7038674968716805e-06, val loss: 0.13129454851150513\n",
      "Epoch 3281: train loss: 1.6969761418295093e-06, val loss: 0.13179247081279755\n",
      "Epoch 3282: train loss: 1.6538384670639061e-06, val loss: 0.13124720752239227\n",
      "Epoch 3283: train loss: 1.3062366406302317e-06, val loss: 0.13158230483531952\n",
      "Epoch 3284: train loss: 1.309947492700303e-06, val loss: 0.13190726935863495\n",
      "Epoch 3285: train loss: 1.2164091458544135e-06, val loss: 0.1314873993396759\n",
      "Epoch 3286: train loss: 9.124133271143364e-07, val loss: 0.1319229006767273\n",
      "Epoch 3287: train loss: 9.928063491315697e-07, val loss: 0.13209186494350433\n",
      "Epoch 3288: train loss: 9.242658620678412e-07, val loss: 0.13175642490386963\n",
      "Epoch 3289: train loss: 7.199362812571053e-07, val loss: 0.13217686116695404\n",
      "Epoch 3290: train loss: 6.346800773826544e-07, val loss: 0.1322348266839981\n",
      "Epoch 3291: train loss: 6.965365173527971e-07, val loss: 0.13208790123462677\n",
      "Epoch 3292: train loss: 6.872560334159061e-07, val loss: 0.13252419233322144\n",
      "Epoch 3293: train loss: 4.1097163716585783e-07, val loss: 0.13242606818675995\n",
      "Epoch 3294: train loss: 3.6074720810574945e-07, val loss: 0.1323462575674057\n",
      "Epoch 3295: train loss: 6.560820224876807e-07, val loss: 0.13280485570430756\n",
      "Epoch 3296: train loss: 4.0841112536327273e-07, val loss: 0.13269810378551483\n",
      "Epoch 3297: train loss: 1.0314124665455893e-07, val loss: 0.13263951241970062\n",
      "Epoch 3298: train loss: 4.1570771713850263e-07, val loss: 0.13298261165618896\n",
      "Epoch 3299: train loss: 4.787466423294973e-07, val loss: 0.1329367607831955\n",
      "Epoch 3300: train loss: 1.2085072853551537e-07, val loss: 0.13303248584270477\n",
      "Epoch 3301: train loss: 1.6244594291947578e-07, val loss: 0.13324497640132904\n",
      "Epoch 3302: train loss: 3.352072610596224e-07, val loss: 0.13312171399593353\n",
      "Epoch 3303: train loss: 2.4000385678846214e-07, val loss: 0.1333256959915161\n",
      "Epoch 3304: train loss: 1.3557142608533468e-07, val loss: 0.1335173100233078\n",
      "Epoch 3305: train loss: 1.4404901094167144e-07, val loss: 0.13338710367679596\n",
      "Epoch 3306: train loss: 1.834386154087042e-07, val loss: 0.13359077274799347\n",
      "Epoch 3307: train loss: 2.0897040542422474e-07, val loss: 0.13374553620815277\n",
      "Epoch 3308: train loss: 1.5596765479131136e-07, val loss: 0.1337495595216751\n",
      "Epoch 3309: train loss: 1.4746873944204708e-07, val loss: 0.13391169905662537\n",
      "Epoch 3310: train loss: 2.74352061069294e-07, val loss: 0.1339356154203415\n",
      "Epoch 3311: train loss: 4.1971821929109865e-07, val loss: 0.13404472172260284\n",
      "Epoch 3312: train loss: 6.454185381699062e-07, val loss: 0.1342037171125412\n",
      "Epoch 3313: train loss: 1.246844817615056e-06, val loss: 0.13419784605503082\n",
      "Epoch 3314: train loss: 2.6288755634595873e-06, val loss: 0.13438419997692108\n",
      "Epoch 3315: train loss: 5.623749984806636e-06, val loss: 0.13441312313079834\n",
      "Epoch 3316: train loss: 1.252881429536501e-05, val loss: 0.1345619261264801\n",
      "Epoch 3317: train loss: 2.647216388140805e-05, val loss: 0.13490693271160126\n",
      "Epoch 3318: train loss: 6.9863358476141e-06, val loss: 0.13409648835659027\n",
      "Epoch 3319: train loss: 1.3588923138740938e-05, val loss: 0.13414119184017181\n",
      "Epoch 3320: train loss: 9.77488889475353e-06, val loss: 0.1360422968864441\n",
      "Epoch 3321: train loss: 1.0771739653137047e-05, val loss: 0.13548684120178223\n",
      "Epoch 3322: train loss: 6.051449872757075e-06, val loss: 0.13426373898983002\n",
      "Epoch 3323: train loss: 9.957500878954306e-06, val loss: 0.13511313498020172\n",
      "Epoch 3324: train loss: 7.543266747234156e-06, val loss: 0.13538281619548798\n",
      "Epoch 3325: train loss: 4.26607721237815e-06, val loss: 0.1357441246509552\n",
      "Epoch 3326: train loss: 7.618116796948016e-06, val loss: 0.13567131757736206\n",
      "Epoch 3327: train loss: 4.414847353473306e-06, val loss: 0.13496631383895874\n",
      "Epoch 3328: train loss: 4.507021913013887e-06, val loss: 0.1353882998228073\n",
      "Epoch 3329: train loss: 4.725529379356885e-06, val loss: 0.1361287534236908\n",
      "Epoch 3330: train loss: 4.334556706453441e-06, val loss: 0.1360984891653061\n",
      "Epoch 3331: train loss: 2.7252035579294898e-06, val loss: 0.1356634646654129\n",
      "Epoch 3332: train loss: 3.590170763345668e-06, val loss: 0.13597483932971954\n",
      "Epoch 3333: train loss: 3.0503010748361703e-06, val loss: 0.13578039407730103\n",
      "Epoch 3334: train loss: 2.214939058831078e-06, val loss: 0.13629738986492157\n",
      "Epoch 3335: train loss: 2.2689951038046274e-06, val loss: 0.13687746226787567\n",
      "Epoch 3336: train loss: 2.5671065486676525e-06, val loss: 0.13566219806671143\n",
      "Epoch 3337: train loss: 1.9942397102568066e-06, val loss: 0.13622038066387177\n",
      "Epoch 3338: train loss: 1.3789704098599032e-06, val loss: 0.13703185319900513\n",
      "Epoch 3339: train loss: 2.315155370524735e-06, val loss: 0.13677752017974854\n",
      "Epoch 3340: train loss: 9.86626218946185e-07, val loss: 0.13625751435756683\n",
      "Epoch 3341: train loss: 1.3085968930681702e-06, val loss: 0.1366834193468094\n",
      "Epoch 3342: train loss: 1.1181607533217175e-06, val loss: 0.1371472179889679\n",
      "Epoch 3343: train loss: 1.2244241816006252e-06, val loss: 0.1371723711490631\n",
      "Epoch 3344: train loss: 8.720120376892737e-07, val loss: 0.1370518058538437\n",
      "Epoch 3345: train loss: 8.894801908354566e-07, val loss: 0.13671818375587463\n",
      "Epoch 3346: train loss: 1.072552549885586e-06, val loss: 0.13770560920238495\n",
      "Epoch 3347: train loss: 7.805809332239733e-07, val loss: 0.13741251826286316\n",
      "Epoch 3348: train loss: 6.894522925904312e-07, val loss: 0.1374289095401764\n",
      "Epoch 3349: train loss: 6.514177925964759e-07, val loss: 0.13751737773418427\n",
      "Epoch 3350: train loss: 7.375716108981578e-07, val loss: 0.13779234886169434\n",
      "Epoch 3351: train loss: 5.021769879931526e-07, val loss: 0.13786372542381287\n",
      "Epoch 3352: train loss: 5.580950528383255e-07, val loss: 0.13809745013713837\n",
      "Epoch 3353: train loss: 6.951540854061022e-07, val loss: 0.1377543956041336\n",
      "Epoch 3354: train loss: 7.243822324198845e-07, val loss: 0.13848823308944702\n",
      "Epoch 3355: train loss: 7.320516601794225e-07, val loss: 0.1383223682641983\n",
      "Epoch 3356: train loss: 6.244187602533202e-07, val loss: 0.13861382007598877\n",
      "Epoch 3357: train loss: 9.957657312043011e-07, val loss: 0.13833539187908173\n",
      "Epoch 3358: train loss: 1.348575096926652e-06, val loss: 0.1392221450805664\n",
      "Epoch 3359: train loss: 2.030021960308659e-06, val loss: 0.13830052316188812\n",
      "Epoch 3360: train loss: 3.50586947206466e-06, val loss: 0.140009805560112\n",
      "Epoch 3361: train loss: 6.789658073103055e-06, val loss: 0.1378028243780136\n",
      "Epoch 3362: train loss: 1.3841591680829879e-05, val loss: 0.1415054351091385\n",
      "Epoch 3363: train loss: 2.9517414077417925e-05, val loss: 0.13658855855464935\n",
      "Epoch 3364: train loss: 6.479885632870719e-05, val loss: 0.1442323923110962\n",
      "Epoch 3365: train loss: 0.00014626131451223046, val loss: 0.1333618015050888\n",
      "Epoch 3366: train loss: 0.0003277416981291026, val loss: 0.15047983825206757\n",
      "Epoch 3367: train loss: 0.0006756617221981287, val loss: 0.12497835606336594\n",
      "Epoch 3368: train loss: 0.0013019389007240534, val loss: 0.16057035326957703\n",
      "Epoch 3369: train loss: 0.002265344141051173, val loss: 0.11427896469831467\n",
      "Epoch 3370: train loss: 0.003422522684559226, val loss: 0.1769765019416809\n",
      "Epoch 3371: train loss: 0.004634734243154526, val loss: 0.12008661031723022\n",
      "Epoch 3372: train loss: 0.004790918901562691, val loss: 0.14205186069011688\n",
      "Epoch 3373: train loss: 0.0037673884071409702, val loss: 0.15484242141246796\n",
      "Epoch 3374: train loss: 0.002058268990367651, val loss: 0.124872587621212\n",
      "Epoch 3375: train loss: 0.0009487422066740692, val loss: 0.14435508847236633\n",
      "Epoch 3376: train loss: 0.001182299805805087, val loss: 0.15071149170398712\n",
      "Epoch 3377: train loss: 0.0015474514802917838, val loss: 0.12682032585144043\n",
      "Epoch 3378: train loss: 0.0007433492573909461, val loss: 0.14387306571006775\n",
      "Epoch 3379: train loss: 0.0005433482583612204, val loss: 0.1612895280122757\n",
      "Epoch 3380: train loss: 0.0010094819590449333, val loss: 0.1451224535703659\n",
      "Epoch 3381: train loss: 0.0003550747351255268, val loss: 0.14966216683387756\n",
      "Epoch 3382: train loss: 0.0005137564148753881, val loss: 0.16707153618335724\n",
      "Epoch 3383: train loss: 0.0005676517030224204, val loss: 0.1601860225200653\n",
      "Epoch 3384: train loss: 0.00018453817756380886, val loss: 0.15370389819145203\n",
      "Epoch 3385: train loss: 0.0005100287380628288, val loss: 0.16465115547180176\n",
      "Epoch 3386: train loss: 0.00017829472199082375, val loss: 0.16805624961853027\n",
      "Epoch 3387: train loss: 0.0003003701276611537, val loss: 0.16026733815670013\n",
      "Epoch 3388: train loss: 0.00024805410066619515, val loss: 0.16053545475006104\n",
      "Epoch 3389: train loss: 0.0001400161418132484, val loss: 0.16788749396800995\n",
      "Epoch 3390: train loss: 0.00025681484839878976, val loss: 0.16858330368995667\n",
      "Epoch 3391: train loss: 8.281481132144108e-05, val loss: 0.1645338386297226\n",
      "Epoch 3392: train loss: 0.00020605632744263858, val loss: 0.1653544157743454\n",
      "Epoch 3393: train loss: 8.210921077989042e-05, val loss: 0.1686263382434845\n",
      "Epoch 3394: train loss: 0.00013784176553599536, val loss: 0.1678258776664734\n",
      "Epoch 3395: train loss: 9.414193482371047e-05, val loss: 0.16606758534908295\n",
      "Epoch 3396: train loss: 8.483161218464375e-05, val loss: 0.16761265695095062\n",
      "Epoch 3397: train loss: 9.604680235497653e-05, val loss: 0.1693142056465149\n",
      "Epoch 3398: train loss: 5.330021303961985e-05, val loss: 0.16838429868221283\n",
      "Epoch 3399: train loss: 8.539735426893458e-05, val loss: 0.1677580028772354\n",
      "Epoch 3400: train loss: 4.036771133542061e-05, val loss: 0.1691230833530426\n",
      "Epoch 3401: train loss: 6.75614646752365e-05, val loss: 0.16988901793956757\n",
      "Epoch 3402: train loss: 3.6775705666514114e-05, val loss: 0.1689596027135849\n",
      "Epoch 3403: train loss: 4.818691741093062e-05, val loss: 0.1685861200094223\n",
      "Epoch 3404: train loss: 3.65049418178387e-05, val loss: 0.16927829384803772\n",
      "Epoch 3405: train loss: 3.244083563913591e-05, val loss: 0.16975809633731842\n",
      "Epoch 3406: train loss: 3.537549855536781e-05, val loss: 0.1695501208305359\n",
      "Epoch 3407: train loss: 2.164524994441308e-05, val loss: 0.16961412131786346\n",
      "Epoch 3408: train loss: 3.169601041008718e-05, val loss: 0.1699599325656891\n",
      "Epoch 3409: train loss: 1.612864616618026e-05, val loss: 0.1700812578201294\n",
      "Epoch 3410: train loss: 2.5672585252323188e-05, val loss: 0.1698605865240097\n",
      "Epoch 3411: train loss: 1.4447286957874894e-05, val loss: 0.1699119657278061\n",
      "Epoch 3412: train loss: 1.854328002082184e-05, val loss: 0.17043066024780273\n",
      "Epoch 3413: train loss: 1.4394736354006454e-05, val loss: 0.17049039900302887\n",
      "Epoch 3414: train loss: 1.2223496923979837e-05, val loss: 0.1698448657989502\n",
      "Epoch 3415: train loss: 1.4374688362295274e-05, val loss: 0.16980448365211487\n",
      "Epoch 3416: train loss: 7.70647602621466e-06, val loss: 0.1706433743238449\n",
      "Epoch 3417: train loss: 1.3389589184953365e-05, val loss: 0.17078053951263428\n",
      "Epoch 3418: train loss: 5.177365437702974e-06, val loss: 0.17022545635700226\n",
      "Epoch 3419: train loss: 1.1409113540139515e-05, val loss: 0.170458123087883\n",
      "Epoch 3420: train loss: 4.307712515583262e-06, val loss: 0.17108391225337982\n",
      "Epoch 3421: train loss: 8.58273051562719e-06, val loss: 0.1707724928855896\n",
      "Epoch 3422: train loss: 4.61870649814955e-06, val loss: 0.17037197947502136\n",
      "Epoch 3423: train loss: 5.646392764901975e-06, val loss: 0.17094504833221436\n",
      "Epoch 3424: train loss: 5.163000878383173e-06, val loss: 0.1712835431098938\n",
      "Epoch 3425: train loss: 3.297404191471287e-06, val loss: 0.1706787794828415\n",
      "Epoch 3426: train loss: 5.254137704469031e-06, val loss: 0.17059162259101868\n",
      "Epoch 3427: train loss: 2.0448271698114695e-06, val loss: 0.17150118947029114\n",
      "Epoch 3428: train loss: 4.572318175632972e-06, val loss: 0.1717299073934555\n",
      "Epoch 3429: train loss: 1.6915289506869158e-06, val loss: 0.17098581790924072\n",
      "Epoch 3430: train loss: 3.4194913496321533e-06, val loss: 0.17092199623584747\n",
      "Epoch 3431: train loss: 1.8490643469704082e-06, val loss: 0.17165008187294006\n",
      "Epoch 3432: train loss: 2.2218355297809467e-06, val loss: 0.1716427057981491\n",
      "Epoch 3433: train loss: 2.0816758024011506e-06, val loss: 0.17111164331436157\n",
      "Epoch 3434: train loss: 1.3378385119722225e-06, val loss: 0.17144140601158142\n",
      "Epoch 3435: train loss: 2.027463324338896e-06, val loss: 0.1720459759235382\n",
      "Epoch 3436: train loss: 9.707737262942828e-07, val loss: 0.17172831296920776\n",
      "Epoch 3437: train loss: 1.6003302789613372e-06, val loss: 0.17140145599842072\n",
      "Epoch 3438: train loss: 9.715274700283771e-07, val loss: 0.17199082672595978\n",
      "Epoch 3439: train loss: 1.0387723250460112e-06, val loss: 0.1722962111234665\n",
      "Epoch 3440: train loss: 1.0360879514337284e-06, val loss: 0.17173762619495392\n",
      "Epoch 3441: train loss: 6.768054845451843e-07, val loss: 0.17168225347995758\n",
      "Epoch 3442: train loss: 9.28582949200063e-07, val loss: 0.1723213642835617\n",
      "Epoch 3443: train loss: 5.584393534263654e-07, val loss: 0.17235594987869263\n",
      "Epoch 3444: train loss: 6.999872539381613e-07, val loss: 0.1720190942287445\n",
      "Epoch 3445: train loss: 5.621186005555501e-07, val loss: 0.1723490208387375\n",
      "Epoch 3446: train loss: 4.6274590204120614e-07, val loss: 0.172714501619339\n",
      "Epoch 3447: train loss: 5.377800107453368e-07, val loss: 0.1724446415901184\n",
      "Epoch 3448: train loss: 3.3765294915610866e-07, val loss: 0.1724405735731125\n",
      "Epoch 3449: train loss: 4.274235436696472e-07, val loss: 0.17292353510856628\n",
      "Epoch 3450: train loss: 3.298163733234105e-07, val loss: 0.1729205697774887\n",
      "Epoch 3451: train loss: 2.726526702190313e-07, val loss: 0.17267782986164093\n",
      "Epoch 3452: train loss: 3.5364607242627244e-07, val loss: 0.17304781079292297\n",
      "Epoch 3453: train loss: 1.6990242102110642e-07, val loss: 0.1733580082654953\n",
      "Epoch 3454: train loss: 3.0257160688051954e-07, val loss: 0.1730811446905136\n",
      "Epoch 3455: train loss: 1.769940780604884e-07, val loss: 0.17311780154705048\n",
      "Epoch 3456: train loss: 1.7123311124578322e-07, val loss: 0.1735125333070755\n",
      "Epoch 3457: train loss: 2.281662290215536e-07, val loss: 0.17341139912605286\n",
      "Epoch 3458: train loss: 7.134274682130126e-08, val loss: 0.173273965716362\n",
      "Epoch 3459: train loss: 2.1702510366594652e-07, val loss: 0.1736433506011963\n",
      "Epoch 3460: train loss: 8.154186303954702e-08, val loss: 0.17372767627239227\n",
      "Epoch 3461: train loss: 1.1774567099109845e-07, val loss: 0.17349298298358917\n",
      "Epoch 3462: train loss: 1.460397243135958e-07, val loss: 0.17375673353672028\n",
      "Epoch 3463: train loss: 3.543793170024401e-08, val loss: 0.17402246594429016\n",
      "Epoch 3464: train loss: 1.4588654551062064e-07, val loss: 0.17379485070705414\n",
      "Epoch 3465: train loss: 4.3263778337632175e-08, val loss: 0.17386355996131897\n",
      "Epoch 3466: train loss: 7.234412180423533e-08, val loss: 0.1742039918899536\n",
      "Epoch 3467: train loss: 8.91451890083772e-08, val loss: 0.17411373555660248\n",
      "Epoch 3468: train loss: 2.103086238491869e-08, val loss: 0.17405857145786285\n",
      "Epoch 3469: train loss: 8.72071055368906e-08, val loss: 0.1743720918893814\n",
      "Epoch 3470: train loss: 3.1720816195957013e-08, val loss: 0.17442502081394196\n",
      "Epoch 3471: train loss: 4.484031990159565e-08, val loss: 0.17430543899536133\n",
      "Epoch 3472: train loss: 5.642582223686077e-08, val loss: 0.17450661957263947\n",
      "Epoch 3473: train loss: 1.5375063000533373e-08, val loss: 0.17467227578163147\n",
      "Epoch 3474: train loss: 5.075233033835502e-08, val loss: 0.1745753288269043\n",
      "Epoch 3475: train loss: 2.4926874786501685e-08, val loss: 0.17465700209140778\n",
      "Epoch 3476: train loss: 1.9092356140504307e-08, val loss: 0.17484861612319946\n",
      "Epoch 3477: train loss: 4.1001857908895545e-08, val loss: 0.17482905089855194\n",
      "Epoch 3478: train loss: 1.1678892519739748e-08, val loss: 0.1748385727405548\n",
      "Epoch 3479: train loss: 2.4093081307796638e-08, val loss: 0.17502537369728088\n",
      "Epoch 3480: train loss: 2.7036589855811144e-08, val loss: 0.17506055533885956\n",
      "Epoch 3481: train loss: 9.224943475771852e-09, val loss: 0.17502202093601227\n",
      "Epoch 3482: train loss: 2.2613646066815818e-08, val loss: 0.17519882321357727\n",
      "Epoch 3483: train loss: 1.4734291120532816e-08, val loss: 0.1752910614013672\n",
      "Epoch 3484: train loss: 9.55415391246106e-09, val loss: 0.17524971067905426\n",
      "Epoch 3485: train loss: 1.8471093099492464e-08, val loss: 0.17540720105171204\n",
      "Epoch 3486: train loss: 9.120620703129134e-09, val loss: 0.1755160540342331\n",
      "Epoch 3487: train loss: 9.952560553472267e-09, val loss: 0.17547626793384552\n",
      "Epoch 3488: train loss: 1.495189039246725e-08, val loss: 0.1755981296300888\n",
      "Epoch 3489: train loss: 7.60965601642738e-09, val loss: 0.17569772899150848\n",
      "Epoch 3490: train loss: 8.389985595158578e-09, val loss: 0.17569194734096527\n",
      "Epoch 3491: train loss: 1.0652826176738017e-08, val loss: 0.1757729947566986\n",
      "Epoch 3492: train loss: 6.11422734664302e-09, val loss: 0.17586401104927063\n",
      "Epoch 3493: train loss: 7.3408199519064965e-09, val loss: 0.17589901387691498\n",
      "Epoch 3494: train loss: 8.590697930799251e-09, val loss: 0.1759525090456009\n",
      "Epoch 3495: train loss: 5.387386980970632e-09, val loss: 0.176056906580925\n",
      "Epoch 3496: train loss: 6.128014984341235e-09, val loss: 0.1760925054550171\n",
      "Epoch 3497: train loss: 6.945825248294568e-09, val loss: 0.1761421412229538\n",
      "Epoch 3498: train loss: 4.992954050209164e-09, val loss: 0.1762436181306839\n",
      "Epoch 3499: train loss: 5.896399368765515e-09, val loss: 0.17627187073230743\n",
      "Epoch 3500: train loss: 6.52319531724288e-09, val loss: 0.1763538420200348\n",
      "Epoch 3501: train loss: 5.1191269001549244e-09, val loss: 0.17642129957675934\n",
      "Epoch 3502: train loss: 6.585707978956634e-09, val loss: 0.17650309205055237\n",
      "Epoch 3503: train loss: 7.446527394705527e-09, val loss: 0.17653708159923553\n",
      "Epoch 3504: train loss: 1.0612827949785242e-08, val loss: 0.1765916347503662\n",
      "Epoch 3505: train loss: 2.2231814611473055e-08, val loss: 0.17659436166286469\n",
      "Epoch 3506: train loss: 6.146498066073036e-08, val loss: 0.17666420340538025\n",
      "Epoch 3507: train loss: 1.4299878614565387e-07, val loss: 0.17661292850971222\n",
      "Epoch 3508: train loss: 3.086867650381464e-07, val loss: 0.1767662614583969\n",
      "Epoch 3509: train loss: 2.837316515069688e-07, val loss: 0.17676639556884766\n",
      "Epoch 3510: train loss: 5.229437860521102e-08, val loss: 0.1768520623445511\n",
      "Epoch 3511: train loss: 8.82851765027226e-08, val loss: 0.17688767611980438\n",
      "Epoch 3512: train loss: 1.3605385618120636e-07, val loss: 0.17689810693264008\n",
      "Epoch 3513: train loss: 7.08657950099223e-08, val loss: 0.1769883632659912\n",
      "Epoch 3514: train loss: 8.177721610991284e-08, val loss: 0.17696033418178558\n",
      "Epoch 3515: train loss: 7.848785088526711e-08, val loss: 0.17700856924057007\n",
      "Epoch 3516: train loss: 4.660378394305553e-08, val loss: 0.17704682052135468\n",
      "Epoch 3517: train loss: 6.898181226233646e-08, val loss: 0.17703752219676971\n",
      "Epoch 3518: train loss: 7.036266680415793e-08, val loss: 0.1771060973405838\n",
      "Epoch 3519: train loss: 2.127796783213398e-08, val loss: 0.17705006897449493\n",
      "Epoch 3520: train loss: 5.0697291698043045e-08, val loss: 0.17710720002651215\n",
      "Epoch 3521: train loss: 6.019492104769597e-08, val loss: 0.17715735733509064\n",
      "Epoch 3522: train loss: 1.6682641046372737e-08, val loss: 0.17707817256450653\n",
      "Epoch 3523: train loss: 3.7682532649796485e-08, val loss: 0.17719750106334686\n",
      "Epoch 3524: train loss: 4.089196536938289e-08, val loss: 0.1771688014268875\n",
      "Epoch 3525: train loss: 1.7770341642631138e-08, val loss: 0.17711395025253296\n",
      "Epoch 3526: train loss: 3.03795530953721e-08, val loss: 0.1772088259458542\n",
      "Epoch 3527: train loss: 2.576126689746161e-08, val loss: 0.1771741658449173\n",
      "Epoch 3528: train loss: 1.681745942505586e-08, val loss: 0.177163764834404\n",
      "Epoch 3529: train loss: 2.4800048237239025e-08, val loss: 0.17716430127620697\n",
      "Epoch 3530: train loss: 2.1184774823268526e-08, val loss: 0.17720696330070496\n",
      "Epoch 3531: train loss: 1.0988140175527406e-08, val loss: 0.17718803882598877\n",
      "Epoch 3532: train loss: 1.4339192055956573e-08, val loss: 0.1771547794342041\n",
      "Epoch 3533: train loss: 2.13389821368537e-08, val loss: 0.177239790558815\n",
      "Epoch 3534: train loss: 1.3976475976562597e-08, val loss: 0.1771734058856964\n",
      "Epoch 3535: train loss: 9.840221082413336e-09, val loss: 0.17718110978603363\n",
      "Epoch 3536: train loss: 1.2783779368419346e-08, val loss: 0.17720790207386017\n",
      "Epoch 3537: train loss: 8.332623480100665e-09, val loss: 0.1771562099456787\n",
      "Epoch 3538: train loss: 9.033254144696912e-09, val loss: 0.17716245353221893\n",
      "Epoch 3539: train loss: 1.4432865569347086e-08, val loss: 0.17715413868427277\n",
      "Epoch 3540: train loss: 9.850614546280667e-09, val loss: 0.17714624106884003\n",
      "Epoch 3541: train loss: 6.355892256948437e-09, val loss: 0.17710967361927032\n",
      "Epoch 3542: train loss: 8.764495795787752e-09, val loss: 0.17720095813274384\n",
      "Epoch 3543: train loss: 9.182632432214177e-09, val loss: 0.17722168564796448\n",
      "Epoch 3544: train loss: 8.056293410163562e-09, val loss: 0.17731605470180511\n",
      "Epoch 3545: train loss: 8.73005578938546e-09, val loss: 0.1773310899734497\n",
      "Epoch 3546: train loss: 9.34469479574318e-09, val loss: 0.17741337418556213\n",
      "Epoch 3547: train loss: 7.865543771856665e-09, val loss: 0.17738886177539825\n",
      "Epoch 3548: train loss: 7.981103777865428e-09, val loss: 0.17743432521820068\n",
      "Epoch 3549: train loss: 1.1559930790383532e-08, val loss: 0.17752687633037567\n",
      "Epoch 3550: train loss: 1.7950711139747e-08, val loss: 0.1777108758687973\n",
      "Epoch 3551: train loss: 2.7529075907750666e-08, val loss: 0.1776954084634781\n",
      "Epoch 3552: train loss: 5.459009599917408e-08, val loss: 0.17796604335308075\n",
      "Epoch 3553: train loss: 1.2198039200939093e-07, val loss: 0.17782805860042572\n",
      "Epoch 3554: train loss: 3.035499105408235e-07, val loss: 0.17828424274921417\n",
      "Epoch 3555: train loss: 8.089003813438467e-07, val loss: 0.17779287695884705\n",
      "Epoch 3556: train loss: 2.274875214425265e-06, val loss: 0.17882207036018372\n",
      "Epoch 3557: train loss: 6.2189519667299464e-06, val loss: 0.1771526336669922\n",
      "Epoch 3558: train loss: 1.5875368262641132e-05, val loss: 0.17979693412780762\n",
      "Epoch 3559: train loss: 3.5607674362836406e-05, val loss: 0.17642837762832642\n",
      "Epoch 3560: train loss: 6.997792661422864e-05, val loss: 0.18128125369548798\n",
      "Epoch 3561: train loss: 0.00012751865142490715, val loss: 0.17447207868099213\n",
      "Epoch 3562: train loss: 0.00021621720225084573, val loss: 0.18305401504039764\n",
      "Epoch 3563: train loss: 0.00035900226794183254, val loss: 0.1705148071050644\n",
      "Epoch 3564: train loss: 0.000564194459002465, val loss: 0.18588069081306458\n",
      "Epoch 3565: train loss: 0.0008301703492179513, val loss: 0.16367772221565247\n",
      "Epoch 3566: train loss: 0.0008786724647507071, val loss: 0.18421946465969086\n",
      "Epoch 3567: train loss: 0.0006124494248069823, val loss: 0.16616864502429962\n",
      "Epoch 3568: train loss: 0.0002694810973480344, val loss: 0.169300839304924\n",
      "Epoch 3569: train loss: 0.00019760358554776758, val loss: 0.1710772067308426\n",
      "Epoch 3570: train loss: 0.0003474882978480309, val loss: 0.16451287269592285\n",
      "Epoch 3571: train loss: 0.00037852846435271204, val loss: 0.16744843125343323\n",
      "Epoch 3572: train loss: 0.0002475871588103473, val loss: 0.16703346371650696\n",
      "Epoch 3573: train loss: 0.0001640733826206997, val loss: 0.16122882068157196\n",
      "Epoch 3574: train loss: 0.00018771977920550853, val loss: 0.16594581305980682\n",
      "Epoch 3575: train loss: 0.0001576333597768098, val loss: 0.1630917340517044\n",
      "Epoch 3576: train loss: 7.082291995175183e-05, val loss: 0.1622159630060196\n",
      "Epoch 3577: train loss: 8.811320003587753e-05, val loss: 0.16249381005764008\n",
      "Epoch 3578: train loss: 0.00013392350228969008, val loss: 0.16112402081489563\n",
      "Epoch 3579: train loss: 7.992063910933211e-05, val loss: 0.16242028772830963\n",
      "Epoch 3580: train loss: 6.216388283064589e-05, val loss: 0.15999563038349152\n",
      "Epoch 3581: train loss: 8.741674537304789e-05, val loss: 0.16114400327205658\n",
      "Epoch 3582: train loss: 6.04383931204211e-05, val loss: 0.16073325276374817\n",
      "Epoch 3583: train loss: 3.11328076350037e-05, val loss: 0.1596979796886444\n",
      "Epoch 3584: train loss: 4.625643487088382e-05, val loss: 0.16166797280311584\n",
      "Epoch 3585: train loss: 6.160651537356898e-05, val loss: 0.1599327176809311\n",
      "Epoch 3586: train loss: 3.5790530091617256e-05, val loss: 0.16010062396526337\n",
      "Epoch 3587: train loss: 2.5879071472445503e-05, val loss: 0.16035687923431396\n",
      "Epoch 3588: train loss: 3.669779471238144e-05, val loss: 0.15991303324699402\n",
      "Epoch 3589: train loss: 2.6045250706374645e-05, val loss: 0.16094273328781128\n",
      "Epoch 3590: train loss: 1.853126013884321e-05, val loss: 0.16007478535175323\n",
      "Epoch 3591: train loss: 2.7042204237659462e-05, val loss: 0.15994839370250702\n",
      "Epoch 3592: train loss: 2.794653846649453e-05, val loss: 0.16087310016155243\n",
      "Epoch 3593: train loss: 1.279214302485343e-05, val loss: 0.16057917475700378\n",
      "Epoch 3594: train loss: 1.1226422429899685e-05, val loss: 0.16003666818141937\n",
      "Epoch 3595: train loss: 1.991315366467461e-05, val loss: 0.16123469173908234\n",
      "Epoch 3596: train loss: 1.3156234672351275e-05, val loss: 0.16106554865837097\n",
      "Epoch 3597: train loss: 1.0043534530268516e-05, val loss: 0.16018112003803253\n",
      "Epoch 3598: train loss: 1.4229308362700976e-05, val loss: 0.16184082627296448\n",
      "Epoch 3599: train loss: 1.044999862642726e-05, val loss: 0.16098371148109436\n",
      "Epoch 3600: train loss: 5.219818831392331e-06, val loss: 0.16074863076210022\n",
      "Epoch 3601: train loss: 7.592474958073581e-06, val loss: 0.16240514814853668\n",
      "Epoch 3602: train loss: 1.0337619642086793e-05, val loss: 0.1613984853029251\n",
      "Epoch 3603: train loss: 6.61916828903486e-06, val loss: 0.16176748275756836\n",
      "Epoch 3604: train loss: 4.771404746861663e-06, val loss: 0.1624341905117035\n",
      "Epoch 3605: train loss: 6.657681751676137e-06, val loss: 0.1621858924627304\n",
      "Epoch 3606: train loss: 4.575563707476249e-06, val loss: 0.1623111218214035\n",
      "Epoch 3607: train loss: 2.534742179705063e-06, val loss: 0.1627262383699417\n",
      "Epoch 3608: train loss: 4.757339866046095e-06, val loss: 0.1632041186094284\n",
      "Epoch 3609: train loss: 5.2345803851494566e-06, val loss: 0.16296635568141937\n",
      "Epoch 3610: train loss: 2.9892512429796625e-06, val loss: 0.1634952425956726\n",
      "Epoch 3611: train loss: 2.6083628199558007e-06, val loss: 0.16365967690944672\n",
      "Epoch 3612: train loss: 3.2962245768430876e-06, val loss: 0.1636214703321457\n",
      "Epoch 3613: train loss: 2.3052916731103323e-06, val loss: 0.1639595329761505\n",
      "Epoch 3614: train loss: 1.2875310630988679e-06, val loss: 0.16431860625743866\n",
      "Epoch 3615: train loss: 2.138015588570852e-06, val loss: 0.1644565612077713\n",
      "Epoch 3616: train loss: 2.824963303282857e-06, val loss: 0.16457511484622955\n",
      "Epoch 3617: train loss: 1.9125284325127723e-06, val loss: 0.16507384181022644\n",
      "Epoch 3618: train loss: 1.3046558251517126e-06, val loss: 0.16480639576911926\n",
      "Epoch 3619: train loss: 1.7337465578748379e-06, val loss: 0.16541926562786102\n",
      "Epoch 3620: train loss: 1.5646887732145842e-06, val loss: 0.16543494164943695\n",
      "Epoch 3621: train loss: 8.929428645387816e-07, val loss: 0.1655353456735611\n",
      "Epoch 3622: train loss: 6.490822670457419e-07, val loss: 0.16598324477672577\n",
      "Epoch 3623: train loss: 9.052540690390742e-07, val loss: 0.16589762270450592\n",
      "Epoch 3624: train loss: 1.1306971146041178e-06, val loss: 0.16628356277942657\n",
      "Epoch 3625: train loss: 9.506575793238881e-07, val loss: 0.16633355617523193\n",
      "Epoch 3626: train loss: 7.657583296349912e-07, val loss: 0.16658762097358704\n",
      "Epoch 3627: train loss: 9.043645263773215e-07, val loss: 0.16666820645332336\n",
      "Epoch 3628: train loss: 1.068061123987718e-06, val loss: 0.16701114177703857\n",
      "Epoch 3629: train loss: 1.0327623840566957e-06, val loss: 0.16700033843517303\n",
      "Epoch 3630: train loss: 9.00043232832104e-07, val loss: 0.167342871427536\n",
      "Epoch 3631: train loss: 8.373184527954436e-07, val loss: 0.1673109382390976\n",
      "Epoch 3632: train loss: 9.985793667510734e-07, val loss: 0.1677687168121338\n",
      "Epoch 3633: train loss: 1.1355667766110855e-06, val loss: 0.16744060814380646\n",
      "Epoch 3634: train loss: 1.2119207895011641e-06, val loss: 0.16814152896404266\n",
      "Epoch 3635: train loss: 1.421557271896745e-06, val loss: 0.16785487532615662\n",
      "Epoch 3636: train loss: 1.9142594283039216e-06, val loss: 0.16828928887844086\n",
      "Epoch 3637: train loss: 2.785626293189125e-06, val loss: 0.16814492642879486\n",
      "Epoch 3638: train loss: 4.329757757659536e-06, val loss: 0.16884295642375946\n",
      "Epoch 3639: train loss: 6.930685685802018e-06, val loss: 0.16810500621795654\n",
      "Epoch 3640: train loss: 1.1856656783493236e-05, val loss: 0.16955456137657166\n",
      "Epoch 3641: train loss: 2.1353323973016813e-05, val loss: 0.16785775125026703\n",
      "Epoch 3642: train loss: 4.0175778849516064e-05, val loss: 0.1705157309770584\n",
      "Epoch 3643: train loss: 7.746662595309317e-05, val loss: 0.1671818345785141\n",
      "Epoch 3644: train loss: 0.00015487174096051604, val loss: 0.1721300333738327\n",
      "Epoch 3645: train loss: 0.0003132529091089964, val loss: 0.1650591939687729\n",
      "Epoch 3646: train loss: 0.000645760796032846, val loss: 0.17549794912338257\n",
      "Epoch 3647: train loss: 0.0012818401446565986, val loss: 0.16037486493587494\n",
      "Epoch 3648: train loss: 0.0024243860971182585, val loss: 0.18071308732032776\n",
      "Epoch 3649: train loss: 0.0034590112045407295, val loss: 0.1601199209690094\n",
      "Epoch 3650: train loss: 0.003473983844742179, val loss: 0.18196351826190948\n",
      "Epoch 3651: train loss: 0.001651343540288508, val loss: 0.16985349357128143\n",
      "Epoch 3652: train loss: 0.00022674023057334125, val loss: 0.17047595977783203\n",
      "Epoch 3653: train loss: 0.0008676016004756093, val loss: 0.18022827804088593\n",
      "Epoch 3654: train loss: 0.0011054761707782745, val loss: 0.16787002980709076\n",
      "Epoch 3655: train loss: 0.00037465724744834006, val loss: 0.17519742250442505\n",
      "Epoch 3656: train loss: 0.0005263432394713163, val loss: 0.17958632111549377\n",
      "Epoch 3657: train loss: 0.0004888076800853014, val loss: 0.17201165854930878\n",
      "Epoch 3658: train loss: 0.00033429599716328084, val loss: 0.17591558396816254\n",
      "Epoch 3659: train loss: 0.0003569006221368909, val loss: 0.1762680858373642\n",
      "Epoch 3660: train loss: 0.00024406662851106375, val loss: 0.1731835901737213\n",
      "Epoch 3661: train loss: 0.0003253669128753245, val loss: 0.18052034080028534\n",
      "Epoch 3662: train loss: 0.00015105953207239509, val loss: 0.18007536232471466\n",
      "Epoch 3663: train loss: 0.00023827461700420827, val loss: 0.17003212869167328\n",
      "Epoch 3664: train loss: 0.00018746215209830552, val loss: 0.1757427304983139\n",
      "Epoch 3665: train loss: 9.748920274432749e-05, val loss: 0.18403875827789307\n",
      "Epoch 3666: train loss: 0.0002181008894694969, val loss: 0.1741715669631958\n",
      "Epoch 3667: train loss: 4.375225762487389e-05, val loss: 0.1710846722126007\n",
      "Epoch 3668: train loss: 0.00016671667981427163, val loss: 0.18292003870010376\n",
      "Epoch 3669: train loss: 6.497692811535671e-05, val loss: 0.18272970616817474\n",
      "Epoch 3670: train loss: 9.798018436413258e-05, val loss: 0.172537162899971\n",
      "Epoch 3671: train loss: 8.321915083797649e-05, val loss: 0.1764775961637497\n",
      "Epoch 3672: train loss: 5.435518323793076e-05, val loss: 0.18577788770198822\n",
      "Epoch 3673: train loss: 7.697308319620788e-05, val loss: 0.18058107793331146\n",
      "Epoch 3674: train loss: 4.454507870832458e-05, val loss: 0.1753653734922409\n",
      "Epoch 3675: train loss: 5.4898267990211025e-05, val loss: 0.1807408630847931\n",
      "Epoch 3676: train loss: 4.410292604006827e-05, val loss: 0.18191197514533997\n",
      "Epoch 3677: train loss: 3.800538252107799e-05, val loss: 0.1766180843114853\n",
      "Epoch 3678: train loss: 3.7165347748668864e-05, val loss: 0.1773156225681305\n",
      "Epoch 3679: train loss: 3.3242107747355476e-05, val loss: 0.1811797320842743\n",
      "Epoch 3680: train loss: 2.4218154067057185e-05, val loss: 0.17870239913463593\n",
      "Epoch 3681: train loss: 3.399479101062752e-05, val loss: 0.17556266486644745\n",
      "Epoch 3682: train loss: 1.3450050573737826e-05, val loss: 0.1780252307653427\n",
      "Epoch 3683: train loss: 3.1007279176265e-05, val loss: 0.17991819977760315\n",
      "Epoch 3684: train loss: 1.0678877515601926e-05, val loss: 0.1779625415802002\n",
      "Epoch 3685: train loss: 2.277047315146774e-05, val loss: 0.17679564654827118\n",
      "Epoch 3686: train loss: 1.27500625239918e-05, val loss: 0.17748326063156128\n",
      "Epoch 3687: train loss: 1.4193630704539828e-05, val loss: 0.17803552746772766\n",
      "Epoch 3688: train loss: 1.3372762623475865e-05, val loss: 0.17798306047916412\n",
      "Epoch 3689: train loss: 1.031011197483167e-05, val loss: 0.1774580031633377\n",
      "Epoch 3690: train loss: 1.0492360161151737e-05, val loss: 0.17729118466377258\n",
      "Epoch 3691: train loss: 9.927961400535423e-06, val loss: 0.17799842357635498\n",
      "Epoch 3692: train loss: 6.5589429141255096e-06, val loss: 0.17816059291362762\n",
      "Epoch 3693: train loss: 9.857437362370547e-06, val loss: 0.17700599133968353\n",
      "Epoch 3694: train loss: 4.337083282734966e-06, val loss: 0.17714159190654755\n",
      "Epoch 3695: train loss: 8.067724593274761e-06, val loss: 0.17866374552249908\n",
      "Epoch 3696: train loss: 4.38905635746778e-06, val loss: 0.177964985370636\n",
      "Epoch 3697: train loss: 5.139066161063965e-06, val loss: 0.17660856246948242\n",
      "Epoch 3698: train loss: 5.105807304062182e-06, val loss: 0.17781242728233337\n",
      "Epoch 3699: train loss: 3.014122739841696e-06, val loss: 0.17875292897224426\n",
      "Epoch 3700: train loss: 4.7112416723393835e-06, val loss: 0.17735806107521057\n",
      "Epoch 3701: train loss: 2.66388087766245e-06, val loss: 0.1769936978816986\n",
      "Epoch 3702: train loss: 3.2120983632921707e-06, val loss: 0.17846596240997314\n",
      "Epoch 3703: train loss: 2.8037175070494413e-06, val loss: 0.17821036279201508\n",
      "Epoch 3704: train loss: 2.254743549201521e-06, val loss: 0.17709267139434814\n",
      "Epoch 3705: train loss: 2.2212218482309254e-06, val loss: 0.17769315838813782\n",
      "Epoch 3706: train loss: 2.281180968566332e-06, val loss: 0.17800122499465942\n",
      "Epoch 3707: train loss: 1.3116869013174437e-06, val loss: 0.17736747860908508\n",
      "Epoch 3708: train loss: 2.251319301649346e-06, val loss: 0.1776210069656372\n",
      "Epoch 3709: train loss: 9.335556114820065e-07, val loss: 0.17797811329364777\n",
      "Epoch 3710: train loss: 1.7212789771292591e-06, val loss: 0.17737142741680145\n",
      "Epoch 3711: train loss: 1.0431858754600398e-06, val loss: 0.17749975621700287\n",
      "Epoch 3712: train loss: 1.049408297149057e-06, val loss: 0.17798210680484772\n",
      "Epoch 3713: train loss: 1.1475600558696897e-06, val loss: 0.17733711004257202\n",
      "Epoch 3714: train loss: 7.440458489327284e-07, val loss: 0.17729228734970093\n",
      "Epoch 3715: train loss: 9.348125331598567e-07, val loss: 0.17798982560634613\n",
      "Epoch 3716: train loss: 7.148976237658644e-07, val loss: 0.17772126197814941\n",
      "Epoch 3717: train loss: 7.276343580997491e-07, val loss: 0.1771690994501114\n",
      "Epoch 3718: train loss: 6.259768952077138e-07, val loss: 0.17790921032428741\n",
      "Epoch 3719: train loss: 6.693355771858478e-07, val loss: 0.17807112634181976\n",
      "Epoch 3720: train loss: 5.870286940989899e-07, val loss: 0.17772535979747772\n",
      "Epoch 3721: train loss: 6.548983151333232e-07, val loss: 0.17757593095302582\n",
      "Epoch 3722: train loss: 8.329261049766501e-07, val loss: 0.17805738747119904\n",
      "Epoch 3723: train loss: 1.0280546121066436e-06, val loss: 0.17764712870121002\n",
      "Epoch 3724: train loss: 1.8003613604378188e-06, val loss: 0.1780463457107544\n",
      "Epoch 3725: train loss: 2.7654537007038016e-06, val loss: 0.17740128934383392\n",
      "Epoch 3726: train loss: 4.281355813873233e-06, val loss: 0.17830507457256317\n",
      "Epoch 3727: train loss: 6.457883500843309e-06, val loss: 0.1774287074804306\n",
      "Epoch 3728: train loss: 9.784595931705553e-06, val loss: 0.17834360897541046\n",
      "Epoch 3729: train loss: 1.5505765986745246e-05, val loss: 0.17673437297344208\n",
      "Epoch 3730: train loss: 2.5260022084694356e-05, val loss: 0.1790371984243393\n",
      "Epoch 3731: train loss: 4.331579475547187e-05, val loss: 0.17598198354244232\n",
      "Epoch 3732: train loss: 7.676991663174704e-05, val loss: 0.17912349104881287\n",
      "Epoch 3733: train loss: 0.00014170139911584556, val loss: 0.17444662749767303\n",
      "Epoch 3734: train loss: 0.00025875450228340924, val loss: 0.17885582149028778\n",
      "Epoch 3735: train loss: 0.0003607152320910245, val loss: 0.17013852298259735\n",
      "Epoch 3736: train loss: 0.00022576663468498737, val loss: 0.17545068264007568\n",
      "Epoch 3737: train loss: 3.057337016798556e-05, val loss: 0.17336609959602356\n",
      "Epoch 3738: train loss: 7.766587077639997e-05, val loss: 0.1673751324415207\n",
      "Epoch 3739: train loss: 0.00019016624719370157, val loss: 0.17367351055145264\n",
      "Epoch 3740: train loss: 0.00013721012510359287, val loss: 0.16818633675575256\n",
      "Epoch 3741: train loss: 4.5247605157783255e-05, val loss: 0.16754014790058136\n",
      "Epoch 3742: train loss: 6.580950866919011e-05, val loss: 0.17034702003002167\n",
      "Epoch 3743: train loss: 9.703017713036388e-05, val loss: 0.16509926319122314\n",
      "Epoch 3744: train loss: 6.665076216449961e-05, val loss: 0.1678779423236847\n",
      "Epoch 3745: train loss: 5.030997272115201e-05, val loss: 0.16696131229400635\n",
      "Epoch 3746: train loss: 5.629268707707524e-05, val loss: 0.16376993060112\n",
      "Epoch 3747: train loss: 4.32749766332563e-05, val loss: 0.16703109443187714\n",
      "Epoch 3748: train loss: 3.4334356314502656e-05, val loss: 0.16481398046016693\n",
      "Epoch 3749: train loss: 3.9585163904121146e-05, val loss: 0.1636921912431717\n",
      "Epoch 3750: train loss: 4.300960063119419e-05, val loss: 0.16570764780044556\n",
      "Epoch 3751: train loss: 2.733172004809603e-05, val loss: 0.16348691284656525\n",
      "Epoch 3752: train loss: 1.2987809896003455e-05, val loss: 0.16238246858119965\n",
      "Epoch 3753: train loss: 3.1632949685445055e-05, val loss: 0.16344186663627625\n",
      "Epoch 3754: train loss: 1.6299833077937365e-05, val loss: 0.16356125473976135\n",
      "Epoch 3755: train loss: 1.1988341611868236e-05, val loss: 0.16452394425868988\n",
      "Epoch 3756: train loss: 1.5083020116435364e-05, val loss: 0.16320259869098663\n",
      "Epoch 3757: train loss: 2.1027781258453615e-05, val loss: 0.1627425104379654\n",
      "Epoch 3758: train loss: 7.790265044604894e-06, val loss: 0.1645517647266388\n",
      "Epoch 3759: train loss: 9.143178431259003e-06, val loss: 0.16327017545700073\n",
      "Epoch 3760: train loss: 1.1662491488095839e-05, val loss: 0.16221769154071808\n",
      "Epoch 3761: train loss: 1.245710882358253e-05, val loss: 0.1636490672826767\n",
      "Epoch 3762: train loss: 6.020467026246479e-06, val loss: 0.16332347691059113\n",
      "Epoch 3763: train loss: 6.413188202714082e-06, val loss: 0.16259698569774628\n",
      "Epoch 3764: train loss: 7.197285412985366e-06, val loss: 0.1630055457353592\n",
      "Epoch 3765: train loss: 7.89098339737393e-06, val loss: 0.16274647414684296\n",
      "Epoch 3766: train loss: 5.722471087210579e-06, val loss: 0.16264058649539948\n",
      "Epoch 3767: train loss: 3.935949735023314e-06, val loss: 0.1629830151796341\n",
      "Epoch 3768: train loss: 3.532638856995618e-06, val loss: 0.16238532960414886\n",
      "Epoch 3769: train loss: 5.949268143012887e-06, val loss: 0.16248738765716553\n",
      "Epoch 3770: train loss: 4.941398856317392e-06, val loss: 0.16285458207130432\n",
      "Epoch 3771: train loss: 2.0896434307360323e-06, val loss: 0.1623748540878296\n",
      "Epoch 3772: train loss: 1.9593185243138578e-06, val loss: 0.16265268623828888\n",
      "Epoch 3773: train loss: 4.034734502056381e-06, val loss: 0.16249994933605194\n",
      "Epoch 3774: train loss: 3.7604006593028316e-06, val loss: 0.16233500838279724\n",
      "Epoch 3775: train loss: 1.5654846947654733e-06, val loss: 0.1627665013074875\n",
      "Epoch 3776: train loss: 1.471843347644608e-06, val loss: 0.162296861410141\n",
      "Epoch 3777: train loss: 2.297086211910937e-06, val loss: 0.1624102145433426\n",
      "Epoch 3778: train loss: 2.5347096652694745e-06, val loss: 0.1624600887298584\n",
      "Epoch 3779: train loss: 1.2313585102674551e-06, val loss: 0.16237016022205353\n",
      "Epoch 3780: train loss: 1.3140470400685444e-06, val loss: 0.1626766324043274\n",
      "Epoch 3781: train loss: 1.6378678537876112e-06, val loss: 0.1620797961950302\n",
      "Epoch 3782: train loss: 1.5508461501667625e-06, val loss: 0.16244852542877197\n",
      "Epoch 3783: train loss: 8.265466817647393e-07, val loss: 0.1625327616930008\n",
      "Epoch 3784: train loss: 7.908602128736675e-07, val loss: 0.16219906508922577\n",
      "Epoch 3785: train loss: 1.1165241176058771e-06, val loss: 0.1625068634748459\n",
      "Epoch 3786: train loss: 1.2459514664442395e-06, val loss: 0.16221201419830322\n",
      "Epoch 3787: train loss: 1.0340254448237829e-06, val loss: 0.1625034213066101\n",
      "Epoch 3788: train loss: 6.151346383376222e-07, val loss: 0.1622779816389084\n",
      "Epoch 3789: train loss: 4.927247232444643e-07, val loss: 0.1623239368200302\n",
      "Epoch 3790: train loss: 4.2339749484199274e-07, val loss: 0.16239941120147705\n",
      "Epoch 3791: train loss: 5.888497298656148e-07, val loss: 0.16224102675914764\n",
      "Epoch 3792: train loss: 6.995009016463882e-07, val loss: 0.16253383457660675\n",
      "Epoch 3793: train loss: 8.860727689352643e-07, val loss: 0.1621101349592209\n",
      "Epoch 3794: train loss: 7.528318519689492e-07, val loss: 0.16266952455043793\n",
      "Epoch 3795: train loss: 4.949041567670065e-07, val loss: 0.16228656470775604\n",
      "Epoch 3796: train loss: 2.7921655032514536e-07, val loss: 0.16247132420539856\n",
      "Epoch 3797: train loss: 2.1190834331719088e-07, val loss: 0.16263389587402344\n",
      "Epoch 3798: train loss: 2.9411876312224194e-07, val loss: 0.16241107881069183\n",
      "Epoch 3799: train loss: 3.6116495039095753e-07, val loss: 0.1626974642276764\n",
      "Epoch 3800: train loss: 3.945850153286301e-07, val loss: 0.16248787939548492\n",
      "Epoch 3801: train loss: 3.3036587865353795e-07, val loss: 0.16278396546840668\n",
      "Epoch 3802: train loss: 2.8698622145384434e-07, val loss: 0.16259241104125977\n",
      "Epoch 3803: train loss: 2.495490889486973e-07, val loss: 0.1627848893404007\n",
      "Epoch 3804: train loss: 2.1872928357424826e-07, val loss: 0.16270659863948822\n",
      "Epoch 3805: train loss: 1.779443152827298e-07, val loss: 0.1628526896238327\n",
      "Epoch 3806: train loss: 1.6725661566852068e-07, val loss: 0.1627826988697052\n",
      "Epoch 3807: train loss: 2.1312938258688519e-07, val loss: 0.16290076076984406\n",
      "Epoch 3808: train loss: 3.3141213862109e-07, val loss: 0.16285517811775208\n",
      "Epoch 3809: train loss: 6.441349569286103e-07, val loss: 0.16303443908691406\n",
      "Epoch 3810: train loss: 1.3104021263643517e-06, val loss: 0.16280041635036469\n",
      "Epoch 3811: train loss: 2.854695139831165e-06, val loss: 0.163238525390625\n",
      "Epoch 3812: train loss: 6.509688319056295e-06, val loss: 0.16264300048351288\n",
      "Epoch 3813: train loss: 1.5483346942346543e-05, val loss: 0.163701593875885\n",
      "Epoch 3814: train loss: 3.822590224444866e-05, val loss: 0.16208775341510773\n",
      "Epoch 3815: train loss: 9.721382230054587e-05, val loss: 0.16510163247585297\n",
      "Epoch 3816: train loss: 0.00022624515986535698, val loss: 0.1624634563922882\n",
      "Epoch 3817: train loss: 0.00038350155227817595, val loss: 0.16804517805576324\n",
      "Epoch 3818: train loss: 0.000511513149831444, val loss: 0.16162724792957306\n",
      "Epoch 3819: train loss: 0.0006167435785755515, val loss: 0.17219872772693634\n",
      "Epoch 3820: train loss: 0.0005612411769106984, val loss: 0.15966211259365082\n",
      "Epoch 3821: train loss: 0.000329015456372872, val loss: 0.17087344825267792\n",
      "Epoch 3822: train loss: 0.000255749182542786, val loss: 0.16489069163799286\n",
      "Epoch 3823: train loss: 0.0003043685283046216, val loss: 0.16473160684108734\n",
      "Epoch 3824: train loss: 0.00026069473824463785, val loss: 0.17011582851409912\n",
      "Epoch 3825: train loss: 0.0001832304842537269, val loss: 0.16204778850078583\n",
      "Epoch 3826: train loss: 0.00016402550681959838, val loss: 0.16626816987991333\n",
      "Epoch 3827: train loss: 0.00016722860164009035, val loss: 0.16902051866054535\n",
      "Epoch 3828: train loss: 0.0001410717813996598, val loss: 0.16142873466014862\n",
      "Epoch 3829: train loss: 7.299292337847874e-05, val loss: 0.16483229398727417\n",
      "Epoch 3830: train loss: 5.8512319810688496e-05, val loss: 0.16687904298305511\n",
      "Epoch 3831: train loss: 0.00012672963202930987, val loss: 0.16217544674873352\n",
      "Epoch 3832: train loss: 0.00011059244570787996, val loss: 0.1652507483959198\n",
      "Epoch 3833: train loss: 2.4863120415830053e-05, val loss: 0.16679523885250092\n",
      "Epoch 3834: train loss: 3.844565071631223e-05, val loss: 0.1633274406194687\n",
      "Epoch 3835: train loss: 7.87896424299106e-05, val loss: 0.1658337116241455\n",
      "Epoch 3836: train loss: 4.970630106981844e-05, val loss: 0.16704629361629486\n",
      "Epoch 3837: train loss: 3.462887252680957e-05, val loss: 0.1636807769536972\n",
      "Epoch 3838: train loss: 3.3637930755503476e-05, val loss: 0.16530732810497284\n",
      "Epoch 3839: train loss: 2.8517139071482234e-05, val loss: 0.16696463525295258\n",
      "Epoch 3840: train loss: 3.623576776590198e-05, val loss: 0.1638634353876114\n",
      "Epoch 3841: train loss: 3.099785317317583e-05, val loss: 0.16492462158203125\n",
      "Epoch 3842: train loss: 1.8997796360054053e-05, val loss: 0.16671013832092285\n",
      "Epoch 3843: train loss: 1.6950903955148533e-05, val loss: 0.16371110081672668\n",
      "Epoch 3844: train loss: 2.0294353817007504e-05, val loss: 0.16437429189682007\n",
      "Epoch 3845: train loss: 2.4435768864350393e-05, val loss: 0.16608557105064392\n",
      "Epoch 3846: train loss: 1.4368256415764336e-05, val loss: 0.16408373415470123\n",
      "Epoch 3847: train loss: 6.841301001259126e-06, val loss: 0.16427777707576752\n",
      "Epoch 3848: train loss: 1.4785667190153617e-05, val loss: 0.164735347032547\n",
      "Epoch 3849: train loss: 1.6317379049723968e-05, val loss: 0.16398575901985168\n",
      "Epoch 3850: train loss: 9.092116670217365e-06, val loss: 0.16462858021259308\n",
      "Epoch 3851: train loss: 7.392335646727588e-06, val loss: 0.16405066847801208\n",
      "Epoch 3852: train loss: 7.559096502518514e-06, val loss: 0.16400499641895294\n",
      "Epoch 3853: train loss: 8.785964382695965e-06, val loss: 0.1645161211490631\n",
      "Epoch 3854: train loss: 8.95058565220097e-06, val loss: 0.1635708063840866\n",
      "Epoch 3855: train loss: 5.920570401940495e-06, val loss: 0.16413943469524384\n",
      "Epoch 3856: train loss: 4.092297331226291e-06, val loss: 0.16408777236938477\n",
      "Epoch 3857: train loss: 4.448513664101483e-06, val loss: 0.16326777637004852\n",
      "Epoch 3858: train loss: 5.9631920521496795e-06, val loss: 0.16424500942230225\n",
      "Epoch 3859: train loss: 6.025056336511625e-06, val loss: 0.1636074036359787\n",
      "Epoch 3860: train loss: 3.2660832403053064e-06, val loss: 0.1630399078130722\n",
      "Epoch 3861: train loss: 1.6498852346558124e-06, val loss: 0.1641443520784378\n",
      "Epoch 3862: train loss: 3.219048949176795e-06, val loss: 0.16354654729366302\n",
      "Epoch 3863: train loss: 4.302259185351431e-06, val loss: 0.16311225295066833\n",
      "Epoch 3864: train loss: 3.331526158945053e-06, val loss: 0.16373585164546967\n",
      "Epoch 3865: train loss: 1.930815415107645e-06, val loss: 0.16339367628097534\n",
      "Epoch 3866: train loss: 1.5399665471704793e-06, val loss: 0.16312430799007416\n",
      "Epoch 3867: train loss: 1.985585186048411e-06, val loss: 0.16343486309051514\n",
      "Epoch 3868: train loss: 1.875879206636455e-06, val loss: 0.16314809024333954\n",
      "Epoch 3869: train loss: 1.9883250388375018e-06, val loss: 0.16291670501232147\n",
      "Epoch 3870: train loss: 2.395359842921607e-06, val loss: 0.16324616968631744\n",
      "Epoch 3871: train loss: 1.5901159713394009e-06, val loss: 0.16293887794017792\n",
      "Epoch 3872: train loss: 5.523412482943968e-07, val loss: 0.16275820136070251\n",
      "Epoch 3873: train loss: 7.011606157902861e-07, val loss: 0.16304969787597656\n",
      "Epoch 3874: train loss: 1.166427182397456e-06, val loss: 0.16267795860767365\n",
      "Epoch 3875: train loss: 1.287497980229091e-06, val loss: 0.16261357069015503\n",
      "Epoch 3876: train loss: 1.3201644151195069e-06, val loss: 0.16284310817718506\n",
      "Epoch 3877: train loss: 1.1130767916256445e-06, val loss: 0.16245833039283752\n",
      "Epoch 3878: train loss: 8.679435268277302e-07, val loss: 0.16244344413280487\n",
      "Epoch 3879: train loss: 7.673593813706248e-07, val loss: 0.1625272035598755\n",
      "Epoch 3880: train loss: 4.927226200379664e-07, val loss: 0.1622941493988037\n",
      "Epoch 3881: train loss: 2.7934299851040123e-07, val loss: 0.16227786242961884\n",
      "Epoch 3882: train loss: 4.155434396579949e-07, val loss: 0.16226568818092346\n",
      "Epoch 3883: train loss: 5.528904694074299e-07, val loss: 0.16217850148677826\n",
      "Epoch 3884: train loss: 5.686300141860556e-07, val loss: 0.1620229035615921\n",
      "Epoch 3885: train loss: 6.365583544720721e-07, val loss: 0.1620919108390808\n",
      "Epoch 3886: train loss: 7.039325851110334e-07, val loss: 0.16198484599590302\n",
      "Epoch 3887: train loss: 8.150599342116038e-07, val loss: 0.16178326308727264\n",
      "Epoch 3888: train loss: 1.036877506521705e-06, val loss: 0.1619679033756256\n",
      "Epoch 3889: train loss: 1.359497218800243e-06, val loss: 0.16165697574615479\n",
      "Epoch 3890: train loss: 1.7936122276296373e-06, val loss: 0.16171613335609436\n",
      "Epoch 3891: train loss: 2.599449771878426e-06, val loss: 0.1615869700908661\n",
      "Epoch 3892: train loss: 4.193034328636713e-06, val loss: 0.16154348850250244\n",
      "Epoch 3893: train loss: 7.2956731855811086e-06, val loss: 0.16147176921367645\n",
      "Epoch 3894: train loss: 1.337936737400014e-05, val loss: 0.16134727001190186\n",
      "Epoch 3895: train loss: 2.5840634407359175e-05, val loss: 0.16134761273860931\n",
      "Epoch 3896: train loss: 5.2457820856943727e-05, val loss: 0.16121497750282288\n",
      "Epoch 3897: train loss: 0.0001108556825784035, val loss: 0.16101180016994476\n",
      "Epoch 3898: train loss: 0.00024300890800077468, val loss: 0.16072218120098114\n",
      "Epoch 3899: train loss: 0.0005217674188315868, val loss: 0.15917204320430756\n",
      "Epoch 3900: train loss: 0.000851675751619041, val loss: 0.16023890674114227\n",
      "Epoch 3901: train loss: 0.0009176449966616929, val loss: 0.15274512767791748\n",
      "Epoch 3902: train loss: 0.0006817393004894257, val loss: 0.16182838380336761\n",
      "Epoch 3903: train loss: 0.0003923521435353905, val loss: 0.15403859317302704\n",
      "Epoch 3904: train loss: 0.0004784048069268465, val loss: 0.16405856609344482\n",
      "Epoch 3905: train loss: 0.00038197662797756493, val loss: 0.15580567717552185\n",
      "Epoch 3906: train loss: 0.0003108477976638824, val loss: 0.1584748476743698\n",
      "Epoch 3907: train loss: 0.00027072845841757953, val loss: 0.15677185356616974\n",
      "Epoch 3908: train loss: 0.00014530860062222928, val loss: 0.15521351993083954\n",
      "Epoch 3909: train loss: 0.0001890591229312122, val loss: 0.1612904816865921\n",
      "Epoch 3910: train loss: 0.00024874185328371823, val loss: 0.15460842847824097\n",
      "Epoch 3911: train loss: 0.00013146192941349, val loss: 0.1544649302959442\n",
      "Epoch 3912: train loss: 0.00012029211211483926, val loss: 0.15668778121471405\n",
      "Epoch 3913: train loss: 0.0001281875156564638, val loss: 0.15631476044654846\n",
      "Epoch 3914: train loss: 8.931617776397616e-05, val loss: 0.1566648930311203\n",
      "Epoch 3915: train loss: 0.0001344857446383685, val loss: 0.15143844485282898\n",
      "Epoch 3916: train loss: 7.712464139331132e-05, val loss: 0.15570767223834991\n",
      "Epoch 3917: train loss: 5.300226257531904e-05, val loss: 0.15540681779384613\n",
      "Epoch 3918: train loss: 7.255711534526199e-05, val loss: 0.15047580003738403\n",
      "Epoch 3919: train loss: 7.97348766354844e-05, val loss: 0.15446914732456207\n",
      "Epoch 3920: train loss: 4.1094324842561036e-05, val loss: 0.15377406775951385\n",
      "Epoch 3921: train loss: 4.637427628040314e-05, val loss: 0.15000362694263458\n",
      "Epoch 3922: train loss: 4.88232217321638e-05, val loss: 0.15286365151405334\n",
      "Epoch 3923: train loss: 4.639126927941106e-05, val loss: 0.15179197490215302\n",
      "Epoch 3924: train loss: 3.789353650063276e-05, val loss: 0.15203337371349335\n",
      "Epoch 3925: train loss: 3.80938254238572e-05, val loss: 0.1520228534936905\n",
      "Epoch 3926: train loss: 3.563982681953348e-05, val loss: 0.1512077897787094\n",
      "Epoch 3927: train loss: 4.0994353184942156e-05, val loss: 0.15155203640460968\n",
      "Epoch 3928: train loss: 4.420150798978284e-05, val loss: 0.1530389040708542\n",
      "Epoch 3929: train loss: 3.536732401698828e-05, val loss: 0.15135496854782104\n",
      "Epoch 3930: train loss: 4.751183223561384e-05, val loss: 0.15261971950531006\n",
      "Epoch 3931: train loss: 5.6731449149083346e-05, val loss: 0.15077467262744904\n",
      "Epoch 3932: train loss: 5.717713793274015e-05, val loss: 0.15300489962100983\n",
      "Epoch 3933: train loss: 6.994156865403056e-05, val loss: 0.15108942985534668\n",
      "Epoch 3934: train loss: 8.114529919112101e-05, val loss: 0.15230998396873474\n",
      "Epoch 3935: train loss: 0.00010257968824589625, val loss: 0.1513393372297287\n",
      "Epoch 3936: train loss: 0.00011800206993939355, val loss: 0.1526729017496109\n",
      "Epoch 3937: train loss: 0.00014185048348736018, val loss: 0.14993421733379364\n",
      "Epoch 3938: train loss: 0.00017169283819384873, val loss: 0.15337823331356049\n",
      "Epoch 3939: train loss: 0.00020253521506674588, val loss: 0.14930105209350586\n",
      "Epoch 3940: train loss: 0.00022846474894322455, val loss: 0.15319205820560455\n",
      "Epoch 3941: train loss: 0.00024756978382356465, val loss: 0.14961442351341248\n",
      "Epoch 3942: train loss: 0.0002539068227633834, val loss: 0.15089204907417297\n",
      "Epoch 3943: train loss: 0.00023485424753744155, val loss: 0.1490437090396881\n",
      "Epoch 3944: train loss: 0.00019161078671459109, val loss: 0.15108440816402435\n",
      "Epoch 3945: train loss: 0.00012784224236384034, val loss: 0.14781233668327332\n",
      "Epoch 3946: train loss: 6.411912909243256e-05, val loss: 0.1493331640958786\n",
      "Epoch 3947: train loss: 1.6426085494458675e-05, val loss: 0.14820106327533722\n",
      "Epoch 3948: train loss: 2.8331241992418654e-06, val loss: 0.14738880097866058\n",
      "Epoch 3949: train loss: 2.0457828213693574e-05, val loss: 0.14864161610603333\n",
      "Epoch 3950: train loss: 4.636063385987654e-05, val loss: 0.14664478600025177\n",
      "Epoch 3951: train loss: 6.324760033749044e-05, val loss: 0.14874200522899628\n",
      "Epoch 3952: train loss: 5.5631546274526045e-05, val loss: 0.146556556224823\n",
      "Epoch 3953: train loss: 3.0564158805646e-05, val loss: 0.14749979972839355\n",
      "Epoch 3954: train loss: 8.16471492726123e-06, val loss: 0.1478012651205063\n",
      "Epoch 3955: train loss: 1.565703314554412e-06, val loss: 0.14692029356956482\n",
      "Epoch 3956: train loss: 1.1191108569619246e-05, val loss: 0.14763741195201874\n",
      "Epoch 3957: train loss: 2.550137105572503e-05, val loss: 0.14650985598564148\n",
      "Epoch 3958: train loss: 2.99783223454142e-05, val loss: 0.1475551426410675\n",
      "Epoch 3959: train loss: 2.203565600211732e-05, val loss: 0.14637909829616547\n",
      "Epoch 3960: train loss: 9.703353498480283e-06, val loss: 0.1468869298696518\n",
      "Epoch 3961: train loss: 1.666047865001019e-06, val loss: 0.14676061272621155\n",
      "Epoch 3962: train loss: 2.2345243451127317e-06, val loss: 0.14607329666614532\n",
      "Epoch 3963: train loss: 9.035652510647196e-06, val loss: 0.14677822589874268\n",
      "Epoch 3964: train loss: 1.4958026440581307e-05, val loss: 0.14554888010025024\n",
      "Epoch 3965: train loss: 1.48879998960183e-05, val loss: 0.14647260308265686\n",
      "Epoch 3966: train loss: 9.69766279013129e-06, val loss: 0.1458262801170349\n",
      "Epoch 3967: train loss: 3.5550363008951535e-06, val loss: 0.14568185806274414\n",
      "Epoch 3968: train loss: 5.17527894317027e-07, val loss: 0.14598236978054047\n",
      "Epoch 3969: train loss: 1.4923688240742194e-06, val loss: 0.14580462872982025\n",
      "Epoch 3970: train loss: 4.8040292313089594e-06, val loss: 0.14604854583740234\n",
      "Epoch 3971: train loss: 7.714133062108885e-06, val loss: 0.14556577801704407\n",
      "Epoch 3972: train loss: 8.312209502037149e-06, val loss: 0.14632146060466766\n",
      "Epoch 3973: train loss: 6.8005479079147335e-06, val loss: 0.14558997750282288\n",
      "Epoch 3974: train loss: 4.451002496352885e-06, val loss: 0.14620976150035858\n",
      "Epoch 3975: train loss: 2.413405127299484e-06, val loss: 0.14584295451641083\n",
      "Epoch 3976: train loss: 1.2098050774511648e-06, val loss: 0.1460236757993698\n",
      "Epoch 3977: train loss: 1.022421656671213e-06, val loss: 0.14610694348812103\n",
      "Epoch 3978: train loss: 1.3768857343166019e-06, val loss: 0.14579705893993378\n",
      "Epoch 3979: train loss: 1.9086025986325694e-06, val loss: 0.14629004895687103\n",
      "Epoch 3980: train loss: 2.543496975704329e-06, val loss: 0.14581330120563507\n",
      "Epoch 3981: train loss: 3.223824478482129e-06, val loss: 0.14639979600906372\n",
      "Epoch 3982: train loss: 3.971085789089557e-06, val loss: 0.14573775231838226\n",
      "Epoch 3983: train loss: 4.767358859680826e-06, val loss: 0.14646939933300018\n",
      "Epoch 3984: train loss: 5.579305252467748e-06, val loss: 0.14589376747608185\n",
      "Epoch 3985: train loss: 6.5152171373483725e-06, val loss: 0.14643460512161255\n",
      "Epoch 3986: train loss: 7.717593689449131e-06, val loss: 0.14590893685817719\n",
      "Epoch 3987: train loss: 9.345111720904242e-06, val loss: 0.14661765098571777\n",
      "Epoch 3988: train loss: 1.1853284377139062e-05, val loss: 0.14586155116558075\n",
      "Epoch 3989: train loss: 1.5933021131786518e-05, val loss: 0.14670804142951965\n",
      "Epoch 3990: train loss: 2.2734613594366238e-05, val loss: 0.14587083458900452\n",
      "Epoch 3991: train loss: 3.4588818380143493e-05, val loss: 0.14709463715553284\n",
      "Epoch 3992: train loss: 5.5909007642185315e-05, val loss: 0.1454872190952301\n",
      "Epoch 3993: train loss: 9.503285400569439e-05, val loss: 0.147859126329422\n",
      "Epoch 3994: train loss: 0.0001638141111470759, val loss: 0.1455155611038208\n",
      "Epoch 3995: train loss: 0.00021850891062058508, val loss: 0.14954040944576263\n",
      "Epoch 3996: train loss: 0.00027648964896798134, val loss: 0.1448196917772293\n",
      "Epoch 3997: train loss: 0.000453942600870505, val loss: 0.15384049713611603\n",
      "Epoch 3998: train loss: 0.0007725710165686905, val loss: 0.14408127963542938\n",
      "Epoch 3999: train loss: 0.0012021701550111175, val loss: 0.15752506256103516\n",
      "Epoch 4000: train loss: 0.0016959240892902017, val loss: 0.14192815124988556\n",
      "Epoch 4001: train loss: 0.0020204014144837856, val loss: 0.15633371472358704\n",
      "Epoch 4002: train loss: 0.0010240910341963172, val loss: 0.1414555013179779\n",
      "Epoch 4003: train loss: 0.0006567855016328394, val loss: 0.14310841262340546\n",
      "Epoch 4004: train loss: 0.0011643655598163605, val loss: 0.14815165102481842\n",
      "Epoch 4005: train loss: 0.000518981774803251, val loss: 0.14504551887512207\n",
      "Epoch 4006: train loss: 0.0003852278459817171, val loss: 0.13960812985897064\n",
      "Epoch 4007: train loss: 0.0005573616945184767, val loss: 0.1453946977853775\n",
      "Epoch 4008: train loss: 0.00033360274392180145, val loss: 0.14640429615974426\n",
      "Epoch 4009: train loss: 0.0003788710164371878, val loss: 0.14263960719108582\n",
      "Epoch 4010: train loss: 0.00025195616763085127, val loss: 0.1465766876935959\n",
      "Epoch 4011: train loss: 0.0003044673940166831, val loss: 0.14433757960796356\n",
      "Epoch 4012: train loss: 0.00017210737860295922, val loss: 0.14015063643455505\n",
      "Epoch 4013: train loss: 0.0002491008781362325, val loss: 0.1456153690814972\n",
      "Epoch 4014: train loss: 0.00014111575728747994, val loss: 0.14751973748207092\n",
      "Epoch 4015: train loss: 0.0001666152529651299, val loss: 0.1437571495771408\n",
      "Epoch 4016: train loss: 0.0001523979299236089, val loss: 0.1442093402147293\n",
      "Epoch 4017: train loss: 0.00010084953100886196, val loss: 0.14534519612789154\n",
      "Epoch 4018: train loss: 0.0001447152317268774, val loss: 0.14715233445167542\n",
      "Epoch 4019: train loss: 6.0924019635422155e-05, val loss: 0.14838097989559174\n",
      "Epoch 4020: train loss: 0.00011280822945991531, val loss: 0.1468312293291092\n",
      "Epoch 4021: train loss: 6.521339673781767e-05, val loss: 0.1473613977432251\n",
      "Epoch 4022: train loss: 7.167948206188157e-05, val loss: 0.14948247373104095\n",
      "Epoch 4023: train loss: 6.269677396630868e-05, val loss: 0.1486503630876541\n",
      "Epoch 4024: train loss: 5.253472409094684e-05, val loss: 0.14741241931915283\n",
      "Epoch 4025: train loss: 5.418654836830683e-05, val loss: 0.14831505715847015\n",
      "Epoch 4026: train loss: 3.8917532947380096e-05, val loss: 0.14892761409282684\n",
      "Epoch 4027: train loss: 4.3654359615175053e-05, val loss: 0.14865358173847198\n",
      "Epoch 4028: train loss: 3.543935599736869e-05, val loss: 0.14858603477478027\n",
      "Epoch 4029: train loss: 3.121361442026682e-05, val loss: 0.14840641617774963\n",
      "Epoch 4030: train loss: 2.782238880172372e-05, val loss: 0.1478295922279358\n",
      "Epoch 4031: train loss: 3.037039095943328e-05, val loss: 0.14789743721485138\n",
      "Epoch 4032: train loss: 1.7905118511407636e-05, val loss: 0.1487288922071457\n",
      "Epoch 4033: train loss: 2.6149442419409752e-05, val loss: 0.14830228686332703\n",
      "Epoch 4034: train loss: 1.5655754395993426e-05, val loss: 0.14779837429523468\n",
      "Epoch 4035: train loss: 1.9487299141474068e-05, val loss: 0.14860676229000092\n",
      "Epoch 4036: train loss: 1.3826025679009035e-05, val loss: 0.14778371155261993\n",
      "Epoch 4037: train loss: 1.459047598473262e-05, val loss: 0.14634659886360168\n",
      "Epoch 4038: train loss: 1.3339805263967719e-05, val loss: 0.14760875701904297\n",
      "Epoch 4039: train loss: 9.718675755721051e-06, val loss: 0.14841189980506897\n",
      "Epoch 4040: train loss: 1.1544039807631634e-05, val loss: 0.14692728221416473\n",
      "Epoch 4041: train loss: 8.703629646333866e-06, val loss: 0.14704495668411255\n",
      "Epoch 4042: train loss: 8.442539183306508e-06, val loss: 0.14796766638755798\n",
      "Epoch 4043: train loss: 6.949355338292662e-06, val loss: 0.14688515663146973\n",
      "Epoch 4044: train loss: 7.751874363748357e-06, val loss: 0.1461573988199234\n",
      "Epoch 4045: train loss: 5.207539288676344e-06, val loss: 0.14725898206233978\n",
      "Epoch 4046: train loss: 5.998089363856707e-06, val loss: 0.1475330889225006\n",
      "Epoch 4047: train loss: 4.65024004370207e-06, val loss: 0.14640001952648163\n",
      "Epoch 4048: train loss: 5.02308148497832e-06, val loss: 0.1461714208126068\n",
      "Epoch 4049: train loss: 3.3094390801124973e-06, val loss: 0.1470591276884079\n",
      "Epoch 4050: train loss: 4.566090865409933e-06, val loss: 0.1466604471206665\n",
      "Epoch 4051: train loss: 2.2766560050513363e-06, val loss: 0.14570017158985138\n",
      "Epoch 4052: train loss: 4.259728029865073e-06, val loss: 0.14629463851451874\n",
      "Epoch 4053: train loss: 1.4437385971177719e-06, val loss: 0.1465776562690735\n",
      "Epoch 4054: train loss: 3.2852403819561005e-06, val loss: 0.1457659751176834\n",
      "Epoch 4055: train loss: 2.038646471191896e-06, val loss: 0.14584405720233917\n",
      "Epoch 4056: train loss: 1.781873038453341e-06, val loss: 0.14594458043575287\n",
      "Epoch 4057: train loss: 2.1207429199421313e-06, val loss: 0.14544227719306946\n",
      "Epoch 4058: train loss: 1.3370257647693506e-06, val loss: 0.1457119733095169\n",
      "Epoch 4059: train loss: 1.8448346281729755e-06, val loss: 0.1458456814289093\n",
      "Epoch 4060: train loss: 9.418898798685404e-07, val loss: 0.14515168964862823\n",
      "Epoch 4061: train loss: 1.5641344361938536e-06, val loss: 0.1449434608221054\n",
      "Epoch 4062: train loss: 7.921159976831404e-07, val loss: 0.1453363597393036\n",
      "Epoch 4063: train loss: 1.3338942608243087e-06, val loss: 0.14540810883045197\n",
      "Epoch 4064: train loss: 5.995048582008167e-07, val loss: 0.14491792023181915\n",
      "Epoch 4065: train loss: 9.938826224242803e-07, val loss: 0.14463290572166443\n",
      "Epoch 4066: train loss: 6.194692900862719e-07, val loss: 0.144849494099617\n",
      "Epoch 4067: train loss: 8.330439982273674e-07, val loss: 0.14481042325496674\n",
      "Epoch 4068: train loss: 4.662082062623085e-07, val loss: 0.14458957314491272\n",
      "Epoch 4069: train loss: 5.518315902008908e-07, val loss: 0.14445507526397705\n",
      "Epoch 4070: train loss: 6.425269134524569e-07, val loss: 0.1443251371383667\n",
      "Epoch 4071: train loss: 2.495320359230391e-07, val loss: 0.14444486796855927\n",
      "Epoch 4072: train loss: 6.096162223911961e-07, val loss: 0.14435513317584991\n",
      "Epoch 4073: train loss: 2.2952146139232354e-07, val loss: 0.1439918428659439\n",
      "Epoch 4074: train loss: 3.690554137847357e-07, val loss: 0.14408348500728607\n",
      "Epoch 4075: train loss: 3.4658225445127755e-07, val loss: 0.14421400427818298\n",
      "Epoch 4076: train loss: 2.5260294478357537e-07, val loss: 0.1439027637243271\n",
      "Epoch 4077: train loss: 2.6051097279378155e-07, val loss: 0.14376023411750793\n",
      "Epoch 4078: train loss: 1.8338461416078644e-07, val loss: 0.1439332216978073\n",
      "Epoch 4079: train loss: 2.6766892347040994e-07, val loss: 0.14380817115306854\n",
      "Epoch 4080: train loss: 1.524764883242824e-07, val loss: 0.1436229795217514\n",
      "Epoch 4081: train loss: 1.982683954793174e-07, val loss: 0.14375661313533783\n",
      "Epoch 4082: train loss: 1.4635892853220867e-07, val loss: 0.14362020790576935\n",
      "Epoch 4083: train loss: 1.3483476379860804e-07, val loss: 0.14353735744953156\n",
      "Epoch 4084: train loss: 1.3773500029401475e-07, val loss: 0.14359597861766815\n",
      "Epoch 4085: train loss: 1.1160750545968767e-07, val loss: 0.14341318607330322\n",
      "Epoch 4086: train loss: 1.402325437993568e-07, val loss: 0.1434013545513153\n",
      "Epoch 4087: train loss: 7.452051420386852e-08, val loss: 0.14345555007457733\n",
      "Epoch 4088: train loss: 8.353943314887147e-08, val loss: 0.14332763850688934\n",
      "Epoch 4089: train loss: 9.426529601341826e-08, val loss: 0.1432134062051773\n",
      "Epoch 4090: train loss: 5.606295161442176e-08, val loss: 0.1432967633008957\n",
      "Epoch 4091: train loss: 7.515694022686148e-08, val loss: 0.14328841865062714\n",
      "Epoch 4092: train loss: 6.985055023278619e-08, val loss: 0.1430576890707016\n",
      "Epoch 4093: train loss: 5.7915219286996944e-08, val loss: 0.1431068480014801\n",
      "Epoch 4094: train loss: 4.101842066006611e-08, val loss: 0.1431606262922287\n",
      "Epoch 4095: train loss: 4.9134541768580675e-08, val loss: 0.14307211339473724\n",
      "Epoch 4096: train loss: 4.078836113308171e-08, val loss: 0.14295583963394165\n",
      "Epoch 4097: train loss: 3.269847681508509e-08, val loss: 0.1429033726453781\n",
      "Epoch 4098: train loss: 4.0534541057013485e-08, val loss: 0.14295528829097748\n",
      "Epoch 4099: train loss: 3.8204788666007516e-08, val loss: 0.1428195685148239\n",
      "Epoch 4100: train loss: 3.0528376271377056e-08, val loss: 0.14278395473957062\n",
      "Epoch 4101: train loss: 3.1216117690746614e-08, val loss: 0.1427479237318039\n",
      "Epoch 4102: train loss: 2.588696901284493e-08, val loss: 0.14270183444023132\n",
      "Epoch 4103: train loss: 2.726664760643871e-08, val loss: 0.14264972507953644\n",
      "Epoch 4104: train loss: 2.0711665271733182e-08, val loss: 0.14263926446437836\n",
      "Epoch 4105: train loss: 1.5105170447782257e-08, val loss: 0.14258134365081787\n",
      "Epoch 4106: train loss: 2.1069853417543527e-08, val loss: 0.14252988994121552\n",
      "Epoch 4107: train loss: 1.614894351575913e-08, val loss: 0.14247854053974152\n",
      "Epoch 4108: train loss: 1.3298813605899795e-08, val loss: 0.14240756630897522\n",
      "Epoch 4109: train loss: 1.5229142391604e-08, val loss: 0.14238879084587097\n",
      "Epoch 4110: train loss: 1.128608939637843e-08, val loss: 0.14229850471019745\n",
      "Epoch 4111: train loss: 9.32428978472899e-09, val loss: 0.14229044318199158\n",
      "Epoch 4112: train loss: 1.1381778186603242e-08, val loss: 0.14237664639949799\n",
      "Epoch 4113: train loss: 1.2408529315166561e-08, val loss: 0.14243179559707642\n",
      "Epoch 4114: train loss: 9.583114746192223e-09, val loss: 0.14254479110240936\n",
      "Epoch 4115: train loss: 6.365156846044329e-09, val loss: 0.14262855052947998\n",
      "Epoch 4116: train loss: 8.360192538248157e-09, val loss: 0.1426951289176941\n",
      "Epoch 4117: train loss: 9.556473834493318e-09, val loss: 0.14280787110328674\n",
      "Epoch 4118: train loss: 8.650874683269194e-09, val loss: 0.1428787112236023\n",
      "Epoch 4119: train loss: 1.2197370224953374e-08, val loss: 0.14298561215400696\n",
      "Epoch 4120: train loss: 1.66303859572281e-08, val loss: 0.14301873743534088\n",
      "Epoch 4121: train loss: 2.5044130325113656e-08, val loss: 0.1431000977754593\n",
      "Epoch 4122: train loss: 8.906919646278766e-08, val loss: 0.14326803386211395\n",
      "Epoch 4123: train loss: 3.989781873769971e-07, val loss: 0.14318092167377472\n",
      "Epoch 4124: train loss: 1.3687562159248046e-06, val loss: 0.14354689419269562\n",
      "Epoch 4125: train loss: 4.027011527796276e-06, val loss: 0.1431179940700531\n",
      "Epoch 4126: train loss: 1.1700609320541844e-05, val loss: 0.14408695697784424\n",
      "Epoch 4127: train loss: 3.413547528907657e-05, val loss: 0.14251159131526947\n",
      "Epoch 4128: train loss: 9.841976861935109e-05, val loss: 0.14507801830768585\n",
      "Epoch 4129: train loss: 0.00026802619686350226, val loss: 0.14181943237781525\n",
      "Epoch 4130: train loss: 0.000618769321590662, val loss: 0.14629676938056946\n",
      "Epoch 4131: train loss: 0.0011520172702148557, val loss: 0.14971505105495453\n",
      "Epoch 4132: train loss: 0.0012606345117092133, val loss: 0.14737065136432648\n",
      "Epoch 4133: train loss: 0.0008844250114634633, val loss: 0.1566583216190338\n",
      "Epoch 4134: train loss: 0.0006840380956418812, val loss: 0.14815741777420044\n",
      "Epoch 4135: train loss: 0.0006870666402392089, val loss: 0.16649168729782104\n",
      "Epoch 4136: train loss: 0.0007605210412293673, val loss: 0.15791013836860657\n",
      "Epoch 4137: train loss: 0.0008674380951561034, val loss: 0.17434467375278473\n",
      "Epoch 4138: train loss: 0.0008277653832919896, val loss: 0.16530227661132812\n",
      "Epoch 4139: train loss: 0.00042329629650339484, val loss: 0.17126081883907318\n",
      "Epoch 4140: train loss: 0.00011973290384048596, val loss: 0.17870305478572845\n",
      "Epoch 4141: train loss: 0.00041900359792634845, val loss: 0.1768210381269455\n",
      "Epoch 4142: train loss: 0.0004225974262226373, val loss: 0.18137578666210175\n",
      "Epoch 4143: train loss: 0.00010756029223557562, val loss: 0.1791948527097702\n",
      "Epoch 4144: train loss: 0.00021380820544436574, val loss: 0.17690089344978333\n",
      "Epoch 4145: train loss: 0.0002338314661756158, val loss: 0.18308287858963013\n",
      "Epoch 4146: train loss: 0.0001298949064221233, val loss: 0.18299224972724915\n",
      "Epoch 4147: train loss: 0.00013858953025192022, val loss: 0.1802504062652588\n",
      "Epoch 4148: train loss: 0.00013333576498553157, val loss: 0.18418584764003754\n",
      "Epoch 4149: train loss: 0.00010760175064206123, val loss: 0.1836741715669632\n",
      "Epoch 4150: train loss: 9.206904360326007e-05, val loss: 0.1798909455537796\n",
      "Epoch 4151: train loss: 9.729582961881533e-05, val loss: 0.18479381501674652\n",
      "Epoch 4152: train loss: 7.252706564031541e-05, val loss: 0.18386176228523254\n",
      "Epoch 4153: train loss: 7.466644456144422e-05, val loss: 0.17806220054626465\n",
      "Epoch 4154: train loss: 5.414469342213124e-05, val loss: 0.18311290442943573\n",
      "Epoch 4155: train loss: 6.600591586902738e-05, val loss: 0.18622814118862152\n",
      "Epoch 4156: train loss: 4.0747287130216137e-05, val loss: 0.17960350215435028\n",
      "Epoch 4157: train loss: 4.435327718965709e-05, val loss: 0.1796921342611313\n",
      "Epoch 4158: train loss: 4.6533354179700837e-05, val loss: 0.18459199368953705\n",
      "Epoch 4159: train loss: 2.8847207431681454e-05, val loss: 0.18114453554153442\n",
      "Epoch 4160: train loss: 3.286975697847083e-05, val loss: 0.17822551727294922\n",
      "Epoch 4161: train loss: 3.331663538119756e-05, val loss: 0.18200528621673584\n",
      "Epoch 4162: train loss: 2.1888419723836705e-05, val loss: 0.1817883849143982\n",
      "Epoch 4163: train loss: 2.1744623154518194e-05, val loss: 0.1787511110305786\n",
      "Epoch 4164: train loss: 2.7233223590883426e-05, val loss: 0.18052580952644348\n",
      "Epoch 4165: train loss: 1.2767095540766604e-05, val loss: 0.18138369917869568\n",
      "Epoch 4166: train loss: 1.7647169443080202e-05, val loss: 0.17930243909358978\n",
      "Epoch 4167: train loss: 2.0493671399890445e-05, val loss: 0.18030425906181335\n",
      "Epoch 4168: train loss: 6.431156180042308e-06, val loss: 0.18108569085597992\n",
      "Epoch 4169: train loss: 1.6742498701205477e-05, val loss: 0.17889197170734406\n",
      "Epoch 4170: train loss: 1.203791907755658e-05, val loss: 0.17943431437015533\n",
      "Epoch 4171: train loss: 5.795483957626857e-06, val loss: 0.1812911033630371\n",
      "Epoch 4172: train loss: 1.3041184502071701e-05, val loss: 0.17977985739707947\n",
      "Epoch 4173: train loss: 7.290337634913158e-06, val loss: 0.17889922857284546\n",
      "Epoch 4174: train loss: 5.566524123423733e-06, val loss: 0.180179625749588\n",
      "Epoch 4175: train loss: 9.071990461961832e-06, val loss: 0.17991255223751068\n",
      "Epoch 4176: train loss: 5.265572326607071e-06, val loss: 0.17937599122524261\n",
      "Epoch 4177: train loss: 4.490701485337922e-06, val loss: 0.17998006939888\n",
      "Epoch 4178: train loss: 6.450788987422129e-06, val loss: 0.17952747642993927\n",
      "Epoch 4179: train loss: 3.6831513625656953e-06, val loss: 0.17895281314849854\n",
      "Epoch 4180: train loss: 4.11460678151343e-06, val loss: 0.17952881753444672\n",
      "Epoch 4181: train loss: 3.5031698644161224e-06, val loss: 0.1795600950717926\n",
      "Epoch 4182: train loss: 4.118938250030624e-06, val loss: 0.17930878698825836\n",
      "Epoch 4183: train loss: 1.9840717868646607e-06, val loss: 0.1794840544462204\n",
      "Epoch 4184: train loss: 3.112658987447503e-06, val loss: 0.17903880774974823\n",
      "Epoch 4185: train loss: 2.9541047297243495e-06, val loss: 0.17877033352851868\n",
      "Epoch 4186: train loss: 1.4814166888754698e-06, val loss: 0.1792323887348175\n",
      "Epoch 4187: train loss: 2.1640275917889085e-06, val loss: 0.17911814153194427\n",
      "Epoch 4188: train loss: 2.405642590019852e-06, val loss: 0.1788153201341629\n",
      "Epoch 4189: train loss: 1.0164590094063897e-06, val loss: 0.17889077961444855\n",
      "Epoch 4190: train loss: 1.5397750985357561e-06, val loss: 0.17860428988933563\n",
      "Epoch 4191: train loss: 1.8701672388488078e-06, val loss: 0.1785648614168167\n",
      "Epoch 4192: train loss: 7.634868666173134e-07, val loss: 0.17894048988819122\n",
      "Epoch 4193: train loss: 1.1742404240067117e-06, val loss: 0.17859070003032684\n",
      "Epoch 4194: train loss: 1.2105357427572017e-06, val loss: 0.17817531526088715\n",
      "Epoch 4195: train loss: 7.800484809195041e-07, val loss: 0.17843523621559143\n",
      "Epoch 4196: train loss: 9.021991331792378e-07, val loss: 0.17843474447727203\n",
      "Epoch 4197: train loss: 6.065250772735453e-07, val loss: 0.17815746366977692\n",
      "Epoch 4198: train loss: 8.54374320624629e-07, val loss: 0.17817196249961853\n",
      "Epoch 4199: train loss: 6.900399398546142e-07, val loss: 0.17814116179943085\n",
      "Epoch 4200: train loss: 2.3186493081084336e-07, val loss: 0.17799201607704163\n",
      "Epoch 4201: train loss: 7.80331504302012e-07, val loss: 0.17792294919490814\n",
      "Epoch 4202: train loss: 4.95547567425092e-07, val loss: 0.17785194516181946\n",
      "Epoch 4203: train loss: 2.6621688675732e-07, val loss: 0.17782816290855408\n",
      "Epoch 4204: train loss: 3.9866912970865087e-07, val loss: 0.17784856259822845\n",
      "Epoch 4205: train loss: 4.246640230576304e-07, val loss: 0.1777358502149582\n",
      "Epoch 4206: train loss: 3.7816451481376134e-07, val loss: 0.1775728166103363\n",
      "Epoch 4207: train loss: 1.368539841450911e-07, val loss: 0.17762111127376556\n",
      "Epoch 4208: train loss: 2.809426860039821e-07, val loss: 0.1776307225227356\n",
      "Epoch 4209: train loss: 3.592129473872774e-07, val loss: 0.17740021646022797\n",
      "Epoch 4210: train loss: 1.679801613363452e-07, val loss: 0.17734694480895996\n",
      "Epoch 4211: train loss: 1.521149926020371e-07, val loss: 0.17744553089141846\n",
      "Epoch 4212: train loss: 1.8641111410033773e-07, val loss: 0.17730598151683807\n",
      "Epoch 4213: train loss: 2.0894117369607557e-07, val loss: 0.17716149985790253\n",
      "Epoch 4214: train loss: 1.6531855351331615e-07, val loss: 0.17719228565692902\n",
      "Epoch 4215: train loss: 1.0311660503248277e-07, val loss: 0.17709895968437195\n",
      "Epoch 4216: train loss: 1.0400939487453797e-07, val loss: 0.1769731342792511\n",
      "Epoch 4217: train loss: 1.3206212656768912e-07, val loss: 0.1769910752773285\n",
      "Epoch 4218: train loss: 1.35290733283e-07, val loss: 0.176849365234375\n",
      "Epoch 4219: train loss: 9.848324111771944e-08, val loss: 0.17676787078380585\n",
      "Epoch 4220: train loss: 5.693572902032429e-08, val loss: 0.17683331668376923\n",
      "Epoch 4221: train loss: 6.774153149535778e-08, val loss: 0.1765957921743393\n",
      "Epoch 4222: train loss: 9.489406949114709e-08, val loss: 0.17658667266368866\n",
      "Epoch 4223: train loss: 7.805203949828865e-08, val loss: 0.17663900554180145\n",
      "Epoch 4224: train loss: 6.752738102022704e-08, val loss: 0.1763225793838501\n",
      "Epoch 4225: train loss: 5.930453994551499e-08, val loss: 0.17640136182308197\n",
      "Epoch 4226: train loss: 2.9068962348333116e-08, val loss: 0.17634496092796326\n",
      "Epoch 4227: train loss: 3.4163043949320127e-08, val loss: 0.1760835498571396\n",
      "Epoch 4228: train loss: 6.467828228551298e-08, val loss: 0.17615416646003723\n",
      "Epoch 4229: train loss: 6.086568760110822e-08, val loss: 0.17596200108528137\n",
      "Epoch 4230: train loss: 4.6972463252359375e-08, val loss: 0.17588333785533905\n",
      "Epoch 4231: train loss: 3.057201780620744e-08, val loss: 0.17584888637065887\n",
      "Epoch 4232: train loss: 1.8771414644902507e-08, val loss: 0.17564216256141663\n",
      "Epoch 4233: train loss: 2.7724077256152668e-08, val loss: 0.17563550174236298\n",
      "Epoch 4234: train loss: 3.25324656103021e-08, val loss: 0.17547917366027832\n",
      "Epoch 4235: train loss: 3.2454224196953874e-08, val loss: 0.17544759809970856\n",
      "Epoch 4236: train loss: 3.610540133536233e-08, val loss: 0.17526431381702423\n",
      "Epoch 4237: train loss: 3.5293517441914446e-08, val loss: 0.17529937624931335\n",
      "Epoch 4238: train loss: 2.798272902282406e-08, val loss: 0.1751151978969574\n",
      "Epoch 4239: train loss: 2.342360794216347e-08, val loss: 0.17501604557037354\n",
      "Epoch 4240: train loss: 2.6564057620248605e-08, val loss: 0.17500148713588715\n",
      "Epoch 4241: train loss: 3.6053144469860854e-08, val loss: 0.17481808364391327\n",
      "Epoch 4242: train loss: 4.521575291960289e-08, val loss: 0.17481593787670135\n",
      "Epoch 4243: train loss: 6.525009155211592e-08, val loss: 0.1746319979429245\n",
      "Epoch 4244: train loss: 1.0716603782157108e-07, val loss: 0.17465488612651825\n",
      "Epoch 4245: train loss: 1.8275045476912055e-07, val loss: 0.17437760531902313\n",
      "Epoch 4246: train loss: 3.2976797115225054e-07, val loss: 0.17453844845294952\n",
      "Epoch 4247: train loss: 6.481072887254413e-07, val loss: 0.1741390973329544\n",
      "Epoch 4248: train loss: 1.3630996136271278e-06, val loss: 0.1744292825460434\n",
      "Epoch 4249: train loss: 3.0466301268461393e-06, val loss: 0.17382597923278809\n",
      "Epoch 4250: train loss: 7.164437192841433e-06, val loss: 0.1744174063205719\n",
      "Epoch 4251: train loss: 1.7715405192575417e-05, val loss: 0.17337654531002045\n",
      "Epoch 4252: train loss: 4.502547380980104e-05, val loss: 0.17465510964393616\n",
      "Epoch 4253: train loss: 0.0001119278822443448, val loss: 0.17390742897987366\n",
      "Epoch 4254: train loss: 0.00028210534946992993, val loss: 0.1727672517299652\n",
      "Epoch 4255: train loss: 0.0007273591472767293, val loss: 0.17588265240192413\n",
      "Epoch 4256: train loss: 0.0017631673254072666, val loss: 0.16753897070884705\n",
      "Epoch 4257: train loss: 0.0030686070676892996, val loss: 0.1690765768289566\n",
      "Epoch 4258: train loss: 0.0010217964882031083, val loss: 0.18091660737991333\n",
      "Epoch 4259: train loss: 0.001584050478413701, val loss: 0.17299717664718628\n",
      "Epoch 4260: train loss: 0.00034357066033408046, val loss: 0.1711573302745819\n",
      "Epoch 4261: train loss: 0.0004866835952270776, val loss: 0.1738060861825943\n",
      "Epoch 4262: train loss: 0.0006646736292168498, val loss: 0.1685289889574051\n",
      "Epoch 4263: train loss: 0.0005299347103573382, val loss: 0.16407530009746552\n",
      "Epoch 4264: train loss: 0.00015516881830990314, val loss: 0.1586507111787796\n",
      "Epoch 4265: train loss: 0.00018761209503281862, val loss: 0.14782457053661346\n",
      "Epoch 4266: train loss: 0.00030618332675658166, val loss: 0.14555753767490387\n",
      "Epoch 4267: train loss: 0.00027348246658220887, val loss: 0.1413784623146057\n",
      "Epoch 4268: train loss: 0.00024484083405695856, val loss: 0.1357836276292801\n",
      "Epoch 4269: train loss: 0.00018562120385468006, val loss: 0.13833415508270264\n",
      "Epoch 4270: train loss: 8.793346933089197e-05, val loss: 0.13969174027442932\n",
      "Epoch 4271: train loss: 8.131566573865712e-05, val loss: 0.13476547598838806\n",
      "Epoch 4272: train loss: 0.0001057779518305324, val loss: 0.13357102870941162\n",
      "Epoch 4273: train loss: 0.00011267224908806384, val loss: 0.1340690553188324\n",
      "Epoch 4274: train loss: 0.00012364787107799202, val loss: 0.13013005256652832\n",
      "Epoch 4275: train loss: 9.386436431668699e-05, val loss: 0.1269650012254715\n",
      "Epoch 4276: train loss: 5.5303833505604416e-05, val loss: 0.12663502991199493\n",
      "Epoch 4277: train loss: 4.7241039283107966e-05, val loss: 0.12515603005886078\n",
      "Epoch 4278: train loss: 3.2158091926248744e-05, val loss: 0.12342619895935059\n",
      "Epoch 4279: train loss: 5.267268716124818e-05, val loss: 0.12264642864465714\n",
      "Epoch 4280: train loss: 5.785185203421861e-05, val loss: 0.12065797299146652\n",
      "Epoch 4281: train loss: 5.190887532080524e-05, val loss: 0.12023049592971802\n",
      "Epoch 4282: train loss: 4.147068102611229e-05, val loss: 0.12258904427289963\n",
      "Epoch 4283: train loss: 2.5144598112092353e-05, val loss: 0.12186173349618912\n",
      "Epoch 4284: train loss: 1.6750134818721563e-05, val loss: 0.11898256838321686\n",
      "Epoch 4285: train loss: 2.1822424969286658e-05, val loss: 0.11938293278217316\n",
      "Epoch 4286: train loss: 2.4805136490613222e-05, val loss: 0.12024533748626709\n",
      "Epoch 4287: train loss: 2.726838101807516e-05, val loss: 0.11871625483036041\n",
      "Epoch 4288: train loss: 2.3658631107537076e-05, val loss: 0.11913442611694336\n",
      "Epoch 4289: train loss: 1.5287720088963397e-05, val loss: 0.12069274485111237\n",
      "Epoch 4290: train loss: 1.3110788131598383e-05, val loss: 0.11852079629898071\n",
      "Epoch 4291: train loss: 6.531669441756094e-06, val loss: 0.11609786003828049\n",
      "Epoch 4292: train loss: 1.1249487215536647e-05, val loss: 0.11697949469089508\n",
      "Epoch 4293: train loss: 1.3387155377131421e-05, val loss: 0.11758454144001007\n",
      "Epoch 4294: train loss: 1.0204339560004883e-05, val loss: 0.11705922335386276\n",
      "Epoch 4295: train loss: 1.2377377970551606e-05, val loss: 0.11744041740894318\n",
      "Epoch 4296: train loss: 5.747516297560651e-06, val loss: 0.11691512912511826\n",
      "Epoch 4297: train loss: 4.459626779862447e-06, val loss: 0.11600569635629654\n",
      "Epoch 4298: train loss: 6.062011834728764e-06, val loss: 0.11703657358884811\n",
      "Epoch 4299: train loss: 4.817567969439551e-06, val loss: 0.11740787327289581\n",
      "Epoch 4300: train loss: 5.78343042434426e-06, val loss: 0.11640069633722305\n",
      "Epoch 4301: train loss: 7.247130270116031e-06, val loss: 0.11662731319665909\n",
      "Epoch 4302: train loss: 3.774319338845089e-06, val loss: 0.11675389856100082\n",
      "Epoch 4303: train loss: 3.3129342682514107e-06, val loss: 0.11596021801233292\n",
      "Epoch 4304: train loss: 3.631148274507723e-06, val loss: 0.11639081686735153\n",
      "Epoch 4305: train loss: 1.3291116829350358e-06, val loss: 0.11648017168045044\n",
      "Epoch 4306: train loss: 2.9295247259142343e-06, val loss: 0.1155184730887413\n",
      "Epoch 4307: train loss: 3.7628101381415036e-06, val loss: 0.11585469543933868\n",
      "Epoch 4308: train loss: 2.2335868834488792e-06, val loss: 0.11625122278928757\n",
      "Epoch 4309: train loss: 2.4349167233594926e-06, val loss: 0.11578989028930664\n",
      "Epoch 4310: train loss: 2.200656354034436e-06, val loss: 0.11604833602905273\n",
      "Epoch 4311: train loss: 1.2321934264036827e-06, val loss: 0.11623554676771164\n",
      "Epoch 4312: train loss: 1.1821064163086703e-06, val loss: 0.11626417934894562\n",
      "Epoch 4313: train loss: 1.2976371408512932e-06, val loss: 0.1163247749209404\n",
      "Epoch 4314: train loss: 1.7140253021352692e-06, val loss: 0.11589365452528\n",
      "Epoch 4315: train loss: 1.3822083246850525e-06, val loss: 0.11597461998462677\n",
      "Epoch 4316: train loss: 1.00220063359302e-06, val loss: 0.11615041643381119\n",
      "Epoch 4317: train loss: 6.940242656128248e-07, val loss: 0.11592216789722443\n",
      "Epoch 4318: train loss: 7.617119308633846e-07, val loss: 0.11594285815954208\n",
      "Epoch 4319: train loss: 7.108194495231146e-07, val loss: 0.11589568108320236\n",
      "Epoch 4320: train loss: 5.688630722033849e-07, val loss: 0.11578519642353058\n",
      "Epoch 4321: train loss: 7.05303705217375e-07, val loss: 0.11588958650827408\n",
      "Epoch 4322: train loss: 6.512864842989075e-07, val loss: 0.11597701162099838\n",
      "Epoch 4323: train loss: 5.63402579700778e-07, val loss: 0.11601996421813965\n",
      "Epoch 4324: train loss: 4.1936456796065613e-07, val loss: 0.11585745960474014\n",
      "Epoch 4325: train loss: 2.488663710664696e-07, val loss: 0.11589021980762482\n",
      "Epoch 4326: train loss: 3.903724348219839e-07, val loss: 0.11623825132846832\n",
      "Epoch 4327: train loss: 4.510457074502483e-07, val loss: 0.11610715836286545\n",
      "Epoch 4328: train loss: 2.615007019812765e-07, val loss: 0.11593998968601227\n",
      "Epoch 4329: train loss: 2.8891119541185617e-07, val loss: 0.1160881295800209\n",
      "Epoch 4330: train loss: 1.869259875775242e-07, val loss: 0.11608447879552841\n",
      "Epoch 4331: train loss: 1.3588059744051861e-07, val loss: 0.11613166332244873\n",
      "Epoch 4332: train loss: 2.8548501518343983e-07, val loss: 0.11606813967227936\n",
      "Epoch 4333: train loss: 2.1985918863265397e-07, val loss: 0.11606023460626602\n",
      "Epoch 4334: train loss: 1.7611914415738283e-07, val loss: 0.11606185883283615\n",
      "Epoch 4335: train loss: 1.804908862368393e-07, val loss: 0.115997813642025\n",
      "Epoch 4336: train loss: 1.4905096179518296e-07, val loss: 0.11625077575445175\n",
      "Epoch 4337: train loss: 1.7926035411619523e-07, val loss: 0.11619219928979874\n",
      "Epoch 4338: train loss: 1.4012049120992742e-07, val loss: 0.11602723598480225\n",
      "Epoch 4339: train loss: 1.457948002325793e-07, val loss: 0.11614344269037247\n",
      "Epoch 4340: train loss: 1.5709261447227618e-07, val loss: 0.11617668718099594\n",
      "Epoch 4341: train loss: 1.4681816651318513e-07, val loss: 0.11621662229299545\n",
      "Epoch 4342: train loss: 1.4281206972555083e-07, val loss: 0.11616750061511993\n",
      "Epoch 4343: train loss: 1.289793232217562e-07, val loss: 0.11620515584945679\n",
      "Epoch 4344: train loss: 1.3356954298160417e-07, val loss: 0.11625983566045761\n",
      "Epoch 4345: train loss: 1.427137874543405e-07, val loss: 0.11609174311161041\n",
      "Epoch 4346: train loss: 2.087889185986569e-07, val loss: 0.1161477342247963\n",
      "Epoch 4347: train loss: 3.5114678098580043e-07, val loss: 0.11626749485731125\n",
      "Epoch 4348: train loss: 7.351345061579195e-07, val loss: 0.11628081649541855\n",
      "Epoch 4349: train loss: 1.797730078578752e-06, val loss: 0.11635523289442062\n",
      "Epoch 4350: train loss: 3.846059826173587e-06, val loss: 0.11618264019489288\n",
      "Epoch 4351: train loss: 6.889050382596906e-06, val loss: 0.11662540584802628\n",
      "Epoch 4352: train loss: 7.238748821691843e-06, val loss: 0.11632927507162094\n",
      "Epoch 4353: train loss: 3.734425035872846e-06, val loss: 0.11642561107873917\n",
      "Epoch 4354: train loss: 1.1859084452225943e-06, val loss: 0.11639343947172165\n",
      "Epoch 4355: train loss: 1.8424615291223745e-06, val loss: 0.11649692058563232\n",
      "Epoch 4356: train loss: 3.476779284028453e-06, val loss: 0.11642465740442276\n",
      "Epoch 4357: train loss: 3.632829020716599e-06, val loss: 0.11658380180597305\n",
      "Epoch 4358: train loss: 2.385444759056554e-06, val loss: 0.11633024364709854\n",
      "Epoch 4359: train loss: 1.18427749384864e-06, val loss: 0.11646371334791183\n",
      "Epoch 4360: train loss: 1.0373158829679596e-06, val loss: 0.11659982055425644\n",
      "Epoch 4361: train loss: 1.727835638121178e-06, val loss: 0.11626166105270386\n",
      "Epoch 4362: train loss: 2.301701670148759e-06, val loss: 0.11671501398086548\n",
      "Epoch 4363: train loss: 2.1119201392139075e-06, val loss: 0.11630918085575104\n",
      "Epoch 4364: train loss: 1.3948158539278666e-06, val loss: 0.1166832447052002\n",
      "Epoch 4365: train loss: 7.78095227360609e-07, val loss: 0.11660762876272202\n",
      "Epoch 4366: train loss: 6.079347372178745e-07, val loss: 0.11642497032880783\n",
      "Epoch 4367: train loss: 8.573170475756342e-07, val loss: 0.11676788330078125\n",
      "Epoch 4368: train loss: 1.2234113455633633e-06, val loss: 0.1163451075553894\n",
      "Epoch 4369: train loss: 1.454809762435616e-06, val loss: 0.11693942546844482\n",
      "Epoch 4370: train loss: 1.439027073502075e-06, val loss: 0.11647150665521622\n",
      "Epoch 4371: train loss: 1.2823285260310513e-06, val loss: 0.11702527105808258\n",
      "Epoch 4372: train loss: 9.921417358782492e-07, val loss: 0.11670821905136108\n",
      "Epoch 4373: train loss: 2.7401201805332676e-05, val loss: 0.1193888708949089\n",
      "Epoch 4374: train loss: 9.742701513459906e-05, val loss: 0.11209620535373688\n",
      "Epoch 4375: train loss: 0.00017359841149300337, val loss: 0.12117209285497665\n",
      "Epoch 4376: train loss: 0.00020802015205845237, val loss: 0.11096563190221786\n",
      "Epoch 4377: train loss: 0.0002039007522398606, val loss: 0.12069910764694214\n",
      "Epoch 4378: train loss: 0.00017235979612451047, val loss: 0.11194580793380737\n",
      "Epoch 4379: train loss: 0.00013468062388710678, val loss: 0.11617948114871979\n",
      "Epoch 4380: train loss: 0.00010840294999070466, val loss: 0.11606430262327194\n",
      "Epoch 4381: train loss: 7.755441765766591e-05, val loss: 0.11367278546094894\n",
      "Epoch 4382: train loss: 4.5299591874936596e-05, val loss: 0.1164683848619461\n",
      "Epoch 4383: train loss: 4.316844206186943e-05, val loss: 0.1151975616812706\n",
      "Epoch 4384: train loss: 6.588525138795376e-05, val loss: 0.11525674164295197\n",
      "Epoch 4385: train loss: 6.62948950775899e-05, val loss: 0.11493512243032455\n",
      "Epoch 4386: train loss: 4.227949466439895e-05, val loss: 0.11512591689825058\n",
      "Epoch 4387: train loss: 2.981284706038423e-05, val loss: 0.11491995304822922\n",
      "Epoch 4388: train loss: 3.327744343550876e-05, val loss: 0.11437699943780899\n",
      "Epoch 4389: train loss: 3.573805952328257e-05, val loss: 0.11453627794981003\n",
      "Epoch 4390: train loss: 2.817883614625316e-05, val loss: 0.11443448066711426\n",
      "Epoch 4391: train loss: 1.8970136807183735e-05, val loss: 0.11438556015491486\n",
      "Epoch 4392: train loss: 2.1282612578943372e-05, val loss: 0.11382196098566055\n",
      "Epoch 4393: train loss: 2.572935954958666e-05, val loss: 0.11447204649448395\n",
      "Epoch 4394: train loss: 1.9145967598888092e-05, val loss: 0.11481478065252304\n",
      "Epoch 4395: train loss: 1.179337687062798e-05, val loss: 0.11373402178287506\n",
      "Epoch 4396: train loss: 1.1414494110795204e-05, val loss: 0.11458145827054977\n",
      "Epoch 4397: train loss: 1.3920498531660996e-05, val loss: 0.1147436872124672\n",
      "Epoch 4398: train loss: 1.5064300896483473e-05, val loss: 0.11408539861440659\n",
      "Epoch 4399: train loss: 1.3962680895929225e-05, val loss: 0.11474552005529404\n",
      "Epoch 4400: train loss: 8.419379810220562e-06, val loss: 0.11444313824176788\n",
      "Epoch 4401: train loss: 3.272711865065503e-06, val loss: 0.11409127712249756\n",
      "Epoch 4402: train loss: 5.80001506023109e-06, val loss: 0.11474212259054184\n",
      "Epoch 4403: train loss: 1.1393431122996844e-05, val loss: 0.11421459168195724\n",
      "Epoch 4404: train loss: 1.1444546544225886e-05, val loss: 0.1143496036529541\n",
      "Epoch 4405: train loss: 6.252833827602444e-06, val loss: 0.11433805525302887\n",
      "Epoch 4406: train loss: 2.121106945196516e-06, val loss: 0.1142633706331253\n",
      "Epoch 4407: train loss: 2.688391077754204e-06, val loss: 0.1149217039346695\n",
      "Epoch 4408: train loss: 5.717364729207475e-06, val loss: 0.11419709026813507\n",
      "Epoch 4409: train loss: 6.867625870654592e-06, val loss: 0.11513085663318634\n",
      "Epoch 4410: train loss: 5.464060450321995e-06, val loss: 0.11449725925922394\n",
      "Epoch 4411: train loss: 4.310993517719908e-06, val loss: 0.1149410754442215\n",
      "Epoch 4412: train loss: 4.054574674228206e-06, val loss: 0.11494498699903488\n",
      "Epoch 4413: train loss: 3.6889898638037266e-06, val loss: 0.1149851456284523\n",
      "Epoch 4414: train loss: 3.2841153370100074e-06, val loss: 0.11474987119436264\n",
      "Epoch 4415: train loss: 3.4853044326155214e-06, val loss: 0.11511056870222092\n",
      "Epoch 4416: train loss: 4.138892563787522e-06, val loss: 0.1146426722407341\n",
      "Epoch 4417: train loss: 5.275392595649464e-06, val loss: 0.11523919552564621\n",
      "Epoch 4418: train loss: 6.931061761861201e-06, val loss: 0.11434521526098251\n",
      "Epoch 4419: train loss: 1.0637186278472655e-05, val loss: 0.11598353832960129\n",
      "Epoch 4420: train loss: 2.00646863959264e-05, val loss: 0.11309832334518433\n",
      "Epoch 4421: train loss: 4.0964332583826035e-05, val loss: 0.11697900295257568\n",
      "Epoch 4422: train loss: 7.976526103448123e-05, val loss: 0.11163335293531418\n",
      "Epoch 4423: train loss: 0.00014709851529914886, val loss: 0.11886719614267349\n",
      "Epoch 4424: train loss: 0.00026582751888781786, val loss: 0.11048372089862823\n",
      "Epoch 4425: train loss: 0.0004885056987404823, val loss: 0.12123862653970718\n",
      "Epoch 4426: train loss: 0.0008867601864039898, val loss: 0.1089535728096962\n",
      "Epoch 4427: train loss: 0.0015986125217750669, val loss: 0.12243906408548355\n",
      "Epoch 4428: train loss: 0.0023964783176779747, val loss: 0.10858452320098877\n",
      "Epoch 4429: train loss: 0.0027692425064742565, val loss: 0.12194158881902695\n",
      "Epoch 4430: train loss: 0.0014501827536150813, val loss: 0.11990150064229965\n",
      "Epoch 4431: train loss: 0.00017384772945661098, val loss: 0.11805994808673859\n",
      "Epoch 4432: train loss: 0.0006937567377462983, val loss: 0.1260121911764145\n",
      "Epoch 4433: train loss: 0.0009573086281307042, val loss: 0.12220802158117294\n",
      "Epoch 4434: train loss: 0.0002654080744832754, val loss: 0.11938469856977463\n",
      "Epoch 4435: train loss: 0.0004152192850597203, val loss: 0.12439646571874619\n",
      "Epoch 4436: train loss: 0.0004257241089362651, val loss: 0.12337690591812134\n",
      "Epoch 4437: train loss: 0.00020380080968607217, val loss: 0.12022425979375839\n",
      "Epoch 4438: train loss: 0.000301605265121907, val loss: 0.12375432252883911\n",
      "Epoch 4439: train loss: 0.00021277547057252377, val loss: 0.125515878200531\n",
      "Epoch 4440: train loss: 0.00016368928481824696, val loss: 0.12195084244012833\n",
      "Epoch 4441: train loss: 0.00020942332048434764, val loss: 0.11954402923583984\n",
      "Epoch 4442: train loss: 9.47391235968098e-05, val loss: 0.12080700695514679\n",
      "Epoch 4443: train loss: 0.0001808659144444391, val loss: 0.12091739475727081\n",
      "Epoch 4444: train loss: 6.558869790751487e-05, val loss: 0.11976950615644455\n",
      "Epoch 4445: train loss: 0.00014035796630196273, val loss: 0.11855518817901611\n",
      "Epoch 4446: train loss: 5.3448686230694875e-05, val loss: 0.11897461861371994\n",
      "Epoch 4447: train loss: 0.00010634806676534936, val loss: 0.11907675117254257\n",
      "Epoch 4448: train loss: 4.772097963723354e-05, val loss: 0.1176050454378128\n",
      "Epoch 4449: train loss: 7.96870153862983e-05, val loss: 0.11640556156635284\n",
      "Epoch 4450: train loss: 4.210116821923293e-05, val loss: 0.1174188181757927\n",
      "Epoch 4451: train loss: 5.9274720115354285e-05, val loss: 0.11859516054391861\n",
      "Epoch 4452: train loss: 3.724454654729925e-05, val loss: 0.11773301661014557\n",
      "Epoch 4453: train loss: 4.422392157721333e-05, val loss: 0.11584556102752686\n",
      "Epoch 4454: train loss: 3.239163925172761e-05, val loss: 0.11571478098630905\n",
      "Epoch 4455: train loss: 3.4198295907117426e-05, val loss: 0.1170867457985878\n",
      "Epoch 4456: train loss: 2.6766847440740094e-05, val loss: 0.11729762703180313\n",
      "Epoch 4457: train loss: 2.696449519135058e-05, val loss: 0.11617709696292877\n",
      "Epoch 4458: train loss: 2.221778231614735e-05, val loss: 0.1156022772192955\n",
      "Epoch 4459: train loss: 2.0787487301276997e-05, val loss: 0.11605747044086456\n",
      "Epoch 4460: train loss: 1.9150405933032744e-05, val loss: 0.11622919887304306\n",
      "Epoch 4461: train loss: 1.5985693607944995e-05, val loss: 0.11582259088754654\n",
      "Epoch 4462: train loss: 1.575509668327868e-05, val loss: 0.11574708670377731\n",
      "Epoch 4463: train loss: 1.3351688721741084e-05, val loss: 0.11607743799686432\n",
      "Epoch 4464: train loss: 1.221058118971996e-05, val loss: 0.11606556177139282\n",
      "Epoch 4465: train loss: 1.1389226528990548e-05, val loss: 0.11571625620126724\n",
      "Epoch 4466: train loss: 9.415827662451193e-06, val loss: 0.11573580652475357\n",
      "Epoch 4467: train loss: 9.492900062468834e-06, val loss: 0.11598584800958633\n",
      "Epoch 4468: train loss: 7.529194590460975e-06, val loss: 0.11586310714483261\n",
      "Epoch 4469: train loss: 7.999646186362952e-06, val loss: 0.11554481089115143\n",
      "Epoch 4470: train loss: 5.683291419700254e-06, val loss: 0.11562855541706085\n",
      "Epoch 4471: train loss: 7.164359431044431e-06, val loss: 0.11597754061222076\n",
      "Epoch 4472: train loss: 3.971205842390191e-06, val loss: 0.11594738811254501\n",
      "Epoch 4473: train loss: 6.222282536327839e-06, val loss: 0.11561673134565353\n",
      "Epoch 4474: train loss: 3.2005411867430666e-06, val loss: 0.11549180001020432\n",
      "Epoch 4475: train loss: 4.85650843984331e-06, val loss: 0.1158013865351677\n",
      "Epoch 4476: train loss: 2.981435500259977e-06, val loss: 0.11598139256238937\n",
      "Epoch 4477: train loss: 3.5893399399355985e-06, val loss: 0.11569645255804062\n",
      "Epoch 4478: train loss: 2.639134208948235e-06, val loss: 0.1155107393860817\n",
      "Epoch 4479: train loss: 2.9204932161519537e-06, val loss: 0.11575984209775925\n",
      "Epoch 4480: train loss: 2.016150801864569e-06, val loss: 0.1158880740404129\n",
      "Epoch 4481: train loss: 2.4368553113163216e-06, val loss: 0.11560662090778351\n",
      "Epoch 4482: train loss: 1.6970124079307425e-06, val loss: 0.11540987342596054\n",
      "Epoch 4483: train loss: 1.8694621530812583e-06, val loss: 0.1155465617775917\n",
      "Epoch 4484: train loss: 1.5353648450400215e-06, val loss: 0.11556705087423325\n",
      "Epoch 4485: train loss: 1.3461086609822814e-06, val loss: 0.11535270512104034\n",
      "Epoch 4486: train loss: 1.371019266116491e-06, val loss: 0.11532127857208252\n",
      "Epoch 4487: train loss: 1.0900167808358674e-06, val loss: 0.11546462774276733\n",
      "Epoch 4488: train loss: 1.0534957937125e-06, val loss: 0.11541067808866501\n",
      "Epoch 4489: train loss: 9.651213304096018e-07, val loss: 0.11527160555124283\n",
      "Epoch 4490: train loss: 7.977541258696874e-07, val loss: 0.11531658470630646\n",
      "Epoch 4491: train loss: 8.600041496720223e-07, val loss: 0.11539395153522491\n",
      "Epoch 4492: train loss: 5.975078920528176e-07, val loss: 0.11534998565912247\n",
      "Epoch 4493: train loss: 7.266080501722172e-07, val loss: 0.11531201750040054\n",
      "Epoch 4494: train loss: 4.686200725245726e-07, val loss: 0.11533185094594955\n",
      "Epoch 4495: train loss: 6.29325938916736e-07, val loss: 0.11529946327209473\n",
      "Epoch 4496: train loss: 3.713321916620771e-07, val loss: 0.11529600620269775\n",
      "Epoch 4497: train loss: 4.938895017403411e-07, val loss: 0.11536772549152374\n",
      "Epoch 4498: train loss: 3.587720129871741e-07, val loss: 0.11535071581602097\n",
      "Epoch 4499: train loss: 3.4369756463092926e-07, val loss: 0.11526000499725342\n",
      "Epoch 4500: train loss: 3.5184012858735514e-07, val loss: 0.11528606712818146\n",
      "Epoch 4501: train loss: 2.418042583940405e-07, val loss: 0.11538832634687424\n",
      "Epoch 4502: train loss: 3.106310657585709e-07, val loss: 0.11537497490644455\n",
      "Epoch 4503: train loss: 1.9493113256885408e-07, val loss: 0.115290068089962\n",
      "Epoch 4504: train loss: 2.7503705268827616e-07, val loss: 0.11528754234313965\n",
      "Epoch 4505: train loss: 1.2519619474460342e-07, val loss: 0.11535431444644928\n",
      "Epoch 4506: train loss: 2.551913382831117e-07, val loss: 0.11536949127912521\n",
      "Epoch 4507: train loss: 1.0627866942058972e-07, val loss: 0.11531275510787964\n",
      "Epoch 4508: train loss: 1.8325779649330798e-07, val loss: 0.1152646541595459\n",
      "Epoch 4509: train loss: 1.2492336054492625e-07, val loss: 0.11531436443328857\n",
      "Epoch 4510: train loss: 1.1002266120385684e-07, val loss: 0.11532105505466461\n",
      "Epoch 4511: train loss: 1.5673697362217354e-07, val loss: 0.11519805341959\n",
      "Epoch 4512: train loss: 5.3189292970046154e-08, val loss: 0.11516141891479492\n",
      "Epoch 4513: train loss: 1.3369141527164174e-07, val loss: 0.1152452602982521\n",
      "Epoch 4514: train loss: 6.000293240049359e-08, val loss: 0.11523944139480591\n",
      "Epoch 4515: train loss: 1.0598589739174713e-07, val loss: 0.11513538658618927\n",
      "Epoch 4516: train loss: 5.7178610290975485e-08, val loss: 0.115114726126194\n",
      "Epoch 4517: train loss: 7.260536705189224e-08, val loss: 0.11514484882354736\n",
      "Epoch 4518: train loss: 6.737516144994515e-08, val loss: 0.11510501056909561\n",
      "Epoch 4519: train loss: 5.041820827500487e-08, val loss: 0.11503119766712189\n",
      "Epoch 4520: train loss: 5.774835898364472e-08, val loss: 0.11507391929626465\n",
      "Epoch 4521: train loss: 5.246714351869741e-08, val loss: 0.1150570660829544\n",
      "Epoch 4522: train loss: 5.632517385834035e-08, val loss: 0.11500872671604156\n",
      "Epoch 4523: train loss: 7.118782718862349e-08, val loss: 0.11491753906011581\n",
      "Epoch 4524: train loss: 1.1592512549896128e-07, val loss: 0.11501424759626389\n",
      "Epoch 4525: train loss: 2.6029317723441636e-07, val loss: 0.11490633338689804\n",
      "Epoch 4526: train loss: 5.125332336319843e-07, val loss: 0.11482596397399902\n",
      "Epoch 4527: train loss: 6.72376700094901e-07, val loss: 0.11478336155414581\n",
      "Epoch 4528: train loss: 6.072906444387627e-07, val loss: 0.11482767015695572\n",
      "Epoch 4529: train loss: 3.441631122313993e-07, val loss: 0.11474084854125977\n",
      "Epoch 4530: train loss: 9.76894156679009e-08, val loss: 0.11463749408721924\n",
      "Epoch 4531: train loss: 3.7446710621225066e-08, val loss: 0.11460931599140167\n",
      "Epoch 4532: train loss: 1.6474419339829183e-07, val loss: 0.11463067680597305\n",
      "Epoch 4533: train loss: 3.420926475428132e-07, val loss: 0.11454307287931442\n",
      "Epoch 4534: train loss: 3.8319478790072026e-07, val loss: 0.11454831808805466\n",
      "Epoch 4535: train loss: 2.9507413046303554e-07, val loss: 0.11447098106145859\n",
      "Epoch 4536: train loss: 1.7198510704474756e-07, val loss: 0.11447366327047348\n",
      "Epoch 4537: train loss: 5.966829519366001e-08, val loss: 0.11439670622348785\n",
      "Epoch 4538: train loss: 1.567498308929771e-08, val loss: 0.11434650421142578\n",
      "Epoch 4539: train loss: 3.672963089229597e-08, val loss: 0.11436479538679123\n",
      "Epoch 4540: train loss: 1.0449303289306044e-07, val loss: 0.11422461271286011\n",
      "Epoch 4541: train loss: 1.9667601236506016e-07, val loss: 0.11423814296722412\n",
      "Epoch 4542: train loss: 3.190220354554185e-07, val loss: 0.11417052894830704\n",
      "Epoch 4543: train loss: 5.082467282591097e-07, val loss: 0.11418557167053223\n",
      "Epoch 4544: train loss: 8.299753062601667e-07, val loss: 0.11405221372842789\n",
      "Epoch 4545: train loss: 1.4453793255597702e-06, val loss: 0.11414491385221481\n",
      "Epoch 4546: train loss: 2.7335211143508786e-06, val loss: 0.11396117508411407\n",
      "Epoch 4547: train loss: 5.610048447124427e-06, val loss: 0.11419478803873062\n",
      "Epoch 4548: train loss: 1.2307601537031587e-05, val loss: 0.11367890983819962\n",
      "Epoch 4549: train loss: 2.9825658202753402e-05, val loss: 0.11420702934265137\n",
      "Epoch 4550: train loss: 7.491387077607214e-05, val loss: 0.11368336528539658\n",
      "Epoch 4551: train loss: 0.00016530582797713578, val loss: 0.1138155460357666\n",
      "Epoch 4552: train loss: 0.0002716741291806102, val loss: 0.11358579248189926\n",
      "Epoch 4553: train loss: 0.00026064779376611114, val loss: 0.11421322822570801\n",
      "Epoch 4554: train loss: 7.570119487354532e-05, val loss: 0.11407627910375595\n",
      "Epoch 4555: train loss: 2.430705717415549e-05, val loss: 0.11487791687250137\n",
      "Epoch 4556: train loss: 0.0001282886223634705, val loss: 0.11378579586744308\n",
      "Epoch 4557: train loss: 7.75192747823894e-05, val loss: 0.11397748440504074\n",
      "Epoch 4558: train loss: 2.488016616553068e-05, val loss: 0.11515569686889648\n",
      "Epoch 4559: train loss: 7.026275852695107e-05, val loss: 0.11323436349630356\n",
      "Epoch 4560: train loss: 3.9203841879498214e-05, val loss: 0.11401467770338058\n",
      "Epoch 4561: train loss: 3.965499490732327e-05, val loss: 0.11488772928714752\n",
      "Epoch 4562: train loss: 3.878579445881769e-05, val loss: 0.11313294619321823\n",
      "Epoch 4563: train loss: 2.267048148496542e-05, val loss: 0.1133369728922844\n",
      "Epoch 4564: train loss: 3.473296601441689e-05, val loss: 0.11528189480304718\n",
      "Epoch 4565: train loss: 2.4893510271795094e-05, val loss: 0.11395406723022461\n",
      "Epoch 4566: train loss: 1.7401869627065025e-05, val loss: 0.11326102167367935\n",
      "Epoch 4567: train loss: 2.2520285710925236e-05, val loss: 0.11537715047597885\n",
      "Epoch 4568: train loss: 2.493590181984473e-05, val loss: 0.11422067880630493\n",
      "Epoch 4569: train loss: 7.893459951446857e-06, val loss: 0.11381576210260391\n",
      "Epoch 4570: train loss: 1.75957156898221e-05, val loss: 0.11504755169153214\n",
      "Epoch 4571: train loss: 1.7981183191295713e-05, val loss: 0.11441774666309357\n",
      "Epoch 4572: train loss: 5.617242550215451e-06, val loss: 0.11383944004774094\n",
      "Epoch 4573: train loss: 1.3978738024889026e-05, val loss: 0.11470760405063629\n",
      "Epoch 4574: train loss: 9.257790225092322e-06, val loss: 0.11466949433088303\n",
      "Epoch 4575: train loss: 9.597239113645628e-06, val loss: 0.11397723108530045\n",
      "Epoch 4576: train loss: 8.232952495745849e-06, val loss: 0.1146261915564537\n",
      "Epoch 4577: train loss: 6.234497050172649e-06, val loss: 0.11448580026626587\n",
      "Epoch 4578: train loss: 8.422665814578068e-06, val loss: 0.11431842297315598\n",
      "Epoch 4579: train loss: 6.585677056136774e-06, val loss: 0.11401864141225815\n",
      "Epoch 4580: train loss: 4.2208266677334905e-06, val loss: 0.11448587477207184\n",
      "Epoch 4581: train loss: 4.927167083224049e-06, val loss: 0.11426331102848053\n",
      "Epoch 4582: train loss: 6.3916263570718e-06, val loss: 0.11412530392408371\n",
      "Epoch 4583: train loss: 3.0455580599664245e-06, val loss: 0.114267997443676\n",
      "Epoch 4584: train loss: 3.0520391192112584e-06, val loss: 0.11426091194152832\n",
      "Epoch 4585: train loss: 4.498841008171439e-06, val loss: 0.11390085518360138\n",
      "Epoch 4586: train loss: 2.8788726922357455e-06, val loss: 0.11427344381809235\n",
      "Epoch 4587: train loss: 2.9123043532308657e-06, val loss: 0.11376027017831802\n",
      "Epoch 4588: train loss: 3.1779786695551593e-06, val loss: 0.11436408758163452\n",
      "Epoch 4589: train loss: 4.056013494846411e-06, val loss: 0.11369743198156357\n",
      "Epoch 4590: train loss: 1.3509371456166264e-05, val loss: 0.11488788574934006\n",
      "Epoch 4591: train loss: 4.665356391342357e-05, val loss: 0.11227314919233322\n",
      "Epoch 4592: train loss: 0.00010574331827228889, val loss: 0.11641436815261841\n",
      "Epoch 4593: train loss: 0.00011675004498101771, val loss: 0.1122046709060669\n",
      "Epoch 4594: train loss: 5.6448559917043895e-05, val loss: 0.11430877447128296\n",
      "Epoch 4595: train loss: 1.0843858945008833e-05, val loss: 0.11494322121143341\n",
      "Epoch 4596: train loss: 2.362403756706044e-05, val loss: 0.11194801330566406\n",
      "Epoch 4597: train loss: 5.508613321580924e-05, val loss: 0.11472451686859131\n",
      "Epoch 4598: train loss: 6.0245645727263764e-05, val loss: 0.11251711845397949\n",
      "Epoch 4599: train loss: 3.415211904211901e-05, val loss: 0.11367485672235489\n",
      "Epoch 4600: train loss: 8.076238373178057e-06, val loss: 0.11401515454053879\n",
      "Epoch 4601: train loss: 1.3992674212204292e-05, val loss: 0.1124412789940834\n",
      "Epoch 4602: train loss: 3.619885683292523e-05, val loss: 0.11467763036489487\n",
      "Epoch 4603: train loss: 3.483506225165911e-05, val loss: 0.11282757669687271\n",
      "Epoch 4604: train loss: 1.1897227523149922e-05, val loss: 0.11363048851490021\n",
      "Epoch 4605: train loss: 2.6770942440634826e-06, val loss: 0.11440044641494751\n",
      "Epoch 4606: train loss: 1.5550149328191765e-05, val loss: 0.11258721351623535\n",
      "Epoch 4607: train loss: 2.495374428690411e-05, val loss: 0.11432327330112457\n",
      "Epoch 4608: train loss: 1.5697671187808737e-05, val loss: 0.11361757665872574\n",
      "Epoch 4609: train loss: 2.579299462013296e-06, val loss: 0.11402616649866104\n",
      "Epoch 4610: train loss: 4.36132950198953e-06, val loss: 0.11455851048231125\n",
      "Epoch 4611: train loss: 1.3743700947088655e-05, val loss: 0.1133793368935585\n",
      "Epoch 4612: train loss: 1.4290203580458183e-05, val loss: 0.11537758260965347\n",
      "Epoch 4613: train loss: 6.397743163688574e-06, val loss: 0.11434447765350342\n",
      "Epoch 4614: train loss: 1.3131497098584077e-06, val loss: 0.11433304846286774\n",
      "Epoch 4615: train loss: 4.290204287826782e-06, val loss: 0.11568789929151535\n",
      "Epoch 4616: train loss: 9.265905646316241e-06, val loss: 0.11412495374679565\n",
      "Epoch 4617: train loss: 8.65517995407572e-06, val loss: 0.11548551172018051\n",
      "Epoch 4618: train loss: 3.3543335575814126e-06, val loss: 0.11528704315423965\n",
      "Epoch 4619: train loss: 5.282777237880509e-07, val loss: 0.11489558219909668\n",
      "Epoch 4620: train loss: 2.846125198630034e-06, val loss: 0.11572524160146713\n",
      "Epoch 4621: train loss: 6.015003691572929e-06, val loss: 0.1147424504160881\n",
      "Epoch 4622: train loss: 5.71839336771518e-06, val loss: 0.11595065891742706\n",
      "Epoch 4623: train loss: 2.6869804514717543e-06, val loss: 0.1153673306107521\n",
      "Epoch 4624: train loss: 5.09022925143654e-07, val loss: 0.11524094641208649\n",
      "Epoch 4625: train loss: 1.0280978131049778e-06, val loss: 0.11600112915039062\n",
      "Epoch 4626: train loss: 2.965325620607473e-06, val loss: 0.11502957344055176\n",
      "Epoch 4627: train loss: 3.999047294200864e-06, val loss: 0.11601898819208145\n",
      "Epoch 4628: train loss: 3.222733766961028e-06, val loss: 0.11529409885406494\n",
      "Epoch 4629: train loss: 1.603551254447666e-06, val loss: 0.11563348770141602\n",
      "Epoch 4630: train loss: 4.780625886269263e-07, val loss: 0.1157161220908165\n",
      "Epoch 4631: train loss: 4.5846257989978767e-07, val loss: 0.11522650718688965\n",
      "Epoch 4632: train loss: 1.237462242897891e-06, val loss: 0.11603417247533798\n",
      "Epoch 4633: train loss: 2.041700327026774e-06, val loss: 0.1151551827788353\n",
      "Epoch 4634: train loss: 2.472844244039152e-06, val loss: 0.11579561233520508\n",
      "Epoch 4635: train loss: 2.4384223706874764e-06, val loss: 0.11536657065153122\n",
      "Epoch 4636: train loss: 2.0577529085130664e-06, val loss: 0.11565299332141876\n",
      "Epoch 4637: train loss: 1.4904452427799697e-06, val loss: 0.11551590263843536\n",
      "Epoch 4638: train loss: 8.949245398071071e-07, val loss: 0.11549367755651474\n",
      "Epoch 4639: train loss: 4.7343266373900406e-07, val loss: 0.11550121754407883\n",
      "Epoch 4640: train loss: 4.0336126971851627e-07, val loss: 0.1155274286866188\n",
      "Epoch 4641: train loss: 7.210395551737747e-07, val loss: 0.11541435867547989\n",
      "Epoch 4642: train loss: 1.5549471754638944e-06, val loss: 0.11550676822662354\n",
      "Epoch 4643: train loss: 3.2187281249207444e-06, val loss: 0.11542705446481705\n",
      "Epoch 4644: train loss: 5.238776793703437e-06, val loss: 0.11531370133161545\n",
      "Epoch 4645: train loss: 6.751472938049119e-06, val loss: 0.11577274650335312\n",
      "Epoch 4646: train loss: 8.220265954150818e-06, val loss: 0.11480815708637238\n",
      "Epoch 4647: train loss: 1.1164755960635375e-05, val loss: 0.11656438559293747\n",
      "Epoch 4648: train loss: 1.7403704987373203e-05, val loss: 0.11374787241220474\n",
      "Epoch 4649: train loss: 2.9990780603839085e-05, val loss: 0.1175641193985939\n",
      "Epoch 4650: train loss: 5.468536983244121e-05, val loss: 0.11254509538412094\n",
      "Epoch 4651: train loss: 0.00010751332592917606, val loss: 0.11953747272491455\n",
      "Epoch 4652: train loss: 0.00020556611707434058, val loss: 0.11220281571149826\n",
      "Epoch 4653: train loss: 0.0003057416470255703, val loss: 0.12093508243560791\n",
      "Epoch 4654: train loss: 0.0003378278634045273, val loss: 0.11357253044843674\n",
      "Epoch 4655: train loss: 0.0003297278890386224, val loss: 0.12114223092794418\n",
      "Epoch 4656: train loss: 0.0003080462629441172, val loss: 0.11650824546813965\n",
      "Epoch 4657: train loss: 0.00027798826340585947, val loss: 0.12034685909748077\n",
      "Epoch 4658: train loss: 0.0002030637115240097, val loss: 0.11759071797132492\n",
      "Epoch 4659: train loss: 0.00011511707998579368, val loss: 0.1168280616402626\n",
      "Epoch 4660: train loss: 7.597829244332388e-05, val loss: 0.11851370334625244\n",
      "Epoch 4661: train loss: 8.193967369152233e-05, val loss: 0.11240307241678238\n",
      "Epoch 4662: train loss: 8.516509842593223e-05, val loss: 0.11145242303609848\n",
      "Epoch 4663: train loss: 8.365818212041631e-05, val loss: 0.10882110893726349\n",
      "Epoch 4664: train loss: 7.791112875565886e-05, val loss: 0.10590534657239914\n",
      "Epoch 4665: train loss: 5.10769386892207e-05, val loss: 0.10667987912893295\n",
      "Epoch 4666: train loss: 2.7696565666701645e-05, val loss: 0.10569945722818375\n",
      "Epoch 4667: train loss: 4.005233131465502e-05, val loss: 0.10506676882505417\n",
      "Epoch 4668: train loss: 5.941220297245309e-05, val loss: 0.10600461810827255\n",
      "Epoch 4669: train loss: 4.173379056737758e-05, val loss: 0.10498683899641037\n",
      "Epoch 4670: train loss: 1.1686326615745202e-05, val loss: 0.1044061928987503\n",
      "Epoch 4671: train loss: 2.012058212130796e-05, val loss: 0.1057262197136879\n",
      "Epoch 4672: train loss: 3.996530722361058e-05, val loss: 0.10545410215854645\n",
      "Epoch 4673: train loss: 2.6372297725174576e-05, val loss: 0.10491533577442169\n",
      "Epoch 4674: train loss: 9.788555871637072e-06, val loss: 0.10689234733581543\n",
      "Epoch 4675: train loss: 1.4946574083296582e-05, val loss: 0.10610008239746094\n",
      "Epoch 4676: train loss: 1.9811061065411195e-05, val loss: 0.1064610630273819\n",
      "Epoch 4677: train loss: 1.730761505314149e-05, val loss: 0.1084892749786377\n",
      "Epoch 4678: train loss: 1.2849328413722105e-05, val loss: 0.10690634697675705\n",
      "Epoch 4679: train loss: 7.549590009148233e-06, val loss: 0.10798607021570206\n",
      "Epoch 4680: train loss: 9.141991540673189e-06, val loss: 0.10906326025724411\n",
      "Epoch 4681: train loss: 1.3818506886309478e-05, val loss: 0.10762979835271835\n",
      "Epoch 4682: train loss: 1.0471329915162642e-05, val loss: 0.10906130075454712\n",
      "Epoch 4683: train loss: 4.477592938201269e-06, val loss: 0.10964806377887726\n",
      "Epoch 4684: train loss: 5.344529654394137e-06, val loss: 0.10826456546783447\n",
      "Epoch 4685: train loss: 8.787714250502177e-06, val loss: 0.11009383201599121\n",
      "Epoch 4686: train loss: 7.843407729524188e-06, val loss: 0.10983739048242569\n",
      "Epoch 4687: train loss: 4.2952451622113585e-06, val loss: 0.10890312492847443\n",
      "Epoch 4688: train loss: 3.5451710118650226e-06, val loss: 0.11063682287931442\n",
      "Epoch 4689: train loss: 4.769221504830057e-06, val loss: 0.11012732982635498\n",
      "Epoch 4690: train loss: 4.707450443675043e-06, val loss: 0.11014985293149948\n",
      "Epoch 4691: train loss: 3.934045253117802e-06, val loss: 0.11082450300455093\n",
      "Epoch 4692: train loss: 3.6356143482407788e-06, val loss: 0.1102200523018837\n",
      "Epoch 4693: train loss: 2.935328438979923e-06, val loss: 0.11111166328191757\n",
      "Epoch 4694: train loss: 2.074658823403297e-06, val loss: 0.11089377850294113\n",
      "Epoch 4695: train loss: 2.3647833131690277e-06, val loss: 0.11090471595525742\n",
      "Epoch 4696: train loss: 3.2237278446700657e-06, val loss: 0.1115729808807373\n",
      "Epoch 4697: train loss: 2.955935769932694e-06, val loss: 0.11095955222845078\n",
      "Epoch 4698: train loss: 1.6649087228870485e-06, val loss: 0.11171771585941315\n",
      "Epoch 4699: train loss: 1.0024690482168808e-06, val loss: 0.11157858371734619\n",
      "Epoch 4700: train loss: 1.4702897033203044e-06, val loss: 0.11150188744068146\n",
      "Epoch 4701: train loss: 2.0730190044560004e-06, val loss: 0.11221498250961304\n",
      "Epoch 4702: train loss: 2.066277602352784e-06, val loss: 0.11152926832437515\n",
      "Epoch 4703: train loss: 1.7287210312133539e-06, val loss: 0.11204787343740463\n",
      "Epoch 4704: train loss: 1.3295242524691275e-06, val loss: 0.1119738221168518\n",
      "Epoch 4705: train loss: 9.239669225280522e-07, val loss: 0.111686110496521\n",
      "Epoch 4706: train loss: 6.884013146191137e-07, val loss: 0.11224939674139023\n",
      "Epoch 4707: train loss: 6.964034469092439e-07, val loss: 0.11176281422376633\n",
      "Epoch 4708: train loss: 8.347617495019222e-07, val loss: 0.11197948455810547\n",
      "Epoch 4709: train loss: 1.0514008863538038e-06, val loss: 0.11211597919464111\n",
      "Epoch 4710: train loss: 1.3638884865940781e-06, val loss: 0.11182814091444016\n",
      "Epoch 4711: train loss: 1.5438004083989654e-06, val loss: 0.11227818578481674\n",
      "Epoch 4712: train loss: 1.4002825992065482e-06, val loss: 0.1118970662355423\n",
      "Epoch 4713: train loss: 1.2601336720763356e-06, val loss: 0.11218344420194626\n",
      "Epoch 4714: train loss: 1.5838628542041988e-06, val loss: 0.11213960498571396\n",
      "Epoch 4715: train loss: 2.536122110541328e-06, val loss: 0.11220254749059677\n",
      "Epoch 4716: train loss: 4.467367034521885e-06, val loss: 0.11209744215011597\n",
      "Epoch 4717: train loss: 8.59983811096754e-06, val loss: 0.11255643516778946\n",
      "Epoch 4718: train loss: 1.8131404431187548e-05, val loss: 0.1116836816072464\n",
      "Epoch 4719: train loss: 4.0964194340631366e-05, val loss: 0.11327259987592697\n",
      "Epoch 4720: train loss: 9.366094309370965e-05, val loss: 0.11172737926244736\n",
      "Epoch 4721: train loss: 0.0001998318184632808, val loss: 0.11351992934942245\n",
      "Epoch 4722: train loss: 0.00038391875568777323, val loss: 0.11076881736516953\n",
      "Epoch 4723: train loss: 0.0006201147334650159, val loss: 0.11308858543634415\n",
      "Epoch 4724: train loss: 0.0008355531026609242, val loss: 0.1090712770819664\n",
      "Epoch 4725: train loss: 0.000877853250131011, val loss: 0.11466598510742188\n",
      "Epoch 4726: train loss: 0.0006685513653792441, val loss: 0.10812313854694366\n",
      "Epoch 4727: train loss: 0.0004289971257094294, val loss: 0.11036667972803116\n",
      "Epoch 4728: train loss: 0.00022064245422370732, val loss: 0.1140686422586441\n",
      "Epoch 4729: train loss: 0.00012171746493550017, val loss: 0.10655061155557632\n",
      "Epoch 4730: train loss: 0.0002390208828728646, val loss: 0.11079908907413483\n",
      "Epoch 4731: train loss: 0.00023105167201720178, val loss: 0.10957486927509308\n",
      "Epoch 4732: train loss: 0.00011394297325750813, val loss: 0.10567599534988403\n",
      "Epoch 4733: train loss: 9.10555972950533e-05, val loss: 0.10747511684894562\n",
      "Epoch 4734: train loss: 0.00013286287139635533, val loss: 0.10592687129974365\n",
      "Epoch 4735: train loss: 0.00012414934462867677, val loss: 0.1058928519487381\n",
      "Epoch 4736: train loss: 3.748292147065513e-05, val loss: 0.1054362878203392\n",
      "Epoch 4737: train loss: 8.96735000424087e-05, val loss: 0.10532479733228683\n",
      "Epoch 4738: train loss: 8.162869926309213e-05, val loss: 0.10686912387609482\n",
      "Epoch 4739: train loss: 3.504402411635965e-05, val loss: 0.10507907718420029\n",
      "Epoch 4740: train loss: 4.938416896038689e-05, val loss: 0.10422316938638687\n",
      "Epoch 4741: train loss: 6.684788240818307e-05, val loss: 0.10577161610126495\n",
      "Epoch 4742: train loss: 1.6292440705001354e-05, val loss: 0.10425414890050888\n",
      "Epoch 4743: train loss: 4.2695442971307784e-05, val loss: 0.10366975516080856\n",
      "Epoch 4744: train loss: 4.062154403072782e-05, val loss: 0.10525818169116974\n",
      "Epoch 4745: train loss: 1.5360652469098568e-05, val loss: 0.10425760596990585\n",
      "Epoch 4746: train loss: 2.8634925911319442e-05, val loss: 0.10312845557928085\n",
      "Epoch 4747: train loss: 3.0444156436715275e-05, val loss: 0.1047423854470253\n",
      "Epoch 4748: train loss: 8.101636922219768e-06, val loss: 0.10484709590673447\n",
      "Epoch 4749: train loss: 2.5325387468910776e-05, val loss: 0.10362379997968674\n",
      "Epoch 4750: train loss: 1.7093967471737415e-05, val loss: 0.10443567484617233\n",
      "Epoch 4751: train loss: 9.350545951747335e-06, val loss: 0.10474944114685059\n",
      "Epoch 4752: train loss: 1.7086440493585542e-05, val loss: 0.10340441763401031\n",
      "Epoch 4753: train loss: 1.2456906915758736e-05, val loss: 0.10419931262731552\n",
      "Epoch 4754: train loss: 7.023225407465361e-06, val loss: 0.10530024766921997\n",
      "Epoch 4755: train loss: 1.2553690794447903e-05, val loss: 0.10395412892103195\n",
      "Epoch 4756: train loss: 9.320319804828614e-06, val loss: 0.10415954887866974\n",
      "Epoch 4757: train loss: 3.97599660573178e-06, val loss: 0.10524012893438339\n",
      "Epoch 4758: train loss: 1.136343962571118e-05, val loss: 0.10457167774438858\n",
      "Epoch 4759: train loss: 4.488297236093786e-06, val loss: 0.10455603897571564\n",
      "Epoch 4760: train loss: 4.667561370297335e-06, val loss: 0.10500786453485489\n",
      "Epoch 4761: train loss: 7.594391718157567e-06, val loss: 0.1043635830283165\n",
      "Epoch 4762: train loss: 3.367075350979576e-06, val loss: 0.1049867793917656\n",
      "Epoch 4763: train loss: 3.7402878660941496e-06, val loss: 0.10557859390974045\n",
      "Epoch 4764: train loss: 5.123420578456717e-06, val loss: 0.10508232563734055\n",
      "Epoch 4765: train loss: 2.2998547137831338e-06, val loss: 0.10529058426618576\n",
      "Epoch 4766: train loss: 2.856059154510149e-06, val loss: 0.10576756298542023\n",
      "Epoch 4767: train loss: 3.4134411635022843e-06, val loss: 0.10566645115613937\n",
      "Epoch 4768: train loss: 1.4633378668804653e-06, val loss: 0.10575969517230988\n",
      "Epoch 4769: train loss: 2.3782754396961536e-06, val loss: 0.1057976707816124\n",
      "Epoch 4770: train loss: 2.3345369299931917e-06, val loss: 0.10578665882349014\n",
      "Epoch 4771: train loss: 1.0426329026813619e-06, val loss: 0.1061411127448082\n",
      "Epoch 4772: train loss: 1.933608018589439e-06, val loss: 0.10621125996112823\n",
      "Epoch 4773: train loss: 1.5228123402266647e-06, val loss: 0.10606469959020615\n",
      "Epoch 4774: train loss: 9.747285503181047e-07, val loss: 0.10632135719060898\n",
      "Epoch 4775: train loss: 1.2666225757129723e-06, val loss: 0.10645010322332382\n",
      "Epoch 4776: train loss: 1.2621694622794166e-06, val loss: 0.10634946078062057\n",
      "Epoch 4777: train loss: 7.373137123067863e-07, val loss: 0.10659513622522354\n",
      "Epoch 4778: train loss: 9.009737027554365e-07, val loss: 0.10666536539793015\n",
      "Epoch 4779: train loss: 9.353678933621268e-07, val loss: 0.10662984848022461\n",
      "Epoch 4780: train loss: 6.154110678835423e-07, val loss: 0.1068928986787796\n",
      "Epoch 4781: train loss: 6.44156727958034e-07, val loss: 0.10693150758743286\n",
      "Epoch 4782: train loss: 6.656364917034807e-07, val loss: 0.10700004547834396\n",
      "Epoch 4783: train loss: 5.240078735369025e-07, val loss: 0.10717236250638962\n",
      "Epoch 4784: train loss: 5.271209602142335e-07, val loss: 0.10727399587631226\n",
      "Epoch 4785: train loss: 5.527851385522808e-07, val loss: 0.10745229572057724\n",
      "Epoch 4786: train loss: 3.997424187218712e-07, val loss: 0.10733002424240112\n",
      "Epoch 4787: train loss: 3.520213454066834e-07, val loss: 0.1076693907380104\n",
      "Epoch 4788: train loss: 4.267825772785727e-07, val loss: 0.10790043324232101\n",
      "Epoch 4789: train loss: 3.4477707799851487e-07, val loss: 0.1076168343424797\n",
      "Epoch 4790: train loss: 3.3786469089136517e-07, val loss: 0.10778322070837021\n",
      "Epoch 4791: train loss: 3.910022883246711e-07, val loss: 0.10821785032749176\n",
      "Epoch 4792: train loss: 4.057051512518228e-07, val loss: 0.10782141983509064\n",
      "Epoch 4793: train loss: 2.540778893944662e-07, val loss: 0.10825927555561066\n",
      "Epoch 4794: train loss: 2.025341672151626e-07, val loss: 0.10824786871671677\n",
      "Epoch 4795: train loss: 2.9372409926509135e-07, val loss: 0.10823912918567657\n",
      "Epoch 4796: train loss: 2.5811004888964817e-07, val loss: 0.10851172357797623\n",
      "Epoch 4797: train loss: 1.5580042145302286e-07, val loss: 0.10840556770563126\n",
      "Epoch 4798: train loss: 1.2773456603554223e-07, val loss: 0.10867934674024582\n",
      "Epoch 4799: train loss: 1.676393424077105e-07, val loss: 0.10866538435220718\n",
      "Epoch 4800: train loss: 2.1955479212465434e-07, val loss: 0.10881604254245758\n",
      "Epoch 4801: train loss: 2.4133714759955183e-07, val loss: 0.10880173742771149\n",
      "Epoch 4802: train loss: 2.3833074180856784e-07, val loss: 0.10894354432821274\n",
      "Epoch 4803: train loss: 2.393126408151147e-07, val loss: 0.10896182060241699\n",
      "Epoch 4804: train loss: 2.3715935526524845e-07, val loss: 0.10917869955301285\n",
      "Epoch 4805: train loss: 2.823725822054257e-07, val loss: 0.10906358063220978\n",
      "Epoch 4806: train loss: 5.173873205421842e-07, val loss: 0.10957039892673492\n",
      "Epoch 4807: train loss: 7.946988489493378e-07, val loss: 0.1089358851313591\n",
      "Epoch 4808: train loss: 1.3907399534218712e-06, val loss: 0.11004719883203506\n",
      "Epoch 4809: train loss: 2.4657267658767523e-06, val loss: 0.1088767871260643\n",
      "Epoch 4810: train loss: 4.611069471138762e-06, val loss: 0.1105075255036354\n",
      "Epoch 4811: train loss: 1.0671952622942626e-05, val loss: 0.10816793888807297\n",
      "Epoch 4812: train loss: 2.7296917323837988e-05, val loss: 0.11205897480249405\n",
      "Epoch 4813: train loss: 7.0092202804517e-05, val loss: 0.10645604133605957\n",
      "Epoch 4814: train loss: 0.00017838552594184875, val loss: 0.11557181179523468\n",
      "Epoch 4815: train loss: 0.00044682895531877875, val loss: 0.1011301651597023\n",
      "Epoch 4816: train loss: 0.0009608694235794246, val loss: 0.11676521599292755\n",
      "Epoch 4817: train loss: 0.0013992502354085445, val loss: 0.09513503313064575\n",
      "Epoch 4818: train loss: 0.0012704082764685154, val loss: 0.11676770448684692\n",
      "Epoch 4819: train loss: 0.000706682272721082, val loss: 0.10184112936258316\n",
      "Epoch 4820: train loss: 0.0005285548977553844, val loss: 0.10367083549499512\n",
      "Epoch 4821: train loss: 0.00039399482193402946, val loss: 0.1106850877404213\n",
      "Epoch 4822: train loss: 0.00024192828277591616, val loss: 0.1045251116156578\n",
      "Epoch 4823: train loss: 0.0003991892153862864, val loss: 0.10894651710987091\n",
      "Epoch 4824: train loss: 0.00021443780860863626, val loss: 0.109296515583992\n",
      "Epoch 4825: train loss: 0.00016676625818945467, val loss: 0.10453250259160995\n",
      "Epoch 4826: train loss: 0.00022029754472896457, val loss: 0.1083875223994255\n",
      "Epoch 4827: train loss: 0.00015005776367615908, val loss: 0.10830175131559372\n",
      "Epoch 4828: train loss: 0.00010537818161537871, val loss: 0.10320599377155304\n",
      "Epoch 4829: train loss: 0.00015329319285228848, val loss: 0.10486173629760742\n",
      "Epoch 4830: train loss: 7.903809455456212e-05, val loss: 0.10660745203495026\n",
      "Epoch 4831: train loss: 8.84294931893237e-05, val loss: 0.10254845768213272\n",
      "Epoch 4832: train loss: 0.00010052693687612191, val loss: 0.10067629814147949\n",
      "Epoch 4833: train loss: 4.129306762479246e-05, val loss: 0.10367155075073242\n",
      "Epoch 4834: train loss: 8.90796582098119e-05, val loss: 0.10339517891407013\n",
      "Epoch 4835: train loss: 4.213033753330819e-05, val loss: 0.10052547603845596\n",
      "Epoch 4836: train loss: 5.622724711429328e-05, val loss: 0.1007513776421547\n",
      "Epoch 4837: train loss: 4.577085201162845e-05, val loss: 0.1018107533454895\n",
      "Epoch 4838: train loss: 3.7109519325895235e-05, val loss: 0.10077691078186035\n",
      "Epoch 4839: train loss: 4.133554466534406e-05, val loss: 0.10005620867013931\n",
      "Epoch 4840: train loss: 2.8027659936924465e-05, val loss: 0.10065487772226334\n",
      "Epoch 4841: train loss: 3.393498627701774e-05, val loss: 0.10037479549646378\n",
      "Epoch 4842: train loss: 2.108180160576012e-05, val loss: 0.09983322769403458\n",
      "Epoch 4843: train loss: 2.8018734155921265e-05, val loss: 0.09996269643306732\n",
      "Epoch 4844: train loss: 1.7728852981235832e-05, val loss: 0.09977566450834274\n",
      "Epoch 4845: train loss: 2.0398292690515518e-05, val loss: 0.09923096746206284\n",
      "Epoch 4846: train loss: 1.778139267116785e-05, val loss: 0.09928024560213089\n",
      "Epoch 4847: train loss: 1.2374252946756314e-05, val loss: 0.09967979043722153\n",
      "Epoch 4848: train loss: 1.7572090655448847e-05, val loss: 0.09959086030721664\n",
      "Epoch 4849: train loss: 8.97969766811002e-06, val loss: 0.09920413792133331\n",
      "Epoch 4850: train loss: 1.2873967534687836e-05, val loss: 0.09913226217031479\n",
      "Epoch 4851: train loss: 1.0376422324043233e-05, val loss: 0.09940116852521896\n",
      "Epoch 4852: train loss: 7.156697847676696e-06, val loss: 0.09952893108129501\n",
      "Epoch 4853: train loss: 1.0999474397976883e-05, val loss: 0.09913088381290436\n",
      "Epoch 4854: train loss: 5.518736543308478e-06, val loss: 0.09870035946369171\n",
      "Epoch 4855: train loss: 7.109446869435487e-06, val loss: 0.09902947396039963\n",
      "Epoch 4856: train loss: 7.767776878608856e-06, val loss: 0.09936392307281494\n",
      "Epoch 4857: train loss: 3.3160090424644295e-06, val loss: 0.09877127408981323\n",
      "Epoch 4858: train loss: 6.964688964217203e-06, val loss: 0.09827067703008652\n",
      "Epoch 4859: train loss: 3.4536772091087187e-06, val loss: 0.09882433712482452\n",
      "Epoch 4860: train loss: 3.891328560712282e-06, val loss: 0.09906070679426193\n",
      "Epoch 4861: train loss: 4.2862452573899645e-06, val loss: 0.09841356426477432\n",
      "Epoch 4862: train loss: 2.261697090943926e-06, val loss: 0.09834685176610947\n",
      "Epoch 4863: train loss: 3.527857188601047e-06, val loss: 0.09897112846374512\n",
      "Epoch 4864: train loss: 2.3279633296624525e-06, val loss: 0.09890937805175781\n",
      "Epoch 4865: train loss: 2.3415636860590894e-06, val loss: 0.09849021583795547\n",
      "Epoch 4866: train loss: 2.173997245336068e-06, val loss: 0.09873710572719574\n",
      "Epoch 4867: train loss: 1.962003125299816e-06, val loss: 0.0989903137087822\n",
      "Epoch 4868: train loss: 1.5779602335896925e-06, val loss: 0.09875401109457016\n",
      "Epoch 4869: train loss: 1.662584850237181e-06, val loss: 0.09876608848571777\n",
      "Epoch 4870: train loss: 1.413915242665098e-06, val loss: 0.09893692284822464\n",
      "Epoch 4871: train loss: 1.2776660014424124e-06, val loss: 0.09874426573514938\n",
      "Epoch 4872: train loss: 1.0513707593418076e-06, val loss: 0.09876792877912521\n",
      "Epoch 4873: train loss: 1.3090796073811362e-06, val loss: 0.09907423704862595\n",
      "Epoch 4874: train loss: 6.041309461579658e-07, val loss: 0.0989561676979065\n",
      "Epoch 4875: train loss: 1.1232458518861677e-06, val loss: 0.0988122969865799\n",
      "Epoch 4876: train loss: 7.348938879658817e-07, val loss: 0.09904773533344269\n",
      "Epoch 4877: train loss: 5.737078367928916e-07, val loss: 0.09900444000959396\n",
      "Epoch 4878: train loss: 8.154998454301676e-07, val loss: 0.09882523864507675\n",
      "Epoch 4879: train loss: 4.794412689079763e-07, val loss: 0.09914886951446533\n",
      "Epoch 4880: train loss: 5.193653578317026e-07, val loss: 0.09927433729171753\n",
      "Epoch 4881: train loss: 5.430335363598715e-07, val loss: 0.09890135377645493\n",
      "Epoch 4882: train loss: 4.1295717778666585e-07, val loss: 0.09893578290939331\n",
      "Epoch 4883: train loss: 3.796835414959787e-07, val loss: 0.09926914423704147\n",
      "Epoch 4884: train loss: 3.7797315144416643e-07, val loss: 0.09919392317533493\n",
      "Epoch 4885: train loss: 3.312066780836176e-07, val loss: 0.09901537746191025\n",
      "Epoch 4886: train loss: 2.976037762891792e-07, val loss: 0.09914419800043106\n",
      "Epoch 4887: train loss: 2.13440955576516e-07, val loss: 0.09927507489919662\n",
      "Epoch 4888: train loss: 3.3574801250324526e-07, val loss: 0.09917213767766953\n",
      "Epoch 4889: train loss: 1.797967996708394e-07, val loss: 0.09907399117946625\n",
      "Epoch 4890: train loss: 1.9679382035064918e-07, val loss: 0.09919847548007965\n",
      "Epoch 4891: train loss: 2.5155060257020523e-07, val loss: 0.09929578751325607\n",
      "Epoch 4892: train loss: 1.9086272118329362e-07, val loss: 0.0990850105881691\n",
      "Epoch 4893: train loss: 1.8701257431530394e-07, val loss: 0.0990765318274498\n",
      "Epoch 4894: train loss: 2.840400554759981e-07, val loss: 0.09922686964273453\n",
      "Epoch 4895: train loss: 4.272847320407891e-07, val loss: 0.09916627407073975\n",
      "Epoch 4896: train loss: 7.611154728692782e-07, val loss: 0.09900685399770737\n",
      "Epoch 4897: train loss: 1.8360295825914363e-06, val loss: 0.09920518845319748\n",
      "Epoch 4898: train loss: 4.345447450759821e-06, val loss: 0.09896307438611984\n",
      "Epoch 4899: train loss: 1.0418832971481606e-05, val loss: 0.09916236251592636\n",
      "Epoch 4900: train loss: 2.4952114472398534e-05, val loss: 0.09883471578359604\n",
      "Epoch 4901: train loss: 6.0966576711507514e-05, val loss: 0.09917646646499634\n",
      "Epoch 4902: train loss: 6.573114660568535e-05, val loss: 0.09859969466924667\n",
      "Epoch 4903: train loss: 1.4303524949355051e-05, val loss: 0.0987553745508194\n",
      "Epoch 4904: train loss: 1.2366262126306538e-05, val loss: 0.09869598597288132\n",
      "Epoch 4905: train loss: 4.385627107694745e-05, val loss: 0.09817496687173843\n",
      "Epoch 4906: train loss: 3.433928941376507e-05, val loss: 0.09870635718107224\n",
      "Epoch 4907: train loss: 7.912104592833202e-06, val loss: 0.09844675660133362\n",
      "Epoch 4908: train loss: 1.4756101336388383e-05, val loss: 0.0979938954114914\n",
      "Epoch 4909: train loss: 2.921609120676294e-05, val loss: 0.09816617518663406\n",
      "Epoch 4910: train loss: 1.8785034626489505e-05, val loss: 0.09781152009963989\n",
      "Epoch 4911: train loss: 6.747745374013903e-06, val loss: 0.09795351326465607\n",
      "Epoch 4912: train loss: 1.2970149327884428e-05, val loss: 0.09778138250112534\n",
      "Epoch 4913: train loss: 1.8067941709887236e-05, val loss: 0.09758055955171585\n",
      "Epoch 4914: train loss: 1.1323914804961532e-05, val loss: 0.09765733778476715\n",
      "Epoch 4915: train loss: 6.251064860407496e-06, val loss: 0.09717811644077301\n",
      "Epoch 4916: train loss: 9.20015008887276e-06, val loss: 0.09775844216346741\n",
      "Epoch 4917: train loss: 1.123378115153173e-05, val loss: 0.09766441583633423\n",
      "Epoch 4918: train loss: 8.439253178949002e-06, val loss: 0.09687571972608566\n",
      "Epoch 4919: train loss: 5.311079803504981e-06, val loss: 0.09797092527151108\n",
      "Epoch 4920: train loss: 5.2496288844849914e-06, val loss: 0.09743273258209229\n",
      "Epoch 4921: train loss: 6.715249128319556e-06, val loss: 0.09705597162246704\n",
      "Epoch 4922: train loss: 6.909613603056641e-06, val loss: 0.09824289381504059\n",
      "Epoch 4923: train loss: 5.102876457385719e-06, val loss: 0.09729024767875671\n",
      "Epoch 4924: train loss: 2.7939529445575317e-06, val loss: 0.09747888892889023\n",
      "Epoch 4925: train loss: 3.0703222364536487e-06, val loss: 0.09795576333999634\n",
      "Epoch 4926: train loss: 5.132349542691372e-06, val loss: 0.09723230451345444\n",
      "Epoch 4927: train loss: 4.982685368304374e-06, val loss: 0.09791582077741623\n",
      "Epoch 4928: train loss: 2.4805067369015887e-06, val loss: 0.09741836041212082\n",
      "Epoch 4929: train loss: 1.3018151321375626e-06, val loss: 0.09744999557733536\n",
      "Epoch 4930: train loss: 2.3582194899063325e-06, val loss: 0.09793253988027573\n",
      "Epoch 4931: train loss: 3.3593280477361986e-06, val loss: 0.09723261743783951\n",
      "Epoch 4932: train loss: 3.1446929824596737e-06, val loss: 0.09784197062253952\n",
      "Epoch 4933: train loss: 2.310261379534495e-06, val loss: 0.09747898578643799\n",
      "Epoch 4934: train loss: 1.5950049601087812e-06, val loss: 0.0975811555981636\n",
      "Epoch 4935: train loss: 1.3770503528576228e-06, val loss: 0.09778019040822983\n",
      "Epoch 4936: train loss: 1.637743025639793e-06, val loss: 0.09756795316934586\n",
      "Epoch 4937: train loss: 1.987112909773714e-06, val loss: 0.09791331738233566\n",
      "Epoch 4938: train loss: 1.9490241811581654e-06, val loss: 0.09753383696079254\n",
      "Epoch 4939: train loss: 1.5791082432770054e-06, val loss: 0.09807775169610977\n",
      "Epoch 4940: train loss: 1.035274635796668e-06, val loss: 0.09762680530548096\n",
      "Epoch 4941: train loss: 6.135766170700663e-07, val loss: 0.09785493463277817\n",
      "Epoch 4942: train loss: 6.481177479145117e-07, val loss: 0.09801921248435974\n",
      "Epoch 4943: train loss: 1.134733906837937e-06, val loss: 0.09766989201307297\n",
      "Epoch 4944: train loss: 1.898272785183508e-06, val loss: 0.09817616641521454\n",
      "Epoch 4945: train loss: 2.691177996894112e-06, val loss: 0.09773540496826172\n",
      "Epoch 4946: train loss: 3.4785878142429283e-06, val loss: 0.09819012135267258\n",
      "Epoch 4947: train loss: 4.46510739493533e-06, val loss: 0.09781938046216965\n",
      "Epoch 4948: train loss: 6.353920525725698e-06, val loss: 0.09821593761444092\n",
      "Epoch 4949: train loss: 1.0592354556138162e-05, val loss: 0.09800783544778824\n",
      "Epoch 4950: train loss: 2.054126889561303e-05, val loss: 0.09804664552211761\n",
      "Epoch 4951: train loss: 4.3970783735858276e-05, val loss: 0.09844239801168442\n",
      "Epoch 4952: train loss: 0.00010260703857056797, val loss: 0.09751998633146286\n",
      "Epoch 4953: train loss: 0.000248772616032511, val loss: 0.09957515448331833\n",
      "Epoch 4954: train loss: 0.0005577853880822659, val loss: 0.0951993390917778\n",
      "Epoch 4955: train loss: 0.0008915603975765407, val loss: 0.10304472595453262\n",
      "Epoch 4956: train loss: 0.0008745582890696824, val loss: 0.09258129447698593\n",
      "Epoch 4957: train loss: 0.0007740027504041791, val loss: 0.10147159546613693\n",
      "Epoch 4958: train loss: 0.00042534488602541387, val loss: 0.10024620592594147\n",
      "Epoch 4959: train loss: 0.00020597163529600948, val loss: 0.0947941318154335\n",
      "Epoch 4960: train loss: 0.0003912195679731667, val loss: 0.0977698266506195\n",
      "Epoch 4961: train loss: 0.0002408467116765678, val loss: 0.10148849338293076\n",
      "Epoch 4962: train loss: 0.00018542894395068288, val loss: 0.09691973030567169\n",
      "Epoch 4963: train loss: 0.00019766624609474093, val loss: 0.09643132239580154\n",
      "Epoch 4964: train loss: 0.00014948246825952083, val loss: 0.10011780261993408\n",
      "Epoch 4965: train loss: 0.00014091770572122186, val loss: 0.10008303076028824\n",
      "Epoch 4966: train loss: 0.00011426475248299539, val loss: 0.09908123314380646\n",
      "Epoch 4967: train loss: 0.00010975005716318265, val loss: 0.10036521404981613\n",
      "Epoch 4968: train loss: 8.528672333341092e-05, val loss: 0.10099595040082932\n",
      "Epoch 4969: train loss: 9.175195737043396e-05, val loss: 0.10016632080078125\n",
      "Epoch 4970: train loss: 5.938836693530902e-05, val loss: 0.09988031536340714\n",
      "Epoch 4971: train loss: 7.628124149050564e-05, val loss: 0.10053546726703644\n",
      "Epoch 4972: train loss: 4.982593600288965e-05, val loss: 0.10061154514551163\n",
      "Epoch 4973: train loss: 5.0086611736333e-05, val loss: 0.09962648153305054\n",
      "Epoch 4974: train loss: 5.0484275561757386e-05, val loss: 0.10027536004781723\n",
      "Epoch 4975: train loss: 3.0954561225371435e-05, val loss: 0.10208733379840851\n",
      "Epoch 4976: train loss: 4.350962262833491e-05, val loss: 0.1014498621225357\n",
      "Epoch 4977: train loss: 2.6304622224415652e-05, val loss: 0.0994633212685585\n",
      "Epoch 4978: train loss: 3.0983668693806976e-05, val loss: 0.0997539609670639\n",
      "Epoch 4979: train loss: 2.4481014406774193e-05, val loss: 0.10156243294477463\n",
      "Epoch 4980: train loss: 2.2717056708643213e-05, val loss: 0.10125572979450226\n",
      "Epoch 4981: train loss: 2.0499350284808315e-05, val loss: 0.09938639402389526\n",
      "Epoch 4982: train loss: 1.8483679014025256e-05, val loss: 0.0994517132639885\n",
      "Epoch 4983: train loss: 1.6037491150200367e-05, val loss: 0.10058863461017609\n",
      "Epoch 4984: train loss: 1.5263520253938623e-05, val loss: 0.10005705803632736\n",
      "Epoch 4985: train loss: 1.3268290786072612e-05, val loss: 0.09922090917825699\n",
      "Epoch 4986: train loss: 1.1363801604602486e-05, val loss: 0.10027261078357697\n",
      "Epoch 4987: train loss: 1.2071207493136171e-05, val loss: 0.10068660229444504\n",
      "Epoch 4988: train loss: 8.049692041822709e-06, val loss: 0.09913931041955948\n",
      "Epoch 4989: train loss: 1.0489534361113328e-05, val loss: 0.09901317209005356\n",
      "Epoch 4990: train loss: 6.090494025556836e-06, val loss: 0.1004805937409401\n",
      "Epoch 4991: train loss: 8.716101547179278e-06, val loss: 0.10027271509170532\n",
      "Epoch 4992: train loss: 4.802953299076762e-06, val loss: 0.09911093860864639\n",
      "Epoch 4993: train loss: 7.185817594290711e-06, val loss: 0.09941841661930084\n",
      "Epoch 4994: train loss: 3.833598839264596e-06, val loss: 0.10017462819814682\n",
      "Epoch 4995: train loss: 6.132648650236661e-06, val loss: 0.09979991614818573\n",
      "Epoch 4996: train loss: 2.872727236535866e-06, val loss: 0.09900861978530884\n",
      "Epoch 4997: train loss: 5.198455710342387e-06, val loss: 0.09908921271562576\n",
      "Epoch 4998: train loss: 2.357538960495731e-06, val loss: 0.09991873800754547\n",
      "Epoch 4999: train loss: 3.915900379070081e-06, val loss: 0.09993147104978561\n",
      "Epoch 5000: train loss: 2.261267809444689e-06, val loss: 0.0989917740225792\n",
      "Epoch 5001: train loss: 2.9375864869507495e-06, val loss: 0.09898481518030167\n",
      "Epoch 5002: train loss: 1.9353797142684925e-06, val loss: 0.09972711652517319\n",
      "Epoch 5003: train loss: 2.2469662326329853e-06, val loss: 0.09967251121997833\n",
      "Epoch 5004: train loss: 1.6643979279251653e-06, val loss: 0.0993225947022438\n",
      "Epoch 5005: train loss: 1.8758275928121293e-06, val loss: 0.09941506385803223\n",
      "Epoch 5006: train loss: 1.3069202395854518e-06, val loss: 0.09925995022058487\n",
      "Epoch 5007: train loss: 1.4685831501992652e-06, val loss: 0.09920134395360947\n",
      "Epoch 5008: train loss: 1.1746827794922865e-06, val loss: 0.09965696930885315\n",
      "Epoch 5009: train loss: 1.2187105085104122e-06, val loss: 0.09953682869672775\n",
      "Epoch 5010: train loss: 8.379960831916833e-07, val loss: 0.0990995392203331\n",
      "Epoch 5011: train loss: 1.040035158439423e-06, val loss: 0.0994425043463707\n",
      "Epoch 5012: train loss: 8.291904691759555e-07, val loss: 0.0996338501572609\n",
      "Epoch 5013: train loss: 6.434771080421342e-07, val loss: 0.09928806871175766\n",
      "Epoch 5014: train loss: 8.297091653730604e-07, val loss: 0.09937788546085358\n",
      "Epoch 5015: train loss: 4.706380991592596e-07, val loss: 0.0994095727801323\n",
      "Epoch 5016: train loss: 6.584630796169222e-07, val loss: 0.09922230988740921\n",
      "Epoch 5017: train loss: 4.6588635882471863e-07, val loss: 0.09950252622365952\n",
      "Epoch 5018: train loss: 4.855113502344466e-07, val loss: 0.09950115531682968\n",
      "Epoch 5019: train loss: 3.8577633176828385e-07, val loss: 0.09919670224189758\n",
      "Epoch 5020: train loss: 3.711615761403664e-07, val loss: 0.09937336295843124\n",
      "Epoch 5021: train loss: 3.8354656339834037e-07, val loss: 0.09957671910524368\n",
      "Epoch 5022: train loss: 2.861465588921419e-07, val loss: 0.09946787357330322\n",
      "Epoch 5023: train loss: 2.560901748438482e-07, val loss: 0.09930837154388428\n",
      "Epoch 5024: train loss: 3.132677761641389e-07, val loss: 0.09945268929004669\n",
      "Epoch 5025: train loss: 1.5699910704825015e-07, val loss: 0.09963780641555786\n",
      "Epoch 5026: train loss: 2.875973450500169e-07, val loss: 0.09934604167938232\n",
      "Epoch 5027: train loss: 1.531288802425479e-07, val loss: 0.09929881244897842\n",
      "Epoch 5028: train loss: 2.0753448382038187e-07, val loss: 0.09964072704315186\n",
      "Epoch 5029: train loss: 1.4347558874305832e-07, val loss: 0.09956134855747223\n",
      "Epoch 5030: train loss: 1.283857216094475e-07, val loss: 0.09940345585346222\n",
      "Epoch 5031: train loss: 1.7004650487706385e-07, val loss: 0.09957190603017807\n",
      "Epoch 5032: train loss: 1.1224877027871116e-07, val loss: 0.09955968707799911\n",
      "Epoch 5033: train loss: 1.285696953345905e-07, val loss: 0.0995178371667862\n",
      "Epoch 5034: train loss: 1.174088666289208e-07, val loss: 0.09968709200620651\n",
      "Epoch 5035: train loss: 1.3857989245025237e-07, val loss: 0.09951438754796982\n",
      "Epoch 5036: train loss: 1.2547828021070018e-07, val loss: 0.09954768419265747\n",
      "Epoch 5037: train loss: 1.5197124980659282e-07, val loss: 0.0996856838464737\n",
      "Epoch 5038: train loss: 1.831876943469979e-07, val loss: 0.09965641796588898\n",
      "Epoch 5039: train loss: 2.469789990300342e-07, val loss: 0.09954836964607239\n",
      "Epoch 5040: train loss: 3.305381994778145e-07, val loss: 0.09971775114536285\n",
      "Epoch 5041: train loss: 4.313123440624622e-07, val loss: 0.09962307661771774\n",
      "Epoch 5042: train loss: 5.913103109378426e-07, val loss: 0.09979809820652008\n",
      "Epoch 5043: train loss: 8.369864303858776e-07, val loss: 0.09957637637853622\n",
      "Epoch 5044: train loss: 1.1588082315938664e-06, val loss: 0.09985488653182983\n",
      "Epoch 5045: train loss: 1.7159565004476462e-06, val loss: 0.09955796599388123\n",
      "Epoch 5046: train loss: 2.6243444608553546e-06, val loss: 0.09997357428073883\n",
      "Epoch 5047: train loss: 4.216715296934126e-06, val loss: 0.09954491257667542\n",
      "Epoch 5048: train loss: 6.9821294346184e-06, val loss: 0.1002519503235817\n",
      "Epoch 5049: train loss: 1.2268903446965851e-05, val loss: 0.09931474924087524\n",
      "Epoch 5050: train loss: 2.2702903152094223e-05, val loss: 0.10092979669570923\n",
      "Epoch 5051: train loss: 4.273606464266777e-05, val loss: 0.09844737499952316\n",
      "Epoch 5052: train loss: 8.232198160840198e-05, val loss: 0.10203780233860016\n",
      "Epoch 5053: train loss: 0.00016169692389667034, val loss: 0.09675061702728271\n",
      "Epoch 5054: train loss: 0.00032142322743311524, val loss: 0.10219626873731613\n",
      "Epoch 5055: train loss: 0.0005107428296469152, val loss: 0.0930723324418068\n",
      "Epoch 5056: train loss: 0.00035316433059051633, val loss: 0.09437456727027893\n",
      "Epoch 5057: train loss: 0.00011624108446994796, val loss: 0.09393923729658127\n",
      "Epoch 5058: train loss: 0.00016187359869945794, val loss: 0.08739794790744781\n",
      "Epoch 5059: train loss: 0.00019332340161781758, val loss: 0.09043582528829575\n",
      "Epoch 5060: train loss: 0.0001413193967891857, val loss: 0.0888632982969284\n",
      "Epoch 5061: train loss: 0.00010848699457710609, val loss: 0.08248748630285263\n",
      "Epoch 5062: train loss: 9.659459465183318e-05, val loss: 0.0854175016283989\n",
      "Epoch 5063: train loss: 0.00010423264029668644, val loss: 0.0857902392745018\n",
      "Epoch 5064: train loss: 7.603667472722009e-05, val loss: 0.08084266632795334\n",
      "Epoch 5065: train loss: 5.841815072926693e-05, val loss: 0.08178975433111191\n",
      "Epoch 5066: train loss: 7.144953997340053e-05, val loss: 0.08365481346845627\n",
      "Epoch 5067: train loss: 5.374697866500355e-05, val loss: 0.08025307953357697\n",
      "Epoch 5068: train loss: 4.261867798049934e-05, val loss: 0.07995037734508514\n",
      "Epoch 5069: train loss: 4.577345680445433e-05, val loss: 0.08295251429080963\n",
      "Epoch 5070: train loss: 3.842312435153872e-05, val loss: 0.08078695833683014\n",
      "Epoch 5071: train loss: 3.135142469545826e-05, val loss: 0.07832317799329758\n",
      "Epoch 5072: train loss: 3.204287713742815e-05, val loss: 0.08148622512817383\n",
      "Epoch 5073: train loss: 2.8410146114765666e-05, val loss: 0.08162420243024826\n",
      "Epoch 5074: train loss: 1.8656042811926454e-05, val loss: 0.07811402529478073\n",
      "Epoch 5075: train loss: 2.8730179110425524e-05, val loss: 0.07955130189657211\n",
      "Epoch 5076: train loss: 1.2624773262359668e-05, val loss: 0.0814284086227417\n",
      "Epoch 5077: train loss: 1.8079052097164094e-05, val loss: 0.07913544774055481\n",
      "Epoch 5078: train loss: 1.9053251889999956e-05, val loss: 0.07892864942550659\n",
      "Epoch 5079: train loss: 8.789989806246012e-06, val loss: 0.08020894974470139\n",
      "Epoch 5080: train loss: 1.4923813068890013e-05, val loss: 0.07931657135486603\n",
      "Epoch 5081: train loss: 1.2356885235931259e-05, val loss: 0.07923623919487\n",
      "Epoch 5082: train loss: 6.491480235126801e-06, val loss: 0.07996735721826553\n",
      "Epoch 5083: train loss: 1.2713109754258767e-05, val loss: 0.07926207035779953\n",
      "Epoch 5084: train loss: 5.926881385676097e-06, val loss: 0.07917273044586182\n",
      "Epoch 5085: train loss: 8.116774552036077e-06, val loss: 0.07963478565216064\n",
      "Epoch 5086: train loss: 7.207457656477345e-06, val loss: 0.07916783541440964\n",
      "Epoch 5087: train loss: 4.765956873598043e-06, val loss: 0.07932349294424057\n",
      "Epoch 5088: train loss: 7.0857104219612665e-06, val loss: 0.07990000396966934\n",
      "Epoch 5089: train loss: 3.3811099910963094e-06, val loss: 0.07926121354103088\n",
      "Epoch 5090: train loss: 5.5994328249653336e-06, val loss: 0.07892614603042603\n",
      "Epoch 5091: train loss: 5.339149083738448e-06, val loss: 0.07973450422286987\n",
      "Epoch 5092: train loss: 6.7703481363423634e-06, val loss: 0.07855630666017532\n",
      "Epoch 5093: train loss: 8.110710950859357e-06, val loss: 0.07896696776151657\n",
      "Epoch 5094: train loss: 8.113641342788469e-06, val loss: 0.07954169064760208\n",
      "Epoch 5095: train loss: 4.804052423423855e-06, val loss: 0.08000566810369492\n",
      "Epoch 5096: train loss: 6.28290808890597e-06, val loss: 0.07954107969999313\n",
      "Epoch 5097: train loss: 4.617373633664101e-06, val loss: 0.07893115282058716\n",
      "Epoch 5098: train loss: 4.737180915981298e-06, val loss: 0.07938840240240097\n",
      "Epoch 5099: train loss: 4.642472504201578e-06, val loss: 0.07966810464859009\n",
      "Epoch 5100: train loss: 2.676772737686406e-06, val loss: 0.07984372973442078\n",
      "Epoch 5101: train loss: 4.140408691455377e-06, val loss: 0.07993269711732864\n",
      "Epoch 5102: train loss: 2.974809831357561e-06, val loss: 0.079414002597332\n",
      "Epoch 5103: train loss: 2.681104433577275e-06, val loss: 0.07945596426725388\n",
      "Epoch 5104: train loss: 2.9559203085227637e-06, val loss: 0.07962444424629211\n",
      "Epoch 5105: train loss: 1.529416977064102e-06, val loss: 0.07991576194763184\n",
      "Epoch 5106: train loss: 2.668037495823228e-06, val loss: 0.08030246943235397\n",
      "Epoch 5107: train loss: 1.9475585304462584e-06, val loss: 0.07965752482414246\n",
      "Epoch 5108: train loss: 1.4680709909953293e-06, val loss: 0.07955918461084366\n",
      "Epoch 5109: train loss: 1.7772041474017897e-06, val loss: 0.07987582683563232\n",
      "Epoch 5110: train loss: 1.1925767466891557e-06, val loss: 0.08012932538986206\n",
      "Epoch 5111: train loss: 1.3672497516381554e-06, val loss: 0.08046108484268188\n",
      "Epoch 5112: train loss: 1.4562078831659164e-06, val loss: 0.07986029982566833\n",
      "Epoch 5113: train loss: 9.184861937683308e-07, val loss: 0.0800766870379448\n",
      "Epoch 5114: train loss: 9.937207323673647e-07, val loss: 0.08036013692617416\n",
      "Epoch 5115: train loss: 7.989244181771937e-07, val loss: 0.08033761382102966\n",
      "Epoch 5116: train loss: 8.177660788533103e-07, val loss: 0.08074331283569336\n",
      "Epoch 5117: train loss: 8.958913326750917e-07, val loss: 0.08040636032819748\n",
      "Epoch 5118: train loss: 7.018946917014546e-07, val loss: 0.08046054095029831\n",
      "Epoch 5119: train loss: 6.716819598295842e-07, val loss: 0.08061634749174118\n",
      "Epoch 5120: train loss: 5.955537858426396e-07, val loss: 0.08095721155405045\n",
      "Epoch 5121: train loss: 5.074756472822628e-07, val loss: 0.08093216270208359\n",
      "Epoch 5122: train loss: 3.5412804777479323e-07, val loss: 0.08088067919015884\n",
      "Epoch 5123: train loss: 4.180750750037987e-07, val loss: 0.08099421113729477\n",
      "Epoch 5124: train loss: 4.32613745715571e-07, val loss: 0.08099758625030518\n",
      "Epoch 5125: train loss: 3.991635821876116e-07, val loss: 0.0813785046339035\n",
      "Epoch 5126: train loss: 3.5859054037246096e-07, val loss: 0.08117946237325668\n",
      "Epoch 5127: train loss: 4.2167980041085684e-07, val loss: 0.08131910860538483\n",
      "Epoch 5128: train loss: 5.671887493008398e-07, val loss: 0.08138061314821243\n",
      "Epoch 5129: train loss: 7.121453791114618e-07, val loss: 0.08193246275186539\n",
      "Epoch 5130: train loss: 9.904880471367505e-07, val loss: 0.08132117986679077\n",
      "Epoch 5131: train loss: 1.7184275975523633e-06, val loss: 0.08213506639003754\n",
      "Epoch 5132: train loss: 3.554295290086884e-06, val loss: 0.08120398968458176\n",
      "Epoch 5133: train loss: 7.735910003248136e-06, val loss: 0.08324923366308212\n",
      "Epoch 5134: train loss: 1.761391104082577e-05, val loss: 0.08027640730142593\n",
      "Epoch 5135: train loss: 4.170096872258e-05, val loss: 0.0852452740073204\n",
      "Epoch 5136: train loss: 9.547126683173701e-05, val loss: 0.078215591609478\n",
      "Epoch 5137: train loss: 0.00022325103054754436, val loss: 0.08925051987171173\n",
      "Epoch 5138: train loss: 0.00048723354120738804, val loss: 0.07635030895471573\n",
      "Epoch 5139: train loss: 0.0009168793330900371, val loss: 0.09146102517843246\n",
      "Epoch 5140: train loss: 0.0011394270695745945, val loss: 0.07778367400169373\n",
      "Epoch 5141: train loss: 0.0005228372756391764, val loss: 0.08090320974588394\n",
      "Epoch 5142: train loss: 5.4915981309022754e-05, val loss: 0.08701828867197037\n",
      "Epoch 5143: train loss: 0.00038580058026127517, val loss: 0.07165976613759995\n",
      "Epoch 5144: train loss: 0.0003230706788599491, val loss: 0.07581298798322678\n",
      "Epoch 5145: train loss: 0.00010153780749533325, val loss: 0.08200100809335709\n",
      "Epoch 5146: train loss: 0.00024008267791941762, val loss: 0.07228633016347885\n",
      "Epoch 5147: train loss: 0.0001312277454417199, val loss: 0.07066665589809418\n",
      "Epoch 5148: train loss: 0.00011967858881689608, val loss: 0.07564536482095718\n",
      "Epoch 5149: train loss: 0.00012156514276284724, val loss: 0.07397029548883438\n",
      "Epoch 5150: train loss: 8.48074778332375e-05, val loss: 0.07080911844968796\n",
      "Epoch 5151: train loss: 9.688969294074923e-05, val loss: 0.07179789990186691\n",
      "Epoch 5152: train loss: 6.103898340370506e-05, val loss: 0.07440385967493057\n",
      "Epoch 5153: train loss: 7.888120308052748e-05, val loss: 0.07460777461528778\n",
      "Epoch 5154: train loss: 4.50698244094383e-05, val loss: 0.07298105210065842\n",
      "Epoch 5155: train loss: 6.325980211840943e-05, val loss: 0.0731421485543251\n",
      "Epoch 5156: train loss: 3.4861914173234254e-05, val loss: 0.07518132776021957\n",
      "Epoch 5157: train loss: 5.0605849537532777e-05, val loss: 0.07555695623159409\n",
      "Epoch 5158: train loss: 2.774425593088381e-05, val loss: 0.07462327927350998\n",
      "Epoch 5159: train loss: 3.9193633710965514e-05, val loss: 0.07525056600570679\n",
      "Epoch 5160: train loss: 2.3719352611806244e-05, val loss: 0.07643581926822662\n",
      "Epoch 5161: train loss: 2.936634700745344e-05, val loss: 0.07597185671329498\n",
      "Epoch 5162: train loss: 2.1282850866555236e-05, val loss: 0.07629170268774033\n",
      "Epoch 5163: train loss: 2.1104695406393148e-05, val loss: 0.07790005207061768\n",
      "Epoch 5164: train loss: 1.9424405763857067e-05, val loss: 0.07803813368082047\n",
      "Epoch 5165: train loss: 1.5301278835977428e-05, val loss: 0.07673650234937668\n",
      "Epoch 5166: train loss: 1.6608033547527157e-05, val loss: 0.07745734602212906\n",
      "Epoch 5167: train loss: 1.21033353934763e-05, val loss: 0.0796300396323204\n",
      "Epoch 5168: train loss: 1.3030020454607438e-05, val loss: 0.07990726828575134\n",
      "Epoch 5169: train loss: 1.0611340258037671e-05, val loss: 0.07832884043455124\n",
      "Epoch 5170: train loss: 9.837096513365395e-06, val loss: 0.07828712463378906\n",
      "Epoch 5171: train loss: 9.321195648226421e-06, val loss: 0.08028341829776764\n",
      "Epoch 5172: train loss: 7.420906513289083e-06, val loss: 0.08130695670843124\n",
      "Epoch 5173: train loss: 7.984443072928116e-06, val loss: 0.0803191140294075\n",
      "Epoch 5174: train loss: 5.783978849649429e-06, val loss: 0.0797123834490776\n",
      "Epoch 5175: train loss: 6.7271384978084825e-06, val loss: 0.08057595789432526\n",
      "Epoch 5176: train loss: 4.843119768338511e-06, val loss: 0.08137717097997665\n",
      "Epoch 5177: train loss: 5.996950676490087e-06, val loss: 0.07918860018253326\n",
      "Epoch 5178: train loss: 7.751341399853118e-06, val loss: 0.08009184896945953\n",
      "Epoch 5179: train loss: 1.6267067621811293e-05, val loss: 0.08182428032159805\n",
      "Epoch 5180: train loss: 5.738472464145161e-05, val loss: 0.08093970268964767\n",
      "Epoch 5181: train loss: 2.7377573132980615e-05, val loss: 0.08027247339487076\n",
      "Epoch 5182: train loss: 1.792622970242519e-05, val loss: 0.08074706047773361\n",
      "Epoch 5183: train loss: 4.255004387232475e-05, val loss: 0.08119761198759079\n",
      "Epoch 5184: train loss: 9.474853868596256e-06, val loss: 0.08087039738893509\n",
      "Epoch 5185: train loss: 2.6801937565323897e-05, val loss: 0.08067912608385086\n",
      "Epoch 5186: train loss: 1.952093225554563e-05, val loss: 0.08107227087020874\n",
      "Epoch 5187: train loss: 1.3280534403747879e-05, val loss: 0.08043066412210464\n",
      "Epoch 5188: train loss: 1.928157507791184e-05, val loss: 0.0798027440905571\n",
      "Epoch 5189: train loss: 1.0807239050336648e-05, val loss: 0.08075562864542007\n",
      "Epoch 5190: train loss: 1.762859756127e-05, val loss: 0.08173687011003494\n",
      "Epoch 5191: train loss: 1.3754129213339183e-05, val loss: 0.0810423418879509\n",
      "Epoch 5192: train loss: 4.618468665285036e-06, val loss: 0.08020094037055969\n",
      "Epoch 5193: train loss: 1.5185930351435672e-05, val loss: 0.08121013641357422\n",
      "Epoch 5194: train loss: 4.395215455588186e-06, val loss: 0.08230304718017578\n",
      "Epoch 5195: train loss: 7.988011020643171e-06, val loss: 0.08106332272291183\n",
      "Epoch 5196: train loss: 9.059362128027715e-06, val loss: 0.08045593649148941\n",
      "Epoch 5197: train loss: 2.6982379495166242e-06, val loss: 0.08190103620290756\n",
      "Epoch 5198: train loss: 8.4672101365868e-06, val loss: 0.08198963850736618\n",
      "Epoch 5199: train loss: 3.668859790195711e-06, val loss: 0.08120997995138168\n",
      "Epoch 5200: train loss: 4.166311555309221e-06, val loss: 0.0821048691868782\n",
      "Epoch 5201: train loss: 5.735933882533573e-06, val loss: 0.08220156282186508\n",
      "Epoch 5202: train loss: 1.760983536769345e-06, val loss: 0.08125566691160202\n",
      "Epoch 5203: train loss: 4.721438926935662e-06, val loss: 0.0820985957980156\n",
      "Epoch 5204: train loss: 3.062931227759691e-06, val loss: 0.08247971534729004\n",
      "Epoch 5205: train loss: 1.5452529851245345e-06, val loss: 0.08162202686071396\n",
      "Epoch 5206: train loss: 4.110458576178644e-06, val loss: 0.0820215716958046\n",
      "Epoch 5207: train loss: 1.4691804608446546e-06, val loss: 0.0825292095541954\n",
      "Epoch 5208: train loss: 1.8487354509488796e-06, val loss: 0.0823180302977562\n",
      "Epoch 5209: train loss: 2.735435828071786e-06, val loss: 0.0825219377875328\n",
      "Epoch 5210: train loss: 1.1081875754825887e-06, val loss: 0.08244847506284714\n",
      "Epoch 5211: train loss: 1.6018602764233947e-06, val loss: 0.08238648623228073\n",
      "Epoch 5212: train loss: 1.794309923752735e-06, val loss: 0.08292048424482346\n",
      "Epoch 5213: train loss: 8.810670806269627e-07, val loss: 0.08294651657342911\n",
      "Epoch 5214: train loss: 1.503734665675438e-06, val loss: 0.08292772620916367\n",
      "Epoch 5215: train loss: 1.4074041700951057e-06, val loss: 0.08304810523986816\n",
      "Epoch 5216: train loss: 5.241042799752904e-07, val loss: 0.08351463079452515\n",
      "Epoch 5217: train loss: 1.1455779258540133e-06, val loss: 0.08283542841672897\n",
      "Epoch 5218: train loss: 1.368264747725334e-06, val loss: 0.08364241570234299\n",
      "Epoch 5219: train loss: 2.5979300062317634e-06, val loss: 0.08290516585111618\n",
      "Epoch 5220: train loss: 2.721856844800641e-06, val loss: 0.08316966146230698\n",
      "Epoch 5221: train loss: 1.6904006088225287e-06, val loss: 0.08336856216192245\n",
      "Epoch 5222: train loss: 1.4859232351227547e-06, val loss: 0.08343899995088577\n",
      "Epoch 5223: train loss: 1.316965494879696e-06, val loss: 0.08357659727334976\n",
      "Epoch 5224: train loss: 1.361097247354337e-06, val loss: 0.08335307240486145\n",
      "Epoch 5225: train loss: 1.2748917015414918e-06, val loss: 0.08349855244159698\n",
      "Epoch 5226: train loss: 8.976135745797365e-07, val loss: 0.08351174741983414\n",
      "Epoch 5227: train loss: 1.2048016060361988e-06, val loss: 0.08394469320774078\n",
      "Epoch 5228: train loss: 8.046768016356509e-07, val loss: 0.08413504809141159\n",
      "Epoch 5229: train loss: 5.111052132633631e-07, val loss: 0.08355189859867096\n",
      "Epoch 5230: train loss: 9.896614301396767e-07, val loss: 0.08397746086120605\n",
      "Epoch 5231: train loss: 6.45917680230923e-07, val loss: 0.0842752605676651\n",
      "Epoch 5232: train loss: 7.264982286869781e-07, val loss: 0.08402077108621597\n",
      "Epoch 5233: train loss: 4.6038664436309773e-07, val loss: 0.08408248424530029\n",
      "Epoch 5234: train loss: 4.6823151933494955e-07, val loss: 0.08435463905334473\n",
      "Epoch 5235: train loss: 5.325333063410653e-07, val loss: 0.08453737199306488\n",
      "Epoch 5236: train loss: 4.0517926436223206e-07, val loss: 0.08450473099946976\n",
      "Epoch 5237: train loss: 4.3719984432755155e-07, val loss: 0.08447817713022232\n",
      "Epoch 5238: train loss: 4.3542766547943756e-07, val loss: 0.0844375416636467\n",
      "Epoch 5239: train loss: 3.657660272438079e-07, val loss: 0.08485451340675354\n",
      "Epoch 5240: train loss: 3.4083694799846853e-07, val loss: 0.08466290682554245\n",
      "Epoch 5241: train loss: 3.403878565677587e-07, val loss: 0.08489231020212173\n",
      "Epoch 5242: train loss: 2.0092269892302284e-07, val loss: 0.08499988168478012\n",
      "Epoch 5243: train loss: 1.8656621136869944e-07, val loss: 0.08493147790431976\n",
      "Epoch 5244: train loss: 2.0046623205871583e-07, val loss: 0.08515862375497818\n",
      "Epoch 5245: train loss: 1.9723407262972614e-07, val loss: 0.08523356169462204\n",
      "Epoch 5246: train loss: 2.0162623570740834e-07, val loss: 0.08525437861680984\n",
      "Epoch 5247: train loss: 2.396117508851603e-07, val loss: 0.08533389866352081\n",
      "Epoch 5248: train loss: 2.646588086463453e-07, val loss: 0.085495725274086\n",
      "Epoch 5249: train loss: 3.479088945823605e-07, val loss: 0.08561400324106216\n",
      "Epoch 5250: train loss: 5.01359238569421e-07, val loss: 0.08569885790348053\n",
      "Epoch 5251: train loss: 7.903110486040532e-07, val loss: 0.08563181012868881\n",
      "Epoch 5252: train loss: 1.598140215719468e-06, val loss: 0.08616138249635696\n",
      "Epoch 5253: train loss: 4.0208492464444134e-06, val loss: 0.08543335646390915\n",
      "Epoch 5254: train loss: 1.0093324817717075e-05, val loss: 0.08729581534862518\n",
      "Epoch 5255: train loss: 2.331887480977457e-05, val loss: 0.08367405086755753\n",
      "Epoch 5256: train loss: 5.8351542975287884e-05, val loss: 0.0909862145781517\n",
      "Epoch 5257: train loss: 0.0001532703754492104, val loss: 0.08008493483066559\n",
      "Epoch 5258: train loss: 0.00044107259600423276, val loss: 0.10163559019565582\n",
      "Epoch 5259: train loss: 0.0010549294529482722, val loss: 0.07575004547834396\n",
      "Epoch 5260: train loss: 0.0019408530788496137, val loss: 0.09149260818958282\n",
      "Epoch 5261: train loss: 0.0016334534157067537, val loss: 0.08256581425666809\n",
      "Epoch 5262: train loss: 0.0006115491851232946, val loss: 0.0850198045372963\n",
      "Epoch 5263: train loss: 0.0004461292701307684, val loss: 0.08635245263576508\n",
      "Epoch 5264: train loss: 0.0005741731147281826, val loss: 0.08917927742004395\n",
      "Epoch 5265: train loss: 0.0003605141828302294, val loss: 0.09168228507041931\n",
      "Epoch 5266: train loss: 0.00032582267886027694, val loss: 0.08915647119283676\n",
      "Epoch 5267: train loss: 0.0003032101085409522, val loss: 0.08911214768886566\n",
      "Epoch 5268: train loss: 0.00021664757514372468, val loss: 0.09354116022586823\n",
      "Epoch 5269: train loss: 0.00022644862474408, val loss: 0.09566500037908554\n",
      "Epoch 5270: train loss: 0.0001587799342814833, val loss: 0.09477993100881577\n",
      "Epoch 5271: train loss: 0.00016475773009005934, val loss: 0.09499936550855637\n",
      "Epoch 5272: train loss: 0.0001271858491236344, val loss: 0.09629135578870773\n",
      "Epoch 5273: train loss: 0.00011879496014444157, val loss: 0.09655775874853134\n",
      "Epoch 5274: train loss: 0.00010248232138110325, val loss: 0.09630678594112396\n",
      "Epoch 5275: train loss: 8.482090925099328e-05, val loss: 0.09701552242040634\n",
      "Epoch 5276: train loss: 8.676428842591122e-05, val loss: 0.09772617369890213\n",
      "Epoch 5277: train loss: 6.303747068159282e-05, val loss: 0.0976206585764885\n",
      "Epoch 5278: train loss: 6.484161713160574e-05, val loss: 0.09764327108860016\n",
      "Epoch 5279: train loss: 5.607013372355141e-05, val loss: 0.09758521616458893\n",
      "Epoch 5280: train loss: 4.438338874024339e-05, val loss: 0.09715240448713303\n",
      "Epoch 5281: train loss: 4.975947013008408e-05, val loss: 0.09698713570833206\n",
      "Epoch 5282: train loss: 3.254618059145287e-05, val loss: 0.09729873389005661\n",
      "Epoch 5283: train loss: 3.9752783777657896e-05, val loss: 0.09723260253667831\n",
      "Epoch 5284: train loss: 2.7352039978723042e-05, val loss: 0.09674812108278275\n",
      "Epoch 5285: train loss: 2.9652544981217943e-05, val loss: 0.0966859683394432\n",
      "Epoch 5286: train loss: 2.3815668100723997e-05, val loss: 0.09727753698825836\n",
      "Epoch 5287: train loss: 2.2304060621536337e-05, val loss: 0.09735118597745895\n",
      "Epoch 5288: train loss: 2.173289431084413e-05, val loss: 0.09657367318868637\n",
      "Epoch 5289: train loss: 1.4313847714220174e-05, val loss: 0.09601496160030365\n",
      "Epoch 5290: train loss: 2.017362930928357e-05, val loss: 0.09635529667139053\n",
      "Epoch 5291: train loss: 1.0829116035893094e-05, val loss: 0.09662964940071106\n",
      "Epoch 5292: train loss: 1.514414634584682e-05, val loss: 0.09589941799640656\n",
      "Epoch 5293: train loss: 1.1198019819858018e-05, val loss: 0.09524601697921753\n",
      "Epoch 5294: train loss: 9.134301762969699e-06, val loss: 0.09541323035955429\n",
      "Epoch 5295: train loss: 1.1554093362065032e-05, val loss: 0.09594829380512238\n",
      "Epoch 5296: train loss: 6.554142146342201e-06, val loss: 0.09608855098485947\n",
      "Epoch 5297: train loss: 8.415257070737425e-06, val loss: 0.0957237035036087\n",
      "Epoch 5298: train loss: 6.874563496239716e-06, val loss: 0.09529463201761246\n",
      "Epoch 5299: train loss: 5.070930455985945e-06, val loss: 0.09523744136095047\n",
      "Epoch 5300: train loss: 6.8396279857552145e-06, val loss: 0.09557726979255676\n",
      "Epoch 5301: train loss: 3.6591613934433553e-06, val loss: 0.09592235088348389\n",
      "Epoch 5302: train loss: 5.137368134455755e-06, val loss: 0.09580393880605698\n",
      "Epoch 5303: train loss: 3.958667093684198e-06, val loss: 0.09536087512969971\n",
      "Epoch 5304: train loss: 3.004259042427293e-06, val loss: 0.09521551430225372\n",
      "Epoch 5305: train loss: 4.156303020863561e-06, val loss: 0.09558644890785217\n",
      "Epoch 5306: train loss: 1.8963733054988552e-06, val loss: 0.09588698297739029\n",
      "Epoch 5307: train loss: 3.5546443086786894e-06, val loss: 0.09570664912462234\n",
      "Epoch 5308: train loss: 1.7935598179974477e-06, val loss: 0.09545505046844482\n",
      "Epoch 5309: train loss: 2.380661044298904e-06, val loss: 0.09558255225419998\n",
      "Epoch 5310: train loss: 1.9692790829139994e-06, val loss: 0.09580977261066437\n",
      "Epoch 5311: train loss: 1.5009667322374298e-06, val loss: 0.09569622576236725\n",
      "Epoch 5312: train loss: 1.9828291897283634e-06, val loss: 0.09541681408882141\n",
      "Epoch 5313: train loss: 1.0716224778661854e-06, val loss: 0.095430389046669\n",
      "Epoch 5314: train loss: 1.6060234884207603e-06, val loss: 0.09573175758123398\n",
      "Epoch 5315: train loss: 8.884176736501104e-07, val loss: 0.0959072932600975\n",
      "Epoch 5316: train loss: 1.2808116025553318e-06, val loss: 0.09575369209051132\n",
      "Epoch 5317: train loss: 8.413495606873767e-07, val loss: 0.09551552683115005\n",
      "Epoch 5318: train loss: 9.542769703330123e-07, val loss: 0.09551738947629929\n",
      "Epoch 5319: train loss: 7.214534321065003e-07, val loss: 0.09574776887893677\n",
      "Epoch 5320: train loss: 7.215556934170309e-07, val loss: 0.09584853798151016\n",
      "Epoch 5321: train loss: 6.63039884329919e-07, val loss: 0.09565533697605133\n",
      "Epoch 5322: train loss: 5.428581744126859e-07, val loss: 0.09548503160476685\n",
      "Epoch 5323: train loss: 5.963730131952616e-07, val loss: 0.09563121199607849\n",
      "Epoch 5324: train loss: 3.958820400384866e-07, val loss: 0.09585072845220566\n",
      "Epoch 5325: train loss: 4.908611686005315e-07, val loss: 0.09579136222600937\n",
      "Epoch 5326: train loss: 3.340797150030994e-07, val loss: 0.09560103714466095\n",
      "Epoch 5327: train loss: 3.901107561432582e-07, val loss: 0.09563211351633072\n",
      "Epoch 5328: train loss: 3.026721628884843e-07, val loss: 0.0957983061671257\n",
      "Epoch 5329: train loss: 2.731711106207513e-07, val loss: 0.09581215679645538\n",
      "Epoch 5330: train loss: 2.749555676473392e-07, val loss: 0.09570934623479843\n",
      "Epoch 5331: train loss: 2.3101057422536542e-07, val loss: 0.09570690244436264\n",
      "Epoch 5332: train loss: 2.2230786100863043e-07, val loss: 0.09578632563352585\n",
      "Epoch 5333: train loss: 1.8628982445534348e-07, val loss: 0.0958300456404686\n",
      "Epoch 5334: train loss: 1.7797727025481436e-07, val loss: 0.09581821411848068\n",
      "Epoch 5335: train loss: 1.4147576621326152e-07, val loss: 0.09582530707120895\n",
      "Epoch 5336: train loss: 1.4159094519072823e-07, val loss: 0.09587594121694565\n",
      "Epoch 5337: train loss: 1.212104763226307e-07, val loss: 0.09588691592216492\n",
      "Epoch 5338: train loss: 1.2716162700598943e-07, val loss: 0.09587736427783966\n",
      "Epoch 5339: train loss: 9.827644475990382e-08, val loss: 0.09592070430517197\n",
      "Epoch 5340: train loss: 9.476379148054548e-08, val loss: 0.09598451107740402\n",
      "Epoch 5341: train loss: 7.532408830002169e-08, val loss: 0.09600279480218887\n",
      "Epoch 5342: train loss: 8.55870112559387e-08, val loss: 0.09598838537931442\n",
      "Epoch 5343: train loss: 6.8530482622009e-08, val loss: 0.095985047519207\n",
      "Epoch 5344: train loss: 6.587525547274709e-08, val loss: 0.09604188054800034\n",
      "Epoch 5345: train loss: 5.396551472358624e-08, val loss: 0.09611888974905014\n",
      "Epoch 5346: train loss: 5.084845966507601e-08, val loss: 0.09611443430185318\n",
      "Epoch 5347: train loss: 5.028464045153669e-08, val loss: 0.09607899934053421\n",
      "Epoch 5348: train loss: 4.6215657079073935e-08, val loss: 0.09612755477428436\n",
      "Epoch 5349: train loss: 3.886079369408435e-08, val loss: 0.09619855135679245\n",
      "Epoch 5350: train loss: 3.428925765547319e-08, val loss: 0.09619536250829697\n",
      "Epoch 5351: train loss: 3.397968839635723e-08, val loss: 0.09617149084806442\n",
      "Epoch 5352: train loss: 3.157271422082886e-08, val loss: 0.09618651121854782\n",
      "Epoch 5353: train loss: 3.1991429949584926e-08, val loss: 0.09628321975469589\n",
      "Epoch 5354: train loss: 2.590728520601715e-08, val loss: 0.09630145877599716\n",
      "Epoch 5355: train loss: 3.333236620051139e-08, val loss: 0.09628542512655258\n",
      "Epoch 5356: train loss: 2.3843078622576286e-08, val loss: 0.09626346081495285\n",
      "Epoch 5357: train loss: 4.10336902234576e-08, val loss: 0.09640864282846451\n",
      "Epoch 5358: train loss: 6.058515111817542e-08, val loss: 0.09635239094495773\n",
      "Epoch 5359: train loss: 7.697339299284067e-08, val loss: 0.09646705538034439\n",
      "Epoch 5360: train loss: 9.068218531638195e-08, val loss: 0.09649493545293808\n",
      "Epoch 5361: train loss: 5.6451746388574975e-08, val loss: 0.09658235311508179\n",
      "Epoch 5362: train loss: 3.5292103461870283e-08, val loss: 0.09656905382871628\n",
      "Epoch 5363: train loss: 2.4219287908522347e-08, val loss: 0.0966157540678978\n",
      "Epoch 5364: train loss: 3.7399590979703135e-08, val loss: 0.0966777577996254\n",
      "Epoch 5365: train loss: 3.733187270427152e-08, val loss: 0.09659802913665771\n",
      "Epoch 5366: train loss: 3.217700950131075e-08, val loss: 0.09666811674833298\n",
      "Epoch 5367: train loss: 1.6324349871865707e-08, val loss: 0.09669972956180573\n",
      "Epoch 5368: train loss: 1.7684287811903232e-08, val loss: 0.09666299819946289\n",
      "Epoch 5369: train loss: 2.8473172264398272e-08, val loss: 0.09674005955457687\n",
      "Epoch 5370: train loss: 2.7236067623448434e-08, val loss: 0.09672870486974716\n",
      "Epoch 5371: train loss: 1.1348025630297798e-08, val loss: 0.09673308581113815\n",
      "Epoch 5372: train loss: 1.1126093824032068e-08, val loss: 0.09677063673734665\n",
      "Epoch 5373: train loss: 1.9000911066768822e-08, val loss: 0.09673096984624863\n",
      "Epoch 5374: train loss: 2.4133218090582886e-08, val loss: 0.09680739045143127\n",
      "Epoch 5375: train loss: 2.594783055087646e-08, val loss: 0.09674489498138428\n",
      "Epoch 5376: train loss: 3.2028754759494404e-08, val loss: 0.09685071557760239\n",
      "Epoch 5377: train loss: 4.27067554653604e-08, val loss: 0.09667383879423141\n",
      "Epoch 5378: train loss: 6.800983953780815e-08, val loss: 0.09689521789550781\n",
      "Epoch 5379: train loss: 9.713616577755602e-08, val loss: 0.0966564267873764\n",
      "Epoch 5380: train loss: 1.493094714533072e-07, val loss: 0.09687644243240356\n",
      "Epoch 5381: train loss: 3.7565487787105667e-07, val loss: 0.09680955857038498\n",
      "Epoch 5382: train loss: 9.245781029676436e-07, val loss: 0.0968160405755043\n",
      "Epoch 5383: train loss: 3.585052468224603e-07, val loss: 0.09662386029958725\n",
      "Epoch 5384: train loss: 6.713144671266491e-07, val loss: 0.09683741629123688\n",
      "Epoch 5385: train loss: 2.788029291878047e-07, val loss: 0.09683700650930405\n",
      "Epoch 5386: train loss: 3.1291745017369976e-07, val loss: 0.09674358367919922\n",
      "Epoch 5387: train loss: 2.2047312597806012e-07, val loss: 0.09675305336713791\n",
      "Epoch 5388: train loss: 3.0135132078612514e-07, val loss: 0.09670849144458771\n",
      "Epoch 5389: train loss: 3.607042629028001e-07, val loss: 0.09689652174711227\n",
      "Epoch 5390: train loss: 3.3858205483738857e-07, val loss: 0.09677095711231232\n",
      "Epoch 5391: train loss: 3.6372679801388585e-07, val loss: 0.0968637615442276\n",
      "Epoch 5392: train loss: 3.46431789921553e-07, val loss: 0.09663493186235428\n",
      "Epoch 5393: train loss: 3.20011338317272e-07, val loss: 0.09686189144849777\n",
      "Epoch 5394: train loss: 2.935102827450464e-07, val loss: 0.09682205319404602\n",
      "Epoch 5395: train loss: 2.913669447934808e-07, val loss: 0.09687425941228867\n",
      "Epoch 5396: train loss: 3.50189310438509e-07, val loss: 0.0966360792517662\n",
      "Epoch 5397: train loss: 4.632360912637523e-07, val loss: 0.09696535021066666\n",
      "Epoch 5398: train loss: 6.948258146621811e-07, val loss: 0.09671715646982193\n",
      "Epoch 5399: train loss: 1.3048220353084616e-06, val loss: 0.09720740467309952\n",
      "Epoch 5400: train loss: 2.6252748739352683e-06, val loss: 0.09646853059530258\n",
      "Epoch 5401: train loss: 5.466401944431709e-06, val loss: 0.09783876687288284\n",
      "Epoch 5402: train loss: 1.1623005775618367e-05, val loss: 0.09608712792396545\n",
      "Epoch 5403: train loss: 2.0898467482766137e-05, val loss: 0.09857205301523209\n",
      "Epoch 5404: train loss: 3.336178997415118e-05, val loss: 0.09572714567184448\n",
      "Epoch 5405: train loss: 5.0933474994963035e-05, val loss: 0.0982283279299736\n",
      "Epoch 5406: train loss: 6.54528703307733e-05, val loss: 0.09611485153436661\n",
      "Epoch 5407: train loss: 7.172952609835193e-05, val loss: 0.09800712019205093\n",
      "Epoch 5408: train loss: 5.682434857590124e-05, val loss: 0.09609822183847427\n",
      "Epoch 5409: train loss: 2.7848589525092393e-05, val loss: 0.09710649400949478\n",
      "Epoch 5410: train loss: 5.0898443078040145e-06, val loss: 0.09702733904123306\n",
      "Epoch 5411: train loss: 3.2959824238787405e-06, val loss: 0.09590186923742294\n",
      "Epoch 5412: train loss: 1.8091142919729464e-05, val loss: 0.09718424826860428\n",
      "Epoch 5413: train loss: 2.9083870686008595e-05, val loss: 0.09583495557308197\n",
      "Epoch 5414: train loss: 2.5594623366487212e-05, val loss: 0.09638967365026474\n",
      "Epoch 5415: train loss: 1.0699551239667926e-05, val loss: 0.09583451598882675\n",
      "Epoch 5416: train loss: 1.0876260603254195e-06, val loss: 0.09572720527648926\n",
      "Epoch 5417: train loss: 4.583058398566209e-06, val loss: 0.09610729664564133\n",
      "Epoch 5418: train loss: 1.3584576663561165e-05, val loss: 0.09494931995868683\n",
      "Epoch 5419: train loss: 1.610129402251914e-05, val loss: 0.09608379751443863\n",
      "Epoch 5420: train loss: 9.658914677856956e-06, val loss: 0.09498634189367294\n",
      "Epoch 5421: train loss: 2.059642156382324e-06, val loss: 0.09501036256551743\n",
      "Epoch 5422: train loss: 1.0471807172507397e-06, val loss: 0.09541457146406174\n",
      "Epoch 5423: train loss: 5.960219368716935e-06, val loss: 0.09423849731683731\n",
      "Epoch 5424: train loss: 9.73534224613104e-06, val loss: 0.09538041055202484\n",
      "Epoch 5425: train loss: 8.357401384273544e-06, val loss: 0.09434852749109268\n",
      "Epoch 5426: train loss: 3.4794347811839543e-06, val loss: 0.09458287805318832\n",
      "Epoch 5427: train loss: 4.0382212773693027e-07, val loss: 0.09462296217679977\n",
      "Epoch 5428: train loss: 1.3407828873823746e-06, val loss: 0.09389521926641464\n",
      "Epoch 5429: train loss: 4.354865268396679e-06, val loss: 0.09473662823438644\n",
      "Epoch 5430: train loss: 6.14046621194575e-06, val loss: 0.09376533329486847\n",
      "Epoch 5431: train loss: 5.239259280642727e-06, val loss: 0.09431621432304382\n",
      "Epoch 5432: train loss: 2.703482095967047e-06, val loss: 0.09391746670007706\n",
      "Epoch 5433: train loss: 6.163744501463952e-07, val loss: 0.0939069613814354\n",
      "Epoch 5434: train loss: 2.3594628828504938e-07, val loss: 0.09407385438680649\n",
      "Epoch 5435: train loss: 1.3426064242594293e-06, val loss: 0.09358703345060349\n",
      "Epoch 5436: train loss: 2.978346401505405e-06, val loss: 0.09420069307088852\n",
      "Epoch 5437: train loss: 3.964681582147023e-06, val loss: 0.09330231696367264\n",
      "Epoch 5438: train loss: 4.0140162127499934e-06, val loss: 0.09415149688720703\n",
      "Epoch 5439: train loss: 3.424079977776273e-06, val loss: 0.0932338610291481\n",
      "Epoch 5440: train loss: 2.4974410735012498e-06, val loss: 0.0938032865524292\n",
      "Epoch 5441: train loss: 1.539610934742086e-06, val loss: 0.0933585911989212\n",
      "Epoch 5442: train loss: 8.311354235956969e-07, val loss: 0.0936385989189148\n",
      "Epoch 5443: train loss: 4.164730285083351e-07, val loss: 0.09335046261548996\n",
      "Epoch 5444: train loss: 2.2761085460842878e-07, val loss: 0.0934474840760231\n",
      "Epoch 5445: train loss: 1.2172284868938732e-07, val loss: 0.09344412386417389\n",
      "Epoch 5446: train loss: 7.178110195127374e-08, val loss: 0.09328887611627579\n",
      "Epoch 5447: train loss: 8.396828121703948e-08, val loss: 0.09344508498907089\n",
      "Epoch 5448: train loss: 1.456112102005136e-07, val loss: 0.09317207336425781\n",
      "Epoch 5449: train loss: 2.76878296290306e-07, val loss: 0.09346829354763031\n",
      "Epoch 5450: train loss: 5.9484221992534e-07, val loss: 0.09299209713935852\n",
      "Epoch 5451: train loss: 1.3429645377982524e-06, val loss: 0.09357967227697372\n",
      "Epoch 5452: train loss: 3.234393489037757e-06, val loss: 0.09255166351795197\n",
      "Epoch 5453: train loss: 8.290750884043518e-06, val loss: 0.09432563930749893\n",
      "Epoch 5454: train loss: 2.2038811948732473e-05, val loss: 0.09131801128387451\n",
      "Epoch 5455: train loss: 6.147194653749466e-05, val loss: 0.0963016226887703\n",
      "Epoch 5456: train loss: 0.0001728176575852558, val loss: 0.08756041526794434\n",
      "Epoch 5457: train loss: 0.00046653285971842706, val loss: 0.10069673508405685\n",
      "Epoch 5458: train loss: 0.0010858086170628667, val loss: 0.08582001179456711\n",
      "Epoch 5459: train loss: 0.0017318519530817866, val loss: 0.10098037868738174\n",
      "Epoch 5460: train loss: 0.0013432000996544957, val loss: 0.10076498985290527\n",
      "Epoch 5461: train loss: 0.00018579050083644688, val loss: 0.09377417713403702\n",
      "Epoch 5462: train loss: 0.0005334397428669035, val loss: 0.09860658645629883\n",
      "Epoch 5463: train loss: 0.0004615330253727734, val loss: 0.0963587686419487\n",
      "Epoch 5464: train loss: 0.00018064831965602934, val loss: 0.09155591577291489\n",
      "Epoch 5465: train loss: 0.0003318224335089326, val loss: 0.08999519795179367\n",
      "Epoch 5466: train loss: 0.00014514177746605128, val loss: 0.09349704533815384\n",
      "Epoch 5467: train loss: 0.00020929596212226897, val loss: 0.09369412064552307\n",
      "Epoch 5468: train loss: 0.00013703927106689662, val loss: 0.08952874690294266\n",
      "Epoch 5469: train loss: 0.00012111716205254197, val loss: 0.08670674264431\n",
      "Epoch 5470: train loss: 0.00013635105278808624, val loss: 0.09085534512996674\n",
      "Epoch 5471: train loss: 6.93645779392682e-05, val loss: 0.09453035145998001\n",
      "Epoch 5472: train loss: 0.00011521957640070468, val loss: 0.09203864634037018\n",
      "Epoch 5473: train loss: 6.021979424986057e-05, val loss: 0.08737165480852127\n",
      "Epoch 5474: train loss: 6.597382889594883e-05, val loss: 0.08680996298789978\n",
      "Epoch 5475: train loss: 7.07345170667395e-05, val loss: 0.09030892699956894\n",
      "Epoch 5476: train loss: 3.979755274485797e-05, val loss: 0.09194573014974594\n",
      "Epoch 5477: train loss: 5.485438305186108e-05, val loss: 0.08982739597558975\n",
      "Epoch 5478: train loss: 3.9236787415575236e-05, val loss: 0.08806716650724411\n",
      "Epoch 5479: train loss: 3.3057236578315496e-05, val loss: 0.08862046152353287\n",
      "Epoch 5480: train loss: 3.920382368960418e-05, val loss: 0.09014717489480972\n",
      "Epoch 5481: train loss: 2.4151244360837154e-05, val loss: 0.09009667485952377\n",
      "Epoch 5482: train loss: 2.627034336910583e-05, val loss: 0.08867725729942322\n",
      "Epoch 5483: train loss: 2.7254851374891587e-05, val loss: 0.08801417052745819\n",
      "Epoch 5484: train loss: 1.658742803556379e-05, val loss: 0.08835392445325851\n",
      "Epoch 5485: train loss: 2.1128162188688293e-05, val loss: 0.08883170038461685\n",
      "Epoch 5486: train loss: 1.6671965568093583e-05, val loss: 0.08900248259305954\n",
      "Epoch 5487: train loss: 1.4958996871428099e-05, val loss: 0.08858749270439148\n",
      "Epoch 5488: train loss: 1.54163790284656e-05, val loss: 0.08813726902008057\n",
      "Epoch 5489: train loss: 1.219190835399786e-05, val loss: 0.08798573166131973\n",
      "Epoch 5490: train loss: 1.1803263078036252e-05, val loss: 0.08843318372964859\n",
      "Epoch 5491: train loss: 9.793659955903422e-06, val loss: 0.08856729418039322\n",
      "Epoch 5492: train loss: 1.0500672942725942e-05, val loss: 0.08836083859205246\n",
      "Epoch 5493: train loss: 1.670488745730836e-05, val loss: 0.0874081403017044\n",
      "Epoch 5494: train loss: 8.600992259744089e-06, val loss: 0.08723714202642441\n",
      "Epoch 5495: train loss: 1.2825291378248949e-05, val loss: 0.08793038129806519\n",
      "Epoch 5496: train loss: 6.089017006161157e-06, val loss: 0.08865073323249817\n",
      "Epoch 5497: train loss: 9.510592462902423e-06, val loss: 0.08855187147855759\n",
      "Epoch 5498: train loss: 6.784897323086625e-06, val loss: 0.08777183294296265\n",
      "Epoch 5499: train loss: 7.288774668268161e-06, val loss: 0.08765822649002075\n",
      "Epoch 5500: train loss: 5.18707929586526e-06, val loss: 0.08788025379180908\n",
      "Epoch 5501: train loss: 5.239064648776548e-06, val loss: 0.08834471553564072\n",
      "Epoch 5502: train loss: 5.645648343488574e-06, val loss: 0.08848494291305542\n",
      "Epoch 5503: train loss: 3.7200506994850002e-06, val loss: 0.08802296966314316\n",
      "Epoch 5504: train loss: 4.380789960123366e-06, val loss: 0.08786538988351822\n",
      "Epoch 5505: train loss: 2.609941020637052e-06, val loss: 0.08806031197309494\n",
      "Epoch 5506: train loss: 3.902810931322165e-06, val loss: 0.08832459896802902\n",
      "Epoch 5507: train loss: 2.8425756681826897e-06, val loss: 0.08838208764791489\n",
      "Epoch 5508: train loss: 3.321417807455873e-06, val loss: 0.08814752846956253\n",
      "Epoch 5509: train loss: 2.109166189256939e-06, val loss: 0.08758527040481567\n",
      "Epoch 5510: train loss: 3.7594479636027245e-06, val loss: 0.08831453323364258\n",
      "Epoch 5511: train loss: 5.072791736893123e-06, val loss: 0.08762097358703613\n",
      "Epoch 5512: train loss: 7.282830665644724e-06, val loss: 0.08812413364648819\n",
      "Epoch 5513: train loss: 1.9531701127561973e-06, val loss: 0.08865928649902344\n",
      "Epoch 5514: train loss: 6.448819931392791e-06, val loss: 0.08770962804555893\n",
      "Epoch 5515: train loss: 2.623328555273474e-06, val loss: 0.08731162548065186\n",
      "Epoch 5516: train loss: 5.263152161205653e-06, val loss: 0.08788037300109863\n",
      "Epoch 5517: train loss: 1.9288816019980004e-06, val loss: 0.08840859681367874\n",
      "Epoch 5518: train loss: 3.995413862867281e-06, val loss: 0.08752591162919998\n",
      "Epoch 5519: train loss: 1.1950219231948722e-06, val loss: 0.08696895092725754\n",
      "Epoch 5520: train loss: 2.7715332180378027e-06, val loss: 0.0875319242477417\n",
      "Epoch 5521: train loss: 1.1166057447553612e-06, val loss: 0.08788060396909714\n",
      "Epoch 5522: train loss: 2.397695880063111e-06, val loss: 0.08743568509817123\n",
      "Epoch 5523: train loss: 1.5484317827940686e-06, val loss: 0.0869787260890007\n",
      "Epoch 5524: train loss: 2.1699204353353707e-06, val loss: 0.08741001039743423\n",
      "Epoch 5525: train loss: 1.4458382793236524e-06, val loss: 0.08758077770471573\n",
      "Epoch 5526: train loss: 1.6979985275611398e-06, val loss: 0.08745380491018295\n",
      "Epoch 5527: train loss: 1.192677473227377e-06, val loss: 0.08664780110120773\n",
      "Epoch 5528: train loss: 1.1100469237135258e-06, val loss: 0.08683633804321289\n",
      "Epoch 5529: train loss: 8.059662377490895e-07, val loss: 0.0859077200293541\n",
      "Epoch 5530: train loss: 1.0294954790879274e-06, val loss: 0.08770588040351868\n",
      "Epoch 5531: train loss: 3.998421561846044e-06, val loss: 0.08533161878585815\n",
      "Epoch 5532: train loss: 2.571558434283361e-05, val loss: 0.08939432352781296\n",
      "Epoch 5533: train loss: 0.00010044538066722453, val loss: 0.08806658536195755\n",
      "Epoch 5534: train loss: 0.0001736870763124898, val loss: 0.08966033160686493\n",
      "Epoch 5535: train loss: 0.00029459616052918136, val loss: 0.08792443573474884\n",
      "Epoch 5536: train loss: 0.0002928712056018412, val loss: 0.08953982591629028\n",
      "Epoch 5537: train loss: 8.795619942247868e-05, val loss: 0.09076123684644699\n",
      "Epoch 5538: train loss: 0.00011181645095348358, val loss: 0.08880813419818878\n",
      "Epoch 5539: train loss: 0.00011983393778791651, val loss: 0.08828476816415787\n",
      "Epoch 5540: train loss: 7.036806346150115e-05, val loss: 0.09048745036125183\n",
      "Epoch 5541: train loss: 8.72799355420284e-05, val loss: 0.09104300290346146\n",
      "Epoch 5542: train loss: 5.536621392820962e-05, val loss: 0.08847290277481079\n",
      "Epoch 5543: train loss: 6.330997712211683e-05, val loss: 0.08752986788749695\n",
      "Epoch 5544: train loss: 4.7257988626370206e-05, val loss: 0.08993125706911087\n",
      "Epoch 5545: train loss: 4.60151641163975e-05, val loss: 0.09050356596708298\n",
      "Epoch 5546: train loss: 4.063135929754935e-05, val loss: 0.08786085993051529\n",
      "Epoch 5547: train loss: 3.367379031260498e-05, val loss: 0.08763261884450912\n",
      "Epoch 5548: train loss: 3.483645195956342e-05, val loss: 0.09002786129713058\n",
      "Epoch 5549: train loss: 2.499832771718502e-05, val loss: 0.08977022022008896\n",
      "Epoch 5550: train loss: 2.956432945211418e-05, val loss: 0.0873093381524086\n",
      "Epoch 5551: train loss: 1.851777960837353e-05, val loss: 0.08698786795139313\n",
      "Epoch 5552: train loss: 2.5532661311444826e-05, val loss: 0.08842523396015167\n",
      "Epoch 5553: train loss: 1.348288424196653e-05, val loss: 0.08751826733350754\n",
      "Epoch 5554: train loss: 2.1415766241261736e-05, val loss: 0.08656328916549683\n",
      "Epoch 5555: train loss: 1.0918031875917222e-05, val loss: 0.08671249449253082\n",
      "Epoch 5556: train loss: 1.6517686162842438e-05, val loss: 0.08648763597011566\n",
      "Epoch 5557: train loss: 1.0701475730456877e-05, val loss: 0.0862237885594368\n",
      "Epoch 5558: train loss: 1.1044360689993482e-05, val loss: 0.08660491555929184\n",
      "Epoch 5559: train loss: 1.1045562132494524e-05, val loss: 0.0863901674747467\n",
      "Epoch 5560: train loss: 7.047957751638023e-06, val loss: 0.08584460616111755\n",
      "Epoch 5561: train loss: 1.0273993211740162e-05, val loss: 0.08612840622663498\n",
      "Epoch 5562: train loss: 6.071808002161561e-06, val loss: 0.08659423887729645\n",
      "Epoch 5563: train loss: 7.482865385100013e-06, val loss: 0.08577512949705124\n",
      "Epoch 5564: train loss: 6.77469142829068e-06, val loss: 0.08373037725687027\n",
      "Epoch 5565: train loss: 7.017061761871446e-06, val loss: 0.08309247344732285\n",
      "Epoch 5566: train loss: 3.1413358101417543e-06, val loss: 0.0835014060139656\n",
      "Epoch 5567: train loss: 6.466616014222382e-06, val loss: 0.0838392898440361\n",
      "Epoch 5568: train loss: 4.2413175833644345e-06, val loss: 0.08330412954092026\n",
      "Epoch 5569: train loss: 4.173962679487886e-06, val loss: 0.08308785408735275\n",
      "Epoch 5570: train loss: 2.8274300802877406e-06, val loss: 0.08364276587963104\n",
      "Epoch 5571: train loss: 3.124044269497972e-06, val loss: 0.0836484357714653\n",
      "Epoch 5572: train loss: 2.4721857698750682e-06, val loss: 0.08323349803686142\n",
      "Epoch 5573: train loss: 1.8778325738821877e-06, val loss: 0.08326297253370285\n",
      "Epoch 5574: train loss: 2.547441908973269e-06, val loss: 0.08352606743574142\n",
      "Epoch 5575: train loss: 1.2211329476485844e-06, val loss: 0.08412700891494751\n",
      "Epoch 5576: train loss: 2.0694378690677695e-06, val loss: 0.08469825237989426\n",
      "Epoch 5577: train loss: 1.145876126429357e-06, val loss: 0.0847490206360817\n",
      "Epoch 5578: train loss: 1.4590358432542416e-06, val loss: 0.08496107906103134\n",
      "Epoch 5579: train loss: 1.1642124491118011e-06, val loss: 0.08478567749261856\n",
      "Epoch 5580: train loss: 1.0305703881385853e-06, val loss: 0.08463682979345322\n",
      "Epoch 5581: train loss: 1.054086283147626e-06, val loss: 0.08489765971899033\n",
      "Epoch 5582: train loss: 8.408927669734112e-07, val loss: 0.08476614207029343\n",
      "Epoch 5583: train loss: 7.889741482358659e-07, val loss: 0.0845026820898056\n",
      "Epoch 5584: train loss: 7.901289222900232e-07, val loss: 0.08479149639606476\n",
      "Epoch 5585: train loss: 6.661616680503357e-07, val loss: 0.08483166992664337\n",
      "Epoch 5586: train loss: 5.330064709596627e-07, val loss: 0.08451521396636963\n",
      "Epoch 5587: train loss: 6.882287948428711e-07, val loss: 0.0845755785703659\n",
      "Epoch 5588: train loss: 4.795064114659908e-07, val loss: 0.08466198295354843\n",
      "Epoch 5589: train loss: 4.2822529167096945e-07, val loss: 0.084697425365448\n",
      "Epoch 5590: train loss: 4.878392019236344e-07, val loss: 0.08465903997421265\n",
      "Epoch 5591: train loss: 5.388637305259181e-07, val loss: 0.0842994675040245\n",
      "Epoch 5592: train loss: 3.7245700923449476e-07, val loss: 0.08461824804544449\n",
      "Epoch 5593: train loss: 2.835446082372073e-07, val loss: 0.08477473258972168\n",
      "Epoch 5594: train loss: 3.0321271538014116e-07, val loss: 0.08434289693832397\n",
      "Epoch 5595: train loss: 2.2687103751195536e-07, val loss: 0.08451417833566666\n",
      "Epoch 5596: train loss: 3.7763675209134817e-07, val loss: 0.08487152308225632\n",
      "Epoch 5597: train loss: 1.3654774875249132e-06, val loss: 0.08490843325853348\n",
      "Epoch 5598: train loss: 5.448635420179926e-06, val loss: 0.08399158716201782\n",
      "Epoch 5599: train loss: 4.904772140434943e-06, val loss: 0.08630844205617905\n",
      "Epoch 5600: train loss: 3.220492089894833e-06, val loss: 0.0855288878083229\n",
      "Epoch 5601: train loss: 5.603695626632543e-06, val loss: 0.08611562103033066\n",
      "Epoch 5602: train loss: 2.0001621123810764e-06, val loss: 0.08572252094745636\n",
      "Epoch 5603: train loss: 1.5528263475061976e-06, val loss: 0.08578242361545563\n",
      "Epoch 5604: train loss: 2.860336508092587e-06, val loss: 0.08631757646799088\n",
      "Epoch 5605: train loss: 1.6270257674477762e-06, val loss: 0.08524269610643387\n",
      "Epoch 5606: train loss: 2.6830675778910518e-06, val loss: 0.08608425408601761\n",
      "Epoch 5607: train loss: 2.8013876089971745e-06, val loss: 0.08563470840454102\n",
      "Epoch 5608: train loss: 2.96370058094908e-06, val loss: 0.08624263852834702\n",
      "Epoch 5609: train loss: 2.89233594230609e-06, val loss: 0.0852462649345398\n",
      "Epoch 5610: train loss: 2.664736030055792e-06, val loss: 0.08655565232038498\n",
      "Epoch 5611: train loss: 2.6654097382561304e-06, val loss: 0.08555930107831955\n",
      "Epoch 5612: train loss: 2.550527369749034e-06, val loss: 0.08634646236896515\n",
      "Epoch 5613: train loss: 2.5880729026539484e-06, val loss: 0.08559129387140274\n",
      "Epoch 5614: train loss: 2.8121628474764293e-06, val loss: 0.08696233481168747\n",
      "Epoch 5615: train loss: 3.248502935093711e-06, val loss: 0.08528615534305573\n",
      "Epoch 5616: train loss: 3.982459020335227e-06, val loss: 0.08687285333871841\n",
      "Epoch 5617: train loss: 5.782967946288409e-06, val loss: 0.08520640432834625\n",
      "Epoch 5618: train loss: 9.2651152954204e-06, val loss: 0.08740140497684479\n",
      "Epoch 5619: train loss: 1.4119388652034104e-05, val loss: 0.08433928340673447\n",
      "Epoch 5620: train loss: 2.5031768018379807e-05, val loss: 0.08849581331014633\n",
      "Epoch 5621: train loss: 3.9139893488027155e-05, val loss: 0.08406480401754379\n",
      "Epoch 5622: train loss: 5.839895675308071e-05, val loss: 0.08935743570327759\n",
      "Epoch 5623: train loss: 5.799860446131788e-05, val loss: 0.08576834201812744\n",
      "Epoch 5624: train loss: 4.124168481212109e-05, val loss: 0.08897814899682999\n",
      "Epoch 5625: train loss: 1.3534153367800172e-05, val loss: 0.08804748952388763\n",
      "Epoch 5626: train loss: 2.96969733426522e-06, val loss: 0.08750096708536148\n",
      "Epoch 5627: train loss: 1.2696547855739482e-05, val loss: 0.08984916657209396\n",
      "Epoch 5628: train loss: 2.330085408175364e-05, val loss: 0.08593034744262695\n",
      "Epoch 5629: train loss: 3.511223985697143e-05, val loss: 0.09123940765857697\n",
      "Epoch 5630: train loss: 6.212807056726888e-05, val loss: 0.0872635617852211\n",
      "Epoch 5631: train loss: 2.057857454929035e-05, val loss: 0.08654230087995529\n",
      "Epoch 5632: train loss: 2.122683872585185e-05, val loss: 0.08839031308889389\n",
      "Epoch 5633: train loss: 2.852354919014033e-05, val loss: 0.08689560741186142\n",
      "Epoch 5634: train loss: 2.1880374333704822e-05, val loss: 0.08659594506025314\n",
      "Epoch 5635: train loss: 1.949106626852881e-05, val loss: 0.08583793044090271\n",
      "Epoch 5636: train loss: 1.5493042155867442e-05, val loss: 0.0857614055275917\n",
      "Epoch 5637: train loss: 1.4895338608766906e-05, val loss: 0.08615106344223022\n",
      "Epoch 5638: train loss: 1.560884811624419e-05, val loss: 0.08571865409612656\n",
      "Epoch 5639: train loss: 1.3201854926592205e-05, val loss: 0.08613202720880508\n",
      "Epoch 5640: train loss: 9.27081418922171e-06, val loss: 0.08528424799442291\n",
      "Epoch 5641: train loss: 1.0788745385070797e-05, val loss: 0.08463259041309357\n",
      "Epoch 5642: train loss: 1.100423378375126e-05, val loss: 0.08593535423278809\n",
      "Epoch 5643: train loss: 7.208802799141267e-06, val loss: 0.08558361977338791\n",
      "Epoch 5644: train loss: 8.450367204204667e-06, val loss: 0.08481069654226303\n",
      "Epoch 5645: train loss: 5.297162260831101e-06, val loss: 0.08477268368005753\n",
      "Epoch 5646: train loss: 7.072781954775564e-06, val loss: 0.08510761708021164\n",
      "Epoch 5647: train loss: 6.879963166284142e-06, val loss: 0.08537548035383224\n",
      "Epoch 5648: train loss: 3.7991051158314804e-06, val loss: 0.08447783440351486\n",
      "Epoch 5649: train loss: 4.554355200525606e-06, val loss: 0.084785595536232\n",
      "Epoch 5650: train loss: 4.018160780105973e-06, val loss: 0.08526737987995148\n",
      "Epoch 5651: train loss: 4.779134997079382e-06, val loss: 0.08450642973184586\n",
      "Epoch 5652: train loss: 4.130231445742538e-06, val loss: 0.08504330366849899\n",
      "Epoch 5653: train loss: 3.125419652860728e-06, val loss: 0.08473965525627136\n",
      "Epoch 5654: train loss: 1.914203267006087e-06, val loss: 0.08441086858510971\n",
      "Epoch 5655: train loss: 2.7589396722760284e-06, val loss: 0.08518581092357635\n",
      "Epoch 5656: train loss: 2.4509733975719428e-06, val loss: 0.08481966704130173\n",
      "Epoch 5657: train loss: 3.0181847705534892e-06, val loss: 0.08442600071430206\n",
      "Epoch 5658: train loss: 3.066485760427895e-06, val loss: 0.08464879542589188\n",
      "Epoch 5659: train loss: 1.9792810235230718e-06, val loss: 0.0850134789943695\n",
      "Epoch 5660: train loss: 2.1277064661262557e-06, val loss: 0.0844271332025528\n",
      "Epoch 5661: train loss: 1.8557726662038476e-06, val loss: 0.08416210860013962\n",
      "Epoch 5662: train loss: 1.5977489056240302e-06, val loss: 0.0848434641957283\n",
      "Epoch 5663: train loss: 3.006610086231376e-06, val loss: 0.08374636620283127\n",
      "Epoch 5664: train loss: 1.2531563697848469e-05, val loss: 0.08536338806152344\n",
      "Epoch 5665: train loss: 2.3426671759807505e-05, val loss: 0.08404289186000824\n",
      "Epoch 5666: train loss: 3.518993253237568e-05, val loss: 0.08593285083770752\n",
      "Epoch 5667: train loss: 5.0443039071979e-05, val loss: 0.08434043079614639\n",
      "Epoch 5668: train loss: 6.472522363765165e-05, val loss: 0.08630208671092987\n",
      "Epoch 5669: train loss: 6.553436833200976e-05, val loss: 0.08477161079645157\n",
      "Epoch 5670: train loss: 5.946575402049348e-05, val loss: 0.09031670540571213\n",
      "Epoch 5671: train loss: 7.405025098705664e-05, val loss: 0.08529239892959595\n",
      "Epoch 5672: train loss: 7.515383185818791e-05, val loss: 0.08826880902051926\n",
      "Epoch 5673: train loss: 5.660594251821749e-05, val loss: 0.08869623392820358\n",
      "Epoch 5674: train loss: 5.69264120713342e-05, val loss: 0.08742039650678635\n",
      "Epoch 5675: train loss: 5.697952292393893e-05, val loss: 0.08775802701711655\n",
      "Epoch 5676: train loss: 5.078162212157622e-05, val loss: 0.08892057090997696\n",
      "Epoch 5677: train loss: 4.2331441363785416e-05, val loss: 0.0874156579375267\n",
      "Epoch 5678: train loss: 3.1569066777592525e-05, val loss: 0.08810573816299438\n",
      "Epoch 5679: train loss: 1.5634424926247448e-05, val loss: 0.0886290892958641\n",
      "Epoch 5680: train loss: 1.744424480421003e-05, val loss: 0.08777300268411636\n",
      "Epoch 5681: train loss: 3.0127856007311493e-05, val loss: 0.08871761709451675\n",
      "Epoch 5682: train loss: 3.157517858198844e-05, val loss: 0.08800356090068817\n",
      "Epoch 5683: train loss: 2.406389103271067e-05, val loss: 0.08804737031459808\n",
      "Epoch 5684: train loss: 9.836080607783515e-06, val loss: 0.08864983171224594\n",
      "Epoch 5685: train loss: 8.372427146241535e-06, val loss: 0.08822139352560043\n",
      "Epoch 5686: train loss: 1.590819374541752e-05, val loss: 0.0884806290268898\n",
      "Epoch 5687: train loss: 1.8924110918305814e-05, val loss: 0.08861440420150757\n",
      "Epoch 5688: train loss: 1.3147390745871235e-05, val loss: 0.08844905346632004\n",
      "Epoch 5689: train loss: 5.186621365282917e-06, val loss: 0.08885849267244339\n",
      "Epoch 5690: train loss: 4.956752945872722e-06, val loss: 0.0890350267291069\n",
      "Epoch 5691: train loss: 7.680494491069112e-06, val loss: 0.08922063559293747\n",
      "Epoch 5692: train loss: 9.57036445470294e-06, val loss: 0.08927897363901138\n",
      "Epoch 5693: train loss: 6.755603408237221e-06, val loss: 0.0890771672129631\n",
      "Epoch 5694: train loss: 4.0193026507040486e-06, val loss: 0.08986707031726837\n",
      "Epoch 5695: train loss: 5.337820311979158e-06, val loss: 0.08965253829956055\n",
      "Epoch 5696: train loss: 5.674417934642406e-06, val loss: 0.08942688256502151\n",
      "Epoch 5697: train loss: 6.255151220102562e-06, val loss: 0.09042906016111374\n",
      "Epoch 5698: train loss: 4.473378794500604e-06, val loss: 0.08978032320737839\n",
      "Epoch 5699: train loss: 2.5729295884957537e-06, val loss: 0.09017522633075714\n",
      "Epoch 5700: train loss: 2.5884196475089993e-06, val loss: 0.09072453528642654\n",
      "Epoch 5701: train loss: 2.9596608328574803e-06, val loss: 0.08973254263401031\n",
      "Epoch 5702: train loss: 4.409581833897391e-06, val loss: 0.09134277701377869\n",
      "Epoch 5703: train loss: 6.735548595315777e-06, val loss: 0.08962205797433853\n",
      "Epoch 5704: train loss: 9.863771083473694e-06, val loss: 0.09115489572286606\n",
      "Epoch 5705: train loss: 1.5127814549487084e-05, val loss: 0.09041117876768112\n",
      "Epoch 5706: train loss: 2.1040506908320822e-05, val loss: 0.09084814786911011\n",
      "Epoch 5707: train loss: 2.579838655947242e-05, val loss: 0.0910470262169838\n",
      "Epoch 5708: train loss: 3.1556370231555775e-05, val loss: 0.09141119569540024\n",
      "Epoch 5709: train loss: 4.067227928317152e-05, val loss: 0.09106045961380005\n",
      "Epoch 5710: train loss: 5.9748534113168716e-05, val loss: 0.09252270311117172\n",
      "Epoch 5711: train loss: 0.0001036756148096174, val loss: 0.09126686304807663\n",
      "Epoch 5712: train loss: 0.00019474087457638234, val loss: 0.09423261135816574\n",
      "Epoch 5713: train loss: 0.00033138226717710495, val loss: 0.09197544306516647\n",
      "Epoch 5714: train loss: 0.000443896627984941, val loss: 0.09053404629230499\n",
      "Epoch 5715: train loss: 0.00045625882921740413, val loss: 0.09539946168661118\n",
      "Epoch 5716: train loss: 0.0002649125817697495, val loss: 0.0887068510055542\n",
      "Epoch 5717: train loss: 7.966275734361261e-05, val loss: 0.08985743671655655\n",
      "Epoch 5718: train loss: 9.52477494138293e-05, val loss: 0.0911753848195076\n",
      "Epoch 5719: train loss: 0.0001353341358480975, val loss: 0.08898214995861053\n",
      "Epoch 5720: train loss: 9.347806189907715e-05, val loss: 0.09084203839302063\n",
      "Epoch 5721: train loss: 6.864478928036988e-05, val loss: 0.0897553488612175\n",
      "Epoch 5722: train loss: 6.0023157857358456e-05, val loss: 0.0886833593249321\n",
      "Epoch 5723: train loss: 6.08940827078186e-05, val loss: 0.0896507129073143\n",
      "Epoch 5724: train loss: 5.3012485295766965e-05, val loss: 0.0875537171959877\n",
      "Epoch 5725: train loss: 3.641233706730418e-05, val loss: 0.08716966211795807\n",
      "Epoch 5726: train loss: 4.223273936077021e-05, val loss: 0.088520847260952\n",
      "Epoch 5727: train loss: 3.240670048398897e-05, val loss: 0.08666374534368515\n",
      "Epoch 5728: train loss: 2.962867984024342e-05, val loss: 0.08609264343976974\n",
      "Epoch 5729: train loss: 2.395605406491086e-05, val loss: 0.08753710240125656\n",
      "Epoch 5730: train loss: 2.7006968593923375e-05, val loss: 0.08612896502017975\n",
      "Epoch 5731: train loss: 1.91042145161191e-05, val loss: 0.0851076990365982\n",
      "Epoch 5732: train loss: 2.0426468836376444e-05, val loss: 0.08630377799272537\n",
      "Epoch 5733: train loss: 1.8829527107300237e-05, val loss: 0.08534914255142212\n",
      "Epoch 5734: train loss: 1.3591116839961614e-05, val loss: 0.08426477015018463\n",
      "Epoch 5735: train loss: 1.383310245728353e-05, val loss: 0.0852213054895401\n",
      "Epoch 5736: train loss: 1.2069271178916097e-05, val loss: 0.08493130654096603\n",
      "Epoch 5737: train loss: 8.743991202209145e-06, val loss: 0.08387976884841919\n",
      "Epoch 5738: train loss: 1.1574332347663585e-05, val loss: 0.08487812429666519\n",
      "Epoch 5739: train loss: 8.023680493352003e-06, val loss: 0.08514361828565598\n",
      "Epoch 5740: train loss: 8.10983055998804e-06, val loss: 0.08368388563394547\n",
      "Epoch 5741: train loss: 8.05576655693585e-06, val loss: 0.08408179879188538\n",
      "Epoch 5742: train loss: 5.499398412212031e-06, val loss: 0.08499827235937119\n",
      "Epoch 5743: train loss: 8.019629603950307e-06, val loss: 0.08403712511062622\n",
      "Epoch 5744: train loss: 1.2502412573667243e-05, val loss: 0.08338581770658493\n",
      "Epoch 5745: train loss: 3.5613942600321025e-05, val loss: 0.08438204973936081\n",
      "Epoch 5746: train loss: 7.121985254343599e-05, val loss: 0.08394870907068253\n",
      "Epoch 5747: train loss: 4.678515688283369e-05, val loss: 0.08334112167358398\n",
      "Epoch 5748: train loss: 6.543129074998433e-06, val loss: 0.08429612219333649\n",
      "Epoch 5749: train loss: 3.9109258068492636e-05, val loss: 0.084433913230896\n",
      "Epoch 5750: train loss: 4.4039559725206345e-05, val loss: 0.08349521458148956\n",
      "Epoch 5751: train loss: 6.019402007950703e-06, val loss: 0.08401655405759811\n",
      "Epoch 5752: train loss: 2.2494752556667663e-05, val loss: 0.08437413722276688\n",
      "Epoch 5753: train loss: 3.194277087459341e-05, val loss: 0.0840955600142479\n",
      "Epoch 5754: train loss: 6.20504670223454e-06, val loss: 0.08439674973487854\n",
      "Epoch 5755: train loss: 1.5134359273361042e-05, val loss: 0.08413667231798172\n",
      "Epoch 5756: train loss: 2.1203008145675994e-05, val loss: 0.08376336842775345\n",
      "Epoch 5757: train loss: 6.276807653193828e-06, val loss: 0.08449435234069824\n",
      "Epoch 5758: train loss: 1.0321015906811226e-05, val loss: 0.08427738398313522\n",
      "Epoch 5759: train loss: 1.4671347344119567e-05, val loss: 0.08353544026613235\n",
      "Epoch 5760: train loss: 5.74526393393171e-06, val loss: 0.08416467159986496\n",
      "Epoch 5761: train loss: 6.952008334337734e-06, val loss: 0.08418402820825577\n",
      "Epoch 5762: train loss: 1.084035920939641e-05, val loss: 0.08366214483976364\n",
      "Epoch 5763: train loss: 4.324084784457227e-06, val loss: 0.08405022323131561\n",
      "Epoch 5764: train loss: 5.286623945721658e-06, val loss: 0.08373840153217316\n",
      "Epoch 5765: train loss: 7.66705852583982e-06, val loss: 0.08369334042072296\n",
      "Epoch 5766: train loss: 3.2497687243449036e-06, val loss: 0.08411421626806259\n",
      "Epoch 5767: train loss: 4.1641674215497915e-06, val loss: 0.08336105942726135\n",
      "Epoch 5768: train loss: 5.536602202482754e-06, val loss: 0.08358651399612427\n",
      "Epoch 5769: train loss: 2.544142262195237e-06, val loss: 0.08408508449792862\n",
      "Epoch 5770: train loss: 3.1057486467034323e-06, val loss: 0.08315353840589523\n",
      "Epoch 5771: train loss: 4.404167157190386e-06, val loss: 0.08367140591144562\n",
      "Epoch 5772: train loss: 2.0820179997826926e-06, val loss: 0.08394491672515869\n",
      "Epoch 5773: train loss: 2.070933987852186e-06, val loss: 0.08324167132377625\n",
      "Epoch 5774: train loss: 3.6415899558051024e-06, val loss: 0.08378029614686966\n",
      "Epoch 5775: train loss: 2.5310337150585838e-06, val loss: 0.08383606374263763\n",
      "Epoch 5776: train loss: 1.3272576779854717e-06, val loss: 0.08351266384124756\n",
      "Epoch 5777: train loss: 2.141617414963548e-06, val loss: 0.08377766609191895\n",
      "Epoch 5778: train loss: 2.3915879410196794e-06, val loss: 0.08406832069158554\n",
      "Epoch 5779: train loss: 1.4597974313801387e-06, val loss: 0.08318690955638885\n",
      "Epoch 5780: train loss: 2.1783948795928154e-06, val loss: 0.0843186005949974\n",
      "Epoch 5781: train loss: 3.2810364700708305e-06, val loss: 0.08327539265155792\n",
      "Epoch 5782: train loss: 3.246260121159139e-06, val loss: 0.08437363803386688\n",
      "Epoch 5783: train loss: 3.928824753529625e-06, val loss: 0.08327385038137436\n",
      "Epoch 5784: train loss: 6.910728188813664e-06, val loss: 0.08441760390996933\n",
      "Epoch 5785: train loss: 1.0833211490535177e-05, val loss: 0.08320355415344238\n",
      "Epoch 5786: train loss: 1.5980091120582074e-05, val loss: 0.08530644327402115\n",
      "Epoch 5787: train loss: 2.6058090952574275e-05, val loss: 0.08203848451375961\n",
      "Epoch 5788: train loss: 4.369304588180967e-05, val loss: 0.08631002902984619\n",
      "Epoch 5789: train loss: 6.258898793021217e-05, val loss: 0.08158092945814133\n",
      "Epoch 5790: train loss: 8.58235580381006e-05, val loss: 0.08588355034589767\n",
      "Epoch 5791: train loss: 8.667205838719383e-05, val loss: 0.0831814631819725\n",
      "Epoch 5792: train loss: 6.358794053085148e-05, val loss: 0.08505287021398544\n",
      "Epoch 5793: train loss: 2.423195655865129e-05, val loss: 0.08611352741718292\n",
      "Epoch 5794: train loss: 9.250639777746983e-06, val loss: 0.08487313240766525\n",
      "Epoch 5795: train loss: 2.4044711608439684e-05, val loss: 0.0872398242354393\n",
      "Epoch 5796: train loss: 4.005707523901947e-05, val loss: 0.0862552672624588\n",
      "Epoch 5797: train loss: 5.1246548537164927e-05, val loss: 0.08650752156972885\n",
      "Epoch 5798: train loss: 9.319913806393743e-05, val loss: 0.08793605864048004\n",
      "Epoch 5799: train loss: 0.00020853809837717563, val loss: 0.08607102930545807\n",
      "Epoch 5800: train loss: 0.00045649809180758893, val loss: 0.08712242543697357\n",
      "Epoch 5801: train loss: 0.0003276714705862105, val loss: 0.08768114447593689\n",
      "Epoch 5802: train loss: 8.353278826689348e-05, val loss: 0.0870959684252739\n",
      "Epoch 5803: train loss: 0.00019026300287805498, val loss: 0.08922398090362549\n",
      "Epoch 5804: train loss: 0.0001702041772659868, val loss: 0.08902068436145782\n",
      "Epoch 5805: train loss: 0.000105305080069229, val loss: 0.08983466029167175\n",
      "Epoch 5806: train loss: 0.00010139308869838715, val loss: 0.09170442074537277\n",
      "Epoch 5807: train loss: 9.210858115693554e-05, val loss: 0.09030970185995102\n",
      "Epoch 5808: train loss: 8.542428986402228e-05, val loss: 0.08998868614435196\n",
      "Epoch 5809: train loss: 5.5695491028018296e-05, val loss: 0.09246422350406647\n",
      "Epoch 5810: train loss: 7.600102253491059e-05, val loss: 0.09231749176979065\n",
      "Epoch 5811: train loss: 4.3881045712623745e-05, val loss: 0.09125334024429321\n",
      "Epoch 5812: train loss: 5.49154901818838e-05, val loss: 0.09234589338302612\n",
      "Epoch 5813: train loss: 3.824646046268754e-05, val loss: 0.09362372010946274\n",
      "Epoch 5814: train loss: 4.1009654523804784e-05, val loss: 0.09330146759748459\n",
      "Epoch 5815: train loss: 3.295997157692909e-05, val loss: 0.09299267083406448\n",
      "Epoch 5816: train loss: 2.961132486234419e-05, val loss: 0.09359626471996307\n",
      "Epoch 5817: train loss: 2.9053928301436827e-05, val loss: 0.093973807990551\n",
      "Epoch 5818: train loss: 2.2089927369961515e-05, val loss: 0.09411277621984482\n",
      "Epoch 5819: train loss: 2.258566382806748e-05, val loss: 0.0944828912615776\n",
      "Epoch 5820: train loss: 2.0018094801343977e-05, val loss: 0.09428893774747849\n",
      "Epoch 5821: train loss: 1.531785528641194e-05, val loss: 0.0940757691860199\n",
      "Epoch 5822: train loss: 1.8298089344170876e-05, val loss: 0.09477683156728745\n",
      "Epoch 5823: train loss: 1.0728602319431957e-05, val loss: 0.09515587985515594\n",
      "Epoch 5824: train loss: 1.540837183711119e-05, val loss: 0.09466079622507095\n",
      "Epoch 5825: train loss: 8.398921636398882e-06, val loss: 0.09470260143280029\n",
      "Epoch 5826: train loss: 1.2380890439089853e-05, val loss: 0.09553360193967819\n",
      "Epoch 5827: train loss: 7.115892003639601e-06, val loss: 0.09552961587905884\n",
      "Epoch 5828: train loss: 9.21739683690248e-06, val loss: 0.09490562230348587\n",
      "Epoch 5829: train loss: 6.763982582924655e-06, val loss: 0.09530240297317505\n",
      "Epoch 5830: train loss: 6.598943855351536e-06, val loss: 0.09599461406469345\n",
      "Epoch 5831: train loss: 5.904683348489925e-06, val loss: 0.0957932248711586\n",
      "Epoch 5832: train loss: 5.441272151074372e-06, val loss: 0.0954926535487175\n",
      "Epoch 5833: train loss: 4.5229835450300016e-06, val loss: 0.09568997472524643\n",
      "Epoch 5834: train loss: 4.73444742965512e-06, val loss: 0.0958978459239006\n",
      "Epoch 5835: train loss: 3.7792087823618203e-06, val loss: 0.09597337245941162\n",
      "Epoch 5836: train loss: 3.647309540610877e-06, val loss: 0.09591422230005264\n",
      "Epoch 5837: train loss: 3.04643549497996e-06, val loss: 0.09588874876499176\n",
      "Epoch 5838: train loss: 3.3417431950510945e-06, val loss: 0.09622755646705627\n",
      "Epoch 5839: train loss: 2.0384359231684357e-06, val loss: 0.09622281789779663\n",
      "Epoch 5840: train loss: 3.0096853151917458e-06, val loss: 0.09574321657419205\n",
      "Epoch 5841: train loss: 1.6007453496058588e-06, val loss: 0.09590914100408554\n",
      "Epoch 5842: train loss: 2.3489835712098284e-06, val loss: 0.09635552018880844\n",
      "Epoch 5843: train loss: 1.4916378177076695e-06, val loss: 0.09600997716188431\n",
      "Epoch 5844: train loss: 1.9178096408722922e-06, val loss: 0.09575439244508743\n",
      "Epoch 5845: train loss: 1.209759716402914e-06, val loss: 0.09612485021352768\n",
      "Epoch 5846: train loss: 1.3947631032351637e-06, val loss: 0.09619797766208649\n",
      "Epoch 5847: train loss: 1.241615905200888e-06, val loss: 0.0958937332034111\n",
      "Epoch 5848: train loss: 1.1016319376722095e-06, val loss: 0.09596206247806549\n",
      "Epoch 5849: train loss: 9.09688765204919e-07, val loss: 0.0961897000670433\n",
      "Epoch 5850: train loss: 9.618951253287378e-07, val loss: 0.09595383703708649\n",
      "Epoch 5851: train loss: 7.898880198808911e-07, val loss: 0.0958213284611702\n",
      "Epoch 5852: train loss: 8.277492042907397e-07, val loss: 0.09604113548994064\n",
      "Epoch 5853: train loss: 6.117809903116722e-07, val loss: 0.09603428840637207\n",
      "Epoch 5854: train loss: 6.534870067298471e-07, val loss: 0.09581972658634186\n",
      "Epoch 5855: train loss: 6.411614208445826e-07, val loss: 0.09602262079715729\n",
      "Epoch 5856: train loss: 6.495466777778347e-07, val loss: 0.09581082314252853\n",
      "Epoch 5857: train loss: 7.759060167700227e-07, val loss: 0.0959007516503334\n",
      "Epoch 5858: train loss: 1.1626995046754018e-06, val loss: 0.09564397484064102\n",
      "Epoch 5859: train loss: 2.896621481340844e-06, val loss: 0.09624501317739487\n",
      "Epoch 5860: train loss: 7.305894541786984e-06, val loss: 0.09530806541442871\n",
      "Epoch 5861: train loss: 2.2429598175222054e-05, val loss: 0.09713072329759598\n",
      "Epoch 5862: train loss: 7.333955727517605e-05, val loss: 0.09382106363773346\n",
      "Epoch 5863: train loss: 0.0002461159019730985, val loss: 0.09789357334375381\n",
      "Epoch 5864: train loss: 0.000763207208365202, val loss: 0.09160327911376953\n",
      "Epoch 5865: train loss: 0.0014955538790673018, val loss: 0.09392846375703812\n",
      "Epoch 5866: train loss: 0.0016070998972281814, val loss: 0.09307397156953812\n",
      "Epoch 5867: train loss: 0.0002596986014395952, val loss: 0.08668079227209091\n",
      "Epoch 5868: train loss: 0.0007429344695992768, val loss: 0.07636558264493942\n",
      "Epoch 5869: train loss: 0.00047455509775318205, val loss: 0.081813745200634\n",
      "Epoch 5870: train loss: 0.0003613227454479784, val loss: 0.08323700726032257\n",
      "Epoch 5871: train loss: 0.0003411848738323897, val loss: 0.07539304345846176\n",
      "Epoch 5872: train loss: 0.0002381948143010959, val loss: 0.07099216431379318\n",
      "Epoch 5873: train loss: 0.00022994913160800934, val loss: 0.07387892156839371\n",
      "Epoch 5874: train loss: 0.00019993951718788594, val loss: 0.07754691690206528\n",
      "Epoch 5875: train loss: 0.00015696889022365212, val loss: 0.07660402357578278\n",
      "Epoch 5876: train loss: 0.0001425420050509274, val loss: 0.07356693595647812\n",
      "Epoch 5877: train loss: 0.00013571094314102083, val loss: 0.07200177758932114\n",
      "Epoch 5878: train loss: 9.690383012639359e-05, val loss: 0.07152736186981201\n",
      "Epoch 5879: train loss: 0.00011165986506966874, val loss: 0.07152008265256882\n",
      "Epoch 5880: train loss: 7.054451998556033e-05, val loss: 0.07274085283279419\n",
      "Epoch 5881: train loss: 8.60933359945193e-05, val loss: 0.07424180954694748\n",
      "Epoch 5882: train loss: 5.158091880730353e-05, val loss: 0.07359727472066879\n",
      "Epoch 5883: train loss: 7.272392394952476e-05, val loss: 0.0707130953669548\n",
      "Epoch 5884: train loss: 4.9155671149492264e-05, val loss: 0.06895389407873154\n",
      "Epoch 5885: train loss: 3.852006557281129e-05, val loss: 0.07013053447008133\n",
      "Epoch 5886: train loss: 4.503935997490771e-05, val loss: 0.07225679606199265\n",
      "Epoch 5887: train loss: 4.244426963850856e-05, val loss: 0.07261960953474045\n",
      "Epoch 5888: train loss: 2.5841882234090008e-05, val loss: 0.07161744683980942\n",
      "Epoch 5889: train loss: 2.9555714718298987e-05, val loss: 0.07111450284719467\n",
      "Epoch 5890: train loss: 2.8605159968719818e-05, val loss: 0.07139754295349121\n",
      "Epoch 5891: train loss: 2.4182572815334424e-05, val loss: 0.07128768414258957\n",
      "Epoch 5892: train loss: 1.945654184964951e-05, val loss: 0.0702209621667862\n",
      "Epoch 5893: train loss: 1.8709844880504534e-05, val loss: 0.06970721483230591\n",
      "Epoch 5894: train loss: 2.0807088731089607e-05, val loss: 0.07019059360027313\n",
      "Epoch 5895: train loss: 1.2412315300025512e-05, val loss: 0.07103968411684036\n",
      "Epoch 5896: train loss: 1.3125729310559109e-05, val loss: 0.07149636745452881\n",
      "Epoch 5897: train loss: 1.5737647117930464e-05, val loss: 0.07118763029575348\n",
      "Epoch 5898: train loss: 7.92490754975006e-06, val loss: 0.07035557180643082\n",
      "Epoch 5899: train loss: 1.1154443200211972e-05, val loss: 0.06957759708166122\n",
      "Epoch 5900: train loss: 8.058264029386919e-06, val loss: 0.06964825838804245\n",
      "Epoch 5901: train loss: 8.98574580787681e-06, val loss: 0.07035807520151138\n",
      "Epoch 5902: train loss: 6.796244633733295e-06, val loss: 0.070687435567379\n",
      "Epoch 5903: train loss: 5.655504537571687e-06, val loss: 0.07029213011264801\n",
      "Epoch 5904: train loss: 6.28401630820008e-06, val loss: 0.06987016648054123\n",
      "Epoch 5905: train loss: 5.202713055041386e-06, val loss: 0.07002025842666626\n",
      "Epoch 5906: train loss: 3.702939238792169e-06, val loss: 0.07022003084421158\n",
      "Epoch 5907: train loss: 4.269607870810432e-06, val loss: 0.06994743645191193\n",
      "Epoch 5908: train loss: 4.285666364012286e-06, val loss: 0.06954403221607208\n",
      "Epoch 5909: train loss: 2.5160584300465416e-06, val loss: 0.06953193247318268\n",
      "Epoch 5910: train loss: 3.009991587532568e-06, val loss: 0.06977885961532593\n",
      "Epoch 5911: train loss: 3.175583970005391e-06, val loss: 0.0698823481798172\n",
      "Epoch 5912: train loss: 2.0668862816819455e-06, val loss: 0.06979751586914062\n",
      "Epoch 5913: train loss: 2.3059355953591876e-06, val loss: 0.06963390111923218\n",
      "Epoch 5914: train loss: 1.874409008451039e-06, val loss: 0.06940582394599915\n",
      "Epoch 5915: train loss: 1.9715773760253796e-06, val loss: 0.06925150752067566\n",
      "Epoch 5916: train loss: 1.6425584590251674e-06, val loss: 0.06938391923904419\n",
      "Epoch 5917: train loss: 1.2186428648419678e-06, val loss: 0.06962447613477707\n",
      "Epoch 5918: train loss: 1.6288056485791458e-06, val loss: 0.06956853717565536\n",
      "Epoch 5919: train loss: 1.1107889577033347e-06, val loss: 0.06922420114278793\n",
      "Epoch 5920: train loss: 1.0456457175678224e-06, val loss: 0.0690581202507019\n",
      "Epoch 5921: train loss: 9.360555850435048e-07, val loss: 0.0692463144659996\n",
      "Epoch 5922: train loss: 9.964658147509908e-07, val loss: 0.06946065276861191\n",
      "Epoch 5923: train loss: 7.087114113346615e-07, val loss: 0.06939058750867844\n",
      "Epoch 5924: train loss: 6.947876727281255e-07, val loss: 0.06916555017232895\n",
      "Epoch 5925: train loss: 7.12461371676909e-07, val loss: 0.06905002892017365\n",
      "Epoch 5926: train loss: 5.654864594362152e-07, val loss: 0.06907651573419571\n",
      "Epoch 5927: train loss: 4.761381831031031e-07, val loss: 0.06907748430967331\n",
      "Epoch 5928: train loss: 5.990818294776545e-07, val loss: 0.06899595260620117\n",
      "Epoch 5929: train loss: 3.5961218713964627e-07, val loss: 0.0689428374171257\n",
      "Epoch 5930: train loss: 3.998488864453975e-07, val loss: 0.06898719817399979\n",
      "Epoch 5931: train loss: 3.8770940591348335e-07, val loss: 0.06899768114089966\n",
      "Epoch 5932: train loss: 3.4089381983903877e-07, val loss: 0.06887663900852203\n",
      "Epoch 5933: train loss: 2.761038047083275e-07, val loss: 0.06874799728393555\n",
      "Epoch 5934: train loss: 2.660001996446226e-07, val loss: 0.06875131279230118\n",
      "Epoch 5935: train loss: 2.444618871777493e-07, val loss: 0.06879643350839615\n",
      "Epoch 5936: train loss: 2.3098684209799103e-07, val loss: 0.06873392313718796\n",
      "Epoch 5937: train loss: 1.9093513969892228e-07, val loss: 0.06860800087451935\n",
      "Epoch 5938: train loss: 1.806042888574666e-07, val loss: 0.06856672465801239\n",
      "Epoch 5939: train loss: 1.4398528946912847e-07, val loss: 0.06858313083648682\n",
      "Epoch 5940: train loss: 1.9383503513381584e-07, val loss: 0.06851916015148163\n",
      "Epoch 5941: train loss: 1.0284028917340038e-07, val loss: 0.06842975318431854\n",
      "Epoch 5942: train loss: 1.2073992650130094e-07, val loss: 0.06843893229961395\n",
      "Epoch 5943: train loss: 1.1025657187246907e-07, val loss: 0.06843262165784836\n",
      "Epoch 5944: train loss: 1.0585075216340556e-07, val loss: 0.06831661611795425\n",
      "Epoch 5945: train loss: 8.243799953788766e-08, val loss: 0.06827450543642044\n",
      "Epoch 5946: train loss: 9.767642694669121e-08, val loss: 0.06833459436893463\n",
      "Epoch 5947: train loss: 4.5549985117077085e-08, val loss: 0.06827574968338013\n",
      "Epoch 5948: train loss: 9.012772039795891e-08, val loss: 0.06813924759626389\n",
      "Epoch 5949: train loss: 6.298352417388742e-08, val loss: 0.06811737269163132\n",
      "Epoch 5950: train loss: 4.8745835812269434e-08, val loss: 0.06811896711587906\n",
      "Epoch 5951: train loss: 4.668242326033578e-08, val loss: 0.06804515421390533\n",
      "Epoch 5952: train loss: 4.786617324725739e-08, val loss: 0.06798896193504333\n",
      "Epoch 5953: train loss: 5.227551724829027e-08, val loss: 0.06797568500041962\n",
      "Epoch 5954: train loss: 3.303181372871222e-08, val loss: 0.06794906407594681\n",
      "Epoch 5955: train loss: 3.628678157951981e-08, val loss: 0.0678812563419342\n",
      "Epoch 5956: train loss: 3.036528894995172e-08, val loss: 0.06779416650533676\n",
      "Epoch 5957: train loss: 3.723333108496263e-08, val loss: 0.06777865439653397\n",
      "Epoch 5958: train loss: 2.4400884868214234e-08, val loss: 0.06776785850524902\n",
      "Epoch 5959: train loss: 2.879744087636027e-08, val loss: 0.06767670065164566\n",
      "Epoch 5960: train loss: 2.549005273522198e-08, val loss: 0.06762766093015671\n",
      "Epoch 5961: train loss: 2.1026089314091223e-08, val loss: 0.06760367006063461\n",
      "Epoch 5962: train loss: 2.4928947794933265e-08, val loss: 0.06755571067333221\n",
      "Epoch 5963: train loss: 1.8603982354647997e-08, val loss: 0.06750800460577011\n",
      "Epoch 5964: train loss: 1.7393778861674036e-08, val loss: 0.06742604076862335\n",
      "Epoch 5965: train loss: 1.736338361979506e-08, val loss: 0.06739968061447144\n",
      "Epoch 5966: train loss: 1.7937988872063215e-08, val loss: 0.06739220023155212\n",
      "Epoch 5967: train loss: 1.2474596466915955e-08, val loss: 0.06732656806707382\n",
      "Epoch 5968: train loss: 1.498882262751522e-08, val loss: 0.06725788116455078\n",
      "Epoch 5969: train loss: 1.6206469055646266e-08, val loss: 0.0671968087553978\n",
      "Epoch 5970: train loss: 1.7073073621531876e-08, val loss: 0.06716059893369675\n",
      "Epoch 5971: train loss: 2.8701855114832142e-08, val loss: 0.06716177612543106\n",
      "Epoch 5972: train loss: 7.670966795103595e-08, val loss: 0.0670795887708664\n",
      "Epoch 5973: train loss: 2.3493174694522168e-07, val loss: 0.0670575425028801\n",
      "Epoch 5974: train loss: 7.82230472395895e-07, val loss: 0.06694578379392624\n",
      "Epoch 5975: train loss: 1.596743913978571e-06, val loss: 0.06699001789093018\n",
      "Epoch 5976: train loss: 2.557030029493035e-06, val loss: 0.0668834000825882\n",
      "Epoch 5977: train loss: 3.2037846722232644e-06, val loss: 0.06699454039335251\n",
      "Epoch 5978: train loss: 5.581209279625909e-06, val loss: 0.06691636890172958\n",
      "Epoch 5979: train loss: 1.0363590263295919e-05, val loss: 0.06721045076847076\n",
      "Epoch 5980: train loss: 4.811175131180789e-06, val loss: 0.06698913872241974\n",
      "Epoch 5981: train loss: 2.9212399113021092e-06, val loss: 0.06738948076963425\n",
      "Epoch 5982: train loss: 7.817781806807034e-06, val loss: 0.06759091466665268\n",
      "Epoch 5983: train loss: 6.377579211402917e-06, val loss: 0.06764937937259674\n",
      "Epoch 5984: train loss: 9.649937965150457e-06, val loss: 0.06726310402154922\n",
      "Epoch 5985: train loss: 1.362605962640373e-05, val loss: 0.06796813011169434\n",
      "Epoch 5986: train loss: 1.588263148732949e-05, val loss: 0.06738108396530151\n",
      "Epoch 5987: train loss: 2.5801728043006733e-05, val loss: 0.06922345608472824\n",
      "Epoch 5988: train loss: 3.8456240872619674e-05, val loss: 0.06690730899572372\n",
      "Epoch 5989: train loss: 5.890017200727016e-05, val loss: 0.06888049095869064\n",
      "Epoch 5990: train loss: 8.803847595117986e-05, val loss: 0.06834142655134201\n",
      "Epoch 5991: train loss: 0.00012878667621407658, val loss: 0.06958241760730743\n",
      "Epoch 5992: train loss: 0.000128280371427536, val loss: 0.06945046037435532\n",
      "Epoch 5993: train loss: 8.857975626597181e-05, val loss: 0.07034208625555038\n",
      "Epoch 5994: train loss: 4.002592322649434e-05, val loss: 0.07210248708724976\n",
      "Epoch 5995: train loss: 3.0904389859642833e-05, val loss: 0.07082580775022507\n",
      "Epoch 5996: train loss: 4.78543879580684e-05, val loss: 0.07403992861509323\n",
      "Epoch 5997: train loss: 4.752405220642686e-05, val loss: 0.07309594005346298\n",
      "Epoch 5998: train loss: 2.8564421882038005e-05, val loss: 0.0731007531285286\n",
      "Epoch 5999: train loss: 1.918311681947671e-05, val loss: 0.07549178600311279\n",
      "Epoch 6000: train loss: 2.1772459149360657e-05, val loss: 0.07433420419692993\n",
      "Epoch 6001: train loss: 2.3294103812077083e-05, val loss: 0.07526495307683945\n",
      "Epoch 6002: train loss: 2.1860836568521336e-05, val loss: 0.07620887458324432\n",
      "Epoch 6003: train loss: 1.5006165995146148e-05, val loss: 0.07558562606573105\n",
      "Epoch 6004: train loss: 8.842454917612486e-06, val loss: 0.07642165571451187\n",
      "Epoch 6005: train loss: 1.4295467735792045e-05, val loss: 0.07634767889976501\n",
      "Epoch 6006: train loss: 1.6683026842656545e-05, val loss: 0.07649560272693634\n",
      "Epoch 6007: train loss: 8.006014468264766e-06, val loss: 0.07717981189489365\n",
      "Epoch 6008: train loss: 6.438740911107743e-06, val loss: 0.07680864632129669\n",
      "Epoch 6009: train loss: 1.0508157174626831e-05, val loss: 0.07688747346401215\n",
      "Epoch 6010: train loss: 8.766302926233038e-06, val loss: 0.07731856405735016\n",
      "Epoch 6011: train loss: 5.951073944743257e-06, val loss: 0.07720973342657089\n",
      "Epoch 6012: train loss: 6.1753180489176884e-06, val loss: 0.07702118158340454\n",
      "Epoch 6013: train loss: 5.846518433827441e-06, val loss: 0.0772104412317276\n",
      "Epoch 6014: train loss: 4.837514097744133e-06, val loss: 0.07746783643960953\n",
      "Epoch 6015: train loss: 4.974977400706848e-06, val loss: 0.07705990225076675\n",
      "Epoch 6016: train loss: 4.367758265289012e-06, val loss: 0.07711408287286758\n",
      "Epoch 6017: train loss: 2.836338808265282e-06, val loss: 0.07745898514986038\n",
      "Epoch 6018: train loss: 3.0483145110338228e-06, val loss: 0.07710003852844238\n",
      "Epoch 6019: train loss: 4.2675446820794605e-06, val loss: 0.07727392762899399\n",
      "Epoch 6020: train loss: 3.245041625632439e-06, val loss: 0.0773761123418808\n",
      "Epoch 6021: train loss: 1.268142682420148e-06, val loss: 0.07718826830387115\n",
      "Epoch 6022: train loss: 1.7333950381726027e-06, val loss: 0.07741691917181015\n",
      "Epoch 6023: train loss: 2.9740810987277655e-06, val loss: 0.07716065645217896\n",
      "Epoch 6024: train loss: 2.4924229364842176e-06, val loss: 0.07715485244989395\n",
      "Epoch 6025: train loss: 1.3973375416753697e-06, val loss: 0.07734985649585724\n",
      "Epoch 6026: train loss: 1.1377080681995722e-06, val loss: 0.07706107944250107\n",
      "Epoch 6027: train loss: 1.3512676559912506e-06, val loss: 0.07717783749103546\n",
      "Epoch 6028: train loss: 1.5099583379196702e-06, val loss: 0.07721826434135437\n",
      "Epoch 6029: train loss: 2.0112390757276444e-06, val loss: 0.0770711824297905\n",
      "Epoch 6030: train loss: 2.5541326067468617e-06, val loss: 0.07694389671087265\n",
      "Epoch 6031: train loss: 2.221776185251656e-06, val loss: 0.07721515744924545\n",
      "Epoch 6032: train loss: 1.587115548318252e-06, val loss: 0.07683984190225601\n",
      "Epoch 6033: train loss: 1.4620762840422685e-06, val loss: 0.07732448726892471\n",
      "Epoch 6034: train loss: 1.698309233688633e-06, val loss: 0.07681658864021301\n",
      "Epoch 6035: train loss: 1.6089718428702326e-06, val loss: 0.07712327688932419\n",
      "Epoch 6036: train loss: 1.0073042631120188e-06, val loss: 0.0768512412905693\n",
      "Epoch 6037: train loss: 6.15937153725099e-07, val loss: 0.07697267830371857\n",
      "Epoch 6038: train loss: 6.027931362950767e-07, val loss: 0.0768953263759613\n",
      "Epoch 6039: train loss: 7.822659426892642e-07, val loss: 0.07685671746730804\n",
      "Epoch 6040: train loss: 9.615268936613575e-07, val loss: 0.07698646932840347\n",
      "Epoch 6041: train loss: 1.011870267575432e-06, val loss: 0.07676317542791367\n",
      "Epoch 6042: train loss: 1.1391629186618957e-06, val loss: 0.07683474570512772\n",
      "Epoch 6043: train loss: 1.3481513860824634e-06, val loss: 0.07683607190847397\n",
      "Epoch 6044: train loss: 1.9169849565514596e-06, val loss: 0.07667334377765656\n",
      "Epoch 6045: train loss: 3.706892812260776e-06, val loss: 0.07686059176921844\n",
      "Epoch 6046: train loss: 9.445403520658147e-06, val loss: 0.07645387947559357\n",
      "Epoch 6047: train loss: 2.687943197088316e-05, val loss: 0.07713886350393295\n",
      "Epoch 6048: train loss: 8.060842083068565e-05, val loss: 0.07593495398759842\n",
      "Epoch 6049: train loss: 0.0002274544385727495, val loss: 0.07778914272785187\n",
      "Epoch 6050: train loss: 0.0005666728829964995, val loss: 0.07627113908529282\n",
      "Epoch 6051: train loss: 0.0009876447729766369, val loss: 0.0763830691576004\n",
      "Epoch 6052: train loss: 0.00085133605170995, val loss: 0.07951830327510834\n",
      "Epoch 6053: train loss: 0.00026841822545975447, val loss: 0.07756363600492477\n",
      "Epoch 6054: train loss: 0.00019790227815974504, val loss: 0.07606818526983261\n",
      "Epoch 6055: train loss: 0.000311540235998109, val loss: 0.0795552134513855\n",
      "Epoch 6056: train loss: 0.00014574985834769905, val loss: 0.08243635296821594\n",
      "Epoch 6057: train loss: 0.00016978144412860274, val loss: 0.08095815032720566\n",
      "Epoch 6058: train loss: 0.0001232097129104659, val loss: 0.08045601844787598\n",
      "Epoch 6059: train loss: 0.00011410421575419605, val loss: 0.083306685090065\n",
      "Epoch 6060: train loss: 0.00010820877650985494, val loss: 0.08543719351291656\n",
      "Epoch 6061: train loss: 7.184607966337353e-05, val loss: 0.0845385193824768\n",
      "Epoch 6062: train loss: 8.822637028060853e-05, val loss: 0.0832759290933609\n",
      "Epoch 6063: train loss: 5.6753717217361555e-05, val loss: 0.08478172868490219\n",
      "Epoch 6064: train loss: 5.59341860935092e-05, val loss: 0.08759637176990509\n",
      "Epoch 6065: train loss: 5.6987097195815295e-05, val loss: 0.08799874037504196\n",
      "Epoch 6066: train loss: 3.455812839092687e-05, val loss: 0.08620117604732513\n",
      "Epoch 6067: train loss: 4.574689592118375e-05, val loss: 0.08516086637973785\n",
      "Epoch 6068: train loss: 3.317142545711249e-05, val loss: 0.08733829110860825\n",
      "Epoch 6069: train loss: 2.7909512937185355e-05, val loss: 0.08935265988111496\n",
      "Epoch 6070: train loss: 3.317234222777188e-05, val loss: 0.08858149498701096\n",
      "Epoch 6071: train loss: 1.889780833153054e-05, val loss: 0.08710562437772751\n",
      "Epoch 6072: train loss: 2.4957049390650354e-05, val loss: 0.08777587860822678\n",
      "Epoch 6073: train loss: 1.925671131175477e-05, val loss: 0.08947541564702988\n",
      "Epoch 6074: train loss: 1.6109914213302545e-05, val loss: 0.08939164876937866\n",
      "Epoch 6075: train loss: 1.708212585072033e-05, val loss: 0.0883159413933754\n",
      "Epoch 6076: train loss: 1.335453907813644e-05, val loss: 0.08841334283351898\n",
      "Epoch 6077: train loss: 1.319115654041525e-05, val loss: 0.08907391875982285\n",
      "Epoch 6078: train loss: 1.0196466064371634e-05, val loss: 0.0894012451171875\n",
      "Epoch 6079: train loss: 1.2146848348493222e-05, val loss: 0.08952046930789948\n",
      "Epoch 6080: train loss: 6.6493635131337214e-06, val loss: 0.08928991854190826\n",
      "Epoch 6081: train loss: 1.0358024155721068e-05, val loss: 0.08916781097650528\n",
      "Epoch 6082: train loss: 5.633917226077756e-06, val loss: 0.08959116041660309\n",
      "Epoch 6083: train loss: 7.567813099740306e-06, val loss: 0.08978195488452911\n",
      "Epoch 6084: train loss: 5.491399406309938e-06, val loss: 0.08931522816419601\n",
      "Epoch 6085: train loss: 5.167608833289705e-06, val loss: 0.08899978548288345\n",
      "Epoch 6086: train loss: 5.020794105803361e-06, val loss: 0.08964083343744278\n",
      "Epoch 6087: train loss: 3.6583780911314534e-06, val loss: 0.0903051495552063\n",
      "Epoch 6088: train loss: 4.490283117775107e-06, val loss: 0.08994177728891373\n",
      "Epoch 6089: train loss: 2.8185743303765776e-06, val loss: 0.08940435945987701\n",
      "Epoch 6090: train loss: 3.7380223147920333e-06, val loss: 0.08941810578107834\n",
      "Epoch 6091: train loss: 2.241403763036942e-06, val loss: 0.08979804813861847\n",
      "Epoch 6092: train loss: 2.8959989322174806e-06, val loss: 0.08990835398435593\n",
      "Epoch 6093: train loss: 2.214114601883921e-06, val loss: 0.0896533727645874\n",
      "Epoch 6094: train loss: 2.1118539734743536e-06, val loss: 0.08967015892267227\n",
      "Epoch 6095: train loss: 1.9518147382768802e-06, val loss: 0.089789979159832\n",
      "Epoch 6096: train loss: 1.4999861832620809e-06, val loss: 0.08987933397293091\n",
      "Epoch 6097: train loss: 1.8768976133287651e-06, val loss: 0.08993752300739288\n",
      "Epoch 6098: train loss: 1.0659621239028638e-06, val loss: 0.08974245935678482\n",
      "Epoch 6099: train loss: 1.59147623435274e-06, val loss: 0.0896986797451973\n",
      "Epoch 6100: train loss: 9.705775028123753e-07, val loss: 0.08987817168235779\n",
      "Epoch 6101: train loss: 1.2355799299257342e-06, val loss: 0.08991938829421997\n",
      "Epoch 6102: train loss: 8.748966706662031e-07, val loss: 0.08966276049613953\n",
      "Epoch 6103: train loss: 9.03719865164021e-07, val loss: 0.08984746038913727\n",
      "Epoch 6104: train loss: 8.591603091190336e-07, val loss: 0.09006677567958832\n",
      "Epoch 6105: train loss: 7.081844728418218e-07, val loss: 0.08989682048559189\n",
      "Epoch 6106: train loss: 6.364431897054601e-07, val loss: 0.08949226140975952\n",
      "Epoch 6107: train loss: 6.801411700507742e-07, val loss: 0.08960220962762833\n",
      "Epoch 6108: train loss: 5.50631284568226e-07, val loss: 0.08993735164403915\n",
      "Epoch 6109: train loss: 6.703381245642959e-07, val loss: 0.0899994894862175\n",
      "Epoch 6110: train loss: 7.761500455671921e-07, val loss: 0.08944424986839294\n",
      "Epoch 6111: train loss: 1.2705191920758807e-06, val loss: 0.08961645513772964\n",
      "Epoch 6112: train loss: 2.8482065772550413e-06, val loss: 0.0894092544913292\n",
      "Epoch 6113: train loss: 7.600224762427388e-06, val loss: 0.09013853967189789\n",
      "Epoch 6114: train loss: 2.3236587367136963e-05, val loss: 0.08869211375713348\n",
      "Epoch 6115: train loss: 7.459853804903105e-05, val loss: 0.09109476953744888\n",
      "Epoch 6116: train loss: 0.0002277676248922944, val loss: 0.08726669102907181\n",
      "Epoch 6117: train loss: 0.0005101663409732282, val loss: 0.09361838549375534\n",
      "Epoch 6118: train loss: 0.00040734908543527126, val loss: 0.09226904064416885\n",
      "Epoch 6119: train loss: 0.0001515543699497357, val loss: 0.09197284281253815\n",
      "Epoch 6120: train loss: 0.00010889593249885365, val loss: 0.09319864213466644\n",
      "Epoch 6121: train loss: 0.00016532037989236414, val loss: 0.09387309849262238\n",
      "Epoch 6122: train loss: 8.304559014504775e-05, val loss: 0.09335395693778992\n",
      "Epoch 6123: train loss: 5.298985342960805e-05, val loss: 0.09311520308256149\n",
      "Epoch 6124: train loss: 6.954160198802128e-05, val loss: 0.09316994994878769\n",
      "Epoch 6125: train loss: 7.032853318378329e-05, val loss: 0.0928848385810852\n",
      "Epoch 6126: train loss: 5.9894617152167484e-05, val loss: 0.09281197935342789\n",
      "Epoch 6127: train loss: 3.393275255803019e-05, val loss: 0.09256698191165924\n",
      "Epoch 6128: train loss: 2.6639991119736806e-05, val loss: 0.09206070750951767\n",
      "Epoch 6129: train loss: 3.278556323493831e-05, val loss: 0.09090795367956161\n",
      "Epoch 6130: train loss: 4.114998591830954e-05, val loss: 0.09035313129425049\n",
      "Epoch 6131: train loss: 3.6438348615774885e-05, val loss: 0.09120789170265198\n",
      "Epoch 6132: train loss: 1.6824391423142515e-05, val loss: 0.09195192903280258\n",
      "Epoch 6133: train loss: 1.101959787774831e-05, val loss: 0.09212133288383484\n",
      "Epoch 6134: train loss: 2.1588881281786598e-05, val loss: 0.09202791750431061\n",
      "Epoch 6135: train loss: 2.3607706680195406e-05, val loss: 0.09201215952634811\n",
      "Epoch 6136: train loss: 1.7330034097540192e-05, val loss: 0.09164514392614365\n",
      "Epoch 6137: train loss: 1.3032930837653112e-05, val loss: 0.091466024518013\n",
      "Epoch 6138: train loss: 1.1331841960782185e-05, val loss: 0.09218104183673859\n",
      "Epoch 6139: train loss: 7.727984666416887e-06, val loss: 0.09267287701368332\n",
      "Epoch 6140: train loss: 9.678131391410716e-06, val loss: 0.09228406846523285\n",
      "Epoch 6141: train loss: 9.898736607283354e-06, val loss: 0.09203299134969711\n",
      "Epoch 6142: train loss: 1.0910224773397204e-05, val loss: 0.09252191334962845\n",
      "Epoch 6143: train loss: 7.645257937838323e-06, val loss: 0.09309985488653183\n",
      "Epoch 6144: train loss: 3.828338776656892e-06, val loss: 0.09318671375513077\n",
      "Epoch 6145: train loss: 4.681413429352688e-06, val loss: 0.09305501729249954\n",
      "Epoch 6146: train loss: 5.892568424314959e-06, val loss: 0.09303214401006699\n",
      "Epoch 6147: train loss: 7.192938028310891e-06, val loss: 0.09322988241910934\n",
      "Epoch 6148: train loss: 4.757752321893349e-06, val loss: 0.0932447761297226\n",
      "Epoch 6149: train loss: 2.25607686843432e-06, val loss: 0.09329580515623093\n",
      "Epoch 6150: train loss: 2.4239259346359177e-06, val loss: 0.09327609837055206\n",
      "Epoch 6151: train loss: 2.823096110660117e-06, val loss: 0.09324073791503906\n",
      "Epoch 6152: train loss: 3.444697540544439e-06, val loss: 0.09326370805501938\n",
      "Epoch 6153: train loss: 2.9214309051894816e-06, val loss: 0.09302415698766708\n",
      "Epoch 6154: train loss: 1.9289159354229923e-06, val loss: 0.09308221191167831\n",
      "Epoch 6155: train loss: 1.8141761302103987e-06, val loss: 0.0933946967124939\n",
      "Epoch 6156: train loss: 1.6967865121841896e-06, val loss: 0.09340429306030273\n",
      "Epoch 6157: train loss: 1.665102104198013e-06, val loss: 0.09305360168218613\n",
      "Epoch 6158: train loss: 1.3987649936098023e-06, val loss: 0.0928824320435524\n",
      "Epoch 6159: train loss: 1.0242705457130796e-06, val loss: 0.09279438853263855\n",
      "Epoch 6160: train loss: 1.2851879773734254e-06, val loss: 0.0926816388964653\n",
      "Epoch 6161: train loss: 1.4229099178919569e-06, val loss: 0.09272494167089462\n",
      "Epoch 6162: train loss: 1.6563787994527956e-06, val loss: 0.09255530685186386\n",
      "Epoch 6163: train loss: 1.4738626532562193e-06, val loss: 0.09253715723752975\n",
      "Epoch 6164: train loss: 1.4146073681331472e-06, val loss: 0.09257283061742783\n",
      "Epoch 6165: train loss: 1.4447737157752272e-06, val loss: 0.09268591552972794\n",
      "Epoch 6166: train loss: 1.901980908769474e-06, val loss: 0.09228222072124481\n",
      "Epoch 6167: train loss: 2.4254534309875453e-06, val loss: 0.09223061054944992\n",
      "Epoch 6168: train loss: 3.6152027860225644e-06, val loss: 0.0921214371919632\n",
      "Epoch 6169: train loss: 5.811964456370333e-06, val loss: 0.09244110435247421\n",
      "Epoch 6170: train loss: 1.0996061064361129e-05, val loss: 0.09194262325763702\n",
      "Epoch 6171: train loss: 2.2632852051174268e-05, val loss: 0.09243148565292358\n",
      "Epoch 6172: train loss: 5.291064007906243e-05, val loss: 0.09163607656955719\n",
      "Epoch 6173: train loss: 0.0001276647235499695, val loss: 0.09291024506092072\n",
      "Epoch 6174: train loss: 0.0003249271831009537, val loss: 0.09157639741897583\n",
      "Epoch 6175: train loss: 0.0006829251069575548, val loss: 0.09515393525362015\n",
      "Epoch 6176: train loss: 0.0010478226467967033, val loss: 0.09259621053934097\n",
      "Epoch 6177: train loss: 0.0006925776833668351, val loss: 0.09324099123477936\n",
      "Epoch 6178: train loss: 8.547043398721144e-05, val loss: 0.09445608407258987\n",
      "Epoch 6179: train loss: 0.00022461044136434793, val loss: 0.09227970242500305\n",
      "Epoch 6180: train loss: 0.00028016522992402315, val loss: 0.09110268205404282\n",
      "Epoch 6181: train loss: 4.558056389214471e-05, val loss: 0.09207278490066528\n",
      "Epoch 6182: train loss: 0.00021149188978597522, val loss: 0.08988042175769806\n",
      "Epoch 6183: train loss: 7.44107092032209e-05, val loss: 0.08858375251293182\n",
      "Epoch 6184: train loss: 0.00011493500642245635, val loss: 0.08886320888996124\n",
      "Epoch 6185: train loss: 9.48989181779325e-05, val loss: 0.0891658142209053\n",
      "Epoch 6186: train loss: 6.0816328186774626e-05, val loss: 0.08856175094842911\n",
      "Epoch 6187: train loss: 8.37997577036731e-05, val loss: 0.08815757185220718\n",
      "Epoch 6188: train loss: 4.037521284772083e-05, val loss: 0.08794038742780685\n",
      "Epoch 6189: train loss: 6.407320324797183e-05, val loss: 0.08764872699975967\n",
      "Epoch 6190: train loss: 3.4806773328455165e-05, val loss: 0.08769475668668747\n",
      "Epoch 6191: train loss: 4.776552668772638e-05, val loss: 0.08814456313848495\n",
      "Epoch 6192: train loss: 2.971924732264597e-05, val loss: 0.08841775357723236\n",
      "Epoch 6193: train loss: 3.520212703733705e-05, val loss: 0.0880909115076065\n",
      "Epoch 6194: train loss: 2.6230240109725855e-05, val loss: 0.08783489465713501\n",
      "Epoch 6195: train loss: 2.6983949283021502e-05, val loss: 0.08788735419511795\n",
      "Epoch 6196: train loss: 2.112906804541126e-05, val loss: 0.08814115077257156\n",
      "Epoch 6197: train loss: 2.2186230125953443e-05, val loss: 0.08834975957870483\n",
      "Epoch 6198: train loss: 1.7133872461272404e-05, val loss: 0.08840806782245636\n",
      "Epoch 6199: train loss: 1.8118762454832904e-05, val loss: 0.08815612643957138\n",
      "Epoch 6200: train loss: 1.204873751703417e-05, val loss: 0.08781316876411438\n",
      "Epoch 6201: train loss: 1.6286925529129803e-05, val loss: 0.08771771937608719\n",
      "Epoch 6202: train loss: 7.677229405089747e-06, val loss: 0.08746936172246933\n",
      "Epoch 6203: train loss: 1.3725827557209413e-05, val loss: 0.08739912509918213\n",
      "Epoch 6204: train loss: 5.846085969096748e-06, val loss: 0.08753474801778793\n",
      "Epoch 6205: train loss: 1.1135208296764176e-05, val loss: 0.08790812641382217\n",
      "Epoch 6206: train loss: 4.994993105356116e-06, val loss: 0.08793572336435318\n",
      "Epoch 6207: train loss: 8.198101568268612e-06, val loss: 0.08771711587905884\n",
      "Epoch 6208: train loss: 5.592744855675846e-06, val loss: 0.08812306076288223\n",
      "Epoch 6209: train loss: 6.270737230806844e-06, val loss: 0.08827825635671616\n",
      "Epoch 6210: train loss: 4.479473318497185e-06, val loss: 0.08817833662033081\n",
      "Epoch 6211: train loss: 4.516220087680267e-06, val loss: 0.0881711021065712\n",
      "Epoch 6212: train loss: 3.922808446077397e-06, val loss: 0.08842119574546814\n",
      "Epoch 6213: train loss: 3.64256607099378e-06, val loss: 0.08855962753295898\n",
      "Epoch 6214: train loss: 3.22451501233445e-06, val loss: 0.08835139870643616\n",
      "Epoch 6215: train loss: 2.8130580176366493e-06, val loss: 0.08851084858179092\n",
      "Epoch 6216: train loss: 2.7310068162478274e-06, val loss: 0.08889325708150864\n",
      "Epoch 6217: train loss: 2.31339322454005e-06, val loss: 0.08919711410999298\n",
      "Epoch 6218: train loss: 2.223296633019345e-06, val loss: 0.0889732614159584\n",
      "Epoch 6219: train loss: 1.947801592905307e-06, val loss: 0.08908435702323914\n",
      "Epoch 6220: train loss: 1.7366423890052829e-06, val loss: 0.08968548476696014\n",
      "Epoch 6221: train loss: 1.7259665128221968e-06, val loss: 0.08972446620464325\n",
      "Epoch 6222: train loss: 1.2797041790690855e-06, val loss: 0.08951427787542343\n",
      "Epoch 6223: train loss: 1.7365920257361722e-06, val loss: 0.08966515213251114\n",
      "Epoch 6224: train loss: 8.511991040904832e-07, val loss: 0.09004681557416916\n",
      "Epoch 6225: train loss: 1.479211732657859e-06, val loss: 0.09014018625020981\n",
      "Epoch 6226: train loss: 8.107911071419949e-07, val loss: 0.08992436528205872\n",
      "Epoch 6227: train loss: 9.561694014337263e-07, val loss: 0.08982237428426743\n",
      "Epoch 6228: train loss: 1.0247463251289446e-06, val loss: 0.08991452306509018\n",
      "Epoch 6229: train loss: 8.463200060759846e-07, val loss: 0.09011255949735641\n",
      "Epoch 6230: train loss: 4.4246223751542857e-07, val loss: 0.09007447212934494\n",
      "Epoch 6231: train loss: 8.013833507902746e-07, val loss: 0.08997135609388351\n",
      "Epoch 6232: train loss: 5.749336651206249e-07, val loss: 0.09016329050064087\n",
      "Epoch 6233: train loss: 3.8477074326692673e-07, val loss: 0.09035097062587738\n",
      "Epoch 6234: train loss: 5.457260954244703e-07, val loss: 0.09024133533239365\n",
      "Epoch 6235: train loss: 4.618357536401163e-07, val loss: 0.09014016389846802\n",
      "Epoch 6236: train loss: 3.833693540400418e-07, val loss: 0.09026679396629333\n",
      "Epoch 6237: train loss: 8.716854722479184e-07, val loss: 0.0902576744556427\n",
      "Epoch 6238: train loss: 4.620926119969226e-06, val loss: 0.09048905223608017\n",
      "Epoch 6239: train loss: 5.400806912803091e-06, val loss: 0.09004838764667511\n",
      "Epoch 6240: train loss: 2.0948718884028494e-06, val loss: 0.08982069045305252\n",
      "Epoch 6241: train loss: 1.7530113609609543e-06, val loss: 0.08991338312625885\n",
      "Epoch 6242: train loss: 2.1308360373950563e-06, val loss: 0.08994705975055695\n",
      "Epoch 6243: train loss: 1.8733272781901178e-06, val loss: 0.0901813879609108\n",
      "Epoch 6244: train loss: 9.913975418385235e-07, val loss: 0.09018845856189728\n",
      "Epoch 6245: train loss: 1.952486172740464e-06, val loss: 0.0900542214512825\n",
      "Epoch 6246: train loss: 7.832481401237601e-07, val loss: 0.09014176577329636\n",
      "Epoch 6247: train loss: 1.3428362990453024e-06, val loss: 0.09017115086317062\n",
      "Epoch 6248: train loss: 1.150915636571881e-06, val loss: 0.09004610776901245\n",
      "Epoch 6249: train loss: 7.247039661706367e-07, val loss: 0.09004898369312286\n",
      "Epoch 6250: train loss: 8.840435725687712e-07, val loss: 0.09021379053592682\n",
      "Epoch 6251: train loss: 9.561688329995377e-07, val loss: 0.09023117274045944\n",
      "Epoch 6252: train loss: 6.873810889373999e-07, val loss: 0.09003559499979019\n",
      "Epoch 6253: train loss: 6.073112217563903e-07, val loss: 0.09006818383932114\n",
      "Epoch 6254: train loss: 6.769022320440854e-07, val loss: 0.09018827974796295\n",
      "Epoch 6255: train loss: 5.085340148980322e-07, val loss: 0.09013159573078156\n",
      "Epoch 6256: train loss: 3.485202455522085e-07, val loss: 0.09001335501670837\n",
      "Epoch 6257: train loss: 5.845058126396907e-07, val loss: 0.09004926681518555\n",
      "Epoch 6258: train loss: 7.469082561328833e-07, val loss: 0.09003817290067673\n",
      "Epoch 6259: train loss: 5.681272909896506e-07, val loss: 0.08993344753980637\n",
      "Epoch 6260: train loss: 4.6060884528742463e-07, val loss: 0.09003706276416779\n",
      "Epoch 6261: train loss: 8.271139790849702e-07, val loss: 0.08988405764102936\n",
      "Epoch 6262: train loss: 7.839743716431258e-07, val loss: 0.08998533338308334\n",
      "Epoch 6263: train loss: 1.1801868140537408e-06, val loss: 0.0898933932185173\n",
      "Epoch 6264: train loss: 2.843172978828079e-06, val loss: 0.09005101770162582\n",
      "Epoch 6265: train loss: 7.613081834279001e-06, val loss: 0.08964603394269943\n",
      "Epoch 6266: train loss: 2.397607204329688e-05, val loss: 0.09068728238344193\n",
      "Epoch 6267: train loss: 6.140852201497182e-05, val loss: 0.08944326639175415\n",
      "Epoch 6268: train loss: 0.00013026368105784059, val loss: 0.09237466752529144\n",
      "Epoch 6269: train loss: 0.00010713961091823876, val loss: 0.09170401096343994\n",
      "Epoch 6270: train loss: 4.37897106166929e-05, val loss: 0.09149052947759628\n",
      "Epoch 6271: train loss: 1.8944661860587075e-05, val loss: 0.09283294528722763\n",
      "Epoch 6272: train loss: 3.6667177482740954e-05, val loss: 0.09172343462705612\n",
      "Epoch 6273: train loss: 5.349831553758122e-05, val loss: 0.09185533970594406\n",
      "Epoch 6274: train loss: 3.966960866819136e-05, val loss: 0.09266682714223862\n",
      "Epoch 6275: train loss: 2.087868779199198e-05, val loss: 0.09274768829345703\n",
      "Epoch 6276: train loss: 1.7590255083632655e-05, val loss: 0.09300711005926132\n",
      "Epoch 6277: train loss: 2.8205127819092013e-05, val loss: 0.09250199049711227\n",
      "Epoch 6278: train loss: 2.6117921152035706e-05, val loss: 0.09298844635486603\n",
      "Epoch 6279: train loss: 1.055558277585078e-05, val loss: 0.09366180002689362\n",
      "Epoch 6280: train loss: 1.2972871445526835e-05, val loss: 0.09316085278987885\n",
      "Epoch 6281: train loss: 1.9587416318245232e-05, val loss: 0.09395155310630798\n",
      "Epoch 6282: train loss: 1.3409220628091134e-05, val loss: 0.09432300180196762\n",
      "Epoch 6283: train loss: 7.309168722713366e-06, val loss: 0.09348268806934357\n",
      "Epoch 6284: train loss: 9.635933565732557e-06, val loss: 0.09441016614437103\n",
      "Epoch 6285: train loss: 1.1991592145932373e-05, val loss: 0.09494844824075699\n",
      "Epoch 6286: train loss: 9.036235496751033e-06, val loss: 0.09463955461978912\n",
      "Epoch 6287: train loss: 5.295343271427555e-06, val loss: 0.09532813727855682\n",
      "Epoch 6288: train loss: 6.45661702947109e-06, val loss: 0.09531769901514053\n",
      "Epoch 6289: train loss: 8.150728717737366e-06, val loss: 0.09524904936552048\n",
      "Epoch 6290: train loss: 6.726783794874791e-06, val loss: 0.09574227780103683\n",
      "Epoch 6291: train loss: 3.4167146623076405e-06, val loss: 0.09578696638345718\n",
      "Epoch 6292: train loss: 3.589522293623304e-06, val loss: 0.09599164873361588\n",
      "Epoch 6293: train loss: 6.545970791194122e-06, val loss: 0.09565477818250656\n",
      "Epoch 6294: train loss: 5.580068318522535e-06, val loss: 0.09582757204771042\n",
      "Epoch 6295: train loss: 3.2479183573741466e-06, val loss: 0.09586462378501892\n",
      "Epoch 6296: train loss: 3.506959046717384e-06, val loss: 0.09629394859075546\n",
      "Epoch 6297: train loss: 8.133204573823605e-06, val loss: 0.0952722430229187\n",
      "Epoch 6298: train loss: 1.5517916835960932e-05, val loss: 0.09652581065893173\n",
      "Epoch 6299: train loss: 2.462177144479938e-05, val loss: 0.09522319585084915\n",
      "Epoch 6300: train loss: 4.0033686673268676e-05, val loss: 0.0958917886018753\n",
      "Epoch 6301: train loss: 6.911281525390223e-05, val loss: 0.09405361860990524\n",
      "Epoch 6302: train loss: 9.311314352089539e-05, val loss: 0.09568344056606293\n",
      "Epoch 6303: train loss: 0.00011092770000686869, val loss: 0.0934651643037796\n",
      "Epoch 6304: train loss: 8.068208262557164e-05, val loss: 0.0948396772146225\n",
      "Epoch 6305: train loss: 3.922202085959725e-05, val loss: 0.09428776055574417\n",
      "Epoch 6306: train loss: 9.281303391617257e-06, val loss: 0.09375184029340744\n",
      "Epoch 6307: train loss: 1.4594917047361378e-05, val loss: 0.09428825229406357\n",
      "Epoch 6308: train loss: 3.0207940653781407e-05, val loss: 0.09387526661157608\n",
      "Epoch 6309: train loss: 2.5290504709118977e-05, val loss: 0.09325557947158813\n",
      "Epoch 6310: train loss: 1.664678165980149e-05, val loss: 0.09369661659002304\n",
      "Epoch 6311: train loss: 1.5012351468612906e-05, val loss: 0.09320133179426193\n",
      "Epoch 6312: train loss: 1.1396016816433985e-05, val loss: 0.09266304224729538\n",
      "Epoch 6313: train loss: 8.556923603464384e-06, val loss: 0.09296518564224243\n",
      "Epoch 6314: train loss: 1.3587700777861755e-05, val loss: 0.09259716421365738\n",
      "Epoch 6315: train loss: 1.3367725841817446e-05, val loss: 0.09244734048843384\n",
      "Epoch 6316: train loss: 7.73520423535956e-06, val loss: 0.09289560467004776\n",
      "Epoch 6317: train loss: 6.735179340466857e-06, val loss: 0.09248092025518417\n",
      "Epoch 6318: train loss: 6.6982279349758755e-06, val loss: 0.09221716225147247\n",
      "Epoch 6319: train loss: 6.377817499014782e-06, val loss: 0.09255001693964005\n",
      "Epoch 6320: train loss: 7.690190614084713e-06, val loss: 0.09248896688222885\n",
      "Epoch 6321: train loss: 7.176555754995206e-06, val loss: 0.0923980176448822\n",
      "Epoch 6322: train loss: 5.668895937560592e-06, val loss: 0.09228423237800598\n",
      "Epoch 6323: train loss: 4.022808298032032e-06, val loss: 0.09264377504587173\n",
      "Epoch 6324: train loss: 3.4161996609327616e-06, val loss: 0.09278284758329391\n",
      "Epoch 6325: train loss: 4.463078767003026e-06, val loss: 0.09203808754682541\n",
      "Epoch 6326: train loss: 3.898989234585315e-06, val loss: 0.09268757700920105\n",
      "Epoch 6327: train loss: 2.361659426242113e-06, val loss: 0.09246760606765747\n",
      "Epoch 6328: train loss: 2.773849928416894e-06, val loss: 0.09208711981773376\n",
      "Epoch 6329: train loss: 4.34174853580771e-06, val loss: 0.09271936863660812\n",
      "Epoch 6330: train loss: 4.841894224227872e-06, val loss: 0.09218962490558624\n",
      "Epoch 6331: train loss: 4.7551202442264184e-06, val loss: 0.09232280403375626\n",
      "Epoch 6332: train loss: 5.598218194791116e-06, val loss: 0.09225662797689438\n",
      "Epoch 6333: train loss: 7.457986157533014e-06, val loss: 0.09225653856992722\n",
      "Epoch 6334: train loss: 1.0506555554457009e-05, val loss: 0.09212072193622589\n",
      "Epoch 6335: train loss: 1.5688352505094372e-05, val loss: 0.09246885031461716\n",
      "Epoch 6336: train loss: 2.5545381504343823e-05, val loss: 0.0919647067785263\n",
      "Epoch 6337: train loss: 4.501061266637407e-05, val loss: 0.0922568142414093\n",
      "Epoch 6338: train loss: 8.553497173124924e-05, val loss: 0.092059426009655\n",
      "Epoch 6339: train loss: 0.00016553746536374092, val loss: 0.09268853813409805\n",
      "Epoch 6340: train loss: 0.0003295449714642018, val loss: 0.09181391447782516\n",
      "Epoch 6341: train loss: 0.0006705933483317494, val loss: 0.09545648843050003\n",
      "Epoch 6342: train loss: 0.0010414541466161609, val loss: 0.09114966541528702\n",
      "Epoch 6343: train loss: 0.0010110351722687483, val loss: 0.09614719450473785\n",
      "Epoch 6344: train loss: 0.0003177007893100381, val loss: 0.09581900388002396\n",
      "Epoch 6345: train loss: 0.00014140165876597166, val loss: 0.09565404057502747\n",
      "Epoch 6346: train loss: 0.0004315295664127916, val loss: 0.09393327683210373\n",
      "Epoch 6347: train loss: 0.00011288258974673226, val loss: 0.09321433305740356\n",
      "Epoch 6348: train loss: 0.00020519942336250097, val loss: 0.09211881458759308\n",
      "Epoch 6349: train loss: 0.00012220449571032077, val loss: 0.09136029332876205\n",
      "Epoch 6350: train loss: 0.00012836740643251687, val loss: 0.09135323017835617\n",
      "Epoch 6351: train loss: 0.00010396118159405887, val loss: 0.08946966379880905\n",
      "Epoch 6352: train loss: 9.193779987981543e-05, val loss: 0.08908422291278839\n",
      "Epoch 6353: train loss: 6.469684012699872e-05, val loss: 0.0897626131772995\n",
      "Epoch 6354: train loss: 8.126242028083652e-05, val loss: 0.08885886520147324\n",
      "Epoch 6355: train loss: 5.249170499155298e-05, val loss: 0.08858548849821091\n",
      "Epoch 6356: train loss: 5.591271838056855e-05, val loss: 0.08791221678256989\n",
      "Epoch 6357: train loss: 4.3017411371693015e-05, val loss: 0.08654334396123886\n",
      "Epoch 6358: train loss: 4.517263369052671e-05, val loss: 0.08756794780492783\n",
      "Epoch 6359: train loss: 3.28826681652572e-05, val loss: 0.08852218836545944\n",
      "Epoch 6360: train loss: 3.1783907616045326e-05, val loss: 0.08711375296115875\n",
      "Epoch 6361: train loss: 3.2382100471295416e-05, val loss: 0.0859975665807724\n",
      "Epoch 6362: train loss: 1.918846282933373e-05, val loss: 0.0864817351102829\n",
      "Epoch 6363: train loss: 2.827015305228997e-05, val loss: 0.08700491487979889\n",
      "Epoch 6364: train loss: 1.650536069064401e-05, val loss: 0.08728235960006714\n",
      "Epoch 6365: train loss: 1.9495459127938375e-05, val loss: 0.08689627796411514\n",
      "Epoch 6366: train loss: 1.6182475519599393e-05, val loss: 0.0857577845454216\n",
      "Epoch 6367: train loss: 1.3884707186662126e-05, val loss: 0.08547548204660416\n",
      "Epoch 6368: train loss: 1.3057373507763259e-05, val loss: 0.08639972656965256\n",
      "Epoch 6369: train loss: 1.1241700121900067e-05, val loss: 0.08626406639814377\n",
      "Epoch 6370: train loss: 1.0205031685472932e-05, val loss: 0.08616199344396591\n",
      "Epoch 6371: train loss: 8.609992619312834e-06, val loss: 0.08607667684555054\n",
      "Epoch 6372: train loss: 8.022267138585448e-06, val loss: 0.08578421920537949\n",
      "Epoch 6373: train loss: 7.338014256674796e-06, val loss: 0.08601023256778717\n",
      "Epoch 6374: train loss: 5.900874839426251e-06, val loss: 0.08625777810811996\n",
      "Epoch 6375: train loss: 6.239136837393744e-06, val loss: 0.0859721228480339\n",
      "Epoch 6376: train loss: 4.4300481931713875e-06, val loss: 0.08592064678668976\n",
      "Epoch 6377: train loss: 5.490899638971314e-06, val loss: 0.0859709307551384\n",
      "Epoch 6378: train loss: 3.5340694921615068e-06, val loss: 0.08555640280246735\n",
      "Epoch 6379: train loss: 4.127773081563646e-06, val loss: 0.0853479728102684\n",
      "Epoch 6380: train loss: 3.2592656680208165e-06, val loss: 0.08566619455814362\n",
      "Epoch 6381: train loss: 3.01488398690708e-06, val loss: 0.08582084625959396\n",
      "Epoch 6382: train loss: 3.2121270123752765e-06, val loss: 0.08577778935432434\n",
      "Epoch 6383: train loss: 1.7462745063312468e-06, val loss: 0.08572719991207123\n",
      "Epoch 6384: train loss: 3.163945621054154e-06, val loss: 0.08562757819890976\n",
      "Epoch 6385: train loss: 1.232927161254338e-06, val loss: 0.08575641363859177\n",
      "Epoch 6386: train loss: 2.557129619162879e-06, val loss: 0.0858505591750145\n",
      "Epoch 6387: train loss: 1.1152784509249614e-06, val loss: 0.08568105846643448\n",
      "Epoch 6388: train loss: 2.0631891857192386e-06, val loss: 0.08589182794094086\n",
      "Epoch 6389: train loss: 9.385389603266958e-07, val loss: 0.08607497066259384\n",
      "Epoch 6390: train loss: 1.625563413654163e-06, val loss: 0.08578439801931381\n",
      "Epoch 6391: train loss: 9.97852566797519e-07, val loss: 0.08570034801959991\n",
      "Epoch 6392: train loss: 1.0908976264545345e-06, val loss: 0.08571024984121323\n",
      "Epoch 6393: train loss: 9.176163189295039e-07, val loss: 0.08575437217950821\n",
      "Epoch 6394: train loss: 7.359442975030106e-07, val loss: 0.08582070469856262\n",
      "Epoch 6395: train loss: 9.9058138403052e-07, val loss: 0.08560486882925034\n",
      "Epoch 6396: train loss: 5.03504054449877e-07, val loss: 0.08565761893987656\n",
      "Epoch 6397: train loss: 7.81853145781497e-07, val loss: 0.08571743965148926\n",
      "Epoch 6398: train loss: 4.031383582514536e-07, val loss: 0.08577392250299454\n",
      "Epoch 6399: train loss: 6.693556997561245e-07, val loss: 0.08585726469755173\n",
      "Epoch 6400: train loss: 3.800419108301867e-07, val loss: 0.08554262667894363\n",
      "Epoch 6401: train loss: 5.262206173028972e-07, val loss: 0.08558918535709381\n",
      "Epoch 6402: train loss: 4.1506655179546215e-07, val loss: 0.08565343171358109\n",
      "Epoch 6403: train loss: 3.776796120291692e-07, val loss: 0.08569882065057755\n",
      "Epoch 6404: train loss: 3.0067315037740627e-07, val loss: 0.08537708222866058\n",
      "Epoch 6405: train loss: 3.117795301932347e-07, val loss: 0.08544423431158066\n",
      "Epoch 6406: train loss: 2.419023132915754e-07, val loss: 0.08563186228275299\n",
      "Epoch 6407: train loss: 2.725050762819592e-07, val loss: 0.08550538122653961\n",
      "Epoch 6408: train loss: 1.5548839371604117e-07, val loss: 0.08530838787555695\n",
      "Epoch 6409: train loss: 2.3740872734379082e-07, val loss: 0.08531077206134796\n",
      "Epoch 6410: train loss: 1.793589490262093e-07, val loss: 0.08537411689758301\n",
      "Epoch 6411: train loss: 1.8096253029398213e-07, val loss: 0.08519518375396729\n",
      "Epoch 6412: train loss: 1.8037547988569713e-07, val loss: 0.08533331006765366\n",
      "Epoch 6413: train loss: 2.903784093177819e-07, val loss: 0.08494687080383301\n",
      "Epoch 6414: train loss: 7.58531712108379e-07, val loss: 0.08556409180164337\n",
      "Epoch 6415: train loss: 2.654142690516892e-06, val loss: 0.08419018238782883\n",
      "Epoch 6416: train loss: 1.0901007044594735e-05, val loss: 0.0868881568312645\n",
      "Epoch 6417: train loss: 4.560196248348802e-05, val loss: 0.08029370754957199\n",
      "Epoch 6418: train loss: 0.00020056436187587678, val loss: 0.09106900542974472\n",
      "Epoch 6419: train loss: 0.0007827189401723444, val loss: 0.07200830429792404\n",
      "Epoch 6420: train loss: 0.001854037749581039, val loss: 0.08864453434944153\n",
      "Epoch 6421: train loss: 0.001799235469661653, val loss: 0.08073706924915314\n",
      "Epoch 6422: train loss: 0.000230298304813914, val loss: 0.07962378114461899\n",
      "Epoch 6423: train loss: 0.0008625172195024788, val loss: 0.08311323076486588\n",
      "Epoch 6424: train loss: 0.00035237843985669315, val loss: 0.08996568620204926\n",
      "Epoch 6425: train loss: 0.0003346074081491679, val loss: 0.09549664705991745\n",
      "Epoch 6426: train loss: 0.0003799493133556098, val loss: 0.0972578227519989\n",
      "Epoch 6427: train loss: 0.0002263613569084555, val loss: 0.09671130776405334\n",
      "Epoch 6428: train loss: 0.0002168601204175502, val loss: 0.09708227217197418\n",
      "Epoch 6429: train loss: 0.00023328937822952867, val loss: 0.09695247560739517\n",
      "Epoch 6430: train loss: 0.0001320057490374893, val loss: 0.0968189612030983\n",
      "Epoch 6431: train loss: 0.00018186343368142843, val loss: 0.09763037413358688\n",
      "Epoch 6432: train loss: 0.00011600131256273016, val loss: 0.10021107643842697\n",
      "Epoch 6433: train loss: 0.0001281801814911887, val loss: 0.10270779579877853\n",
      "Epoch 6434: train loss: 8.678170706843957e-05, val loss: 0.10334783792495728\n",
      "Epoch 6435: train loss: 0.00010068804112961516, val loss: 0.10132160037755966\n",
      "Epoch 6436: train loss: 8.50942378747277e-05, val loss: 0.09760750830173492\n",
      "Epoch 6437: train loss: 6.180265336297452e-05, val loss: 0.09712495654821396\n",
      "Epoch 6438: train loss: 6.615516758756712e-05, val loss: 0.1000925675034523\n",
      "Epoch 6439: train loss: 5.526282620849088e-05, val loss: 0.10237815231084824\n",
      "Epoch 6440: train loss: 5.2691142627736554e-05, val loss: 0.10180359333753586\n",
      "Epoch 6441: train loss: 3.671296872198582e-05, val loss: 0.10126738995313644\n",
      "Epoch 6442: train loss: 4.8254845751216635e-05, val loss: 0.1021977886557579\n",
      "Epoch 6443: train loss: 2.7440995836514048e-05, val loss: 0.10219979286193848\n",
      "Epoch 6444: train loss: 3.62350583600346e-05, val loss: 0.10049028694629669\n",
      "Epoch 6445: train loss: 2.5933399228961207e-05, val loss: 0.09969746321439743\n",
      "Epoch 6446: train loss: 2.493886677257251e-05, val loss: 0.10140568017959595\n",
      "Epoch 6447: train loss: 2.1116309653734788e-05, val loss: 0.10344266891479492\n",
      "Epoch 6448: train loss: 1.9864031855831854e-05, val loss: 0.10373770445585251\n",
      "Epoch 6449: train loss: 1.852159584814217e-05, val loss: 0.10285399109125137\n",
      "Epoch 6450: train loss: 1.5547544535365887e-05, val loss: 0.10212328284978867\n",
      "Epoch 6451: train loss: 1.4312267921923194e-05, val loss: 0.10187607258558273\n",
      "Epoch 6452: train loss: 9.90783519227989e-06, val loss: 0.10194262117147446\n",
      "Epoch 6453: train loss: 1.4513298083329573e-05, val loss: 0.10212906450033188\n",
      "Epoch 6454: train loss: 8.919049832911696e-06, val loss: 0.10227055847644806\n",
      "Epoch 6455: train loss: 8.758738658798393e-06, val loss: 0.10260102897882462\n",
      "Epoch 6456: train loss: 7.614508376718732e-06, val loss: 0.10332097113132477\n",
      "Epoch 6457: train loss: 7.470945547538577e-06, val loss: 0.10380466282367706\n",
      "Epoch 6458: train loss: 7.161974281189032e-06, val loss: 0.10343603044748306\n",
      "Epoch 6459: train loss: 5.15750025442685e-06, val loss: 0.10271810740232468\n",
      "Epoch 6460: train loss: 5.160066393727902e-06, val loss: 0.10249996185302734\n",
      "Epoch 6461: train loss: 5.046235855843406e-06, val loss: 0.1026243343949318\n",
      "Epoch 6462: train loss: 4.797157089342363e-06, val loss: 0.10260200500488281\n",
      "Epoch 6463: train loss: 3.0542769309249707e-06, val loss: 0.10275004059076309\n",
      "Epoch 6464: train loss: 3.977065262006363e-06, val loss: 0.10329753160476685\n",
      "Epoch 6465: train loss: 2.8675519843091024e-06, val loss: 0.10369445383548737\n",
      "Epoch 6466: train loss: 3.526555246935459e-06, val loss: 0.10352473706007004\n",
      "Epoch 6467: train loss: 2.106224201270379e-06, val loss: 0.10296478122472763\n",
      "Epoch 6468: train loss: 2.4660901090101106e-06, val loss: 0.10239662230014801\n",
      "Epoch 6469: train loss: 2.2897604594618315e-06, val loss: 0.10231899470090866\n",
      "Epoch 6470: train loss: 1.981624563995865e-06, val loss: 0.10293620824813843\n",
      "Epoch 6471: train loss: 1.6398980733356439e-06, val loss: 0.10354623943567276\n",
      "Epoch 6472: train loss: 1.569436676618352e-06, val loss: 0.1033155545592308\n",
      "Epoch 6473: train loss: 1.3917044725531014e-06, val loss: 0.10285834223031998\n",
      "Epoch 6474: train loss: 1.5345955262091593e-06, val loss: 0.10290893167257309\n",
      "Epoch 6475: train loss: 8.3900368963441e-07, val loss: 0.1030227467417717\n",
      "Epoch 6476: train loss: 1.05179776710429e-06, val loss: 0.10280263423919678\n",
      "Epoch 6477: train loss: 1.1330373581586173e-06, val loss: 0.10270513594150543\n",
      "Epoch 6478: train loss: 5.634326498693554e-07, val loss: 0.10283026844263077\n",
      "Epoch 6479: train loss: 9.232027764483064e-07, val loss: 0.10294067114591599\n",
      "Epoch 6480: train loss: 5.68695213587489e-07, val loss: 0.10306137055158615\n",
      "Epoch 6481: train loss: 6.840942887720303e-07, val loss: 0.10294695943593979\n",
      "Epoch 6482: train loss: 5.942441134720866e-07, val loss: 0.102485790848732\n",
      "Epoch 6483: train loss: 4.1799350469773344e-07, val loss: 0.10235879570245743\n",
      "Epoch 6484: train loss: 5.349273237698071e-07, val loss: 0.10272432863712311\n",
      "Epoch 6485: train loss: 4.339804320352414e-07, val loss: 0.10280072689056396\n",
      "Epoch 6486: train loss: 3.3804488452915393e-07, val loss: 0.10256006568670273\n",
      "Epoch 6487: train loss: 3.847741822937678e-07, val loss: 0.10249429196119308\n",
      "Epoch 6488: train loss: 3.2010476047616976e-07, val loss: 0.10245463997125626\n",
      "Epoch 6489: train loss: 3.0055522870497953e-07, val loss: 0.10232184082269669\n",
      "Epoch 6490: train loss: 2.628007109706232e-07, val loss: 0.10231287777423859\n",
      "Epoch 6491: train loss: 2.958575464617752e-07, val loss: 0.10233845561742783\n",
      "Epoch 6492: train loss: 1.7996251244767336e-07, val loss: 0.10240556299686432\n",
      "Epoch 6493: train loss: 2.4171228574232373e-07, val loss: 0.10245122015476227\n",
      "Epoch 6494: train loss: 1.75970939153558e-07, val loss: 0.10229182243347168\n",
      "Epoch 6495: train loss: 1.8267844836827862e-07, val loss: 0.1021350845694542\n",
      "Epoch 6496: train loss: 1.403408020905772e-07, val loss: 0.10205288231372833\n",
      "Epoch 6497: train loss: 1.4972027884141426e-07, val loss: 0.1020522341132164\n",
      "Epoch 6498: train loss: 1.446977364594204e-07, val loss: 0.10220377892255783\n",
      "Epoch 6499: train loss: 1.2956013506482122e-07, val loss: 0.10221021622419357\n",
      "Epoch 6500: train loss: 9.02866759133758e-08, val loss: 0.10209492594003677\n",
      "Epoch 6501: train loss: 1.1113070286228321e-07, val loss: 0.10193287581205368\n",
      "Epoch 6502: train loss: 8.399295126082507e-08, val loss: 0.10181720554828644\n",
      "Epoch 6503: train loss: 9.525898292395141e-08, val loss: 0.10189878195524216\n",
      "Epoch 6504: train loss: 9.732401906603627e-08, val loss: 0.10188581794500351\n",
      "Epoch 6505: train loss: 8.583938182482598e-08, val loss: 0.10186729580163956\n",
      "Epoch 6506: train loss: 6.316008693829644e-08, val loss: 0.10178538411855698\n",
      "Epoch 6507: train loss: 4.7688047288829694e-08, val loss: 0.10174622386693954\n",
      "Epoch 6508: train loss: 5.205735575941617e-08, val loss: 0.10169508308172226\n",
      "Epoch 6509: train loss: 4.662164343471886e-08, val loss: 0.10158301889896393\n",
      "Epoch 6510: train loss: 6.359986315374044e-08, val loss: 0.10167413204908371\n",
      "Epoch 6511: train loss: 9.62786046443398e-08, val loss: 0.1015772819519043\n",
      "Epoch 6512: train loss: 1.5828400989903457e-07, val loss: 0.10162664949893951\n",
      "Epoch 6513: train loss: 3.153602108341147e-07, val loss: 0.101390041410923\n",
      "Epoch 6514: train loss: 8.355264640158566e-07, val loss: 0.10168170928955078\n",
      "Epoch 6515: train loss: 2.5885808554448886e-06, val loss: 0.10113994032144547\n",
      "Epoch 6516: train loss: 8.927053386287298e-06, val loss: 0.10206550359725952\n",
      "Epoch 6517: train loss: 3.07075897580944e-05, val loss: 0.10008299350738525\n",
      "Epoch 6518: train loss: 9.209182462655008e-05, val loss: 0.10347183793783188\n",
      "Epoch 6519: train loss: 0.000187934550922364, val loss: 0.09983290731906891\n",
      "Epoch 6520: train loss: 0.00027665909146890044, val loss: 0.10469353199005127\n",
      "Epoch 6521: train loss: 0.00030677919858135283, val loss: 0.0995170995593071\n",
      "Epoch 6522: train loss: 0.00027499045245349407, val loss: 0.1023174449801445\n",
      "Epoch 6523: train loss: 0.0002454589703120291, val loss: 0.10110266506671906\n",
      "Epoch 6524: train loss: 0.00010826237121364102, val loss: 0.10105261951684952\n",
      "Epoch 6525: train loss: 5.238462836132385e-05, val loss: 0.10088393837213516\n",
      "Epoch 6526: train loss: 0.0001341879542451352, val loss: 0.09889671951532364\n",
      "Epoch 6527: train loss: 9.617042087484151e-05, val loss: 0.10032059997320175\n",
      "Epoch 6528: train loss: 2.5780758733162656e-05, val loss: 0.10003205388784409\n",
      "Epoch 6529: train loss: 7.862896018195897e-05, val loss: 0.09786780178546906\n",
      "Epoch 6530: train loss: 6.0264421335887164e-05, val loss: 0.09927167743444443\n",
      "Epoch 6531: train loss: 1.9409610104048625e-05, val loss: 0.09963655471801758\n",
      "Epoch 6532: train loss: 5.8707861171569675e-05, val loss: 0.09751491993665695\n",
      "Epoch 6533: train loss: 2.4526922061340883e-05, val loss: 0.09796180576086044\n",
      "Epoch 6534: train loss: 2.515655978641007e-05, val loss: 0.09917429834604263\n",
      "Epoch 6535: train loss: 3.375573214725591e-05, val loss: 0.09786978363990784\n",
      "Epoch 6536: train loss: 1.6905652955756523e-05, val loss: 0.09750252217054367\n",
      "Epoch 6537: train loss: 2.1904670575167984e-05, val loss: 0.09835740178823471\n",
      "Epoch 6538: train loss: 2.03089202841511e-05, val loss: 0.09760908782482147\n",
      "Epoch 6539: train loss: 1.3241953638498671e-05, val loss: 0.09712745249271393\n",
      "Epoch 6540: train loss: 1.6873280401341617e-05, val loss: 0.0979052484035492\n",
      "Epoch 6541: train loss: 1.2892526683572214e-05, val loss: 0.09749086946249008\n",
      "Epoch 6542: train loss: 1.0513347660889849e-05, val loss: 0.09683968126773834\n",
      "Epoch 6543: train loss: 1.2218108167871833e-05, val loss: 0.09749807417392731\n",
      "Epoch 6544: train loss: 8.747185347601771e-06, val loss: 0.09738599509000778\n",
      "Epoch 6545: train loss: 6.793639386160066e-06, val loss: 0.09663804620504379\n",
      "Epoch 6546: train loss: 1.0739411663962528e-05, val loss: 0.09721213579177856\n",
      "Epoch 6547: train loss: 3.2296190966008e-06, val loss: 0.09749529510736465\n",
      "Epoch 6548: train loss: 8.63502464198973e-06, val loss: 0.09639745205640793\n",
      "Epoch 6549: train loss: 4.830828856938751e-06, val loss: 0.09648921340703964\n",
      "Epoch 6550: train loss: 3.922624273400288e-06, val loss: 0.09746334701776505\n",
      "Epoch 6551: train loss: 6.8843069129798096e-06, val loss: 0.09692471474409103\n",
      "Epoch 6552: train loss: 1.6934043287619716e-06, val loss: 0.09631551802158356\n",
      "Epoch 6553: train loss: 5.237561254034517e-06, val loss: 0.09695010632276535\n",
      "Epoch 6554: train loss: 3.383892817510059e-06, val loss: 0.09693094342947006\n",
      "Epoch 6555: train loss: 2.0949253212165786e-06, val loss: 0.09624254703521729\n",
      "Epoch 6556: train loss: 3.8712105379090644e-06, val loss: 0.09650442749261856\n",
      "Epoch 6557: train loss: 2.2187555259733927e-06, val loss: 0.09681393951177597\n",
      "Epoch 6558: train loss: 1.938109562615864e-06, val loss: 0.096537284553051\n",
      "Epoch 6559: train loss: 2.8777521947631612e-06, val loss: 0.09645891189575195\n",
      "Epoch 6560: train loss: 1.3494856148099643e-06, val loss: 0.09634827822446823\n",
      "Epoch 6561: train loss: 1.9731533029698767e-06, val loss: 0.09634194523096085\n",
      "Epoch 6562: train loss: 1.7557998717165901e-06, val loss: 0.09658784419298172\n",
      "Epoch 6563: train loss: 1.50835262502369e-06, val loss: 0.09632663428783417\n",
      "Epoch 6564: train loss: 1.1711633760569384e-06, val loss: 0.09611760079860687\n",
      "Epoch 6565: train loss: 1.2110267562093213e-06, val loss: 0.09643977135419846\n",
      "Epoch 6566: train loss: 1.3922229982199497e-06, val loss: 0.0963100716471672\n",
      "Epoch 6567: train loss: 5.865199455001857e-07, val loss: 0.09603308886289597\n",
      "Epoch 6568: train loss: 1.0591387535896502e-06, val loss: 0.09623260796070099\n",
      "Epoch 6569: train loss: 8.856474664753478e-07, val loss: 0.09616170078516006\n",
      "Epoch 6570: train loss: 7.149166094677639e-07, val loss: 0.09603449702262878\n",
      "Epoch 6571: train loss: 5.424033133749617e-07, val loss: 0.09617847204208374\n",
      "Epoch 6572: train loss: 7.463745532731991e-07, val loss: 0.09611008316278458\n",
      "Epoch 6573: train loss: 6.718055374221876e-07, val loss: 0.09598955512046814\n",
      "Epoch 6574: train loss: 4.1229625935557124e-07, val loss: 0.09589903801679611\n",
      "Epoch 6575: train loss: 4.577204890665598e-07, val loss: 0.09589077532291412\n",
      "Epoch 6576: train loss: 4.831852038478246e-07, val loss: 0.09594941884279251\n",
      "Epoch 6577: train loss: 5.602593660114508e-07, val loss: 0.09575437754392624\n",
      "Epoch 6578: train loss: 3.5917608443014615e-07, val loss: 0.09570439904928207\n",
      "Epoch 6579: train loss: 2.413151491964527e-07, val loss: 0.09576346725225449\n",
      "Epoch 6580: train loss: 3.302739344235306e-07, val loss: 0.09568603336811066\n",
      "Epoch 6581: train loss: 4.290089918868034e-07, val loss: 0.09559645503759384\n",
      "Epoch 6582: train loss: 3.7444505096573266e-07, val loss: 0.09550588577985764\n",
      "Epoch 6583: train loss: 3.615571699810971e-07, val loss: 0.09569406509399414\n",
      "Epoch 6584: train loss: 3.357718014740385e-07, val loss: 0.09540432691574097\n",
      "Epoch 6585: train loss: 2.2801364707447647e-07, val loss: 0.09549643844366074\n",
      "Epoch 6586: train loss: 1.9891126612492371e-07, val loss: 0.0954555869102478\n",
      "Epoch 6587: train loss: 1.903193975749673e-07, val loss: 0.09535732120275497\n",
      "Epoch 6588: train loss: 1.9008217577720643e-07, val loss: 0.09536109119653702\n",
      "Epoch 6589: train loss: 2.3102022339571704e-07, val loss: 0.09525399655103683\n",
      "Epoch 6590: train loss: 2.907313785271981e-07, val loss: 0.09535742551088333\n",
      "Epoch 6591: train loss: 3.7998802326910663e-07, val loss: 0.09509926289319992\n",
      "Epoch 6592: train loss: 6.172384132696607e-07, val loss: 0.09530919045209885\n",
      "Epoch 6593: train loss: 1.1712392051776988e-06, val loss: 0.09492180496454239\n",
      "Epoch 6594: train loss: 2.6517020614846842e-06, val loss: 0.09553759545087814\n",
      "Epoch 6595: train loss: 6.947626388864592e-06, val loss: 0.09451688826084137\n",
      "Epoch 6596: train loss: 2.081425736832898e-05, val loss: 0.09601209312677383\n",
      "Epoch 6597: train loss: 6.707214924972504e-05, val loss: 0.09422381222248077\n",
      "Epoch 6598: train loss: 0.00022495820303447545, val loss: 0.09944887459278107\n",
      "Epoch 6599: train loss: 0.0007486572139896452, val loss: 0.09399516880512238\n",
      "Epoch 6600: train loss: 0.001959026325494051, val loss: 0.10391034930944443\n",
      "Epoch 6601: train loss: 0.0034965132363140583, val loss: 0.08414389938116074\n",
      "Epoch 6602: train loss: 0.002085266634821892, val loss: 0.09041543304920197\n",
      "Epoch 6603: train loss: 0.0007467435789294541, val loss: 0.10030441731214523\n",
      "Epoch 6604: train loss: 0.0011948716128244996, val loss: 0.09246258437633514\n",
      "Epoch 6605: train loss: 0.0004892171127721667, val loss: 0.0883537083864212\n",
      "Epoch 6606: train loss: 0.0005459442618303001, val loss: 0.09020816534757614\n",
      "Epoch 6607: train loss: 0.0005952017963863909, val loss: 0.0919341892004013\n",
      "Epoch 6608: train loss: 0.0001646926102694124, val loss: 0.09128797799348831\n",
      "Epoch 6609: train loss: 0.0004132533213123679, val loss: 0.08954256027936935\n",
      "Epoch 6610: train loss: 0.0003042332537006587, val loss: 0.08866798132658005\n",
      "Epoch 6611: train loss: 0.00011405965778976679, val loss: 0.08934938907623291\n",
      "Epoch 6612: train loss: 0.0002447302103973925, val loss: 0.09033958613872528\n",
      "Epoch 6613: train loss: 0.00020250833767931908, val loss: 0.09093328565359116\n",
      "Epoch 6614: train loss: 0.00011639631702564657, val loss: 0.09109660238027573\n",
      "Epoch 6615: train loss: 0.00012528333172667772, val loss: 0.09031092375516891\n",
      "Epoch 6616: train loss: 0.00011534113582456484, val loss: 0.0887819230556488\n",
      "Epoch 6617: train loss: 0.00010694824595702812, val loss: 0.08733849227428436\n",
      "Epoch 6618: train loss: 8.627845090813935e-05, val loss: 0.08711926639080048\n",
      "Epoch 6619: train loss: 6.372571806423366e-05, val loss: 0.08847855776548386\n",
      "Epoch 6620: train loss: 8.170435467036441e-05, val loss: 0.0902203693985939\n",
      "Epoch 6621: train loss: 7.127047865651548e-05, val loss: 0.09107198566198349\n",
      "Epoch 6622: train loss: 4.3557098251767457e-05, val loss: 0.09073337912559509\n",
      "Epoch 6623: train loss: 4.006452581961639e-05, val loss: 0.08897922188043594\n",
      "Epoch 6624: train loss: 5.241316830506548e-05, val loss: 0.08733931183815002\n",
      "Epoch 6625: train loss: 4.206645462545566e-05, val loss: 0.08706758171319962\n",
      "Epoch 6626: train loss: 2.5691455448395573e-05, val loss: 0.08833995461463928\n",
      "Epoch 6627: train loss: 2.1965368432574905e-05, val loss: 0.08995302766561508\n",
      "Epoch 6628: train loss: 2.7541957024368457e-05, val loss: 0.09066809713840485\n",
      "Epoch 6629: train loss: 2.244815004814882e-05, val loss: 0.09022804349660873\n",
      "Epoch 6630: train loss: 1.3598417353932746e-05, val loss: 0.08909755945205688\n",
      "Epoch 6631: train loss: 1.625160439289175e-05, val loss: 0.0879754051566124\n",
      "Epoch 6632: train loss: 1.6703945220797323e-05, val loss: 0.08769045025110245\n",
      "Epoch 6633: train loss: 1.1634080692601856e-05, val loss: 0.08827235549688339\n",
      "Epoch 6634: train loss: 9.77191302808933e-06, val loss: 0.08885600417852402\n",
      "Epoch 6635: train loss: 1.2427721230778843e-05, val loss: 0.08868449181318283\n",
      "Epoch 6636: train loss: 9.805292393139098e-06, val loss: 0.08804314583539963\n",
      "Epoch 6637: train loss: 6.800366918469081e-06, val loss: 0.08753611147403717\n",
      "Epoch 6638: train loss: 7.677319445065223e-06, val loss: 0.08744525909423828\n",
      "Epoch 6639: train loss: 7.684961019549519e-06, val loss: 0.08766201883554459\n",
      "Epoch 6640: train loss: 6.107914487074595e-06, val loss: 0.08789151161909103\n",
      "Epoch 6641: train loss: 4.9337450036546215e-06, val loss: 0.08790551126003265\n",
      "Epoch 6642: train loss: 5.5088971748773474e-06, val loss: 0.08774000406265259\n",
      "Epoch 6643: train loss: 4.7769972297828645e-06, val loss: 0.08766501396894455\n",
      "Epoch 6644: train loss: 3.9966371332411654e-06, val loss: 0.08766280859708786\n",
      "Epoch 6645: train loss: 3.788978574448265e-06, val loss: 0.08756832033395767\n",
      "Epoch 6646: train loss: 3.497600118862465e-06, val loss: 0.08730455487966537\n",
      "Epoch 6647: train loss: 3.0482763122563483e-06, val loss: 0.08705111593008041\n",
      "Epoch 6648: train loss: 2.7821238290925976e-06, val loss: 0.08701244741678238\n",
      "Epoch 6649: train loss: 2.444917072352837e-06, val loss: 0.08713654428720474\n",
      "Epoch 6650: train loss: 2.0534080249490216e-06, val loss: 0.08728097379207611\n",
      "Epoch 6651: train loss: 2.337201976843062e-06, val loss: 0.0871538519859314\n",
      "Epoch 6652: train loss: 1.582092068019847e-06, val loss: 0.08671154081821442\n",
      "Epoch 6653: train loss: 1.0995203183483682e-06, val loss: 0.08629965037107468\n",
      "Epoch 6654: train loss: 1.891291503852699e-06, val loss: 0.08627896755933762\n",
      "Epoch 6655: train loss: 1.386785243084887e-06, val loss: 0.08654133975505829\n",
      "Epoch 6656: train loss: 6.460237500505173e-07, val loss: 0.08669518679380417\n",
      "Epoch 6657: train loss: 1.2565682254717103e-06, val loss: 0.08660286664962769\n",
      "Epoch 6658: train loss: 1.1666140835586702e-06, val loss: 0.08644752949476242\n",
      "Epoch 6659: train loss: 5.352587209017656e-07, val loss: 0.08641117066144943\n",
      "Epoch 6660: train loss: 8.327589284817805e-07, val loss: 0.08648434281349182\n",
      "Epoch 6661: train loss: 7.913233730505453e-07, val loss: 0.08654029667377472\n",
      "Epoch 6662: train loss: 5.692909894605691e-07, val loss: 0.08645646274089813\n",
      "Epoch 6663: train loss: 5.272470957606856e-07, val loss: 0.0863015428185463\n",
      "Epoch 6664: train loss: 5.436213541543111e-07, val loss: 0.08627159148454666\n",
      "Epoch 6665: train loss: 5.585978897215682e-07, val loss: 0.08644302934408188\n",
      "Epoch 6666: train loss: 3.3195328796864487e-07, val loss: 0.08660572022199631\n",
      "Epoch 6667: train loss: 3.8990265238680877e-07, val loss: 0.08657246083021164\n",
      "Epoch 6668: train loss: 3.760019069432019e-07, val loss: 0.0864327922463417\n",
      "Epoch 6669: train loss: 3.1267782674149203e-07, val loss: 0.08637329190969467\n",
      "Epoch 6670: train loss: 2.9234945486678043e-07, val loss: 0.08640702813863754\n",
      "Epoch 6671: train loss: 2.6669803787626734e-07, val loss: 0.08643592894077301\n",
      "Epoch 6672: train loss: 2.6072007131006103e-07, val loss: 0.08641461282968521\n",
      "Epoch 6673: train loss: 2.0440187142867217e-07, val loss: 0.08639881759881973\n",
      "Epoch 6674: train loss: 1.939024940611489e-07, val loss: 0.08647674322128296\n",
      "Epoch 6675: train loss: 1.5386505936021422e-07, val loss: 0.08655994385480881\n",
      "Epoch 6676: train loss: 1.638608750909043e-07, val loss: 0.08655142039060593\n",
      "Epoch 6677: train loss: 1.3834480228069879e-07, val loss: 0.08649074286222458\n",
      "Epoch 6678: train loss: 9.96443603185071e-08, val loss: 0.08646903187036514\n",
      "Epoch 6679: train loss: 1.225456429665428e-07, val loss: 0.08650936931371689\n",
      "Epoch 6680: train loss: 1.1236861752195182e-07, val loss: 0.08655667304992676\n",
      "Epoch 6681: train loss: 7.128544865508957e-08, val loss: 0.0865434780716896\n",
      "Epoch 6682: train loss: 1.0049094356645583e-07, val loss: 0.08650700002908707\n",
      "Epoch 6683: train loss: 7.565834181377795e-08, val loss: 0.08645602315664291\n",
      "Epoch 6684: train loss: 6.849384703855321e-08, val loss: 0.08633830398321152\n",
      "Epoch 6685: train loss: 6.444136602112849e-08, val loss: 0.08623035997152328\n",
      "Epoch 6686: train loss: 6.019509157795255e-08, val loss: 0.08623076975345612\n",
      "Epoch 6687: train loss: 5.322575447053168e-08, val loss: 0.08622574061155319\n",
      "Epoch 6688: train loss: 4.9149821990113196e-08, val loss: 0.08611353486776352\n",
      "Epoch 6689: train loss: 4.228456162991279e-08, val loss: 0.08599942177534103\n",
      "Epoch 6690: train loss: 4.604682501962998e-08, val loss: 0.08596701174974442\n",
      "Epoch 6691: train loss: 3.397428471885178e-08, val loss: 0.0859551951289177\n",
      "Epoch 6692: train loss: 3.462016806565771e-08, val loss: 0.08587872982025146\n",
      "Epoch 6693: train loss: 2.98698275003062e-08, val loss: 0.08578283339738846\n",
      "Epoch 6694: train loss: 3.310854168603328e-08, val loss: 0.08573057502508163\n",
      "Epoch 6695: train loss: 2.6600778468832686e-08, val loss: 0.08567573875188828\n",
      "Epoch 6696: train loss: 2.6236367745013922e-08, val loss: 0.08564939349889755\n",
      "Epoch 6697: train loss: 2.185478997773771e-08, val loss: 0.08559063076972961\n",
      "Epoch 6698: train loss: 2.3846009611361296e-08, val loss: 0.08548735827207565\n",
      "Epoch 6699: train loss: 1.593749665573796e-08, val loss: 0.0854237824678421\n",
      "Epoch 6700: train loss: 1.9790025618249274e-08, val loss: 0.08539830893278122\n",
      "Epoch 6701: train loss: 1.8471761009664078e-08, val loss: 0.08534826338291168\n",
      "Epoch 6702: train loss: 1.5942447362249368e-08, val loss: 0.08524785190820694\n",
      "Epoch 6703: train loss: 1.578291985993019e-08, val loss: 0.08520867675542831\n",
      "Epoch 6704: train loss: 2.128301090920104e-08, val loss: 0.08510258793830872\n",
      "Epoch 6705: train loss: 3.770277245962461e-08, val loss: 0.08513214439153671\n",
      "Epoch 6706: train loss: 9.263818157023707e-08, val loss: 0.0849272683262825\n",
      "Epoch 6707: train loss: 3.3678048794172355e-07, val loss: 0.08525107055902481\n",
      "Epoch 6708: train loss: 1.4588003978133202e-06, val loss: 0.08444681018590927\n",
      "Epoch 6709: train loss: 6.7956934799440205e-06, val loss: 0.08682353794574738\n",
      "Epoch 6710: train loss: 3.246488267905079e-05, val loss: 0.08420735597610474\n",
      "Epoch 6711: train loss: 0.00013363406469579786, val loss: 0.09002409130334854\n",
      "Epoch 6712: train loss: 0.00041032020817510784, val loss: 0.08073264360427856\n",
      "Epoch 6713: train loss: 0.0006765816360712051, val loss: 0.08477864414453506\n",
      "Epoch 6714: train loss: 0.0005823963438160717, val loss: 0.08114119619131088\n",
      "Epoch 6715: train loss: 3.6427329177968204e-05, val loss: 0.0828884020447731\n",
      "Epoch 6716: train loss: 0.0004565174167510122, val loss: 0.0833178237080574\n",
      "Epoch 6717: train loss: 0.00011179441207787022, val loss: 0.08519792556762695\n",
      "Epoch 6718: train loss: 0.00023394852178171277, val loss: 0.08652647584676743\n",
      "Epoch 6719: train loss: 8.529992192052305e-05, val loss: 0.08614220470190048\n",
      "Epoch 6720: train loss: 0.00017174959066323936, val loss: 0.08409631252288818\n",
      "Epoch 6721: train loss: 6.387662142515182e-05, val loss: 0.08555369079113007\n",
      "Epoch 6722: train loss: 0.00011782190995290875, val loss: 0.08909256756305695\n",
      "Epoch 6723: train loss: 6.34028619970195e-05, val loss: 0.09026697278022766\n",
      "Epoch 6724: train loss: 7.018473115749657e-05, val loss: 0.08819855749607086\n",
      "Epoch 6725: train loss: 5.980607238598168e-05, val loss: 0.08741163462400436\n",
      "Epoch 6726: train loss: 5.670827522408217e-05, val loss: 0.0894368514418602\n",
      "Epoch 6727: train loss: 3.705970448208973e-05, val loss: 0.09095948934555054\n",
      "Epoch 6728: train loss: 5.9425165090942755e-05, val loss: 0.08982779830694199\n",
      "Epoch 6729: train loss: 2.0483461412368342e-05, val loss: 0.08828606456518173\n",
      "Epoch 6730: train loss: 4.495642497204244e-05, val loss: 0.08906351774930954\n",
      "Epoch 6731: train loss: 2.7321302695781924e-05, val loss: 0.09131181985139847\n",
      "Epoch 6732: train loss: 2.385197876719758e-05, val loss: 0.09191934019327164\n",
      "Epoch 6733: train loss: 3.184894740115851e-05, val loss: 0.09057841449975967\n",
      "Epoch 6734: train loss: 1.5174578038568143e-05, val loss: 0.08964867889881134\n",
      "Epoch 6735: train loss: 2.607118040032219e-05, val loss: 0.09035450220108032\n",
      "Epoch 6736: train loss: 1.317236547038192e-05, val loss: 0.09163570404052734\n",
      "Epoch 6737: train loss: 1.8303397155250423e-05, val loss: 0.09193781018257141\n",
      "Epoch 6738: train loss: 1.2991373296245001e-05, val loss: 0.09124628454446793\n",
      "Epoch 6739: train loss: 1.4172284863889217e-05, val loss: 0.09086551517248154\n",
      "Epoch 6740: train loss: 1.0161817044718191e-05, val loss: 0.09138768911361694\n",
      "Epoch 6741: train loss: 1.2498396245064214e-05, val loss: 0.09169270098209381\n",
      "Epoch 6742: train loss: 7.5578095675155055e-06, val loss: 0.09107902646064758\n",
      "Epoch 6743: train loss: 1.0061934517580085e-05, val loss: 0.09051167219877243\n",
      "Epoch 6744: train loss: 6.9979178078938276e-06, val loss: 0.09110865741968155\n",
      "Epoch 6745: train loss: 7.14424959369353e-06, val loss: 0.09217698872089386\n",
      "Epoch 6746: train loss: 6.307525836746208e-06, val loss: 0.09212687611579895\n",
      "Epoch 6747: train loss: 5.512045845534885e-06, val loss: 0.09107211232185364\n",
      "Epoch 6748: train loss: 5.70859037907212e-06, val loss: 0.09048619121313095\n",
      "Epoch 6749: train loss: 4.063788765051868e-06, val loss: 0.09106909483671188\n",
      "Epoch 6750: train loss: 4.6966260924818926e-06, val loss: 0.09164516627788544\n",
      "Epoch 6751: train loss: 3.5463006042846246e-06, val loss: 0.09148187935352325\n",
      "Epoch 6752: train loss: 3.7516326756303897e-06, val loss: 0.09115097671747208\n",
      "Epoch 6753: train loss: 2.643045718286885e-06, val loss: 0.0912695899605751\n",
      "Epoch 6754: train loss: 3.49967262991413e-06, val loss: 0.09157007187604904\n",
      "Epoch 6755: train loss: 1.8450750758347567e-06, val loss: 0.09157945215702057\n",
      "Epoch 6756: train loss: 3.0981907457316993e-06, val loss: 0.09124626219272614\n",
      "Epoch 6757: train loss: 1.5923802720863023e-06, val loss: 0.09114386141300201\n",
      "Epoch 6758: train loss: 2.305012003489537e-06, val loss: 0.0914013609290123\n",
      "Epoch 6759: train loss: 1.418877104697458e-06, val loss: 0.0915912613272667\n",
      "Epoch 6760: train loss: 1.7908796507981606e-06, val loss: 0.09147924184799194\n",
      "Epoch 6761: train loss: 1.2619666449609213e-06, val loss: 0.09131129086017609\n",
      "Epoch 6762: train loss: 1.3832583363182493e-06, val loss: 0.0913451537489891\n",
      "Epoch 6763: train loss: 1.0363686442360631e-06, val loss: 0.09145615249872208\n",
      "Epoch 6764: train loss: 1.1951386795772123e-06, val loss: 0.09146138280630112\n",
      "Epoch 6765: train loss: 8.727925546736515e-07, val loss: 0.09141352027654648\n",
      "Epoch 6766: train loss: 8.558164381611277e-07, val loss: 0.09142804890871048\n",
      "Epoch 6767: train loss: 8.601431886745559e-07, val loss: 0.09144521504640579\n",
      "Epoch 6768: train loss: 6.567416335201415e-07, val loss: 0.09138958901166916\n",
      "Epoch 6769: train loss: 6.895830892972299e-07, val loss: 0.09137692302465439\n",
      "Epoch 6770: train loss: 6.22487618784362e-07, val loss: 0.09141328185796738\n",
      "Epoch 6771: train loss: 4.995995936951658e-07, val loss: 0.09143492579460144\n",
      "Epoch 6772: train loss: 5.266994094199617e-07, val loss: 0.09147796779870987\n",
      "Epoch 6773: train loss: 4.551522749807191e-07, val loss: 0.09145384281873703\n",
      "Epoch 6774: train loss: 3.82862680226026e-07, val loss: 0.09138517081737518\n",
      "Epoch 6775: train loss: 4.143562648550869e-07, val loss: 0.09142883121967316\n",
      "Epoch 6776: train loss: 3.387747256056173e-07, val loss: 0.09146221727132797\n",
      "Epoch 6777: train loss: 3.2970237384688517e-07, val loss: 0.09140082448720932\n",
      "Epoch 6778: train loss: 2.381679706786599e-07, val loss: 0.09135513752698898\n",
      "Epoch 6779: train loss: 2.8503484372777166e-07, val loss: 0.09142249822616577\n",
      "Epoch 6780: train loss: 2.6414221565573826e-07, val loss: 0.09150712937116623\n",
      "Epoch 6781: train loss: 1.6649418910219538e-07, val loss: 0.09146475046873093\n",
      "Epoch 6782: train loss: 2.1902087610214949e-07, val loss: 0.09141986072063446\n",
      "Epoch 6783: train loss: 1.6509839895206824e-07, val loss: 0.09137136489152908\n",
      "Epoch 6784: train loss: 2.0256804589280364e-07, val loss: 0.0913100466132164\n",
      "Epoch 6785: train loss: 1.405945084798077e-07, val loss: 0.09141313284635544\n",
      "Epoch 6786: train loss: 1.466639929503799e-07, val loss: 0.0914575606584549\n",
      "Epoch 6787: train loss: 1.2150270833899413e-07, val loss: 0.09132646769285202\n",
      "Epoch 6788: train loss: 1.058666825315413e-07, val loss: 0.09123492240905762\n",
      "Epoch 6789: train loss: 1.3155634803752037e-07, val loss: 0.09134232997894287\n",
      "Epoch 6790: train loss: 1.4348057675306336e-07, val loss: 0.09136894345283508\n",
      "Epoch 6791: train loss: 1.1173889191695707e-07, val loss: 0.09122397750616074\n",
      "Epoch 6792: train loss: 1.416970007994678e-07, val loss: 0.09129723161458969\n",
      "Epoch 6793: train loss: 1.846577788455761e-07, val loss: 0.09124580770730972\n",
      "Epoch 6794: train loss: 1.9767497860812e-07, val loss: 0.09123548120260239\n",
      "Epoch 6795: train loss: 2.0875344830528775e-07, val loss: 0.09124251455068588\n",
      "Epoch 6796: train loss: 2.797631566409109e-07, val loss: 0.09120935201644897\n",
      "Epoch 6797: train loss: 4.6742289327994513e-07, val loss: 0.09117783606052399\n",
      "Epoch 6798: train loss: 1.1564803799046786e-06, val loss: 0.09107376635074615\n",
      "Epoch 6799: train loss: 3.956537966587348e-06, val loss: 0.09129669517278671\n",
      "Epoch 6800: train loss: 1.5648009139113128e-05, val loss: 0.09066330641508102\n",
      "Epoch 6801: train loss: 6.156206654850394e-05, val loss: 0.09164126962423325\n",
      "Epoch 6802: train loss: 0.00024284431128762662, val loss: 0.08935792744159698\n",
      "Epoch 6803: train loss: 0.0007570773595944047, val loss: 0.09036729484796524\n",
      "Epoch 6804: train loss: 0.001537129282951355, val loss: 0.08912737667560577\n",
      "Epoch 6805: train loss: 0.0006873569218441844, val loss: 0.09021534025669098\n",
      "Epoch 6806: train loss: 0.0002837955253198743, val loss: 0.09281682223081589\n",
      "Epoch 6807: train loss: 0.00038305498310364783, val loss: 0.08889927715063095\n",
      "Epoch 6808: train loss: 0.00023892679018899798, val loss: 0.08878625929355621\n",
      "Epoch 6809: train loss: 0.00016924287774600089, val loss: 0.0926656648516655\n",
      "Epoch 6810: train loss: 0.00022985671239439398, val loss: 0.09216687828302383\n",
      "Epoch 6811: train loss: 0.00011303202336421236, val loss: 0.0922946184873581\n",
      "Epoch 6812: train loss: 0.00011203673057025298, val loss: 0.09428752213716507\n",
      "Epoch 6813: train loss: 0.00014550484775099903, val loss: 0.09552120417356491\n",
      "Epoch 6814: train loss: 7.451769488397986e-05, val loss: 0.09776761382818222\n",
      "Epoch 6815: train loss: 7.856537558836862e-05, val loss: 0.09900892525911331\n",
      "Epoch 6816: train loss: 8.030500612221658e-05, val loss: 0.09770761430263519\n",
      "Epoch 6817: train loss: 7.38571397960186e-05, val loss: 0.09933453053236008\n",
      "Epoch 6818: train loss: 3.980302426498383e-05, val loss: 0.1029115691781044\n",
      "Epoch 6819: train loss: 4.7932036977726966e-05, val loss: 0.10289990156888962\n",
      "Epoch 6820: train loss: 5.6380045862169936e-05, val loss: 0.10106606781482697\n",
      "Epoch 6821: train loss: 3.301498873042874e-05, val loss: 0.10149326175451279\n",
      "Epoch 6822: train loss: 2.979107557621319e-05, val loss: 0.10292813926935196\n",
      "Epoch 6823: train loss: 3.142251443932764e-05, val loss: 0.10283058881759644\n",
      "Epoch 6824: train loss: 3.238652061554603e-05, val loss: 0.1017916351556778\n",
      "Epoch 6825: train loss: 2.4144103008438833e-05, val loss: 0.10189701616764069\n",
      "Epoch 6826: train loss: 1.7382992155035026e-05, val loss: 0.10272441059350967\n",
      "Epoch 6827: train loss: 2.1528408979065716e-05, val loss: 0.10264710336923599\n",
      "Epoch 6828: train loss: 2.1510482838493772e-05, val loss: 0.10305457562208176\n",
      "Epoch 6829: train loss: 1.8262035155203193e-05, val loss: 0.10295160859823227\n",
      "Epoch 6830: train loss: 1.0865911463042721e-05, val loss: 0.10152081400156021\n",
      "Epoch 6831: train loss: 1.7570126146893017e-05, val loss: 0.1019635945558548\n",
      "Epoch 6832: train loss: 1.7892414689413272e-05, val loss: 0.10333817452192307\n",
      "Epoch 6833: train loss: 1.3107886843499728e-05, val loss: 0.10370086878538132\n",
      "Epoch 6834: train loss: 8.199625881388783e-06, val loss: 0.10402116924524307\n",
      "Epoch 6835: train loss: 1.100749068427831e-05, val loss: 0.1038733646273613\n",
      "Epoch 6836: train loss: 9.759356544236653e-06, val loss: 0.10392097383737564\n",
      "Epoch 6837: train loss: 6.3951938500395045e-06, val loss: 0.10449317842721939\n",
      "Epoch 6838: train loss: 6.182103788887616e-06, val loss: 0.10422220081090927\n",
      "Epoch 6839: train loss: 6.975842097745044e-06, val loss: 0.10406827926635742\n",
      "Epoch 6840: train loss: 5.69954090678948e-06, val loss: 0.10448986291885376\n",
      "Epoch 6841: train loss: 4.987560259905877e-06, val loss: 0.1044476255774498\n",
      "Epoch 6842: train loss: 4.49141316494206e-06, val loss: 0.10511016845703125\n",
      "Epoch 6843: train loss: 4.117496246180963e-06, val loss: 0.10469436645507812\n",
      "Epoch 6844: train loss: 4.257781256455928e-06, val loss: 0.10408493131399155\n",
      "Epoch 6845: train loss: 2.953323246401851e-06, val loss: 0.10520746558904648\n",
      "Epoch 6846: train loss: 3.204523181921104e-06, val loss: 0.10522657632827759\n",
      "Epoch 6847: train loss: 3.7618071928591235e-06, val loss: 0.1051698550581932\n",
      "Epoch 6848: train loss: 2.3825589323678287e-06, val loss: 0.10527750104665756\n",
      "Epoch 6849: train loss: 2.22206290345639e-06, val loss: 0.10517846792936325\n",
      "Epoch 6850: train loss: 1.879072328847542e-06, val loss: 0.10492541640996933\n",
      "Epoch 6851: train loss: 2.2291881123237545e-06, val loss: 0.10497915744781494\n",
      "Epoch 6852: train loss: 2.0789445898117265e-06, val loss: 0.10525353252887726\n",
      "Epoch 6853: train loss: 2.316905010957271e-06, val loss: 0.10502339899539948\n",
      "Epoch 6854: train loss: 4.162567165622022e-06, val loss: 0.105540432035923\n",
      "Epoch 6855: train loss: 1.1164565876242705e-05, val loss: 0.10483592748641968\n",
      "Epoch 6856: train loss: 4.347494541434571e-05, val loss: 0.10552982240915298\n",
      "Epoch 6857: train loss: 0.0001266016624867916, val loss: 0.10221897810697556\n",
      "Epoch 6858: train loss: 0.00026008463464677334, val loss: 0.10565488785505295\n",
      "Epoch 6859: train loss: 0.00037108545075170696, val loss: 0.09986884891986847\n",
      "Epoch 6860: train loss: 0.0002353977324673906, val loss: 0.09925434738397598\n",
      "Epoch 6861: train loss: 6.982737977523357e-05, val loss: 0.09972504526376724\n",
      "Epoch 6862: train loss: 0.00013265106827020645, val loss: 0.10067252069711685\n",
      "Epoch 6863: train loss: 0.0001341596944257617, val loss: 0.09797850251197815\n",
      "Epoch 6864: train loss: 9.178993059322238e-05, val loss: 0.09721138328313828\n",
      "Epoch 6865: train loss: 7.756439299555495e-05, val loss: 0.09955998510122299\n",
      "Epoch 6866: train loss: 6.835113890701905e-05, val loss: 0.09848172962665558\n",
      "Epoch 6867: train loss: 6.279384979279712e-05, val loss: 0.0953737124800682\n",
      "Epoch 6868: train loss: 5.5748369049979374e-05, val loss: 0.09716188162565231\n",
      "Epoch 6869: train loss: 4.2200248572044075e-05, val loss: 0.09918775409460068\n",
      "Epoch 6870: train loss: 4.213758074911311e-05, val loss: 0.09719918668270111\n",
      "Epoch 6871: train loss: 4.119016375625506e-05, val loss: 0.09629225730895996\n",
      "Epoch 6872: train loss: 2.437730654492043e-05, val loss: 0.0973406583070755\n",
      "Epoch 6873: train loss: 3.983237184002064e-05, val loss: 0.09653334319591522\n",
      "Epoch 6874: train loss: 1.9273766156402417e-05, val loss: 0.09550952911376953\n",
      "Epoch 6875: train loss: 2.6309804525226355e-05, val loss: 0.09659209102392197\n",
      "Epoch 6876: train loss: 2.342118568776641e-05, val loss: 0.09733082354068756\n",
      "Epoch 6877: train loss: 1.3220298569649458e-05, val loss: 0.09659593552350998\n",
      "Epoch 6878: train loss: 2.3434740796801634e-05, val loss: 0.09681370109319687\n",
      "Epoch 6879: train loss: 1.0414738426334225e-05, val loss: 0.09733158349990845\n",
      "Epoch 6880: train loss: 1.6078163753263652e-05, val loss: 0.09650136530399323\n",
      "Epoch 6881: train loss: 1.2894141946162563e-05, val loss: 0.09608675539493561\n",
      "Epoch 6882: train loss: 1.0610519893816672e-05, val loss: 0.09709256887435913\n",
      "Epoch 6883: train loss: 1.1884291780006606e-05, val loss: 0.09743054956197739\n",
      "Epoch 6884: train loss: 1.0424823813082185e-05, val loss: 0.09678194671869278\n",
      "Epoch 6885: train loss: 9.53579910856206e-06, val loss: 0.096671462059021\n",
      "Epoch 6886: train loss: 6.263071554712951e-06, val loss: 0.09745580703020096\n",
      "Epoch 6887: train loss: 8.164652172126807e-06, val loss: 0.09782221913337708\n",
      "Epoch 6888: train loss: 5.565244919125689e-06, val loss: 0.09707161039113998\n",
      "Epoch 6889: train loss: 4.935412562190322e-06, val loss: 0.09651391953229904\n",
      "Epoch 6890: train loss: 6.4079949879669584e-06, val loss: 0.0970078781247139\n",
      "Epoch 6891: train loss: 2.703395011849352e-06, val loss: 0.09736689180135727\n",
      "Epoch 6892: train loss: 5.567905645875726e-06, val loss: 0.09691186249256134\n",
      "Epoch 6893: train loss: 3.0970247735240264e-06, val loss: 0.09685331583023071\n",
      "Epoch 6894: train loss: 2.8812146410928108e-06, val loss: 0.09688109159469604\n",
      "Epoch 6895: train loss: 4.090272341272794e-06, val loss: 0.09654083102941513\n",
      "Epoch 6896: train loss: 1.8939066421808093e-06, val loss: 0.09660357981920242\n",
      "Epoch 6897: train loss: 2.7230539672018494e-06, val loss: 0.09682457894086838\n",
      "Epoch 6898: train loss: 2.9133752832422033e-06, val loss: 0.09637728333473206\n",
      "Epoch 6899: train loss: 1.130009309235902e-06, val loss: 0.09626368433237076\n",
      "Epoch 6900: train loss: 2.7194030280952575e-06, val loss: 0.09656783193349838\n",
      "Epoch 6901: train loss: 1.3256618558443733e-06, val loss: 0.09621065855026245\n",
      "Epoch 6902: train loss: 1.4913816812622827e-06, val loss: 0.09589661657810211\n",
      "Epoch 6903: train loss: 1.8669435348783736e-06, val loss: 0.09616626799106598\n",
      "Epoch 6904: train loss: 1.1844447271869285e-06, val loss: 0.09585218131542206\n",
      "Epoch 6905: train loss: 1.0327636346119107e-06, val loss: 0.09562476724386215\n",
      "Epoch 6906: train loss: 1.5430431403729017e-06, val loss: 0.09592951834201813\n",
      "Epoch 6907: train loss: 1.0083016377393506e-06, val loss: 0.0955420657992363\n",
      "Epoch 6908: train loss: 1.0246451438433724e-06, val loss: 0.09541983902454376\n",
      "Epoch 6909: train loss: 1.3650943628817913e-06, val loss: 0.09566900879144669\n",
      "Epoch 6910: train loss: 1.557023097120691e-06, val loss: 0.09552488476037979\n",
      "Epoch 6911: train loss: 1.0164928880840307e-06, val loss: 0.09522226452827454\n",
      "Epoch 6912: train loss: 1.0675441899365978e-06, val loss: 0.09529393166303635\n",
      "Epoch 6913: train loss: 2.4108508114295546e-06, val loss: 0.09541547298431396\n",
      "Epoch 6914: train loss: 3.864682184939738e-06, val loss: 0.0951775535941124\n",
      "Epoch 6915: train loss: 5.5158338909677695e-06, val loss: 0.09522555023431778\n",
      "Epoch 6916: train loss: 8.81662799656624e-06, val loss: 0.0952870175242424\n",
      "Epoch 6917: train loss: 1.446932310500415e-05, val loss: 0.09543100744485855\n",
      "Epoch 6918: train loss: 2.4989036319311708e-05, val loss: 0.09534096717834473\n",
      "Epoch 6919: train loss: 4.2714516894193366e-05, val loss: 0.09516198933124542\n",
      "Epoch 6920: train loss: 7.395025750156492e-05, val loss: 0.09514475613832474\n",
      "Epoch 6921: train loss: 0.00012195567978778854, val loss: 0.09628073126077652\n",
      "Epoch 6922: train loss: 0.00017993853543885052, val loss: 0.09387025982141495\n",
      "Epoch 6923: train loss: 0.0002319119084859267, val loss: 0.09632943570613861\n",
      "Epoch 6924: train loss: 0.00020235628471709788, val loss: 0.09416324645280838\n",
      "Epoch 6925: train loss: 0.00010537535126786679, val loss: 0.09564360231161118\n",
      "Epoch 6926: train loss: 2.676547592272982e-05, val loss: 0.09560488909482956\n",
      "Epoch 6927: train loss: 3.992894198745489e-05, val loss: 0.09273360669612885\n",
      "Epoch 6928: train loss: 8.08736149338074e-05, val loss: 0.09484338760375977\n",
      "Epoch 6929: train loss: 6.377640238497406e-05, val loss: 0.09207760542631149\n",
      "Epoch 6930: train loss: 3.127022864646278e-05, val loss: 0.09232859313488007\n",
      "Epoch 6931: train loss: 3.380293492227793e-05, val loss: 0.09365189075469971\n",
      "Epoch 6932: train loss: 3.6808069125982e-05, val loss: 0.0911702886223793\n",
      "Epoch 6933: train loss: 2.1367502995417453e-05, val loss: 0.09229087829589844\n",
      "Epoch 6934: train loss: 2.330341885681264e-05, val loss: 0.09310723841190338\n",
      "Epoch 6935: train loss: 3.01461732306052e-05, val loss: 0.09126295149326324\n",
      "Epoch 6936: train loss: 1.5054061805130914e-05, val loss: 0.09283193200826645\n",
      "Epoch 6937: train loss: 8.349368727067485e-06, val loss: 0.09254548698663712\n",
      "Epoch 6938: train loss: 2.2338470444083214e-05, val loss: 0.0907718762755394\n",
      "Epoch 6939: train loss: 1.782275467121508e-05, val loss: 0.09172027558088303\n",
      "Epoch 6940: train loss: 2.508622401364846e-06, val loss: 0.09164359420537949\n",
      "Epoch 6941: train loss: 1.2077368410245981e-05, val loss: 0.09089719504117966\n",
      "Epoch 6942: train loss: 1.7786531316232868e-05, val loss: 0.09226471930742264\n",
      "Epoch 6943: train loss: 4.267130407242803e-06, val loss: 0.092189721763134\n",
      "Epoch 6944: train loss: 4.216113211441552e-06, val loss: 0.0914173498749733\n",
      "Epoch 6945: train loss: 1.2402174434100743e-05, val loss: 0.09251739084720612\n",
      "Epoch 6946: train loss: 7.0323640102287754e-06, val loss: 0.09239169210195541\n",
      "Epoch 6947: train loss: 2.1782086605526274e-06, val loss: 0.09189394861459732\n",
      "Epoch 6948: train loss: 6.015602593834046e-06, val loss: 0.09281507879495621\n",
      "Epoch 6949: train loss: 6.94755863150931e-06, val loss: 0.09254484623670578\n",
      "Epoch 6950: train loss: 4.148137577431044e-06, val loss: 0.0922500416636467\n",
      "Epoch 6951: train loss: 2.7406731533119455e-06, val loss: 0.09284874051809311\n",
      "Epoch 6952: train loss: 3.5088846743747126e-06, val loss: 0.09245707839727402\n",
      "Epoch 6953: train loss: 4.501081093621906e-06, val loss: 0.09239207208156586\n",
      "Epoch 6954: train loss: 4.006903054687427e-06, val loss: 0.09280368685722351\n",
      "Epoch 6955: train loss: 2.2251035716180922e-06, val loss: 0.09243831038475037\n",
      "Epoch 6956: train loss: 1.7089236052925116e-06, val loss: 0.09261371940374374\n",
      "Epoch 6957: train loss: 3.082711145907524e-06, val loss: 0.0925561711192131\n",
      "Epoch 6958: train loss: 3.548411314113764e-06, val loss: 0.09237364679574966\n",
      "Epoch 6959: train loss: 2.499639776942786e-06, val loss: 0.09256913512945175\n",
      "Epoch 6960: train loss: 1.6989743016893044e-06, val loss: 0.09237704426050186\n",
      "Epoch 6961: train loss: 1.689373789304227e-06, val loss: 0.09253653138875961\n",
      "Epoch 6962: train loss: 1.881252273960854e-06, val loss: 0.09221407771110535\n",
      "Epoch 6963: train loss: 2.204166776209604e-06, val loss: 0.0922640785574913\n",
      "Epoch 6964: train loss: 2.531913196435198e-06, val loss: 0.09232646226882935\n",
      "Epoch 6965: train loss: 2.6396362500236137e-06, val loss: 0.09215420484542847\n",
      "Epoch 6966: train loss: 2.534503664719523e-06, val loss: 0.09221615642309189\n",
      "Epoch 6967: train loss: 2.7701860290108016e-06, val loss: 0.09199313074350357\n",
      "Epoch 6968: train loss: 4.229840214975411e-06, val loss: 0.09217424690723419\n",
      "Epoch 6969: train loss: 8.607435120211449e-06, val loss: 0.09174920618534088\n",
      "Epoch 6970: train loss: 1.8193950381828472e-05, val loss: 0.09231780469417572\n",
      "Epoch 6971: train loss: 4.259185152477585e-05, val loss: 0.09117811173200607\n",
      "Epoch 6972: train loss: 0.00010370279051130638, val loss: 0.0930432602763176\n",
      "Epoch 6973: train loss: 0.00025883674970827997, val loss: 0.09046653658151627\n",
      "Epoch 6974: train loss: 0.0005156996194273233, val loss: 0.09639465063810349\n",
      "Epoch 6975: train loss: 0.0005706030060537159, val loss: 0.09488071501255035\n",
      "Epoch 6976: train loss: 0.0005368936690501869, val loss: 0.09609706699848175\n",
      "Epoch 6977: train loss: 0.0010384820634499192, val loss: 0.09416735172271729\n",
      "Epoch 6978: train loss: 0.0017164014279842377, val loss: 0.09502381831407547\n",
      "Epoch 6979: train loss: 0.0016547892009839416, val loss: 0.08937379717826843\n",
      "Epoch 6980: train loss: 0.00039527550688944757, val loss: 0.08818884193897247\n",
      "Epoch 6981: train loss: 0.0005114644300192595, val loss: 0.09050184488296509\n",
      "Epoch 6982: train loss: 0.0005152418743818998, val loss: 0.08961499482393265\n",
      "Epoch 6983: train loss: 0.00024108680372592062, val loss: 0.08594085276126862\n",
      "Epoch 6984: train loss: 0.0003621320065576583, val loss: 0.08225754648447037\n",
      "Epoch 6985: train loss: 0.0001628529716981575, val loss: 0.08126896619796753\n",
      "Epoch 6986: train loss: 0.0002595849218778312, val loss: 0.08179271221160889\n",
      "Epoch 6987: train loss: 0.00011994098167633638, val loss: 0.08286111801862717\n",
      "Epoch 6988: train loss: 0.00019433173292782158, val loss: 0.0826459527015686\n",
      "Epoch 6989: train loss: 0.00010428536188555881, val loss: 0.08012492954730988\n",
      "Epoch 6990: train loss: 0.00012058560241712257, val loss: 0.07738538831472397\n",
      "Epoch 6991: train loss: 0.00010970746370730922, val loss: 0.07772912830114365\n",
      "Epoch 6992: train loss: 6.532922270707786e-05, val loss: 0.07989797741174698\n",
      "Epoch 6993: train loss: 0.0001046220277203247, val loss: 0.08002720028162003\n",
      "Epoch 6994: train loss: 4.51532396255061e-05, val loss: 0.0787525326013565\n",
      "Epoch 6995: train loss: 7.746794290142134e-05, val loss: 0.07801977545022964\n",
      "Epoch 6996: train loss: 4.859138425672427e-05, val loss: 0.07775702327489853\n",
      "Epoch 6997: train loss: 4.313791578169912e-05, val loss: 0.0773201659321785\n",
      "Epoch 6998: train loss: 5.463260458782315e-05, val loss: 0.07687003165483475\n",
      "Epoch 6999: train loss: 2.859386950149201e-05, val loss: 0.07681246846914291\n",
      "Epoch 7000: train loss: 4.3230025767115876e-05, val loss: 0.07666754722595215\n",
      "Epoch 7001: train loss: 2.786837467283476e-05, val loss: 0.07692154496908188\n",
      "Epoch 7002: train loss: 2.7608884920482524e-05, val loss: 0.07617677003145218\n",
      "Epoch 7003: train loss: 2.7137302822666243e-05, val loss: 0.07557286322116852\n",
      "Epoch 7004: train loss: 1.814134157029912e-05, val loss: 0.0763225108385086\n",
      "Epoch 7005: train loss: 1.6867683370946907e-05, val loss: 0.07767657190561295\n",
      "Epoch 7006: train loss: 1.924739990499802e-05, val loss: 0.07802077382802963\n",
      "Epoch 7007: train loss: 1.1019435987691395e-05, val loss: 0.07759038358926773\n",
      "Epoch 7008: train loss: 1.3736863365920726e-05, val loss: 0.07755991071462631\n",
      "Epoch 7009: train loss: 1.2158929166616872e-05, val loss: 0.07785970717668533\n",
      "Epoch 7010: train loss: 8.196211638278328e-06, val loss: 0.07803376764059067\n",
      "Epoch 7011: train loss: 1.1721591363311745e-05, val loss: 0.0780264362692833\n",
      "Epoch 7012: train loss: 6.91287050358369e-06, val loss: 0.07769317924976349\n",
      "Epoch 7013: train loss: 7.35546291252831e-06, val loss: 0.07739079743623734\n",
      "Epoch 7014: train loss: 7.631398148078006e-06, val loss: 0.07743807137012482\n",
      "Epoch 7015: train loss: 5.712910478905542e-06, val loss: 0.07755159586668015\n",
      "Epoch 7016: train loss: 5.881178822164657e-06, val loss: 0.07762011885643005\n",
      "Epoch 7017: train loss: 4.612104930856731e-06, val loss: 0.07791377604007721\n",
      "Epoch 7018: train loss: 4.610548785421997e-06, val loss: 0.07815530151128769\n",
      "Epoch 7019: train loss: 4.702314981841482e-06, val loss: 0.0778748020529747\n",
      "Epoch 7020: train loss: 3.196876605215948e-06, val loss: 0.07740955799818039\n",
      "Epoch 7021: train loss: 3.18892875839083e-06, val loss: 0.07737182825803757\n",
      "Epoch 7022: train loss: 3.944628588214982e-06, val loss: 0.07775483280420303\n",
      "Epoch 7023: train loss: 1.9774972770392196e-06, val loss: 0.07814911752939224\n",
      "Epoch 7024: train loss: 3.3008820992108667e-06, val loss: 0.07772558927536011\n",
      "Epoch 7025: train loss: 2.0425638922461076e-06, val loss: 0.07728463411331177\n",
      "Epoch 7026: train loss: 1.883090362753137e-06, val loss: 0.07736501842737198\n",
      "Epoch 7027: train loss: 1.9975166196672944e-06, val loss: 0.07771963626146317\n",
      "Epoch 7028: train loss: 1.0509269259273424e-06, val loss: 0.07774671167135239\n",
      "Epoch 7029: train loss: 1.4462814306170912e-06, val loss: 0.07737737149000168\n",
      "Epoch 7030: train loss: 1.2800001059076749e-06, val loss: 0.07709266990423203\n",
      "Epoch 7031: train loss: 7.161993380577769e-07, val loss: 0.07705586403608322\n",
      "Epoch 7032: train loss: 1.3598630630440312e-06, val loss: 0.07714855670928955\n",
      "Epoch 7033: train loss: 6.31299258202489e-07, val loss: 0.0773354172706604\n",
      "Epoch 7034: train loss: 8.133555411404814e-07, val loss: 0.07741189748048782\n",
      "Epoch 7035: train loss: 8.693083941579971e-07, val loss: 0.07725895196199417\n",
      "Epoch 7036: train loss: 4.865838150180934e-07, val loss: 0.07710779458284378\n",
      "Epoch 7037: train loss: 7.579544671898475e-07, val loss: 0.07708454877138138\n",
      "Epoch 7038: train loss: 4.817554213332187e-07, val loss: 0.07706954330205917\n",
      "Epoch 7039: train loss: 5.105936224936158e-07, val loss: 0.07708767801523209\n",
      "Epoch 7040: train loss: 4.993593165636412e-07, val loss: 0.07714781165122986\n",
      "Epoch 7041: train loss: 4.0194848338614975e-07, val loss: 0.07705523073673248\n",
      "Epoch 7042: train loss: 3.82807741061697e-07, val loss: 0.07689900696277618\n",
      "Epoch 7043: train loss: 3.7925056517451594e-07, val loss: 0.07689433544874191\n",
      "Epoch 7044: train loss: 3.882655619236175e-07, val loss: 0.0768871009349823\n",
      "Epoch 7045: train loss: 2.0014357460240717e-07, val loss: 0.07683967798948288\n",
      "Epoch 7046: train loss: 3.7764354487990204e-07, val loss: 0.07678062468767166\n",
      "Epoch 7047: train loss: 3.5007101928385964e-07, val loss: 0.07665242999792099\n",
      "Epoch 7048: train loss: 6.188527663653076e-07, val loss: 0.07693791389465332\n",
      "Epoch 7049: train loss: 9.60523266257951e-07, val loss: 0.07684978097677231\n",
      "Epoch 7050: train loss: 1.235767058460624e-06, val loss: 0.07682174444198608\n",
      "Epoch 7051: train loss: 9.467887025493837e-07, val loss: 0.07658940553665161\n",
      "Epoch 7052: train loss: 6.173680731080822e-07, val loss: 0.07666745781898499\n",
      "Epoch 7053: train loss: 1.7164354915166768e-07, val loss: 0.07669913023710251\n",
      "Epoch 7054: train loss: 1.7504478933005885e-07, val loss: 0.07656531780958176\n",
      "Epoch 7055: train loss: 4.4281989630690077e-07, val loss: 0.07652312517166138\n",
      "Epoch 7056: train loss: 5.928043833591801e-07, val loss: 0.07639351487159729\n",
      "Epoch 7057: train loss: 6.04382421443006e-07, val loss: 0.07644703984260559\n",
      "Epoch 7058: train loss: 4.1468123868071416e-07, val loss: 0.0763670951128006\n",
      "Epoch 7059: train loss: 2.081597187952866e-07, val loss: 0.07626932114362717\n",
      "Epoch 7060: train loss: 9.045299265153517e-08, val loss: 0.07622038573026657\n",
      "Epoch 7061: train loss: 1.0576304987353069e-07, val loss: 0.07616233080625534\n",
      "Epoch 7062: train loss: 2.1600376953756495e-07, val loss: 0.07613074034452438\n",
      "Epoch 7063: train loss: 3.4785833236128383e-07, val loss: 0.07603176683187485\n",
      "Epoch 7064: train loss: 5.924819674874016e-07, val loss: 0.07603438198566437\n",
      "Epoch 7065: train loss: 9.313113764619629e-07, val loss: 0.07592573016881943\n",
      "Epoch 7066: train loss: 1.6714309367671376e-06, val loss: 0.07592811435461044\n",
      "Epoch 7067: train loss: 3.2021432616602397e-06, val loss: 0.07577856630086899\n",
      "Epoch 7068: train loss: 6.551103524543578e-06, val loss: 0.07588395476341248\n",
      "Epoch 7069: train loss: 8.308349606522825e-06, val loss: 0.07574295997619629\n",
      "Epoch 7070: train loss: 7.086442565196194e-06, val loss: 0.07572139799594879\n",
      "Epoch 7071: train loss: 6.314942766039167e-06, val loss: 0.07592566311359406\n",
      "Epoch 7072: train loss: 6.496061359939631e-06, val loss: 0.07549472898244858\n",
      "Epoch 7073: train loss: 2.6204954792774515e-06, val loss: 0.07560694217681885\n",
      "Epoch 7074: train loss: 2.552551677581505e-06, val loss: 0.07592692226171494\n",
      "Epoch 7075: train loss: 6.06034745942452e-06, val loss: 0.07579823583364487\n",
      "Epoch 7076: train loss: 4.476511549000861e-06, val loss: 0.07583337277173996\n",
      "Epoch 7077: train loss: 2.5082636057049967e-06, val loss: 0.07604455947875977\n",
      "Epoch 7078: train loss: 3.2991817988659022e-06, val loss: 0.07562317699193954\n",
      "Epoch 7079: train loss: 3.1289234811993083e-06, val loss: 0.07583743333816528\n",
      "Epoch 7080: train loss: 1.651168076932663e-06, val loss: 0.07611993700265884\n",
      "Epoch 7081: train loss: 1.8087896478391485e-06, val loss: 0.07566478103399277\n",
      "Epoch 7082: train loss: 2.4711880541872233e-06, val loss: 0.07586678862571716\n",
      "Epoch 7083: train loss: 3.173810910084285e-06, val loss: 0.07590639591217041\n",
      "Epoch 7084: train loss: 4.215300123178167e-06, val loss: 0.0757041946053505\n",
      "Epoch 7085: train loss: 6.037592356733512e-06, val loss: 0.0756574273109436\n",
      "Epoch 7086: train loss: 1.0154574738407973e-05, val loss: 0.07670848816633224\n",
      "Epoch 7087: train loss: 1.9889788745786063e-05, val loss: 0.07549068331718445\n",
      "Epoch 7088: train loss: 3.9632323023397475e-05, val loss: 0.0780077800154686\n",
      "Epoch 7089: train loss: 8.463128324365243e-05, val loss: 0.07714153826236725\n",
      "Epoch 7090: train loss: 0.00015087697829585522, val loss: 0.07882817089557648\n",
      "Epoch 7091: train loss: 0.00026250857627019286, val loss: 0.07670072466135025\n",
      "Epoch 7092: train loss: 0.00020087999291718006, val loss: 0.07970515638589859\n",
      "Epoch 7093: train loss: 6.185344682307914e-05, val loss: 0.0799747109413147\n",
      "Epoch 7094: train loss: 6.38166384305805e-05, val loss: 0.07908815145492554\n",
      "Epoch 7095: train loss: 0.00011008237197529525, val loss: 0.08026208728551865\n",
      "Epoch 7096: train loss: 5.291993511491455e-05, val loss: 0.08083628863096237\n",
      "Epoch 7097: train loss: 3.966859367210418e-05, val loss: 0.08055367320775986\n",
      "Epoch 7098: train loss: 4.948984496877529e-05, val loss: 0.08186967670917511\n",
      "Epoch 7099: train loss: 2.0528390450635925e-05, val loss: 0.08163608610630035\n",
      "Epoch 7100: train loss: 3.934215419576503e-05, val loss: 0.08017374575138092\n",
      "Epoch 7101: train loss: 2.6974979846272618e-05, val loss: 0.08132904767990112\n",
      "Epoch 7102: train loss: 2.1588330127997324e-05, val loss: 0.08190151304006577\n",
      "Epoch 7103: train loss: 3.3815471397247165e-05, val loss: 0.08111350238323212\n",
      "Epoch 7104: train loss: 8.155357136274688e-06, val loss: 0.08091162145137787\n",
      "Epoch 7105: train loss: 1.829222856031265e-05, val loss: 0.08130090683698654\n",
      "Epoch 7106: train loss: 1.9585608242778108e-05, val loss: 0.08099420368671417\n",
      "Epoch 7107: train loss: 1.214682197314687e-05, val loss: 0.08029936999082565\n",
      "Epoch 7108: train loss: 1.5557001461274922e-05, val loss: 0.08102795481681824\n",
      "Epoch 7109: train loss: 1.1473643098725006e-05, val loss: 0.08094817399978638\n",
      "Epoch 7110: train loss: 9.344342288386542e-06, val loss: 0.08026865869760513\n",
      "Epoch 7111: train loss: 9.541906365484465e-06, val loss: 0.08049947023391724\n",
      "Epoch 7112: train loss: 6.301090706983814e-06, val loss: 0.08053909242153168\n",
      "Epoch 7113: train loss: 1.2131299627071712e-05, val loss: 0.08015956729650497\n",
      "Epoch 7114: train loss: 6.16234547123895e-06, val loss: 0.07983874529600143\n",
      "Epoch 7115: train loss: 5.267595952318516e-06, val loss: 0.08049093931913376\n",
      "Epoch 7116: train loss: 6.785398909414653e-06, val loss: 0.08010494709014893\n",
      "Epoch 7117: train loss: 4.513296516961418e-06, val loss: 0.07984539121389389\n",
      "Epoch 7118: train loss: 5.882698587811319e-06, val loss: 0.08085811138153076\n",
      "Epoch 7119: train loss: 4.489149432629347e-06, val loss: 0.07995609939098358\n",
      "Epoch 7120: train loss: 5.188104751141509e-06, val loss: 0.07940925657749176\n",
      "Epoch 7121: train loss: 5.769581093773013e-06, val loss: 0.08040188997983932\n",
      "Epoch 7122: train loss: 5.972799499431858e-06, val loss: 0.08023197948932648\n",
      "Epoch 7123: train loss: 4.610905307345092e-06, val loss: 0.07971372455358505\n",
      "Epoch 7124: train loss: 5.422887625172734e-06, val loss: 0.08041606098413467\n",
      "Epoch 7125: train loss: 8.02783870312851e-06, val loss: 0.07993513345718384\n",
      "Epoch 7126: train loss: 1.002072895062156e-05, val loss: 0.07993755489587784\n",
      "Epoch 7127: train loss: 1.5922349120955914e-05, val loss: 0.07991518825292587\n",
      "Epoch 7128: train loss: 2.865700844267849e-05, val loss: 0.0800260528922081\n",
      "Epoch 7129: train loss: 5.616752605419606e-05, val loss: 0.07947337627410889\n",
      "Epoch 7130: train loss: 0.00011418572830734774, val loss: 0.08042978495359421\n",
      "Epoch 7131: train loss: 0.000236718071391806, val loss: 0.07839428633451462\n",
      "Epoch 7132: train loss: 0.0004479996277950704, val loss: 0.08066492527723312\n",
      "Epoch 7133: train loss: 0.0007083776290528476, val loss: 0.08001947402954102\n",
      "Epoch 7134: train loss: 0.0007043113582767546, val loss: 0.08221416920423508\n",
      "Epoch 7135: train loss: 0.0003119022585451603, val loss: 0.08171377331018448\n",
      "Epoch 7136: train loss: 9.769147436600178e-05, val loss: 0.08391225337982178\n",
      "Epoch 7137: train loss: 0.0002578312123659998, val loss: 0.08513417094945908\n",
      "Epoch 7138: train loss: 0.0001913986779982224, val loss: 0.08376604318618774\n",
      "Epoch 7139: train loss: 0.00011529551557032391, val loss: 0.0855604037642479\n",
      "Epoch 7140: train loss: 0.0001351135433651507, val loss: 0.08579718321561813\n",
      "Epoch 7141: train loss: 0.00011988741607638076, val loss: 0.08522691577672958\n",
      "Epoch 7142: train loss: 8.46610710141249e-05, val loss: 0.08652450144290924\n",
      "Epoch 7143: train loss: 7.746981282252818e-05, val loss: 0.0871974304318428\n",
      "Epoch 7144: train loss: 8.448297012364492e-05, val loss: 0.08542740345001221\n",
      "Epoch 7145: train loss: 4.3316551455063745e-05, val loss: 0.08593004196882248\n",
      "Epoch 7146: train loss: 7.400091999443248e-05, val loss: 0.08890459686517715\n",
      "Epoch 7147: train loss: 4.796248322236352e-05, val loss: 0.08736981451511383\n",
      "Epoch 7148: train loss: 4.4250860810279846e-05, val loss: 0.08464989811182022\n",
      "Epoch 7149: train loss: 4.2557541746646166e-05, val loss: 0.0875248834490776\n",
      "Epoch 7150: train loss: 3.8318052247632295e-05, val loss: 0.08934428542852402\n",
      "Epoch 7151: train loss: 3.0580566090065986e-05, val loss: 0.08654308319091797\n",
      "Epoch 7152: train loss: 2.9309141609701328e-05, val loss: 0.08676175773143768\n",
      "Epoch 7153: train loss: 2.6954961867886595e-05, val loss: 0.08922082930803299\n",
      "Epoch 7154: train loss: 2.3600436179549433e-05, val loss: 0.0884045660495758\n",
      "Epoch 7155: train loss: 1.593846536707133e-05, val loss: 0.08662351220846176\n",
      "Epoch 7156: train loss: 2.5485485821263865e-05, val loss: 0.0879800096154213\n",
      "Epoch 7157: train loss: 8.439194061793387e-06, val loss: 0.08958519995212555\n",
      "Epoch 7158: train loss: 1.9428949599387124e-05, val loss: 0.08811654895544052\n",
      "Epoch 7159: train loss: 1.0523000128159765e-05, val loss: 0.0870218351483345\n",
      "Epoch 7160: train loss: 1.314644487138139e-05, val loss: 0.08847435563802719\n",
      "Epoch 7161: train loss: 8.410304872086272e-06, val loss: 0.08895396441221237\n",
      "Epoch 7162: train loss: 1.1413272659410723e-05, val loss: 0.08782875537872314\n",
      "Epoch 7163: train loss: 7.378985174000263e-06, val loss: 0.08790230005979538\n",
      "Epoch 7164: train loss: 8.129166417347733e-06, val loss: 0.08825128525495529\n",
      "Epoch 7165: train loss: 7.823342457413673e-06, val loss: 0.08803875744342804\n",
      "Epoch 7166: train loss: 4.504959179030266e-06, val loss: 0.0878315344452858\n",
      "Epoch 7167: train loss: 8.27316671347944e-06, val loss: 0.08737283200025558\n",
      "Epoch 7168: train loss: 3.8377856981242076e-06, val loss: 0.08769819885492325\n",
      "Epoch 7169: train loss: 4.961469585396117e-06, val loss: 0.08800695836544037\n",
      "Epoch 7170: train loss: 4.859574346482987e-06, val loss: 0.08751436322927475\n",
      "Epoch 7171: train loss: 3.7559063912340207e-06, val loss: 0.08753499388694763\n",
      "Epoch 7172: train loss: 4.365929271443747e-06, val loss: 0.08726435154676437\n",
      "Epoch 7173: train loss: 2.627172762004193e-06, val loss: 0.08723783493041992\n",
      "Epoch 7174: train loss: 3.5102625588478986e-06, val loss: 0.08774580806493759\n",
      "Epoch 7175: train loss: 2.289710892000585e-06, val loss: 0.08742059767246246\n",
      "Epoch 7176: train loss: 3.347032361489255e-06, val loss: 0.08721832185983658\n",
      "Epoch 7177: train loss: 2.764849114100798e-06, val loss: 0.08727055788040161\n",
      "Epoch 7178: train loss: 2.1978137283440446e-06, val loss: 0.0878080278635025\n",
      "Epoch 7179: train loss: 2.393845988990506e-06, val loss: 0.0871141105890274\n",
      "Epoch 7180: train loss: 2.4398607365583302e-06, val loss: 0.08735930919647217\n",
      "Epoch 7181: train loss: 1.512839503448049e-06, val loss: 0.08723235130310059\n",
      "Epoch 7182: train loss: 2.3354161839961307e-06, val loss: 0.08719975501298904\n",
      "Epoch 7183: train loss: 3.3602836992940865e-06, val loss: 0.08685934543609619\n",
      "Epoch 7184: train loss: 4.350263679953059e-06, val loss: 0.08787272125482559\n",
      "Epoch 7185: train loss: 1.0281987670168746e-05, val loss: 0.08607923239469528\n",
      "Epoch 7186: train loss: 2.3984239305718802e-05, val loss: 0.08848767727613449\n",
      "Epoch 7187: train loss: 6.331096665235236e-05, val loss: 0.08523658663034439\n",
      "Epoch 7188: train loss: 0.00013108510756865144, val loss: 0.09044308215379715\n",
      "Epoch 7189: train loss: 0.0003003849124070257, val loss: 0.08340030163526535\n",
      "Epoch 7190: train loss: 0.00034026516368612647, val loss: 0.08536384254693985\n",
      "Epoch 7191: train loss: 0.0002528043696656823, val loss: 0.0863841325044632\n",
      "Epoch 7192: train loss: 2.3225291442940943e-05, val loss: 0.08779417723417282\n",
      "Epoch 7193: train loss: 0.00016961693472694606, val loss: 0.08660780638456345\n",
      "Epoch 7194: train loss: 0.0001284068712266162, val loss: 0.08963616192340851\n",
      "Epoch 7195: train loss: 7.258391269715503e-05, val loss: 0.09123235195875168\n",
      "Epoch 7196: train loss: 8.291396807180718e-05, val loss: 0.089032843708992\n",
      "Epoch 7197: train loss: 6.832913641119376e-05, val loss: 0.08940872550010681\n",
      "Epoch 7198: train loss: 7.144284609239548e-05, val loss: 0.09051523357629776\n",
      "Epoch 7199: train loss: 4.312213059165515e-05, val loss: 0.09135087579488754\n",
      "Epoch 7200: train loss: 5.2085302741033956e-05, val loss: 0.09055545181035995\n",
      "Epoch 7201: train loss: 4.55115114164073e-05, val loss: 0.0895317792892456\n",
      "Epoch 7202: train loss: 4.145263665122911e-05, val loss: 0.0911104679107666\n",
      "Epoch 7203: train loss: 3.287724030087702e-05, val loss: 0.09127561002969742\n",
      "Epoch 7204: train loss: 3.2645068131387234e-05, val loss: 0.08944318443536758\n",
      "Epoch 7205: train loss: 2.6987017918145284e-05, val loss: 0.08967138826847076\n",
      "Epoch 7206: train loss: 2.9462546081049368e-05, val loss: 0.0905209332704544\n",
      "Epoch 7207: train loss: 2.0087260054424405e-05, val loss: 0.09074978530406952\n",
      "Epoch 7208: train loss: 2.047735506494064e-05, val loss: 0.09038236737251282\n",
      "Epoch 7209: train loss: 2.048274654953275e-05, val loss: 0.08950173109769821\n",
      "Epoch 7210: train loss: 1.749156763253268e-05, val loss: 0.09004805237054825\n",
      "Epoch 7211: train loss: 1.4276637557486538e-05, val loss: 0.09050469100475311\n",
      "Epoch 7212: train loss: 1.319959028478479e-05, val loss: 0.08934541046619415\n",
      "Epoch 7213: train loss: 1.5189420992101077e-05, val loss: 0.08885570615530014\n",
      "Epoch 7214: train loss: 1.0532193300605286e-05, val loss: 0.08959157764911652\n",
      "Epoch 7215: train loss: 1.0283340998284984e-05, val loss: 0.08975320309400558\n",
      "Epoch 7216: train loss: 8.117464858514722e-06, val loss: 0.08890137821435928\n",
      "Epoch 7217: train loss: 9.367113307234831e-06, val loss: 0.08848588913679123\n",
      "Epoch 7218: train loss: 8.48421586852055e-06, val loss: 0.08928308635950089\n",
      "Epoch 7219: train loss: 5.483107543113874e-06, val loss: 0.08871100842952728\n",
      "Epoch 7220: train loss: 6.202294116519624e-06, val loss: 0.08786119520664215\n",
      "Epoch 7221: train loss: 6.617642611672636e-06, val loss: 0.08866434544324875\n",
      "Epoch 7222: train loss: 4.25366806666716e-06, val loss: 0.08855680376291275\n",
      "Epoch 7223: train loss: 6.5261324380117e-06, val loss: 0.08792968094348907\n",
      "Epoch 7224: train loss: 3.4805361792678013e-06, val loss: 0.08799900114536285\n",
      "Epoch 7225: train loss: 3.6435312722460367e-06, val loss: 0.08825615793466568\n",
      "Epoch 7226: train loss: 3.2907482818700373e-06, val loss: 0.08795376121997833\n",
      "Epoch 7227: train loss: 3.716278570209397e-06, val loss: 0.08764798194169998\n",
      "Epoch 7228: train loss: 3.1984250199457165e-06, val loss: 0.08800049126148224\n",
      "Epoch 7229: train loss: 2.866017894120887e-06, val loss: 0.08778844028711319\n",
      "Epoch 7230: train loss: 2.884327159335953e-06, val loss: 0.08783623576164246\n",
      "Epoch 7231: train loss: 2.559883341746172e-06, val loss: 0.08799086511135101\n",
      "Epoch 7232: train loss: 2.2818624074716354e-06, val loss: 0.08778152614831924\n",
      "Epoch 7233: train loss: 2.2578981315746205e-06, val loss: 0.08735589683055878\n",
      "Epoch 7234: train loss: 2.050189095825772e-06, val loss: 0.08786343783140182\n",
      "Epoch 7235: train loss: 2.392613623669604e-06, val loss: 0.08757951110601425\n",
      "Epoch 7236: train loss: 2.123662625308498e-06, val loss: 0.08772601187229156\n",
      "Epoch 7237: train loss: 2.440490561639308e-06, val loss: 0.08734585344791412\n",
      "Epoch 7238: train loss: 4.057447767991107e-06, val loss: 0.08754359185695648\n",
      "Epoch 7239: train loss: 8.720086043467745e-06, val loss: 0.08718407154083252\n",
      "Epoch 7240: train loss: 2.3108499590307474e-05, val loss: 0.08812589943408966\n",
      "Epoch 7241: train loss: 7.032066059764475e-05, val loss: 0.08601927757263184\n",
      "Epoch 7242: train loss: 0.00024205243971664459, val loss: 0.09030269831418991\n",
      "Epoch 7243: train loss: 0.0008559553534723818, val loss: 0.08528013527393341\n",
      "Epoch 7244: train loss: 0.002801794558763504, val loss: 0.10410647839307785\n",
      "Epoch 7245: train loss: 0.005561411380767822, val loss: 0.08635704219341278\n",
      "Epoch 7246: train loss: 0.002827631775289774, val loss: 0.08683054149150848\n",
      "Epoch 7247: train loss: 0.0011764738010242581, val loss: 0.0968819111585617\n",
      "Epoch 7248: train loss: 0.0011637709103524685, val loss: 0.09227463603019714\n",
      "Epoch 7249: train loss: 0.0010925345122814178, val loss: 0.07927985489368439\n",
      "Epoch 7250: train loss: 0.00041632578358985484, val loss: 0.07381915301084518\n",
      "Epoch 7251: train loss: 0.0007921665091998875, val loss: 0.07959182560443878\n",
      "Epoch 7252: train loss: 0.0004354662378318608, val loss: 0.08380679786205292\n",
      "Epoch 7253: train loss: 0.0003902945027220994, val loss: 0.08391337841749191\n",
      "Epoch 7254: train loss: 0.0003878680581692606, val loss: 0.08172442018985748\n",
      "Epoch 7255: train loss: 0.0002670653339009732, val loss: 0.08135982602834702\n",
      "Epoch 7256: train loss: 0.00021965392807032913, val loss: 0.08179833739995956\n",
      "Epoch 7257: train loss: 0.00027808951563201845, val loss: 0.08082660287618637\n",
      "Epoch 7258: train loss: 0.00019357186101842672, val loss: 0.08014653623104095\n",
      "Epoch 7259: train loss: 0.0001341439492534846, val loss: 0.07975875586271286\n",
      "Epoch 7260: train loss: 0.00013475061859935522, val loss: 0.07936739921569824\n",
      "Epoch 7261: train loss: 0.00016566115664318204, val loss: 0.07895176857709885\n",
      "Epoch 7262: train loss: 0.00010840118920896202, val loss: 0.07900377362966537\n",
      "Epoch 7263: train loss: 8.42586305225268e-05, val loss: 0.07931891828775406\n",
      "Epoch 7264: train loss: 8.30279168440029e-05, val loss: 0.07799790799617767\n",
      "Epoch 7265: train loss: 0.0001041504874592647, val loss: 0.07943673431873322\n",
      "Epoch 7266: train loss: 6.544965435750782e-05, val loss: 0.0815829336643219\n",
      "Epoch 7267: train loss: 7.133930921554565e-05, val loss: 0.08220144361257553\n",
      "Epoch 7268: train loss: 6.605684029636905e-05, val loss: 0.08114536851644516\n",
      "Epoch 7269: train loss: 3.1725456210551783e-05, val loss: 0.08004436641931534\n",
      "Epoch 7270: train loss: 4.131613968638703e-05, val loss: 0.07999022305011749\n",
      "Epoch 7271: train loss: 4.322614404372871e-05, val loss: 0.08061527460813522\n",
      "Epoch 7272: train loss: 3.470126466709189e-05, val loss: 0.08059025555849075\n",
      "Epoch 7273: train loss: 2.440224852762185e-05, val loss: 0.07973456382751465\n",
      "Epoch 7274: train loss: 2.135000795533415e-05, val loss: 0.07918312400579453\n",
      "Epoch 7275: train loss: 2.4245415261248127e-05, val loss: 0.07948022335767746\n",
      "Epoch 7276: train loss: 2.024562309088651e-05, val loss: 0.07985415309667587\n",
      "Epoch 7277: train loss: 1.80553543032147e-05, val loss: 0.07958509027957916\n",
      "Epoch 7278: train loss: 1.653942854318302e-05, val loss: 0.07882589846849442\n",
      "Epoch 7279: train loss: 1.449033970857272e-05, val loss: 0.07848599553108215\n",
      "Epoch 7280: train loss: 1.2755981515510939e-05, val loss: 0.07913242280483246\n",
      "Epoch 7281: train loss: 1.0723591003625188e-05, val loss: 0.08006536960601807\n",
      "Epoch 7282: train loss: 1.093007085728459e-05, val loss: 0.08035323768854141\n",
      "Epoch 7283: train loss: 9.905797924147919e-06, val loss: 0.07954062521457672\n",
      "Epoch 7284: train loss: 9.735059393278789e-06, val loss: 0.07934582233428955\n",
      "Epoch 7285: train loss: 7.981538146850653e-06, val loss: 0.07927971333265305\n",
      "Epoch 7286: train loss: 7.478830866602948e-06, val loss: 0.07932545989751816\n",
      "Epoch 7287: train loss: 5.52631445316365e-06, val loss: 0.07919567078351974\n",
      "Epoch 7288: train loss: 1.0861958799068816e-05, val loss: 0.0792279839515686\n",
      "Epoch 7289: train loss: 1.3274187949718907e-05, val loss: 0.07894569635391235\n",
      "Epoch 7290: train loss: 1.8911727238446474e-05, val loss: 0.08019087463617325\n",
      "Epoch 7291: train loss: 9.671665793575812e-06, val loss: 0.08067535609006882\n",
      "Epoch 7292: train loss: 1.0242934877169318e-05, val loss: 0.08034156262874603\n",
      "Epoch 7293: train loss: 6.803722953918623e-06, val loss: 0.08108356595039368\n",
      "Epoch 7294: train loss: 7.740834917058237e-06, val loss: 0.08147677034139633\n",
      "Epoch 7295: train loss: 7.966036719153635e-06, val loss: 0.08189646899700165\n",
      "Epoch 7296: train loss: 5.1264332796563394e-06, val loss: 0.08183262497186661\n",
      "Epoch 7297: train loss: 4.917984824714949e-06, val loss: 0.08169372379779816\n",
      "Epoch 7298: train loss: 4.858003649133025e-06, val loss: 0.08176996558904648\n",
      "Epoch 7299: train loss: 3.9576489143655635e-06, val loss: 0.08154278993606567\n",
      "Epoch 7300: train loss: 3.888170340360375e-06, val loss: 0.08109543472528458\n",
      "Epoch 7301: train loss: 4.053243173984811e-06, val loss: 0.08122440427541733\n",
      "Epoch 7302: train loss: 3.2538978302909527e-06, val loss: 0.08156315237283707\n",
      "Epoch 7303: train loss: 2.7819369279313833e-06, val loss: 0.08144423365592957\n",
      "Epoch 7304: train loss: 2.214961568824947e-06, val loss: 0.08151429891586304\n",
      "Epoch 7305: train loss: 3.1491003937844653e-06, val loss: 0.08164269477128983\n",
      "Epoch 7306: train loss: 2.3856762254581554e-06, val loss: 0.08124785870313644\n",
      "Epoch 7307: train loss: 1.8467279687683913e-06, val loss: 0.08084862679243088\n",
      "Epoch 7308: train loss: 2.906988356699003e-06, val loss: 0.08200231194496155\n",
      "Epoch 7309: train loss: 3.8315415622491855e-06, val loss: 0.0801197737455368\n",
      "Epoch 7310: train loss: 9.533452612231486e-06, val loss: 0.08127044886350632\n",
      "Epoch 7311: train loss: 8.632871868030634e-06, val loss: 0.0797952190041542\n",
      "Epoch 7312: train loss: 2.031196800089674e-06, val loss: 0.07938132435083389\n",
      "Epoch 7313: train loss: 2.937346152975806e-06, val loss: 0.079784095287323\n",
      "Epoch 7314: train loss: 4.711392648459878e-06, val loss: 0.07930605858564377\n",
      "Epoch 7315: train loss: 1.953896799022914e-06, val loss: 0.0793265849351883\n",
      "Epoch 7316: train loss: 2.3793393211235525e-06, val loss: 0.07988268882036209\n",
      "Epoch 7317: train loss: 2.984773118441808e-06, val loss: 0.07931734621524811\n",
      "Epoch 7318: train loss: 2.0557606603688328e-06, val loss: 0.079283706843853\n",
      "Epoch 7319: train loss: 1.3123741382514709e-06, val loss: 0.0796552449464798\n",
      "Epoch 7320: train loss: 2.7153405426361132e-06, val loss: 0.07926494628190994\n",
      "Epoch 7321: train loss: 1.4585524468202493e-06, val loss: 0.07933979481458664\n",
      "Epoch 7322: train loss: 1.0359370890000719e-06, val loss: 0.07966035604476929\n",
      "Epoch 7323: train loss: 1.513646111561684e-06, val loss: 0.0791974887251854\n",
      "Epoch 7324: train loss: 2.000848780880915e-06, val loss: 0.0791187658905983\n",
      "Epoch 7325: train loss: 6.408710646610416e-07, val loss: 0.07878771424293518\n",
      "Epoch 7326: train loss: 9.300325132244325e-07, val loss: 0.07845578342676163\n",
      "Epoch 7327: train loss: 1.2955815691384487e-06, val loss: 0.07884543389081955\n",
      "Epoch 7328: train loss: 1.4585065173378098e-06, val loss: 0.07845818251371384\n",
      "Epoch 7329: train loss: 9.145222747974913e-07, val loss: 0.07865589112043381\n",
      "Epoch 7330: train loss: 7.894366262917174e-07, val loss: 0.07848017662763596\n",
      "Epoch 7331: train loss: 9.956413578038337e-07, val loss: 0.07841428369283676\n",
      "Epoch 7332: train loss: 1.6932855260165525e-06, val loss: 0.07704855501651764\n",
      "Epoch 7333: train loss: 6.573236078111222e-06, val loss: 0.08193477243185043\n",
      "Epoch 7334: train loss: 2.988265987369232e-05, val loss: 0.07604330033063889\n",
      "Epoch 7335: train loss: 8.004065603017807e-05, val loss: 0.08358870446681976\n",
      "Epoch 7336: train loss: 0.00010302534064976498, val loss: 0.0783785954117775\n",
      "Epoch 7337: train loss: 2.0741959815495647e-05, val loss: 0.07808326929807663\n",
      "Epoch 7338: train loss: 2.0846224288106896e-05, val loss: 0.08310125023126602\n",
      "Epoch 7339: train loss: 6.610872515011579e-05, val loss: 0.07740678638219833\n",
      "Epoch 7340: train loss: 1.2355401850072667e-05, val loss: 0.07656196504831314\n",
      "Epoch 7341: train loss: 1.973940379684791e-05, val loss: 0.08111413568258286\n",
      "Epoch 7342: train loss: 4.564832124742679e-05, val loss: 0.07638229429721832\n",
      "Epoch 7343: train loss: 4.564364189718617e-06, val loss: 0.07531633228063583\n",
      "Epoch 7344: train loss: 2.3567576135974377e-05, val loss: 0.07891563326120377\n",
      "Epoch 7345: train loss: 3.167967952322215e-05, val loss: 0.07628438621759415\n",
      "Epoch 7346: train loss: 1.436498337170633e-06, val loss: 0.07433702796697617\n",
      "Epoch 7347: train loss: 2.4825258151395246e-05, val loss: 0.07847141474485397\n",
      "Epoch 7348: train loss: 1.630638143979013e-05, val loss: 0.07739722728729248\n",
      "Epoch 7349: train loss: 4.198831902613165e-06, val loss: 0.0745825320482254\n",
      "Epoch 7350: train loss: 2.349604073970113e-05, val loss: 0.07779055088758469\n",
      "Epoch 7351: train loss: 6.355129244184354e-06, val loss: 0.0779670774936676\n",
      "Epoch 7352: train loss: 7.477920462406473e-06, val loss: 0.0762062594294548\n",
      "Epoch 7353: train loss: 1.7219916117028333e-05, val loss: 0.07734008133411407\n",
      "Epoch 7354: train loss: 2.4156604467862053e-06, val loss: 0.07765640318393707\n",
      "Epoch 7355: train loss: 8.884109774953686e-06, val loss: 0.07672976702451706\n",
      "Epoch 7356: train loss: 1.065842207026435e-05, val loss: 0.07735513150691986\n",
      "Epoch 7357: train loss: 1.5826954040676355e-06, val loss: 0.0775560736656189\n",
      "Epoch 7358: train loss: 6.729259894200368e-06, val loss: 0.07673981040716171\n",
      "Epoch 7359: train loss: 5.800881808681879e-06, val loss: 0.07722082734107971\n",
      "Epoch 7360: train loss: 2.138051058864221e-06, val loss: 0.07730931043624878\n",
      "Epoch 7361: train loss: 4.651193648896879e-06, val loss: 0.07652287930250168\n",
      "Epoch 7362: train loss: 4.639804046746576e-06, val loss: 0.07695939391851425\n",
      "Epoch 7363: train loss: 1.2063719623256475e-06, val loss: 0.0773395299911499\n",
      "Epoch 7364: train loss: 3.791152039411827e-06, val loss: 0.07647433131933212\n",
      "Epoch 7365: train loss: 3.33041407429846e-06, val loss: 0.07658039778470993\n",
      "Epoch 7366: train loss: 1.3105308198646526e-06, val loss: 0.07677175849676132\n",
      "Epoch 7367: train loss: 2.0658212633861694e-06, val loss: 0.07628681510686874\n",
      "Epoch 7368: train loss: 3.3975056794588454e-06, val loss: 0.07654397934675217\n",
      "Epoch 7369: train loss: 1.6709798273950582e-06, val loss: 0.0762980729341507\n",
      "Epoch 7370: train loss: 1.023469963001844e-06, val loss: 0.07601822912693024\n",
      "Epoch 7371: train loss: 2.190550958403037e-06, val loss: 0.0764133557677269\n",
      "Epoch 7372: train loss: 2.565196837167605e-06, val loss: 0.07588324695825577\n",
      "Epoch 7373: train loss: 1.0989017482643249e-06, val loss: 0.07600618898868561\n",
      "Epoch 7374: train loss: 1.0324234835934476e-06, val loss: 0.07620437443256378\n",
      "Epoch 7375: train loss: 2.0238433080521645e-06, val loss: 0.07540883868932724\n",
      "Epoch 7376: train loss: 6.375062184815761e-06, val loss: 0.07624580711126328\n",
      "Epoch 7377: train loss: 1.2663299457926769e-05, val loss: 0.07494865357875824\n",
      "Epoch 7378: train loss: 1.24106163639226e-05, val loss: 0.07609777897596359\n",
      "Epoch 7379: train loss: 9.165092706098221e-06, val loss: 0.07504470646381378\n",
      "Epoch 7380: train loss: 6.52271592116449e-06, val loss: 0.07538200169801712\n",
      "Epoch 7381: train loss: 4.444838396011619e-06, val loss: 0.07564320415258408\n",
      "Epoch 7382: train loss: 6.543181370943785e-06, val loss: 0.07475288957357407\n",
      "Epoch 7383: train loss: 1.0537062735238578e-05, val loss: 0.07547896355390549\n",
      "Epoch 7384: train loss: 1.0063061381515581e-05, val loss: 0.07471876591444016\n",
      "Epoch 7385: train loss: 7.219255167001393e-06, val loss: 0.07518260180950165\n",
      "Epoch 7386: train loss: 6.704806310153799e-06, val loss: 0.07444250583648682\n",
      "Epoch 7387: train loss: 4.546167019725544e-06, val loss: 0.07489608973264694\n",
      "Epoch 7388: train loss: 4.758682280225912e-06, val loss: 0.07420849055051804\n",
      "Epoch 7389: train loss: 6.367331479850691e-06, val loss: 0.07492955774068832\n",
      "Epoch 7390: train loss: 1.1011237802449614e-05, val loss: 0.07299774140119553\n",
      "Epoch 7391: train loss: 2.6738240194390528e-05, val loss: 0.07507912069559097\n",
      "Epoch 7392: train loss: 7.102423114702106e-05, val loss: 0.07151876389980316\n",
      "Epoch 7393: train loss: 0.00017927761655300856, val loss: 0.07729436457157135\n",
      "Epoch 7394: train loss: 0.00037241040263324976, val loss: 0.0709204375743866\n",
      "Epoch 7395: train loss: 0.0003247319837100804, val loss: 0.07887343317270279\n",
      "Epoch 7396: train loss: 0.0004253765509929508, val loss: 0.07471021264791489\n",
      "Epoch 7397: train loss: 0.0003213072195649147, val loss: 0.07596790790557861\n",
      "Epoch 7398: train loss: 7.355058187386021e-05, val loss: 0.07936858385801315\n",
      "Epoch 7399: train loss: 0.0001710201904643327, val loss: 0.07652836292982101\n",
      "Epoch 7400: train loss: 0.0001626862067496404, val loss: 0.077161505818367\n",
      "Epoch 7401: train loss: 4.497955887927674e-05, val loss: 0.07870402187108994\n",
      "Epoch 7402: train loss: 0.0001402807974955067, val loss: 0.07722539454698563\n",
      "Epoch 7403: train loss: 5.857587530044839e-05, val loss: 0.07783505320549011\n",
      "Epoch 7404: train loss: 7.067324622767046e-05, val loss: 0.07851924747228622\n",
      "Epoch 7405: train loss: 6.788458995288238e-05, val loss: 0.0770212784409523\n",
      "Epoch 7406: train loss: 3.846855543088168e-05, val loss: 0.07666847854852676\n",
      "Epoch 7407: train loss: 5.8145782531937584e-05, val loss: 0.07880397886037827\n",
      "Epoch 7408: train loss: 2.6095020075445063e-05, val loss: 0.07965250313282013\n",
      "Epoch 7409: train loss: 4.974698094883934e-05, val loss: 0.078246109187603\n",
      "Epoch 7410: train loss: 1.9413566406001337e-05, val loss: 0.07822688668966293\n",
      "Epoch 7411: train loss: 3.923724580090493e-05, val loss: 0.07915505021810532\n",
      "Epoch 7412: train loss: 1.829785469453782e-05, val loss: 0.07913728058338165\n",
      "Epoch 7413: train loss: 2.6485904527362436e-05, val loss: 0.07832445204257965\n",
      "Epoch 7414: train loss: 1.7250138625968248e-05, val loss: 0.07878660410642624\n",
      "Epoch 7415: train loss: 2.1839336113771424e-05, val loss: 0.07970508188009262\n",
      "Epoch 7416: train loss: 1.333098225586582e-05, val loss: 0.07941193878650665\n",
      "Epoch 7417: train loss: 1.6367419448215514e-05, val loss: 0.07910879701375961\n",
      "Epoch 7418: train loss: 1.2286202036193572e-05, val loss: 0.07947414368391037\n",
      "Epoch 7419: train loss: 1.2758002412738279e-05, val loss: 0.07921057939529419\n",
      "Epoch 7420: train loss: 7.629809260834008e-06, val loss: 0.07866786420345306\n",
      "Epoch 7421: train loss: 1.2676380720222369e-05, val loss: 0.07900328189134598\n",
      "Epoch 7422: train loss: 5.368134679883951e-06, val loss: 0.0791294202208519\n",
      "Epoch 7423: train loss: 9.976174624171108e-06, val loss: 0.07865392416715622\n",
      "Epoch 7424: train loss: 5.402044280344853e-06, val loss: 0.07886452227830887\n",
      "Epoch 7425: train loss: 7.907579856691882e-06, val loss: 0.07942694425582886\n",
      "Epoch 7426: train loss: 4.8769456952868495e-06, val loss: 0.07911130040884018\n",
      "Epoch 7427: train loss: 4.960596015735064e-06, val loss: 0.07842647284269333\n",
      "Epoch 7428: train loss: 5.583226538874442e-06, val loss: 0.07857052981853485\n",
      "Epoch 7429: train loss: 3.9195142562675755e-06, val loss: 0.07899495959281921\n",
      "Epoch 7430: train loss: 4.007568804809125e-06, val loss: 0.0786837711930275\n",
      "Epoch 7431: train loss: 3.588572326407302e-06, val loss: 0.07841459661722183\n",
      "Epoch 7432: train loss: 3.668603312689811e-06, val loss: 0.07853429019451141\n",
      "Epoch 7433: train loss: 3.3701155643939273e-06, val loss: 0.07824869453907013\n",
      "Epoch 7434: train loss: 5.656713710777694e-06, val loss: 0.07835403084754944\n",
      "Epoch 7435: train loss: 7.410591024381574e-06, val loss: 0.07799676805734634\n",
      "Epoch 7436: train loss: 1.7849637515610084e-05, val loss: 0.07843577116727829\n",
      "Epoch 7437: train loss: 3.506170469336212e-05, val loss: 0.07743213325738907\n",
      "Epoch 7438: train loss: 6.458368443418294e-05, val loss: 0.07777448743581772\n",
      "Epoch 7439: train loss: 0.0001051513318088837, val loss: 0.07663732022047043\n",
      "Epoch 7440: train loss: 0.0001164329078164883, val loss: 0.07746787369251251\n",
      "Epoch 7441: train loss: 8.462258119834587e-05, val loss: 0.07699566334486008\n",
      "Epoch 7442: train loss: 2.404400765954051e-05, val loss: 0.07636138796806335\n",
      "Epoch 7443: train loss: 1.4775597264815588e-05, val loss: 0.07755260914564133\n",
      "Epoch 7444: train loss: 6.306103023234755e-05, val loss: 0.0759114995598793\n",
      "Epoch 7445: train loss: 9.505109483143315e-05, val loss: 0.07773196697235107\n",
      "Epoch 7446: train loss: 0.00010495191963855177, val loss: 0.07632475346326828\n",
      "Epoch 7447: train loss: 9.573184070177376e-05, val loss: 0.07739874720573425\n",
      "Epoch 7448: train loss: 5.4073287174105644e-05, val loss: 0.07734803110361099\n",
      "Epoch 7449: train loss: 2.0026891434099525e-05, val loss: 0.07641199976205826\n",
      "Epoch 7450: train loss: 3.996918894699775e-05, val loss: 0.07727508991956711\n",
      "Epoch 7451: train loss: 7.559445657534525e-05, val loss: 0.07659915089607239\n",
      "Epoch 7452: train loss: 4.783491021953523e-05, val loss: 0.0770568996667862\n",
      "Epoch 7453: train loss: 3.5000564366782783e-06, val loss: 0.07666345685720444\n",
      "Epoch 7454: train loss: 2.2473730496130884e-05, val loss: 0.07609295099973679\n",
      "Epoch 7455: train loss: 4.990660818293691e-05, val loss: 0.07690992206335068\n",
      "Epoch 7456: train loss: 2.9404131055343896e-05, val loss: 0.07595784962177277\n",
      "Epoch 7457: train loss: 6.293539172474993e-06, val loss: 0.07631959766149521\n",
      "Epoch 7458: train loss: 1.5042877748783212e-05, val loss: 0.07676686346530914\n",
      "Epoch 7459: train loss: 2.8153390303486958e-05, val loss: 0.07538698613643646\n",
      "Epoch 7460: train loss: 2.4377151930821128e-05, val loss: 0.0762321949005127\n",
      "Epoch 7461: train loss: 1.262655496248044e-05, val loss: 0.07617826759815216\n",
      "Epoch 7462: train loss: 7.704710696998518e-06, val loss: 0.07560896128416061\n",
      "Epoch 7463: train loss: 1.1295314834569581e-05, val loss: 0.07662234455347061\n",
      "Epoch 7464: train loss: 1.7030699382303283e-05, val loss: 0.07587042450904846\n",
      "Epoch 7465: train loss: 1.6981139197014272e-05, val loss: 0.07579993456602097\n",
      "Epoch 7466: train loss: 1.0608534466882702e-05, val loss: 0.07654228806495667\n",
      "Epoch 7467: train loss: 5.490239345817827e-06, val loss: 0.07579097151756287\n",
      "Epoch 7468: train loss: 5.851124114997219e-06, val loss: 0.07640675455331802\n",
      "Epoch 7469: train loss: 9.304689228883944e-06, val loss: 0.0762869268655777\n",
      "Epoch 7470: train loss: 1.2770383364113513e-05, val loss: 0.07619085162878036\n",
      "Epoch 7471: train loss: 1.537107709737029e-05, val loss: 0.07616685330867767\n",
      "Epoch 7472: train loss: 3.3279262424912304e-05, val loss: 0.0767938643693924\n",
      "Epoch 7473: train loss: 4.058436752529815e-05, val loss: 0.0764506608247757\n",
      "Epoch 7474: train loss: 2.2486121451947838e-05, val loss: 0.07604974508285522\n",
      "Epoch 7475: train loss: 2.0021747332066298e-05, val loss: 0.0772785022854805\n",
      "Epoch 7476: train loss: 1.954069739440456e-05, val loss: 0.07639916986227036\n",
      "Epoch 7477: train loss: 1.3679731637239456e-05, val loss: 0.07679914683103561\n",
      "Epoch 7478: train loss: 1.598301241756417e-05, val loss: 0.0773768350481987\n",
      "Epoch 7479: train loss: 1.6419789972133003e-05, val loss: 0.07686648517847061\n",
      "Epoch 7480: train loss: 1.4753331925021484e-05, val loss: 0.07648098468780518\n",
      "Epoch 7481: train loss: 2.5836063286988065e-05, val loss: 0.07748229801654816\n",
      "Epoch 7482: train loss: 5.001385943614878e-05, val loss: 0.07623577862977982\n",
      "Epoch 7483: train loss: 8.606964547652751e-05, val loss: 0.07855042070150375\n",
      "Epoch 7484: train loss: 0.0001473318407079205, val loss: 0.0760936364531517\n",
      "Epoch 7485: train loss: 0.00022673234343528748, val loss: 0.08040018379688263\n",
      "Epoch 7486: train loss: 0.000286119116935879, val loss: 0.07668929547071457\n",
      "Epoch 7487: train loss: 0.00023244638578034937, val loss: 0.07821128517389297\n",
      "Epoch 7488: train loss: 0.00022690932382829487, val loss: 0.08146414905786514\n",
      "Epoch 7489: train loss: 8.517946116626263e-05, val loss: 0.0787154883146286\n",
      "Epoch 7490: train loss: 0.00015624558727722615, val loss: 0.07843174785375595\n",
      "Epoch 7491: train loss: 8.153405360644683e-05, val loss: 0.08312295377254486\n",
      "Epoch 7492: train loss: 9.812924690777436e-05, val loss: 0.0823051854968071\n",
      "Epoch 7493: train loss: 6.340354593703523e-05, val loss: 0.07989706844091415\n",
      "Epoch 7494: train loss: 6.363628199324012e-05, val loss: 0.08357011526823044\n",
      "Epoch 7495: train loss: 7.588445441797376e-05, val loss: 0.08527129888534546\n",
      "Epoch 7496: train loss: 3.783032298088074e-05, val loss: 0.08448920398950577\n",
      "Epoch 7497: train loss: 5.3142939577810466e-05, val loss: 0.08521085232496262\n",
      "Epoch 7498: train loss: 3.4762055292958394e-05, val loss: 0.08618448674678802\n",
      "Epoch 7499: train loss: 4.072156662005e-05, val loss: 0.08654846996068954\n",
      "Epoch 7500: train loss: 3.867154009640217e-05, val loss: 0.08624609559774399\n",
      "Epoch 7501: train loss: 1.6928979675867595e-05, val loss: 0.0865810289978981\n",
      "Epoch 7502: train loss: 3.5901837691199034e-05, val loss: 0.087259940803051\n",
      "Epoch 7503: train loss: 1.975751183636021e-05, val loss: 0.08709822595119476\n",
      "Epoch 7504: train loss: 2.3836031687096693e-05, val loss: 0.08753017336130142\n",
      "Epoch 7505: train loss: 1.874704503279645e-05, val loss: 0.08808603137731552\n",
      "Epoch 7506: train loss: 1.2718413017864805e-05, val loss: 0.08825111389160156\n",
      "Epoch 7507: train loss: 2.0764275177498348e-05, val loss: 0.08890610188245773\n",
      "Epoch 7508: train loss: 8.718240678717848e-06, val loss: 0.08886575698852539\n",
      "Epoch 7509: train loss: 1.4458953955909237e-05, val loss: 0.08804495632648468\n",
      "Epoch 7510: train loss: 1.51569092849968e-05, val loss: 0.08853821456432343\n",
      "Epoch 7511: train loss: 2.963287988677621e-05, val loss: 0.08811336010694504\n",
      "Epoch 7512: train loss: 0.00011018405348295346, val loss: 0.08932679891586304\n",
      "Epoch 7513: train loss: 0.0004078228957951069, val loss: 0.08529185503721237\n",
      "Epoch 7514: train loss: 0.0005434461636468768, val loss: 0.08803005516529083\n",
      "Epoch 7515: train loss: 0.00019598574726842344, val loss: 0.08826844394207001\n",
      "Epoch 7516: train loss: 8.731795969652012e-05, val loss: 0.08369427174329758\n",
      "Epoch 7517: train loss: 0.00025943387299776077, val loss: 0.08444692939519882\n",
      "Epoch 7518: train loss: 0.00011017383076250553, val loss: 0.08448310196399689\n",
      "Epoch 7519: train loss: 0.000122773417388089, val loss: 0.08145608752965927\n",
      "Epoch 7520: train loss: 8.94321347004734e-05, val loss: 0.07986005395650864\n",
      "Epoch 7521: train loss: 8.238392183557153e-05, val loss: 0.0815342590212822\n",
      "Epoch 7522: train loss: 6.43824168946594e-05, val loss: 0.08083650469779968\n",
      "Epoch 7523: train loss: 8.446420542895794e-05, val loss: 0.0775739774107933\n",
      "Epoch 7524: train loss: 5.159379361430183e-05, val loss: 0.07810322940349579\n",
      "Epoch 7525: train loss: 5.4185467888601124e-05, val loss: 0.07919064909219742\n",
      "Epoch 7526: train loss: 3.7879450246691704e-05, val loss: 0.07716338336467743\n",
      "Epoch 7527: train loss: 4.848853495786898e-05, val loss: 0.07640589028596878\n",
      "Epoch 7528: train loss: 3.6298286431701854e-05, val loss: 0.07964504510164261\n",
      "Epoch 7529: train loss: 3.956020009354688e-05, val loss: 0.07802917063236237\n",
      "Epoch 7530: train loss: 3.0962404707679525e-05, val loss: 0.07719452679157257\n",
      "Epoch 7531: train loss: 2.743834374996368e-05, val loss: 0.07857061922550201\n",
      "Epoch 7532: train loss: 2.20019937842153e-05, val loss: 0.07756280899047852\n",
      "Epoch 7533: train loss: 2.037018748524133e-05, val loss: 0.07632511854171753\n",
      "Epoch 7534: train loss: 2.035572651948314e-05, val loss: 0.07873949408531189\n",
      "Epoch 7535: train loss: 2.1225154341664165e-05, val loss: 0.08022593706846237\n",
      "Epoch 7536: train loss: 1.6751895600464195e-05, val loss: 0.0765761062502861\n",
      "Epoch 7537: train loss: 1.861826422100421e-05, val loss: 0.077212855219841\n",
      "Epoch 7538: train loss: 1.6466708984808065e-05, val loss: 0.07911943644285202\n",
      "Epoch 7539: train loss: 1.1155173524457496e-05, val loss: 0.07889451086521149\n",
      "Epoch 7540: train loss: 9.93119192571612e-06, val loss: 0.07801751047372818\n",
      "Epoch 7541: train loss: 9.627932740841061e-06, val loss: 0.07888980954885483\n",
      "Epoch 7542: train loss: 7.5814650699612685e-06, val loss: 0.07885891199111938\n",
      "Epoch 7543: train loss: 7.596574505441822e-06, val loss: 0.07818958163261414\n",
      "Epoch 7544: train loss: 6.357063739415025e-06, val loss: 0.07853873074054718\n",
      "Epoch 7545: train loss: 8.964067092165351e-06, val loss: 0.07831236720085144\n",
      "Epoch 7546: train loss: 1.0352989193052053e-05, val loss: 0.078799307346344\n",
      "Epoch 7547: train loss: 1.385985979140969e-05, val loss: 0.07811973243951797\n",
      "Epoch 7548: train loss: 2.4354794732062146e-05, val loss: 0.07973506301641464\n",
      "Epoch 7549: train loss: 5.205251000006683e-05, val loss: 0.0771993026137352\n",
      "Epoch 7550: train loss: 0.00012999553291592747, val loss: 0.08062790334224701\n",
      "Epoch 7551: train loss: 0.00031843973556533456, val loss: 0.0772477239370346\n",
      "Epoch 7552: train loss: 0.0007981602684594691, val loss: 0.08068348467350006\n",
      "Epoch 7553: train loss: 0.0010268822079524398, val loss: 0.07686235010623932\n",
      "Epoch 7554: train loss: 0.0004785920027643442, val loss: 0.08071929216384888\n",
      "Epoch 7555: train loss: 8.124225860228762e-05, val loss: 0.08830807358026505\n",
      "Epoch 7556: train loss: 0.00034901939216069877, val loss: 0.08512116968631744\n",
      "Epoch 7557: train loss: 0.0002229532110504806, val loss: 0.08381276577711105\n",
      "Epoch 7558: train loss: 0.0001462814980186522, val loss: 0.08773189783096313\n",
      "Epoch 7559: train loss: 0.00016389056690968573, val loss: 0.09037632495164871\n",
      "Epoch 7560: train loss: 0.0001269857893930748, val loss: 0.08943220227956772\n",
      "Epoch 7561: train loss: 0.00011391170119168237, val loss: 0.08719125390052795\n",
      "Epoch 7562: train loss: 9.929838415700942e-05, val loss: 0.08677511662244797\n",
      "Epoch 7563: train loss: 8.738418546272442e-05, val loss: 0.08826743811368942\n",
      "Epoch 7564: train loss: 7.471710705431178e-05, val loss: 0.08996053040027618\n",
      "Epoch 7565: train loss: 7.163902773754671e-05, val loss: 0.09052273631095886\n",
      "Epoch 7566: train loss: 5.502357453224249e-05, val loss: 0.0903000459074974\n",
      "Epoch 7567: train loss: 6.369011680362746e-05, val loss: 0.08959992974996567\n",
      "Epoch 7568: train loss: 3.8842954381834716e-05, val loss: 0.08899406343698502\n",
      "Epoch 7569: train loss: 4.8958791012410074e-05, val loss: 0.08923425525426865\n",
      "Epoch 7570: train loss: 3.457488492131233e-05, val loss: 0.09031841903924942\n",
      "Epoch 7571: train loss: 3.5441291402094066e-05, val loss: 0.09081108868122101\n",
      "Epoch 7572: train loss: 3.100062895100564e-05, val loss: 0.09019328653812408\n",
      "Epoch 7573: train loss: 2.5739789634826593e-05, val loss: 0.08971533179283142\n",
      "Epoch 7574: train loss: 2.802199924190063e-05, val loss: 0.08985248953104019\n",
      "Epoch 7575: train loss: 1.629445432627108e-05, val loss: 0.09023963660001755\n",
      "Epoch 7576: train loss: 2.6830319256987423e-05, val loss: 0.09018182754516602\n",
      "Epoch 7577: train loss: 1.087416512746131e-05, val loss: 0.08951302617788315\n",
      "Epoch 7578: train loss: 2.2003199774189852e-05, val loss: 0.08913475275039673\n",
      "Epoch 7579: train loss: 1.110515404434409e-05, val loss: 0.08993692696094513\n",
      "Epoch 7580: train loss: 1.341363531537354e-05, val loss: 0.09068582952022552\n",
      "Epoch 7581: train loss: 1.429994881618768e-05, val loss: 0.08989476412534714\n",
      "Epoch 7582: train loss: 6.7913520069851074e-06, val loss: 0.08856747299432755\n",
      "Epoch 7583: train loss: 1.3821458196616732e-05, val loss: 0.08871668577194214\n",
      "Epoch 7584: train loss: 5.6805170061124954e-06, val loss: 0.08990692347288132\n",
      "Epoch 7585: train loss: 9.830852832237724e-06, val loss: 0.09008518606424332\n",
      "Epoch 7586: train loss: 6.815264896431472e-06, val loss: 0.08915030211210251\n",
      "Epoch 7587: train loss: 5.60616672373726e-06, val loss: 0.08867799490690231\n",
      "Epoch 7588: train loss: 7.77527384343557e-06, val loss: 0.08894858509302139\n",
      "Epoch 7589: train loss: 4.008303676528158e-06, val loss: 0.0890541672706604\n",
      "Epoch 7590: train loss: 5.156815859663766e-06, val loss: 0.08866633474826813\n",
      "Epoch 7591: train loss: 5.615723239316139e-06, val loss: 0.08848657459020615\n",
      "Epoch 7592: train loss: 2.576606448201346e-06, val loss: 0.08870966732501984\n",
      "Epoch 7593: train loss: 4.786192221217789e-06, val loss: 0.0885816365480423\n",
      "Epoch 7594: train loss: 3.5418411243881565e-06, val loss: 0.08818112313747406\n",
      "Epoch 7595: train loss: 2.08601795748109e-06, val loss: 0.08815532922744751\n",
      "Epoch 7596: train loss: 3.809463805737323e-06, val loss: 0.08823811262845993\n",
      "Epoch 7597: train loss: 2.4592416139057605e-06, val loss: 0.08801762759685516\n",
      "Epoch 7598: train loss: 1.5541356788162375e-06, val loss: 0.08772485703229904\n",
      "Epoch 7599: train loss: 3.1335523544839816e-06, val loss: 0.08759605884552002\n",
      "Epoch 7600: train loss: 1.3889909951103618e-06, val loss: 0.08765354007482529\n",
      "Epoch 7601: train loss: 1.617324755898153e-06, val loss: 0.08760646730661392\n",
      "Epoch 7602: train loss: 1.7770043996279128e-06, val loss: 0.08711934834718704\n",
      "Epoch 7603: train loss: 1.732117084429774e-06, val loss: 0.08683701604604721\n",
      "Epoch 7604: train loss: 8.738122119211766e-07, val loss: 0.08695171028375626\n",
      "Epoch 7605: train loss: 1.2979005532542942e-06, val loss: 0.08688122034072876\n",
      "Epoch 7606: train loss: 1.5595480817864882e-06, val loss: 0.08652835339307785\n",
      "Epoch 7607: train loss: 8.599208172199724e-07, val loss: 0.08633972704410553\n",
      "Epoch 7608: train loss: 7.866732971706369e-07, val loss: 0.0863875299692154\n",
      "Epoch 7609: train loss: 8.704348033461429e-07, val loss: 0.08661502599716187\n",
      "Epoch 7610: train loss: 1.1574702512007207e-06, val loss: 0.08649169653654099\n",
      "Epoch 7611: train loss: 9.046634659171104e-07, val loss: 0.08609668910503387\n",
      "Epoch 7612: train loss: 6.531391250064189e-07, val loss: 0.08617228269577026\n",
      "Epoch 7613: train loss: 4.940352482663002e-07, val loss: 0.08628904819488525\n",
      "Epoch 7614: train loss: 6.412600441763061e-07, val loss: 0.08604196459054947\n",
      "Epoch 7615: train loss: 9.532478202345374e-07, val loss: 0.08607911318540573\n",
      "Epoch 7616: train loss: 1.3607111668534344e-06, val loss: 0.08576225489377975\n",
      "Epoch 7617: train loss: 2.7847406727232737e-06, val loss: 0.0861620232462883\n",
      "Epoch 7618: train loss: 7.399756213999353e-06, val loss: 0.08535129576921463\n",
      "Epoch 7619: train loss: 2.5374101824127138e-05, val loss: 0.0865808054804802\n",
      "Epoch 7620: train loss: 9.42324913921766e-05, val loss: 0.08506780862808228\n",
      "Epoch 7621: train loss: 0.00030757024069316685, val loss: 0.08996658772230148\n",
      "Epoch 7622: train loss: 0.0006357316742651165, val loss: 0.08928939700126648\n",
      "Epoch 7623: train loss: 0.001383853959850967, val loss: 0.09228090941905975\n",
      "Epoch 7624: train loss: 0.0017557397950440645, val loss: 0.08937066793441772\n",
      "Epoch 7625: train loss: 0.00109042227268219, val loss: 0.09004636108875275\n",
      "Epoch 7626: train loss: 0.0002547218755353242, val loss: 0.09478650242090225\n",
      "Epoch 7627: train loss: 0.0006219366332516074, val loss: 0.08970785140991211\n",
      "Epoch 7628: train loss: 0.0003992447454947978, val loss: 0.08851279318332672\n",
      "Epoch 7629: train loss: 0.00029717572033405304, val loss: 0.09293659031391144\n",
      "Epoch 7630: train loss: 0.00035925573320128024, val loss: 0.09111865609884262\n",
      "Epoch 7631: train loss: 0.00015115458518266678, val loss: 0.08738571405410767\n",
      "Epoch 7632: train loss: 0.00024823856074362993, val loss: 0.08698030561208725\n",
      "Epoch 7633: train loss: 0.00015128559607546777, val loss: 0.08680088818073273\n",
      "Epoch 7634: train loss: 0.00019988277927041054, val loss: 0.08632927387952805\n",
      "Epoch 7635: train loss: 7.525143882958218e-05, val loss: 0.08591298758983612\n",
      "Epoch 7636: train loss: 0.00016070131096057594, val loss: 0.08531174808740616\n",
      "Epoch 7637: train loss: 8.292938582599163e-05, val loss: 0.08564911782741547\n",
      "Epoch 7638: train loss: 0.0001226105378009379, val loss: 0.08442424982786179\n",
      "Epoch 7639: train loss: 5.7363831729162484e-05, val loss: 0.0844346210360527\n",
      "Epoch 7640: train loss: 7.439804176101461e-05, val loss: 0.08535107225179672\n",
      "Epoch 7641: train loss: 7.834059215383604e-05, val loss: 0.08470731228590012\n",
      "Epoch 7642: train loss: 4.8871876060729846e-05, val loss: 0.08398408442735672\n",
      "Epoch 7643: train loss: 5.962704017292708e-05, val loss: 0.08382628113031387\n",
      "Epoch 7644: train loss: 3.3657459425739944e-05, val loss: 0.0834682509303093\n",
      "Epoch 7645: train loss: 5.204627086641267e-05, val loss: 0.08287042379379272\n",
      "Epoch 7646: train loss: 3.383772127563134e-05, val loss: 0.08191772550344467\n",
      "Epoch 7647: train loss: 3.0286941182566807e-05, val loss: 0.08202672749757767\n",
      "Epoch 7648: train loss: 2.68684234470129e-05, val loss: 0.0830768272280693\n",
      "Epoch 7649: train loss: 3.2450374419568107e-05, val loss: 0.08322615176439285\n",
      "Epoch 7650: train loss: 2.0383131413836963e-05, val loss: 0.08276225626468658\n",
      "Epoch 7651: train loss: 2.0375651729409583e-05, val loss: 0.0821596160531044\n",
      "Epoch 7652: train loss: 1.842473284341395e-05, val loss: 0.08177599310874939\n",
      "Epoch 7653: train loss: 1.9492601495585404e-05, val loss: 0.08203445374965668\n",
      "Epoch 7654: train loss: 1.6498566765221767e-05, val loss: 0.0819116160273552\n",
      "Epoch 7655: train loss: 1.0955599464068655e-05, val loss: 0.08168382942676544\n",
      "Epoch 7656: train loss: 1.3418528396869078e-05, val loss: 0.08176343888044357\n",
      "Epoch 7657: train loss: 1.1945197002205532e-05, val loss: 0.08169912546873093\n",
      "Epoch 7658: train loss: 1.1524929504957981e-05, val loss: 0.08148228377103806\n",
      "Epoch 7659: train loss: 5.962219347566133e-06, val loss: 0.08082182705402374\n",
      "Epoch 7660: train loss: 1.056806922861142e-05, val loss: 0.0803380236029625\n",
      "Epoch 7661: train loss: 6.729077540512662e-06, val loss: 0.08100719749927521\n",
      "Epoch 7662: train loss: 7.871138222981244e-06, val loss: 0.08178678154945374\n",
      "Epoch 7663: train loss: 5.045123543823138e-06, val loss: 0.0816277489066124\n",
      "Epoch 7664: train loss: 5.318666808307171e-06, val loss: 0.08093138784170151\n",
      "Epoch 7665: train loss: 5.916727332078153e-06, val loss: 0.08054490387439728\n",
      "Epoch 7666: train loss: 4.061780600750353e-06, val loss: 0.0810309424996376\n",
      "Epoch 7667: train loss: 3.996699888375588e-06, val loss: 0.08130208402872086\n",
      "Epoch 7668: train loss: 2.923903821283602e-06, val loss: 0.08093991130590439\n",
      "Epoch 7669: train loss: 4.058850208821241e-06, val loss: 0.08071977645158768\n",
      "Epoch 7670: train loss: 3.1698939437774243e-06, val loss: 0.08083026111125946\n",
      "Epoch 7671: train loss: 2.0853283331234707e-06, val loss: 0.08103113621473312\n",
      "Epoch 7672: train loss: 2.6613315640133806e-06, val loss: 0.08081686496734619\n",
      "Epoch 7673: train loss: 2.1823925635544583e-06, val loss: 0.08024878799915314\n",
      "Epoch 7674: train loss: 2.4791681880742544e-06, val loss: 0.08011119067668915\n",
      "Epoch 7675: train loss: 1.4714477174493368e-06, val loss: 0.0801362618803978\n",
      "Epoch 7676: train loss: 1.5912927437966573e-06, val loss: 0.07989683002233505\n",
      "Epoch 7677: train loss: 1.7816992112784646e-06, val loss: 0.07972710579633713\n",
      "Epoch 7678: train loss: 1.458909082430182e-06, val loss: 0.07963889092206955\n",
      "Epoch 7679: train loss: 1.3362132449401543e-06, val loss: 0.07950683683156967\n",
      "Epoch 7680: train loss: 9.304158652412298e-07, val loss: 0.07936294376850128\n",
      "Epoch 7681: train loss: 1.352039362245705e-06, val loss: 0.07928628474473953\n",
      "Epoch 7682: train loss: 1.0989389238602598e-06, val loss: 0.07927798479795456\n",
      "Epoch 7683: train loss: 1.0035898867499782e-06, val loss: 0.07893820106983185\n",
      "Epoch 7684: train loss: 6.860719850010355e-07, val loss: 0.07888681441545486\n",
      "Epoch 7685: train loss: 8.156509352374997e-07, val loss: 0.07899581640958786\n",
      "Epoch 7686: train loss: 7.223839020298328e-07, val loss: 0.07866144925355911\n",
      "Epoch 7687: train loss: 6.342519327517948e-07, val loss: 0.07845684140920639\n",
      "Epoch 7688: train loss: 5.299880285747349e-07, val loss: 0.07858731597661972\n",
      "Epoch 7689: train loss: 4.2153220647378475e-07, val loss: 0.0785309374332428\n",
      "Epoch 7690: train loss: 5.430430292108213e-07, val loss: 0.07835306972265244\n",
      "Epoch 7691: train loss: 4.5500340206672263e-07, val loss: 0.07814977318048477\n",
      "Epoch 7692: train loss: 2.9383849664554873e-07, val loss: 0.07808861881494522\n",
      "Epoch 7693: train loss: 4.280090308839135e-07, val loss: 0.07805237919092178\n",
      "Epoch 7694: train loss: 3.0132628126011696e-07, val loss: 0.0778861865401268\n",
      "Epoch 7695: train loss: 3.749649408746336e-07, val loss: 0.07779528200626373\n",
      "Epoch 7696: train loss: 2.9118760380697495e-07, val loss: 0.07760214060544968\n",
      "Epoch 7697: train loss: 3.129060814899276e-07, val loss: 0.07761593163013458\n",
      "Epoch 7698: train loss: 2.815051800553192e-07, val loss: 0.07749690115451813\n",
      "Epoch 7699: train loss: 2.7439173777565884e-07, val loss: 0.07740744203329086\n",
      "Epoch 7700: train loss: 4.627869998330425e-07, val loss: 0.07725763320922852\n",
      "Epoch 7701: train loss: 9.297331189372926e-07, val loss: 0.0774175152182579\n",
      "Epoch 7702: train loss: 2.814723984556622e-06, val loss: 0.07675925642251968\n",
      "Epoch 7703: train loss: 1.0589600606181193e-05, val loss: 0.07788205146789551\n",
      "Epoch 7704: train loss: 4.426331724971533e-05, val loss: 0.07654806971549988\n",
      "Epoch 7705: train loss: 0.00020303895871620625, val loss: 0.0821729302406311\n",
      "Epoch 7706: train loss: 0.0007213447242975235, val loss: 0.07874266058206558\n",
      "Epoch 7707: train loss: 0.002533125225454569, val loss: 0.08593380451202393\n",
      "Epoch 7708: train loss: 0.001559644821099937, val loss: 0.09466283768415451\n",
      "Epoch 7709: train loss: 0.0003817728138528764, val loss: 0.09644283354282379\n",
      "Epoch 7710: train loss: 0.0006671345909126103, val loss: 0.10135727375745773\n",
      "Epoch 7711: train loss: 0.00034055308788083494, val loss: 0.10738315433263779\n",
      "Epoch 7712: train loss: 0.0003587837563827634, val loss: 0.11118810623884201\n",
      "Epoch 7713: train loss: 0.0003073794359806925, val loss: 0.11170943826436996\n",
      "Epoch 7714: train loss: 0.00022744065790902823, val loss: 0.1114739179611206\n",
      "Epoch 7715: train loss: 0.0002272242563776672, val loss: 0.11368274688720703\n",
      "Epoch 7716: train loss: 0.00020678316650446504, val loss: 0.11764233559370041\n",
      "Epoch 7717: train loss: 0.00013415042485576123, val loss: 0.11757632344961166\n",
      "Epoch 7718: train loss: 0.00017532872152514756, val loss: 0.11332371085882187\n",
      "Epoch 7719: train loss: 9.034784307004884e-05, val loss: 0.11256124824285507\n",
      "Epoch 7720: train loss: 0.00014336440654005855, val loss: 0.11681089550256729\n",
      "Epoch 7721: train loss: 8.358952618436888e-05, val loss: 0.11961710453033447\n",
      "Epoch 7722: train loss: 8.862302638590336e-05, val loss: 0.11762607097625732\n",
      "Epoch 7723: train loss: 7.492743316106498e-05, val loss: 0.11486498266458511\n",
      "Epoch 7724: train loss: 6.754828064003959e-05, val loss: 0.11540504544973373\n",
      "Epoch 7725: train loss: 6.048051727702841e-05, val loss: 0.11798220127820969\n",
      "Epoch 7726: train loss: 5.542161670746282e-05, val loss: 0.11888515949249268\n",
      "Epoch 7727: train loss: 4.5365875848801807e-05, val loss: 0.11797530949115753\n",
      "Epoch 7728: train loss: 3.4150751162087545e-05, val loss: 0.11680709570646286\n",
      "Epoch 7729: train loss: 4.6293422201415524e-05, val loss: 0.11645471304655075\n",
      "Epoch 7730: train loss: 2.9199985874583945e-05, val loss: 0.11685649305582047\n",
      "Epoch 7731: train loss: 2.758841401373502e-05, val loss: 0.1172265037894249\n",
      "Epoch 7732: train loss: 2.9186145184212364e-05, val loss: 0.11757110804319382\n",
      "Epoch 7733: train loss: 2.1195817680563778e-05, val loss: 0.11816507577896118\n",
      "Epoch 7734: train loss: 2.155427137040533e-05, val loss: 0.11864793300628662\n",
      "Epoch 7735: train loss: 1.7904441847349517e-05, val loss: 0.11859104782342911\n",
      "Epoch 7736: train loss: 1.894220622489229e-05, val loss: 0.11811299622058868\n",
      "Epoch 7737: train loss: 1.3259295883472078e-05, val loss: 0.118010513484478\n",
      "Epoch 7738: train loss: 1.3575686352851335e-05, val loss: 0.11870451271533966\n",
      "Epoch 7739: train loss: 1.2320999303483404e-05, val loss: 0.11939281225204468\n",
      "Epoch 7740: train loss: 1.3029746696702205e-05, val loss: 0.11927272379398346\n",
      "Epoch 7741: train loss: 6.7794517235597596e-06, val loss: 0.11856187880039215\n",
      "Epoch 7742: train loss: 1.1167175216542091e-05, val loss: 0.11796414852142334\n",
      "Epoch 7743: train loss: 8.159757271641865e-06, val loss: 0.1184072494506836\n",
      "Epoch 7744: train loss: 7.259602170961443e-06, val loss: 0.11939728260040283\n",
      "Epoch 7745: train loss: 6.0225752349651884e-06, val loss: 0.1200937032699585\n",
      "Epoch 7746: train loss: 7.246631412272109e-06, val loss: 0.11992905288934708\n",
      "Epoch 7747: train loss: 4.410331257531652e-06, val loss: 0.11937122792005539\n",
      "Epoch 7748: train loss: 4.706083473138278e-06, val loss: 0.11926672607660294\n",
      "Epoch 7749: train loss: 5.241230155661469e-06, val loss: 0.1195109412074089\n",
      "Epoch 7750: train loss: 2.7529658837011084e-06, val loss: 0.11956164985895157\n",
      "Epoch 7751: train loss: 4.073031050211284e-06, val loss: 0.11934669315814972\n",
      "Epoch 7752: train loss: 3.3327237360936124e-06, val loss: 0.11918371915817261\n",
      "Epoch 7753: train loss: 2.488277687007212e-06, val loss: 0.11923134326934814\n",
      "Epoch 7754: train loss: 2.6693505787989125e-06, val loss: 0.1193946972489357\n",
      "Epoch 7755: train loss: 2.608013119242969e-06, val loss: 0.11962177604436874\n",
      "Epoch 7756: train loss: 2.0912482341373106e-06, val loss: 0.1197398453950882\n",
      "Epoch 7757: train loss: 1.8477177263775957e-06, val loss: 0.1194465383887291\n",
      "Epoch 7758: train loss: 1.9214944586565252e-06, val loss: 0.11899589747190475\n",
      "Epoch 7759: train loss: 1.5562756061626715e-06, val loss: 0.11894726753234863\n",
      "Epoch 7760: train loss: 1.471506834604952e-06, val loss: 0.11914515495300293\n",
      "Epoch 7761: train loss: 1.510756419520476e-06, val loss: 0.11919903755187988\n",
      "Epoch 7762: train loss: 1.1215179256396368e-06, val loss: 0.11906407028436661\n",
      "Epoch 7763: train loss: 1.1388425491531962e-06, val loss: 0.11891336739063263\n",
      "Epoch 7764: train loss: 1.1945581945838057e-06, val loss: 0.11881007999181747\n",
      "Epoch 7765: train loss: 7.671487196603266e-07, val loss: 0.11876244843006134\n",
      "Epoch 7766: train loss: 9.74725139712973e-07, val loss: 0.1186700239777565\n",
      "Epoch 7767: train loss: 7.860710979912255e-07, val loss: 0.1185452938079834\n",
      "Epoch 7768: train loss: 6.997167361078027e-07, val loss: 0.11872782558202744\n",
      "Epoch 7769: train loss: 7.193930855464714e-07, val loss: 0.11895289272069931\n",
      "Epoch 7770: train loss: 4.835674758396635e-07, val loss: 0.11873669922351837\n",
      "Epoch 7771: train loss: 7.377269639619044e-07, val loss: 0.11838314682245255\n",
      "Epoch 7772: train loss: 3.815971467702184e-07, val loss: 0.11820896714925766\n",
      "Epoch 7773: train loss: 4.884369673163746e-07, val loss: 0.11833804845809937\n",
      "Epoch 7774: train loss: 4.061586764692038e-07, val loss: 0.11850356310606003\n",
      "Epoch 7775: train loss: 4.808303515346779e-07, val loss: 0.1182805523276329\n",
      "Epoch 7776: train loss: 3.380446855771879e-07, val loss: 0.11808426678180695\n",
      "Epoch 7777: train loss: 3.095181568824046e-07, val loss: 0.1181146502494812\n",
      "Epoch 7778: train loss: 2.515589869744872e-07, val loss: 0.11814544349908829\n",
      "Epoch 7779: train loss: 3.4597312037476513e-07, val loss: 0.11808126419782639\n",
      "Epoch 7780: train loss: 3.058233914998709e-07, val loss: 0.11784380674362183\n",
      "Epoch 7781: train loss: 3.1034363701110124e-07, val loss: 0.1177855134010315\n",
      "Epoch 7782: train loss: 2.9841191917512333e-07, val loss: 0.11774551868438721\n",
      "Epoch 7783: train loss: 2.826799345712061e-07, val loss: 0.11781828850507736\n",
      "Epoch 7784: train loss: 3.1051436621964967e-07, val loss: 0.11765293031930923\n",
      "Epoch 7785: train loss: 4.599240526204085e-07, val loss: 0.11766934394836426\n",
      "Epoch 7786: train loss: 7.045207439659862e-07, val loss: 0.11730184406042099\n",
      "Epoch 7787: train loss: 1.5938429669404286e-06, val loss: 0.11775853484869003\n",
      "Epoch 7788: train loss: 4.472010004974436e-06, val loss: 0.11708682030439377\n",
      "Epoch 7789: train loss: 1.5040182006487157e-05, val loss: 0.11817151308059692\n",
      "Epoch 7790: train loss: 5.5546661315020174e-05, val loss: 0.11642169952392578\n",
      "Epoch 7791: train loss: 0.0002048357273451984, val loss: 0.11954253911972046\n",
      "Epoch 7792: train loss: 0.0006556028383783996, val loss: 0.11473719030618668\n",
      "Epoch 7793: train loss: 0.0018682364607229829, val loss: 0.12030772119760513\n",
      "Epoch 7794: train loss: 0.002519186120480299, val loss: 0.11480474472045898\n",
      "Epoch 7795: train loss: 0.0009182458161376417, val loss: 0.1178571954369545\n",
      "Epoch 7796: train loss: 0.0004301825538277626, val loss: 0.12350975722074509\n",
      "Epoch 7797: train loss: 0.0008442966500297189, val loss: 0.1142551451921463\n",
      "Epoch 7798: train loss: 0.00018961702880915254, val loss: 0.10953295230865479\n",
      "Epoch 7799: train loss: 0.0005269258981570601, val loss: 0.11082252115011215\n",
      "Epoch 7800: train loss: 0.00023632981174159795, val loss: 0.11302199214696884\n",
      "Epoch 7801: train loss: 0.00033107047784142196, val loss: 0.11157949268817902\n",
      "Epoch 7802: train loss: 0.00016170252638403326, val loss: 0.10895233601331711\n",
      "Epoch 7803: train loss: 0.00025302026188001037, val loss: 0.10631541162729263\n",
      "Epoch 7804: train loss: 0.00013796651910524815, val loss: 0.10560128837823868\n",
      "Epoch 7805: train loss: 0.00018161626940127462, val loss: 0.10458376258611679\n",
      "Epoch 7806: train loss: 0.00013216254592407495, val loss: 0.10558655112981796\n",
      "Epoch 7807: train loss: 0.00012111227260902524, val loss: 0.10591202229261398\n",
      "Epoch 7808: train loss: 8.496709051541984e-05, val loss: 0.10401784628629684\n",
      "Epoch 7809: train loss: 0.0001282460434595123, val loss: 0.101875901222229\n",
      "Epoch 7810: train loss: 6.463062163675204e-05, val loss: 0.10235073417425156\n",
      "Epoch 7811: train loss: 8.045028516789898e-05, val loss: 0.10555697977542877\n",
      "Epoch 7812: train loss: 6.955654680496082e-05, val loss: 0.10662628710269928\n",
      "Epoch 7813: train loss: 5.430643432191573e-05, val loss: 0.10317202657461166\n",
      "Epoch 7814: train loss: 5.034066271036863e-05, val loss: 0.10040117800235748\n",
      "Epoch 7815: train loss: 4.994469054508954e-05, val loss: 0.10113978385925293\n",
      "Epoch 7816: train loss: 3.526792352204211e-05, val loss: 0.10361337661743164\n",
      "Epoch 7817: train loss: 3.096788714174181e-05, val loss: 0.10385387390851974\n",
      "Epoch 7818: train loss: 3.786074012168683e-05, val loss: 0.1014922484755516\n",
      "Epoch 7819: train loss: 2.110758941853419e-05, val loss: 0.10106456279754639\n",
      "Epoch 7820: train loss: 3.239349098294042e-05, val loss: 0.10250013321638107\n",
      "Epoch 7821: train loss: 1.56950864038663e-05, val loss: 0.10339798778295517\n",
      "Epoch 7822: train loss: 2.473155655025039e-05, val loss: 0.10220427811145782\n",
      "Epoch 7823: train loss: 1.506887474533869e-05, val loss: 0.10060887783765793\n",
      "Epoch 7824: train loss: 1.9246417650720105e-05, val loss: 0.10140097141265869\n",
      "Epoch 7825: train loss: 1.4463904335570987e-05, val loss: 0.10325092077255249\n",
      "Epoch 7826: train loss: 1.2757899639836978e-05, val loss: 0.10330400615930557\n",
      "Epoch 7827: train loss: 1.2607251846930012e-05, val loss: 0.10145635902881622\n",
      "Epoch 7828: train loss: 1.0041593668574933e-05, val loss: 0.10001178830862045\n",
      "Epoch 7829: train loss: 1.392189096804941e-05, val loss: 0.1006588563323021\n",
      "Epoch 7830: train loss: 4.757140686706407e-06, val loss: 0.10150658339262009\n",
      "Epoch 7831: train loss: 1.2775980394508224e-05, val loss: 0.10126767307519913\n",
      "Epoch 7832: train loss: 9.086516911338549e-06, val loss: 0.1006305143237114\n",
      "Epoch 7833: train loss: 9.403783224115614e-06, val loss: 0.10041554272174835\n",
      "Epoch 7834: train loss: 9.000131285574753e-06, val loss: 0.10081689804792404\n",
      "Epoch 7835: train loss: 5.6021008276729845e-06, val loss: 0.1007387638092041\n",
      "Epoch 7836: train loss: 8.184060789062642e-06, val loss: 0.10028576105833054\n",
      "Epoch 7837: train loss: 5.086537839815719e-06, val loss: 0.10066413134336472\n",
      "Epoch 7838: train loss: 6.589446456928272e-06, val loss: 0.1015477403998375\n",
      "Epoch 7839: train loss: 4.533816081675468e-06, val loss: 0.101852647960186\n",
      "Epoch 7840: train loss: 4.6186946747184265e-06, val loss: 0.10097487270832062\n",
      "Epoch 7841: train loss: 3.832881247944897e-06, val loss: 0.09987207502126694\n",
      "Epoch 7842: train loss: 4.0826835174812e-06, val loss: 0.09989432990550995\n",
      "Epoch 7843: train loss: 3.5599307466327446e-06, val loss: 0.10064653307199478\n",
      "Epoch 7844: train loss: 2.9854345484636724e-06, val loss: 0.10099338740110397\n",
      "Epoch 7845: train loss: 2.6857985631068004e-06, val loss: 0.10038194805383682\n",
      "Epoch 7846: train loss: 2.818772600221564e-06, val loss: 0.09964773803949356\n",
      "Epoch 7847: train loss: 2.1175326310185483e-06, val loss: 0.0999675914645195\n",
      "Epoch 7848: train loss: 2.7268938538327347e-06, val loss: 0.10049833357334137\n",
      "Epoch 7849: train loss: 1.8044805756289861e-06, val loss: 0.10012342780828476\n",
      "Epoch 7850: train loss: 1.9859965050272876e-06, val loss: 0.09906739741563797\n",
      "Epoch 7851: train loss: 3.220789039914962e-06, val loss: 0.09915546327829361\n",
      "Epoch 7852: train loss: 1.350865659333067e-06, val loss: 0.0996689721941948\n",
      "Epoch 7853: train loss: 2.09283302865515e-06, val loss: 0.09938779473304749\n",
      "Epoch 7854: train loss: 1.8438757933836314e-06, val loss: 0.09906251728534698\n",
      "Epoch 7855: train loss: 1.0369315077696228e-06, val loss: 0.09901564568281174\n",
      "Epoch 7856: train loss: 1.8328249780097394e-06, val loss: 0.09875940531492233\n",
      "Epoch 7857: train loss: 1.1011495644197566e-06, val loss: 0.09858709573745728\n",
      "Epoch 7858: train loss: 1.1425196362324641e-06, val loss: 0.09861208498477936\n",
      "Epoch 7859: train loss: 1.2763554195771576e-06, val loss: 0.09854534268379211\n",
      "Epoch 7860: train loss: 9.028329372995358e-07, val loss: 0.09829306602478027\n",
      "Epoch 7861: train loss: 6.952847684260632e-07, val loss: 0.09801080077886581\n",
      "Epoch 7862: train loss: 1.3290306242197403e-06, val loss: 0.09788904339075089\n",
      "Epoch 7863: train loss: 5.571924930336536e-07, val loss: 0.0980878546833992\n",
      "Epoch 7864: train loss: 6.573183668479032e-07, val loss: 0.09813583642244339\n",
      "Epoch 7865: train loss: 9.293325433645805e-07, val loss: 0.09762727469205856\n",
      "Epoch 7866: train loss: 8.538946190128627e-07, val loss: 0.0975487008690834\n",
      "Epoch 7867: train loss: 8.634037840238307e-07, val loss: 0.09764768928289413\n",
      "Epoch 7868: train loss: 4.629195586858259e-07, val loss: 0.09741516411304474\n",
      "Epoch 7869: train loss: 4.7091222654671583e-07, val loss: 0.0973900631070137\n",
      "Epoch 7870: train loss: 4.3168620322830975e-07, val loss: 0.09741156548261642\n",
      "Epoch 7871: train loss: 4.3871477828361094e-07, val loss: 0.09717017412185669\n",
      "Epoch 7872: train loss: 2.4208679860748816e-07, val loss: 0.09697552025318146\n",
      "Epoch 7873: train loss: 3.838768520836311e-07, val loss: 0.0970413014292717\n",
      "Epoch 7874: train loss: 4.388981551528559e-07, val loss: 0.09694577753543854\n",
      "Epoch 7875: train loss: 5.461996579469997e-07, val loss: 0.09670966118574142\n",
      "Epoch 7876: train loss: 1.2012144452455686e-06, val loss: 0.09651923924684525\n",
      "Epoch 7877: train loss: 3.4785396110237343e-06, val loss: 0.0967131182551384\n",
      "Epoch 7878: train loss: 1.2207758118165657e-05, val loss: 0.09572577476501465\n",
      "Epoch 7879: train loss: 3.5053915780736133e-05, val loss: 0.09668108075857162\n",
      "Epoch 7880: train loss: 7.951114093884826e-05, val loss: 0.09576359391212463\n",
      "Epoch 7881: train loss: 0.0001840682962210849, val loss: 0.0987357422709465\n",
      "Epoch 7882: train loss: 0.00025462848134338856, val loss: 0.09459739923477173\n",
      "Epoch 7883: train loss: 0.0002857089857570827, val loss: 0.09520622342824936\n",
      "Epoch 7884: train loss: 0.00014182789891492575, val loss: 0.0963316485285759\n",
      "Epoch 7885: train loss: 3.1235438655130565e-05, val loss: 0.09506408125162125\n",
      "Epoch 7886: train loss: 9.256904013454914e-05, val loss: 0.0961761623620987\n",
      "Epoch 7887: train loss: 0.00011226756760152057, val loss: 0.09708938002586365\n",
      "Epoch 7888: train loss: 2.2445192371378653e-05, val loss: 0.09594216197729111\n",
      "Epoch 7889: train loss: 5.687638986273669e-05, val loss: 0.09649993479251862\n",
      "Epoch 7890: train loss: 5.7536566600902006e-05, val loss: 0.09805392473936081\n",
      "Epoch 7891: train loss: 2.3287178919417784e-05, val loss: 0.09730881452560425\n",
      "Epoch 7892: train loss: 3.834537710645236e-05, val loss: 0.09698496013879776\n",
      "Epoch 7893: train loss: 3.552681300789118e-05, val loss: 0.098027303814888\n",
      "Epoch 7894: train loss: 1.9863577108480968e-05, val loss: 0.09809515625238419\n",
      "Epoch 7895: train loss: 2.6745918148662895e-05, val loss: 0.09771516174077988\n",
      "Epoch 7896: train loss: 2.3231057639350183e-05, val loss: 0.09840774536132812\n",
      "Epoch 7897: train loss: 1.8386437659501098e-05, val loss: 0.09831412881612778\n",
      "Epoch 7898: train loss: 1.6062676877481863e-05, val loss: 0.09757532924413681\n",
      "Epoch 7899: train loss: 1.9289458577986807e-05, val loss: 0.09889483451843262\n",
      "Epoch 7900: train loss: 1.0766060768219177e-05, val loss: 0.09976399689912796\n",
      "Epoch 7901: train loss: 1.2051656085532159e-05, val loss: 0.09811552613973618\n",
      "Epoch 7902: train loss: 1.9159753719577566e-05, val loss: 0.0986391231417656\n",
      "Epoch 7903: train loss: 2.3676719138165936e-05, val loss: 0.0988551527261734\n",
      "Epoch 7904: train loss: 1.5924617400742136e-05, val loss: 0.10005159676074982\n",
      "Epoch 7905: train loss: 1.1195222214155365e-05, val loss: 0.10011090338230133\n",
      "Epoch 7906: train loss: 1.4684272173326463e-05, val loss: 0.0993698462843895\n",
      "Epoch 7907: train loss: 1.2527794751804322e-05, val loss: 0.10051315277814865\n",
      "Epoch 7908: train loss: 1.0892448699451052e-05, val loss: 0.10143706947565079\n",
      "Epoch 7909: train loss: 1.0395816389063839e-05, val loss: 0.1010226234793663\n",
      "Epoch 7910: train loss: 6.440726792789064e-06, val loss: 0.10105613619089127\n",
      "Epoch 7911: train loss: 1.155173231381923e-05, val loss: 0.10148079693317413\n",
      "Epoch 7912: train loss: 1.0141094207938295e-05, val loss: 0.10139641910791397\n",
      "Epoch 7913: train loss: 4.570702458295273e-06, val loss: 0.10153231769800186\n",
      "Epoch 7914: train loss: 3.960305093642091e-06, val loss: 0.10196516662836075\n",
      "Epoch 7915: train loss: 9.49419063545065e-06, val loss: 0.10167490690946579\n",
      "Epoch 7916: train loss: 1.0348603609600104e-05, val loss: 0.10152273625135422\n",
      "Epoch 7917: train loss: 6.377591944328742e-06, val loss: 0.1026531383395195\n",
      "Epoch 7918: train loss: 3.8294269870675635e-06, val loss: 0.10176189988851547\n",
      "Epoch 7919: train loss: 4.578886546369176e-06, val loss: 0.10186012089252472\n",
      "Epoch 7920: train loss: 4.69363931188127e-06, val loss: 0.10228746384382248\n",
      "Epoch 7921: train loss: 4.566334155242657e-06, val loss: 0.10149224102497101\n",
      "Epoch 7922: train loss: 5.188723207538715e-06, val loss: 0.10220308601856232\n",
      "Epoch 7923: train loss: 6.995937383180717e-06, val loss: 0.10155852138996124\n",
      "Epoch 7924: train loss: 1.0229556210106239e-05, val loss: 0.10206401348114014\n",
      "Epoch 7925: train loss: 1.618172063899692e-05, val loss: 0.10133876651525497\n",
      "Epoch 7926: train loss: 2.9716817152802832e-05, val loss: 0.10255105793476105\n",
      "Epoch 7927: train loss: 6.36142649454996e-05, val loss: 0.09986111521720886\n",
      "Epoch 7928: train loss: 0.00014738149184267968, val loss: 0.10366328805685043\n",
      "Epoch 7929: train loss: 0.0003560447948984802, val loss: 0.09766259044408798\n",
      "Epoch 7930: train loss: 0.000763530028052628, val loss: 0.10285317897796631\n",
      "Epoch 7931: train loss: 0.000827376323286444, val loss: 0.1002509593963623\n",
      "Epoch 7932: train loss: 0.000420826836489141, val loss: 0.0974339172244072\n",
      "Epoch 7933: train loss: 0.0003396493848413229, val loss: 0.09555937349796295\n",
      "Epoch 7934: train loss: 0.00023626470647286624, val loss: 0.09701067209243774\n",
      "Epoch 7935: train loss: 0.00022694228391628712, val loss: 0.09574370831251144\n",
      "Epoch 7936: train loss: 0.00018625376105774194, val loss: 0.09441311657428741\n",
      "Epoch 7937: train loss: 0.00016835134010761976, val loss: 0.09509625285863876\n",
      "Epoch 7938: train loss: 0.000127740393509157, val loss: 0.0950360894203186\n",
      "Epoch 7939: train loss: 0.00018472844385541975, val loss: 0.09854119271039963\n",
      "Epoch 7940: train loss: 7.51755724195391e-05, val loss: 0.09434958547353745\n",
      "Epoch 7941: train loss: 8.77538914210163e-05, val loss: 0.09154146164655685\n",
      "Epoch 7942: train loss: 7.356173591688275e-05, val loss: 0.0965433120727539\n",
      "Epoch 7943: train loss: 7.791832467773929e-05, val loss: 0.09677398949861526\n",
      "Epoch 7944: train loss: 8.367530244868249e-05, val loss: 0.0937633216381073\n",
      "Epoch 7945: train loss: 3.741217005881481e-05, val loss: 0.09086302667856216\n",
      "Epoch 7946: train loss: 5.801497172797099e-05, val loss: 0.09221948683261871\n",
      "Epoch 7947: train loss: 4.1743220208445564e-05, val loss: 0.09578493982553482\n",
      "Epoch 7948: train loss: 3.7529407563852146e-05, val loss: 0.09339052438735962\n",
      "Epoch 7949: train loss: 3.1433093681698665e-05, val loss: 0.08993254601955414\n",
      "Epoch 7950: train loss: 3.640050636022352e-05, val loss: 0.09128906577825546\n",
      "Epoch 7951: train loss: 3.663928873720579e-05, val loss: 0.09334144741296768\n",
      "Epoch 7952: train loss: 1.8888915292336605e-05, val loss: 0.09179965406656265\n",
      "Epoch 7953: train loss: 2.000286076508928e-05, val loss: 0.08989708870649338\n",
      "Epoch 7954: train loss: 1.930100188474171e-05, val loss: 0.09007588028907776\n",
      "Epoch 7955: train loss: 2.0772387870238163e-05, val loss: 0.09087023138999939\n",
      "Epoch 7956: train loss: 1.5940677258186042e-05, val loss: 0.09017831087112427\n",
      "Epoch 7957: train loss: 1.696815888863057e-05, val loss: 0.08954524248838425\n",
      "Epoch 7958: train loss: 1.2428854461177252e-05, val loss: 0.09063931554555893\n",
      "Epoch 7959: train loss: 1.3040426892985124e-05, val loss: 0.09002166241407394\n",
      "Epoch 7960: train loss: 8.5663195932284e-06, val loss: 0.0889226645231247\n",
      "Epoch 7961: train loss: 9.453172424400691e-06, val loss: 0.08995622396469116\n",
      "Epoch 7962: train loss: 1.017901195154991e-05, val loss: 0.08949942886829376\n",
      "Epoch 7963: train loss: 9.599752957001328e-06, val loss: 0.08910321444272995\n",
      "Epoch 7964: train loss: 7.218993232527282e-06, val loss: 0.08948575705289841\n",
      "Epoch 7965: train loss: 4.6785580707364716e-06, val loss: 0.08986307680606842\n",
      "Epoch 7966: train loss: 7.030096185189905e-06, val loss: 0.08942844718694687\n",
      "Epoch 7967: train loss: 4.907201400783379e-06, val loss: 0.08843987435102463\n",
      "Epoch 7968: train loss: 6.011076038703322e-06, val loss: 0.08932114392518997\n",
      "Epoch 7969: train loss: 4.41459815192502e-06, val loss: 0.08917210251092911\n",
      "Epoch 7970: train loss: 3.6519688819680596e-06, val loss: 0.08845903724431992\n",
      "Epoch 7971: train loss: 4.486405941861449e-06, val loss: 0.08893771469593048\n",
      "Epoch 7972: train loss: 3.578111773094861e-06, val loss: 0.08903949707746506\n",
      "Epoch 7973: train loss: 3.442568640821264e-06, val loss: 0.08937731385231018\n",
      "Epoch 7974: train loss: 3.5390041830396513e-06, val loss: 0.0886421725153923\n",
      "Epoch 7975: train loss: 3.2088480566017097e-06, val loss: 0.08857262134552002\n",
      "Epoch 7976: train loss: 3.141042270726757e-06, val loss: 0.08844731003046036\n",
      "Epoch 7977: train loss: 2.2173867364472244e-06, val loss: 0.08930662274360657\n",
      "Epoch 7978: train loss: 2.343950427530217e-06, val loss: 0.0886145606637001\n",
      "Epoch 7979: train loss: 2.489941834937781e-06, val loss: 0.0884406715631485\n",
      "Epoch 7980: train loss: 2.632664973134524e-06, val loss: 0.08822213858366013\n",
      "Epoch 7981: train loss: 5.540990514418809e-06, val loss: 0.08938027173280716\n",
      "Epoch 7982: train loss: 1.3775206753052771e-05, val loss: 0.08778844028711319\n",
      "Epoch 7983: train loss: 4.182194243185222e-05, val loss: 0.0906853899359703\n",
      "Epoch 7984: train loss: 0.00014660513261333108, val loss: 0.086278535425663\n",
      "Epoch 7985: train loss: 0.0006389233749359846, val loss: 0.09863334149122238\n",
      "Epoch 7986: train loss: 0.001871114014647901, val loss: 0.08310006558895111\n",
      "Epoch 7987: train loss: 0.005105370189994574, val loss: 0.09123367816209793\n",
      "Epoch 7988: train loss: 0.0016932252328842878, val loss: 0.08741883933544159\n",
      "Epoch 7989: train loss: 0.0016822190955281258, val loss: 0.08043929934501648\n",
      "Epoch 7990: train loss: 0.0017337269382551312, val loss: 0.08276289701461792\n",
      "Epoch 7991: train loss: 0.0006607314571738243, val loss: 0.0880177766084671\n",
      "Epoch 7992: train loss: 0.0012000823626294732, val loss: 0.09186631441116333\n",
      "Epoch 7993: train loss: 0.0004868702089879662, val loss: 0.09254500269889832\n",
      "Epoch 7994: train loss: 0.000619014143012464, val loss: 0.08918655663728714\n",
      "Epoch 7995: train loss: 0.0003864807076752186, val loss: 0.08653613179922104\n",
      "Epoch 7996: train loss: 0.0005602578748948872, val loss: 0.08612127602100372\n",
      "Epoch 7997: train loss: 0.0003632909501902759, val loss: 0.0886211171746254\n",
      "Epoch 7998: train loss: 0.0002479644026607275, val loss: 0.09041167050600052\n",
      "Epoch 7999: train loss: 0.0002977368712890893, val loss: 0.08869611471891403\n",
      "Epoch 8000: train loss: 0.00020765785302501172, val loss: 0.08495544642210007\n",
      "Epoch 8001: train loss: 0.00022148645075503737, val loss: 0.08187811821699142\n",
      "Epoch 8002: train loss: 0.00016372553363908082, val loss: 0.0826331153512001\n",
      "Epoch 8003: train loss: 0.00015150164836086333, val loss: 0.08484108746051788\n",
      "Epoch 8004: train loss: 0.00015869039634708315, val loss: 0.0863880068063736\n",
      "Epoch 8005: train loss: 9.163560025626794e-05, val loss: 0.08719271421432495\n",
      "Epoch 8006: train loss: 0.00010618073429213837, val loss: 0.08690669387578964\n",
      "Epoch 8007: train loss: 9.836294339038432e-05, val loss: 0.08702023327350616\n",
      "Epoch 8008: train loss: 6.28028210485354e-05, val loss: 0.08776717633008957\n",
      "Epoch 8009: train loss: 8.02376089268364e-05, val loss: 0.08823452144861221\n",
      "Epoch 8010: train loss: 6.669198774034157e-05, val loss: 0.08785857260227203\n",
      "Epoch 8011: train loss: 4.749689469463192e-05, val loss: 0.08726632595062256\n",
      "Epoch 8012: train loss: 5.460732791107148e-05, val loss: 0.08700575679540634\n",
      "Epoch 8013: train loss: 3.5324133932590485e-05, val loss: 0.08676541596651077\n",
      "Epoch 8014: train loss: 3.5474127798806876e-05, val loss: 0.08648516237735748\n",
      "Epoch 8015: train loss: 4.648923277272843e-05, val loss: 0.08559107035398483\n",
      "Epoch 8016: train loss: 2.759090784820728e-05, val loss: 0.08485231548547745\n",
      "Epoch 8017: train loss: 2.3615066311322153e-05, val loss: 0.08484693616628647\n",
      "Epoch 8018: train loss: 3.131953053525649e-05, val loss: 0.08558358997106552\n",
      "Epoch 8019: train loss: 1.9586741473176517e-05, val loss: 0.08682683110237122\n",
      "Epoch 8020: train loss: 1.9775434338953346e-05, val loss: 0.08760800212621689\n",
      "Epoch 8021: train loss: 2.414327536826022e-05, val loss: 0.08795888721942902\n",
      "Epoch 8022: train loss: 1.2543823686428368e-05, val loss: 0.08790995925664902\n",
      "Epoch 8023: train loss: 1.2298883120820392e-05, val loss: 0.08772451430559158\n",
      "Epoch 8024: train loss: 1.5589752365485765e-05, val loss: 0.08737790584564209\n",
      "Epoch 8025: train loss: 1.1304236977593973e-05, val loss: 0.0869627445936203\n",
      "Epoch 8026: train loss: 1.0615342944220174e-05, val loss: 0.08661847561597824\n",
      "Epoch 8027: train loss: 8.394184078497346e-06, val loss: 0.08623950928449631\n",
      "Epoch 8028: train loss: 7.303763140953379e-06, val loss: 0.08615699410438538\n",
      "Epoch 8029: train loss: 8.300342415168416e-06, val loss: 0.08640456199645996\n",
      "Epoch 8030: train loss: 5.187218903301982e-06, val loss: 0.0866231694817543\n",
      "Epoch 8031: train loss: 5.776232228527078e-06, val loss: 0.08673065900802612\n",
      "Epoch 8032: train loss: 6.560756901308196e-06, val loss: 0.08641161769628525\n",
      "Epoch 8033: train loss: 3.820087385975057e-06, val loss: 0.08593232929706573\n",
      "Epoch 8034: train loss: 4.428507054399233e-06, val loss: 0.08572132885456085\n",
      "Epoch 8035: train loss: 4.033372988487827e-06, val loss: 0.08588355034589767\n",
      "Epoch 8036: train loss: 2.8497313451225637e-06, val loss: 0.08614372462034225\n",
      "Epoch 8037: train loss: 3.863109213853022e-06, val loss: 0.08595845848321915\n",
      "Epoch 8038: train loss: 3.1010533803055296e-06, val loss: 0.08550386875867844\n",
      "Epoch 8039: train loss: 2.8884724088129587e-06, val loss: 0.08509258925914764\n",
      "Epoch 8040: train loss: 2.2835972686152672e-06, val loss: 0.08504150062799454\n",
      "Epoch 8041: train loss: 1.6691348037056741e-06, val loss: 0.0850982815027237\n",
      "Epoch 8042: train loss: 2.504001258785138e-06, val loss: 0.08492087572813034\n",
      "Epoch 8043: train loss: 1.4780849824092002e-06, val loss: 0.08466441929340363\n",
      "Epoch 8044: train loss: 1.8298885606782278e-06, val loss: 0.0844719186425209\n",
      "Epoch 8045: train loss: 1.540631501484313e-06, val loss: 0.08459066599607468\n",
      "Epoch 8046: train loss: 1.1412928415666102e-06, val loss: 0.08445163816213608\n",
      "Epoch 8047: train loss: 1.2442786783140036e-06, val loss: 0.08403234928846359\n",
      "Epoch 8048: train loss: 8.313518264913e-07, val loss: 0.08373954147100449\n",
      "Epoch 8049: train loss: 1.0579558420431567e-06, val loss: 0.08371304720640182\n",
      "Epoch 8050: train loss: 8.766892278799787e-07, val loss: 0.08368495851755142\n",
      "Epoch 8051: train loss: 1.0170723498958978e-06, val loss: 0.08335789293050766\n",
      "Epoch 8052: train loss: 4.881655399913143e-07, val loss: 0.08308673650026321\n",
      "Epoch 8053: train loss: 7.822271186341823e-07, val loss: 0.08308324962854385\n",
      "Epoch 8054: train loss: 4.6034486445023504e-07, val loss: 0.08315890282392502\n",
      "Epoch 8055: train loss: 5.70702411550883e-07, val loss: 0.08297083526849747\n",
      "Epoch 8056: train loss: 4.572260650093085e-07, val loss: 0.08262722939252853\n",
      "Epoch 8057: train loss: 5.24421750469628e-07, val loss: 0.08253247290849686\n",
      "Epoch 8058: train loss: 4.267696738224913e-07, val loss: 0.08255007863044739\n",
      "Epoch 8059: train loss: 3.8024433024475e-07, val loss: 0.08245790749788284\n",
      "Epoch 8060: train loss: 2.9704600024160754e-07, val loss: 0.08221643418073654\n",
      "Epoch 8061: train loss: 2.9099251719344466e-07, val loss: 0.08202851563692093\n",
      "Epoch 8062: train loss: 3.1179314419205184e-07, val loss: 0.08195564895868301\n",
      "Epoch 8063: train loss: 2.392010287621815e-07, val loss: 0.08178626745939255\n",
      "Epoch 8064: train loss: 3.4096888157364447e-07, val loss: 0.08168759942054749\n",
      "Epoch 8065: train loss: 4.827863335776783e-07, val loss: 0.08157137781381607\n",
      "Epoch 8066: train loss: 1.3285676914165379e-06, val loss: 0.08164159208536148\n",
      "Epoch 8067: train loss: 5.607005277852295e-06, val loss: 0.08117427676916122\n",
      "Epoch 8068: train loss: 1.813418020901736e-05, val loss: 0.08129582554101944\n",
      "Epoch 8069: train loss: 4.3898512558371294e-06, val loss: 0.08146589994430542\n",
      "Epoch 8070: train loss: 7.740577530057635e-06, val loss: 0.0813557505607605\n",
      "Epoch 8071: train loss: 4.974342573405011e-06, val loss: 0.08087661117315292\n",
      "Epoch 8072: train loss: 5.114035502629122e-06, val loss: 0.0809134840965271\n",
      "Epoch 8073: train loss: 4.006134531664429e-06, val loss: 0.081178680062294\n",
      "Epoch 8074: train loss: 4.288816398911877e-06, val loss: 0.08089768141508102\n",
      "Epoch 8075: train loss: 3.3894523312483216e-06, val loss: 0.08072882145643234\n",
      "Epoch 8076: train loss: 3.375047754161642e-06, val loss: 0.08061672002077103\n",
      "Epoch 8077: train loss: 2.9555017135862727e-06, val loss: 0.08037477731704712\n",
      "Epoch 8078: train loss: 2.8902436497446615e-06, val loss: 0.08031969517469406\n",
      "Epoch 8079: train loss: 2.3172044620878296e-06, val loss: 0.07959593832492828\n",
      "Epoch 8080: train loss: 2.5105666736635612e-06, val loss: 0.0794002115726471\n",
      "Epoch 8081: train loss: 1.8456316865922417e-06, val loss: 0.07950824499130249\n",
      "Epoch 8082: train loss: 2.0482925719989e-06, val loss: 0.07930313795804977\n",
      "Epoch 8083: train loss: 1.4644235761807067e-06, val loss: 0.07862124592065811\n",
      "Epoch 8084: train loss: 1.9822994090645807e-06, val loss: 0.07864488661289215\n",
      "Epoch 8085: train loss: 8.809880682747462e-07, val loss: 0.07884585857391357\n",
      "Epoch 8086: train loss: 1.9857939150824677e-06, val loss: 0.07826629281044006\n",
      "Epoch 8087: train loss: 7.686114145144529e-07, val loss: 0.0780492052435875\n",
      "Epoch 8088: train loss: 1.1952020031458233e-06, val loss: 0.07823691517114639\n",
      "Epoch 8089: train loss: 1.1649049156403635e-06, val loss: 0.07800371944904327\n",
      "Epoch 8090: train loss: 6.745437985955505e-07, val loss: 0.07774096727371216\n",
      "Epoch 8091: train loss: 1.1350176691848901e-06, val loss: 0.07764983177185059\n",
      "Epoch 8092: train loss: 6.892285000503762e-07, val loss: 0.07769545167684555\n",
      "Epoch 8093: train loss: 8.709083658686723e-07, val loss: 0.07744883000850677\n",
      "Epoch 8094: train loss: 6.858183496660786e-07, val loss: 0.07739515602588654\n",
      "Epoch 8095: train loss: 7.537586839134747e-07, val loss: 0.07731473445892334\n",
      "Epoch 8096: train loss: 6.103325063122611e-07, val loss: 0.07730487734079361\n",
      "Epoch 8097: train loss: 1.4071192708797753e-06, val loss: 0.07738689333200455\n",
      "Epoch 8098: train loss: 2.7522855816641822e-06, val loss: 0.07714387029409409\n",
      "Epoch 8099: train loss: 2.701915946090594e-06, val loss: 0.0767727643251419\n",
      "Epoch 8100: train loss: 4.303817149775568e-06, val loss: 0.07766272127628326\n",
      "Epoch 8101: train loss: 1.0111763003806118e-05, val loss: 0.07622573524713516\n",
      "Epoch 8102: train loss: 1.744080873322673e-05, val loss: 0.07801077514886856\n",
      "Epoch 8103: train loss: 2.1407871827250347e-05, val loss: 0.07696948200464249\n",
      "Epoch 8104: train loss: 2.2522397557622753e-05, val loss: 0.0769272968173027\n",
      "Epoch 8105: train loss: 3.393401493667625e-05, val loss: 0.07855308800935745\n",
      "Epoch 8106: train loss: 6.359403778333217e-05, val loss: 0.07515277713537216\n",
      "Epoch 8107: train loss: 0.00015392608474940062, val loss: 0.08087500184774399\n",
      "Epoch 8108: train loss: 0.00032873693271540105, val loss: 0.07594072073698044\n",
      "Epoch 8109: train loss: 0.000543751462828368, val loss: 0.08193427324295044\n",
      "Epoch 8110: train loss: 0.0006209930870682001, val loss: 0.07795846462249756\n",
      "Epoch 8111: train loss: 0.0004320770676713437, val loss: 0.08103110641241074\n",
      "Epoch 8112: train loss: 0.0002825790143106133, val loss: 0.08421750366687775\n",
      "Epoch 8113: train loss: 0.0002529471821617335, val loss: 0.08304523676633835\n",
      "Epoch 8114: train loss: 0.0002548026095610112, val loss: 0.0825856477022171\n",
      "Epoch 8115: train loss: 0.00011240999447181821, val loss: 0.08676396310329437\n",
      "Epoch 8116: train loss: 5.160613727639429e-05, val loss: 0.08875270187854767\n",
      "Epoch 8117: train loss: 0.00014158569683786482, val loss: 0.08772685378789902\n",
      "Epoch 8118: train loss: 0.00012377710663713515, val loss: 0.08926980197429657\n",
      "Epoch 8119: train loss: 0.00013092860172037035, val loss: 0.08872711658477783\n",
      "Epoch 8120: train loss: 0.00012501094897743315, val loss: 0.08917982131242752\n",
      "Epoch 8121: train loss: 5.2702926041092724e-05, val loss: 0.08958281576633453\n",
      "Epoch 8122: train loss: 4.4112861360190436e-05, val loss: 0.09000935405492783\n",
      "Epoch 8123: train loss: 5.584796963375993e-05, val loss: 0.09062103927135468\n",
      "Epoch 8124: train loss: 5.1391642045928165e-05, val loss: 0.08995314687490463\n",
      "Epoch 8125: train loss: 8.847467688610777e-05, val loss: 0.09059201925992966\n",
      "Epoch 8126: train loss: 4.485085082706064e-05, val loss: 0.09049208462238312\n",
      "Epoch 8127: train loss: 3.185182868037373e-05, val loss: 0.08993857353925705\n",
      "Epoch 8128: train loss: 2.0575256712618284e-05, val loss: 0.08953830599784851\n",
      "Epoch 8129: train loss: 2.5250501494156197e-05, val loss: 0.09018198400735855\n",
      "Epoch 8130: train loss: 3.927543730242178e-05, val loss: 0.09147483110427856\n",
      "Epoch 8131: train loss: 3.2074454793473706e-05, val loss: 0.0901464968919754\n",
      "Epoch 8132: train loss: 2.8090244086342864e-05, val loss: 0.08939246088266373\n",
      "Epoch 8133: train loss: 1.3571103409049101e-05, val loss: 0.09002715349197388\n",
      "Epoch 8134: train loss: 9.990892067435198e-06, val loss: 0.0903773382306099\n",
      "Epoch 8135: train loss: 1.695908031251747e-05, val loss: 0.0898950919508934\n",
      "Epoch 8136: train loss: 2.045378460024949e-05, val loss: 0.08939319849014282\n",
      "Epoch 8137: train loss: 1.9829883967759088e-05, val loss: 0.09011697769165039\n",
      "Epoch 8138: train loss: 1.1568437003006693e-05, val loss: 0.08983296155929565\n",
      "Epoch 8139: train loss: 7.784392437315546e-06, val loss: 0.08949929475784302\n",
      "Epoch 8140: train loss: 6.4835949160624295e-06, val loss: 0.08973782509565353\n",
      "Epoch 8141: train loss: 1.0681847015803214e-05, val loss: 0.08945196866989136\n",
      "Epoch 8142: train loss: 1.0859894246095791e-05, val loss: 0.08952870219945908\n",
      "Epoch 8143: train loss: 1.1142030416522175e-05, val loss: 0.08911028504371643\n",
      "Epoch 8144: train loss: 6.6471211539465e-06, val loss: 0.08889272063970566\n",
      "Epoch 8145: train loss: 3.482544343569316e-06, val loss: 0.08954241126775742\n",
      "Epoch 8146: train loss: 4.459340743778739e-06, val loss: 0.08991027623414993\n",
      "Epoch 8147: train loss: 6.54167070024414e-06, val loss: 0.08896418660879135\n",
      "Epoch 8148: train loss: 6.055189714970766e-06, val loss: 0.0883062556385994\n",
      "Epoch 8149: train loss: 6.092530838941457e-06, val loss: 0.08912036567926407\n",
      "Epoch 8150: train loss: 4.216667548462283e-06, val loss: 0.08897048979997635\n",
      "Epoch 8151: train loss: 2.4490459509252105e-06, val loss: 0.08884190768003464\n",
      "Epoch 8152: train loss: 1.9494755179039203e-06, val loss: 0.0889328345656395\n",
      "Epoch 8153: train loss: 3.6940166410204256e-06, val loss: 0.08834870904684067\n",
      "Epoch 8154: train loss: 4.688861736212857e-06, val loss: 0.08857806771993637\n",
      "Epoch 8155: train loss: 3.753099463210674e-06, val loss: 0.0881018415093422\n",
      "Epoch 8156: train loss: 2.0492218482104363e-06, val loss: 0.08801769465208054\n",
      "Epoch 8157: train loss: 1.1142443554490455e-06, val loss: 0.08836430311203003\n",
      "Epoch 8158: train loss: 2.128309461113531e-06, val loss: 0.08788628876209259\n",
      "Epoch 8159: train loss: 3.0037610940780723e-06, val loss: 0.08802586793899536\n",
      "Epoch 8160: train loss: 2.8892472982988693e-06, val loss: 0.08751434832811356\n",
      "Epoch 8161: train loss: 2.7497001156007173e-06, val loss: 0.0878363624215126\n",
      "Epoch 8162: train loss: 2.4760036012594355e-06, val loss: 0.08746547251939774\n",
      "Epoch 8163: train loss: 2.718515816013678e-06, val loss: 0.08753702044487\n",
      "Epoch 8164: train loss: 3.1157301236817148e-06, val loss: 0.08737104386091232\n",
      "Epoch 8165: train loss: 5.663505817210535e-06, val loss: 0.08764740079641342\n",
      "Epoch 8166: train loss: 1.3439441318041645e-05, val loss: 0.08695745468139648\n",
      "Epoch 8167: train loss: 3.43266328854952e-05, val loss: 0.08728644996881485\n",
      "Epoch 8168: train loss: 0.00010037492029368877, val loss: 0.08537527918815613\n",
      "Epoch 8169: train loss: 0.00025032326811924577, val loss: 0.08684244751930237\n",
      "Epoch 8170: train loss: 0.0005761591019108891, val loss: 0.08323285728693008\n",
      "Epoch 8171: train loss: 0.0008126129978336394, val loss: 0.08785157650709152\n",
      "Epoch 8172: train loss: 0.0011091581545770168, val loss: 0.08629436790943146\n",
      "Epoch 8173: train loss: 0.0007252125651575625, val loss: 0.0848173052072525\n",
      "Epoch 8174: train loss: 0.0005818983190692961, val loss: 0.08611003309488297\n",
      "Epoch 8175: train loss: 0.0003291071916464716, val loss: 0.08958116918802261\n",
      "Epoch 8176: train loss: 0.00039463426219299436, val loss: 0.08924221247434616\n",
      "Epoch 8177: train loss: 0.00023807089019101113, val loss: 0.08670540899038315\n",
      "Epoch 8178: train loss: 0.00022482252097688615, val loss: 0.08587981760501862\n",
      "Epoch 8179: train loss: 0.0002030364121310413, val loss: 0.08799748867750168\n",
      "Epoch 8180: train loss: 0.0001636991510167718, val loss: 0.09125412255525589\n",
      "Epoch 8181: train loss: 0.00012004577729385346, val loss: 0.09162401407957077\n",
      "Epoch 8182: train loss: 0.00014570004714187235, val loss: 0.08958520740270615\n",
      "Epoch 8183: train loss: 9.064856567420065e-05, val loss: 0.08816275745630264\n",
      "Epoch 8184: train loss: 8.899931708583608e-05, val loss: 0.0879625752568245\n",
      "Epoch 8185: train loss: 9.387383761350065e-05, val loss: 0.08900780975818634\n",
      "Epoch 8186: train loss: 5.996198524371721e-05, val loss: 0.0905548632144928\n",
      "Epoch 8187: train loss: 6.409425259334967e-05, val loss: 0.0921793207526207\n",
      "Epoch 8188: train loss: 5.725329538108781e-05, val loss: 0.09252464026212692\n",
      "Epoch 8189: train loss: 4.320047446526587e-05, val loss: 0.09073402732610703\n",
      "Epoch 8190: train loss: 4.576006904244423e-05, val loss: 0.08866158872842789\n",
      "Epoch 8191: train loss: 4.07633779104799e-05, val loss: 0.08836402744054794\n",
      "Epoch 8192: train loss: 2.763469638011884e-05, val loss: 0.0899847075343132\n",
      "Epoch 8193: train loss: 3.738250597962178e-05, val loss: 0.09146822988986969\n",
      "Epoch 8194: train loss: 2.294594196428079e-05, val loss: 0.09167131036520004\n",
      "Epoch 8195: train loss: 2.4892757210182026e-05, val loss: 0.09128164499998093\n",
      "Epoch 8196: train loss: 2.369784670008812e-05, val loss: 0.09107259660959244\n",
      "Epoch 8197: train loss: 1.7310043403995223e-05, val loss: 0.09102705121040344\n",
      "Epoch 8198: train loss: 1.8244851162307896e-05, val loss: 0.09089905023574829\n",
      "Epoch 8199: train loss: 1.8472639567335136e-05, val loss: 0.09106140583753586\n",
      "Epoch 8200: train loss: 9.879751814878546e-06, val loss: 0.09166687726974487\n",
      "Epoch 8201: train loss: 1.5456227629329078e-05, val loss: 0.09231191128492355\n",
      "Epoch 8202: train loss: 1.2190543202450499e-05, val loss: 0.09243740886449814\n",
      "Epoch 8203: train loss: 8.109938789857551e-06, val loss: 0.09200620651245117\n",
      "Epoch 8204: train loss: 1.1556008757906966e-05, val loss: 0.09174853563308716\n",
      "Epoch 8205: train loss: 7.933212145871948e-06, val loss: 0.09192156046628952\n",
      "Epoch 8206: train loss: 7.201477728813188e-06, val loss: 0.09223954379558563\n",
      "Epoch 8207: train loss: 8.164533028320875e-06, val loss: 0.09238090366125107\n",
      "Epoch 8208: train loss: 5.414201496023452e-06, val loss: 0.09257590025663376\n",
      "Epoch 8209: train loss: 6.106647560955025e-06, val loss: 0.09276837855577469\n",
      "Epoch 8210: train loss: 5.38010726813809e-06, val loss: 0.09246327728033066\n",
      "Epoch 8211: train loss: 4.9018631216313224e-06, val loss: 0.09215357154607773\n",
      "Epoch 8212: train loss: 3.6958992950530956e-06, val loss: 0.09261828660964966\n",
      "Epoch 8213: train loss: 4.352915766503429e-06, val loss: 0.09327052533626556\n",
      "Epoch 8214: train loss: 3.5442774333205307e-06, val loss: 0.09288471937179565\n",
      "Epoch 8215: train loss: 2.570623792053084e-06, val loss: 0.09223591536283493\n",
      "Epoch 8216: train loss: 3.6627016015700065e-06, val loss: 0.09269804507493973\n",
      "Epoch 8217: train loss: 2.0220836631779093e-06, val loss: 0.0934222936630249\n",
      "Epoch 8218: train loss: 2.7509843221196206e-06, val loss: 0.09312587976455688\n",
      "Epoch 8219: train loss: 2.0408756427059416e-06, val loss: 0.09243125468492508\n",
      "Epoch 8220: train loss: 1.9379381228645798e-06, val loss: 0.09255102276802063\n",
      "Epoch 8221: train loss: 2.0666841464844765e-06, val loss: 0.09307149797677994\n",
      "Epoch 8222: train loss: 1.306749823015707e-06, val loss: 0.09314819425344467\n",
      "Epoch 8223: train loss: 1.7339765463475487e-06, val loss: 0.0928068682551384\n",
      "Epoch 8224: train loss: 1.2907164546049898e-06, val loss: 0.09265146404504776\n",
      "Epoch 8225: train loss: 1.2702078038273612e-06, val loss: 0.09285365045070648\n",
      "Epoch 8226: train loss: 1.1459671895863721e-06, val loss: 0.09297124296426773\n",
      "Epoch 8227: train loss: 9.862188790066284e-07, val loss: 0.09296019375324249\n",
      "Epoch 8228: train loss: 9.90747594187269e-07, val loss: 0.09276203066110611\n",
      "Epoch 8229: train loss: 8.056108526943717e-07, val loss: 0.09259919077157974\n",
      "Epoch 8230: train loss: 8.205307153730246e-07, val loss: 0.09288562089204788\n",
      "Epoch 8231: train loss: 7.144080313992163e-07, val loss: 0.09298300743103027\n",
      "Epoch 8232: train loss: 5.973951715532166e-07, val loss: 0.09264951199293137\n",
      "Epoch 8233: train loss: 7.260863412739127e-07, val loss: 0.09276913851499557\n",
      "Epoch 8234: train loss: 4.481313453652547e-07, val loss: 0.09300132840871811\n",
      "Epoch 8235: train loss: 6.646183123848459e-07, val loss: 0.09268935024738312\n",
      "Epoch 8236: train loss: 3.335428289119591e-07, val loss: 0.09259995073080063\n",
      "Epoch 8237: train loss: 5.559319902204152e-07, val loss: 0.093058742582798\n",
      "Epoch 8238: train loss: 3.181382339789707e-07, val loss: 0.09289544820785522\n",
      "Epoch 8239: train loss: 4.2784867559930717e-07, val loss: 0.09252476692199707\n",
      "Epoch 8240: train loss: 3.2185403142648283e-07, val loss: 0.09280974417924881\n",
      "Epoch 8241: train loss: 3.002931805440312e-07, val loss: 0.09304090589284897\n",
      "Epoch 8242: train loss: 3.501242531456228e-07, val loss: 0.09269517660140991\n",
      "Epoch 8243: train loss: 2.598671073883452e-07, val loss: 0.09255947917699814\n",
      "Epoch 8244: train loss: 4.122988457311294e-07, val loss: 0.09265897423028946\n",
      "Epoch 8245: train loss: 8.626511203146947e-07, val loss: 0.09310312569141388\n",
      "Epoch 8246: train loss: 3.0355424769368256e-06, val loss: 0.09196165949106216\n",
      "Epoch 8247: train loss: 1.4716987607243937e-05, val loss: 0.0940493643283844\n",
      "Epoch 8248: train loss: 7.768301293253899e-05, val loss: 0.09127181768417358\n",
      "Epoch 8249: train loss: 0.00037958804750815034, val loss: 0.10316536575555801\n",
      "Epoch 8250: train loss: 0.0015856276731938124, val loss: 0.10336489975452423\n",
      "Epoch 8251: train loss: 0.0044403886422514915, val loss: 0.11618844419717789\n",
      "Epoch 8252: train loss: 0.0031861166935414076, val loss: 0.10115545243024826\n",
      "Epoch 8253: train loss: 0.0012992830015718937, val loss: 0.0968291163444519\n",
      "Epoch 8254: train loss: 0.0023079318925738335, val loss: 0.1081729531288147\n",
      "Epoch 8255: train loss: 0.0009847658220678568, val loss: 0.11057551205158234\n",
      "Epoch 8256: train loss: 0.0010908173862844706, val loss: 0.10080713033676147\n",
      "Epoch 8257: train loss: 0.0006438767304643989, val loss: 0.09826218336820602\n",
      "Epoch 8258: train loss: 0.0008337291656062007, val loss: 0.10169269144535065\n",
      "Epoch 8259: train loss: 0.0005248964880593121, val loss: 0.10405608266592026\n",
      "Epoch 8260: train loss: 0.0006522316834889352, val loss: 0.09851741790771484\n",
      "Epoch 8261: train loss: 0.00034399383002892137, val loss: 0.09479931741952896\n",
      "Epoch 8262: train loss: 0.00031810704967938364, val loss: 0.09660711884498596\n",
      "Epoch 8263: train loss: 0.00026553880888968706, val loss: 0.09934328496456146\n",
      "Epoch 8264: train loss: 0.0003243287210352719, val loss: 0.09803814440965652\n",
      "Epoch 8265: train loss: 0.0002944464504253119, val loss: 0.09239036589860916\n",
      "Epoch 8266: train loss: 0.0001453810400562361, val loss: 0.08924204856157303\n",
      "Epoch 8267: train loss: 0.000186759396456182, val loss: 0.09145914763212204\n",
      "Epoch 8268: train loss: 0.00014336174353957176, val loss: 0.09599412977695465\n",
      "Epoch 8269: train loss: 0.00015061491285450757, val loss: 0.09771483391523361\n",
      "Epoch 8270: train loss: 0.0001601266849320382, val loss: 0.09465727210044861\n",
      "Epoch 8271: train loss: 8.854325278662145e-05, val loss: 0.09126629680395126\n",
      "Epoch 8272: train loss: 0.00010696244135033339, val loss: 0.09094540029764175\n",
      "Epoch 8273: train loss: 7.208164606709033e-05, val loss: 0.09287459403276443\n",
      "Epoch 8274: train loss: 7.067537808325142e-05, val loss: 0.09452221542596817\n",
      "Epoch 8275: train loss: 0.00010906656825682148, val loss: 0.09350883960723877\n",
      "Epoch 8276: train loss: 4.5639237214345485e-05, val loss: 0.09179296344518661\n",
      "Epoch 8277: train loss: 3.836802352452651e-05, val loss: 0.09164875000715256\n",
      "Epoch 8278: train loss: 6.523948104586452e-05, val loss: 0.09298241138458252\n",
      "Epoch 8279: train loss: 4.390412141219713e-05, val loss: 0.09433973580598831\n",
      "Epoch 8280: train loss: 4.5567896449938416e-05, val loss: 0.09359478950500488\n",
      "Epoch 8281: train loss: 3.391935024410486e-05, val loss: 0.09174903482198715\n",
      "Epoch 8282: train loss: 2.1302827008184977e-05, val loss: 0.09088703244924545\n",
      "Epoch 8283: train loss: 3.4656703064683825e-05, val loss: 0.0918877124786377\n",
      "Epoch 8284: train loss: 2.9452239687088877e-05, val loss: 0.09354706108570099\n",
      "Epoch 8285: train loss: 2.79126652458217e-05, val loss: 0.0936022400856018\n",
      "Epoch 8286: train loss: 1.9836628780467436e-05, val loss: 0.09211836755275726\n",
      "Epoch 8287: train loss: 1.0398194717708975e-05, val loss: 0.09086301177740097\n",
      "Epoch 8288: train loss: 2.297606442880351e-05, val loss: 0.09088163077831268\n",
      "Epoch 8289: train loss: 1.777889337972738e-05, val loss: 0.09206165373325348\n",
      "Epoch 8290: train loss: 1.0524217941565439e-05, val loss: 0.09275620430707932\n",
      "Epoch 8291: train loss: 1.4361118701344822e-05, val loss: 0.09234144538640976\n",
      "Epoch 8292: train loss: 8.342664841620717e-06, val loss: 0.0915614515542984\n",
      "Epoch 8293: train loss: 1.123044694395503e-05, val loss: 0.09128151834011078\n",
      "Epoch 8294: train loss: 1.1471466677903663e-05, val loss: 0.09175681322813034\n",
      "Epoch 8295: train loss: 5.591147782979533e-06, val loss: 0.0920591652393341\n",
      "Epoch 8296: train loss: 8.865969903126825e-06, val loss: 0.0917569175362587\n",
      "Epoch 8297: train loss: 5.589013198914472e-06, val loss: 0.09134437888860703\n",
      "Epoch 8298: train loss: 5.634654371533543e-06, val loss: 0.09139963239431381\n",
      "Epoch 8299: train loss: 7.838028068363201e-06, val loss: 0.09205329418182373\n",
      "Epoch 8300: train loss: 3.537140855769394e-06, val loss: 0.09231269359588623\n",
      "Epoch 8301: train loss: 4.421931862452766e-06, val loss: 0.09170126169919968\n",
      "Epoch 8302: train loss: 3.43278134096181e-06, val loss: 0.09079843759536743\n",
      "Epoch 8303: train loss: 3.888727860612562e-06, val loss: 0.09042956680059433\n",
      "Epoch 8304: train loss: 4.842022008233471e-06, val loss: 0.09096603840589523\n",
      "Epoch 8305: train loss: 1.8464223785485956e-06, val loss: 0.09158065170049667\n",
      "Epoch 8306: train loss: 2.877290853575687e-06, val loss: 0.09154041111469269\n",
      "Epoch 8307: train loss: 2.2430501758208266e-06, val loss: 0.09106006473302841\n",
      "Epoch 8308: train loss: 2.292179487994872e-06, val loss: 0.09072320908308029\n",
      "Epoch 8309: train loss: 2.5819178972597e-06, val loss: 0.09088336676359177\n",
      "Epoch 8310: train loss: 1.5150410490605282e-06, val loss: 0.09108030796051025\n",
      "Epoch 8311: train loss: 1.7681654753687326e-06, val loss: 0.09101086854934692\n",
      "Epoch 8312: train loss: 1.0689756209103507e-06, val loss: 0.09083914011716843\n",
      "Epoch 8313: train loss: 1.807704393286258e-06, val loss: 0.0907573327422142\n",
      "Epoch 8314: train loss: 1.2464112160159857e-06, val loss: 0.09077467769384384\n",
      "Epoch 8315: train loss: 1.2106469284844934e-06, val loss: 0.09060407429933548\n",
      "Epoch 8316: train loss: 8.886161140253535e-07, val loss: 0.09037481248378754\n",
      "Epoch 8317: train loss: 6.932365295142517e-07, val loss: 0.09038278460502625\n",
      "Epoch 8318: train loss: 1.1042548067052849e-06, val loss: 0.09056118875741959\n",
      "Epoch 8319: train loss: 7.129720529519545e-07, val loss: 0.09066669642925262\n",
      "Epoch 8320: train loss: 8.698290230313432e-07, val loss: 0.0904497280716896\n",
      "Epoch 8321: train loss: 4.103823982859467e-07, val loss: 0.09019715338945389\n",
      "Epoch 8322: train loss: 6.89803982822923e-07, val loss: 0.09015251696109772\n",
      "Epoch 8323: train loss: 4.0820791014084534e-07, val loss: 0.09018611907958984\n",
      "Epoch 8324: train loss: 6.900428388689761e-07, val loss: 0.09015214443206787\n",
      "Epoch 8325: train loss: 3.990313075519225e-07, val loss: 0.09004251658916473\n",
      "Epoch 8326: train loss: 4.6147991383804765e-07, val loss: 0.09001760929822922\n",
      "Epoch 8327: train loss: 3.12662308488143e-07, val loss: 0.08996938169002533\n",
      "Epoch 8328: train loss: 3.2898347512855253e-07, val loss: 0.08980808407068253\n",
      "Epoch 8329: train loss: 4.096401937658811e-07, val loss: 0.08969157934188843\n",
      "Epoch 8330: train loss: 2.5987495178014797e-07, val loss: 0.0896134078502655\n",
      "Epoch 8331: train loss: 3.4711743523985206e-07, val loss: 0.08959268778562546\n",
      "Epoch 8332: train loss: 1.4357075883708603e-07, val loss: 0.08960255980491638\n",
      "Epoch 8333: train loss: 2.6088153504133516e-07, val loss: 0.08955466747283936\n",
      "Epoch 8334: train loss: 2.394013165485376e-07, val loss: 0.08938594907522202\n",
      "Epoch 8335: train loss: 1.508844320596836e-07, val loss: 0.08920198678970337\n",
      "Epoch 8336: train loss: 2.8197149504194385e-07, val loss: 0.08919601887464523\n",
      "Epoch 8337: train loss: 1.1570529068194446e-07, val loss: 0.08919571340084076\n",
      "Epoch 8338: train loss: 1.1830735502371681e-07, val loss: 0.0890674889087677\n",
      "Epoch 8339: train loss: 1.9969380105067103e-07, val loss: 0.0889330729842186\n",
      "Epoch 8340: train loss: 1.432334642004207e-07, val loss: 0.08884372562170029\n",
      "Epoch 8341: train loss: 1.405664562525999e-07, val loss: 0.08882605284452438\n",
      "Epoch 8342: train loss: 1.1098446606183643e-07, val loss: 0.08875862509012222\n",
      "Epoch 8343: train loss: 1.8780536947815563e-07, val loss: 0.08860523253679276\n",
      "Epoch 8344: train loss: 2.0417350299339887e-07, val loss: 0.08849742263555527\n",
      "Epoch 8345: train loss: 3.1341542694462987e-07, val loss: 0.08838418871164322\n",
      "Epoch 8346: train loss: 5.327847816261055e-07, val loss: 0.08837258815765381\n",
      "Epoch 8347: train loss: 6.068477205189993e-07, val loss: 0.08812994509935379\n",
      "Epoch 8348: train loss: 5.273472538647184e-07, val loss: 0.08807822316884995\n",
      "Epoch 8349: train loss: 3.0282257057479e-07, val loss: 0.08792775124311447\n",
      "Epoch 8350: train loss: 1.9024992070626467e-07, val loss: 0.08779648691415787\n",
      "Epoch 8351: train loss: 3.945324635878933e-07, val loss: 0.08781146258115768\n",
      "Epoch 8352: train loss: 4.291672439649119e-07, val loss: 0.08755417168140411\n",
      "Epoch 8353: train loss: 4.290885442514991e-07, val loss: 0.08754367381334305\n",
      "Epoch 8354: train loss: 1.3151332041161368e-06, val loss: 0.08769511431455612\n",
      "Epoch 8355: train loss: 5.25697942066472e-06, val loss: 0.0872952789068222\n",
      "Epoch 8356: train loss: 1.1914556125702802e-05, val loss: 0.08723475784063339\n",
      "Epoch 8357: train loss: 1.3367704013944604e-05, val loss: 0.08711177110671997\n",
      "Epoch 8358: train loss: 7.14181260264013e-06, val loss: 0.08740078657865524\n",
      "Epoch 8359: train loss: 4.611548774846597e-06, val loss: 0.08711396157741547\n",
      "Epoch 8360: train loss: 5.823837454954628e-06, val loss: 0.08696454763412476\n",
      "Epoch 8361: train loss: 6.040295829734532e-06, val loss: 0.0874294564127922\n",
      "Epoch 8362: train loss: 3.5431896776572103e-06, val loss: 0.08712144196033478\n",
      "Epoch 8363: train loss: 3.2253810786642134e-06, val loss: 0.08718889951705933\n",
      "Epoch 8364: train loss: 6.3420829974347726e-06, val loss: 0.08738970756530762\n",
      "Epoch 8365: train loss: 3.721725533978315e-06, val loss: 0.08701740950345993\n",
      "Epoch 8366: train loss: 2.867483999580145e-06, val loss: 0.08696021884679794\n",
      "Epoch 8367: train loss: 4.147575054957997e-06, val loss: 0.08743889629840851\n",
      "Epoch 8368: train loss: 5.107644938107114e-06, val loss: 0.08691487461328506\n",
      "Epoch 8369: train loss: 6.621901775361039e-06, val loss: 0.08730562031269073\n",
      "Epoch 8370: train loss: 1.1874866686412133e-05, val loss: 0.08737466484308243\n",
      "Epoch 8371: train loss: 3.335365909151733e-05, val loss: 0.08698075264692307\n",
      "Epoch 8372: train loss: 9.444414172321558e-05, val loss: 0.08679851144552231\n",
      "Epoch 8373: train loss: 0.0002578187850303948, val loss: 0.08723204582929611\n",
      "Epoch 8374: train loss: 0.00048701392370276153, val loss: 0.08665122836828232\n",
      "Epoch 8375: train loss: 0.0006933886324986815, val loss: 0.09001391381025314\n",
      "Epoch 8376: train loss: 0.0005819064099341631, val loss: 0.0867009162902832\n",
      "Epoch 8377: train loss: 0.0005818594945594668, val loss: 0.09257727861404419\n",
      "Epoch 8378: train loss: 0.00017699481395538896, val loss: 0.09197075664997101\n",
      "Epoch 8379: train loss: 0.00018430421187076718, val loss: 0.08943771570920944\n",
      "Epoch 8380: train loss: 0.0002994681126438081, val loss: 0.09442784637212753\n",
      "Epoch 8381: train loss: 0.0001835732109611854, val loss: 0.0944732055068016\n",
      "Epoch 8382: train loss: 0.00010237946844426915, val loss: 0.09311199188232422\n",
      "Epoch 8383: train loss: 0.0001767618377925828, val loss: 0.09527870267629623\n",
      "Epoch 8384: train loss: 0.00011229686060687527, val loss: 0.09635462611913681\n",
      "Epoch 8385: train loss: 7.143410766730085e-05, val loss: 0.09604146331548691\n",
      "Epoch 8386: train loss: 0.0001174804856418632, val loss: 0.0973828062415123\n",
      "Epoch 8387: train loss: 6.443118763854727e-05, val loss: 0.09727801382541656\n",
      "Epoch 8388: train loss: 6.212021980900317e-05, val loss: 0.09517824649810791\n",
      "Epoch 8389: train loss: 7.695763633819297e-05, val loss: 0.096429243683815\n",
      "Epoch 8390: train loss: 3.600054333219305e-05, val loss: 0.09954553097486496\n",
      "Epoch 8391: train loss: 5.7952769566327333e-05, val loss: 0.09921570867300034\n",
      "Epoch 8392: train loss: 3.508998634060845e-05, val loss: 0.09694778919219971\n",
      "Epoch 8393: train loss: 3.8011068681953475e-05, val loss: 0.095900759100914\n",
      "Epoch 8394: train loss: 3.893516623065807e-05, val loss: 0.09629666805267334\n",
      "Epoch 8395: train loss: 2.2621292373514734e-05, val loss: 0.09758958965539932\n",
      "Epoch 8396: train loss: 3.166438182233833e-05, val loss: 0.09863851219415665\n",
      "Epoch 8397: train loss: 1.9108740161755122e-05, val loss: 0.0974801704287529\n",
      "Epoch 8398: train loss: 2.1759016817668453e-05, val loss: 0.09583204239606857\n",
      "Epoch 8399: train loss: 2.0589768610079773e-05, val loss: 0.09623583406209946\n",
      "Epoch 8400: train loss: 1.428450832463568e-05, val loss: 0.09708508104085922\n",
      "Epoch 8401: train loss: 1.8002077922574244e-05, val loss: 0.09651646763086319\n",
      "Epoch 8402: train loss: 1.3269282135297544e-05, val loss: 0.09589636325836182\n",
      "Epoch 8403: train loss: 1.1467965123301838e-05, val loss: 0.09613440930843353\n",
      "Epoch 8404: train loss: 1.2673681339947507e-05, val loss: 0.09598281979560852\n",
      "Epoch 8405: train loss: 9.071231033885852e-06, val loss: 0.09550997614860535\n",
      "Epoch 8406: train loss: 1.0839757123903837e-05, val loss: 0.09595344215631485\n",
      "Epoch 8407: train loss: 6.935998953849776e-06, val loss: 0.09608769416809082\n",
      "Epoch 8408: train loss: 9.771662917046342e-06, val loss: 0.09489959478378296\n",
      "Epoch 8409: train loss: 4.8050665100163314e-06, val loss: 0.09443178027868271\n",
      "Epoch 8410: train loss: 7.85924476076616e-06, val loss: 0.09566687792539597\n",
      "Epoch 8411: train loss: 5.261671503831167e-06, val loss: 0.09583313018083572\n",
      "Epoch 8412: train loss: 4.738142706628423e-06, val loss: 0.09451224654912949\n",
      "Epoch 8413: train loss: 5.460163720272249e-06, val loss: 0.09439422935247421\n",
      "Epoch 8414: train loss: 4.567127689369954e-06, val loss: 0.09505797922611237\n",
      "Epoch 8415: train loss: 2.8040992674505105e-06, val loss: 0.09507258981466293\n",
      "Epoch 8416: train loss: 5.452047844300978e-06, val loss: 0.09451552480459213\n",
      "Epoch 8417: train loss: 1.6199370520553202e-06, val loss: 0.09399406611919403\n",
      "Epoch 8418: train loss: 3.870842192554846e-06, val loss: 0.09421171247959137\n",
      "Epoch 8419: train loss: 3.0043424885661807e-06, val loss: 0.09466966241598129\n",
      "Epoch 8420: train loss: 2.085592768708011e-06, val loss: 0.09425051510334015\n",
      "Epoch 8421: train loss: 2.3916186364658643e-06, val loss: 0.09347822517156601\n",
      "Epoch 8422: train loss: 2.6953453016176354e-06, val loss: 0.09377051889896393\n",
      "Epoch 8423: train loss: 1.4088026318859193e-06, val loss: 0.09399773925542831\n",
      "Epoch 8424: train loss: 1.9047716932618641e-06, val loss: 0.09374383836984634\n",
      "Epoch 8425: train loss: 2.070737991743954e-06, val loss: 0.09375601261854172\n",
      "Epoch 8426: train loss: 1.7034562915796414e-06, val loss: 0.09310011565685272\n",
      "Epoch 8427: train loss: 1.8949384639199707e-06, val loss: 0.09331770986318588\n",
      "Epoch 8428: train loss: 2.4696182663319632e-06, val loss: 0.09351477771997452\n",
      "Epoch 8429: train loss: 4.57309670309769e-06, val loss: 0.09336044639348984\n",
      "Epoch 8430: train loss: 1.1757190804928541e-05, val loss: 0.09245717525482178\n",
      "Epoch 8431: train loss: 3.841134093818255e-05, val loss: 0.09381132572889328\n",
      "Epoch 8432: train loss: 0.00014612353697884828, val loss: 0.09117493778467178\n",
      "Epoch 8433: train loss: 0.0005842518876306713, val loss: 0.09365969896316528\n",
      "Epoch 8434: train loss: 0.0004052208678331226, val loss: 0.09763827174901962\n",
      "Epoch 8435: train loss: 0.0002731488784775138, val loss: 0.09756840765476227\n",
      "Epoch 8436: train loss: 7.23454577382654e-05, val loss: 0.09579852968454361\n",
      "Epoch 8437: train loss: 6.790940824430436e-05, val loss: 0.09539323300123215\n",
      "Epoch 8438: train loss: 5.9463680372573435e-05, val loss: 0.09299597889184952\n",
      "Epoch 8439: train loss: 5.367693302105181e-05, val loss: 0.08968625217676163\n",
      "Epoch 8440: train loss: 5.0337221182417125e-05, val loss: 0.08797834068536758\n",
      "Epoch 8441: train loss: 2.872316872526426e-05, val loss: 0.08699562400579453\n",
      "Epoch 8442: train loss: 3.4272514312760904e-05, val loss: 0.08510006964206696\n",
      "Epoch 8443: train loss: 3.6192370316712186e-05, val loss: 0.08384031802415848\n",
      "Epoch 8444: train loss: 2.5341965738334693e-05, val loss: 0.08327934890985489\n",
      "Epoch 8445: train loss: 2.3893291654530913e-05, val loss: 0.08166409283876419\n",
      "Epoch 8446: train loss: 2.8080692572984844e-05, val loss: 0.08147086203098297\n",
      "Epoch 8447: train loss: 2.4750828742980957e-05, val loss: 0.08219927549362183\n",
      "Epoch 8448: train loss: 1.631853956496343e-05, val loss: 0.08061208575963974\n",
      "Epoch 8449: train loss: 1.5108535080798902e-05, val loss: 0.07967248558998108\n",
      "Epoch 8450: train loss: 2.0502935512922704e-05, val loss: 0.08010319620370865\n",
      "Epoch 8451: train loss: 1.6622549082967453e-05, val loss: 0.07856642454862595\n",
      "Epoch 8452: train loss: 1.984853224712424e-05, val loss: 0.0796603336930275\n",
      "Epoch 8453: train loss: 3.0322977181640454e-05, val loss: 0.07929112762212753\n",
      "Epoch 8454: train loss: 5.768188930233009e-05, val loss: 0.0785231739282608\n",
      "Epoch 8455: train loss: 0.00012227143452037126, val loss: 0.07731524854898453\n",
      "Epoch 8456: train loss: 0.0003043523174710572, val loss: 0.0793386921286583\n",
      "Epoch 8457: train loss: 0.0005985010648146272, val loss: 0.07977098226547241\n",
      "Epoch 8458: train loss: 0.001117949839681387, val loss: 0.08152776211500168\n",
      "Epoch 8459: train loss: 0.0012533764820545912, val loss: 0.07943737506866455\n",
      "Epoch 8460: train loss: 0.00046407224726863205, val loss: 0.08250656723976135\n",
      "Epoch 8461: train loss: 0.00021461433789227158, val loss: 0.08638729900121689\n",
      "Epoch 8462: train loss: 0.0005898247472941875, val loss: 0.08021145313978195\n",
      "Epoch 8463: train loss: 0.00017772290448192507, val loss: 0.08062051236629486\n",
      "Epoch 8464: train loss: 0.0003029268700629473, val loss: 0.08318682760000229\n",
      "Epoch 8465: train loss: 0.0001993238111026585, val loss: 0.08112336695194244\n",
      "Epoch 8466: train loss: 0.00018099766748491675, val loss: 0.08063425868749619\n",
      "Epoch 8467: train loss: 0.00016485968080814928, val loss: 0.08232079446315765\n",
      "Epoch 8468: train loss: 0.00013854951248504221, val loss: 0.08395877480506897\n",
      "Epoch 8469: train loss: 0.00015475918189622462, val loss: 0.08401825278997421\n",
      "Epoch 8470: train loss: 7.993331382749602e-05, val loss: 0.08401934057474136\n",
      "Epoch 8471: train loss: 0.00012373868958093226, val loss: 0.08368712663650513\n",
      "Epoch 8472: train loss: 4.388991146697663e-05, val loss: 0.084211565554142\n",
      "Epoch 8473: train loss: 9.142285125562921e-05, val loss: 0.08630204945802689\n",
      "Epoch 8474: train loss: 4.5838529331376776e-05, val loss: 0.08750327676534653\n",
      "Epoch 8475: train loss: 6.17541663814336e-05, val loss: 0.08642297238111496\n",
      "Epoch 8476: train loss: 5.821044760523364e-05, val loss: 0.08446452021598816\n",
      "Epoch 8477: train loss: 3.997411113232374e-05, val loss: 0.08550482988357544\n",
      "Epoch 8478: train loss: 4.6331497287610546e-05, val loss: 0.08795769512653351\n",
      "Epoch 8479: train loss: 2.5167581043206155e-05, val loss: 0.08902279287576675\n",
      "Epoch 8480: train loss: 3.393476436031051e-05, val loss: 0.08776237815618515\n",
      "Epoch 8481: train loss: 2.1644984371960163e-05, val loss: 0.08653999119997025\n",
      "Epoch 8482: train loss: 3.0080118449404836e-05, val loss: 0.08769287914037704\n",
      "Epoch 8483: train loss: 1.995288948819507e-05, val loss: 0.0893993005156517\n",
      "Epoch 8484: train loss: 2.4968041543615982e-05, val loss: 0.08970904350280762\n",
      "Epoch 8485: train loss: 1.2807828170480207e-05, val loss: 0.08866826444864273\n",
      "Epoch 8486: train loss: 1.956062624230981e-05, val loss: 0.08830703794956207\n",
      "Epoch 8487: train loss: 8.871260433807038e-06, val loss: 0.08894505351781845\n",
      "Epoch 8488: train loss: 1.5012597032182384e-05, val loss: 0.08913671225309372\n",
      "Epoch 8489: train loss: 1.018144848785596e-05, val loss: 0.08875805139541626\n",
      "Epoch 8490: train loss: 1.1296953744022176e-05, val loss: 0.08845916390419006\n",
      "Epoch 8491: train loss: 1.091567082767142e-05, val loss: 0.08871211856603622\n",
      "Epoch 8492: train loss: 7.6147930485603865e-06, val loss: 0.08855846524238586\n",
      "Epoch 8493: train loss: 8.943033208197448e-06, val loss: 0.08798724412918091\n",
      "Epoch 8494: train loss: 5.367571247916203e-06, val loss: 0.08775365352630615\n",
      "Epoch 8495: train loss: 6.224226126505528e-06, val loss: 0.08796929568052292\n",
      "Epoch 8496: train loss: 5.4533161346626e-06, val loss: 0.08819776773452759\n",
      "Epoch 8497: train loss: 5.409179721027613e-06, val loss: 0.08790162205696106\n",
      "Epoch 8498: train loss: 5.016440354665974e-06, val loss: 0.08752772957086563\n",
      "Epoch 8499: train loss: 5.253292329143733e-06, val loss: 0.08728557825088501\n",
      "Epoch 8500: train loss: 3.1958945783117088e-06, val loss: 0.08762307465076447\n",
      "Epoch 8501: train loss: 4.592841378325829e-06, val loss: 0.08772195875644684\n",
      "Epoch 8502: train loss: 1.693474359854008e-06, val loss: 0.08714134991168976\n",
      "Epoch 8503: train loss: 4.268785232852679e-06, val loss: 0.08671732991933823\n",
      "Epoch 8504: train loss: 1.3571019508162863e-06, val loss: 0.08676020056009293\n",
      "Epoch 8505: train loss: 3.921514689864125e-06, val loss: 0.08682812750339508\n",
      "Epoch 8506: train loss: 1.692203909442469e-06, val loss: 0.0864475890994072\n",
      "Epoch 8507: train loss: 3.1817201033845777e-06, val loss: 0.08644358813762665\n",
      "Epoch 8508: train loss: 1.6999989611576893e-06, val loss: 0.08655595779418945\n",
      "Epoch 8509: train loss: 2.062696466964553e-06, val loss: 0.08633629977703094\n",
      "Epoch 8510: train loss: 2.263891701659304e-06, val loss: 0.08608243614435196\n",
      "Epoch 8511: train loss: 1.8416898228679202e-06, val loss: 0.08643883466720581\n",
      "Epoch 8512: train loss: 1.5870632523729e-06, val loss: 0.08615602552890778\n",
      "Epoch 8513: train loss: 1.4738679965375923e-06, val loss: 0.08605889230966568\n",
      "Epoch 8514: train loss: 1.9233361854276154e-06, val loss: 0.08560287207365036\n",
      "Epoch 8515: train loss: 2.284528818563558e-06, val loss: 0.08624795079231262\n",
      "Epoch 8516: train loss: 2.192929741795524e-06, val loss: 0.08597373217344284\n",
      "Epoch 8517: train loss: 1.4541202517648344e-06, val loss: 0.08615567535161972\n",
      "Epoch 8518: train loss: 1.3146471928848769e-06, val loss: 0.08626070618629456\n",
      "Epoch 8519: train loss: 1.1766909437938011e-06, val loss: 0.0859060287475586\n",
      "Epoch 8520: train loss: 1.214732151311182e-06, val loss: 0.0860181450843811\n",
      "Epoch 8521: train loss: 7.646829658369825e-07, val loss: 0.0860079899430275\n",
      "Epoch 8522: train loss: 7.294815986824688e-07, val loss: 0.08571317046880722\n",
      "Epoch 8523: train loss: 1.3188940783948055e-06, val loss: 0.08588946610689163\n",
      "Epoch 8524: train loss: 1.2824594932681066e-06, val loss: 0.0855116918683052\n",
      "Epoch 8525: train loss: 1.6129931736941217e-06, val loss: 0.08554822206497192\n",
      "Epoch 8526: train loss: 3.071834726142697e-06, val loss: 0.08557547628879547\n",
      "Epoch 8527: train loss: 6.754885816917522e-06, val loss: 0.08551361411809921\n",
      "Epoch 8528: train loss: 1.5039948266348802e-05, val loss: 0.08493369817733765\n",
      "Epoch 8529: train loss: 3.5920162190450355e-05, val loss: 0.08541398495435715\n",
      "Epoch 8530: train loss: 7.089712744345888e-05, val loss: 0.08457581698894501\n",
      "Epoch 8531: train loss: 0.00011919838289031759, val loss: 0.08539438247680664\n",
      "Epoch 8532: train loss: 0.00014773225120734423, val loss: 0.08440392464399338\n",
      "Epoch 8533: train loss: 0.00019760253780987114, val loss: 0.08442001789808273\n",
      "Epoch 8534: train loss: 0.000306636153254658, val loss: 0.08506164699792862\n",
      "Epoch 8535: train loss: 0.00037773826625198126, val loss: 0.08853942900896072\n",
      "Epoch 8536: train loss: 0.0003333730564918369, val loss: 0.08790843933820724\n",
      "Epoch 8537: train loss: 0.00028225212008692324, val loss: 0.08815968036651611\n",
      "Epoch 8538: train loss: 0.00017884165572468191, val loss: 0.09102726727724075\n",
      "Epoch 8539: train loss: 0.0001970141165656969, val loss: 0.09332849830389023\n",
      "Epoch 8540: train loss: 0.0002524089068174362, val loss: 0.09174533188343048\n",
      "Epoch 8541: train loss: 0.00024289485008921474, val loss: 0.09304153174161911\n",
      "Epoch 8542: train loss: 0.00012404574954416603, val loss: 0.09458496421575546\n",
      "Epoch 8543: train loss: 9.391794446855783e-05, val loss: 0.09662128239870071\n",
      "Epoch 8544: train loss: 8.10868659755215e-05, val loss: 0.09467282146215439\n",
      "Epoch 8545: train loss: 3.470645606284961e-05, val loss: 0.09348461776971817\n",
      "Epoch 8546: train loss: 8.5757790657226e-05, val loss: 0.09630008786916733\n",
      "Epoch 8547: train loss: 0.00011408817954361439, val loss: 0.09491560608148575\n",
      "Epoch 8548: train loss: 7.513804303016514e-05, val loss: 0.0949312373995781\n",
      "Epoch 8549: train loss: 7.167542935349047e-05, val loss: 0.09602344036102295\n",
      "Epoch 8550: train loss: 4.386702130432241e-05, val loss: 0.0963086262345314\n",
      "Epoch 8551: train loss: 1.8181486666435376e-05, val loss: 0.09568730741739273\n",
      "Epoch 8552: train loss: 3.378892506589182e-05, val loss: 0.09467795491218567\n",
      "Epoch 8553: train loss: 4.451767381397076e-05, val loss: 0.09588338434696198\n",
      "Epoch 8554: train loss: 3.862871744786389e-05, val loss: 0.09593673050403595\n",
      "Epoch 8555: train loss: 5.1131293730577454e-05, val loss: 0.09561113268136978\n",
      "Epoch 8556: train loss: 2.9548418751801364e-05, val loss: 0.09624888747930527\n",
      "Epoch 8557: train loss: 1.4883118637953885e-05, val loss: 0.09600348025560379\n",
      "Epoch 8558: train loss: 1.569709820614662e-05, val loss: 0.09601616859436035\n",
      "Epoch 8559: train loss: 1.4615558939112816e-05, val loss: 0.09616240113973618\n",
      "Epoch 8560: train loss: 2.031758413068019e-05, val loss: 0.09578708559274673\n",
      "Epoch 8561: train loss: 2.843489892256912e-05, val loss: 0.09695890545845032\n",
      "Epoch 8562: train loss: 2.1900568754062988e-05, val loss: 0.09639781713485718\n",
      "Epoch 8563: train loss: 1.4724695574841462e-05, val loss: 0.09726668149232864\n",
      "Epoch 8564: train loss: 1.296776008530287e-05, val loss: 0.09727445989847183\n",
      "Epoch 8565: train loss: 1.1118967449874617e-05, val loss: 0.0968947634100914\n",
      "Epoch 8566: train loss: 1.0205604667135049e-05, val loss: 0.09737228602170944\n",
      "Epoch 8567: train loss: 2.0283370758988895e-05, val loss: 0.09689723700284958\n",
      "Epoch 8568: train loss: 3.667449709610082e-05, val loss: 0.09998669475317001\n",
      "Epoch 8569: train loss: 7.858720346121117e-05, val loss: 0.09414904564619064\n",
      "Epoch 8570: train loss: 0.00021591951372101903, val loss: 0.10505402088165283\n",
      "Epoch 8571: train loss: 0.0006251127924770117, val loss: 0.08950632810592651\n",
      "Epoch 8572: train loss: 0.0012866327306255698, val loss: 0.10062532871961594\n",
      "Epoch 8573: train loss: 0.0004965715925209224, val loss: 0.10693486779928207\n",
      "Epoch 8574: train loss: 0.0005821994855068624, val loss: 0.09910573810338974\n",
      "Epoch 8575: train loss: 0.0003777547972276807, val loss: 0.09753420203924179\n",
      "Epoch 8576: train loss: 0.00013881239283364266, val loss: 0.09693615138530731\n",
      "Epoch 8577: train loss: 0.00014803333033341914, val loss: 0.097572460770607\n",
      "Epoch 8578: train loss: 0.00019042083295062184, val loss: 0.09742826223373413\n",
      "Epoch 8579: train loss: 0.00017453492910135537, val loss: 0.09061305969953537\n",
      "Epoch 8580: train loss: 0.00015511589299421757, val loss: 0.08871245384216309\n",
      "Epoch 8581: train loss: 7.859570905566216e-05, val loss: 0.08823449909687042\n",
      "Epoch 8582: train loss: 8.950273331720382e-05, val loss: 0.08806494623422623\n",
      "Epoch 8583: train loss: 0.0001048742706188932, val loss: 0.09224152565002441\n",
      "Epoch 8584: train loss: 5.608669016510248e-05, val loss: 0.0879874974489212\n",
      "Epoch 8585: train loss: 4.9376529204891995e-05, val loss: 0.08568107336759567\n",
      "Epoch 8586: train loss: 6.984669744269922e-05, val loss: 0.09083174914121628\n",
      "Epoch 8587: train loss: 5.9738740674220026e-05, val loss: 0.08613752573728561\n",
      "Epoch 8588: train loss: 2.5769328203750774e-05, val loss: 0.08151265233755112\n",
      "Epoch 8589: train loss: 3.694836050271988e-05, val loss: 0.08338824659585953\n",
      "Epoch 8590: train loss: 3.1346316973213106e-05, val loss: 0.08460255712270737\n",
      "Epoch 8591: train loss: 1.967657772183884e-05, val loss: 0.08258610218763351\n",
      "Epoch 8592: train loss: 2.316815334779676e-05, val loss: 0.08072087168693542\n",
      "Epoch 8593: train loss: 2.5736060706549324e-05, val loss: 0.08042587339878082\n",
      "Epoch 8594: train loss: 1.4323250979941804e-05, val loss: 0.0812663584947586\n",
      "Epoch 8595: train loss: 1.919871829159092e-05, val loss: 0.08115404844284058\n",
      "Epoch 8596: train loss: 1.1605400686676148e-05, val loss: 0.07898837327957153\n",
      "Epoch 8597: train loss: 1.3815122656524181e-05, val loss: 0.07887648791074753\n",
      "Epoch 8598: train loss: 9.804641194932628e-06, val loss: 0.08005088567733765\n",
      "Epoch 8599: train loss: 1.3944269085186534e-05, val loss: 0.07853896915912628\n",
      "Epoch 8600: train loss: 9.964947821572423e-06, val loss: 0.07747678458690643\n",
      "Epoch 8601: train loss: 6.965661668800749e-06, val loss: 0.07850991934537888\n",
      "Epoch 8602: train loss: 1.0084392670250963e-05, val loss: 0.07803945988416672\n",
      "Epoch 8603: train loss: 6.878933163534384e-06, val loss: 0.07655949145555496\n",
      "Epoch 8604: train loss: 5.927688562223921e-06, val loss: 0.076801598072052\n",
      "Epoch 8605: train loss: 6.666494300588965e-06, val loss: 0.07782711833715439\n",
      "Epoch 8606: train loss: 6.861987458250951e-06, val loss: 0.0773044005036354\n",
      "Epoch 8607: train loss: 5.492272066476289e-06, val loss: 0.07606610655784607\n",
      "Epoch 8608: train loss: 3.562612846508273e-06, val loss: 0.07661166042089462\n",
      "Epoch 8609: train loss: 3.876672963087913e-06, val loss: 0.07726515084505081\n",
      "Epoch 8610: train loss: 4.767605332745006e-06, val loss: 0.07668374478816986\n",
      "Epoch 8611: train loss: 3.091440703428816e-06, val loss: 0.07652759552001953\n",
      "Epoch 8612: train loss: 3.290782387921354e-06, val loss: 0.07649096101522446\n",
      "Epoch 8613: train loss: 3.002651737915585e-06, val loss: 0.0764957070350647\n",
      "Epoch 8614: train loss: 2.511274942662567e-06, val loss: 0.07673061639070511\n",
      "Epoch 8615: train loss: 3.612264663388487e-06, val loss: 0.07640840113162994\n",
      "Epoch 8616: train loss: 3.593863311834866e-06, val loss: 0.07673396170139313\n",
      "Epoch 8617: train loss: 3.780359293159563e-06, val loss: 0.07563648372888565\n",
      "Epoch 8618: train loss: 5.245593911240576e-06, val loss: 0.07653549313545227\n",
      "Epoch 8619: train loss: 5.588901785813505e-06, val loss: 0.07625370472669601\n",
      "Epoch 8620: train loss: 7.281865691766143e-06, val loss: 0.0767923966050148\n",
      "Epoch 8621: train loss: 1.086892189050559e-05, val loss: 0.07565481960773468\n",
      "Epoch 8622: train loss: 1.9585693735280074e-05, val loss: 0.07776784896850586\n",
      "Epoch 8623: train loss: 3.795227166847326e-05, val loss: 0.07471513003110886\n",
      "Epoch 8624: train loss: 8.744608203414828e-05, val loss: 0.07811339944601059\n",
      "Epoch 8625: train loss: 0.00017227848002221435, val loss: 0.07235971838235855\n",
      "Epoch 8626: train loss: 0.0004055322497151792, val loss: 0.07858431339263916\n",
      "Epoch 8627: train loss: 0.0005258554010652006, val loss: 0.06727087497711182\n",
      "Epoch 8628: train loss: 0.0005046232836320996, val loss: 0.06917699426412582\n",
      "Epoch 8629: train loss: 0.00016865739598870277, val loss: 0.06945567578077316\n",
      "Epoch 8630: train loss: 0.0002816508640535176, val loss: 0.06955856084823608\n",
      "Epoch 8631: train loss: 0.00023660159786231816, val loss: 0.06573373079299927\n",
      "Epoch 8632: train loss: 0.00025181207456626, val loss: 0.06856005638837814\n",
      "Epoch 8633: train loss: 0.00010641184053383768, val loss: 0.0651271864771843\n",
      "Epoch 8634: train loss: 0.00011592029477469623, val loss: 0.06401095539331436\n",
      "Epoch 8635: train loss: 0.00014975739759393036, val loss: 0.07280101627111435\n",
      "Epoch 8636: train loss: 8.902332774596289e-05, val loss: 0.07002612203359604\n",
      "Epoch 8637: train loss: 9.126762597588822e-05, val loss: 0.06489850580692291\n",
      "Epoch 8638: train loss: 8.038990927161649e-05, val loss: 0.07106657326221466\n",
      "Epoch 8639: train loss: 5.871831672266126e-05, val loss: 0.07500436156988144\n",
      "Epoch 8640: train loss: 5.450661774375476e-05, val loss: 0.07207926362752914\n",
      "Epoch 8641: train loss: 4.101188824279234e-05, val loss: 0.06993880122900009\n",
      "Epoch 8642: train loss: 5.1072223868686706e-05, val loss: 0.06981044262647629\n",
      "Epoch 8643: train loss: 4.902294676867314e-05, val loss: 0.07278560847043991\n",
      "Epoch 8644: train loss: 3.699495573528111e-05, val loss: 0.07338737696409225\n",
      "Epoch 8645: train loss: 2.824612420226913e-05, val loss: 0.07263731211423874\n",
      "Epoch 8646: train loss: 3.4288961614947766e-05, val loss: 0.07294517010450363\n",
      "Epoch 8647: train loss: 1.9975395844085142e-05, val loss: 0.07158201187849045\n",
      "Epoch 8648: train loss: 2.4169308744603768e-05, val loss: 0.07399740070104599\n",
      "Epoch 8649: train loss: 1.8629434634931386e-05, val loss: 0.07566121965646744\n",
      "Epoch 8650: train loss: 2.1942036255495623e-05, val loss: 0.07157623022794724\n",
      "Epoch 8651: train loss: 2.2535301468451507e-05, val loss: 0.0734877809882164\n",
      "Epoch 8652: train loss: 1.4089907381276134e-05, val loss: 0.0757116824388504\n",
      "Epoch 8653: train loss: 1.325501398241613e-05, val loss: 0.07603360712528229\n",
      "Epoch 8654: train loss: 1.2049775250488892e-05, val loss: 0.07655250281095505\n",
      "Epoch 8655: train loss: 1.3236804988991935e-05, val loss: 0.073427215218544\n",
      "Epoch 8656: train loss: 1.0608910088194534e-05, val loss: 0.07486921548843384\n",
      "Epoch 8657: train loss: 1.1319649274810217e-05, val loss: 0.07702374458312988\n",
      "Epoch 8658: train loss: 8.62406159285456e-06, val loss: 0.07659836858510971\n",
      "Epoch 8659: train loss: 9.406241588294506e-06, val loss: 0.07673720270395279\n",
      "Epoch 8660: train loss: 9.53011021920247e-06, val loss: 0.07629942893981934\n",
      "Epoch 8661: train loss: 7.96269978309283e-06, val loss: 0.07752338796854019\n",
      "Epoch 8662: train loss: 5.5886548580019735e-06, val loss: 0.07715965807437897\n",
      "Epoch 8663: train loss: 6.5804520090750884e-06, val loss: 0.07904095947742462\n",
      "Epoch 8664: train loss: 4.703236754721729e-06, val loss: 0.07751595228910446\n",
      "Epoch 8665: train loss: 4.9782884161686525e-06, val loss: 0.0774703323841095\n",
      "Epoch 8666: train loss: 4.9006171138898935e-06, val loss: 0.07878332585096359\n",
      "Epoch 8667: train loss: 4.04501452067052e-06, val loss: 0.07823184877634048\n",
      "Epoch 8668: train loss: 4.256909505784279e-06, val loss: 0.0789366215467453\n",
      "Epoch 8669: train loss: 3.7806548789376393e-06, val loss: 0.07854335755109787\n",
      "Epoch 8670: train loss: 6.934742486919276e-06, val loss: 0.08045478165149689\n",
      "Epoch 8671: train loss: 1.7424788893549703e-05, val loss: 0.07616949081420898\n",
      "Epoch 8672: train loss: 5.874125781701878e-05, val loss: 0.08384117484092712\n",
      "Epoch 8673: train loss: 0.0002334400633117184, val loss: 0.06930873543024063\n",
      "Epoch 8674: train loss: 0.000905183725990355, val loss: 0.09079916775226593\n",
      "Epoch 8675: train loss: 0.0030730089638382196, val loss: 0.07531369477510452\n",
      "Epoch 8676: train loss: 0.00665863323956728, val loss: 0.11283107101917267\n",
      "Epoch 8677: train loss: 0.0031625365372747183, val loss: 0.10316524654626846\n",
      "Epoch 8678: train loss: 0.00196131132543087, val loss: 0.09770314395427704\n",
      "Epoch 8679: train loss: 0.0015201757196336985, val loss: 0.08782600611448288\n",
      "Epoch 8680: train loss: 0.0012953801779076457, val loss: 0.09652786701917648\n",
      "Epoch 8681: train loss: 0.0009701914386823773, val loss: 0.10468167066574097\n",
      "Epoch 8682: train loss: 0.0007593703921884298, val loss: 0.10263075679540634\n",
      "Epoch 8683: train loss: 0.0005845270934514701, val loss: 0.10153374820947647\n",
      "Epoch 8684: train loss: 0.0006716485950164497, val loss: 0.1018013283610344\n",
      "Epoch 8685: train loss: 0.0005413552280515432, val loss: 0.0997486412525177\n",
      "Epoch 8686: train loss: 0.0004125844279769808, val loss: 0.09286250919103622\n",
      "Epoch 8687: train loss: 0.0003327832673676312, val loss: 0.08776338398456573\n",
      "Epoch 8688: train loss: 0.00028755850507877767, val loss: 0.0879121646285057\n",
      "Epoch 8689: train loss: 0.00028563709929585457, val loss: 0.09353756159543991\n",
      "Epoch 8690: train loss: 0.00023999343102332205, val loss: 0.10117136687040329\n",
      "Epoch 8691: train loss: 0.00020386261167004704, val loss: 0.10493679344654083\n",
      "Epoch 8692: train loss: 0.0002264935610583052, val loss: 0.10449294000864029\n",
      "Epoch 8693: train loss: 0.00015190927661024034, val loss: 0.1011791005730629\n",
      "Epoch 8694: train loss: 9.402783325640485e-05, val loss: 0.09761284291744232\n",
      "Epoch 8695: train loss: 0.00012549963139463216, val loss: 0.0967557355761528\n",
      "Epoch 8696: train loss: 0.00013436118024401367, val loss: 0.09726186841726303\n",
      "Epoch 8697: train loss: 0.0001081474547390826, val loss: 0.09855852276086807\n",
      "Epoch 8698: train loss: 7.079329225234687e-05, val loss: 0.09959103912115097\n",
      "Epoch 8699: train loss: 5.3938514611218125e-05, val loss: 0.10071718692779541\n",
      "Epoch 8700: train loss: 7.715808897046372e-05, val loss: 0.10309123992919922\n",
      "Epoch 8701: train loss: 7.80781265348196e-05, val loss: 0.10523805767297745\n",
      "Epoch 8702: train loss: 5.306686580297537e-05, val loss: 0.10672324895858765\n",
      "Epoch 8703: train loss: 4.496451947488822e-05, val loss: 0.10620107501745224\n",
      "Epoch 8704: train loss: 4.1472634620731696e-05, val loss: 0.1039639338850975\n",
      "Epoch 8705: train loss: 3.934847700293176e-05, val loss: 0.10200893878936768\n",
      "Epoch 8706: train loss: 4.139114389545284e-05, val loss: 0.10095486789941788\n",
      "Epoch 8707: train loss: 3.4019467420876026e-05, val loss: 0.1015113964676857\n",
      "Epoch 8708: train loss: 2.6343333956901915e-05, val loss: 0.10264468193054199\n",
      "Epoch 8709: train loss: 2.5020177417900413e-05, val loss: 0.103528693318367\n",
      "Epoch 8710: train loss: 2.1552719772444107e-05, val loss: 0.10460900515317917\n",
      "Epoch 8711: train loss: 2.2754838937544264e-05, val loss: 0.10573150217533112\n",
      "Epoch 8712: train loss: 2.2293937945505604e-05, val loss: 0.10701411962509155\n",
      "Epoch 8713: train loss: 1.4840807125438005e-05, val loss: 0.1074739471077919\n",
      "Epoch 8714: train loss: 1.318884824286215e-05, val loss: 0.10668438673019409\n",
      "Epoch 8715: train loss: 1.407533090969082e-05, val loss: 0.10532673448324203\n",
      "Epoch 8716: train loss: 1.2746502761729062e-05, val loss: 0.10398509353399277\n",
      "Epoch 8717: train loss: 1.2381667147565167e-05, val loss: 0.10380556434392929\n",
      "Epoch 8718: train loss: 9.68187850958202e-06, val loss: 0.10445797443389893\n",
      "Epoch 8719: train loss: 8.23582104203524e-06, val loss: 0.10526040941476822\n",
      "Epoch 8720: train loss: 8.160543075064197e-06, val loss: 0.10572540760040283\n",
      "Epoch 8721: train loss: 7.139160061342409e-06, val loss: 0.10546956211328506\n",
      "Epoch 8722: train loss: 8.300593435706105e-06, val loss: 0.10516177862882614\n",
      "Epoch 8723: train loss: 7.061373253236525e-06, val loss: 0.10497363656759262\n",
      "Epoch 8724: train loss: 4.053811608173419e-06, val loss: 0.10498867183923721\n",
      "Epoch 8725: train loss: 4.240601356286788e-06, val loss: 0.10502085834741592\n",
      "Epoch 8726: train loss: 5.601791599474382e-06, val loss: 0.10486205667257309\n",
      "Epoch 8727: train loss: 5.480317213368835e-06, val loss: 0.10497061163187027\n",
      "Epoch 8728: train loss: 3.3932228689081967e-06, val loss: 0.10531165450811386\n",
      "Epoch 8729: train loss: 2.422040779492818e-06, val loss: 0.105684295296669\n",
      "Epoch 8730: train loss: 3.019304585905047e-06, val loss: 0.10572236031293869\n",
      "Epoch 8731: train loss: 3.429922799114138e-06, val loss: 0.10512378066778183\n",
      "Epoch 8732: train loss: 2.9939583328086883e-06, val loss: 0.10444493591785431\n",
      "Epoch 8733: train loss: 2.1632945390592795e-06, val loss: 0.10404541343450546\n",
      "Epoch 8734: train loss: 1.7734739685693057e-06, val loss: 0.1041790246963501\n",
      "Epoch 8735: train loss: 1.8038277858067886e-06, val loss: 0.10461971908807755\n",
      "Epoch 8736: train loss: 2.121201987392851e-06, val loss: 0.10493848472833633\n",
      "Epoch 8737: train loss: 1.593745196259988e-06, val loss: 0.10517141968011856\n",
      "Epoch 8738: train loss: 1.3907107359045767e-06, val loss: 0.10519309341907501\n",
      "Epoch 8739: train loss: 1.5346716963904328e-06, val loss: 0.1050235778093338\n",
      "Epoch 8740: train loss: 1.0056692190119065e-06, val loss: 0.10459661483764648\n",
      "Epoch 8741: train loss: 8.832680578052532e-07, val loss: 0.10409142822027206\n",
      "Epoch 8742: train loss: 1.187359544019273e-06, val loss: 0.10400787740945816\n",
      "Epoch 8743: train loss: 1.1466913747426588e-06, val loss: 0.10431128740310669\n",
      "Epoch 8744: train loss: 6.331683266580512e-07, val loss: 0.10471516847610474\n",
      "Epoch 8745: train loss: 6.452514753618743e-07, val loss: 0.10477028042078018\n",
      "Epoch 8746: train loss: 7.162016117945313e-07, val loss: 0.10446721315383911\n",
      "Epoch 8747: train loss: 5.549757702283387e-07, val loss: 0.10420149564743042\n",
      "Epoch 8748: train loss: 6.005649879625707e-07, val loss: 0.10411286354064941\n",
      "Epoch 8749: train loss: 5.596161827270407e-07, val loss: 0.10424675792455673\n",
      "Epoch 8750: train loss: 3.9198511103677447e-07, val loss: 0.10434573143720627\n",
      "Epoch 8751: train loss: 3.4595481679389195e-07, val loss: 0.10429912060499191\n",
      "Epoch 8752: train loss: 3.942921580346592e-07, val loss: 0.10418602079153061\n",
      "Epoch 8753: train loss: 3.663028849132388e-07, val loss: 0.10404757410287857\n",
      "Epoch 8754: train loss: 3.7712399603151425e-07, val loss: 0.10401489585638046\n",
      "Epoch 8755: train loss: 2.7305890171192004e-07, val loss: 0.10404690355062485\n",
      "Epoch 8756: train loss: 2.1317970322343172e-07, val loss: 0.10415990650653839\n",
      "Epoch 8757: train loss: 2.7471210728435835e-07, val loss: 0.10418637841939926\n",
      "Epoch 8758: train loss: 1.7837598420555878e-07, val loss: 0.10396581143140793\n",
      "Epoch 8759: train loss: 2.3620557954018295e-07, val loss: 0.10371223837137222\n",
      "Epoch 8760: train loss: 1.936867448648627e-07, val loss: 0.10367920249700546\n",
      "Epoch 8761: train loss: 1.8170014470797469e-07, val loss: 0.10391531139612198\n",
      "Epoch 8762: train loss: 1.376356806304102e-07, val loss: 0.10394752025604248\n",
      "Epoch 8763: train loss: 1.391013597640267e-07, val loss: 0.10379445552825928\n",
      "Epoch 8764: train loss: 1.570640222325892e-07, val loss: 0.10369523614645004\n",
      "Epoch 8765: train loss: 1.621292540221475e-07, val loss: 0.10362949222326279\n",
      "Epoch 8766: train loss: 2.502449376606819e-07, val loss: 0.10360489040613174\n",
      "Epoch 8767: train loss: 6.378978127941082e-07, val loss: 0.10371971130371094\n",
      "Epoch 8768: train loss: 1.8389415572528378e-06, val loss: 0.10346084088087082\n",
      "Epoch 8769: train loss: 3.710695182235213e-06, val loss: 0.10380347818136215\n",
      "Epoch 8770: train loss: 6.2982007875689305e-06, val loss: 0.1030661091208458\n",
      "Epoch 8771: train loss: 1.3700959243578836e-05, val loss: 0.10401791334152222\n",
      "Epoch 8772: train loss: 1.9858267478412017e-05, val loss: 0.10277681797742844\n",
      "Epoch 8773: train loss: 2.9478826036211103e-05, val loss: 0.10391086339950562\n",
      "Epoch 8774: train loss: 2.4919227143982425e-05, val loss: 0.10297005623579025\n",
      "Epoch 8775: train loss: 1.3755952750216238e-05, val loss: 0.10387571901082993\n",
      "Epoch 8776: train loss: 3.367495537531795e-06, val loss: 0.10341053456068039\n",
      "Epoch 8777: train loss: 1.1636533372438862e-06, val loss: 0.10288359969854355\n",
      "Epoch 8778: train loss: 6.176484930620063e-06, val loss: 0.10380376875400543\n",
      "Epoch 8779: train loss: 1.4110250958765391e-05, val loss: 0.10242479294538498\n",
      "Epoch 8780: train loss: 1.891036117740441e-05, val loss: 0.10360784828662872\n",
      "Epoch 8781: train loss: 1.829925167839974e-05, val loss: 0.10222792625427246\n",
      "Epoch 8782: train loss: 1.685574352450203e-05, val loss: 0.10320869833230972\n",
      "Epoch 8783: train loss: 1.2548936865641735e-05, val loss: 0.10169046372175217\n",
      "Epoch 8784: train loss: 1.0030103112512734e-05, val loss: 0.10261254757642746\n",
      "Epoch 8785: train loss: 7.230481969600078e-06, val loss: 0.10147052258253098\n",
      "Epoch 8786: train loss: 5.826623237226158e-06, val loss: 0.10206563770771027\n",
      "Epoch 8787: train loss: 5.200285613682354e-06, val loss: 0.10117095708847046\n",
      "Epoch 8788: train loss: 6.233428848645417e-06, val loss: 0.10182268917560577\n",
      "Epoch 8789: train loss: 8.890566277841572e-06, val loss: 0.10041560232639313\n",
      "Epoch 8790: train loss: 1.7322463463642634e-05, val loss: 0.10218522697687149\n",
      "Epoch 8791: train loss: 4.019858533865772e-05, val loss: 0.09917143732309341\n",
      "Epoch 8792: train loss: 0.00010905640374403447, val loss: 0.10432076454162598\n",
      "Epoch 8793: train loss: 0.0002770902938209474, val loss: 0.09539236128330231\n",
      "Epoch 8794: train loss: 0.0007883348735049367, val loss: 0.11032652854919434\n",
      "Epoch 8795: train loss: 0.0011500673135742545, val loss: 0.08922497183084488\n",
      "Epoch 8796: train loss: 0.0012210480635985732, val loss: 0.09145604819059372\n",
      "Epoch 8797: train loss: 9.544756176183e-05, val loss: 0.099183090031147\n",
      "Epoch 8798: train loss: 0.0008615090046077967, val loss: 0.09079576283693314\n",
      "Epoch 8799: train loss: 0.00010407203080831096, val loss: 0.0858759805560112\n",
      "Epoch 8800: train loss: 0.00045717909233644605, val loss: 0.08863560110330582\n",
      "Epoch 8801: train loss: 0.00016859838797245175, val loss: 0.09454997628927231\n",
      "Epoch 8802: train loss: 0.00026661870651878417, val loss: 0.09363894909620285\n",
      "Epoch 8803: train loss: 0.00022810861992184073, val loss: 0.08917184174060822\n",
      "Epoch 8804: train loss: 0.00015224104572553188, val loss: 0.08831492811441422\n",
      "Epoch 8805: train loss: 0.0001647411845624447, val loss: 0.09206118434667587\n",
      "Epoch 8806: train loss: 0.00011385799734853208, val loss: 0.0953136458992958\n",
      "Epoch 8807: train loss: 0.00011679063754854724, val loss: 0.09324689954519272\n",
      "Epoch 8808: train loss: 9.246104309568182e-05, val loss: 0.08988628536462784\n",
      "Epoch 8809: train loss: 0.0001170164396171458, val loss: 0.08951953798532486\n",
      "Epoch 8810: train loss: 7.783841283526272e-05, val loss: 0.0925503671169281\n",
      "Epoch 8811: train loss: 6.596838647965342e-05, val loss: 0.09431077539920807\n",
      "Epoch 8812: train loss: 6.616369500989094e-05, val loss: 0.09241697192192078\n",
      "Epoch 8813: train loss: 5.103231887915172e-05, val loss: 0.08972104638814926\n",
      "Epoch 8814: train loss: 5.203342152526602e-05, val loss: 0.08990337699651718\n",
      "Epoch 8815: train loss: 5.036048241890967e-05, val loss: 0.09294060617685318\n",
      "Epoch 8816: train loss: 4.3647301936289296e-05, val loss: 0.09433478116989136\n",
      "Epoch 8817: train loss: 3.9667622331762686e-05, val loss: 0.09253900498151779\n",
      "Epoch 8818: train loss: 2.59298558376031e-05, val loss: 0.09015028923749924\n",
      "Epoch 8819: train loss: 3.1610452424502e-05, val loss: 0.08991669863462448\n",
      "Epoch 8820: train loss: 2.5736313546076417e-05, val loss: 0.09124540537595749\n",
      "Epoch 8821: train loss: 2.6694029656937346e-05, val loss: 0.09158848971128464\n",
      "Epoch 8822: train loss: 2.4054839741438627e-05, val loss: 0.09094371646642685\n",
      "Epoch 8823: train loss: 1.8660464775166474e-05, val loss: 0.09041952341794968\n",
      "Epoch 8824: train loss: 1.903381235024426e-05, val loss: 0.090714231133461\n",
      "Epoch 8825: train loss: 1.1476941836008336e-05, val loss: 0.09118693321943283\n",
      "Epoch 8826: train loss: 1.608519050932955e-05, val loss: 0.09101863205432892\n",
      "Epoch 8827: train loss: 1.5298906873795204e-05, val loss: 0.09054002910852432\n",
      "Epoch 8828: train loss: 9.901015801005997e-06, val loss: 0.08977879583835602\n",
      "Epoch 8829: train loss: 1.3758109162154142e-05, val loss: 0.08952536433935165\n",
      "Epoch 8830: train loss: 8.376583537028637e-06, val loss: 0.08956703543663025\n",
      "Epoch 8831: train loss: 7.339741841860814e-06, val loss: 0.08981218934059143\n",
      "Epoch 8832: train loss: 8.88090562511934e-06, val loss: 0.09023740887641907\n",
      "Epoch 8833: train loss: 6.345174369926099e-06, val loss: 0.0905192419886589\n",
      "Epoch 8834: train loss: 8.817748494038824e-06, val loss: 0.09027490764856339\n",
      "Epoch 8835: train loss: 4.6786385610175785e-06, val loss: 0.0894966721534729\n",
      "Epoch 8836: train loss: 6.63886703478056e-06, val loss: 0.08926665037870407\n",
      "Epoch 8837: train loss: 3.967545126215555e-06, val loss: 0.08955657482147217\n",
      "Epoch 8838: train loss: 4.1231087379856035e-06, val loss: 0.08946933597326279\n",
      "Epoch 8839: train loss: 4.315005753596779e-06, val loss: 0.0891915112733841\n",
      "Epoch 8840: train loss: 4.063535925524775e-06, val loss: 0.0892748013138771\n",
      "Epoch 8841: train loss: 3.48675803252263e-06, val loss: 0.08936472982168198\n",
      "Epoch 8842: train loss: 3.407962822166155e-06, val loss: 0.08892273157835007\n",
      "Epoch 8843: train loss: 2.7190073978999862e-06, val loss: 0.0887983962893486\n",
      "Epoch 8844: train loss: 2.063262172669056e-06, val loss: 0.08915471285581589\n",
      "Epoch 8845: train loss: 2.5706942778924713e-06, val loss: 0.089167021214962\n",
      "Epoch 8846: train loss: 2.1178066162974574e-06, val loss: 0.0887303575873375\n",
      "Epoch 8847: train loss: 2.114675226039253e-06, val loss: 0.08831261843442917\n",
      "Epoch 8848: train loss: 2.023655724769924e-06, val loss: 0.08827479928731918\n",
      "Epoch 8849: train loss: 1.7413663044862915e-06, val loss: 0.08831281214952469\n",
      "Epoch 8850: train loss: 1.2832986158173298e-06, val loss: 0.08811870962381363\n",
      "Epoch 8851: train loss: 1.4529995269185747e-06, val loss: 0.0880562886595726\n",
      "Epoch 8852: train loss: 1.0543094504100736e-06, val loss: 0.08816065639257431\n",
      "Epoch 8853: train loss: 1.5073118220243487e-06, val loss: 0.08762051910161972\n",
      "Epoch 8854: train loss: 8.431262017438712e-07, val loss: 0.08724639564752579\n",
      "Epoch 8855: train loss: 1.3926196515967604e-06, val loss: 0.08772479742765427\n",
      "Epoch 8856: train loss: 7.931943741823488e-07, val loss: 0.08764614909887314\n",
      "Epoch 8857: train loss: 1.0114335964317434e-06, val loss: 0.08711547404527664\n",
      "Epoch 8858: train loss: 6.279263402575452e-07, val loss: 0.0870785042643547\n",
      "Epoch 8859: train loss: 7.379020985354146e-07, val loss: 0.08716540783643723\n",
      "Epoch 8860: train loss: 6.048703653505072e-07, val loss: 0.08700748533010483\n",
      "Epoch 8861: train loss: 7.592170163661649e-07, val loss: 0.08688484877347946\n",
      "Epoch 8862: train loss: 7.853491865716933e-07, val loss: 0.086866095662117\n",
      "Epoch 8863: train loss: 4.622837082024489e-07, val loss: 0.08647274225950241\n",
      "Epoch 8864: train loss: 9.454461746827292e-07, val loss: 0.08667973428964615\n",
      "Epoch 8865: train loss: 1.2116355492253206e-06, val loss: 0.08625034242868423\n",
      "Epoch 8866: train loss: 2.8485271741374163e-06, val loss: 0.08677826821804047\n",
      "Epoch 8867: train loss: 9.060756383405533e-06, val loss: 0.0854557529091835\n",
      "Epoch 8868: train loss: 3.505449421936646e-05, val loss: 0.08776633441448212\n",
      "Epoch 8869: train loss: 0.0001502471131971106, val loss: 0.0835268422961235\n",
      "Epoch 8870: train loss: 0.000668035470880568, val loss: 0.08875063061714172\n",
      "Epoch 8871: train loss: 0.0025886278599500656, val loss: 0.09843450039625168\n",
      "Epoch 8872: train loss: 0.007265914231538773, val loss: 0.09204366058111191\n",
      "Epoch 8873: train loss: 0.005005592480301857, val loss: 0.09494773298501968\n",
      "Epoch 8874: train loss: 0.0025174652691930532, val loss: 0.09337405115365982\n",
      "Epoch 8875: train loss: 0.0014872080646455288, val loss: 0.09368836134672165\n",
      "Epoch 8876: train loss: 0.0023495655041188, val loss: 0.0919346958398819\n",
      "Epoch 8877: train loss: 0.0006638222257606685, val loss: 0.09020406752824783\n",
      "Epoch 8878: train loss: 0.0007462609210051596, val loss: 0.08436845988035202\n",
      "Epoch 8879: train loss: 0.00111542665399611, val loss: 0.07984664291143417\n",
      "Epoch 8880: train loss: 0.000737025635316968, val loss: 0.08000030368566513\n",
      "Epoch 8881: train loss: 0.000377634831238538, val loss: 0.08048801124095917\n",
      "Epoch 8882: train loss: 0.00042396524804644287, val loss: 0.0794394388794899\n",
      "Epoch 8883: train loss: 0.0005244753556326032, val loss: 0.07805293053388596\n",
      "Epoch 8884: train loss: 0.0004606259462889284, val loss: 0.07812847197055817\n",
      "Epoch 8885: train loss: 0.0002711162669584155, val loss: 0.08098961412906647\n",
      "Epoch 8886: train loss: 0.00018786614236887544, val loss: 0.08394201844930649\n",
      "Epoch 8887: train loss: 0.00028649403247982264, val loss: 0.0839085504412651\n",
      "Epoch 8888: train loss: 0.0003123004571534693, val loss: 0.08265115320682526\n",
      "Epoch 8889: train loss: 0.00020017925999127328, val loss: 0.08128692954778671\n",
      "Epoch 8890: train loss: 0.00011618052667472512, val loss: 0.0811823159456253\n",
      "Epoch 8891: train loss: 0.00013204710558056831, val loss: 0.08254265040159225\n",
      "Epoch 8892: train loss: 0.00016708120529074222, val loss: 0.08419418334960938\n",
      "Epoch 8893: train loss: 0.00014636872219853103, val loss: 0.08597661554813385\n",
      "Epoch 8894: train loss: 9.6501549705863e-05, val loss: 0.08746372163295746\n",
      "Epoch 8895: train loss: 8.384867396671325e-05, val loss: 0.08846545219421387\n",
      "Epoch 8896: train loss: 9.20495149330236e-05, val loss: 0.08897893875837326\n",
      "Epoch 8897: train loss: 8.56019978527911e-05, val loss: 0.08801205456256866\n",
      "Epoch 8898: train loss: 7.343025208683684e-05, val loss: 0.08600868284702301\n",
      "Epoch 8899: train loss: 6.176727765705436e-05, val loss: 0.08416906744241714\n",
      "Epoch 8900: train loss: 5.335089736036025e-05, val loss: 0.08328813314437866\n",
      "Epoch 8901: train loss: 4.8768615670269355e-05, val loss: 0.08419237285852432\n",
      "Epoch 8902: train loss: 4.758295472129248e-05, val loss: 0.08628565073013306\n",
      "Epoch 8903: train loss: 4.764430195791647e-05, val loss: 0.08838768303394318\n",
      "Epoch 8904: train loss: 3.745965295820497e-05, val loss: 0.089872345328331\n",
      "Epoch 8905: train loss: 2.3974422219907865e-05, val loss: 0.0901525542140007\n",
      "Epoch 8906: train loss: 2.8631066015805118e-05, val loss: 0.08967871963977814\n",
      "Epoch 8907: train loss: 3.643944000941701e-05, val loss: 0.08880478143692017\n",
      "Epoch 8908: train loss: 2.7885740564670414e-05, val loss: 0.08752666413784027\n",
      "Epoch 8909: train loss: 1.4942644156690221e-05, val loss: 0.08653934299945831\n",
      "Epoch 8910: train loss: 1.4467720575339627e-05, val loss: 0.08618433028459549\n",
      "Epoch 8911: train loss: 2.300200139870867e-05, val loss: 0.08660104125738144\n",
      "Epoch 8912: train loss: 2.326417961739935e-05, val loss: 0.08769919723272324\n",
      "Epoch 8913: train loss: 1.2363141649984755e-05, val loss: 0.08872837573289871\n",
      "Epoch 8914: train loss: 7.199846550065558e-06, val loss: 0.08957912027835846\n",
      "Epoch 8915: train loss: 1.2196945135656279e-05, val loss: 0.08985283225774765\n",
      "Epoch 8916: train loss: 1.572441033204086e-05, val loss: 0.08975334465503693\n",
      "Epoch 8917: train loss: 1.0944730092887767e-05, val loss: 0.08979552239179611\n",
      "Epoch 8918: train loss: 4.840014753426658e-06, val loss: 0.0898752510547638\n",
      "Epoch 8919: train loss: 7.408045348711312e-06, val loss: 0.08983975648880005\n",
      "Epoch 8920: train loss: 1.0863629540835973e-05, val loss: 0.08964686840772629\n",
      "Epoch 8921: train loss: 6.996112915658159e-06, val loss: 0.08936389535665512\n",
      "Epoch 8922: train loss: 3.7875838643230963e-06, val loss: 0.08940283954143524\n",
      "Epoch 8923: train loss: 4.7191319936246146e-06, val loss: 0.08967547118663788\n",
      "Epoch 8924: train loss: 5.959739155514399e-06, val loss: 0.08967870473861694\n",
      "Epoch 8925: train loss: 5.50798040421796e-06, val loss: 0.08942605555057526\n",
      "Epoch 8926: train loss: 3.276446932432009e-06, val loss: 0.08898117393255234\n",
      "Epoch 8927: train loss: 2.882197577491752e-06, val loss: 0.08869575709104538\n",
      "Epoch 8928: train loss: 4.0957202145364136e-06, val loss: 0.08867110311985016\n",
      "Epoch 8929: train loss: 3.6312035263108555e-06, val loss: 0.08852332830429077\n",
      "Epoch 8930: train loss: 2.5516706045891624e-06, val loss: 0.08828417211771011\n",
      "Epoch 8931: train loss: 1.8169735085393768e-06, val loss: 0.0881720557808876\n",
      "Epoch 8932: train loss: 2.7543649139261106e-06, val loss: 0.08832051604986191\n",
      "Epoch 8933: train loss: 2.949477220681729e-06, val loss: 0.08854230493307114\n",
      "Epoch 8934: train loss: 1.3865873143004137e-06, val loss: 0.08823700249195099\n",
      "Epoch 8935: train loss: 1.577918396833411e-06, val loss: 0.0874851644039154\n",
      "Epoch 8936: train loss: 1.7766845985534019e-06, val loss: 0.08682530373334885\n",
      "Epoch 8937: train loss: 1.49474044519593e-06, val loss: 0.08636488020420074\n",
      "Epoch 8938: train loss: 1.4702864064020105e-06, val loss: 0.08635304868221283\n",
      "Epoch 8939: train loss: 1.1381358717699186e-06, val loss: 0.08662131428718567\n",
      "Epoch 8940: train loss: 9.133976277553302e-07, val loss: 0.086949422955513\n",
      "Epoch 8941: train loss: 1.392070089423214e-06, val loss: 0.08718632161617279\n",
      "Epoch 8942: train loss: 9.073526143765775e-07, val loss: 0.08694807440042496\n",
      "Epoch 8943: train loss: 7.243717163873953e-07, val loss: 0.08641839027404785\n",
      "Epoch 8944: train loss: 8.710678685019957e-07, val loss: 0.08621426671743393\n",
      "Epoch 8945: train loss: 7.062985787342768e-07, val loss: 0.08638464659452438\n",
      "Epoch 8946: train loss: 7.714191383456637e-07, val loss: 0.08668515086174011\n",
      "Epoch 8947: train loss: 5.775842737421044e-07, val loss: 0.08673291653394699\n",
      "Epoch 8948: train loss: 5.358672297006706e-07, val loss: 0.08654236048460007\n",
      "Epoch 8949: train loss: 7.395794341391593e-07, val loss: 0.0866534560918808\n",
      "Epoch 8950: train loss: 7.314615686482284e-07, val loss: 0.08671588450670242\n",
      "Epoch 8951: train loss: 8.413288696829113e-07, val loss: 0.08693214505910873\n",
      "Epoch 8952: train loss: 1.5385019196401117e-06, val loss: 0.0868932232260704\n",
      "Epoch 8953: train loss: 3.159457037327229e-06, val loss: 0.08682060241699219\n",
      "Epoch 8954: train loss: 4.46691819888656e-06, val loss: 0.08615847676992416\n",
      "Epoch 8955: train loss: 2.9226312108221464e-06, val loss: 0.08677616715431213\n",
      "Epoch 8956: train loss: 4.620541176336701e-07, val loss: 0.08691500127315521\n",
      "Epoch 8957: train loss: 1.690192220848985e-06, val loss: 0.0863855704665184\n",
      "Epoch 8958: train loss: 6.60775810956693e-07, val loss: 0.08621151745319366\n",
      "Epoch 8959: train loss: 1.120543856814038e-06, val loss: 0.08666165918111801\n",
      "Epoch 8960: train loss: 9.144350769929588e-07, val loss: 0.08639445155858994\n",
      "Epoch 8961: train loss: 7.198175921985239e-07, val loss: 0.08625759929418564\n",
      "Epoch 8962: train loss: 1.0148099818252376e-06, val loss: 0.08625425398349762\n",
      "Epoch 8963: train loss: 4.0924413724496844e-07, val loss: 0.08640646189451218\n",
      "Epoch 8964: train loss: 9.14887493763672e-07, val loss: 0.08600202947854996\n",
      "Epoch 8965: train loss: 3.901568561559543e-07, val loss: 0.08604131639003754\n",
      "Epoch 8966: train loss: 7.727056754447403e-07, val loss: 0.08586061000823975\n",
      "Epoch 8967: train loss: 1.309095182477904e-06, val loss: 0.08620351552963257\n",
      "Epoch 8968: train loss: 3.96619361708872e-06, val loss: 0.0850522518157959\n",
      "Epoch 8969: train loss: 1.7365642634104006e-05, val loss: 0.0879928469657898\n",
      "Epoch 8970: train loss: 4.3500185711309314e-05, val loss: 0.08418739587068558\n",
      "Epoch 8971: train loss: 0.00010714829113567248, val loss: 0.08920206874608994\n",
      "Epoch 8972: train loss: 0.0001594756031408906, val loss: 0.08721005171537399\n",
      "Epoch 8973: train loss: 0.00014855418703518808, val loss: 0.0915922075510025\n",
      "Epoch 8974: train loss: 2.487423989805393e-05, val loss: 0.0919983983039856\n",
      "Epoch 8975: train loss: 3.373778235982172e-05, val loss: 0.08853540569543839\n",
      "Epoch 8976: train loss: 9.715728810988367e-05, val loss: 0.0940646156668663\n",
      "Epoch 8977: train loss: 3.4011074603768066e-05, val loss: 0.0933891087770462\n",
      "Epoch 8978: train loss: 1.6400765161961317e-05, val loss: 0.09045565128326416\n",
      "Epoch 8979: train loss: 6.430397479562089e-05, val loss: 0.09433212876319885\n",
      "Epoch 8980: train loss: 3.15609504468739e-05, val loss: 0.09160391986370087\n",
      "Epoch 8981: train loss: 9.793619028641842e-06, val loss: 0.09043186157941818\n",
      "Epoch 8982: train loss: 4.745203477796167e-05, val loss: 0.09427597373723984\n",
      "Epoch 8983: train loss: 1.926670483953785e-05, val loss: 0.09484447538852692\n",
      "Epoch 8984: train loss: 1.2743868865072727e-05, val loss: 0.09169088304042816\n",
      "Epoch 8985: train loss: 3.005412145284936e-05, val loss: 0.093989297747612\n",
      "Epoch 8986: train loss: 1.007109403872164e-05, val loss: 0.09517975896596909\n",
      "Epoch 8987: train loss: 1.4654720871476457e-05, val loss: 0.09202968329191208\n",
      "Epoch 8988: train loss: 2.045433211605996e-05, val loss: 0.09358477592468262\n",
      "Epoch 8989: train loss: 5.701902409782633e-06, val loss: 0.09632306545972824\n",
      "Epoch 8990: train loss: 1.5809677279321477e-05, val loss: 0.09324081987142563\n",
      "Epoch 8991: train loss: 1.0993621799570974e-05, val loss: 0.09357502311468124\n",
      "Epoch 8992: train loss: 7.226627985801315e-06, val loss: 0.09623973071575165\n",
      "Epoch 8993: train loss: 1.4048071534489281e-05, val loss: 0.09392540901899338\n",
      "Epoch 8994: train loss: 6.767296326870564e-06, val loss: 0.09343893080949783\n",
      "Epoch 8995: train loss: 6.4113914959307294e-06, val loss: 0.09577455371618271\n",
      "Epoch 8996: train loss: 1.0391023351985496e-05, val loss: 0.09436160326004028\n",
      "Epoch 8997: train loss: 5.762193723057862e-06, val loss: 0.09368999302387238\n",
      "Epoch 8998: train loss: 5.198166945774574e-06, val loss: 0.09540823847055435\n",
      "Epoch 8999: train loss: 7.402320079563651e-06, val loss: 0.09411690384149551\n",
      "Epoch 9000: train loss: 4.641210125555517e-06, val loss: 0.09386172145605087\n",
      "Epoch 9001: train loss: 4.029479896416888e-06, val loss: 0.09550388902425766\n",
      "Epoch 9002: train loss: 6.5096860453195404e-06, val loss: 0.09379581362009048\n",
      "Epoch 9003: train loss: 4.3832142182509415e-06, val loss: 0.09391067177057266\n",
      "Epoch 9004: train loss: 2.079204477922758e-06, val loss: 0.09498143196105957\n",
      "Epoch 9005: train loss: 4.293590336601483e-06, val loss: 0.0935363620519638\n",
      "Epoch 9006: train loss: 5.949335900368169e-06, val loss: 0.09476836025714874\n",
      "Epoch 9007: train loss: 3.423754606046714e-06, val loss: 0.09432009607553482\n",
      "Epoch 9008: train loss: 1.7699712770991027e-06, val loss: 0.09333308786153793\n",
      "Epoch 9009: train loss: 2.6546056233200943e-06, val loss: 0.09466236084699631\n",
      "Epoch 9010: train loss: 3.602305469030398e-06, val loss: 0.09342772513628006\n",
      "Epoch 9011: train loss: 3.6370204270497197e-06, val loss: 0.09412098675966263\n",
      "Epoch 9012: train loss: 3.6840897337242495e-06, val loss: 0.09367334097623825\n",
      "Epoch 9013: train loss: 5.316128408594523e-06, val loss: 0.09375154227018356\n",
      "Epoch 9014: train loss: 9.460735782340635e-06, val loss: 0.09364921599626541\n",
      "Epoch 9015: train loss: 2.1884459783905186e-05, val loss: 0.09364477545022964\n",
      "Epoch 9016: train loss: 5.4310756240738556e-05, val loss: 0.0928449034690857\n",
      "Epoch 9017: train loss: 0.00010542431118665263, val loss: 0.0948251411318779\n",
      "Epoch 9018: train loss: 0.00017624262545723468, val loss: 0.09227584302425385\n",
      "Epoch 9019: train loss: 0.0002485180157236755, val loss: 0.09281637519598007\n",
      "Epoch 9020: train loss: 0.0002736123569775373, val loss: 0.10212545841932297\n",
      "Epoch 9021: train loss: 0.00028677587397396564, val loss: 0.08842448890209198\n",
      "Epoch 9022: train loss: 0.0005077904206700623, val loss: 0.10814248770475388\n",
      "Epoch 9023: train loss: 0.0003991481498815119, val loss: 0.09467004984617233\n",
      "Epoch 9024: train loss: 0.00019706915190909058, val loss: 0.09639663994312286\n",
      "Epoch 9025: train loss: 9.217362094204873e-05, val loss: 0.1024441048502922\n",
      "Epoch 9026: train loss: 0.00020426597620826215, val loss: 0.09432220458984375\n",
      "Epoch 9027: train loss: 0.0001052636798704043, val loss: 0.09506485611200333\n",
      "Epoch 9028: train loss: 9.815565135795623e-05, val loss: 0.10207939147949219\n",
      "Epoch 9029: train loss: 9.892057278193533e-05, val loss: 0.09983795136213303\n",
      "Epoch 9030: train loss: 7.092444866430014e-05, val loss: 0.09387272596359253\n",
      "Epoch 9031: train loss: 5.919570321566425e-05, val loss: 0.09469318389892578\n",
      "Epoch 9032: train loss: 6.231011502677575e-05, val loss: 0.09918631613254547\n",
      "Epoch 9033: train loss: 5.763153967563994e-05, val loss: 0.09742768108844757\n",
      "Epoch 9034: train loss: 4.4397918827598915e-05, val loss: 0.09551806002855301\n",
      "Epoch 9035: train loss: 4.782087853527628e-05, val loss: 0.09683860838413239\n",
      "Epoch 9036: train loss: 2.6694075131672435e-05, val loss: 0.09790634363889694\n",
      "Epoch 9037: train loss: 4.524493488133885e-05, val loss: 0.09660405665636063\n",
      "Epoch 9038: train loss: 2.029331517405808e-05, val loss: 0.0961599349975586\n",
      "Epoch 9039: train loss: 4.179613461019471e-05, val loss: 0.09724647551774979\n",
      "Epoch 9040: train loss: 1.338104993919842e-05, val loss: 0.0971100851893425\n",
      "Epoch 9041: train loss: 2.7055857572122477e-05, val loss: 0.09546029567718506\n",
      "Epoch 9042: train loss: 1.9651084585348144e-05, val loss: 0.09541439265012741\n",
      "Epoch 9043: train loss: 2.0739120373036712e-05, val loss: 0.09732215851545334\n",
      "Epoch 9044: train loss: 1.7145328456535935e-05, val loss: 0.09753488749265671\n",
      "Epoch 9045: train loss: 1.5383391655632295e-05, val loss: 0.09552419185638428\n",
      "Epoch 9046: train loss: 1.1569344678719062e-05, val loss: 0.09523027390241623\n",
      "Epoch 9047: train loss: 1.6114981917780824e-05, val loss: 0.09580781310796738\n",
      "Epoch 9048: train loss: 8.6795362221892e-06, val loss: 0.09537976235151291\n",
      "Epoch 9049: train loss: 1.3987685633765068e-05, val loss: 0.09561699628829956\n",
      "Epoch 9050: train loss: 7.52901769374148e-06, val loss: 0.09607020765542984\n",
      "Epoch 9051: train loss: 8.822084964776877e-06, val loss: 0.09563493728637695\n",
      "Epoch 9052: train loss: 8.005054951354396e-06, val loss: 0.09493225067853928\n",
      "Epoch 9053: train loss: 9.258905265596695e-06, val loss: 0.0942864939570427\n",
      "Epoch 9054: train loss: 5.426615189207951e-06, val loss: 0.0947437435388565\n",
      "Epoch 9055: train loss: 7.111716513463762e-06, val loss: 0.09513472765684128\n",
      "Epoch 9056: train loss: 4.566328243527096e-06, val loss: 0.0944112166762352\n",
      "Epoch 9057: train loss: 6.496713922388153e-06, val loss: 0.09404011815786362\n",
      "Epoch 9058: train loss: 5.099811460240744e-06, val loss: 0.09404739737510681\n",
      "Epoch 9059: train loss: 4.661272214434575e-06, val loss: 0.09472282975912094\n",
      "Epoch 9060: train loss: 4.786734280060045e-06, val loss: 0.09423323720693588\n",
      "Epoch 9061: train loss: 3.043112428713357e-06, val loss: 0.093300960958004\n",
      "Epoch 9062: train loss: 2.9423597425193293e-06, val loss: 0.09329839795827866\n",
      "Epoch 9063: train loss: 3.706044935825048e-06, val loss: 0.09344471991062164\n",
      "Epoch 9064: train loss: 4.905446076008957e-06, val loss: 0.09357202053070068\n",
      "Epoch 9065: train loss: 5.190926003706409e-06, val loss: 0.09299596399068832\n",
      "Epoch 9066: train loss: 1.1690282008203212e-05, val loss: 0.09361594170331955\n",
      "Epoch 9067: train loss: 3.2539392122998834e-05, val loss: 0.092853844165802\n",
      "Epoch 9068: train loss: 5.030065949540585e-05, val loss: 0.09169164299964905\n",
      "Epoch 9069: train loss: 8.108154725050554e-05, val loss: 0.09329088777303696\n",
      "Epoch 9070: train loss: 0.00013113426393829286, val loss: 0.09238117188215256\n",
      "Epoch 9071: train loss: 0.00020422642410267144, val loss: 0.09413554519414902\n",
      "Epoch 9072: train loss: 0.0004521368828136474, val loss: 0.0952700525522232\n",
      "Epoch 9073: train loss: 0.000693378911819309, val loss: 0.09152629226446152\n",
      "Epoch 9074: train loss: 0.0009506636415608227, val loss: 0.09713590890169144\n",
      "Epoch 9075: train loss: 0.0003504962951410562, val loss: 0.09688570350408554\n",
      "Epoch 9076: train loss: 0.0003452892124187201, val loss: 0.09508472681045532\n",
      "Epoch 9077: train loss: 0.0002574531827121973, val loss: 0.09439607709646225\n",
      "Epoch 9078: train loss: 0.00023754488211125135, val loss: 0.09031009674072266\n",
      "Epoch 9079: train loss: 0.00020254732226021588, val loss: 0.08996652811765671\n",
      "Epoch 9080: train loss: 0.00014222291065379977, val loss: 0.09130629152059555\n",
      "Epoch 9081: train loss: 0.0001709951611701399, val loss: 0.092595674097538\n",
      "Epoch 9082: train loss: 0.00013739900896325707, val loss: 0.09192463010549545\n",
      "Epoch 9083: train loss: 9.545433567836881e-05, val loss: 0.09206118434667587\n",
      "Epoch 9084: train loss: 0.00010547994315857068, val loss: 0.09223941713571548\n",
      "Epoch 9085: train loss: 8.524360600858927e-05, val loss: 0.08938228338956833\n",
      "Epoch 9086: train loss: 7.203314453363419e-05, val loss: 0.08824778348207474\n",
      "Epoch 9087: train loss: 7.130602898541838e-05, val loss: 0.09065085649490356\n",
      "Epoch 9088: train loss: 5.3155035857344046e-05, val loss: 0.09206018596887589\n",
      "Epoch 9089: train loss: 5.343837619875558e-05, val loss: 0.09108177572488785\n",
      "Epoch 9090: train loss: 4.4309319491731e-05, val loss: 0.09014526754617691\n",
      "Epoch 9091: train loss: 3.887613274855539e-05, val loss: 0.09011665731668472\n",
      "Epoch 9092: train loss: 3.779367034439929e-05, val loss: 0.08998628705739975\n",
      "Epoch 9093: train loss: 3.082082184846513e-05, val loss: 0.0904315859079361\n",
      "Epoch 9094: train loss: 2.8636004572035745e-05, val loss: 0.09140472859144211\n",
      "Epoch 9095: train loss: 2.7605166906141676e-05, val loss: 0.09083657711744308\n",
      "Epoch 9096: train loss: 2.2984617316978984e-05, val loss: 0.0890999585390091\n",
      "Epoch 9097: train loss: 1.9608580259955488e-05, val loss: 0.08885011821985245\n",
      "Epoch 9098: train loss: 1.888151018647477e-05, val loss: 0.0899752601981163\n",
      "Epoch 9099: train loss: 1.9693510694196448e-05, val loss: 0.08956580609083176\n",
      "Epoch 9100: train loss: 1.2027357115584891e-05, val loss: 0.08869851380586624\n",
      "Epoch 9101: train loss: 1.3373968613450415e-05, val loss: 0.08934568613767624\n",
      "Epoch 9102: train loss: 1.5118320334295277e-05, val loss: 0.08973753452301025\n",
      "Epoch 9103: train loss: 8.946391972131096e-06, val loss: 0.0885167047381401\n",
      "Epoch 9104: train loss: 1.0363186447648332e-05, val loss: 0.08669587224721909\n",
      "Epoch 9105: train loss: 9.660955583967734e-06, val loss: 0.08688795566558838\n",
      "Epoch 9106: train loss: 7.261638529598713e-06, val loss: 0.08810427039861679\n",
      "Epoch 9107: train loss: 8.790791071078274e-06, val loss: 0.08775090426206589\n",
      "Epoch 9108: train loss: 6.0779566410928965e-06, val loss: 0.08684810250997543\n",
      "Epoch 9109: train loss: 7.019489657977829e-06, val loss: 0.08710082620382309\n",
      "Epoch 9110: train loss: 4.247769538778812e-06, val loss: 0.08746485412120819\n",
      "Epoch 9111: train loss: 5.551238700718386e-06, val loss: 0.0868336632847786\n",
      "Epoch 9112: train loss: 5.369730388338212e-06, val loss: 0.08693834394216537\n",
      "Epoch 9113: train loss: 4.558419277600478e-06, val loss: 0.08787772059440613\n",
      "Epoch 9114: train loss: 5.943160886090482e-06, val loss: 0.0872684046626091\n",
      "Epoch 9115: train loss: 5.836867785546929e-06, val loss: 0.08673267811536789\n",
      "Epoch 9116: train loss: 8.625163900433108e-06, val loss: 0.08705705404281616\n",
      "Epoch 9117: train loss: 1.5211146092042327e-05, val loss: 0.08746003359556198\n",
      "Epoch 9118: train loss: 3.605231540859677e-05, val loss: 0.08708282560110092\n",
      "Epoch 9119: train loss: 9.865367610473186e-05, val loss: 0.08859707415103912\n",
      "Epoch 9120: train loss: 0.00032489243312738836, val loss: 0.08925414085388184\n",
      "Epoch 9121: train loss: 0.0009208559640683234, val loss: 0.0909397229552269\n",
      "Epoch 9122: train loss: 0.002418026328086853, val loss: 0.08227567374706268\n",
      "Epoch 9123: train loss: 0.0009774701902642846, val loss: 0.08203167468309402\n",
      "Epoch 9124: train loss: 0.0005647966754622757, val loss: 0.08232954889535904\n",
      "Epoch 9125: train loss: 0.0005546380416490138, val loss: 0.08755757659673691\n",
      "Epoch 9126: train loss: 0.0005158856511116028, val loss: 0.09125661104917526\n",
      "Epoch 9127: train loss: 0.0002108805492753163, val loss: 0.0942707508802414\n",
      "Epoch 9128: train loss: 0.0003978394961450249, val loss: 0.09557706862688065\n",
      "Epoch 9129: train loss: 0.0002805452386382967, val loss: 0.0938054546713829\n",
      "Epoch 9130: train loss: 0.0001590492611285299, val loss: 0.0948125571012497\n",
      "Epoch 9131: train loss: 0.00019486712699290365, val loss: 0.09880482405424118\n",
      "Epoch 9132: train loss: 0.00017935712821781635, val loss: 0.10222023725509644\n",
      "Epoch 9133: train loss: 0.0001401868212269619, val loss: 0.10093431919813156\n",
      "Epoch 9134: train loss: 0.00011214860569452867, val loss: 0.09792062640190125\n",
      "Epoch 9135: train loss: 0.0001213417126564309, val loss: 0.09873654693365097\n",
      "Epoch 9136: train loss: 0.00010291932267136872, val loss: 0.10248558968305588\n",
      "Epoch 9137: train loss: 7.597983494633809e-05, val loss: 0.10434486716985703\n",
      "Epoch 9138: train loss: 7.212960190372542e-05, val loss: 0.10549550503492355\n",
      "Epoch 9139: train loss: 6.910675438120961e-05, val loss: 0.10724722594022751\n",
      "Epoch 9140: train loss: 6.943641346879303e-05, val loss: 0.10686779022216797\n",
      "Epoch 9141: train loss: 4.701648504124023e-05, val loss: 0.10425525158643723\n",
      "Epoch 9142: train loss: 4.2968538764398545e-05, val loss: 0.10255272686481476\n",
      "Epoch 9143: train loss: 4.140503369853832e-05, val loss: 0.10425432026386261\n",
      "Epoch 9144: train loss: 4.257210093783215e-05, val loss: 0.10733935981988907\n",
      "Epoch 9145: train loss: 3.6383560654940084e-05, val loss: 0.10780153423547745\n",
      "Epoch 9146: train loss: 3.07124100800138e-05, val loss: 0.10690169781446457\n",
      "Epoch 9147: train loss: 2.594825673440937e-05, val loss: 0.10640094429254532\n",
      "Epoch 9148: train loss: 2.591148040664848e-05, val loss: 0.10679399967193604\n",
      "Epoch 9149: train loss: 1.8348740923102014e-05, val loss: 0.10687457770109177\n",
      "Epoch 9150: train loss: 2.4925782781792805e-05, val loss: 0.1055770069360733\n",
      "Epoch 9151: train loss: 1.9434912246651947e-05, val loss: 0.10430856794118881\n",
      "Epoch 9152: train loss: 1.4991992429713719e-05, val loss: 0.10439834743738174\n",
      "Epoch 9153: train loss: 1.4917956832505297e-05, val loss: 0.10616061836481094\n",
      "Epoch 9154: train loss: 1.2915892511955462e-05, val loss: 0.10770779848098755\n",
      "Epoch 9155: train loss: 1.303076533076819e-05, val loss: 0.10741076618432999\n",
      "Epoch 9156: train loss: 1.0988404028466903e-05, val loss: 0.10623776167631149\n",
      "Epoch 9157: train loss: 1.0901997484324966e-05, val loss: 0.10567300766706467\n",
      "Epoch 9158: train loss: 9.864481398835778e-06, val loss: 0.10537689924240112\n",
      "Epoch 9159: train loss: 6.560626843565842e-06, val loss: 0.10538168996572495\n",
      "Epoch 9160: train loss: 8.300045010400936e-06, val loss: 0.10564833134412766\n",
      "Epoch 9161: train loss: 5.805870387121104e-06, val loss: 0.10584592074155807\n",
      "Epoch 9162: train loss: 7.118617304513464e-06, val loss: 0.1058383509516716\n",
      "Epoch 9163: train loss: 5.44646263733739e-06, val loss: 0.10627279430627823\n",
      "Epoch 9164: train loss: 4.6310592551890295e-06, val loss: 0.10617692768573761\n",
      "Epoch 9165: train loss: 4.605167305271607e-06, val loss: 0.1054672822356224\n",
      "Epoch 9166: train loss: 3.7792715374962427e-06, val loss: 0.105106882750988\n",
      "Epoch 9167: train loss: 3.433115807638387e-06, val loss: 0.10461964458227158\n",
      "Epoch 9168: train loss: 3.6928008739778306e-06, val loss: 0.10408198833465576\n",
      "Epoch 9169: train loss: 3.389186758795404e-06, val loss: 0.10452574491500854\n",
      "Epoch 9170: train loss: 2.6805998913914664e-06, val loss: 0.1051110252737999\n",
      "Epoch 9171: train loss: 2.241395577584626e-06, val loss: 0.10461677610874176\n",
      "Epoch 9172: train loss: 2.6045413505926263e-06, val loss: 0.10406625270843506\n",
      "Epoch 9173: train loss: 2.041166681010509e-06, val loss: 0.10350799560546875\n",
      "Epoch 9174: train loss: 1.981157083719154e-06, val loss: 0.10307163000106812\n",
      "Epoch 9175: train loss: 1.9519820853020065e-06, val loss: 0.1035185381770134\n",
      "Epoch 9176: train loss: 1.6951154293565196e-06, val loss: 0.10347483307123184\n",
      "Epoch 9177: train loss: 1.8029886632575653e-06, val loss: 0.10322610288858414\n",
      "Epoch 9178: train loss: 1.2625872614080436e-06, val loss: 0.10300929844379425\n",
      "Epoch 9179: train loss: 1.1265869943599682e-06, val loss: 0.10244705528020859\n",
      "Epoch 9180: train loss: 1.1458821518317563e-06, val loss: 0.10211744159460068\n",
      "Epoch 9181: train loss: 1.2755884881698876e-06, val loss: 0.10215570032596588\n",
      "Epoch 9182: train loss: 1.488234147473122e-06, val loss: 0.10204293578863144\n",
      "Epoch 9183: train loss: 1.9701951714523602e-06, val loss: 0.101529560983181\n",
      "Epoch 9184: train loss: 3.585799959182623e-06, val loss: 0.10187279433012009\n",
      "Epoch 9185: train loss: 8.610822987975553e-06, val loss: 0.10085995495319366\n",
      "Epoch 9186: train loss: 2.7777770810644142e-05, val loss: 0.1018684133887291\n",
      "Epoch 9187: train loss: 0.00010562127863522619, val loss: 0.09798534959554672\n",
      "Epoch 9188: train loss: 0.0004026306269224733, val loss: 0.10464807599782944\n",
      "Epoch 9189: train loss: 0.001212536939419806, val loss: 0.09277912974357605\n",
      "Epoch 9190: train loss: 0.0021392928902059793, val loss: 0.10818088054656982\n",
      "Epoch 9191: train loss: 0.0010069765849038959, val loss: 0.10281626135110855\n",
      "Epoch 9192: train loss: 0.00036899230326525867, val loss: 0.10009291023015976\n",
      "Epoch 9193: train loss: 0.0007397962035611272, val loss: 0.10655927658081055\n",
      "Epoch 9194: train loss: 0.00034390203654766083, val loss: 0.10671825706958771\n",
      "Epoch 9195: train loss: 0.00041269781650044024, val loss: 0.10308525711297989\n",
      "Epoch 9196: train loss: 0.0002209549566032365, val loss: 0.10606640577316284\n",
      "Epoch 9197: train loss: 0.000314712873660028, val loss: 0.10904914140701294\n",
      "Epoch 9198: train loss: 0.00013399808085523546, val loss: 0.1093396320939064\n",
      "Epoch 9199: train loss: 0.00026857273769564927, val loss: 0.10739987343549728\n",
      "Epoch 9200: train loss: 0.00011267620720900595, val loss: 0.10808593034744263\n",
      "Epoch 9201: train loss: 0.0001951051235664636, val loss: 0.10991141945123672\n",
      "Epoch 9202: train loss: 8.561884897062555e-05, val loss: 0.11017417907714844\n",
      "Epoch 9203: train loss: 0.00012723742111120373, val loss: 0.10825381428003311\n",
      "Epoch 9204: train loss: 9.887104533845559e-05, val loss: 0.10776384174823761\n",
      "Epoch 9205: train loss: 7.841139449737966e-05, val loss: 0.10958539694547653\n",
      "Epoch 9206: train loss: 9.67249070527032e-05, val loss: 0.11015381664037704\n",
      "Epoch 9207: train loss: 5.932214480708353e-05, val loss: 0.10876595973968506\n",
      "Epoch 9208: train loss: 6.39028221485205e-05, val loss: 0.10805816948413849\n",
      "Epoch 9209: train loss: 5.940971095697023e-05, val loss: 0.10989706963300705\n",
      "Epoch 9210: train loss: 3.6870507756248116e-05, val loss: 0.11132454872131348\n",
      "Epoch 9211: train loss: 5.956773384241387e-05, val loss: 0.10989689081907272\n",
      "Epoch 9212: train loss: 2.9732760594924912e-05, val loss: 0.10897622257471085\n",
      "Epoch 9213: train loss: 4.3457566789584234e-05, val loss: 0.11068828403949738\n",
      "Epoch 9214: train loss: 3.3460764825576916e-05, val loss: 0.11314456909894943\n",
      "Epoch 9215: train loss: 2.4517703423043713e-05, val loss: 0.1123960018157959\n",
      "Epoch 9216: train loss: 2.8684999051620252e-05, val loss: 0.110369972884655\n",
      "Epoch 9217: train loss: 2.26612883125199e-05, val loss: 0.11054867506027222\n",
      "Epoch 9218: train loss: 2.209673766628839e-05, val loss: 0.11140298843383789\n",
      "Epoch 9219: train loss: 2.2827154680271633e-05, val loss: 0.11054259538650513\n",
      "Epoch 9220: train loss: 1.530083864054177e-05, val loss: 0.10893750190734863\n",
      "Epoch 9221: train loss: 1.7195698092109524e-05, val loss: 0.10918072611093521\n",
      "Epoch 9222: train loss: 1.4453072253672872e-05, val loss: 0.11073990166187286\n",
      "Epoch 9223: train loss: 1.1363954399712384e-05, val loss: 0.11118322610855103\n",
      "Epoch 9224: train loss: 1.460987186874263e-05, val loss: 0.11089950054883957\n",
      "Epoch 9225: train loss: 1.0170703717449214e-05, val loss: 0.11040104925632477\n",
      "Epoch 9226: train loss: 1.059234364220174e-05, val loss: 0.10956916958093643\n",
      "Epoch 9227: train loss: 8.643915862194262e-06, val loss: 0.10865471512079239\n",
      "Epoch 9228: train loss: 8.151768270181492e-06, val loss: 0.10860562324523926\n",
      "Epoch 9229: train loss: 6.9582333708240185e-06, val loss: 0.10913344472646713\n",
      "Epoch 9230: train loss: 7.587507752759848e-06, val loss: 0.10920655727386475\n",
      "Epoch 9231: train loss: 5.833530394738773e-06, val loss: 0.10926499217748642\n",
      "Epoch 9232: train loss: 6.286503776209429e-06, val loss: 0.10921459645032883\n",
      "Epoch 9233: train loss: 4.918084869132144e-06, val loss: 0.10849262773990631\n",
      "Epoch 9234: train loss: 4.744090347230667e-06, val loss: 0.10827741771936417\n",
      "Epoch 9235: train loss: 3.9926535464474e-06, val loss: 0.109000563621521\n",
      "Epoch 9236: train loss: 4.24663994635921e-06, val loss: 0.10901618003845215\n",
      "Epoch 9237: train loss: 3.2059895147540374e-06, val loss: 0.10790660232305527\n",
      "Epoch 9238: train loss: 4.011730197817087e-06, val loss: 0.10805495828390121\n",
      "Epoch 9239: train loss: 2.4921139356592903e-06, val loss: 0.10826088488101959\n",
      "Epoch 9240: train loss: 3.446727987466147e-06, val loss: 0.10754276812076569\n",
      "Epoch 9241: train loss: 1.8202352976004477e-06, val loss: 0.10781051963567734\n",
      "Epoch 9242: train loss: 2.8734457373502664e-06, val loss: 0.10820017009973526\n",
      "Epoch 9243: train loss: 1.5641063555449364e-06, val loss: 0.10760805755853653\n",
      "Epoch 9244: train loss: 2.554834964030306e-06, val loss: 0.10751288384199142\n",
      "Epoch 9245: train loss: 1.7571609305377933e-06, val loss: 0.10758649557828903\n",
      "Epoch 9246: train loss: 1.889875193228363e-06, val loss: 0.10742683708667755\n",
      "Epoch 9247: train loss: 1.5601200402670656e-06, val loss: 0.10720574855804443\n",
      "Epoch 9248: train loss: 1.5690098962295451e-06, val loss: 0.10687669366598129\n",
      "Epoch 9249: train loss: 1.966458512470126e-06, val loss: 0.107572041451931\n",
      "Epoch 9250: train loss: 2.059197868220508e-06, val loss: 0.10692330449819565\n",
      "Epoch 9251: train loss: 2.826611535056145e-06, val loss: 0.10710543394088745\n",
      "Epoch 9252: train loss: 3.5095397379336646e-06, val loss: 0.1069786548614502\n",
      "Epoch 9253: train loss: 5.4743950386182405e-06, val loss: 0.10715073347091675\n",
      "Epoch 9254: train loss: 3.2629586712573655e-06, val loss: 0.1066604033112526\n",
      "Epoch 9255: train loss: 3.1288041100197006e-06, val loss: 0.10741063207387924\n",
      "Epoch 9256: train loss: 3.8395360206777696e-06, val loss: 0.10591445118188858\n",
      "Epoch 9257: train loss: 4.487640126171755e-06, val loss: 0.1072937473654747\n",
      "Epoch 9258: train loss: 9.275740012526512e-06, val loss: 0.10478837788105011\n",
      "Epoch 9259: train loss: 2.194524677179288e-05, val loss: 0.10880851745605469\n",
      "Epoch 9260: train loss: 5.341291398508474e-05, val loss: 0.10205116122961044\n",
      "Epoch 9261: train loss: 0.00016294684610329568, val loss: 0.11286848038434982\n",
      "Epoch 9262: train loss: 0.00037943056668154895, val loss: 0.09652789682149887\n",
      "Epoch 9263: train loss: 0.0010070304851979017, val loss: 0.10689990967512131\n",
      "Epoch 9264: train loss: 0.0007976467604748905, val loss: 0.08725201338529587\n",
      "Epoch 9265: train loss: 0.00032526467111893, val loss: 0.0886356458067894\n",
      "Epoch 9266: train loss: 0.00019823623006232083, val loss: 0.09615931659936905\n",
      "Epoch 9267: train loss: 0.00034403405152261257, val loss: 0.09335701167583466\n",
      "Epoch 9268: train loss: 0.00010857087909244001, val loss: 0.09030792862176895\n",
      "Epoch 9269: train loss: 0.00019299844279885292, val loss: 0.09130855649709702\n",
      "Epoch 9270: train loss: 0.00010937684419332072, val loss: 0.09274269640445709\n",
      "Epoch 9271: train loss: 0.00011544382869033143, val loss: 0.0908038541674614\n",
      "Epoch 9272: train loss: 0.00011864708358189091, val loss: 0.0890461802482605\n",
      "Epoch 9273: train loss: 9.51914262259379e-05, val loss: 0.09350977092981339\n",
      "Epoch 9274: train loss: 9.271343151340261e-05, val loss: 0.09751930087804794\n",
      "Epoch 9275: train loss: 7.842729974072427e-05, val loss: 0.09410896897315979\n",
      "Epoch 9276: train loss: 5.0669459596974775e-05, val loss: 0.09031851589679718\n",
      "Epoch 9277: train loss: 5.781450818176381e-05, val loss: 0.09080708771944046\n",
      "Epoch 9278: train loss: 4.1018298361450434e-05, val loss: 0.09307614713907242\n",
      "Epoch 9279: train loss: 5.033343404647894e-05, val loss: 0.09366586059331894\n",
      "Epoch 9280: train loss: 4.0565006202086806e-05, val loss: 0.09274357557296753\n",
      "Epoch 9281: train loss: 3.78462063963525e-05, val loss: 0.09220966696739197\n",
      "Epoch 9282: train loss: 3.764501525438391e-05, val loss: 0.09120245277881622\n",
      "Epoch 9283: train loss: 2.51765814027749e-05, val loss: 0.09039891511201859\n",
      "Epoch 9284: train loss: 2.5952043870347552e-05, val loss: 0.09065233170986176\n",
      "Epoch 9285: train loss: 2.2505535525851883e-05, val loss: 0.09109840542078018\n",
      "Epoch 9286: train loss: 2.0474122720770538e-05, val loss: 0.09094297140836716\n",
      "Epoch 9287: train loss: 2.254753781016916e-05, val loss: 0.09027685970067978\n",
      "Epoch 9288: train loss: 1.8139924577553757e-05, val loss: 0.09036598354578018\n",
      "Epoch 9289: train loss: 1.715899088594597e-05, val loss: 0.09121079742908478\n",
      "Epoch 9290: train loss: 1.5830566553631797e-05, val loss: 0.0912826657295227\n",
      "Epoch 9291: train loss: 1.2078915460733697e-05, val loss: 0.0903167650103569\n",
      "Epoch 9292: train loss: 1.063995387085015e-05, val loss: 0.09027016162872314\n",
      "Epoch 9293: train loss: 1.1825234651041683e-05, val loss: 0.09079992026090622\n",
      "Epoch 9294: train loss: 8.736982636037283e-06, val loss: 0.0903034433722496\n",
      "Epoch 9295: train loss: 1.115038958232617e-05, val loss: 0.08984514325857162\n",
      "Epoch 9296: train loss: 8.555275599064771e-06, val loss: 0.09001652896404266\n",
      "Epoch 9297: train loss: 7.869653927627951e-06, val loss: 0.09049830585718155\n",
      "Epoch 9298: train loss: 6.592145382455783e-06, val loss: 0.090303935110569\n",
      "Epoch 9299: train loss: 6.564296654687496e-06, val loss: 0.08959916979074478\n",
      "Epoch 9300: train loss: 5.709297056455398e-06, val loss: 0.08965795487165451\n",
      "Epoch 9301: train loss: 5.112617600389058e-06, val loss: 0.08974011987447739\n",
      "Epoch 9302: train loss: 7.16817021384486e-06, val loss: 0.08983462303876877\n",
      "Epoch 9303: train loss: 7.978100256877951e-06, val loss: 0.08915732800960541\n",
      "Epoch 9304: train loss: 1.2958952538610902e-05, val loss: 0.08951225876808167\n",
      "Epoch 9305: train loss: 2.9601389542222023e-05, val loss: 0.08853702992200851\n",
      "Epoch 9306: train loss: 7.723065209574997e-05, val loss: 0.09033966064453125\n",
      "Epoch 9307: train loss: 0.00013463295181281865, val loss: 0.08873447775840759\n",
      "Epoch 9308: train loss: 0.00018785696011036634, val loss: 0.08976972103118896\n",
      "Epoch 9309: train loss: 0.00018045352771878242, val loss: 0.08888715505599976\n",
      "Epoch 9310: train loss: 0.00010323183232685551, val loss: 0.09085167944431305\n",
      "Epoch 9311: train loss: 0.00011791873112088069, val loss: 0.09021271765232086\n",
      "Epoch 9312: train loss: 9.612215944798663e-05, val loss: 0.08884559571743011\n",
      "Epoch 9313: train loss: 6.889813084853813e-05, val loss: 0.0898771807551384\n",
      "Epoch 9314: train loss: 7.56181834731251e-05, val loss: 0.08897372335195541\n",
      "Epoch 9315: train loss: 4.139412703807466e-05, val loss: 0.08930499106645584\n",
      "Epoch 9316: train loss: 3.7305260775610805e-05, val loss: 0.08871248364448547\n",
      "Epoch 9317: train loss: 3.618357368395664e-05, val loss: 0.08848578482866287\n",
      "Epoch 9318: train loss: 4.837604137719609e-05, val loss: 0.08936946839094162\n",
      "Epoch 9319: train loss: 4.584643102134578e-05, val loss: 0.08814499527215958\n",
      "Epoch 9320: train loss: 4.324062683735974e-05, val loss: 0.08906472474336624\n",
      "Epoch 9321: train loss: 3.029369872820098e-05, val loss: 0.08838403224945068\n",
      "Epoch 9322: train loss: 2.9563330826931633e-05, val loss: 0.08688528835773468\n",
      "Epoch 9323: train loss: 2.0830922949244268e-05, val loss: 0.08863629400730133\n",
      "Epoch 9324: train loss: 1.2997724297747482e-05, val loss: 0.08749458938837051\n",
      "Epoch 9325: train loss: 1.596875154064037e-05, val loss: 0.08681833744049072\n",
      "Epoch 9326: train loss: 2.252521699119825e-05, val loss: 0.08780237287282944\n",
      "Epoch 9327: train loss: 2.5671297407825477e-05, val loss: 0.0871647521853447\n",
      "Epoch 9328: train loss: 1.506194439571118e-05, val loss: 0.08692393451929092\n",
      "Epoch 9329: train loss: 1.022669403027976e-05, val loss: 0.08677564561367035\n",
      "Epoch 9330: train loss: 1.0513606866879854e-05, val loss: 0.08678381890058517\n",
      "Epoch 9331: train loss: 2.23484148591524e-05, val loss: 0.086485356092453\n",
      "Epoch 9332: train loss: 3.086166543653235e-05, val loss: 0.08729815483093262\n",
      "Epoch 9333: train loss: 4.5298766053747386e-05, val loss: 0.08471079915761948\n",
      "Epoch 9334: train loss: 6.913405377417803e-05, val loss: 0.08756070584058762\n",
      "Epoch 9335: train loss: 0.0001248157350346446, val loss: 0.08427537977695465\n",
      "Epoch 9336: train loss: 0.00025488983374089, val loss: 0.08974285423755646\n",
      "Epoch 9337: train loss: 0.0004813229606952518, val loss: 0.08063817024230957\n",
      "Epoch 9338: train loss: 0.0008257559966295958, val loss: 0.08988148719072342\n",
      "Epoch 9339: train loss: 0.0010706206085160375, val loss: 0.08260884881019592\n",
      "Epoch 9340: train loss: 0.0006741121760569513, val loss: 0.08790715783834457\n",
      "Epoch 9341: train loss: 0.000205109769012779, val loss: 0.09252309054136276\n",
      "Epoch 9342: train loss: 0.0003210672875866294, val loss: 0.08655408024787903\n",
      "Epoch 9343: train loss: 0.0003452856617514044, val loss: 0.09025716781616211\n",
      "Epoch 9344: train loss: 5.4168242058949545e-05, val loss: 0.09463963657617569\n",
      "Epoch 9345: train loss: 0.0002501570852473378, val loss: 0.08959638327360153\n",
      "Epoch 9346: train loss: 0.0002074069925583899, val loss: 0.09097550064325333\n",
      "Epoch 9347: train loss: 0.0001615337241673842, val loss: 0.09456214308738708\n",
      "Epoch 9348: train loss: 0.00018259137868881226, val loss: 0.09352131932973862\n",
      "Epoch 9349: train loss: 6.71758025418967e-05, val loss: 0.09295754879713058\n",
      "Epoch 9350: train loss: 0.00011052576155634597, val loss: 0.09603359550237656\n",
      "Epoch 9351: train loss: 6.694958574371412e-05, val loss: 0.09818414598703384\n",
      "Epoch 9352: train loss: 0.00012636458268389106, val loss: 0.09616615623235703\n",
      "Epoch 9353: train loss: 6.858997221570462e-05, val loss: 0.09636344015598297\n",
      "Epoch 9354: train loss: 8.637762221042067e-05, val loss: 0.09798217564821243\n",
      "Epoch 9355: train loss: 2.469795072101988e-05, val loss: 0.09902467578649521\n",
      "Epoch 9356: train loss: 6.03322041570209e-05, val loss: 0.0994136780500412\n",
      "Epoch 9357: train loss: 3.374474181327969e-05, val loss: 0.09927618503570557\n",
      "Epoch 9358: train loss: 6.537712033605203e-05, val loss: 0.09961877018213272\n",
      "Epoch 9359: train loss: 3.36593875545077e-05, val loss: 0.09935358911752701\n",
      "Epoch 9360: train loss: 3.432809535297565e-05, val loss: 0.09963133186101913\n",
      "Epoch 9361: train loss: 2.0287428924348205e-05, val loss: 0.10024409741163254\n",
      "Epoch 9362: train loss: 2.612518255773466e-05, val loss: 0.10057689994573593\n",
      "Epoch 9363: train loss: 2.9658713174285367e-05, val loss: 0.10064949095249176\n",
      "Epoch 9364: train loss: 2.776698056550231e-05, val loss: 0.09966520220041275\n",
      "Epoch 9365: train loss: 2.0690951714641415e-05, val loss: 0.10001301765441895\n",
      "Epoch 9366: train loss: 1.5033696399768814e-05, val loss: 0.1010935828089714\n",
      "Epoch 9367: train loss: 1.1988013284280896e-05, val loss: 0.1009022518992424\n",
      "Epoch 9368: train loss: 1.7566015230841003e-05, val loss: 0.10014910995960236\n",
      "Epoch 9369: train loss: 1.5835406884434633e-05, val loss: 0.10046865046024323\n",
      "Epoch 9370: train loss: 1.5094851733010728e-05, val loss: 0.10150390863418579\n",
      "Epoch 9371: train loss: 1.1184724826307502e-05, val loss: 0.10038745403289795\n",
      "Epoch 9372: train loss: 5.625540325127076e-06, val loss: 0.09962312877178192\n",
      "Epoch 9373: train loss: 1.0964932698698249e-05, val loss: 0.10109062492847443\n",
      "Epoch 9374: train loss: 7.4871422839351e-06, val loss: 0.10117661952972412\n",
      "Epoch 9375: train loss: 1.1315794836264104e-05, val loss: 0.0996183231472969\n",
      "Epoch 9376: train loss: 7.323896170419175e-06, val loss: 0.09961076825857162\n",
      "Epoch 9377: train loss: 5.37477717443835e-06, val loss: 0.10096319019794464\n",
      "Epoch 9378: train loss: 4.922215794067597e-06, val loss: 0.10052955150604248\n",
      "Epoch 9379: train loss: 4.90305728817475e-06, val loss: 0.0994768887758255\n",
      "Epoch 9380: train loss: 6.0467473304015584e-06, val loss: 0.10015284270048141\n",
      "Epoch 9381: train loss: 5.094165771879489e-06, val loss: 0.1006617322564125\n",
      "Epoch 9382: train loss: 4.408618679008214e-06, val loss: 0.10034266859292984\n",
      "Epoch 9383: train loss: 3.7107713524164865e-06, val loss: 0.09961745142936707\n",
      "Epoch 9384: train loss: 2.562871486588847e-06, val loss: 0.0999687910079956\n",
      "Epoch 9385: train loss: 2.762333906503045e-06, val loss: 0.10029733180999756\n",
      "Epoch 9386: train loss: 4.140757027926156e-06, val loss: 0.09961079061031342\n",
      "Epoch 9387: train loss: 3.11229837279825e-06, val loss: 0.09998387843370438\n",
      "Epoch 9388: train loss: 3.040285264432896e-06, val loss: 0.09968852251768112\n",
      "Epoch 9389: train loss: 2.9384698336798465e-06, val loss: 0.09975282102823257\n",
      "Epoch 9390: train loss: 8.600788987678243e-07, val loss: 0.09965676069259644\n",
      "Epoch 9391: train loss: 1.6958521200649557e-06, val loss: 0.09932080656290054\n",
      "Epoch 9392: train loss: 2.754332854237873e-06, val loss: 0.10011893510818481\n",
      "Epoch 9393: train loss: 2.9020911824773066e-06, val loss: 0.09921149164438248\n",
      "Epoch 9394: train loss: 3.7234556202747626e-06, val loss: 0.0995650440454483\n",
      "Epoch 9395: train loss: 6.324209607555531e-06, val loss: 0.09937553852796555\n",
      "Epoch 9396: train loss: 1.5397288734675385e-05, val loss: 0.09973838180303574\n",
      "Epoch 9397: train loss: 4.1919534851331264e-05, val loss: 0.09765123575925827\n",
      "Epoch 9398: train loss: 0.00013013830175623298, val loss: 0.1023564264178276\n",
      "Epoch 9399: train loss: 0.0003603006771299988, val loss: 0.09547591209411621\n",
      "Epoch 9400: train loss: 0.0010644093854352832, val loss: 0.10893987864255905\n",
      "Epoch 9401: train loss: 0.001107262447476387, val loss: 0.09237416833639145\n",
      "Epoch 9402: train loss: 0.0009819762781262398, val loss: 0.10026450455188751\n",
      "Epoch 9403: train loss: 0.0005146284238435328, val loss: 0.10917909443378448\n",
      "Epoch 9404: train loss: 0.0005104459705762565, val loss: 0.10206764191389084\n",
      "Epoch 9405: train loss: 0.00030120316660031676, val loss: 0.10073833912611008\n",
      "Epoch 9406: train loss: 0.000327043206198141, val loss: 0.10423814505338669\n",
      "Epoch 9407: train loss: 0.00030829577008262277, val loss: 0.10339975357055664\n",
      "Epoch 9408: train loss: 0.0002069879847113043, val loss: 0.10072731226682663\n",
      "Epoch 9409: train loss: 0.0002744185330811888, val loss: 0.10248684883117676\n",
      "Epoch 9410: train loss: 0.0001231166097568348, val loss: 0.10305418819189072\n",
      "Epoch 9411: train loss: 0.00016306799079757184, val loss: 0.10016115754842758\n",
      "Epoch 9412: train loss: 0.00012229912681505084, val loss: 0.09785303473472595\n",
      "Epoch 9413: train loss: 0.000132062123157084, val loss: 0.0988270565867424\n",
      "Epoch 9414: train loss: 0.00011542357242433354, val loss: 0.10039430111646652\n",
      "Epoch 9415: train loss: 8.354902092833072e-05, val loss: 0.10034763813018799\n",
      "Epoch 9416: train loss: 8.578230335842818e-05, val loss: 0.10014941543340683\n",
      "Epoch 9417: train loss: 5.1863233238691464e-05, val loss: 0.09981205314397812\n",
      "Epoch 9418: train loss: 8.207844075514004e-05, val loss: 0.09787571430206299\n",
      "Epoch 9419: train loss: 5.414173210738227e-05, val loss: 0.09687650203704834\n",
      "Epoch 9420: train loss: 5.6993725593201816e-05, val loss: 0.09807351976633072\n",
      "Epoch 9421: train loss: 4.221911513013765e-05, val loss: 0.10032415390014648\n",
      "Epoch 9422: train loss: 2.9707111025345512e-05, val loss: 0.100145123898983\n",
      "Epoch 9423: train loss: 4.121997699257918e-05, val loss: 0.09763221442699432\n",
      "Epoch 9424: train loss: 3.4519001928856596e-05, val loss: 0.09676662087440491\n",
      "Epoch 9425: train loss: 3.062680843868293e-05, val loss: 0.09774713963270187\n",
      "Epoch 9426: train loss: 2.7571411919780076e-05, val loss: 0.09920074790716171\n",
      "Epoch 9427: train loss: 1.848463034548331e-05, val loss: 0.09965922683477402\n",
      "Epoch 9428: train loss: 2.220215719717089e-05, val loss: 0.09922429174184799\n",
      "Epoch 9429: train loss: 1.92069146578433e-05, val loss: 0.099271260201931\n",
      "Epoch 9430: train loss: 2.204997872468084e-05, val loss: 0.09853743761777878\n",
      "Epoch 9431: train loss: 1.6096983017632738e-05, val loss: 0.09768456220626831\n",
      "Epoch 9432: train loss: 1.1499543688842095e-05, val loss: 0.09695041924715042\n",
      "Epoch 9433: train loss: 1.4248232218960766e-05, val loss: 0.09731069207191467\n",
      "Epoch 9434: train loss: 9.272267561755143e-06, val loss: 0.09842965751886368\n",
      "Epoch 9435: train loss: 1.3724101336265448e-05, val loss: 0.09828568994998932\n",
      "Epoch 9436: train loss: 1.072002669388894e-05, val loss: 0.09783449023962021\n",
      "Epoch 9437: train loss: 9.182237590721343e-06, val loss: 0.09742264449596405\n",
      "Epoch 9438: train loss: 8.182971214409918e-06, val loss: 0.09817291796207428\n",
      "Epoch 9439: train loss: 7.603076937812148e-06, val loss: 0.09783022850751877\n",
      "Epoch 9440: train loss: 6.804819804528961e-06, val loss: 0.09726065397262573\n",
      "Epoch 9441: train loss: 5.932003659836482e-06, val loss: 0.09716319292783737\n",
      "Epoch 9442: train loss: 6.06482581133605e-06, val loss: 0.09762939065694809\n",
      "Epoch 9443: train loss: 4.606339189194841e-06, val loss: 0.09743054956197739\n",
      "Epoch 9444: train loss: 4.553548023977783e-06, val loss: 0.09641364961862564\n",
      "Epoch 9445: train loss: 5.453485755424481e-06, val loss: 0.09734395891427994\n",
      "Epoch 9446: train loss: 6.092200692364713e-06, val loss: 0.09730377048254013\n",
      "Epoch 9447: train loss: 9.403706826560665e-06, val loss: 0.09781002998352051\n",
      "Epoch 9448: train loss: 1.4806915714871138e-05, val loss: 0.09550994634628296\n",
      "Epoch 9449: train loss: 3.060227754758671e-05, val loss: 0.09730838984251022\n",
      "Epoch 9450: train loss: 2.2696440282743424e-05, val loss: 0.0953308567404747\n",
      "Epoch 9451: train loss: 4.7461013309657574e-05, val loss: 0.09768380969762802\n",
      "Epoch 9452: train loss: 6.485362973762676e-05, val loss: 0.09353281557559967\n",
      "Epoch 9453: train loss: 0.00012372212950140238, val loss: 0.09862162917852402\n",
      "Epoch 9454: train loss: 0.0001351744867861271, val loss: 0.09353017807006836\n",
      "Epoch 9455: train loss: 8.032769983401522e-05, val loss: 0.09617620706558228\n",
      "Epoch 9456: train loss: 1.5173389328992926e-05, val loss: 0.09582049399614334\n",
      "Epoch 9457: train loss: 1.4799244127061684e-05, val loss: 0.09299244731664658\n",
      "Epoch 9458: train loss: 5.661989780492149e-05, val loss: 0.0959271788597107\n",
      "Epoch 9459: train loss: 7.920677307993174e-05, val loss: 0.09231200069189072\n",
      "Epoch 9460: train loss: 5.6752942327875644e-05, val loss: 0.09470167011022568\n",
      "Epoch 9461: train loss: 1.5099707525223494e-05, val loss: 0.09351110458374023\n",
      "Epoch 9462: train loss: 6.1071632444509305e-06, val loss: 0.09173816442489624\n",
      "Epoch 9463: train loss: 2.6405981770949438e-05, val loss: 0.0938074141740799\n",
      "Epoch 9464: train loss: 4.7165998694254085e-05, val loss: 0.09047847241163254\n",
      "Epoch 9465: train loss: 3.8861169741721824e-05, val loss: 0.09221623092889786\n",
      "Epoch 9466: train loss: 1.7115784430643544e-05, val loss: 0.09129571169614792\n",
      "Epoch 9467: train loss: 2.4900461994548095e-06, val loss: 0.09040170162916183\n",
      "Epoch 9468: train loss: 1.0747300621005706e-05, val loss: 0.09170560538768768\n",
      "Epoch 9469: train loss: 2.5048260795301758e-05, val loss: 0.08952897787094116\n",
      "Epoch 9470: train loss: 3.0535684345522895e-05, val loss: 0.09179500490427017\n",
      "Epoch 9471: train loss: 2.2492933567264117e-05, val loss: 0.08959531784057617\n",
      "Epoch 9472: train loss: 9.996525477617979e-06, val loss: 0.0896107479929924\n",
      "Epoch 9473: train loss: 3.510070428092149e-06, val loss: 0.09050213545560837\n",
      "Epoch 9474: train loss: 7.521919542341493e-06, val loss: 0.0887371078133583\n",
      "Epoch 9475: train loss: 2.57474875979824e-05, val loss: 0.09204349666833878\n",
      "Epoch 9476: train loss: 7.131858001230285e-05, val loss: 0.08678507059812546\n",
      "Epoch 9477: train loss: 0.00023008191783446819, val loss: 0.09441956132650375\n",
      "Epoch 9478: train loss: 0.0007208008319139481, val loss: 0.08555832505226135\n",
      "Epoch 9479: train loss: 0.0016203155973926187, val loss: 0.10215099155902863\n",
      "Epoch 9480: train loss: 0.0024276827462017536, val loss: 0.09058652073144913\n",
      "Epoch 9481: train loss: 0.00073622987838462, val loss: 0.09591501951217651\n",
      "Epoch 9482: train loss: 0.0006385913584381342, val loss: 0.10325147211551666\n",
      "Epoch 9483: train loss: 0.0007339227595366538, val loss: 0.09552310407161713\n",
      "Epoch 9484: train loss: 0.0003809741174336523, val loss: 0.09587991237640381\n",
      "Epoch 9485: train loss: 0.0004225884040351957, val loss: 0.10206224769353867\n",
      "Epoch 9486: train loss: 0.00033435862860642374, val loss: 0.09632883220911026\n",
      "Epoch 9487: train loss: 0.0003370799240656197, val loss: 0.0921245738863945\n",
      "Epoch 9488: train loss: 0.0001632341300137341, val loss: 0.09557536989450455\n",
      "Epoch 9489: train loss: 0.000286361260805279, val loss: 0.09802699089050293\n",
      "Epoch 9490: train loss: 0.0001339772279607132, val loss: 0.09398508071899414\n",
      "Epoch 9491: train loss: 0.0001773891126504168, val loss: 0.09158382564783096\n",
      "Epoch 9492: train loss: 0.00015136910951696336, val loss: 0.09461705386638641\n",
      "Epoch 9493: train loss: 0.00012638945190701634, val loss: 0.09558596462011337\n",
      "Epoch 9494: train loss: 0.00012155594595242292, val loss: 0.09261801093816757\n",
      "Epoch 9495: train loss: 6.282139656832442e-05, val loss: 0.09029819071292877\n",
      "Epoch 9496: train loss: 0.00012318456720095128, val loss: 0.09142464399337769\n",
      "Epoch 9497: train loss: 6.0508245951496065e-05, val loss: 0.09313258528709412\n",
      "Epoch 9498: train loss: 6.876029510749504e-05, val loss: 0.09191573411226273\n",
      "Epoch 9499: train loss: 6.502254109364003e-05, val loss: 0.09062942117452621\n",
      "Epoch 9500: train loss: 5.470507312566042e-05, val loss: 0.09035374969244003\n",
      "Epoch 9501: train loss: 4.8542726290179417e-05, val loss: 0.09019752591848373\n",
      "Epoch 9502: train loss: 3.523656778270379e-05, val loss: 0.08952992409467697\n",
      "Epoch 9503: train loss: 4.1950617742259055e-05, val loss: 0.08970440924167633\n",
      "Epoch 9504: train loss: 4.0221610106527805e-05, val loss: 0.09075188636779785\n",
      "Epoch 9505: train loss: 2.4831611881381832e-05, val loss: 0.09089615195989609\n",
      "Epoch 9506: train loss: 3.239252691855654e-05, val loss: 0.0900503545999527\n",
      "Epoch 9507: train loss: 2.299970219610259e-05, val loss: 0.08946692943572998\n",
      "Epoch 9508: train loss: 1.9995759430457838e-05, val loss: 0.08944783359766006\n",
      "Epoch 9509: train loss: 2.5841496608336456e-05, val loss: 0.08938354253768921\n",
      "Epoch 9510: train loss: 1.224183233716758e-05, val loss: 0.089024618268013\n",
      "Epoch 9511: train loss: 2.248190503451042e-05, val loss: 0.0892701968550682\n",
      "Epoch 9512: train loss: 1.4201586054696236e-05, val loss: 0.08974506705999374\n",
      "Epoch 9513: train loss: 1.1782059118559118e-05, val loss: 0.0895942971110344\n",
      "Epoch 9514: train loss: 1.298684310313547e-05, val loss: 0.08905022591352463\n",
      "Epoch 9515: train loss: 1.0674380064301658e-05, val loss: 0.0888829454779625\n",
      "Epoch 9516: train loss: 1.2089599294995423e-05, val loss: 0.08913924545049667\n",
      "Epoch 9517: train loss: 8.329238880833145e-06, val loss: 0.08918335288763046\n",
      "Epoch 9518: train loss: 8.289507604786195e-06, val loss: 0.08938661962747574\n",
      "Epoch 9519: train loss: 9.450538527744357e-06, val loss: 0.0895310565829277\n",
      "Epoch 9520: train loss: 3.960602498409571e-06, val loss: 0.08892698585987091\n",
      "Epoch 9521: train loss: 7.932780135888606e-06, val loss: 0.08821127563714981\n",
      "Epoch 9522: train loss: 6.419757937692339e-06, val loss: 0.08847711235284805\n",
      "Epoch 9523: train loss: 4.333337983553065e-06, val loss: 0.08920202404260635\n",
      "Epoch 9524: train loss: 5.7449060477665626e-06, val loss: 0.08888943493366241\n",
      "Epoch 9525: train loss: 3.6218859804648673e-06, val loss: 0.08829563111066818\n",
      "Epoch 9526: train loss: 4.67805466541904e-06, val loss: 0.08837266266345978\n",
      "Epoch 9527: train loss: 2.551965962993563e-06, val loss: 0.08841274678707123\n",
      "Epoch 9528: train loss: 4.577559138851939e-06, val loss: 0.08827488869428635\n",
      "Epoch 9529: train loss: 2.4326429866050603e-06, val loss: 0.08846980333328247\n",
      "Epoch 9530: train loss: 3.227965407859301e-06, val loss: 0.08835206180810928\n",
      "Epoch 9531: train loss: 2.4340154141100356e-06, val loss: 0.0876481905579567\n",
      "Epoch 9532: train loss: 2.5230360733985435e-06, val loss: 0.08779238909482956\n",
      "Epoch 9533: train loss: 1.7278651966989855e-06, val loss: 0.08830630779266357\n",
      "Epoch 9534: train loss: 2.5224080673069693e-06, val loss: 0.08793633431196213\n",
      "Epoch 9535: train loss: 1.645532847760478e-06, val loss: 0.08772210776805878\n",
      "Epoch 9536: train loss: 1.956625965249259e-06, val loss: 0.08769189566373825\n",
      "Epoch 9537: train loss: 1.5371340396086453e-06, val loss: 0.08764874190092087\n",
      "Epoch 9538: train loss: 1.5378111584141152e-06, val loss: 0.08768836408853531\n",
      "Epoch 9539: train loss: 1.3132207641319837e-06, val loss: 0.08718544989824295\n",
      "Epoch 9540: train loss: 1.4101093483986915e-06, val loss: 0.08715512603521347\n",
      "Epoch 9541: train loss: 8.511102009833849e-07, val loss: 0.08742480725049973\n",
      "Epoch 9542: train loss: 1.5691887256252812e-06, val loss: 0.08713674545288086\n",
      "Epoch 9543: train loss: 8.73081489771721e-07, val loss: 0.08724662661552429\n",
      "Epoch 9544: train loss: 1.2332651522228844e-06, val loss: 0.08679132908582687\n",
      "Epoch 9545: train loss: 8.627446845821396e-07, val loss: 0.08700909465551376\n",
      "Epoch 9546: train loss: 5.818031354465347e-07, val loss: 0.0869300365447998\n",
      "Epoch 9547: train loss: 1.0435331887492794e-06, val loss: 0.086676687002182\n",
      "Epoch 9548: train loss: 9.472497595197638e-07, val loss: 0.08688712120056152\n",
      "Epoch 9549: train loss: 8.767951271693164e-07, val loss: 0.08684329688549042\n",
      "Epoch 9550: train loss: 1.3892295100959018e-06, val loss: 0.08630423992872238\n",
      "Epoch 9551: train loss: 3.2622681374050444e-06, val loss: 0.0870811939239502\n",
      "Epoch 9552: train loss: 1.1112291758763604e-05, val loss: 0.08555841445922852\n",
      "Epoch 9553: train loss: 4.998492659069598e-05, val loss: 0.0894865170121193\n",
      "Epoch 9554: train loss: 0.00020687216601800174, val loss: 0.07991154491901398\n",
      "Epoch 9555: train loss: 0.0008466412546113133, val loss: 0.10229168087244034\n",
      "Epoch 9556: train loss: 0.0015600865008309484, val loss: 0.07522469013929367\n",
      "Epoch 9557: train loss: 0.002173260785639286, val loss: 0.07977939397096634\n",
      "Epoch 9558: train loss: 0.000845856498926878, val loss: 0.0898512527346611\n",
      "Epoch 9559: train loss: 0.0009581830236129463, val loss: 0.08655060082674026\n",
      "Epoch 9560: train loss: 0.0006424211314879358, val loss: 0.08135491609573364\n",
      "Epoch 9561: train loss: 0.000509042467456311, val loss: 0.08377581834793091\n",
      "Epoch 9562: train loss: 0.0004507506964728236, val loss: 0.08929478377103806\n",
      "Epoch 9563: train loss: 0.0004739357391372323, val loss: 0.08878938108682632\n",
      "Epoch 9564: train loss: 0.00028442489565350115, val loss: 0.08325125277042389\n",
      "Epoch 9565: train loss: 0.0003499332524370402, val loss: 0.08014219254255295\n",
      "Epoch 9566: train loss: 0.00025680093676783144, val loss: 0.08290012180805206\n",
      "Epoch 9567: train loss: 0.00015760883979965, val loss: 0.08705507963895798\n",
      "Epoch 9568: train loss: 0.00020572390349116176, val loss: 0.0864749327301979\n",
      "Epoch 9569: train loss: 0.00020996738749090582, val loss: 0.08260904252529144\n",
      "Epoch 9570: train loss: 0.000134544083266519, val loss: 0.0808543860912323\n",
      "Epoch 9571: train loss: 0.00015267542039509863, val loss: 0.08161081373691559\n",
      "Epoch 9572: train loss: 9.8548480309546e-05, val loss: 0.08145655691623688\n",
      "Epoch 9573: train loss: 0.00010025803931057453, val loss: 0.07955177873373032\n",
      "Epoch 9574: train loss: 0.00010458616452524439, val loss: 0.0787682756781578\n",
      "Epoch 9575: train loss: 8.555503154639155e-05, val loss: 0.07990408688783646\n",
      "Epoch 9576: train loss: 7.964583346620202e-05, val loss: 0.08167897909879684\n",
      "Epoch 9577: train loss: 6.690784357488155e-05, val loss: 0.08210080116987228\n",
      "Epoch 9578: train loss: 5.77197533857543e-05, val loss: 0.0809716209769249\n",
      "Epoch 9579: train loss: 4.5854678319301456e-05, val loss: 0.07907126098871231\n",
      "Epoch 9580: train loss: 4.9707316065905616e-05, val loss: 0.07764255255460739\n",
      "Epoch 9581: train loss: 5.466973743750714e-05, val loss: 0.07813286036252975\n",
      "Epoch 9582: train loss: 3.638265116023831e-05, val loss: 0.0799688920378685\n",
      "Epoch 9583: train loss: 3.068789737881161e-05, val loss: 0.08125071227550507\n",
      "Epoch 9584: train loss: 2.8669457606156357e-05, val loss: 0.08064230531454086\n",
      "Epoch 9585: train loss: 2.6146561140194535e-05, val loss: 0.0789557471871376\n",
      "Epoch 9586: train loss: 3.262684913352132e-05, val loss: 0.07843247801065445\n",
      "Epoch 9587: train loss: 2.1912341253482737e-05, val loss: 0.07904386520385742\n",
      "Epoch 9588: train loss: 1.8271604858455248e-05, val loss: 0.07943642139434814\n",
      "Epoch 9589: train loss: 2.0231651433277875e-05, val loss: 0.07897114008665085\n",
      "Epoch 9590: train loss: 1.4201364137989003e-05, val loss: 0.07863124459981918\n",
      "Epoch 9591: train loss: 1.6027452147682197e-05, val loss: 0.0791744813323021\n",
      "Epoch 9592: train loss: 1.7667511201580055e-05, val loss: 0.0795041099190712\n",
      "Epoch 9593: train loss: 1.226507083629258e-05, val loss: 0.07887906581163406\n",
      "Epoch 9594: train loss: 8.829715625324752e-06, val loss: 0.0776747465133667\n",
      "Epoch 9595: train loss: 9.695873814052902e-06, val loss: 0.07701631635427475\n",
      "Epoch 9596: train loss: 1.0048546755569987e-05, val loss: 0.07744630426168442\n",
      "Epoch 9597: train loss: 9.620810487831477e-06, val loss: 0.07832616567611694\n",
      "Epoch 9598: train loss: 8.394141332246363e-06, val loss: 0.07870807498693466\n",
      "Epoch 9599: train loss: 5.81260928811389e-06, val loss: 0.07817794382572174\n",
      "Epoch 9600: train loss: 6.528170615638373e-06, val loss: 0.07760527729988098\n",
      "Epoch 9601: train loss: 5.526657787413569e-06, val loss: 0.07771987468004227\n",
      "Epoch 9602: train loss: 4.768302005686564e-06, val loss: 0.07796964794397354\n",
      "Epoch 9603: train loss: 6.266288892220473e-06, val loss: 0.07779674977064133\n",
      "Epoch 9604: train loss: 4.1431712816120125e-06, val loss: 0.07745935767889023\n",
      "Epoch 9605: train loss: 3.6216956686985213e-06, val loss: 0.07750158756971359\n",
      "Epoch 9606: train loss: 3.2108325740409782e-06, val loss: 0.07773922383785248\n",
      "Epoch 9607: train loss: 2.8351489618216874e-06, val loss: 0.07765759527683258\n",
      "Epoch 9608: train loss: 4.2102001316379756e-06, val loss: 0.07719120383262634\n",
      "Epoch 9609: train loss: 2.4634671262901975e-06, val loss: 0.07667695730924606\n",
      "Epoch 9610: train loss: 2.360510279686423e-06, val loss: 0.07689185440540314\n",
      "Epoch 9611: train loss: 2.2022431949153543e-06, val loss: 0.07743658870458603\n",
      "Epoch 9612: train loss: 2.0062350358784897e-06, val loss: 0.07732173055410385\n",
      "Epoch 9613: train loss: 1.8251225810672622e-06, val loss: 0.07683858275413513\n",
      "Epoch 9614: train loss: 1.884937091745087e-06, val loss: 0.07669074833393097\n",
      "Epoch 9615: train loss: 1.8450383549861726e-06, val loss: 0.07676716893911362\n",
      "Epoch 9616: train loss: 1.5394440424643108e-06, val loss: 0.07672635465860367\n",
      "Epoch 9617: train loss: 1.2315915682847844e-06, val loss: 0.07649414986371994\n",
      "Epoch 9618: train loss: 9.50833225488168e-07, val loss: 0.0764322504401207\n",
      "Epoch 9619: train loss: 1.3244464298622916e-06, val loss: 0.07673940807580948\n",
      "Epoch 9620: train loss: 1.304575221183768e-06, val loss: 0.07675515860319138\n",
      "Epoch 9621: train loss: 1.2073023754055612e-06, val loss: 0.07656127959489822\n",
      "Epoch 9622: train loss: 1.1629498430920648e-06, val loss: 0.07634350657463074\n",
      "Epoch 9623: train loss: 1.395099047840631e-06, val loss: 0.07640963047742844\n",
      "Epoch 9624: train loss: 1.4309930520539638e-06, val loss: 0.07633687555789948\n",
      "Epoch 9625: train loss: 2.077396175081958e-06, val loss: 0.07652192562818527\n",
      "Epoch 9626: train loss: 4.1308949221274815e-06, val loss: 0.07621679455041885\n",
      "Epoch 9627: train loss: 1.097704534913646e-05, val loss: 0.07620324939489365\n",
      "Epoch 9628: train loss: 3.422361260163598e-05, val loss: 0.07549986243247986\n",
      "Epoch 9629: train loss: 0.00010716227552620694, val loss: 0.07739605754613876\n",
      "Epoch 9630: train loss: 0.00033461020211689174, val loss: 0.07444828748703003\n",
      "Epoch 9631: train loss: 0.0010164141422137618, val loss: 0.0808810144662857\n",
      "Epoch 9632: train loss: 0.0018278030911460519, val loss: 0.07481441646814346\n",
      "Epoch 9633: train loss: 0.0011917836964130402, val loss: 0.0806986466050148\n",
      "Epoch 9634: train loss: 0.0002211435785284266, val loss: 0.08870086818933487\n",
      "Epoch 9635: train loss: 0.000805385410785675, val loss: 0.0816471129655838\n",
      "Epoch 9636: train loss: 0.00027848995523527265, val loss: 0.08019528537988663\n",
      "Epoch 9637: train loss: 0.0002879849052987993, val loss: 0.0832153707742691\n",
      "Epoch 9638: train loss: 0.00031383332679979503, val loss: 0.08683832734823227\n",
      "Epoch 9639: train loss: 0.00018007411563303322, val loss: 0.08848794549703598\n",
      "Epoch 9640: train loss: 0.00027070139185525477, val loss: 0.08820877224206924\n",
      "Epoch 9641: train loss: 0.00014633774117100984, val loss: 0.08848617225885391\n",
      "Epoch 9642: train loss: 0.00014627189375460148, val loss: 0.08808708190917969\n",
      "Epoch 9643: train loss: 0.00012001103459624574, val loss: 0.08864924311637878\n",
      "Epoch 9644: train loss: 0.00012186045933049172, val loss: 0.0909031331539154\n",
      "Epoch 9645: train loss: 0.00012195404269732535, val loss: 0.0922025814652443\n",
      "Epoch 9646: train loss: 9.887800115393475e-05, val loss: 0.0921374186873436\n",
      "Epoch 9647: train loss: 7.532881863880903e-05, val loss: 0.09235477447509766\n",
      "Epoch 9648: train loss: 7.880457997089252e-05, val loss: 0.09424126893281937\n",
      "Epoch 9649: train loss: 5.1752391300396994e-05, val loss: 0.09567561745643616\n",
      "Epoch 9650: train loss: 7.882990757934749e-05, val loss: 0.09404821693897247\n",
      "Epoch 9651: train loss: 5.468253220897168e-05, val loss: 0.09284796565771103\n",
      "Epoch 9652: train loss: 5.265898653306067e-05, val loss: 0.0942671075463295\n",
      "Epoch 9653: train loss: 3.347183155710809e-05, val loss: 0.09682514518499374\n",
      "Epoch 9654: train loss: 4.247564356774092e-05, val loss: 0.09690071642398834\n",
      "Epoch 9655: train loss: 3.756336082005873e-05, val loss: 0.0943296030163765\n",
      "Epoch 9656: train loss: 3.5737270081881434e-05, val loss: 0.09317025542259216\n",
      "Epoch 9657: train loss: 2.8725999072776176e-05, val loss: 0.09446138143539429\n",
      "Epoch 9658: train loss: 2.6551662813290022e-05, val loss: 0.09637733548879623\n",
      "Epoch 9659: train loss: 1.8787024600896984e-05, val loss: 0.09662061929702759\n",
      "Epoch 9660: train loss: 2.3906244678073563e-05, val loss: 0.09522649645805359\n",
      "Epoch 9661: train loss: 2.0642231902456842e-05, val loss: 0.09451448172330856\n",
      "Epoch 9662: train loss: 2.0170198695268482e-05, val loss: 0.09492003172636032\n",
      "Epoch 9663: train loss: 1.398226959281601e-05, val loss: 0.09571458399295807\n",
      "Epoch 9664: train loss: 1.307290767726954e-05, val loss: 0.09560715407133102\n",
      "Epoch 9665: train loss: 1.3378729818214197e-05, val loss: 0.0947103202342987\n",
      "Epoch 9666: train loss: 1.2933489415445365e-05, val loss: 0.09432279318571091\n",
      "Epoch 9667: train loss: 1.2055947991029825e-05, val loss: 0.09458666294813156\n",
      "Epoch 9668: train loss: 1.031393367156852e-05, val loss: 0.0953017920255661\n",
      "Epoch 9669: train loss: 7.020476459729252e-06, val loss: 0.09567631781101227\n",
      "Epoch 9670: train loss: 9.192574907501694e-06, val loss: 0.09566111117601395\n",
      "Epoch 9671: train loss: 6.1503155848186e-06, val loss: 0.09563011676073074\n",
      "Epoch 9672: train loss: 1.01572049970855e-05, val loss: 0.09535461664199829\n",
      "Epoch 9673: train loss: 4.22560742663336e-06, val loss: 0.09520023316144943\n",
      "Epoch 9674: train loss: 7.189901680249022e-06, val loss: 0.09524639695882797\n",
      "Epoch 9675: train loss: 4.015520062239375e-06, val loss: 0.095627062022686\n",
      "Epoch 9676: train loss: 5.017677722207736e-06, val loss: 0.09607436507940292\n",
      "Epoch 9677: train loss: 4.545322099147597e-06, val loss: 0.09587683528661728\n",
      "Epoch 9678: train loss: 5.053561380918836e-06, val loss: 0.09551217406988144\n",
      "Epoch 9679: train loss: 2.9941645607323153e-06, val loss: 0.09563878923654556\n",
      "Epoch 9680: train loss: 3.438939074840164e-06, val loss: 0.09601529687643051\n",
      "Epoch 9681: train loss: 2.9140981041564373e-06, val loss: 0.09624432772397995\n",
      "Epoch 9682: train loss: 2.53929852078727e-06, val loss: 0.09638720750808716\n",
      "Epoch 9683: train loss: 3.1415684134117328e-06, val loss: 0.09615346044301987\n",
      "Epoch 9684: train loss: 2.3097434223018354e-06, val loss: 0.09565632790327072\n",
      "Epoch 9685: train loss: 2.531466179789277e-06, val loss: 0.09617619961500168\n",
      "Epoch 9686: train loss: 1.04848004411906e-06, val loss: 0.09654746949672699\n",
      "Epoch 9687: train loss: 2.356979848627816e-06, val loss: 0.09602329134941101\n",
      "Epoch 9688: train loss: 2.1966752683511004e-06, val loss: 0.09629115462303162\n",
      "Epoch 9689: train loss: 1.4221493529475993e-06, val loss: 0.09648024290800095\n",
      "Epoch 9690: train loss: 1.5074791690494749e-06, val loss: 0.09628285467624664\n",
      "Epoch 9691: train loss: 1.385270593345922e-06, val loss: 0.09661270678043365\n",
      "Epoch 9692: train loss: 1.2707271253020735e-06, val loss: 0.09646530449390411\n",
      "Epoch 9693: train loss: 1.1725012427632464e-06, val loss: 0.09606321901082993\n",
      "Epoch 9694: train loss: 1.991780209209537e-06, val loss: 0.0963171049952507\n",
      "Epoch 9695: train loss: 2.578708517830819e-06, val loss: 0.09641342610120773\n",
      "Epoch 9696: train loss: 3.4571264677651925e-06, val loss: 0.09623514860868454\n",
      "Epoch 9697: train loss: 6.277750799199566e-06, val loss: 0.09603806585073471\n",
      "Epoch 9698: train loss: 1.4686898794025183e-05, val loss: 0.09593965858221054\n",
      "Epoch 9699: train loss: 4.4596123188966885e-05, val loss: 0.09637806564569473\n",
      "Epoch 9700: train loss: 0.00014988600742071867, val loss: 0.0956529751420021\n",
      "Epoch 9701: train loss: 0.0005734617006964982, val loss: 0.09794794768095016\n",
      "Epoch 9702: train loss: 0.0015239394269883633, val loss: 0.08912650495767593\n",
      "Epoch 9703: train loss: 0.0024083913303911686, val loss: 0.0986589714884758\n",
      "Epoch 9704: train loss: 0.0005824294639751315, val loss: 0.10230503231287003\n",
      "Epoch 9705: train loss: 0.0008212075917981565, val loss: 0.09176211804151535\n",
      "Epoch 9706: train loss: 0.0007053534500300884, val loss: 0.09022369235754013\n",
      "Epoch 9707: train loss: 0.0004125736013520509, val loss: 0.09536351263523102\n",
      "Epoch 9708: train loss: 0.0004106756823603064, val loss: 0.09534969925880432\n",
      "Epoch 9709: train loss: 0.00043262020335532725, val loss: 0.09096810221672058\n",
      "Epoch 9710: train loss: 0.00017612939700484276, val loss: 0.0888419970870018\n",
      "Epoch 9711: train loss: 0.0003581828495953232, val loss: 0.0895635113120079\n",
      "Epoch 9712: train loss: 0.0001884130178950727, val loss: 0.09096091985702515\n",
      "Epoch 9713: train loss: 0.0001394652499584481, val loss: 0.09030839055776596\n",
      "Epoch 9714: train loss: 0.0002059369726339355, val loss: 0.08735515177249908\n",
      "Epoch 9715: train loss: 0.0001473691954743117, val loss: 0.08469773828983307\n",
      "Epoch 9716: train loss: 9.837142715696245e-05, val loss: 0.0827256441116333\n",
      "Epoch 9717: train loss: 0.00013646692968904972, val loss: 0.08290504664182663\n",
      "Epoch 9718: train loss: 0.0001016726964735426, val loss: 0.08544065058231354\n",
      "Epoch 9719: train loss: 5.8878733398159966e-05, val loss: 0.0881355032324791\n",
      "Epoch 9720: train loss: 9.215556929120794e-05, val loss: 0.08921349048614502\n",
      "Epoch 9721: train loss: 7.239562546601519e-05, val loss: 0.08854243904352188\n",
      "Epoch 9722: train loss: 6.139362085377797e-05, val loss: 0.08708827942609787\n",
      "Epoch 9723: train loss: 5.427419091574848e-05, val loss: 0.0852125883102417\n",
      "Epoch 9724: train loss: 5.04991585330572e-05, val loss: 0.0840676948428154\n",
      "Epoch 9725: train loss: 3.9693019061814994e-05, val loss: 0.08488578349351883\n",
      "Epoch 9726: train loss: 4.677486140280962e-05, val loss: 0.08646456152200699\n",
      "Epoch 9727: train loss: 3.3000134862959385e-05, val loss: 0.08718366920948029\n",
      "Epoch 9728: train loss: 3.64040897693485e-05, val loss: 0.08622726052999496\n",
      "Epoch 9729: train loss: 2.7349680749466643e-05, val loss: 0.08541205525398254\n",
      "Epoch 9730: train loss: 2.6436466214363463e-05, val loss: 0.08621149510145187\n",
      "Epoch 9731: train loss: 2.4609662432339974e-05, val loss: 0.08748238533735275\n",
      "Epoch 9732: train loss: 1.9650749891297892e-05, val loss: 0.08761497586965561\n",
      "Epoch 9733: train loss: 2.1003312212997116e-05, val loss: 0.0863683894276619\n",
      "Epoch 9734: train loss: 1.94307322090026e-05, val loss: 0.08551458269357681\n",
      "Epoch 9735: train loss: 1.4028437362867408e-05, val loss: 0.08609059453010559\n",
      "Epoch 9736: train loss: 1.5001289284555241e-05, val loss: 0.0871322974562645\n",
      "Epoch 9737: train loss: 1.2894669453089591e-05, val loss: 0.08743641525506973\n",
      "Epoch 9738: train loss: 1.1942572200496215e-05, val loss: 0.08706261962652206\n",
      "Epoch 9739: train loss: 1.0995568118232768e-05, val loss: 0.08695260435342789\n",
      "Epoch 9740: train loss: 1.1921519217139576e-05, val loss: 0.08732008188962936\n",
      "Epoch 9741: train loss: 6.243817551876418e-06, val loss: 0.08761417865753174\n",
      "Epoch 9742: train loss: 8.943578905018512e-06, val loss: 0.08719300478696823\n",
      "Epoch 9743: train loss: 7.76181241235463e-06, val loss: 0.0866154357790947\n",
      "Epoch 9744: train loss: 5.692045760952169e-06, val loss: 0.08666933327913284\n",
      "Epoch 9745: train loss: 6.89639318807167e-06, val loss: 0.08761897683143616\n",
      "Epoch 9746: train loss: 5.197602604312124e-06, val loss: 0.08819364756345749\n",
      "Epoch 9747: train loss: 5.2831533139396925e-06, val loss: 0.08755194395780563\n",
      "Epoch 9748: train loss: 4.474932666198583e-06, val loss: 0.08686171472072601\n",
      "Epoch 9749: train loss: 3.257090838815202e-06, val loss: 0.08712642639875412\n",
      "Epoch 9750: train loss: 4.576856099447468e-06, val loss: 0.08762215822935104\n",
      "Epoch 9751: train loss: 3.2792947877169354e-06, val loss: 0.08765070885419846\n",
      "Epoch 9752: train loss: 3.047478912776569e-06, val loss: 0.08747664839029312\n",
      "Epoch 9753: train loss: 3.1092631616047584e-06, val loss: 0.08734879642724991\n",
      "Epoch 9754: train loss: 2.438129740767181e-06, val loss: 0.08701347559690475\n",
      "Epoch 9755: train loss: 2.2742860892321914e-06, val loss: 0.08677320927381516\n",
      "Epoch 9756: train loss: 2.326878529856913e-06, val loss: 0.08713492006063461\n",
      "Epoch 9757: train loss: 1.8520869389249128e-06, val loss: 0.08764412254095078\n",
      "Epoch 9758: train loss: 2.198201627834351e-06, val loss: 0.08742667734622955\n",
      "Epoch 9759: train loss: 1.6369268678317894e-06, val loss: 0.08681751042604446\n",
      "Epoch 9760: train loss: 1.495287278885371e-06, val loss: 0.0869007557630539\n",
      "Epoch 9761: train loss: 1.526823325548321e-06, val loss: 0.0872938260436058\n",
      "Epoch 9762: train loss: 1.1786881941588945e-06, val loss: 0.08727364987134933\n",
      "Epoch 9763: train loss: 1.5103818213901832e-06, val loss: 0.08690750598907471\n",
      "Epoch 9764: train loss: 9.660647037890158e-07, val loss: 0.08666442334651947\n",
      "Epoch 9765: train loss: 1.2576765584526584e-06, val loss: 0.08695942163467407\n",
      "Epoch 9766: train loss: 9.491679406892217e-07, val loss: 0.08687062561511993\n",
      "Epoch 9767: train loss: 1.1471606740087736e-06, val loss: 0.08669152110815048\n",
      "Epoch 9768: train loss: 1.6802447362351813e-06, val loss: 0.08674868196249008\n",
      "Epoch 9769: train loss: 5.374449756345712e-06, val loss: 0.08654124289751053\n",
      "Epoch 9770: train loss: 2.6682182578952052e-05, val loss: 0.08660495281219482\n",
      "Epoch 9771: train loss: 0.00011428991274442524, val loss: 0.08731424063444138\n",
      "Epoch 9772: train loss: 0.0002695698931347579, val loss: 0.08626203238964081\n",
      "Epoch 9773: train loss: 0.0002135040267603472, val loss: 0.09062966704368591\n",
      "Epoch 9774: train loss: 0.0001802268234314397, val loss: 0.08726345747709274\n",
      "Epoch 9775: train loss: 0.00015010063361842185, val loss: 0.08881368488073349\n",
      "Epoch 9776: train loss: 7.090609869919717e-05, val loss: 0.09225254505872726\n",
      "Epoch 9777: train loss: 6.633271550526842e-05, val loss: 0.08922936022281647\n",
      "Epoch 9778: train loss: 0.0001001495256787166, val loss: 0.08972073346376419\n",
      "Epoch 9779: train loss: 6.893065437907353e-05, val loss: 0.09285487979650497\n",
      "Epoch 9780: train loss: 3.523736086208373e-05, val loss: 0.09118200838565826\n",
      "Epoch 9781: train loss: 5.776189937023446e-05, val loss: 0.09093908220529556\n",
      "Epoch 9782: train loss: 5.3267995099304244e-05, val loss: 0.09307364374399185\n",
      "Epoch 9783: train loss: 2.203844087489415e-05, val loss: 0.09311558306217194\n",
      "Epoch 9784: train loss: 4.561933019431308e-05, val loss: 0.09278155863285065\n",
      "Epoch 9785: train loss: 2.9339656975935213e-05, val loss: 0.09367390722036362\n",
      "Epoch 9786: train loss: 2.6207422706647776e-05, val loss: 0.09422191977500916\n",
      "Epoch 9787: train loss: 2.6453395548742265e-05, val loss: 0.0940609946846962\n",
      "Epoch 9788: train loss: 2.1169909814489074e-05, val loss: 0.09368590265512466\n",
      "Epoch 9789: train loss: 1.94092390302103e-05, val loss: 0.09401168674230576\n",
      "Epoch 9790: train loss: 1.8823435311787762e-05, val loss: 0.09430234879255295\n",
      "Epoch 9791: train loss: 1.726566370052751e-05, val loss: 0.0939377173781395\n",
      "Epoch 9792: train loss: 1.5224031812977046e-05, val loss: 0.094770647585392\n",
      "Epoch 9793: train loss: 1.8624215954332612e-05, val loss: 0.09396684914827347\n",
      "Epoch 9794: train loss: 1.7204562027473003e-05, val loss: 0.09491359442472458\n",
      "Epoch 9795: train loss: 1.0205304533883464e-05, val loss: 0.0951298251748085\n",
      "Epoch 9796: train loss: 1.2200097444292624e-05, val loss: 0.09481770545244217\n",
      "Epoch 9797: train loss: 1.5185017218755092e-05, val loss: 0.09446900337934494\n",
      "Epoch 9798: train loss: 1.2700135812337976e-05, val loss: 0.09557341784238815\n",
      "Epoch 9799: train loss: 1.4322935385280289e-05, val loss: 0.09550263732671738\n",
      "Epoch 9800: train loss: 2.8300471967668273e-05, val loss: 0.09576044976711273\n",
      "Epoch 9801: train loss: 6.042184031684883e-05, val loss: 0.0948275625705719\n",
      "Epoch 9802: train loss: 0.00014027672295924276, val loss: 0.09800802171230316\n",
      "Epoch 9803: train loss: 0.0002815354382619262, val loss: 0.09961652010679245\n",
      "Epoch 9804: train loss: 0.0005617993883788586, val loss: 0.09871681779623032\n",
      "Epoch 9805: train loss: 0.0007887125830166042, val loss: 0.10528256744146347\n",
      "Epoch 9806: train loss: 0.0008043108973652124, val loss: 0.10203182697296143\n",
      "Epoch 9807: train loss: 0.0003818339027930051, val loss: 0.10654547065496445\n",
      "Epoch 9808: train loss: 0.00033866471494548023, val loss: 0.1128050833940506\n",
      "Epoch 9809: train loss: 0.0003682692477013916, val loss: 0.10557533800601959\n",
      "Epoch 9810: train loss: 0.00022784304746892303, val loss: 0.10522846132516861\n",
      "Epoch 9811: train loss: 0.00012701336527243257, val loss: 0.10603126138448715\n",
      "Epoch 9812: train loss: 0.0002022296393988654, val loss: 0.10714924335479736\n",
      "Epoch 9813: train loss: 0.0001668610202614218, val loss: 0.11135784536600113\n",
      "Epoch 9814: train loss: 0.00017465830023866147, val loss: 0.10789040476083755\n",
      "Epoch 9815: train loss: 0.00013404461788013577, val loss: 0.10257446765899658\n",
      "Epoch 9816: train loss: 8.818662900011986e-05, val loss: 0.10629131644964218\n",
      "Epoch 9817: train loss: 7.539438956882805e-05, val loss: 0.11238515377044678\n",
      "Epoch 9818: train loss: 8.428400906268507e-05, val loss: 0.10982681810855865\n",
      "Epoch 9819: train loss: 9.167443931801245e-05, val loss: 0.10404335707426071\n",
      "Epoch 9820: train loss: 8.213629917008802e-05, val loss: 0.10669424384832382\n",
      "Epoch 9821: train loss: 6.113055860623717e-05, val loss: 0.10924184322357178\n",
      "Epoch 9822: train loss: 3.839501732727513e-05, val loss: 0.10693883895874023\n",
      "Epoch 9823: train loss: 4.445489321369678e-05, val loss: 0.10648204386234283\n",
      "Epoch 9824: train loss: 4.4115717173554e-05, val loss: 0.10851036757230759\n",
      "Epoch 9825: train loss: 5.375722685130313e-05, val loss: 0.10843353718519211\n",
      "Epoch 9826: train loss: 3.8567355659324676e-05, val loss: 0.10554671287536621\n",
      "Epoch 9827: train loss: 2.8056621886207722e-05, val loss: 0.10633856058120728\n",
      "Epoch 9828: train loss: 2.2164456822793e-05, val loss: 0.10879925638437271\n",
      "Epoch 9829: train loss: 2.3575701561640017e-05, val loss: 0.10776632279157639\n",
      "Epoch 9830: train loss: 3.150987686240114e-05, val loss: 0.10667643696069717\n",
      "Epoch 9831: train loss: 2.3006828996585682e-05, val loss: 0.10675811767578125\n",
      "Epoch 9832: train loss: 2.3546266675111838e-05, val loss: 0.1075354740023613\n",
      "Epoch 9833: train loss: 9.170487828669138e-06, val loss: 0.10737339407205582\n",
      "Epoch 9834: train loss: 1.694664388196543e-05, val loss: 0.10673855990171432\n",
      "Epoch 9835: train loss: 1.3118739843775984e-05, val loss: 0.10674148052930832\n",
      "Epoch 9836: train loss: 1.7595830286154523e-05, val loss: 0.10684599727392197\n",
      "Epoch 9837: train loss: 1.354568394162925e-05, val loss: 0.10737752914428711\n",
      "Epoch 9838: train loss: 8.943495231505949e-06, val loss: 0.10639460384845734\n",
      "Epoch 9839: train loss: 8.341484317497816e-06, val loss: 0.10576468706130981\n",
      "Epoch 9840: train loss: 7.809309863660019e-06, val loss: 0.10697902739048004\n",
      "Epoch 9841: train loss: 1.038015579979401e-05, val loss: 0.1063757911324501\n",
      "Epoch 9842: train loss: 8.4404318840825e-06, val loss: 0.10542235523462296\n",
      "Epoch 9843: train loss: 8.91903619049117e-06, val loss: 0.10601329803466797\n",
      "Epoch 9844: train loss: 5.873019745195052e-06, val loss: 0.10641777515411377\n",
      "Epoch 9845: train loss: 7.73438296164386e-06, val loss: 0.10473433881998062\n",
      "Epoch 9846: train loss: 8.464873644697946e-06, val loss: 0.104721799492836\n",
      "Epoch 9847: train loss: 1.0306216609023977e-05, val loss: 0.10607989877462387\n",
      "Epoch 9848: train loss: 1.1248849659750704e-05, val loss: 0.10524638742208481\n",
      "Epoch 9849: train loss: 1.3914304872741923e-05, val loss: 0.10487323254346848\n",
      "Epoch 9850: train loss: 2.0624864191631787e-05, val loss: 0.1048957109451294\n",
      "Epoch 9851: train loss: 1.3469276382238604e-05, val loss: 0.1044069454073906\n",
      "Epoch 9852: train loss: 2.5018111955432687e-06, val loss: 0.10407034307718277\n",
      "Epoch 9853: train loss: 1.2917493222630583e-05, val loss: 0.10404330492019653\n",
      "Epoch 9854: train loss: 1.7498476154287346e-05, val loss: 0.10389336198568344\n",
      "Epoch 9855: train loss: 8.410605005337857e-06, val loss: 0.10260005295276642\n",
      "Epoch 9856: train loss: 5.449699074233649e-06, val loss: 0.10417022556066513\n",
      "Epoch 9857: train loss: 1.1788428309955634e-05, val loss: 0.10300242155790329\n",
      "Epoch 9858: train loss: 1.3048636901658028e-05, val loss: 0.10264725983142853\n",
      "Epoch 9859: train loss: 7.225408808153588e-06, val loss: 0.10340210050344467\n",
      "Epoch 9860: train loss: 2.677528527783579e-06, val loss: 0.10231246799230576\n",
      "Epoch 9861: train loss: 4.88887826577411e-06, val loss: 0.10317964851856232\n",
      "Epoch 9862: train loss: 9.399193913850468e-06, val loss: 0.10156229883432388\n",
      "Epoch 9863: train loss: 1.2253592103661504e-05, val loss: 0.10320484638214111\n",
      "Epoch 9864: train loss: 1.3636604307976086e-05, val loss: 0.10105482488870621\n",
      "Epoch 9865: train loss: 1.6993113604257815e-05, val loss: 0.10299694538116455\n",
      "Epoch 9866: train loss: 2.944243715319317e-05, val loss: 0.10034225136041641\n",
      "Epoch 9867: train loss: 7.717873813817278e-05, val loss: 0.10442245006561279\n",
      "Epoch 9868: train loss: 0.00024987582582980394, val loss: 0.09669945389032364\n",
      "Epoch 9869: train loss: 0.0009312403271906078, val loss: 0.12062976509332657\n",
      "Epoch 9870: train loss: 0.0022923762444406748, val loss: 0.09064129739999771\n",
      "Epoch 9871: train loss: 0.0042489925399422646, val loss: 0.10973107814788818\n",
      "Epoch 9872: train loss: 0.0014150781789794564, val loss: 0.11499859392642975\n",
      "Epoch 9873: train loss: 0.0013182985130697489, val loss: 0.09392698109149933\n",
      "Epoch 9874: train loss: 0.0009506556671112776, val loss: 0.07956623286008835\n",
      "Epoch 9875: train loss: 0.00087700568838045, val loss: 0.08651330322027206\n",
      "Epoch 9876: train loss: 0.0006286450079642236, val loss: 0.09430285543203354\n",
      "Epoch 9877: train loss: 0.00042385782580822706, val loss: 0.093479685485363\n",
      "Epoch 9878: train loss: 0.0005152959492988884, val loss: 0.08402672410011292\n",
      "Epoch 9879: train loss: 0.00029444877873174846, val loss: 0.07567137479782104\n",
      "Epoch 9880: train loss: 0.00030043956940062344, val loss: 0.0741555243730545\n",
      "Epoch 9881: train loss: 0.0002654386917129159, val loss: 0.07761998474597931\n",
      "Epoch 9882: train loss: 0.00023561489069834352, val loss: 0.08154944330453873\n",
      "Epoch 9883: train loss: 0.0001894672168418765, val loss: 0.08361586183309555\n",
      "Epoch 9884: train loss: 0.00014943019778002053, val loss: 0.08411940187215805\n",
      "Epoch 9885: train loss: 0.0001569752930663526, val loss: 0.0836564376950264\n",
      "Epoch 9886: train loss: 0.00011904434359166771, val loss: 0.08224769681692123\n",
      "Epoch 9887: train loss: 0.00012821401469409466, val loss: 0.07973205298185349\n",
      "Epoch 9888: train loss: 0.0001157012302428484, val loss: 0.07776834815740585\n",
      "Epoch 9889: train loss: 6.421208672691137e-05, val loss: 0.07777285575866699\n",
      "Epoch 9890: train loss: 7.187800656538457e-05, val loss: 0.07959749549627304\n",
      "Epoch 9891: train loss: 8.42680165078491e-05, val loss: 0.0818675309419632\n",
      "Epoch 9892: train loss: 6.93105612299405e-05, val loss: 0.08276969194412231\n",
      "Epoch 9893: train loss: 5.7086483138846233e-05, val loss: 0.08173009753227234\n",
      "Epoch 9894: train loss: 4.161379547440447e-05, val loss: 0.07989829033613205\n",
      "Epoch 9895: train loss: 4.194833672954701e-05, val loss: 0.07885248959064484\n",
      "Epoch 9896: train loss: 4.8553221859037876e-05, val loss: 0.07864872366189957\n",
      "Epoch 9897: train loss: 3.5986155126011e-05, val loss: 0.07899653911590576\n",
      "Epoch 9898: train loss: 2.9907261705375277e-05, val loss: 0.07939662784337997\n",
      "Epoch 9899: train loss: 3.1946350645739585e-05, val loss: 0.07954274863004684\n",
      "Epoch 9900: train loss: 2.658418998180423e-05, val loss: 0.07967677712440491\n",
      "Epoch 9901: train loss: 2.3788355974829756e-05, val loss: 0.07970040291547775\n",
      "Epoch 9902: train loss: 2.0672843675129116e-05, val loss: 0.07955536991357803\n",
      "Epoch 9903: train loss: 1.9526696632965468e-05, val loss: 0.07926588505506516\n",
      "Epoch 9904: train loss: 2.1244908566586673e-05, val loss: 0.07877426594495773\n",
      "Epoch 9905: train loss: 1.5333664123318158e-05, val loss: 0.07848133891820908\n",
      "Epoch 9906: train loss: 1.1335269846313167e-05, val loss: 0.07864220440387726\n",
      "Epoch 9907: train loss: 1.4197795280779246e-05, val loss: 0.07914845645427704\n",
      "Epoch 9908: train loss: 1.3768035387329292e-05, val loss: 0.07961679250001907\n",
      "Epoch 9909: train loss: 1.1805983376689255e-05, val loss: 0.07930608838796616\n",
      "Epoch 9910: train loss: 9.195688107865863e-06, val loss: 0.07838765531778336\n",
      "Epoch 9911: train loss: 6.6654324655246455e-06, val loss: 0.07748327404260635\n",
      "Epoch 9912: train loss: 9.335632967122365e-06, val loss: 0.07720737904310226\n",
      "Epoch 9913: train loss: 8.901612090994604e-06, val loss: 0.07755596935749054\n",
      "Epoch 9914: train loss: 5.801142378913937e-06, val loss: 0.07797962427139282\n",
      "Epoch 9915: train loss: 4.870254997513257e-06, val loss: 0.07809390872716904\n",
      "Epoch 9916: train loss: 5.615211648546392e-06, val loss: 0.07785855978727341\n",
      "Epoch 9917: train loss: 6.901270353409927e-06, val loss: 0.07752753794193268\n",
      "Epoch 9918: train loss: 3.496396629998344e-06, val loss: 0.07713159173727036\n",
      "Epoch 9919: train loss: 3.001418690473656e-06, val loss: 0.07669484615325928\n",
      "Epoch 9920: train loss: 4.350889412307879e-06, val loss: 0.0763552337884903\n",
      "Epoch 9921: train loss: 4.422402980708284e-06, val loss: 0.07628833502531052\n",
      "Epoch 9922: train loss: 2.471311518092989e-06, val loss: 0.07646763324737549\n",
      "Epoch 9923: train loss: 2.215375161540578e-06, val loss: 0.07661082595586777\n",
      "Epoch 9924: train loss: 3.324636281831772e-06, val loss: 0.07647059112787247\n",
      "Epoch 9925: train loss: 2.4378300622629467e-06, val loss: 0.07610172778367996\n",
      "Epoch 9926: train loss: 1.5167981928243535e-06, val loss: 0.07588514685630798\n",
      "Epoch 9927: train loss: 2.0928564481437206e-06, val loss: 0.07597918808460236\n",
      "Epoch 9928: train loss: 2.2868446194479475e-06, val loss: 0.07593290507793427\n",
      "Epoch 9929: train loss: 1.1848227359223529e-06, val loss: 0.0757780373096466\n",
      "Epoch 9930: train loss: 1.484674271523545e-06, val loss: 0.0751466378569603\n",
      "Epoch 9931: train loss: 1.2188885420982842e-06, val loss: 0.07466518133878708\n",
      "Epoch 9932: train loss: 1.2157242963439785e-06, val loss: 0.07436273247003555\n",
      "Epoch 9933: train loss: 1.0795466778290574e-06, val loss: 0.07457077503204346\n",
      "Epoch 9934: train loss: 1.2132046549595543e-06, val loss: 0.07472144812345505\n",
      "Epoch 9935: train loss: 7.952203873173858e-07, val loss: 0.07457037270069122\n",
      "Epoch 9936: train loss: 7.101533014974848e-07, val loss: 0.07413496822118759\n",
      "Epoch 9937: train loss: 8.249011216321378e-07, val loss: 0.07380782067775726\n",
      "Epoch 9938: train loss: 7.68604024870001e-07, val loss: 0.07386857271194458\n",
      "Epoch 9939: train loss: 5.436667720459809e-07, val loss: 0.07385893166065216\n",
      "Epoch 9940: train loss: 6.815253641434538e-07, val loss: 0.07364485412836075\n",
      "Epoch 9941: train loss: 4.887219802185427e-07, val loss: 0.07336420565843582\n",
      "Epoch 9942: train loss: 3.907131542746356e-07, val loss: 0.07319503277540207\n",
      "Epoch 9943: train loss: 5.659030648530461e-07, val loss: 0.07320486009120941\n",
      "Epoch 9944: train loss: 5.504392106558953e-07, val loss: 0.07302802056074142\n",
      "Epoch 9945: train loss: 9.397832627655589e-07, val loss: 0.07294096797704697\n",
      "Epoch 9946: train loss: 1.866352818069572e-06, val loss: 0.07246046513319016\n",
      "Epoch 9947: train loss: 3.7824961509613786e-06, val loss: 0.07260919362306595\n",
      "Epoch 9948: train loss: 6.364026830851799e-06, val loss: 0.0723334476351738\n",
      "Epoch 9949: train loss: 7.4457125265325885e-06, val loss: 0.07263153046369553\n",
      "Epoch 9950: train loss: 3.998193278675899e-06, val loss: 0.07234054058790207\n",
      "Epoch 9951: train loss: 6.103041414462496e-07, val loss: 0.07221347093582153\n",
      "Epoch 9952: train loss: 1.6347570408470347e-06, val loss: 0.07226002961397171\n",
      "Epoch 9953: train loss: 3.7701186101912754e-06, val loss: 0.07184804230928421\n",
      "Epoch 9954: train loss: 3.8368407331290655e-06, val loss: 0.0719708725810051\n",
      "Epoch 9955: train loss: 3.695038685691543e-06, val loss: 0.07158789038658142\n",
      "Epoch 9956: train loss: 3.4367526495771017e-06, val loss: 0.07171628624200821\n",
      "Epoch 9957: train loss: 2.514483185223071e-06, val loss: 0.07164209336042404\n",
      "Epoch 9958: train loss: 3.1800036595086567e-06, val loss: 0.07141224294900894\n",
      "Epoch 9959: train loss: 6.957991445233347e-06, val loss: 0.07131291180849075\n",
      "Epoch 9960: train loss: 1.1210860066057649e-05, val loss: 0.07137700170278549\n",
      "Epoch 9961: train loss: 1.0320502042304724e-05, val loss: 0.07147406786680222\n",
      "Epoch 9962: train loss: 7.355398338404484e-06, val loss: 0.07111364603042603\n",
      "Epoch 9963: train loss: 1.5185814845608547e-05, val loss: 0.07119853794574738\n",
      "Epoch 9964: train loss: 2.324162414879538e-05, val loss: 0.07084568589925766\n",
      "Epoch 9965: train loss: 3.847584594041109e-05, val loss: 0.07057630270719528\n",
      "Epoch 9966: train loss: 9.372302884003147e-05, val loss: 0.07138784229755402\n",
      "Epoch 9967: train loss: 0.0003107440716121346, val loss: 0.06882800161838531\n",
      "Epoch 9968: train loss: 0.001063504721969366, val loss: 0.07459298521280289\n",
      "Epoch 9969: train loss: 0.0023837052285671234, val loss: 0.06784415245056152\n",
      "Epoch 9970: train loss: 0.001465668436139822, val loss: 0.0681079551577568\n",
      "Epoch 9971: train loss: 0.0006910742376931012, val loss: 0.07091321051120758\n",
      "Epoch 9972: train loss: 0.00031991323339752853, val loss: 0.07276644557714462\n",
      "Epoch 9973: train loss: 0.0006162894424051046, val loss: 0.06978969275951385\n",
      "Epoch 9974: train loss: 0.00037404068280011415, val loss: 0.06503672152757645\n",
      "Epoch 9975: train loss: 0.0001923971576616168, val loss: 0.06113896891474724\n",
      "Epoch 9976: train loss: 0.00016508239787071943, val loss: 0.05956757068634033\n",
      "Epoch 9977: train loss: 0.00022710353368893266, val loss: 0.06177776679396629\n",
      "Epoch 9978: train loss: 0.000255940540228039, val loss: 0.060729604214429855\n",
      "Epoch 9979: train loss: 0.00011110243940493092, val loss: 0.059110481292009354\n",
      "Epoch 9980: train loss: 0.00010561045201029629, val loss: 0.059919219464063644\n",
      "Epoch 9981: train loss: 0.00010628844029270113, val loss: 0.0571102611720562\n",
      "Epoch 9982: train loss: 0.00011592656665015966, val loss: 0.05573587492108345\n",
      "Epoch 9983: train loss: 7.414910942316055e-05, val loss: 0.055290259420871735\n",
      "Epoch 9984: train loss: 6.02241198066622e-05, val loss: 0.05530266836285591\n",
      "Epoch 9985: train loss: 6.102996121626347e-05, val loss: 0.05567023158073425\n",
      "Epoch 9986: train loss: 6.144143117126077e-05, val loss: 0.0543542206287384\n",
      "Epoch 9987: train loss: 5.165847687749192e-05, val loss: 0.05484984070062637\n",
      "Epoch 9988: train loss: 4.239528061589226e-05, val loss: 0.05697833374142647\n",
      "Epoch 9989: train loss: 3.7374233215814456e-05, val loss: 0.05625581741333008\n",
      "Epoch 9990: train loss: 3.0013603463885374e-05, val loss: 0.05530891567468643\n",
      "Epoch 9991: train loss: 3.8172485801624134e-05, val loss: 0.05487390235066414\n",
      "Epoch 9992: train loss: 3.129009928670712e-05, val loss: 0.05422058328986168\n",
      "Epoch 9993: train loss: 2.7974494514637627e-05, val loss: 0.05375851318240166\n",
      "Epoch 9994: train loss: 1.8317465219297446e-05, val loss: 0.053399886935949326\n",
      "Epoch 9995: train loss: 1.9044386135647073e-05, val loss: 0.05434774234890938\n",
      "Epoch 9996: train loss: 1.750040792103391e-05, val loss: 0.055104512721300125\n",
      "Epoch 9997: train loss: 2.2596059352508746e-05, val loss: 0.05464889481663704\n",
      "Epoch 9998: train loss: 1.6085818060673773e-05, val loss: 0.05407274514436722\n",
      "Epoch 9999: train loss: 1.4854124856356066e-05, val loss: 0.05285550281405449\n",
      "Epoch 10000: train loss: 1.1083619938290212e-05, val loss: 0.053017254918813705\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "            \"\"\"\n",
    "            The model class, which defines our feature extractor used in pretraining.\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                \"\"\"\n",
    "                The constructor of the model.\n",
    "                \"\"\"\n",
    "                super().__init__()\n",
    "                # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "                # and then used to extract features from the training and test data.\n",
    "                self.seq = nn.Sequential(\n",
    "                    nn.Linear(1000, 100),\n",
    "                    nn.LeakyReLU(0.01),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.Linear(100, 1)\n",
    "                )\n",
    "\n",
    "                for m in self.modules():\n",
    "                    if isinstance(m, nn.Linear):    \n",
    "                        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "            def forward(self, x):\n",
    "                \"\"\"\n",
    "                The forward pass of the model.\n",
    "\n",
    "                input: x: torch.Tensor, the input to the model\n",
    "\n",
    "                output: x: torch.Tensor, the output of the model\n",
    "                \"\"\"\n",
    "                # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "                # defined in the constructor.\n",
    "                x = self.seq(x)\n",
    "                return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=10, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.4, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr = loss.item()\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val = loss.item()\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-8):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "one_model = get_regression_model(featured_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-ae.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = one_model(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
