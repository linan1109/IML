{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beeed8ac07fb4aa59b801143a0c68665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.2468429364943108, val loss: 1.3335063755512238\n",
      "Epoch 2: train loss: 1.236247278630675, val loss: 1.3234477639198303\n",
      "Epoch 3: train loss: 1.2262322879702154, val loss: 1.314057171344757\n",
      "Epoch 4: train loss: 1.2166956142442984, val loss: 1.3049940466880798\n",
      "Epoch 5: train loss: 1.2075564524037743, val loss: 1.2961877286434174\n",
      "Epoch 6: train loss: 1.1986793045591038, val loss: 1.2876657545566559\n",
      "Epoch 7: train loss: 1.1899396043242836, val loss: 1.2791130542755127\n",
      "Epoch 8: train loss: 1.1811766089352467, val loss: 1.2704075574874878\n",
      "Epoch 9: train loss: 1.1722350078484312, val loss: 1.2615307569503784\n",
      "Epoch 10: train loss: 1.1629440669178457, val loss: 1.2519768178462982\n",
      "Epoch 11: train loss: 1.153098899529129, val loss: 1.2418641149997711\n",
      "Epoch 12: train loss: 1.1424624092708964, val loss: 1.2306165099143982\n",
      "Epoch 13: train loss: 1.130789080736334, val loss: 1.2185284793376923\n",
      "Epoch 14: train loss: 1.1178619136455128, val loss: 1.204876333475113\n",
      "Epoch 15: train loss: 1.103490811124887, val loss: 1.1898844838142395\n",
      "Epoch 16: train loss: 1.0875551646848551, val loss: 1.1733523905277252\n",
      "Epoch 17: train loss: 1.0701115938698844, val loss: 1.1552299559116364\n",
      "Epoch 18: train loss: 1.0514384563239483, val loss: 1.1362515538930893\n",
      "Epoch 19: train loss: 1.032007090993194, val loss: 1.1168765127658844\n",
      "Epoch 20: train loss: 1.012543801192714, val loss: 1.0977508425712585\n",
      "Epoch 21: train loss: 0.9938277593238491, val loss: 1.0798141062259674\n",
      "Epoch 22: train loss: 0.9765281316619494, val loss: 1.0636720210313797\n",
      "Epoch 23: train loss: 0.9611404204594469, val loss: 1.0494028180837631\n",
      "Epoch 24: train loss: 0.947812909629757, val loss: 1.0372295528650284\n",
      "Epoch 25: train loss: 0.9365009736173568, val loss: 1.0269957482814789\n",
      "Epoch 26: train loss: 0.926985669782527, val loss: 1.018446907401085\n",
      "Epoch 27: train loss: 0.9189797084302693, val loss: 1.0110421180725098\n",
      "Epoch 28: train loss: 0.9122078507720003, val loss: 1.004906788468361\n",
      "Epoch 29: train loss: 0.9064271075590807, val loss: 0.9996154457330704\n",
      "Epoch 30: train loss: 0.9014090469988761, val loss: 0.9949106276035309\n",
      "Epoch 31: train loss: 0.8970149961689335, val loss: 0.9908443689346313\n",
      "Epoch 32: train loss: 0.8930672102603832, val loss: 0.9870417416095734\n",
      "Epoch 33: train loss: 0.8894914971461915, val loss: 0.983659490942955\n",
      "Epoch 34: train loss: 0.8862184679847027, val loss: 0.9806458503007889\n",
      "Epoch 35: train loss: 0.8832223078504337, val loss: 0.9776470810174942\n",
      "Epoch 36: train loss: 0.8803982675873895, val loss: 0.9750357121229172\n",
      "Epoch 37: train loss: 0.8777545629198822, val loss: 0.9724532663822174\n",
      "Epoch 38: train loss: 0.8752609408591082, val loss: 0.9699634462594986\n",
      "Epoch 39: train loss: 0.872884098531685, val loss: 0.9676852375268936\n",
      "Epoch 40: train loss: 0.8706426215460004, val loss: 0.965509831905365\n",
      "Epoch 41: train loss: 0.8684737633077665, val loss: 0.9633583724498749\n",
      "Epoch 42: train loss: 0.8664243253217496, val loss: 0.9613298177719116\n",
      "Epoch 43: train loss: 0.8644570004741415, val loss: 0.959304541349411\n",
      "Epoch 44: train loss: 0.8625356306324049, val loss: 0.9574891328811646\n",
      "Epoch 45: train loss: 0.8607098826081564, val loss: 0.9556905925273895\n",
      "Epoch 46: train loss: 0.8589405940273002, val loss: 0.953898087143898\n",
      "Epoch 47: train loss: 0.8572177447668285, val loss: 0.9522069096565247\n",
      "Epoch 48: train loss: 0.8555591688246479, val loss: 0.9506666660308838\n",
      "Epoch 49: train loss: 0.853973471565365, val loss: 0.9491375535726547\n",
      "Epoch 50: train loss: 0.8524266767252586, val loss: 0.9475502818822861\n",
      "Epoch 51: train loss: 0.8509102611516823, val loss: 0.945965051651001\n",
      "Epoch 52: train loss: 0.8494570846847365, val loss: 0.9444772303104401\n",
      "Epoch 53: train loss: 0.8480298730019299, val loss: 0.943053275346756\n",
      "Epoch 54: train loss: 0.8466634871483784, val loss: 0.9417598247528076\n",
      "Epoch 55: train loss: 0.8453020672905643, val loss: 0.940290093421936\n",
      "Epoch 56: train loss: 0.8440246956288444, val loss: 0.9390628188848495\n",
      "Epoch 57: train loss: 0.8427296691336533, val loss: 0.9376955628395081\n",
      "Epoch 58: train loss: 0.8414871717971433, val loss: 0.9365532994270325\n",
      "Epoch 59: train loss: 0.8402913462216073, val loss: 0.935308963060379\n",
      "Epoch 60: train loss: 0.8391187864945083, val loss: 0.9340551942586899\n",
      "Epoch 61: train loss: 0.837972549432557, val loss: 0.9328453242778778\n",
      "Epoch 62: train loss: 0.8368323506657807, val loss: 0.9317814260721207\n",
      "Epoch 63: train loss: 0.8357358251237978, val loss: 0.9308342337608337\n",
      "Epoch 64: train loss: 0.8346728768974923, val loss: 0.9296023845672607\n",
      "Epoch 65: train loss: 0.8336328416235188, val loss: 0.9286490678787231\n",
      "Epoch 66: train loss: 0.832617074533523, val loss: 0.9275127649307251\n",
      "Epoch 67: train loss: 0.8316092466225385, val loss: 0.9266407489776611\n",
      "Epoch 68: train loss: 0.8306166714836826, val loss: 0.9255661219358444\n",
      "Epoch 69: train loss: 0.8296741147868503, val loss: 0.9246652722358704\n",
      "Epoch 70: train loss: 0.8287273509484496, val loss: 0.9236563891172409\n",
      "Epoch 71: train loss: 0.8278028266525704, val loss: 0.9226996749639511\n",
      "Epoch 72: train loss: 0.8269054384568049, val loss: 0.9217303693294525\n",
      "Epoch 73: train loss: 0.8260182087930034, val loss: 0.9208498448133469\n",
      "Epoch 74: train loss: 0.8251431891436048, val loss: 0.9200615286827087\n",
      "Epoch 75: train loss: 0.8242948153797064, val loss: 0.9192168861627579\n",
      "Epoch 76: train loss: 0.8234670930577042, val loss: 0.9184034019708633\n",
      "Epoch 77: train loss: 0.8226321909755556, val loss: 0.9175108075141907\n",
      "Epoch 78: train loss: 0.8218312852525197, val loss: 0.9166112244129181\n",
      "Epoch 79: train loss: 0.821036191517837, val loss: 0.9159202724695206\n",
      "Epoch 80: train loss: 0.820262303795545, val loss: 0.9150419682264328\n",
      "Epoch 81: train loss: 0.8194987065688492, val loss: 0.9143595993518829\n",
      "Epoch 82: train loss: 0.818740960995538, val loss: 0.9136368632316589\n",
      "Epoch 83: train loss: 0.8180153376718862, val loss: 0.912873387336731\n",
      "Epoch 84: train loss: 0.8172666133326356, val loss: 0.9121197760105133\n",
      "Epoch 85: train loss: 0.8165645818623092, val loss: 0.9114424139261246\n",
      "Epoch 86: train loss: 0.8158640652768404, val loss: 0.91069096326828\n",
      "Epoch 87: train loss: 0.8151732170866113, val loss: 0.9101210385560989\n",
      "Epoch 88: train loss: 0.8144994397951629, val loss: 0.9092987030744553\n",
      "Epoch 89: train loss: 0.813833985617643, val loss: 0.9086601734161377\n",
      "Epoch 90: train loss: 0.813169707838362, val loss: 0.9080371707677841\n",
      "Epoch 91: train loss: 0.8125192395848645, val loss: 0.9073933362960815\n",
      "Epoch 92: train loss: 0.8118899903512399, val loss: 0.9066744595766068\n",
      "Epoch 93: train loss: 0.8112518993790186, val loss: 0.9059998393058777\n",
      "Epoch 94: train loss: 0.8106572961581373, val loss: 0.9053856581449509\n",
      "Epoch 95: train loss: 0.8100445866195333, val loss: 0.9048040956258774\n",
      "Epoch 96: train loss: 0.8094493173493625, val loss: 0.9042071104049683\n",
      "Epoch 97: train loss: 0.8088767605449436, val loss: 0.9036025702953339\n",
      "Epoch 98: train loss: 0.8082974330150625, val loss: 0.9029430150985718\n",
      "Epoch 99: train loss: 0.8077340810682913, val loss: 0.9025028944015503\n",
      "Epoch 100: train loss: 0.8071640266853389, val loss: 0.9019383192062378\n",
      "Epoch 101: train loss: 0.8066423266045372, val loss: 0.9013266712427139\n",
      "Epoch 102: train loss: 0.8060887002625601, val loss: 0.9008487313985825\n",
      "Epoch 103: train loss: 0.805559397483721, val loss: 0.900335967540741\n",
      "Epoch 104: train loss: 0.8050596091880474, val loss: 0.8998076915740967\n",
      "Epoch 105: train loss: 0.8045374600959736, val loss: 0.8994109630584717\n",
      "Epoch 106: train loss: 0.8040432701582848, val loss: 0.8987680524587631\n",
      "Epoch 107: train loss: 0.8035559606334034, val loss: 0.8982826918363571\n",
      "Epoch 108: train loss: 0.8030805786580519, val loss: 0.8978665769100189\n",
      "Epoch 109: train loss: 0.8025949302937851, val loss: 0.8972799926996231\n",
      "Epoch 110: train loss: 0.8021474464272254, val loss: 0.8968129009008408\n",
      "Epoch 111: train loss: 0.8016837701537336, val loss: 0.8963630646467209\n",
      "Epoch 112: train loss: 0.8012513633219879, val loss: 0.8959753960371017\n",
      "Epoch 113: train loss: 0.8008025512707775, val loss: 0.8954464942216873\n",
      "Epoch 114: train loss: 0.8003800954938675, val loss: 0.8951207101345062\n",
      "Epoch 115: train loss: 0.7999442437823712, val loss: 0.8946068286895752\n",
      "Epoch 116: train loss: 0.7995457083755987, val loss: 0.8942370414733887\n",
      "Epoch 117: train loss: 0.7991270054714698, val loss: 0.8938016593456268\n",
      "Epoch 118: train loss: 0.7987268904345132, val loss: 0.8934510052204132\n",
      "Epoch 119: train loss: 0.7983300583847829, val loss: 0.8930296897888184\n",
      "Epoch 120: train loss: 0.797958795734575, val loss: 0.8926567733287811\n",
      "Epoch 121: train loss: 0.7975790277488557, val loss: 0.8922415971755981\n",
      "Epoch 122: train loss: 0.7972070628309204, val loss: 0.8919191658496857\n",
      "Epoch 123: train loss: 0.7968528329527717, val loss: 0.891626164317131\n",
      "Epoch 124: train loss: 0.7964872274084132, val loss: 0.8912831246852875\n",
      "Epoch 125: train loss: 0.796140134665593, val loss: 0.8908649384975433\n",
      "Epoch 126: train loss: 0.7957950718493992, val loss: 0.8904755413532257\n",
      "Epoch 127: train loss: 0.7954565925521814, val loss: 0.8901462107896805\n",
      "Epoch 128: train loss: 0.7951367538920107, val loss: 0.8898583501577377\n",
      "Epoch 129: train loss: 0.7947939740641762, val loss: 0.8895253539085388\n",
      "Epoch 130: train loss: 0.7944737574081334, val loss: 0.8892108350992203\n",
      "Epoch 131: train loss: 0.7941661239721852, val loss: 0.8888993263244629\n",
      "Epoch 132: train loss: 0.7938528327293857, val loss: 0.8885612785816193\n",
      "Epoch 133: train loss: 0.7935417866208357, val loss: 0.888255313038826\n",
      "Epoch 134: train loss: 0.7932623196800952, val loss: 0.8880109190940857\n",
      "Epoch 135: train loss: 0.7929477512505427, val loss: 0.8876703083515167\n",
      "Epoch 136: train loss: 0.7926616489750932, val loss: 0.8873487263917923\n",
      "Epoch 137: train loss: 0.7923882441441245, val loss: 0.8871201574802399\n",
      "Epoch 138: train loss: 0.7920828686357129, val loss: 0.8868691623210907\n",
      "Epoch 139: train loss: 0.7918202094453801, val loss: 0.8864779770374298\n",
      "Epoch 140: train loss: 0.7915486327108703, val loss: 0.8862561732530594\n",
      "Epoch 141: train loss: 0.7912889987726844, val loss: 0.8859923779964447\n",
      "Epoch 142: train loss: 0.7910199438150861, val loss: 0.885771632194519\n",
      "Epoch 143: train loss: 0.7907634366964523, val loss: 0.8854763954877853\n",
      "Epoch 144: train loss: 0.7905026955839778, val loss: 0.885343000292778\n",
      "Epoch 145: train loss: 0.7902475458194679, val loss: 0.8849633634090424\n",
      "Epoch 146: train loss: 0.7900068303682726, val loss: 0.884645015001297\n",
      "Epoch 147: train loss: 0.7897627938031293, val loss: 0.8844229876995087\n",
      "Epoch 148: train loss: 0.7895103256245214, val loss: 0.8842510730028152\n",
      "Epoch 149: train loss: 0.7892812025270521, val loss: 0.8840103894472122\n",
      "Epoch 150: train loss: 0.7890438742397736, val loss: 0.8838903456926346\n",
      "Epoch 151: train loss: 0.7888246315711855, val loss: 0.8835399597883224\n",
      "Epoch 152: train loss: 0.7886023689274382, val loss: 0.8833220899105072\n",
      "Epoch 153: train loss: 0.7883754113615435, val loss: 0.8831199407577515\n",
      "Epoch 154: train loss: 0.7881537899983471, val loss: 0.882852092385292\n",
      "Epoch 155: train loss: 0.7879373173003336, val loss: 0.8826407641172409\n",
      "Epoch 156: train loss: 0.7877143244461389, val loss: 0.8825325071811676\n",
      "Epoch 157: train loss: 0.7875019321056879, val loss: 0.8823061585426331\n",
      "Epoch 158: train loss: 0.7872978613019106, val loss: 0.882032498717308\n",
      "Epoch 159: train loss: 0.7870953605479565, val loss: 0.8818056434392929\n",
      "Epoch 160: train loss: 0.7868771736273692, val loss: 0.8816568106412888\n",
      "Epoch 161: train loss: 0.7866927017109412, val loss: 0.881427675485611\n",
      "Epoch 162: train loss: 0.7864880510616521, val loss: 0.8812808096408844\n",
      "Epoch 163: train loss: 0.7862893014474462, val loss: 0.8810464292764664\n",
      "Epoch 164: train loss: 0.7860934276161767, val loss: 0.8808559030294418\n",
      "Epoch 165: train loss: 0.7859246880935711, val loss: 0.8806465119123459\n",
      "Epoch 166: train loss: 0.7857241145499739, val loss: 0.8804809302091599\n",
      "Epoch 167: train loss: 0.7855361740170878, val loss: 0.8802702575922012\n",
      "Epoch 168: train loss: 0.7853585654168065, val loss: 0.8801048696041107\n",
      "Epoch 169: train loss: 0.7851868919865129, val loss: 0.8798801153898239\n",
      "Epoch 170: train loss: 0.7850028913186992, val loss: 0.8797196596860886\n",
      "Epoch 171: train loss: 0.7848211849136938, val loss: 0.879522517323494\n",
      "Epoch 172: train loss: 0.7846638544327836, val loss: 0.8793730437755585\n",
      "Epoch 173: train loss: 0.7844866632207941, val loss: 0.8791677951812744\n",
      "Epoch 174: train loss: 0.7843220805623335, val loss: 0.8791020810604095\n",
      "Epoch 175: train loss: 0.7841400887131964, val loss: 0.8789160698652267\n",
      "Epoch 176: train loss: 0.7839787974345143, val loss: 0.8787261545658112\n",
      "Epoch 177: train loss: 0.7838164425994477, val loss: 0.8785668611526489\n",
      "Epoch 178: train loss: 0.7836556558864485, val loss: 0.8783816397190094\n",
      "Epoch 179: train loss: 0.7835064261460453, val loss: 0.8783861100673676\n",
      "Epoch 180: train loss: 0.7833413101029295, val loss: 0.8781077861785889\n",
      "Epoch 181: train loss: 0.7831989746785405, val loss: 0.8780275136232376\n",
      "Epoch 182: train loss: 0.7830354867321103, val loss: 0.8777232617139816\n",
      "Epoch 183: train loss: 0.7828960793542534, val loss: 0.8776241689920425\n",
      "Epoch 184: train loss: 0.7827396658618868, val loss: 0.8774377256631851\n",
      "Epoch 185: train loss: 0.7825922920243563, val loss: 0.8773721307516098\n",
      "Epoch 186: train loss: 0.7824372173485044, val loss: 0.8771809786558151\n",
      "Epoch 187: train loss: 0.7822965950757138, val loss: 0.8770419508218765\n",
      "Epoch 188: train loss: 0.7821515108665895, val loss: 0.8769270330667496\n",
      "Epoch 189: train loss: 0.7820123241096492, val loss: 0.8768365383148193\n",
      "Epoch 190: train loss: 0.781864467749055, val loss: 0.8765922635793686\n",
      "Epoch 191: train loss: 0.7817460095831787, val loss: 0.8764860033988953\n",
      "Epoch 192: train loss: 0.7816052925193361, val loss: 0.8763012588024139\n",
      "Epoch 193: train loss: 0.78146545632296, val loss: 0.8762164115905762\n",
      "Epoch 194: train loss: 0.7813402780984905, val loss: 0.8760930448770523\n",
      "Epoch 195: train loss: 0.7811967078200823, val loss: 0.8759919553995132\n",
      "Epoch 196: train loss: 0.7810817890134523, val loss: 0.8757703900337219\n",
      "Epoch 197: train loss: 0.7809435546846025, val loss: 0.8756497800350189\n",
      "Epoch 198: train loss: 0.7808218452323694, val loss: 0.8755511939525604\n",
      "Epoch 199: train loss: 0.7807046732883833, val loss: 0.875455230474472\n",
      "Epoch 200: train loss: 0.7805689111989684, val loss: 0.8754079192876816\n",
      "Epoch 201: train loss: 0.7804527521639703, val loss: 0.875200167298317\n",
      "Epoch 202: train loss: 0.780321342966129, val loss: 0.8751243948936462\n",
      "Epoch 203: train loss: 0.7801950722689412, val loss: 0.8750062584877014\n",
      "Epoch 204: train loss: 0.7800918327402889, val loss: 0.8748191148042679\n",
      "Epoch 205: train loss: 0.7799729612558272, val loss: 0.8746275901794434\n",
      "Epoch 206: train loss: 0.7798490783857466, val loss: 0.8746006488800049\n",
      "Epoch 207: train loss: 0.7797427596277912, val loss: 0.874491959810257\n",
      "Epoch 208: train loss: 0.7796189887038402, val loss: 0.8743721842765808\n",
      "Epoch 209: train loss: 0.7795125978595094, val loss: 0.8743205368518829\n",
      "Epoch 210: train loss: 0.7793889207919412, val loss: 0.8741336613893509\n",
      "Epoch 211: train loss: 0.7792933252620292, val loss: 0.8740028291940689\n",
      "Epoch 212: train loss: 0.7791841580407443, val loss: 0.8739212602376938\n",
      "Epoch 213: train loss: 0.7790657953863944, val loss: 0.8738167881965637\n",
      "Epoch 214: train loss: 0.7789558265303443, val loss: 0.8737402558326721\n",
      "Epoch 215: train loss: 0.7788643074830114, val loss: 0.8736641854047775\n",
      "Epoch 216: train loss: 0.778749758532841, val loss: 0.873494416475296\n",
      "Epoch 217: train loss: 0.7786552005054046, val loss: 0.8734844028949738\n",
      "Epoch 218: train loss: 0.7785506418661836, val loss: 0.8732593357563019\n",
      "Epoch 219: train loss: 0.7784429559400758, val loss: 0.8731438517570496\n",
      "Epoch 220: train loss: 0.7783530205536886, val loss: 0.8730461597442627\n",
      "Epoch 221: train loss: 0.7782400205259189, val loss: 0.8730037957429886\n",
      "Epoch 222: train loss: 0.7781463115093016, val loss: 0.872872993350029\n",
      "Epoch 223: train loss: 0.7780559028102385, val loss: 0.8727410137653351\n",
      "Epoch 224: train loss: 0.7779550051151713, val loss: 0.8727623075246811\n",
      "Epoch 225: train loss: 0.7778633614257829, val loss: 0.8725347965955734\n",
      "Epoch 226: train loss: 0.7777702759504708, val loss: 0.8725168406963348\n",
      "Epoch 227: train loss: 0.7776756148562638, val loss: 0.872396856546402\n",
      "Epoch 228: train loss: 0.7775776998664148, val loss: 0.8723241090774536\n",
      "Epoch 229: train loss: 0.7775002436597376, val loss: 0.8722728341817856\n",
      "Epoch 230: train loss: 0.7773915896498272, val loss: 0.8721649050712585\n",
      "Epoch 231: train loss: 0.7773081183394588, val loss: 0.8720577359199524\n",
      "Epoch 232: train loss: 0.7772204734187856, val loss: 0.8720078617334366\n",
      "Epoch 233: train loss: 0.7771255125293776, val loss: 0.8718393594026566\n",
      "Epoch 234: train loss: 0.7770462716104781, val loss: 0.8717373609542847\n",
      "Epoch 235: train loss: 0.7769619793683163, val loss: 0.8716646581888199\n",
      "Epoch 236: train loss: 0.7768734065546237, val loss: 0.871595561504364\n",
      "Epoch 237: train loss: 0.7767912709257331, val loss: 0.8715679049491882\n",
      "Epoch 238: train loss: 0.7767036138738304, val loss: 0.8714452832937241\n",
      "Epoch 239: train loss: 0.7766187757847551, val loss: 0.8713432848453522\n",
      "Epoch 240: train loss: 0.7765322984803233, val loss: 0.8713090121746063\n",
      "Epoch 241: train loss: 0.776455699112321, val loss: 0.8712726831436157\n",
      "Epoch 242: train loss: 0.7763690600906156, val loss: 0.8711162954568863\n",
      "Epoch 243: train loss: 0.7762964778147426, val loss: 0.8710096925497055\n",
      "Epoch 244: train loss: 0.7762106292117698, val loss: 0.8709606230258942\n",
      "Epoch 245: train loss: 0.7761332429689896, val loss: 0.8708731979131699\n",
      "Epoch 246: train loss: 0.7760543436328012, val loss: 0.8708043247461319\n",
      "Epoch 247: train loss: 0.7759880000607946, val loss: 0.870719388127327\n",
      "Epoch 248: train loss: 0.7759091562790171, val loss: 0.870647981762886\n",
      "Epoch 249: train loss: 0.7758355787165685, val loss: 0.8706197738647461\n",
      "Epoch 250: train loss: 0.7757560942968252, val loss: 0.8705302625894547\n",
      "Epoch 251: train loss: 0.7756851793153346, val loss: 0.8703226298093796\n",
      "Epoch 252: train loss: 0.7756126768173871, val loss: 0.8703845739364624\n",
      "Epoch 253: train loss: 0.7755324360359264, val loss: 0.8703000843524933\n",
      "Epoch 254: train loss: 0.7754630798103372, val loss: 0.8702972680330276\n",
      "Epoch 255: train loss: 0.7753849182982571, val loss: 0.8701156675815582\n",
      "Epoch 256: train loss: 0.7753209711990493, val loss: 0.8700616657733917\n",
      "Epoch 257: train loss: 0.7752425782978827, val loss: 0.8699867725372314\n",
      "Epoch 258: train loss: 0.7751844848966489, val loss: 0.8698914498090744\n",
      "Epoch 259: train loss: 0.7751098194958843, val loss: 0.8698300272226334\n",
      "Epoch 260: train loss: 0.7750430975580325, val loss: 0.8697828352451324\n",
      "Epoch 261: train loss: 0.7749683121106703, val loss: 0.8697739094495773\n",
      "Epoch 262: train loss: 0.7749050062141244, val loss: 0.8696365654468536\n",
      "Epoch 263: train loss: 0.7748374703155549, val loss: 0.8696455508470535\n",
      "Epoch 264: train loss: 0.7747692747156592, val loss: 0.8694650530815125\n",
      "Epoch 265: train loss: 0.7747147207710331, val loss: 0.8695403784513474\n",
      "Epoch 266: train loss: 0.7746375951841741, val loss: 0.8693817257881165\n",
      "Epoch 267: train loss: 0.774582003503405, val loss: 0.8693405836820602\n",
      "Epoch 268: train loss: 0.7745143767471837, val loss: 0.8693082630634308\n",
      "Epoch 269: train loss: 0.7744505546210599, val loss: 0.8691350072622299\n",
      "Epoch 270: train loss: 0.7743809842198786, val loss: 0.869117483496666\n",
      "Epoch 271: train loss: 0.7743383412733456, val loss: 0.8690671622753143\n",
      "Epoch 272: train loss: 0.7742651023143493, val loss: 0.8689997792243958\n",
      "Epoch 273: train loss: 0.7742014723796152, val loss: 0.8689609169960022\n",
      "Epoch 274: train loss: 0.7741320717961755, val loss: 0.8689438551664352\n",
      "Epoch 275: train loss: 0.77409245932036, val loss: 0.8688033819198608\n",
      "Epoch 276: train loss: 0.7740307928144211, val loss: 0.8687710762023926\n",
      "Epoch 277: train loss: 0.7739609540268239, val loss: 0.8686321824789047\n",
      "Epoch 278: train loss: 0.7739060606414374, val loss: 0.8686713725328445\n",
      "Epoch 279: train loss: 0.7738464780782259, val loss: 0.8685750812292099\n",
      "Epoch 280: train loss: 0.7737939496984361, val loss: 0.8685861527919769\n",
      "Epoch 281: train loss: 0.7737275181896194, val loss: 0.8684620261192322\n",
      "Epoch 282: train loss: 0.7736705308722266, val loss: 0.8683848083019257\n",
      "Epoch 283: train loss: 0.7736168738012803, val loss: 0.8683031499385834\n",
      "Epoch 284: train loss: 0.7735646134184607, val loss: 0.8682187497615814\n",
      "Epoch 285: train loss: 0.7735076207072468, val loss: 0.8682901561260223\n",
      "Epoch 286: train loss: 0.7734480007962049, val loss: 0.8682121187448502\n",
      "Epoch 287: train loss: 0.7734017906645395, val loss: 0.8681424409151077\n",
      "Epoch 288: train loss: 0.7733492156486425, val loss: 0.8679858446121216\n",
      "Epoch 289: train loss: 0.7732836662535183, val loss: 0.8680356442928314\n",
      "Epoch 290: train loss: 0.7732390557695393, val loss: 0.8679203987121582\n",
      "Epoch 291: train loss: 0.7731819997150031, val loss: 0.8678550273180008\n",
      "Epoch 292: train loss: 0.7731329030127589, val loss: 0.8678779751062393\n",
      "Epoch 293: train loss: 0.7730862527491804, val loss: 0.8678447604179382\n",
      "Epoch 294: train loss: 0.7730341330730459, val loss: 0.8677403479814529\n",
      "Epoch 295: train loss: 0.7729752426714493, val loss: 0.8676875680685043\n",
      "Epoch 296: train loss: 0.7729282730819742, val loss: 0.8676324784755707\n",
      "Epoch 297: train loss: 0.7728783954829416, val loss: 0.8675610423088074\n",
      "Epoch 298: train loss: 0.7728312485010084, val loss: 0.8675826042890549\n",
      "Epoch 299: train loss: 0.7727771068624093, val loss: 0.8675171881914139\n",
      "Epoch 300: train loss: 0.7727314839173048, val loss: 0.8674379140138626\n",
      "Epoch 301: train loss: 0.7726780150654035, val loss: 0.8674623817205429\n",
      "Epoch 302: train loss: 0.7726204384584436, val loss: 0.8673557788133621\n",
      "Epoch 303: train loss: 0.7725791638289273, val loss: 0.8673128336668015\n",
      "Epoch 304: train loss: 0.7725459753746536, val loss: 0.8673109263181686\n",
      "Epoch 305: train loss: 0.772492125372371, val loss: 0.8671726286411285\n",
      "Epoch 306: train loss: 0.7724327582893945, val loss: 0.867158055305481\n",
      "Epoch 307: train loss: 0.7724013121367201, val loss: 0.8671625405550003\n",
      "Epoch 308: train loss: 0.7723537947920481, val loss: 0.8670429885387421\n",
      "Epoch 309: train loss: 0.7723075746516316, val loss: 0.8670473545789719\n",
      "Epoch 310: train loss: 0.7722580123866785, val loss: 0.8668925613164902\n",
      "Epoch 311: train loss: 0.7722180953644258, val loss: 0.8669055998325348\n",
      "Epoch 312: train loss: 0.7721698130707615, val loss: 0.8668968379497528\n",
      "Epoch 313: train loss: 0.7721248172303891, val loss: 0.8668777346611023\n",
      "Epoch 314: train loss: 0.772078414985223, val loss: 0.8667377382516861\n",
      "Epoch 315: train loss: 0.7720346116077195, val loss: 0.866784006357193\n",
      "Epoch 316: train loss: 0.7719921519537448, val loss: 0.8667259961366653\n",
      "Epoch 317: train loss: 0.7719563441859161, val loss: 0.8666840046644211\n",
      "Epoch 318: train loss: 0.7719009395169105, val loss: 0.8666600584983826\n",
      "Epoch 319: train loss: 0.7718717411272434, val loss: 0.8666379004716873\n",
      "Epoch 320: train loss: 0.7718199762360249, val loss: 0.8665475100278854\n",
      "Epoch 321: train loss: 0.7717782965441694, val loss: 0.8664795011281967\n",
      "Epoch 322: train loss: 0.7717480216295802, val loss: 0.8664865791797638\n",
      "Epoch 323: train loss: 0.7716992457525982, val loss: 0.8664102107286453\n",
      "Epoch 324: train loss: 0.7716572413994416, val loss: 0.8663841038942337\n",
      "Epoch 325: train loss: 0.7716250157714553, val loss: 0.8663174211978912\n",
      "Epoch 326: train loss: 0.7715791172351918, val loss: 0.8663285374641418\n",
      "Epoch 327: train loss: 0.771535847067093, val loss: 0.8662132024765015\n",
      "Epoch 328: train loss: 0.7715007130206313, val loss: 0.8661916553974152\n",
      "Epoch 329: train loss: 0.7714651203046473, val loss: 0.8661676645278931\n",
      "Epoch 330: train loss: 0.7714244682817668, val loss: 0.8661630898714066\n",
      "Epoch 331: train loss: 0.7713756780342431, val loss: 0.8660538047552109\n",
      "Epoch 332: train loss: 0.7713305597132384, val loss: 0.8660346120595932\n",
      "Epoch 333: train loss: 0.7713030200696233, val loss: 0.8659756183624268\n",
      "Epoch 334: train loss: 0.7712597937336236, val loss: 0.8659737557172775\n",
      "Epoch 335: train loss: 0.7712343163406176, val loss: 0.8658791929483414\n",
      "Epoch 336: train loss: 0.7711914964105443, val loss: 0.8659096658229828\n",
      "Epoch 337: train loss: 0.7711478388219907, val loss: 0.8658968061208725\n",
      "Epoch 338: train loss: 0.7711103814211362, val loss: 0.8658628016710281\n",
      "Epoch 339: train loss: 0.7710827051794867, val loss: 0.8657969087362289\n",
      "Epoch 340: train loss: 0.7710456967704322, val loss: 0.865816742181778\n",
      "Epoch 341: train loss: 0.7709979075538065, val loss: 0.8656899929046631\n",
      "Epoch 342: train loss: 0.7709724516192514, val loss: 0.8656041920185089\n",
      "Epoch 343: train loss: 0.7709233280842073, val loss: 0.8656068444252014\n",
      "Epoch 344: train loss: 0.770905428406305, val loss: 0.8656982332468033\n",
      "Epoch 345: train loss: 0.7708619719274447, val loss: 0.8655169010162354\n",
      "Epoch 346: train loss: 0.7708313865857902, val loss: 0.8656159788370132\n",
      "Epoch 347: train loss: 0.7707937290488563, val loss: 0.8654634356498718\n",
      "Epoch 348: train loss: 0.7707571045747188, val loss: 0.8654596358537674\n",
      "Epoch 349: train loss: 0.7707245993832287, val loss: 0.8655493855476379\n",
      "Epoch 350: train loss: 0.7706842971798331, val loss: 0.8653929680585861\n",
      "Epoch 351: train loss: 0.7706565283474989, val loss: 0.8653419464826584\n",
      "Epoch 352: train loss: 0.7706355397430988, val loss: 0.865341454744339\n",
      "Epoch 353: train loss: 0.7705954988187375, val loss: 0.8652721643447876\n",
      "Epoch 354: train loss: 0.7705416784922074, val loss: 0.8652143031358719\n",
      "Epoch 355: train loss: 0.7705180711000038, val loss: 0.8652443885803223\n",
      "Epoch 356: train loss: 0.7704839868430257, val loss: 0.865162804722786\n",
      "Epoch 357: train loss: 0.7704534968181749, val loss: 0.8651420772075653\n",
      "Epoch 358: train loss: 0.7704237681204725, val loss: 0.86513651907444\n",
      "Epoch 359: train loss: 0.770385812287703, val loss: 0.8650988340377808\n",
      "Epoch 360: train loss: 0.7703593530050488, val loss: 0.8650576323270798\n",
      "Epoch 361: train loss: 0.770319517824783, val loss: 0.865000918507576\n",
      "Epoch 362: train loss: 0.7702968568826805, val loss: 0.8649693131446838\n",
      "Epoch 363: train loss: 0.7702690138609648, val loss: 0.8649850338697433\n",
      "Epoch 364: train loss: 0.7702373377796249, val loss: 0.864886000752449\n",
      "Epoch 365: train loss: 0.77020238034673, val loss: 0.8649342656135559\n",
      "Epoch 366: train loss: 0.7701774490148325, val loss: 0.864879846572876\n",
      "Epoch 367: train loss: 0.7701454732161805, val loss: 0.864846482872963\n",
      "Epoch 368: train loss: 0.7701104776955087, val loss: 0.8648230731487274\n",
      "Epoch 369: train loss: 0.7700873298296076, val loss: 0.8647481799125671\n",
      "Epoch 370: train loss: 0.770052933879866, val loss: 0.8647963255643845\n",
      "Epoch 371: train loss: 0.7700200241323157, val loss: 0.8646604269742966\n",
      "Epoch 372: train loss: 0.769995955772861, val loss: 0.8646943271160126\n",
      "Epoch 373: train loss: 0.7699618061084991, val loss: 0.8646740615367889\n",
      "Epoch 374: train loss: 0.7699275607378878, val loss: 0.8646299839019775\n",
      "Epoch 375: train loss: 0.7698976671878749, val loss: 0.8645845651626587\n",
      "Epoch 376: train loss: 0.7698781073424443, val loss: 0.864619567990303\n",
      "Epoch 377: train loss: 0.7698390331972428, val loss: 0.8645366728305817\n",
      "Epoch 378: train loss: 0.7698107458765465, val loss: 0.8644547164440155\n",
      "Epoch 379: train loss: 0.7697997809196367, val loss: 0.8644397258758545\n",
      "Epoch 380: train loss: 0.7697639004812642, val loss: 0.8644371628761292\n",
      "Epoch 381: train loss: 0.7697315914924532, val loss: 0.8643894493579865\n",
      "Epoch 382: train loss: 0.7697053962149522, val loss: 0.8643133044242859\n",
      "Epoch 383: train loss: 0.7696736327614982, val loss: 0.8643062859773636\n",
      "Epoch 384: train loss: 0.7696479678115047, val loss: 0.8643019944429398\n",
      "Epoch 385: train loss: 0.7696215740920126, val loss: 0.8642518371343613\n",
      "Epoch 386: train loss: 0.7695982330805802, val loss: 0.8642613589763641\n",
      "Epoch 387: train loss: 0.7695794828535489, val loss: 0.8642060160636902\n",
      "Epoch 388: train loss: 0.7695456764854586, val loss: 0.8642037659883499\n",
      "Epoch 389: train loss: 0.7695155285106535, val loss: 0.8641840815544128\n",
      "Epoch 390: train loss: 0.7694874631739332, val loss: 0.864202231168747\n",
      "Epoch 391: train loss: 0.7694732535901859, val loss: 0.8640799075365067\n",
      "Epoch 392: train loss: 0.7694265291761706, val loss: 0.8640566766262054\n",
      "Epoch 393: train loss: 0.7694091750920422, val loss: 0.8641231954097748\n",
      "Epoch 394: train loss: 0.7693835527863389, val loss: 0.8640790283679962\n",
      "Epoch 395: train loss: 0.7693604222507657, val loss: 0.8639324903488159\n",
      "Epoch 396: train loss: 0.7693322495049567, val loss: 0.8640279471874237\n",
      "Epoch 397: train loss: 0.7693062947490409, val loss: 0.8639969825744629\n",
      "Epoch 398: train loss: 0.7692806165773702, val loss: 0.8639613538980484\n",
      "Epoch 399: train loss: 0.7692563584800329, val loss: 0.863922506570816\n",
      "Epoch 400: train loss: 0.7692308374301782, val loss: 0.8638666868209839\n",
      "Epoch 401: train loss: 0.7692131260102655, val loss: 0.8638456165790558\n",
      "Epoch 402: train loss: 0.769181260833628, val loss: 0.8638882488012314\n",
      "Epoch 403: train loss: 0.7691585855794008, val loss: 0.8637820780277252\n",
      "Epoch 404: train loss: 0.7691272729091962, val loss: 0.8638389408588409\n",
      "Epoch 405: train loss: 0.7691007444415362, val loss: 0.8637789040803909\n",
      "Epoch 406: train loss: 0.76908101491467, val loss: 0.8637556880712509\n",
      "Epoch 407: train loss: 0.7690615560135317, val loss: 0.8637163937091827\n",
      "Epoch 408: train loss: 0.7690278130861249, val loss: 0.8636481165885925\n",
      "Epoch 409: train loss: 0.7690144270045116, val loss: 0.863728478550911\n",
      "Epoch 410: train loss: 0.7689835117050964, val loss: 0.863671064376831\n",
      "Epoch 411: train loss: 0.7689660137597738, val loss: 0.8636634349822998\n",
      "Epoch 412: train loss: 0.7689415911411546, val loss: 0.8635578006505966\n",
      "Epoch 413: train loss: 0.7689128389587514, val loss: 0.8635390847921371\n",
      "Epoch 414: train loss: 0.7688963740762251, val loss: 0.8635721802711487\n",
      "Epoch 415: train loss: 0.7688712103037381, val loss: 0.8635856360197067\n",
      "Epoch 416: train loss: 0.7688508734365025, val loss: 0.8635002821683884\n",
      "Epoch 417: train loss: 0.7688321843134815, val loss: 0.8635038435459137\n",
      "Epoch 418: train loss: 0.7688112028164632, val loss: 0.8634347468614578\n",
      "Epoch 419: train loss: 0.7687820690358164, val loss: 0.8634515404701233\n",
      "Epoch 420: train loss: 0.7687678398240123, val loss: 0.8634324818849564\n",
      "Epoch 421: train loss: 0.7687340933915954, val loss: 0.8633313775062561\n",
      "Epoch 422: train loss: 0.7687087325791213, val loss: 0.863360121846199\n",
      "Epoch 423: train loss: 0.7686863556207297, val loss: 0.863280400633812\n",
      "Epoch 424: train loss: 0.768668974314356, val loss: 0.863324299454689\n",
      "Epoch 425: train loss: 0.768654565412365, val loss: 0.863301083445549\n",
      "Epoch 426: train loss: 0.7686335711999487, val loss: 0.8633326888084412\n",
      "Epoch 427: train loss: 0.768605914586356, val loss: 0.8631902188062668\n",
      "Epoch 428: train loss: 0.7685847093333843, val loss: 0.8632004708051682\n",
      "Epoch 429: train loss: 0.7685626593534949, val loss: 0.8632005751132965\n",
      "Epoch 430: train loss: 0.7685406060633181, val loss: 0.8631413877010345\n",
      "Epoch 431: train loss: 0.7685191884651483, val loss: 0.8631613254547119\n",
      "Epoch 432: train loss: 0.7684941165239271, val loss: 0.8632226586341858\n",
      "Epoch 433: train loss: 0.7684805755598099, val loss: 0.8630950003862381\n",
      "Epoch 434: train loss: 0.7684585478562072, val loss: 0.8630734086036682\n",
      "Epoch 435: train loss: 0.7684439217993653, val loss: 0.8630411326885223\n",
      "Epoch 436: train loss: 0.7684059981932168, val loss: 0.8630475699901581\n",
      "Epoch 437: train loss: 0.7683938710138869, val loss: 0.8630394041538239\n",
      "Epoch 438: train loss: 0.7683688322534283, val loss: 0.8630416691303253\n",
      "Epoch 439: train loss: 0.7683546941524158, val loss: 0.8629363626241684\n",
      "Epoch 440: train loss: 0.7683288827124705, val loss: 0.8629323244094849\n",
      "Epoch 441: train loss: 0.7683134895452134, val loss: 0.8629930764436722\n",
      "Epoch 442: train loss: 0.7682888072442868, val loss: 0.863039493560791\n",
      "Epoch 443: train loss: 0.7682782685745, val loss: 0.8629850149154663\n",
      "Epoch 444: train loss: 0.7682566843909868, val loss: 0.8629099428653717\n",
      "Epoch 00445: reducing learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 445: train loss: 0.7682313238511246, val loss: 0.8629018217325211\n",
      "Epoch 446: train loss: 0.7682194247116493, val loss: 0.8628781735897064\n",
      "Epoch 447: train loss: 0.7682125810625038, val loss: 0.8628357499837875\n",
      "Epoch 448: train loss: 0.7681966702052481, val loss: 0.8629334568977356\n",
      "Epoch 449: train loss: 0.768186158076177, val loss: 0.8628777116537094\n",
      "Epoch 450: train loss: 0.7681939567237538, val loss: 0.8628620058298111\n",
      "Epoch 451: train loss: 0.76818147127397, val loss: 0.8627288490533829\n",
      "Epoch 452: train loss: 0.7681822368460751, val loss: 0.862811952829361\n",
      "Epoch 453: train loss: 0.7681684804990844, val loss: 0.8629304468631744\n",
      "Epoch 454: train loss: 0.7681707134631598, val loss: 0.8628178238868713\n",
      "Epoch 455: train loss: 0.7681469837071575, val loss: 0.862792581319809\n",
      "Epoch 456: train loss: 0.7681610506369919, val loss: 0.8627841472625732\n",
      "Epoch 00457: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 457: train loss: 0.7681475551211728, val loss: 0.8627365678548813\n",
      "Epoch 458: train loss: 0.7681492491315214, val loss: 0.8627134412527084\n",
      "Epoch 459: train loss: 0.7681511158415089, val loss: 0.8627279102802277\n",
      "Epoch 460: train loss: 0.7681405382466371, val loss: 0.8627997487783432\n",
      "Epoch 461: train loss: 0.7681291149901783, val loss: 0.8627792298793793\n",
      "Epoch 462: train loss: 0.768128722857432, val loss: 0.8627708554267883\n",
      "Epoch 00463: reducing learning rate of group 0 to 2.7000e-03.\n",
      "Epoch 463: train loss: 0.7681256763805014, val loss: 0.8627983033657074\n",
      "Epoch 464: train loss: 0.7681296917785425, val loss: 0.8627840876579285\n",
      "Epoch 465: train loss: 0.768134151942899, val loss: 0.8627796173095703\n",
      "Epoch 466: train loss: 0.768132053493324, val loss: 0.8627874851226807\n",
      "Epoch 467: train loss: 0.7681335660609806, val loss: 0.8628007471561432\n",
      "Epoch 468: train loss: 0.768130837040608, val loss: 0.8627897053956985\n",
      "Epoch 00469: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 469: train loss: 0.7681206641770468, val loss: 0.8627852946519852\n",
      "Epoch 470: train loss: 0.7681215183480197, val loss: 0.8627505749464035\n",
      "Epoch 471: train loss: 0.7681270436265429, val loss: 0.8627531379461288\n",
      "Epoch 472: train loss: 0.7681283580248203, val loss: 0.8627432435750961\n",
      "Epoch 473: train loss: 0.768124811051913, val loss: 0.8627737015485764\n",
      "Epoch 474: train loss: 0.768128416441656, val loss: 0.8627209961414337\n",
      "Epoch 00475: reducing learning rate of group 0 to 2.4300e-04.\n",
      "Epoch 475: train loss: 0.7681298797444478, val loss: 0.8627727329730988\n",
      "Epoch 476: train loss: 0.7681393361060458, val loss: 0.8627045601606369\n",
      "Epoch 477: train loss: 0.7681309801229111, val loss: 0.8627627938985825\n",
      "Epoch 478: train loss: 0.7681300913107547, val loss: 0.8628000915050507\n",
      "Epoch 479: train loss: 0.768124673149236, val loss: 0.8627981543540955\n",
      "Epoch 480: train loss: 0.7681290179792863, val loss: 0.862781897187233\n",
      "Epoch 00481: reducing learning rate of group 0 to 7.2900e-05.\n",
      "Epoch 481: train loss: 0.7681329078395474, val loss: 0.8627651780843735\n",
      "Epoch 482: train loss: 0.7681246056777907, val loss: 0.862759456038475\n",
      "Epoch 483: train loss: 0.7681206472945813, val loss: 0.8627486526966095\n",
      "Epoch 484: train loss: 0.7681296459213264, val loss: 0.8628270328044891\n",
      "Epoch 485: train loss: 0.7681286821214252, val loss: 0.862773597240448\n",
      "Epoch 486: train loss: 0.7681273259356379, val loss: 0.86279296875\n",
      "Epoch 00487: reducing learning rate of group 0 to 2.1870e-05.\n",
      "Epoch 487: train loss: 0.7681334600733682, val loss: 0.8627841919660568\n",
      "Epoch 488: train loss: 0.7681211974448684, val loss: 0.8627780824899673\n",
      "Epoch 489: train loss: 0.7681248743952352, val loss: 0.8627729266881943\n",
      "Epoch 490: train loss: 0.7681243451387031, val loss: 0.8627771884202957\n",
      "Epoch 491: train loss: 0.7681247830702487, val loss: 0.8627687692642212\n",
      "Epoch 492: train loss: 0.7681359150993088, val loss: 0.8627433627843857\n",
      "Epoch 00493: reducing learning rate of group 0 to 6.5610e-06.\n",
      "Epoch 493: train loss: 0.7681225355603811, val loss: 0.8627521842718124\n",
      "Epoch 494: train loss: 0.7681286887809444, val loss: 0.8627659678459167\n",
      "Epoch 495: train loss: 0.7681268144962407, val loss: 0.862763449549675\n",
      "Epoch 496: train loss: 0.7681275567989729, val loss: 0.8627813756465912\n",
      "Epoch 497: train loss: 0.7681254552338335, val loss: 0.8628260046243668\n",
      "Epoch 498: train loss: 0.768128237296693, val loss: 0.8627278655767441\n",
      "Epoch 00499: reducing learning rate of group 0 to 1.9683e-06.\n",
      "Epoch 499: train loss: 0.7681283589789619, val loss: 0.8627894073724747\n",
      "Epoch 500: train loss: 0.76812280961323, val loss: 0.86279296875\n",
      "Epoch 501: train loss: 0.7681304468356173, val loss: 0.8627598881721497\n",
      "Epoch 502: train loss: 0.7681288521144173, val loss: 0.8627672642469406\n",
      "Epoch 503: train loss: 0.7681251136900669, val loss: 0.8628346920013428\n",
      "Epoch 504: train loss: 0.7681312340608962, val loss: 0.8627492189407349\n",
      "Epoch 00505: reducing learning rate of group 0 to 5.9049e-07.\n",
      "Epoch 505: train loss: 0.7681346387888084, val loss: 0.8628797978162766\n",
      "Early stop at epoch 505\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_x = ae_model.encoder(torch.tensor(x_pretrain, dtype=torch.float).to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf4ea610c38473b8221fad7917a6539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.1516941652621993, val loss: 0.6298345625400543\n",
      "Epoch 2: train loss: 0.5363350652063333, val loss: 0.4943477138876915\n",
      "Epoch 3: train loss: 0.47830825309977737, val loss: 0.46450424939393997\n",
      "Epoch 4: train loss: 0.4339446583650191, val loss: 0.42630717158317566\n",
      "Epoch 5: train loss: 0.4121505918942102, val loss: 0.3848237693309784\n",
      "Epoch 6: train loss: 0.3806552706162784, val loss: 0.39149825274944305\n",
      "Epoch 7: train loss: 0.34093005140791216, val loss: 0.31563978642225266\n",
      "Epoch 8: train loss: 0.3052934439635596, val loss: 0.29482533037662506\n",
      "Epoch 9: train loss: 0.280585488494959, val loss: 0.27971335500478745\n",
      "Epoch 10: train loss: 0.2593205975127041, val loss: 0.26149865612387657\n",
      "Epoch 11: train loss: 0.23717140270794423, val loss: 0.20947310328483582\n",
      "Epoch 12: train loss: 0.2148386099220919, val loss: 0.21185561269521713\n",
      "Epoch 13: train loss: 0.19455166818249736, val loss: 0.1873474456369877\n",
      "Epoch 14: train loss: 0.1777015946652133, val loss: 0.1461114976555109\n",
      "Epoch 15: train loss: 0.158997150485479, val loss: 0.1532117798924446\n",
      "Epoch 16: train loss: 0.14262752141468824, val loss: 0.13272357173264027\n",
      "Epoch 17: train loss: 0.12647043995629803, val loss: 0.11987384781241417\n",
      "Epoch 18: train loss: 0.11490213867594548, val loss: 0.10966015420854092\n",
      "Epoch 19: train loss: 0.10491364631815231, val loss: 0.09610402770340443\n",
      "Epoch 20: train loss: 0.09164510274714249, val loss: 0.08264616318047047\n",
      "Epoch 21: train loss: 0.0826198016042337, val loss: 0.07700977101922035\n",
      "Epoch 22: train loss: 0.07459336172147966, val loss: 0.07289610989391804\n",
      "Epoch 23: train loss: 0.06695502457062585, val loss: 0.0634210528805852\n",
      "Epoch 24: train loss: 0.05900588247174143, val loss: 0.0535883791744709\n",
      "Epoch 25: train loss: 0.05311138659219109, val loss: 0.04878779407590628\n",
      "Epoch 26: train loss: 0.046527641584712745, val loss: 0.04941506404429674\n",
      "Epoch 27: train loss: 0.04132069058762544, val loss: 0.041504702530801296\n",
      "Epoch 28: train loss: 0.03732510407360034, val loss: 0.03800389729440212\n",
      "Epoch 29: train loss: 0.033464152922099887, val loss: 0.030689595267176628\n",
      "Epoch 30: train loss: 0.0305140493976178, val loss: 0.027770948130637407\n",
      "Epoch 31: train loss: 0.027933854360847797, val loss: 0.02758071292191744\n",
      "Epoch 32: train loss: 0.026081052344999488, val loss: 0.0248863622546196\n",
      "Epoch 33: train loss: 0.024463712824681974, val loss: 0.023158080875873566\n",
      "Epoch 34: train loss: 0.023115718205407997, val loss: 0.02377088600769639\n",
      "Epoch 35: train loss: 0.022119614389676044, val loss: 0.021174789872020483\n",
      "Epoch 36: train loss: 0.02110894533207081, val loss: 0.021328980568796396\n",
      "Epoch 37: train loss: 0.020576769879774814, val loss: 0.021849620155990124\n",
      "Epoch 38: train loss: 0.0201093336486351, val loss: 0.02038888167589903\n",
      "Epoch 39: train loss: 0.01963284155685541, val loss: 0.02072814665734768\n",
      "Epoch 40: train loss: 0.019187455253881866, val loss: 0.021517762914299965\n",
      "Epoch 41: train loss: 0.01920168724961333, val loss: 0.021741847973316908\n",
      "Epoch 42: train loss: 0.019182985693542483, val loss: 0.018884793855249882\n",
      "Epoch 43: train loss: 0.018979330477479704, val loss: 0.01892799139022827\n",
      "Epoch 44: train loss: 0.018711457348452094, val loss: 0.021217565052211285\n",
      "Epoch 45: train loss: 0.01881758126374535, val loss: 0.019938183017075062\n",
      "Epoch 46: train loss: 0.01866767419047583, val loss: 0.021706179715692997\n",
      "Epoch 47: train loss: 0.018641936631702258, val loss: 0.018609354738146067\n",
      "Epoch 48: train loss: 0.01822702216464577, val loss: 0.02069671079516411\n",
      "Epoch 49: train loss: 0.018535092231452135, val loss: 0.02050459710881114\n",
      "Epoch 50: train loss: 0.01824184446219855, val loss: 0.01966549176722765\n",
      "Epoch 51: train loss: 0.018247615951254882, val loss: 0.01968403672799468\n",
      "Epoch 52: train loss: 0.018044444329230976, val loss: 0.01878244150429964\n",
      "Epoch 00053: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 53: train loss: 0.018436002831970583, val loss: 0.01939850812777877\n",
      "Epoch 54: train loss: 0.01739770803407044, val loss: 0.0185323441401124\n",
      "Epoch 55: train loss: 0.01737976754987865, val loss: 0.019125221064314246\n",
      "Epoch 56: train loss: 0.0171958029681524, val loss: 0.016848847502842546\n",
      "Epoch 57: train loss: 0.017143334691375932, val loss: 0.017440042924135923\n",
      "Epoch 58: train loss: 0.016862036380703854, val loss: 0.01849025720730424\n",
      "Epoch 59: train loss: 0.016784382447945997, val loss: 0.01656201365403831\n",
      "Epoch 60: train loss: 0.016908950138288224, val loss: 0.017749547958374023\n",
      "Epoch 61: train loss: 0.01700652499124286, val loss: 0.017465385608375072\n",
      "Epoch 62: train loss: 0.016629803022421466, val loss: 0.016748129855841398\n",
      "Epoch 63: train loss: 0.0166774782404177, val loss: 0.019167593214660883\n",
      "Epoch 64: train loss: 0.016970351912210246, val loss: 0.01811078330501914\n",
      "Epoch 00065: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 65: train loss: 0.01686211612485702, val loss: 0.017951685469597578\n",
      "Epoch 66: train loss: 0.016597417243158493, val loss: 0.015943495091050863\n",
      "Epoch 67: train loss: 0.016665362880456606, val loss: 0.018277422059327364\n",
      "Epoch 68: train loss: 0.01639645614432173, val loss: 0.01925029419362545\n",
      "Epoch 69: train loss: 0.016544378551537987, val loss: 0.01759660616517067\n",
      "Epoch 70: train loss: 0.016430316525166116, val loss: 0.0181609857827425\n",
      "Epoch 71: train loss: 0.01630448994197241, val loss: 0.017545892857015133\n",
      "Epoch 00072: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 72: train loss: 0.016208445425737104, val loss: 0.017791380174458027\n",
      "Epoch 73: train loss: 0.016297902068204154, val loss: 0.019493278115987778\n",
      "Epoch 74: train loss: 0.016261242946189376, val loss: 0.018044669181108475\n",
      "Epoch 75: train loss: 0.016426797097013963, val loss: 0.019316072575747967\n",
      "Epoch 76: train loss: 0.016253965215228695, val loss: 0.018982699140906334\n",
      "Epoch 77: train loss: 0.01624826924453682, val loss: 0.017615247517824173\n",
      "Epoch 00078: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 78: train loss: 0.016279897188578543, val loss: 0.018241440877318382\n",
      "Epoch 79: train loss: 0.016028760763524757, val loss: 0.017736372537910938\n",
      "Epoch 80: train loss: 0.01619321777528211, val loss: 0.017653378657996655\n",
      "Epoch 81: train loss: 0.016084370320758393, val loss: 0.018283738289028406\n",
      "Epoch 82: train loss: 0.016486249694327482, val loss: 0.0185015550814569\n",
      "Epoch 83: train loss: 0.016189965280427026, val loss: 0.01858598319813609\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 84: train loss: 0.016205525824697295, val loss: 0.017950577195733786\n",
      "Epoch 85: train loss: 0.01609453610797181, val loss: 0.01727727660909295\n",
      "Epoch 86: train loss: 0.01618512814371016, val loss: 0.017651349306106567\n",
      "Epoch 87: train loss: 0.016356509796513215, val loss: 0.017694054171442986\n",
      "Epoch 88: train loss: 0.01629007202066537, val loss: 0.018563177436590195\n",
      "Epoch 89: train loss: 0.016283407023286692, val loss: 0.018195169512182474\n",
      "Epoch 00090: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 90: train loss: 0.01620538133065399, val loss: 0.019520684611052275\n",
      "Early stop at epoch 90\n"
     ]
    }
   ],
   "source": [
    "# model declaration\n",
    "nn_model = Net()\n",
    "nn_model.to(device)\n",
    "nn_model.train()\n",
    "    \n",
    "def train_nn():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(pretrained_x, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001, weight_decay=0.0001)    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_x_train = ae_model.encoder(torch.tensor(x_train, dtype=torch.float).to(device))\n",
    "featured_x_train = nn_model.encode(featured_x_train).detach().cpu().numpy()\n",
    "scaler = StandardScaler()\n",
    "featured_x_train = scaler.fit_transform(featured_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class onelayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16, 1)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03f904b10c24866b954cb7eb0c54b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 3.026984214782715\n",
      "Epoch 20: train loss: 2.976902961730957\n",
      "Epoch 30: train loss: 2.936845064163208\n",
      "Epoch 40: train loss: 2.895547389984131\n",
      "Epoch 50: train loss: 2.8560314178466797\n",
      "Epoch 60: train loss: 2.817919969558716\n",
      "Epoch 70: train loss: 2.78074312210083\n",
      "Epoch 80: train loss: 2.744584798812866\n",
      "Epoch 90: train loss: 2.70926570892334\n",
      "Epoch 100: train loss: 2.6746826171875\n",
      "Epoch 110: train loss: 2.640730619430542\n",
      "Epoch 120: train loss: 2.6073248386383057\n",
      "Epoch 130: train loss: 2.574399471282959\n",
      "Epoch 140: train loss: 2.5419046878814697\n",
      "Epoch 150: train loss: 2.5098044872283936\n",
      "Epoch 160: train loss: 2.478074312210083\n",
      "Epoch 170: train loss: 2.4466981887817383\n",
      "Epoch 180: train loss: 2.415666103363037\n",
      "Epoch 190: train loss: 2.3849713802337646\n",
      "Epoch 200: train loss: 2.3546102046966553\n",
      "Epoch 210: train loss: 2.3245797157287598\n",
      "Epoch 220: train loss: 2.294877767562866\n",
      "Epoch 230: train loss: 2.2655022144317627\n",
      "Epoch 240: train loss: 2.2364513874053955\n",
      "Epoch 250: train loss: 2.2077231407165527\n",
      "Epoch 260: train loss: 2.179314613342285\n",
      "Epoch 270: train loss: 2.151223659515381\n",
      "Epoch 280: train loss: 2.123448371887207\n",
      "Epoch 290: train loss: 2.095984935760498\n",
      "Epoch 300: train loss: 2.068830728530884\n",
      "Epoch 310: train loss: 2.0419833660125732\n",
      "Epoch 320: train loss: 2.0154385566711426\n",
      "Epoch 330: train loss: 1.9891940355300903\n",
      "Epoch 340: train loss: 1.963246464729309\n",
      "Epoch 350: train loss: 1.9375921487808228\n",
      "Epoch 360: train loss: 1.9122282266616821\n",
      "Epoch 370: train loss: 1.8871511220932007\n",
      "Epoch 380: train loss: 1.862357497215271\n",
      "Epoch 390: train loss: 1.8378442525863647\n",
      "Epoch 400: train loss: 1.8136080503463745\n",
      "Epoch 410: train loss: 1.7896450757980347\n",
      "Epoch 420: train loss: 1.7659530639648438\n",
      "Epoch 430: train loss: 1.7425278425216675\n",
      "Epoch 440: train loss: 1.719366431236267\n",
      "Epoch 450: train loss: 1.696466326713562\n",
      "Epoch 460: train loss: 1.6738238334655762\n",
      "Epoch 470: train loss: 1.651435375213623\n",
      "Epoch 480: train loss: 1.6292986869812012\n",
      "Epoch 490: train loss: 1.607411503791809\n",
      "Epoch 500: train loss: 1.5857692956924438\n",
      "Epoch 510: train loss: 1.5643694400787354\n",
      "Epoch 520: train loss: 1.5432099103927612\n",
      "Epoch 530: train loss: 1.522288203239441\n",
      "Epoch 540: train loss: 1.5016008615493774\n",
      "Epoch 550: train loss: 1.4811458587646484\n",
      "Epoch 560: train loss: 1.4609203338623047\n",
      "Epoch 570: train loss: 1.4409219026565552\n",
      "Epoch 580: train loss: 1.4211480617523193\n",
      "Epoch 590: train loss: 1.4015960693359375\n",
      "Epoch 600: train loss: 1.3822637796401978\n",
      "Epoch 610: train loss: 1.3631489276885986\n",
      "Epoch 620: train loss: 1.34424889087677\n",
      "Epoch 630: train loss: 1.325561761856079\n",
      "Epoch 640: train loss: 1.3070855140686035\n",
      "Epoch 650: train loss: 1.2888174057006836\n",
      "Epoch 660: train loss: 1.2707560062408447\n",
      "Epoch 670: train loss: 1.2529003620147705\n",
      "Epoch 680: train loss: 1.235247015953064\n",
      "Epoch 690: train loss: 1.2177940607070923\n",
      "Epoch 700: train loss: 1.2005404233932495\n",
      "Epoch 710: train loss: 1.1834839582443237\n",
      "Epoch 720: train loss: 1.1666224002838135\n",
      "Epoch 730: train loss: 1.1499550342559814\n",
      "Epoch 740: train loss: 1.1334788799285889\n",
      "Epoch 750: train loss: 1.1171934604644775\n",
      "Epoch 760: train loss: 1.1010959148406982\n",
      "Epoch 770: train loss: 1.0851855278015137\n",
      "Epoch 780: train loss: 1.0694602727890015\n",
      "Epoch 790: train loss: 1.0539182424545288\n",
      "Epoch 800: train loss: 1.0385586023330688\n",
      "Epoch 810: train loss: 1.0233793258666992\n",
      "Epoch 820: train loss: 1.0083789825439453\n",
      "Epoch 830: train loss: 0.993556022644043\n",
      "Epoch 840: train loss: 0.9789091348648071\n",
      "Epoch 850: train loss: 0.9644365310668945\n",
      "Epoch 860: train loss: 0.9501370191574097\n",
      "Epoch 870: train loss: 0.9360090494155884\n",
      "Epoch 880: train loss: 0.922051191329956\n",
      "Epoch 890: train loss: 0.9082622528076172\n",
      "Epoch 900: train loss: 0.8946405053138733\n",
      "Epoch 910: train loss: 0.8811846971511841\n",
      "Epoch 920: train loss: 0.8678935170173645\n",
      "Epoch 930: train loss: 0.8547655940055847\n",
      "Epoch 940: train loss: 0.8417994379997253\n",
      "Epoch 950: train loss: 0.8289937973022461\n",
      "Epoch 960: train loss: 0.8163474798202515\n",
      "Epoch 970: train loss: 0.803858757019043\n",
      "Epoch 980: train loss: 0.7915267944335938\n",
      "Epoch 990: train loss: 0.7793500423431396\n",
      "Epoch 1000: train loss: 0.7673271894454956\n",
      "Epoch 1010: train loss: 0.755456805229187\n",
      "Epoch 1020: train loss: 0.7437379360198975\n",
      "Epoch 1030: train loss: 0.7321691513061523\n",
      "Epoch 1040: train loss: 0.7207491993904114\n",
      "Epoch 1050: train loss: 0.7094767689704895\n",
      "Epoch 1060: train loss: 0.6983504891395569\n",
      "Epoch 1070: train loss: 0.6873694658279419\n",
      "Epoch 1080: train loss: 0.6765321493148804\n",
      "Epoch 1090: train loss: 0.6658372282981873\n",
      "Epoch 1100: train loss: 0.655283510684967\n",
      "Epoch 1110: train loss: 0.6448698043823242\n",
      "Epoch 1120: train loss: 0.6345950365066528\n",
      "Epoch 1130: train loss: 0.6244576573371887\n",
      "Epoch 1140: train loss: 0.614456832408905\n",
      "Epoch 1150: train loss: 0.6045911312103271\n",
      "Epoch 1160: train loss: 0.5948593616485596\n",
      "Epoch 1170: train loss: 0.5852603316307068\n",
      "Epoch 1180: train loss: 0.5757927298545837\n",
      "Epoch 1190: train loss: 0.5664556622505188\n",
      "Epoch 1200: train loss: 0.5572478175163269\n",
      "Epoch 1210: train loss: 0.5481677651405334\n",
      "Epoch 1220: train loss: 0.5392144918441772\n",
      "Epoch 1230: train loss: 0.5303869247436523\n",
      "Epoch 1240: train loss: 0.5216836929321289\n",
      "Epoch 1250: train loss: 0.5131038427352905\n",
      "Epoch 1260: train loss: 0.5046460032463074\n",
      "Epoch 1270: train loss: 0.4963091313838959\n",
      "Epoch 1280: train loss: 0.4880920350551605\n",
      "Epoch 1290: train loss: 0.4799935519695282\n",
      "Epoch 1300: train loss: 0.47201257944107056\n",
      "Epoch 1310: train loss: 0.46414804458618164\n",
      "Epoch 1320: train loss: 0.45639845728874207\n",
      "Epoch 1330: train loss: 0.44876307249069214\n",
      "Epoch 1340: train loss: 0.44124072790145874\n",
      "Epoch 1350: train loss: 0.4338301122188568\n",
      "Epoch 1360: train loss: 0.4265301823616028\n",
      "Epoch 1370: train loss: 0.4193398952484131\n",
      "Epoch 1380: train loss: 0.4122580289840698\n",
      "Epoch 1390: train loss: 0.4052838087081909\n",
      "Epoch 1400: train loss: 0.398415744304657\n",
      "Epoch 1410: train loss: 0.39165282249450684\n",
      "Epoch 1420: train loss: 0.38499417901039124\n",
      "Epoch 1430: train loss: 0.37843847274780273\n",
      "Epoch 1440: train loss: 0.37198466062545776\n",
      "Epoch 1450: train loss: 0.36563169956207275\n",
      "Epoch 1460: train loss: 0.3593785762786865\n",
      "Epoch 1470: train loss: 0.35322412848472595\n",
      "Epoch 1480: train loss: 0.347167432308197\n",
      "Epoch 1490: train loss: 0.34120726585388184\n",
      "Epoch 1500: train loss: 0.3353427052497864\n",
      "Epoch 1510: train loss: 0.3295726776123047\n",
      "Epoch 1520: train loss: 0.32389605045318604\n",
      "Epoch 1530: train loss: 0.318311870098114\n",
      "Epoch 1540: train loss: 0.3128190040588379\n",
      "Epoch 1550: train loss: 0.30741655826568604\n",
      "Epoch 1560: train loss: 0.30210357904434204\n",
      "Epoch 1570: train loss: 0.29687872529029846\n",
      "Epoch 1580: train loss: 0.2917412519454956\n",
      "Epoch 1590: train loss: 0.2866901457309723\n",
      "Epoch 1600: train loss: 0.28172433376312256\n",
      "Epoch 1610: train loss: 0.2768428325653076\n",
      "Epoch 1620: train loss: 0.2720446288585663\n",
      "Epoch 1630: train loss: 0.2673287093639374\n",
      "Epoch 1640: train loss: 0.26269420981407166\n",
      "Epoch 1650: train loss: 0.258139967918396\n",
      "Epoch 1660: train loss: 0.25366514921188354\n",
      "Epoch 1670: train loss: 0.2492687702178955\n",
      "Epoch 1680: train loss: 0.24494986236095428\n",
      "Epoch 1690: train loss: 0.24070744216442108\n",
      "Epoch 1700: train loss: 0.23654061555862427\n",
      "Epoch 1710: train loss: 0.23244833946228027\n",
      "Epoch 1720: train loss: 0.22842974960803986\n",
      "Epoch 1730: train loss: 0.2244839072227478\n",
      "Epoch 1740: train loss: 0.22060993313789368\n",
      "Epoch 1750: train loss: 0.21680684387683868\n",
      "Epoch 1760: train loss: 0.21307368576526642\n",
      "Epoch 1770: train loss: 0.20940957963466644\n",
      "Epoch 1780: train loss: 0.20581357181072235\n",
      "Epoch 1790: train loss: 0.20228491723537445\n",
      "Epoch 1800: train loss: 0.19882263243198395\n",
      "Epoch 1810: train loss: 0.19542577862739563\n",
      "Epoch 1820: train loss: 0.19209350645542145\n",
      "Epoch 1830: train loss: 0.18882492184638977\n",
      "Epoch 1840: train loss: 0.18561919033527374\n",
      "Epoch 1850: train loss: 0.18247544765472412\n",
      "Epoch 1860: train loss: 0.1793927401304245\n",
      "Epoch 1870: train loss: 0.17637023329734802\n",
      "Epoch 1880: train loss: 0.1734071671962738\n",
      "Epoch 1890: train loss: 0.17050261795520782\n",
      "Epoch 1900: train loss: 0.16765588521957397\n",
      "Epoch 1910: train loss: 0.16486603021621704\n",
      "Epoch 1920: train loss: 0.16213220357894897\n",
      "Epoch 1930: train loss: 0.15945367515087128\n",
      "Epoch 1940: train loss: 0.15682953596115112\n",
      "Epoch 1950: train loss: 0.15425901114940643\n",
      "Epoch 1960: train loss: 0.15174135565757751\n",
      "Epoch 1970: train loss: 0.14927566051483154\n",
      "Epoch 1980: train loss: 0.1468612402677536\n",
      "Epoch 1990: train loss: 0.14449724555015564\n",
      "Epoch 2000: train loss: 0.14218302071094513\n",
      "Epoch 2010: train loss: 0.13991759717464447\n",
      "Epoch 2020: train loss: 0.1377003937959671\n",
      "Epoch 2030: train loss: 0.13553054630756378\n",
      "Epoch 2040: train loss: 0.13340741395950317\n",
      "Epoch 2050: train loss: 0.13133008778095245\n",
      "Epoch 2060: train loss: 0.1292978823184967\n",
      "Epoch 2070: train loss: 0.1273101419210434\n",
      "Epoch 2080: train loss: 0.12536607682704926\n",
      "Epoch 2090: train loss: 0.12346503883600235\n",
      "Epoch 2100: train loss: 0.12160635739564896\n",
      "Epoch 2110: train loss: 0.11978916078805923\n",
      "Epoch 2120: train loss: 0.11801290512084961\n",
      "Epoch 2130: train loss: 0.11627674102783203\n",
      "Epoch 2140: train loss: 0.11458015441894531\n",
      "Epoch 2150: train loss: 0.11292237788438797\n",
      "Epoch 2160: train loss: 0.11130271852016449\n",
      "Epoch 2170: train loss: 0.10972055047750473\n",
      "Epoch 2180: train loss: 0.10817514359951019\n",
      "Epoch 2190: train loss: 0.10666593909263611\n",
      "Epoch 2200: train loss: 0.10519227385520935\n",
      "Epoch 2210: train loss: 0.10375352948904037\n",
      "Epoch 2220: train loss: 0.1023489385843277\n",
      "Epoch 2230: train loss: 0.10097803175449371\n",
      "Epoch 2240: train loss: 0.09964010119438171\n",
      "Epoch 2250: train loss: 0.09833456575870514\n",
      "Epoch 2260: train loss: 0.09706082940101624\n",
      "Epoch 2270: train loss: 0.09581831097602844\n",
      "Epoch 2280: train loss: 0.09460639953613281\n",
      "Epoch 2290: train loss: 0.09342445433139801\n",
      "Epoch 2300: train loss: 0.09227197617292404\n",
      "Epoch 2310: train loss: 0.09114833921194077\n",
      "Epoch 2320: train loss: 0.09005305916070938\n",
      "Epoch 2330: train loss: 0.08898548036813736\n",
      "Epoch 2340: train loss: 0.08794503659009933\n",
      "Epoch 2350: train loss: 0.08693122863769531\n",
      "Epoch 2360: train loss: 0.0859435647726059\n",
      "Epoch 2370: train loss: 0.0849815383553505\n",
      "Epoch 2380: train loss: 0.08404465764760971\n",
      "Epoch 2390: train loss: 0.08313222974538803\n",
      "Epoch 2400: train loss: 0.08224388211965561\n",
      "Epoch 2410: train loss: 0.08137908577919006\n",
      "Epoch 2420: train loss: 0.0805373340845108\n",
      "Epoch 2430: train loss: 0.07971809804439545\n",
      "Epoch 2440: train loss: 0.07892096042633057\n",
      "Epoch 2450: train loss: 0.07814540714025497\n",
      "Epoch 2460: train loss: 0.07739099115133286\n",
      "Epoch 2470: train loss: 0.07665721327066422\n",
      "Epoch 2480: train loss: 0.07594362646341324\n",
      "Epoch 2490: train loss: 0.07524982839822769\n",
      "Epoch 2500: train loss: 0.07457535713911057\n",
      "Epoch 2510: train loss: 0.07391973584890366\n",
      "Epoch 2520: train loss: 0.0732826218008995\n",
      "Epoch 2530: train loss: 0.07266353070735931\n",
      "Epoch 2540: train loss: 0.07206199318170547\n",
      "Epoch 2550: train loss: 0.07147769629955292\n",
      "Epoch 2560: train loss: 0.07091023027896881\n",
      "Epoch 2570: train loss: 0.07035913318395615\n",
      "Epoch 2580: train loss: 0.06982406228780746\n",
      "Epoch 2590: train loss: 0.06930461525917053\n",
      "Epoch 2600: train loss: 0.0688004121184349\n",
      "Epoch 2610: train loss: 0.06831114739179611\n",
      "Epoch 2620: train loss: 0.06783639639616013\n",
      "Epoch 2630: train loss: 0.06737580895423889\n",
      "Epoch 2640: train loss: 0.06692903488874435\n",
      "Epoch 2650: train loss: 0.06649572402238846\n",
      "Epoch 2660: train loss: 0.06607553362846375\n",
      "Epoch 2670: train loss: 0.06566818803548813\n",
      "Epoch 2680: train loss: 0.06527327746152878\n",
      "Epoch 2690: train loss: 0.06489051878452301\n",
      "Epoch 2700: train loss: 0.06451960653066635\n",
      "Epoch 2710: train loss: 0.06416019052267075\n",
      "Epoch 2720: train loss: 0.0638120099902153\n",
      "Epoch 2730: train loss: 0.06347475200891495\n",
      "Epoch 2740: train loss: 0.06314816325902939\n",
      "Epoch 2750: train loss: 0.06283184885978699\n",
      "Epoch 2760: train loss: 0.06252564489841461\n",
      "Epoch 2770: train loss: 0.062229275703430176\n",
      "Epoch 2780: train loss: 0.06194243207573891\n",
      "Epoch 2790: train loss: 0.061664845794439316\n",
      "Epoch 2800: train loss: 0.06139618158340454\n",
      "Epoch 2810: train loss: 0.06113627180457115\n",
      "Epoch 2820: train loss: 0.060884878039360046\n",
      "Epoch 2830: train loss: 0.060641754418611526\n",
      "Epoch 2840: train loss: 0.060406643897295\n",
      "Epoch 2850: train loss: 0.06017932668328285\n",
      "Epoch 2860: train loss: 0.05995955690741539\n",
      "Epoch 2870: train loss: 0.05974713712930679\n",
      "Epoch 2880: train loss: 0.05954185500741005\n",
      "Epoch 2890: train loss: 0.059343475848436356\n",
      "Epoch 2900: train loss: 0.05915180221199989\n",
      "Epoch 2910: train loss: 0.05896662920713425\n",
      "Epoch 2920: train loss: 0.05878777056932449\n",
      "Epoch 2930: train loss: 0.058615025132894516\n",
      "Epoch 2940: train loss: 0.05844820663332939\n",
      "Epoch 2950: train loss: 0.0582871288061142\n",
      "Epoch 2960: train loss: 0.058131616562604904\n",
      "Epoch 2970: train loss: 0.057981498539447784\n",
      "Epoch 2980: train loss: 0.05783659964799881\n",
      "Epoch 2990: train loss: 0.05769675597548485\n",
      "Epoch 3000: train loss: 0.05756179243326187\n",
      "Epoch 3010: train loss: 0.05743156373500824\n",
      "Epoch 3020: train loss: 0.05730590224266052\n",
      "Epoch 3030: train loss: 0.05718467757105827\n",
      "Epoch 3040: train loss: 0.057067736983299255\n",
      "Epoch 3050: train loss: 0.05695495009422302\n",
      "Epoch 3060: train loss: 0.05684616416692734\n",
      "Epoch 3070: train loss: 0.05674124136567116\n",
      "Epoch 3080: train loss: 0.05664006620645523\n",
      "Epoch 3090: train loss: 0.05654248967766762\n",
      "Epoch 3100: train loss: 0.05644841119647026\n",
      "Epoch 3110: train loss: 0.05635770037770271\n",
      "Epoch 3120: train loss: 0.056270234286785126\n",
      "Epoch 3130: train loss: 0.05618591234087944\n",
      "Epoch 3140: train loss: 0.05610461160540581\n",
      "Epoch 3150: train loss: 0.05602622777223587\n",
      "Epoch 3160: train loss: 0.05595067888498306\n",
      "Epoch 3170: train loss: 0.05587783828377724\n",
      "Epoch 3180: train loss: 0.05580759793519974\n",
      "Epoch 3190: train loss: 0.055739883333444595\n",
      "Epoch 3200: train loss: 0.05567460507154465\n",
      "Epoch 3210: train loss: 0.055611684918403625\n",
      "Epoch 3220: train loss: 0.055551011115312576\n",
      "Epoch 3230: train loss: 0.05549251288175583\n",
      "Epoch 3240: train loss: 0.05543610081076622\n",
      "Epoch 3250: train loss: 0.05538170039653778\n",
      "Epoch 3260: train loss: 0.055329255759716034\n",
      "Epoch 3270: train loss: 0.05527865141630173\n",
      "Epoch 3280: train loss: 0.055229853838682175\n",
      "Epoch 3290: train loss: 0.05518278107047081\n",
      "Epoch 3300: train loss: 0.05513736233115196\n",
      "Epoch 3310: train loss: 0.05509353056550026\n",
      "Epoch 3320: train loss: 0.055051229894161224\n",
      "Epoch 3330: train loss: 0.05501040443778038\n",
      "Epoch 3340: train loss: 0.054970987141132355\n",
      "Epoch 3350: train loss: 0.054932937026023865\n",
      "Epoch 3360: train loss: 0.054896168410778046\n",
      "Epoch 3370: train loss: 0.054860685020685196\n",
      "Epoch 3380: train loss: 0.05482637137174606\n",
      "Epoch 3390: train loss: 0.05479322001338005\n",
      "Epoch 3400: train loss: 0.05476117134094238\n",
      "Epoch 3410: train loss: 0.05473017692565918\n",
      "Epoch 3420: train loss: 0.05470022186636925\n",
      "Epoch 3430: train loss: 0.05467122048139572\n",
      "Epoch 3440: train loss: 0.0546431690454483\n",
      "Epoch 3450: train loss: 0.05461600795388222\n",
      "Epoch 3460: train loss: 0.054589707404375076\n",
      "Epoch 3470: train loss: 0.0545642264187336\n",
      "Epoch 3480: train loss: 0.05453953519463539\n",
      "Epoch 3490: train loss: 0.05451560020446777\n",
      "Epoch 3500: train loss: 0.05449238047003746\n",
      "Epoch 3510: train loss: 0.054469864815473557\n",
      "Epoch 3520: train loss: 0.05444801226258278\n",
      "Epoch 3530: train loss: 0.05442678928375244\n",
      "Epoch 3540: train loss: 0.05440618470311165\n",
      "Epoch 3550: train loss: 0.05438615381717682\n",
      "Epoch 3560: train loss: 0.05436667799949646\n",
      "Epoch 3570: train loss: 0.05434773862361908\n",
      "Epoch 3580: train loss: 0.05432930961251259\n",
      "Epoch 3590: train loss: 0.0543113648891449\n",
      "Epoch 3600: train loss: 0.05429389700293541\n",
      "Epoch 3610: train loss: 0.05427686497569084\n",
      "Epoch 3620: train loss: 0.054260268807411194\n",
      "Epoch 3630: train loss: 0.05424407497048378\n",
      "Epoch 3640: train loss: 0.0542282871901989\n",
      "Epoch 3650: train loss: 0.054212864488363266\n",
      "Epoch 3660: train loss: 0.05419779568910599\n",
      "Epoch 3670: train loss: 0.05418308079242706\n",
      "Epoch 3680: train loss: 0.05416868254542351\n",
      "Epoch 3690: train loss: 0.05415460467338562\n",
      "Epoch 3700: train loss: 0.054140832275152206\n",
      "Epoch 3710: train loss: 0.05412733927369118\n",
      "Epoch 3720: train loss: 0.05411412939429283\n",
      "Epoch 3730: train loss: 0.05410118028521538\n",
      "Epoch 3740: train loss: 0.05408848822116852\n",
      "Epoch 3750: train loss: 0.05407603085041046\n",
      "Epoch 3760: train loss: 0.054063815623521805\n",
      "Epoch 3770: train loss: 0.05405181646347046\n",
      "Epoch 3780: train loss: 0.054040029644966125\n",
      "Epoch 3790: train loss: 0.054028451442718506\n",
      "Epoch 3800: train loss: 0.054017066955566406\n",
      "Epoch 3810: train loss: 0.05400587618350983\n",
      "Epoch 3820: train loss: 0.053994860500097275\n",
      "Epoch 3830: train loss: 0.053984012454748154\n",
      "Epoch 3840: train loss: 0.05397333949804306\n",
      "Epoch 3850: train loss: 0.0539628267288208\n",
      "Epoch 3860: train loss: 0.05395247042179108\n",
      "Epoch 3870: train loss: 0.05394225940108299\n",
      "Epoch 3880: train loss: 0.05393218994140625\n",
      "Epoch 3890: train loss: 0.05392226204276085\n",
      "Epoch 3900: train loss: 0.05391246825456619\n",
      "Epoch 3910: train loss: 0.053902797400951385\n",
      "Epoch 3920: train loss: 0.05389324948191643\n",
      "Epoch 3930: train loss: 0.05388382449746132\n",
      "Epoch 3940: train loss: 0.053874511271715164\n",
      "Epoch 3950: train loss: 0.053865306079387665\n",
      "Epoch 3960: train loss: 0.053856220096349716\n",
      "Epoch 3970: train loss: 0.05384722724556923\n",
      "Epoch 03971: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 03978: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 3980: train loss: 0.05384349077939987\n",
      "Epoch 03984: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 03990: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 3990: train loss: 0.05384298041462898\n",
      "Epoch 03996: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 4000: train loss: 0.053842902183532715\n",
      "Epoch 04002: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 04008: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 4010: train loss: 0.05384289473295212\n",
      "Epoch 04014: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 4014, loss: 0.05384290590882301\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    # x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "one_model = get_regression_model(featured_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-bbbb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-bbbb.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "featured_x_test = nn_model.encode(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device)))\n",
    "featured_x_test = scaler.transform(featured_x_test.detach().cpu().numpy())\n",
    "featured_x_test = torch.tensor(featured_x_test, dtype=torch.float).to(device)\n",
    "y_pred = one_model(featured_x_test).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
