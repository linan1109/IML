{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8c4363de50425e925105d18db854e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.247252483338322, val loss: 1.3341416716575623\n",
      "Epoch 2: train loss: 1.236640982166765, val loss: 1.3240869641304016\n",
      "Epoch 3: train loss: 1.2265893205323666, val loss: 1.3144888579845428\n",
      "Epoch 4: train loss: 1.2169813255748108, val loss: 1.3054885566234589\n",
      "Epoch 5: train loss: 1.2077651739665707, val loss: 1.2964848279953003\n",
      "Epoch 6: train loss: 1.1988220776424265, val loss: 1.287909746170044\n",
      "Epoch 7: train loss: 1.1899575259474438, val loss: 1.2792080640792847\n",
      "Epoch 8: train loss: 1.1810770009476077, val loss: 1.270420104265213\n",
      "Epoch 9: train loss: 1.1719984040856946, val loss: 1.260985791683197\n",
      "Epoch 10: train loss: 1.1625497852108597, val loss: 1.251431554555893\n",
      "Epoch 11: train loss: 1.1525043814215774, val loss: 1.241013616323471\n",
      "Epoch 12: train loss: 1.1415919258251022, val loss: 1.2294908165931702\n",
      "Epoch 13: train loss: 1.1296438334308314, val loss: 1.2170547842979431\n",
      "Epoch 14: train loss: 1.1163753685472995, val loss: 1.2031743228435516\n",
      "Epoch 15: train loss: 1.1016485840422934, val loss: 1.1877355575561523\n",
      "Epoch 16: train loss: 1.085344968415366, val loss: 1.1707412898540497\n",
      "Epoch 17: train loss: 1.067567916681314, val loss: 1.1526424884796143\n",
      "Epoch 18: train loss: 1.0486208886657808, val loss: 1.133398860692978\n",
      "Epoch 19: train loss: 1.0290665788340514, val loss: 1.1139986217021942\n",
      "Epoch 20: train loss: 1.0096419097083489, val loss: 1.0952118337154388\n",
      "Epoch 21: train loss: 0.9911311010604044, val loss: 1.0776253640651703\n",
      "Epoch 22: train loss: 0.9741263123206632, val loss: 1.0618006438016891\n",
      "Epoch 23: train loss: 0.9590940430677627, val loss: 1.0480074137449265\n",
      "Epoch 24: train loss: 0.9461341231759566, val loss: 1.0361282676458359\n",
      "Epoch 25: train loss: 0.9351522795167168, val loss: 1.0262713879346848\n",
      "Epoch 26: train loss: 0.9259062247391582, val loss: 1.0179355442523956\n",
      "Epoch 27: train loss: 0.918117015590001, val loss: 1.0108792781829834\n",
      "Epoch 28: train loss: 0.9115268644730432, val loss: 1.0048320591449738\n",
      "Epoch 29: train loss: 0.9058679572237508, val loss: 0.9994711428880692\n",
      "Epoch 30: train loss: 0.9009586541296726, val loss: 0.9949701279401779\n",
      "Epoch 31: train loss: 0.8965931365358327, val loss: 0.9910386949777603\n",
      "Epoch 32: train loss: 0.8927026297757144, val loss: 0.9873243421316147\n",
      "Epoch 33: train loss: 0.8891649062008065, val loss: 0.9839272946119308\n",
      "Epoch 34: train loss: 0.8859307053859465, val loss: 0.9808230847120285\n",
      "Epoch 35: train loss: 0.8829348716070666, val loss: 0.9779138714075089\n",
      "Epoch 36: train loss: 0.8801189962067428, val loss: 0.9752781540155411\n",
      "Epoch 37: train loss: 0.8774685451151771, val loss: 0.9726253598928452\n",
      "Epoch 38: train loss: 0.8749728027569317, val loss: 0.9702495336532593\n",
      "Epoch 39: train loss: 0.8726231324754482, val loss: 0.9680394977331161\n",
      "Epoch 40: train loss: 0.870356846112747, val loss: 0.9657618552446365\n",
      "Epoch 41: train loss: 0.8682008939255145, val loss: 0.9635864943265915\n",
      "Epoch 42: train loss: 0.8661345719940636, val loss: 0.9615999609231949\n",
      "Epoch 43: train loss: 0.864149346319689, val loss: 0.9596802294254303\n",
      "Epoch 44: train loss: 0.862248981614317, val loss: 0.9577144533395767\n",
      "Epoch 45: train loss: 0.8604056330063654, val loss: 0.9559541940689087\n",
      "Epoch 46: train loss: 0.8586428933033013, val loss: 0.9542241096496582\n",
      "Epoch 47: train loss: 0.8569178810426512, val loss: 0.9525905400514603\n",
      "Epoch 48: train loss: 0.8552575927667079, val loss: 0.9507934302091599\n",
      "Epoch 49: train loss: 0.8536505278945943, val loss: 0.9492113143205643\n",
      "Epoch 50: train loss: 0.852100829348304, val loss: 0.9476341754198074\n",
      "Epoch 51: train loss: 0.8505863462231277, val loss: 0.9460989236831665\n",
      "Epoch 52: train loss: 0.8491148416297337, val loss: 0.9446505010128021\n",
      "Epoch 53: train loss: 0.8477067500693625, val loss: 0.9431534111499786\n",
      "Epoch 54: train loss: 0.8463197825236166, val loss: 0.9419611096382141\n",
      "Epoch 55: train loss: 0.8449694433659364, val loss: 0.9406779110431671\n",
      "Epoch 56: train loss: 0.8436578467794977, val loss: 0.9391476660966873\n",
      "Epoch 57: train loss: 0.8423968544975288, val loss: 0.937930166721344\n",
      "Epoch 58: train loss: 0.8411528361034487, val loss: 0.9366339296102524\n",
      "Epoch 59: train loss: 0.8399371427089238, val loss: 0.9354632347822189\n",
      "Epoch 60: train loss: 0.8387578219730298, val loss: 0.9342573136091232\n",
      "Epoch 61: train loss: 0.837610293567979, val loss: 0.9330492168664932\n",
      "Epoch 62: train loss: 0.8364697390621451, val loss: 0.931993156671524\n",
      "Epoch 63: train loss: 0.8353827351723103, val loss: 0.9308538734912872\n",
      "Epoch 64: train loss: 0.834301093013331, val loss: 0.9297623932361603\n",
      "Epoch 65: train loss: 0.8332535050461316, val loss: 0.9287214875221252\n",
      "Epoch 66: train loss: 0.83222403087401, val loss: 0.9277771860361099\n",
      "Epoch 67: train loss: 0.8312349207687129, val loss: 0.9265931397676468\n",
      "Epoch 68: train loss: 0.8302274872103146, val loss: 0.9257570505142212\n",
      "Epoch 69: train loss: 0.8292821570378976, val loss: 0.9246159642934799\n",
      "Epoch 70: train loss: 0.8283461228533923, val loss: 0.9237188547849655\n",
      "Epoch 71: train loss: 0.8274136204923613, val loss: 0.9227802008390427\n",
      "Epoch 72: train loss: 0.8265060109860054, val loss: 0.9219372570514679\n",
      "Epoch 73: train loss: 0.8256097278420499, val loss: 0.9209256768226624\n",
      "Epoch 74: train loss: 0.8247550849780582, val loss: 0.9200434982776642\n",
      "Epoch 75: train loss: 0.8238806872148211, val loss: 0.9191855639219284\n",
      "Epoch 76: train loss: 0.8230510853895445, val loss: 0.9183453470468521\n",
      "Epoch 77: train loss: 0.8222323490373062, val loss: 0.9174918830394745\n",
      "Epoch 78: train loss: 0.8214242725697891, val loss: 0.9166616797447205\n",
      "Epoch 79: train loss: 0.820634258899452, val loss: 0.915790930390358\n",
      "Epoch 80: train loss: 0.8198562771890024, val loss: 0.9150811433792114\n",
      "Epoch 81: train loss: 0.8190848526904801, val loss: 0.914401113986969\n",
      "Epoch 82: train loss: 0.818341484006734, val loss: 0.9135157316923141\n",
      "Epoch 83: train loss: 0.8175962317110627, val loss: 0.9127731174230576\n",
      "Epoch 84: train loss: 0.816847413846458, val loss: 0.9121825695037842\n",
      "Epoch 85: train loss: 0.8161493919909059, val loss: 0.9112837612628937\n",
      "Epoch 86: train loss: 0.8154541524708252, val loss: 0.9106091558933258\n",
      "Epoch 87: train loss: 0.8147658643594796, val loss: 0.9098496586084366\n",
      "Epoch 88: train loss: 0.8140804014810352, val loss: 0.9092623740434647\n",
      "Epoch 89: train loss: 0.8134173425495606, val loss: 0.9084679484367371\n",
      "Epoch 90: train loss: 0.8127748275924764, val loss: 0.9078960716724396\n",
      "Epoch 91: train loss: 0.8121232232873014, val loss: 0.9071547389030457\n",
      "Epoch 92: train loss: 0.8115018512134807, val loss: 0.9065661430358887\n",
      "Epoch 93: train loss: 0.8108558196212995, val loss: 0.9059370309114456\n",
      "Epoch 94: train loss: 0.8102473296281054, val loss: 0.9052445143461227\n",
      "Epoch 95: train loss: 0.8096580470710438, val loss: 0.9046423882246017\n",
      "Epoch 96: train loss: 0.8090765181027063, val loss: 0.9040236175060272\n",
      "Epoch 97: train loss: 0.8084753766553693, val loss: 0.9034344702959061\n",
      "Epoch 98: train loss: 0.8079205544818191, val loss: 0.9029552340507507\n",
      "Epoch 99: train loss: 0.8073609790082307, val loss: 0.9023363143205643\n",
      "Epoch 100: train loss: 0.8068003371937218, val loss: 0.9016532003879547\n",
      "Epoch 101: train loss: 0.8062589734802446, val loss: 0.9011797904968262\n",
      "Epoch 102: train loss: 0.8057420106932681, val loss: 0.9006689488887787\n",
      "Epoch 103: train loss: 0.8052057883818139, val loss: 0.9000995308160782\n",
      "Epoch 104: train loss: 0.8047077410636249, val loss: 0.8996042758226395\n",
      "Epoch 105: train loss: 0.8042166103766815, val loss: 0.8991076201200485\n",
      "Epoch 106: train loss: 0.8037199945435809, val loss: 0.8985864222049713\n",
      "Epoch 107: train loss: 0.8032437670624205, val loss: 0.898164227604866\n",
      "Epoch 108: train loss: 0.8027678907832769, val loss: 0.8975473940372467\n",
      "Epoch 109: train loss: 0.8023097017784205, val loss: 0.897179126739502\n",
      "Epoch 110: train loss: 0.8018571618763071, val loss: 0.8966961950063705\n",
      "Epoch 111: train loss: 0.8014037469094659, val loss: 0.8962786644697189\n",
      "Epoch 112: train loss: 0.8009551517599667, val loss: 0.8957870900630951\n",
      "Epoch 113: train loss: 0.800526371348025, val loss: 0.8953403681516647\n",
      "Epoch 114: train loss: 0.8001123644954353, val loss: 0.8948466628789902\n",
      "Epoch 115: train loss: 0.7997020075851622, val loss: 0.8945754319429398\n",
      "Epoch 116: train loss: 0.7992967052305496, val loss: 0.8941152691841125\n",
      "Epoch 117: train loss: 0.7989042035819113, val loss: 0.8937117904424667\n",
      "Epoch 118: train loss: 0.7985036509249656, val loss: 0.8934082537889481\n",
      "Epoch 119: train loss: 0.7981228322929823, val loss: 0.8929778039455414\n",
      "Epoch 120: train loss: 0.7977522611423319, val loss: 0.8925600945949554\n",
      "Epoch 121: train loss: 0.7973890568220082, val loss: 0.892243281006813\n",
      "Epoch 122: train loss: 0.7970092639507168, val loss: 0.8917564153671265\n",
      "Epoch 123: train loss: 0.7966715955960131, val loss: 0.8914568722248077\n",
      "Epoch 124: train loss: 0.7963177954515808, val loss: 0.8910492360591888\n",
      "Epoch 125: train loss: 0.7959578253598978, val loss: 0.8907162547111511\n",
      "Epoch 126: train loss: 0.7956271128146643, val loss: 0.890433594584465\n",
      "Epoch 127: train loss: 0.7952936132800847, val loss: 0.8900770097970963\n",
      "Epoch 128: train loss: 0.7949639669199934, val loss: 0.8897333592176437\n",
      "Epoch 129: train loss: 0.7946530845927787, val loss: 0.8893873691558838\n",
      "Epoch 130: train loss: 0.7943250317855506, val loss: 0.8890540152788162\n",
      "Epoch 131: train loss: 0.7940073684515108, val loss: 0.8887939155101776\n",
      "Epoch 132: train loss: 0.7937306298496131, val loss: 0.8883972018957138\n",
      "Epoch 133: train loss: 0.79340421154545, val loss: 0.8881013840436935\n",
      "Epoch 134: train loss: 0.7931123141446405, val loss: 0.8878528475761414\n",
      "Epoch 135: train loss: 0.7928308210159508, val loss: 0.8875441700220108\n",
      "Epoch 136: train loss: 0.7925295798033135, val loss: 0.8872124403715134\n",
      "Epoch 137: train loss: 0.7922637366812356, val loss: 0.8869511634111404\n",
      "Epoch 138: train loss: 0.7919677221981462, val loss: 0.8867113590240479\n",
      "Epoch 139: train loss: 0.7917090258501746, val loss: 0.8864264041185379\n",
      "Epoch 140: train loss: 0.7914401513227097, val loss: 0.8861460238695145\n",
      "Epoch 141: train loss: 0.7911703117685901, val loss: 0.8859190195798874\n",
      "Epoch 142: train loss: 0.7909234670866786, val loss: 0.8856546580791473\n",
      "Epoch 143: train loss: 0.7906368013224311, val loss: 0.8853486329317093\n",
      "Epoch 144: train loss: 0.7903920082397922, val loss: 0.8851223140954971\n",
      "Epoch 145: train loss: 0.7901480684713459, val loss: 0.8848733752965927\n",
      "Epoch 146: train loss: 0.7898982793861883, val loss: 0.8846365064382553\n",
      "Epoch 147: train loss: 0.7896629540174859, val loss: 0.8844106048345566\n",
      "Epoch 148: train loss: 0.7894234234544429, val loss: 0.8840988278388977\n",
      "Epoch 149: train loss: 0.7891871547706764, val loss: 0.8839318305253983\n",
      "Epoch 150: train loss: 0.7889688124223614, val loss: 0.8836451917886734\n",
      "Epoch 151: train loss: 0.7887361142340146, val loss: 0.8834726214408875\n",
      "Epoch 152: train loss: 0.7885018468136804, val loss: 0.8831741213798523\n",
      "Epoch 153: train loss: 0.7882853746998353, val loss: 0.8829462081193924\n",
      "Epoch 154: train loss: 0.7880572764868052, val loss: 0.8828382641077042\n",
      "Epoch 155: train loss: 0.787851859505992, val loss: 0.8825149685144424\n",
      "Epoch 156: train loss: 0.7876372967074798, val loss: 0.8823748081922531\n",
      "Epoch 157: train loss: 0.7874314872628916, val loss: 0.8820580691099167\n",
      "Epoch 158: train loss: 0.7872240515130674, val loss: 0.8818825632333755\n",
      "Epoch 159: train loss: 0.7870208304600558, val loss: 0.881649985909462\n",
      "Epoch 160: train loss: 0.7868110668382704, val loss: 0.8815229684114456\n",
      "Epoch 161: train loss: 0.7866143140478175, val loss: 0.8812658786773682\n",
      "Epoch 162: train loss: 0.7864247417769297, val loss: 0.8811066597700119\n",
      "Epoch 163: train loss: 0.7862223416167667, val loss: 0.8808914721012115\n",
      "Epoch 164: train loss: 0.7860166478569856, val loss: 0.8807437866926193\n",
      "Epoch 165: train loss: 0.785841193046495, val loss: 0.8806238174438477\n",
      "Epoch 166: train loss: 0.7856625620542262, val loss: 0.8803748935461044\n",
      "Epoch 167: train loss: 0.7854726614028322, val loss: 0.8800871521234512\n",
      "Epoch 168: train loss: 0.785286989721431, val loss: 0.8800409734249115\n",
      "Epoch 169: train loss: 0.78511421340075, val loss: 0.8798031210899353\n",
      "Epoch 170: train loss: 0.7849259052896609, val loss: 0.8796515166759491\n",
      "Epoch 171: train loss: 0.7847615186391567, val loss: 0.8793856203556061\n",
      "Epoch 172: train loss: 0.7845967815071101, val loss: 0.879316970705986\n",
      "Epoch 173: train loss: 0.7844102869856322, val loss: 0.8791158646345139\n",
      "Epoch 174: train loss: 0.7842498330149297, val loss: 0.8789647221565247\n",
      "Epoch 175: train loss: 0.7840842136328846, val loss: 0.8788231015205383\n",
      "Epoch 176: train loss: 0.7839200903060038, val loss: 0.8785694241523743\n",
      "Epoch 177: train loss: 0.7837647888754676, val loss: 0.878509059548378\n",
      "Epoch 178: train loss: 0.783592125026668, val loss: 0.8782696574926376\n",
      "Epoch 179: train loss: 0.7834432406076532, val loss: 0.8781717419624329\n",
      "Epoch 180: train loss: 0.7832818667275654, val loss: 0.8779781758785248\n",
      "Epoch 181: train loss: 0.7831179198779455, val loss: 0.8777749389410019\n",
      "Epoch 182: train loss: 0.7829769238542397, val loss: 0.8776204884052277\n",
      "Epoch 183: train loss: 0.7828290406608457, val loss: 0.8774617165327072\n",
      "Epoch 184: train loss: 0.7826685253524656, val loss: 0.8772783279418945\n",
      "Epoch 185: train loss: 0.7825218023953193, val loss: 0.8771596252918243\n",
      "Epoch 186: train loss: 0.7823888813892403, val loss: 0.877037450671196\n",
      "Epoch 187: train loss: 0.7822414871642341, val loss: 0.8768484890460968\n",
      "Epoch 188: train loss: 0.782102539955265, val loss: 0.8767309486865997\n",
      "Epoch 189: train loss: 0.7819699762849704, val loss: 0.8766369223594666\n",
      "Epoch 190: train loss: 0.7818158098156645, val loss: 0.8765004277229309\n",
      "Epoch 191: train loss: 0.7816787312054938, val loss: 0.8763923645019531\n",
      "Epoch 192: train loss: 0.7815393255022529, val loss: 0.8761662095785141\n",
      "Epoch 193: train loss: 0.7814098529588238, val loss: 0.876119539141655\n",
      "Epoch 194: train loss: 0.7812800385600716, val loss: 0.8759363293647766\n",
      "Epoch 195: train loss: 0.7811408448320555, val loss: 0.8758421242237091\n",
      "Epoch 196: train loss: 0.7810189403649523, val loss: 0.8757410049438477\n",
      "Epoch 197: train loss: 0.7808930866860829, val loss: 0.8755815178155899\n",
      "Epoch 198: train loss: 0.7807468043476568, val loss: 0.8754445016384125\n",
      "Epoch 199: train loss: 0.7806325322153989, val loss: 0.8753271996974945\n",
      "Epoch 200: train loss: 0.7805105547856677, val loss: 0.8751647472381592\n",
      "Epoch 201: train loss: 0.7803768181053107, val loss: 0.8750279396772385\n",
      "Epoch 202: train loss: 0.7802586771428527, val loss: 0.8748856335878372\n",
      "Epoch 203: train loss: 0.7801359069187709, val loss: 0.8748816698789597\n",
      "Epoch 204: train loss: 0.7800272243625002, val loss: 0.874719500541687\n",
      "Epoch 205: train loss: 0.7799156371653762, val loss: 0.8745451718568802\n",
      "Epoch 206: train loss: 0.779792304406638, val loss: 0.8744787126779556\n",
      "Epoch 207: train loss: 0.7796691461584607, val loss: 0.8743633180856705\n",
      "Epoch 208: train loss: 0.7795649733429678, val loss: 0.8742188066244125\n",
      "Epoch 209: train loss: 0.7794599221524597, val loss: 0.8741785436868668\n",
      "Epoch 210: train loss: 0.7793391146561711, val loss: 0.8739821016788483\n",
      "Epoch 211: train loss: 0.779237013866371, val loss: 0.8739112913608551\n",
      "Epoch 212: train loss: 0.7791231694037678, val loss: 0.8738195598125458\n",
      "Epoch 213: train loss: 0.7790172599920531, val loss: 0.8736827522516251\n",
      "Epoch 214: train loss: 0.7789047442009074, val loss: 0.8735958784818649\n",
      "Epoch 215: train loss: 0.778797908356594, val loss: 0.8734042048454285\n",
      "Epoch 216: train loss: 0.7787071587368773, val loss: 0.8732795715332031\n",
      "Epoch 217: train loss: 0.7786028790769917, val loss: 0.8732516169548035\n",
      "Epoch 218: train loss: 0.7784913181869161, val loss: 0.8731502294540405\n",
      "Epoch 219: train loss: 0.7783995567097145, val loss: 0.8730452060699463\n",
      "Epoch 220: train loss: 0.7782942938360659, val loss: 0.8729095011949539\n",
      "Epoch 221: train loss: 0.7781927942918119, val loss: 0.8728204965591431\n",
      "Epoch 222: train loss: 0.778100210480813, val loss: 0.8727722316980362\n",
      "Epoch 223: train loss: 0.7780020558651036, val loss: 0.8726339340209961\n",
      "Epoch 224: train loss: 0.7779026800649145, val loss: 0.8725961744785309\n",
      "Epoch 225: train loss: 0.7778017437368466, val loss: 0.8725097477436066\n",
      "Epoch 226: train loss: 0.777710880827413, val loss: 0.8723905384540558\n",
      "Epoch 227: train loss: 0.7776135053559871, val loss: 0.8722841888666153\n",
      "Epoch 228: train loss: 0.7775392224881353, val loss: 0.8720794916152954\n",
      "Epoch 229: train loss: 0.7774419399968396, val loss: 0.872093066573143\n",
      "Epoch 230: train loss: 0.7773313964749037, val loss: 0.8719575107097626\n",
      "Epoch 231: train loss: 0.7772609036913576, val loss: 0.8718583434820175\n",
      "Epoch 232: train loss: 0.7771618598545739, val loss: 0.8717422038316727\n",
      "Epoch 233: train loss: 0.7770834666418507, val loss: 0.8718090653419495\n",
      "Epoch 234: train loss: 0.7769898755882971, val loss: 0.8716746121644974\n",
      "Epoch 235: train loss: 0.7769090801779253, val loss: 0.8714965432882309\n",
      "Epoch 236: train loss: 0.7768149291990006, val loss: 0.8714412301778793\n",
      "Epoch 237: train loss: 0.7767346752542796, val loss: 0.87130306661129\n",
      "Epoch 238: train loss: 0.776647422622131, val loss: 0.8712560087442398\n",
      "Epoch 239: train loss: 0.776574451854828, val loss: 0.8711475878953934\n",
      "Epoch 240: train loss: 0.7764878838679947, val loss: 0.8711106777191162\n",
      "Epoch 241: train loss: 0.7764082557428355, val loss: 0.8710476905107498\n",
      "Epoch 242: train loss: 0.7763255534706377, val loss: 0.8709346950054169\n",
      "Epoch 243: train loss: 0.776243800160465, val loss: 0.8709027320146561\n",
      "Epoch 244: train loss: 0.7761667974047395, val loss: 0.8707297891378403\n",
      "Epoch 245: train loss: 0.7760924323443998, val loss: 0.8707183003425598\n",
      "Epoch 246: train loss: 0.7760100771850404, val loss: 0.8706542700529099\n",
      "Epoch 247: train loss: 0.7759345713802039, val loss: 0.8705893754959106\n",
      "Epoch 248: train loss: 0.7758476167323347, val loss: 0.8704376071691513\n",
      "Epoch 249: train loss: 0.7757730417024151, val loss: 0.8702945560216904\n",
      "Epoch 250: train loss: 0.7757145469969918, val loss: 0.8702715486288071\n",
      "Epoch 251: train loss: 0.7756332208501946, val loss: 0.8702148944139481\n",
      "Epoch 252: train loss: 0.775558909747539, val loss: 0.870149165391922\n",
      "Epoch 253: train loss: 0.7754808126497564, val loss: 0.8700308799743652\n",
      "Epoch 254: train loss: 0.7754150835255009, val loss: 0.8700419068336487\n",
      "Epoch 255: train loss: 0.775340943564702, val loss: 0.8699958026409149\n",
      "Epoch 256: train loss: 0.7752627541293743, val loss: 0.8698853403329849\n",
      "Epoch 257: train loss: 0.7751918672074372, val loss: 0.8697988092899323\n",
      "Epoch 258: train loss: 0.7751405856181733, val loss: 0.8697468936443329\n",
      "Epoch 259: train loss: 0.7750531108462705, val loss: 0.8696168810129166\n",
      "Epoch 260: train loss: 0.7750054924599449, val loss: 0.8696321398019791\n",
      "Epoch 261: train loss: 0.7749363810203699, val loss: 0.8695048242807388\n",
      "Epoch 262: train loss: 0.7748635238348678, val loss: 0.8695135712623596\n",
      "Epoch 263: train loss: 0.7747944270384464, val loss: 0.8694514632225037\n",
      "Epoch 264: train loss: 0.7747371079718431, val loss: 0.8693079948425293\n",
      "Epoch 265: train loss: 0.7746596472475974, val loss: 0.8692616820335388\n",
      "Epoch 266: train loss: 0.7746032060886746, val loss: 0.8692699521780014\n",
      "Epoch 267: train loss: 0.774537082125026, val loss: 0.8691729307174683\n",
      "Epoch 268: train loss: 0.7744652027594657, val loss: 0.8691083490848541\n",
      "Epoch 269: train loss: 0.7744071001088997, val loss: 0.8690443485975266\n",
      "Epoch 270: train loss: 0.7743584779850294, val loss: 0.8689346462488174\n",
      "Epoch 271: train loss: 0.7742792900112555, val loss: 0.8688436299562454\n",
      "Epoch 272: train loss: 0.7742149297297682, val loss: 0.8688176423311234\n",
      "Epoch 273: train loss: 0.7741564502516668, val loss: 0.868754118680954\n",
      "Epoch 274: train loss: 0.7740874811544484, val loss: 0.8686638921499252\n",
      "Epoch 275: train loss: 0.7740502578882718, val loss: 0.868688628077507\n",
      "Epoch 276: train loss: 0.7739834899763701, val loss: 0.868553414940834\n",
      "Epoch 277: train loss: 0.7739155954957904, val loss: 0.8685512095689774\n",
      "Epoch 278: train loss: 0.7738566030983985, val loss: 0.8684442937374115\n",
      "Epoch 279: train loss: 0.773793820811424, val loss: 0.8683431446552277\n",
      "Epoch 280: train loss: 0.773747770625055, val loss: 0.868397444486618\n",
      "Epoch 281: train loss: 0.7736958795729434, val loss: 0.8682660907506943\n",
      "Epoch 282: train loss: 0.7736311178314884, val loss: 0.8682315349578857\n",
      "Epoch 283: train loss: 0.7735724904030142, val loss: 0.8681775778532028\n",
      "Epoch 284: train loss: 0.773522060589633, val loss: 0.8681463897228241\n",
      "Epoch 285: train loss: 0.7734509383062645, val loss: 0.868108719587326\n",
      "Epoch 286: train loss: 0.7734141469157548, val loss: 0.8680067658424377\n",
      "Epoch 287: train loss: 0.7733619385433914, val loss: 0.8679973185062408\n",
      "Epoch 288: train loss: 0.7733077149654439, val loss: 0.8678362369537354\n",
      "Epoch 289: train loss: 0.7732491360229435, val loss: 0.8677856177091599\n",
      "Epoch 290: train loss: 0.7731997054879551, val loss: 0.8677988201379776\n",
      "Epoch 291: train loss: 0.7731387967262654, val loss: 0.8677137494087219\n",
      "Epoch 292: train loss: 0.7730866795620549, val loss: 0.8677107989788055\n",
      "Epoch 293: train loss: 0.7730383602126445, val loss: 0.8676233738660812\n",
      "Epoch 294: train loss: 0.7729893900607389, val loss: 0.867583230137825\n",
      "Epoch 295: train loss: 0.7729378695271444, val loss: 0.8674975484609604\n",
      "Epoch 296: train loss: 0.7728840120085623, val loss: 0.8674852550029755\n",
      "Epoch 297: train loss: 0.7728418993311199, val loss: 0.8674742430448532\n",
      "Epoch 298: train loss: 0.7727801634259802, val loss: 0.867376908659935\n",
      "Epoch 299: train loss: 0.7727407358394499, val loss: 0.8673707544803619\n",
      "Epoch 300: train loss: 0.7726923019346246, val loss: 0.8674132078886032\n",
      "Epoch 301: train loss: 0.7726464911404485, val loss: 0.8671977072954178\n",
      "Epoch 302: train loss: 0.7725982357380009, val loss: 0.8671416789293289\n",
      "Epoch 303: train loss: 0.7725503344194673, val loss: 0.8670557290315628\n",
      "Epoch 304: train loss: 0.7724989149275265, val loss: 0.8672104179859161\n",
      "Epoch 305: train loss: 0.7724476786307828, val loss: 0.8670833855867386\n",
      "Epoch 306: train loss: 0.7724021942504128, val loss: 0.8670404702425003\n",
      "Epoch 307: train loss: 0.7723510505248814, val loss: 0.8669501096010208\n",
      "Epoch 308: train loss: 0.7723167527348962, val loss: 0.8669540733098984\n",
      "Epoch 309: train loss: 0.7722626656381494, val loss: 0.8668535202741623\n",
      "Epoch 310: train loss: 0.772215812337122, val loss: 0.8668573647737503\n",
      "Epoch 311: train loss: 0.7721717748054684, val loss: 0.8667947202920914\n",
      "Epoch 312: train loss: 0.7721317075935309, val loss: 0.8667151927947998\n",
      "Epoch 313: train loss: 0.7720911536330454, val loss: 0.8666803538799286\n",
      "Epoch 314: train loss: 0.7720512594517754, val loss: 0.8666431605815887\n",
      "Epoch 315: train loss: 0.7719962769807145, val loss: 0.8665715157985687\n",
      "Epoch 316: train loss: 0.7719512440845959, val loss: 0.8665467351675034\n",
      "Epoch 317: train loss: 0.7719080317686369, val loss: 0.8665158897638321\n",
      "Epoch 318: train loss: 0.7718686041626343, val loss: 0.8664933294057846\n",
      "Epoch 319: train loss: 0.771824205439685, val loss: 0.8664029538631439\n",
      "Epoch 320: train loss: 0.7717754511185153, val loss: 0.8663126677274704\n",
      "Epoch 321: train loss: 0.7717400810485648, val loss: 0.8663650453090668\n",
      "Epoch 322: train loss: 0.7717012375525032, val loss: 0.8662640154361725\n",
      "Epoch 323: train loss: 0.7716590176606949, val loss: 0.8662378042936325\n",
      "Epoch 324: train loss: 0.771625125770357, val loss: 0.8661824762821198\n",
      "Epoch 325: train loss: 0.7715782721188286, val loss: 0.8661525100469589\n",
      "Epoch 326: train loss: 0.7715280858841802, val loss: 0.8662292510271072\n",
      "Epoch 327: train loss: 0.7714922943562318, val loss: 0.8659936487674713\n",
      "Epoch 328: train loss: 0.7714678328904168, val loss: 0.8660595417022705\n",
      "Epoch 329: train loss: 0.7714259890861972, val loss: 0.8660389631986618\n",
      "Epoch 330: train loss: 0.7713846053535041, val loss: 0.8659479767084122\n",
      "Epoch 331: train loss: 0.7713432644598237, val loss: 0.86587955057621\n",
      "Epoch 332: train loss: 0.771292656806122, val loss: 0.8659133464097977\n",
      "Epoch 333: train loss: 0.7712617255394383, val loss: 0.8658650368452072\n",
      "Epoch 334: train loss: 0.7712268108676672, val loss: 0.8657621592283249\n",
      "Epoch 335: train loss: 0.7711901497856459, val loss: 0.8657690286636353\n",
      "Epoch 336: train loss: 0.7711566512937524, val loss: 0.8658254146575928\n",
      "Epoch 337: train loss: 0.7711075321594433, val loss: 0.8656096756458282\n",
      "Epoch 338: train loss: 0.7710718833243524, val loss: 0.8656835556030273\n",
      "Epoch 339: train loss: 0.7710399763407328, val loss: 0.8656172305345535\n",
      "Epoch 340: train loss: 0.771002226680449, val loss: 0.865583524107933\n",
      "Epoch 341: train loss: 0.770967195778647, val loss: 0.8655385226011276\n",
      "Epoch 342: train loss: 0.7709263100300066, val loss: 0.8655320107936859\n",
      "Epoch 343: train loss: 0.7708906082692625, val loss: 0.8654590398073196\n",
      "Epoch 344: train loss: 0.7708535006294761, val loss: 0.8654637187719345\n",
      "Epoch 345: train loss: 0.7708258444832182, val loss: 0.8654187172651291\n",
      "Epoch 346: train loss: 0.7707980616308435, val loss: 0.8653710633516312\n",
      "Epoch 347: train loss: 0.7707558281863293, val loss: 0.8652963489294052\n",
      "Epoch 348: train loss: 0.7707304989773632, val loss: 0.865288183093071\n",
      "Epoch 349: train loss: 0.7707015568039379, val loss: 0.8652662187814713\n",
      "Epoch 350: train loss: 0.7706491074231823, val loss: 0.8651753962039948\n",
      "Epoch 351: train loss: 0.7706173011891322, val loss: 0.8651680052280426\n",
      "Epoch 352: train loss: 0.7705878084954465, val loss: 0.865153044462204\n",
      "Epoch 353: train loss: 0.7705595785011383, val loss: 0.8651551157236099\n",
      "Epoch 354: train loss: 0.7705188094498635, val loss: 0.8651155531406403\n",
      "Epoch 355: train loss: 0.7704779132056649, val loss: 0.8651042729616165\n",
      "Epoch 356: train loss: 0.770455018790942, val loss: 0.8650089502334595\n",
      "Epoch 357: train loss: 0.7704276586817356, val loss: 0.8649552911520004\n",
      "Epoch 358: train loss: 0.7703752845613366, val loss: 0.8649156093597412\n",
      "Epoch 359: train loss: 0.7703530465960697, val loss: 0.8649689704179764\n",
      "Epoch 360: train loss: 0.7703273468491144, val loss: 0.8649456053972244\n",
      "Epoch 361: train loss: 0.7702879744080772, val loss: 0.8648556917905807\n",
      "Epoch 362: train loss: 0.7702596056543117, val loss: 0.8648608773946762\n",
      "Epoch 363: train loss: 0.7702394350608631, val loss: 0.8648080825805664\n",
      "Epoch 364: train loss: 0.7701959117920871, val loss: 0.8647327870130539\n",
      "Epoch 365: train loss: 0.7701690035371055, val loss: 0.8647723942995071\n",
      "Epoch 366: train loss: 0.7701422930425852, val loss: 0.8646828085184097\n",
      "Epoch 367: train loss: 0.7701128815677734, val loss: 0.8646918386220932\n",
      "Epoch 368: train loss: 0.770076061747737, val loss: 0.8646261692047119\n",
      "Epoch 369: train loss: 0.7700425537338992, val loss: 0.8645793348550797\n",
      "Epoch 370: train loss: 0.7700229539897698, val loss: 0.8646079152822495\n",
      "Epoch 371: train loss: 0.769987834898018, val loss: 0.8645592331886292\n",
      "Epoch 372: train loss: 0.7699579719778994, val loss: 0.8645177334547043\n",
      "Epoch 373: train loss: 0.769928328062498, val loss: 0.8644754588603973\n",
      "Epoch 374: train loss: 0.7699072310169784, val loss: 0.8645067662000656\n",
      "Epoch 375: train loss: 0.7698679765390049, val loss: 0.8643933981657028\n",
      "Epoch 376: train loss: 0.7698463589215894, val loss: 0.8643447309732437\n",
      "Epoch 377: train loss: 0.7698132366146461, val loss: 0.864435002207756\n",
      "Epoch 378: train loss: 0.7697860648413181, val loss: 0.864311084151268\n",
      "Epoch 379: train loss: 0.7697579927477172, val loss: 0.8642866462469101\n",
      "Epoch 380: train loss: 0.7697308149574551, val loss: 0.8643224090337753\n",
      "Epoch 381: train loss: 0.7697008257206652, val loss: 0.864258885383606\n",
      "Epoch 382: train loss: 0.7696663775073278, val loss: 0.8642164021730423\n",
      "Epoch 383: train loss: 0.7696441897026506, val loss: 0.8642094284296036\n",
      "Epoch 384: train loss: 0.7696268448289101, val loss: 0.864122673869133\n",
      "Epoch 385: train loss: 0.7695874243692338, val loss: 0.8641061186790466\n",
      "Epoch 386: train loss: 0.7695662284629557, val loss: 0.8641633838415146\n",
      "Epoch 387: train loss: 0.769538123324898, val loss: 0.8640990555286407\n",
      "Epoch 388: train loss: 0.7695138751000059, val loss: 0.8641363680362701\n",
      "Epoch 389: train loss: 0.7694903133702956, val loss: 0.8640766590833664\n",
      "Epoch 390: train loss: 0.7694596605377234, val loss: 0.863899901509285\n",
      "Epoch 391: train loss: 0.7694370839495318, val loss: 0.8640621602535248\n",
      "Epoch 392: train loss: 0.769402692809443, val loss: 0.8639589250087738\n",
      "Epoch 393: train loss: 0.7693749129947437, val loss: 0.8639457374811172\n",
      "Epoch 394: train loss: 0.7693414816043221, val loss: 0.8639346808195114\n",
      "Epoch 395: train loss: 0.7693291292280903, val loss: 0.8638576716184616\n",
      "Epoch 00396: reducing learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 396: train loss: 0.7693013161936838, val loss: 0.8638557642698288\n",
      "Epoch 397: train loss: 0.769272825446093, val loss: 0.8638724088668823\n",
      "Epoch 398: train loss: 0.7692720025865442, val loss: 0.8638636022806168\n",
      "Epoch 399: train loss: 0.7692646053015738, val loss: 0.8637634813785553\n",
      "Epoch 400: train loss: 0.7692567853902688, val loss: 0.8638808280229568\n",
      "Epoch 401: train loss: 0.7692405704344382, val loss: 0.8638381212949753\n",
      "Epoch 402: train loss: 0.7692349356827647, val loss: 0.8638694286346436\n",
      "Epoch 403: train loss: 0.7692278770986082, val loss: 0.8637923300266266\n",
      "Epoch 404: train loss: 0.7692192148165312, val loss: 0.8637702018022537\n",
      "Epoch 00405: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 405: train loss: 0.7692108776046731, val loss: 0.8637587130069733\n",
      "Epoch 406: train loss: 0.7692052906379712, val loss: 0.8637792021036148\n",
      "Epoch 407: train loss: 0.7691992150144381, val loss: 0.8637598901987076\n",
      "Epoch 408: train loss: 0.7692054567949245, val loss: 0.8637656420469284\n",
      "Epoch 409: train loss: 0.7692077145472106, val loss: 0.8637442737817764\n",
      "Epoch 410: train loss: 0.7692036297499686, val loss: 0.8637674301862717\n",
      "Epoch 00411: reducing learning rate of group 0 to 2.7000e-03.\n",
      "Epoch 411: train loss: 0.7691950096643193, val loss: 0.8638138025999069\n",
      "Epoch 412: train loss: 0.769192619656317, val loss: 0.8637151271104813\n",
      "Epoch 413: train loss: 0.7691951669808581, val loss: 0.8637778908014297\n",
      "Epoch 414: train loss: 0.7691973509916251, val loss: 0.8637767434120178\n",
      "Epoch 415: train loss: 0.7691947467301414, val loss: 0.8637289851903915\n",
      "Epoch 416: train loss: 0.7691861508680068, val loss: 0.8638595342636108\n",
      "Epoch 00417: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 417: train loss: 0.7691781619178513, val loss: 0.8637753576040268\n",
      "Epoch 418: train loss: 0.7691951243170957, val loss: 0.8637775331735611\n",
      "Epoch 419: train loss: 0.7691857723463832, val loss: 0.863781213760376\n",
      "Epoch 420: train loss: 0.7691972211504715, val loss: 0.8637686371803284\n",
      "Epoch 421: train loss: 0.7691873021470064, val loss: 0.8637595921754837\n",
      "Epoch 422: train loss: 0.7691909608323758, val loss: 0.863795131444931\n",
      "Epoch 00423: reducing learning rate of group 0 to 2.4300e-04.\n",
      "Epoch 423: train loss: 0.7691929502956075, val loss: 0.8638031333684921\n",
      "Epoch 424: train loss: 0.769190384491874, val loss: 0.863812580704689\n",
      "Epoch 425: train loss: 0.7691887708436193, val loss: 0.8636999875307083\n",
      "Epoch 426: train loss: 0.7691800609323488, val loss: 0.8637931495904922\n",
      "Epoch 427: train loss: 0.7691886270018973, val loss: 0.8637942522764206\n",
      "Epoch 428: train loss: 0.7691814570947241, val loss: 0.8638354241847992\n",
      "Epoch 00429: reducing learning rate of group 0 to 7.2900e-05.\n",
      "Epoch 429: train loss: 0.7691903704718335, val loss: 0.8637101054191589\n",
      "Epoch 430: train loss: 0.769183964286899, val loss: 0.8637833148241043\n",
      "Epoch 431: train loss: 0.7691941082341264, val loss: 0.8637824207544327\n",
      "Epoch 432: train loss: 0.7692007956571807, val loss: 0.8637770414352417\n",
      "Epoch 433: train loss: 0.7691889343328704, val loss: 0.8638138324022293\n",
      "Epoch 434: train loss: 0.7691888103139279, val loss: 0.8637203276157379\n",
      "Epoch 00435: reducing learning rate of group 0 to 2.1870e-05.\n",
      "Epoch 435: train loss: 0.7691833224221797, val loss: 0.8637658059597015\n",
      "Epoch 436: train loss: 0.7691775893744438, val loss: 0.8637464046478271\n",
      "Epoch 437: train loss: 0.7691860608866073, val loss: 0.8638148009777069\n",
      "Epoch 438: train loss: 0.7691896186861015, val loss: 0.8637206554412842\n",
      "Epoch 439: train loss: 0.7691913518746746, val loss: 0.8637547492980957\n",
      "Epoch 440: train loss: 0.7691952130717415, val loss: 0.8637060821056366\n",
      "Epoch 00441: reducing learning rate of group 0 to 6.5610e-06.\n",
      "Epoch 441: train loss: 0.7691884240033929, val loss: 0.8637866973876953\n",
      "Epoch 442: train loss: 0.7691862893743244, val loss: 0.8637365102767944\n",
      "Epoch 443: train loss: 0.7691804144515667, val loss: 0.8637355715036392\n",
      "Epoch 444: train loss: 0.769189980636816, val loss: 0.8637721389532089\n",
      "Epoch 445: train loss: 0.7692000386139335, val loss: 0.8638014495372772\n",
      "Epoch 446: train loss: 0.769180449560085, val loss: 0.8638269454240799\n",
      "Epoch 00447: reducing learning rate of group 0 to 1.9683e-06.\n",
      "Epoch 447: train loss: 0.7691806013270244, val loss: 0.8637653589248657\n",
      "Epoch 448: train loss: 0.7691868150284851, val loss: 0.8636813908815384\n",
      "Epoch 449: train loss: 0.7691926904380497, val loss: 0.8637171983718872\n",
      "Epoch 450: train loss: 0.7691832796221113, val loss: 0.8638018220663071\n",
      "Epoch 451: train loss: 0.7691904107794502, val loss: 0.8637440651655197\n",
      "Epoch 452: train loss: 0.7691868340723735, val loss: 0.863740935921669\n",
      "Epoch 00453: reducing learning rate of group 0 to 5.9049e-07.\n",
      "Epoch 453: train loss: 0.7691946452600977, val loss: 0.8637379705905914\n",
      "Early stop at epoch 453\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_x = ae_model.encoder(torch.tensor(x_pretrain, dtype=torch.float).to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2165fe6a93248ea80364f0cf713cd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.4721688601324971, val loss: 0.8142157047986984\n",
      "Epoch 2: train loss: 0.8000914789383335, val loss: 0.8289318531751633\n",
      "Epoch 3: train loss: 0.743526157951947, val loss: 0.6710024178028107\n",
      "Epoch 4: train loss: 0.6867553824421939, val loss: 0.6526826620101929\n",
      "Epoch 5: train loss: 0.6338142802106987, val loss: 0.5939285904169083\n",
      "Epoch 6: train loss: 0.5818165031884551, val loss: 0.5477640926837921\n",
      "Epoch 7: train loss: 0.531721235353625, val loss: 0.4980751648545265\n",
      "Epoch 8: train loss: 0.47984679607502273, val loss: 0.4464002028107643\n",
      "Epoch 9: train loss: 0.439322679183406, val loss: 0.42803681641817093\n",
      "Epoch 10: train loss: 0.39083435661221827, val loss: 0.3675011023879051\n",
      "Epoch 11: train loss: 0.354710235592977, val loss: 0.3383420705795288\n",
      "Epoch 12: train loss: 0.31793590893312357, val loss: 0.31042467057704926\n",
      "Epoch 13: train loss: 0.2851351860476179, val loss: 0.2533969134092331\n",
      "Epoch 14: train loss: 0.24876254861240332, val loss: 0.2503293305635452\n",
      "Epoch 15: train loss: 0.2263034752374457, val loss: 0.2130519524216652\n",
      "Epoch 16: train loss: 0.19384966035151707, val loss: 0.1716039702296257\n",
      "Epoch 17: train loss: 0.16753731801380561, val loss: 0.1462203897535801\n",
      "Epoch 18: train loss: 0.1454579627268535, val loss: 0.13226314261555672\n",
      "Epoch 19: train loss: 0.1284158379569703, val loss: 0.12619979679584503\n",
      "Epoch 20: train loss: 0.11000556812999841, val loss: 0.09973492473363876\n",
      "Epoch 21: train loss: 0.09476407782844919, val loss: 0.08990036882460117\n",
      "Epoch 22: train loss: 0.08316812387520642, val loss: 0.07379126735031605\n",
      "Epoch 23: train loss: 0.07237224517665787, val loss: 0.0635918015614152\n",
      "Epoch 24: train loss: 0.062018190130557316, val loss: 0.05998734571039677\n",
      "Epoch 25: train loss: 0.054397615721240536, val loss: 0.05440671183168888\n",
      "Epoch 26: train loss: 0.04759353243741908, val loss: 0.043774484656751156\n",
      "Epoch 27: train loss: 0.042452511540408695, val loss: 0.0407171044498682\n",
      "Epoch 28: train loss: 0.03744575308744754, val loss: 0.03364804293960333\n",
      "Epoch 29: train loss: 0.03338035363770902, val loss: 0.03165564127266407\n",
      "Epoch 30: train loss: 0.0297147439739812, val loss: 0.029689129441976547\n",
      "Epoch 31: train loss: 0.027139325405610072, val loss: 0.0262056109495461\n",
      "Epoch 32: train loss: 0.024882301819023647, val loss: 0.02522866101935506\n",
      "Epoch 33: train loss: 0.023700457681499833, val loss: 0.023948030546307564\n",
      "Epoch 34: train loss: 0.02202417539315203, val loss: 0.021884402260184288\n",
      "Epoch 35: train loss: 0.02117928082646108, val loss: 0.021964541636407375\n",
      "Epoch 36: train loss: 0.020147689595351185, val loss: 0.02063629264011979\n",
      "Epoch 37: train loss: 0.01970825659957578, val loss: 0.021937918849289417\n",
      "Epoch 38: train loss: 0.0191922303567872, val loss: 0.019190863240510225\n",
      "Epoch 39: train loss: 0.0186230782900165, val loss: 0.020820567850023508\n",
      "Epoch 40: train loss: 0.01841427566047316, val loss: 0.02171883499249816\n",
      "Epoch 41: train loss: 0.01819410812558788, val loss: 0.02029486419633031\n",
      "Epoch 42: train loss: 0.018151641720209156, val loss: 0.019596677739173174\n",
      "Epoch 43: train loss: 0.017787463785955734, val loss: 0.016613174229860306\n",
      "Epoch 44: train loss: 0.017863903830557505, val loss: 0.01946868607774377\n",
      "Epoch 45: train loss: 0.017500338574511793, val loss: 0.018050002865493298\n",
      "Epoch 46: train loss: 0.017569017057708376, val loss: 0.01907764980569482\n",
      "Epoch 47: train loss: 0.017372820272613335, val loss: 0.020054558757692575\n",
      "Epoch 48: train loss: 0.017133772888910593, val loss: 0.020134957041591406\n",
      "Epoch 00049: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 49: train loss: 0.017280822512698267, val loss: 0.019429777283221483\n",
      "Epoch 50: train loss: 0.016555583835918698, val loss: 0.019170995336025953\n",
      "Epoch 51: train loss: 0.016397620815076654, val loss: 0.0182066704146564\n",
      "Epoch 52: train loss: 0.01624484857947744, val loss: 0.019062872976064682\n",
      "Epoch 53: train loss: 0.0161567784530013, val loss: 0.019581403583288193\n",
      "Epoch 54: train loss: 0.016053390158878866, val loss: 0.01977450493723154\n",
      "Epoch 00055: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 55: train loss: 0.015858739415200093, val loss: 0.01780583243817091\n",
      "Epoch 56: train loss: 0.015644106871245518, val loss: 0.019702755380421877\n",
      "Epoch 57: train loss: 0.015925680596304753, val loss: 0.0187872052192688\n",
      "Epoch 58: train loss: 0.015743612089370522, val loss: 0.01751625002361834\n",
      "Epoch 59: train loss: 0.015722067770479707, val loss: 0.016341780312359333\n",
      "Epoch 60: train loss: 0.015586680055093002, val loss: 0.019308762159198523\n",
      "Epoch 61: train loss: 0.015757297915374578, val loss: 0.01806570217013359\n",
      "Epoch 62: train loss: 0.01580893942597634, val loss: 0.017436225898563862\n",
      "Epoch 63: train loss: 0.015628783748208582, val loss: 0.017938737757503986\n",
      "Epoch 64: train loss: 0.01569111850349995, val loss: 0.020310619845986366\n",
      "Epoch 00065: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 65: train loss: 0.015632898611865745, val loss: 0.017864907160401344\n",
      "Epoch 66: train loss: 0.015535239174071187, val loss: 0.01898336806334555\n",
      "Epoch 67: train loss: 0.01558459801821489, val loss: 0.01856724312528968\n",
      "Epoch 68: train loss: 0.015352245488978998, val loss: 0.018283469369634986\n",
      "Epoch 69: train loss: 0.015378033800116944, val loss: 0.018021598923951387\n",
      "Epoch 70: train loss: 0.015544198663666119, val loss: 0.017686385195702314\n",
      "Epoch 00071: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 71: train loss: 0.01564711450311005, val loss: 0.017947292886674404\n",
      "Epoch 72: train loss: 0.015364276531539742, val loss: 0.017348717199638486\n",
      "Epoch 73: train loss: 0.01556644587978667, val loss: 0.01747974194586277\n",
      "Epoch 74: train loss: 0.015453885988798145, val loss: 0.017165253404527903\n",
      "Epoch 75: train loss: 0.015490427594090337, val loss: 0.018229824025183916\n",
      "Epoch 76: train loss: 0.015257969928849993, val loss: 0.018504867795854807\n",
      "Epoch 00077: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 77: train loss: 0.015443023105572735, val loss: 0.017355655319988728\n",
      "Epoch 78: train loss: 0.015328482940442537, val loss: 0.01666566776111722\n",
      "Epoch 79: train loss: 0.015332696461007704, val loss: 0.018958881497383118\n",
      "Epoch 80: train loss: 0.015287562606297007, val loss: 0.01847058581188321\n",
      "Epoch 81: train loss: 0.01556545840463717, val loss: 0.020123756490647793\n",
      "Epoch 82: train loss: 0.01538261491318508, val loss: 0.017626385204494\n",
      "Epoch 00083: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 83: train loss: 0.015363303467397673, val loss: 0.0198074784129858\n",
      "Early stop at epoch 83\n"
     ]
    }
   ],
   "source": [
    "# model declaration\n",
    "nn_model = Net()\n",
    "nn_model.to(device)\n",
    "nn_model.train()\n",
    "    \n",
    "def train_nn():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(pretrained_x, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001, weight_decay=0.0001)    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_x_train = ae_model.encoder(torch.tensor(x_train, dtype=torch.float).to(device))\n",
    "featured_x_train = nn_model.encode(featured_x_train).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6021049886248567\n"
     ]
    }
   ],
   "source": [
    "huber_reg = HuberRegressor(max_iter=10000)\n",
    "huber_reg.fit(featured_x_train, y_train)\n",
    "print(huber_reg.score(featured_x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-huber.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-huber.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "y_pred = huber_reg.predict(nn_model.encode(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))).detach().cpu().numpy())\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
