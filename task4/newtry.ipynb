{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "#     x_train = scaler.transform(x_train)\n",
    "#     x_test_transed = scaler.transform(x_test)\n",
    "#     x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20806e83dcc24103b3f3b8803d166071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.2802698064263215, val loss: 0.2782503440976143\n",
      "Epoch 2: train loss: 0.2764578491634343, val loss: 0.27451617270708084\n",
      "Epoch 3: train loss: 0.27272464971221455, val loss: 0.2708417698740959\n",
      "Epoch 4: train loss: 0.26901175914656605, val loss: 0.2671213075518608\n",
      "Epoch 5: train loss: 0.2652498954205449, val loss: 0.26334628462791443\n",
      "Epoch 6: train loss: 0.26138282014980146, val loss: 0.2594311237335205\n",
      "Epoch 7: train loss: 0.257339344562872, val loss: 0.2552710548043251\n",
      "Epoch 8: train loss: 0.2530451943889158, val loss: 0.25085053592920303\n",
      "Epoch 9: train loss: 0.24840109469414692, val loss: 0.24599934741854668\n",
      "Epoch 10: train loss: 0.2433054954037484, val loss: 0.24066056311130524\n",
      "Epoch 11: train loss: 0.23763303788258958, val loss: 0.23463580384850502\n",
      "Epoch 12: train loss: 0.23124398124136203, val loss: 0.22783591598272324\n",
      "Epoch 13: train loss: 0.22397513688097434, val loss: 0.2200760468840599\n",
      "Epoch 14: train loss: 0.21568109935812216, val loss: 0.21121688187122345\n",
      "Epoch 15: train loss: 0.20620037344634554, val loss: 0.2011183798313141\n",
      "Epoch 16: train loss: 0.19545057439328953, val loss: 0.18972956761717796\n",
      "Epoch 17: train loss: 0.1834056287838407, val loss: 0.17703167721629143\n",
      "Epoch 18: train loss: 0.17016976642398343, val loss: 0.1632693111896515\n",
      "Epoch 19: train loss: 0.15602118030983964, val loss: 0.1487717367708683\n",
      "Epoch 20: train loss: 0.14136731351991372, val loss: 0.13404523208737373\n",
      "Epoch 21: train loss: 0.12674654727909387, val loss: 0.11963402293622494\n",
      "Epoch 22: train loss: 0.11272644406146218, val loss: 0.106088662520051\n",
      "Epoch 23: train loss: 0.0997767468627782, val loss: 0.09383533336222172\n",
      "Epoch 24: train loss: 0.08822567663096166, val loss: 0.08304019086062908\n",
      "Epoch 25: train loss: 0.07822520888755229, val loss: 0.07382816635072231\n",
      "Epoch 26: train loss: 0.0697536798390599, val loss: 0.06608571112155914\n",
      "Epoch 27: train loss: 0.06268817880177217, val loss: 0.05966846086084843\n",
      "Epoch 28: train loss: 0.056850329996839685, val loss: 0.054385462775826454\n",
      "Epoch 29: train loss: 0.052043838517138556, val loss: 0.05001866817474365\n",
      "Epoch 30: train loss: 0.04808141209479642, val loss: 0.0464228019118309\n",
      "Epoch 31: train loss: 0.044810767610773704, val loss: 0.04344574362039566\n",
      "Epoch 32: train loss: 0.042091733478167914, val loss: 0.04096921160817146\n",
      "Epoch 33: train loss: 0.03981279683771261, val loss: 0.03888721577823162\n",
      "Epoch 34: train loss: 0.0378908934385042, val loss: 0.037145527079701424\n",
      "Epoch 35: train loss: 0.03625395843422674, val loss: 0.03560231439769268\n",
      "Epoch 36: train loss: 0.03485290708348315, val loss: 0.03429788816720247\n",
      "Epoch 37: train loss: 0.03363510033457118, val loss: 0.03316473215818405\n",
      "Epoch 38: train loss: 0.03257592096657038, val loss: 0.03216072730720043\n",
      "Epoch 39: train loss: 0.03164047405685973, val loss: 0.03129194676876068\n",
      "Epoch 40: train loss: 0.030813118618752043, val loss: 0.030510474927723408\n",
      "Epoch 41: train loss: 0.03007601526721324, val loss: 0.029811701737344265\n",
      "Epoch 42: train loss: 0.0294102640802449, val loss: 0.029185657389461994\n",
      "Epoch 43: train loss: 0.028808751345844257, val loss: 0.028614844661206007\n",
      "Epoch 44: train loss: 0.02826397845747326, val loss: 0.028095534536987543\n",
      "Epoch 45: train loss: 0.027766478773996316, val loss: 0.027595017105340958\n",
      "Epoch 46: train loss: 0.02730594476644158, val loss: 0.027162262238562107\n",
      "Epoch 47: train loss: 0.026880236819620148, val loss: 0.02675655297935009\n",
      "Epoch 48: train loss: 0.02649002053191638, val loss: 0.0263910130597651\n",
      "Epoch 49: train loss: 0.026128463911386107, val loss: 0.026024213060736656\n",
      "Epoch 50: train loss: 0.025784037195508754, val loss: 0.025696248281747103\n",
      "Epoch 51: train loss: 0.025467254592021163, val loss: 0.02538673486560583\n",
      "Epoch 52: train loss: 0.025170453676539518, val loss: 0.025093620643019676\n",
      "Epoch 53: train loss: 0.02488660649480149, val loss: 0.024833008646965027\n",
      "Epoch 54: train loss: 0.024619258265383723, val loss: 0.02456584945321083\n",
      "Epoch 55: train loss: 0.02436825024114349, val loss: 0.024340863805264235\n",
      "Epoch 56: train loss: 0.024128152298497945, val loss: 0.02407787460833788\n",
      "Epoch 57: train loss: 0.023901017246435025, val loss: 0.023861144203692675\n",
      "Epoch 58: train loss: 0.023682563098032502, val loss: 0.023643012158572674\n",
      "Epoch 59: train loss: 0.023476616311203766, val loss: 0.023448923137038946\n",
      "Epoch 60: train loss: 0.02327821632054303, val loss: 0.023253426421433687\n",
      "Epoch 61: train loss: 0.023091454028460138, val loss: 0.02305454807356\n",
      "Epoch 62: train loss: 0.02290973810516164, val loss: 0.022900341544300318\n",
      "Epoch 63: train loss: 0.022733936929081593, val loss: 0.022720325272530317\n",
      "Epoch 64: train loss: 0.0225683134176964, val loss: 0.022560596000403166\n",
      "Epoch 65: train loss: 0.02240640508076094, val loss: 0.02238840749487281\n",
      "Epoch 66: train loss: 0.02225211180371714, val loss: 0.022242332343012094\n",
      "Epoch 67: train loss: 0.022104859767114755, val loss: 0.02210275176912546\n",
      "Epoch 68: train loss: 0.021961437387964532, val loss: 0.02194920741021633\n",
      "Epoch 69: train loss: 0.02182035651928126, val loss: 0.021813674364238977\n",
      "Epoch 70: train loss: 0.021683698554140842, val loss: 0.02168432157486677\n",
      "Epoch 71: train loss: 0.02155402550648451, val loss: 0.021544120740145445\n",
      "Epoch 72: train loss: 0.02142827952331147, val loss: 0.021444333251565695\n",
      "Epoch 73: train loss: 0.021307486912971303, val loss: 0.021309760864824057\n",
      "Epoch 74: train loss: 0.021187559298973813, val loss: 0.021185748279094696\n",
      "Epoch 75: train loss: 0.021073989727136085, val loss: 0.021084658335894346\n",
      "Epoch 76: train loss: 0.02096163594778244, val loss: 0.020964658353477716\n",
      "Epoch 77: train loss: 0.02085326178433244, val loss: 0.02086797822266817\n",
      "Epoch 78: train loss: 0.020747252619176936, val loss: 0.020750476978719234\n",
      "Epoch 79: train loss: 0.02064373244509242, val loss: 0.02065285947173834\n",
      "Epoch 80: train loss: 0.02054316308793582, val loss: 0.020555038936436176\n",
      "Epoch 81: train loss: 0.020445283337434457, val loss: 0.020458909682929516\n",
      "Epoch 82: train loss: 0.020351822253429824, val loss: 0.02036196505650878\n",
      "Epoch 83: train loss: 0.020259480473117788, val loss: 0.020269340369850397\n",
      "Epoch 84: train loss: 0.020168088736759217, val loss: 0.020183619111776352\n",
      "Epoch 85: train loss: 0.02008191926106056, val loss: 0.020104883704334497\n",
      "Epoch 86: train loss: 0.019993322808630167, val loss: 0.020016697235405445\n",
      "Epoch 87: train loss: 0.019912631316941105, val loss: 0.019940097350627184\n",
      "Epoch 88: train loss: 0.019830570340507055, val loss: 0.019865100271999836\n",
      "Epoch 89: train loss: 0.019750479097018363, val loss: 0.019767583347857\n",
      "Epoch 90: train loss: 0.019672234621469744, val loss: 0.019700078759342432\n",
      "Epoch 91: train loss: 0.01959682637640013, val loss: 0.019618946593254805\n",
      "Epoch 92: train loss: 0.019520003295648687, val loss: 0.01955161662772298\n",
      "Epoch 93: train loss: 0.01944776395337619, val loss: 0.01947664562612772\n",
      "Epoch 94: train loss: 0.019374336741258606, val loss: 0.01940604578703642\n",
      "Epoch 95: train loss: 0.01930702683612904, val loss: 0.019341213162988424\n",
      "Epoch 96: train loss: 0.019237969363365675, val loss: 0.01927199074998498\n",
      "Epoch 97: train loss: 0.01917230869208002, val loss: 0.019204365089535713\n",
      "Epoch 98: train loss: 0.0191067491890178, val loss: 0.019134437665343285\n",
      "Epoch 99: train loss: 0.01904297745471704, val loss: 0.019065487664192915\n",
      "Epoch 100: train loss: 0.01898155312429005, val loss: 0.01901512360200286\n",
      "Epoch 101: train loss: 0.018918522999021633, val loss: 0.01896373787894845\n",
      "Epoch 102: train loss: 0.018857596345350502, val loss: 0.018889483530074358\n",
      "Epoch 103: train loss: 0.018798991760293163, val loss: 0.01883073290809989\n",
      "Epoch 104: train loss: 0.01874028233283682, val loss: 0.01877688756212592\n",
      "Epoch 105: train loss: 0.01868222686481881, val loss: 0.018714988138526678\n",
      "Epoch 106: train loss: 0.018626285434543248, val loss: 0.018657321576029062\n",
      "Epoch 107: train loss: 0.018572351436761423, val loss: 0.0186024340800941\n",
      "Epoch 108: train loss: 0.01852053996420148, val loss: 0.018552361521869898\n",
      "Epoch 109: train loss: 0.018468367068504087, val loss: 0.018497785087674856\n",
      "Epoch 110: train loss: 0.018414639128808374, val loss: 0.01844792068004608\n",
      "Epoch 111: train loss: 0.01836647401688263, val loss: 0.01840823981910944\n",
      "Epoch 112: train loss: 0.018313374350864255, val loss: 0.018345416523516178\n",
      "Epoch 113: train loss: 0.01826680842354986, val loss: 0.018316544126719236\n",
      "Epoch 114: train loss: 0.018216914865159396, val loss: 0.018260774668306112\n",
      "Epoch 115: train loss: 0.01816930124649948, val loss: 0.018206889741122723\n",
      "Epoch 116: train loss: 0.018125702506214213, val loss: 0.018180863931775093\n",
      "Epoch 117: train loss: 0.01808035281072719, val loss: 0.01811678847298026\n",
      "Epoch 118: train loss: 0.018034831817605377, val loss: 0.01806633360683918\n",
      "Epoch 119: train loss: 0.017990202786281037, val loss: 0.018023953773081303\n",
      "Epoch 120: train loss: 0.017949052536552813, val loss: 0.017997185233980417\n",
      "Epoch 121: train loss: 0.01790690552035565, val loss: 0.017954949755221605\n",
      "Epoch 122: train loss: 0.017862654533584068, val loss: 0.01790638593956828\n",
      "Epoch 123: train loss: 0.017823091531351906, val loss: 0.017852147575467825\n",
      "Epoch 124: train loss: 0.017781522608161633, val loss: 0.017832412384450436\n",
      "Epoch 125: train loss: 0.01774086985507069, val loss: 0.017799516208469868\n",
      "Epoch 126: train loss: 0.01770579808643249, val loss: 0.017757438123226166\n",
      "Epoch 127: train loss: 0.01766753000959058, val loss: 0.01771423825994134\n",
      "Epoch 128: train loss: 0.017626293989951726, val loss: 0.017671857494860888\n",
      "Epoch 129: train loss: 0.017591815677948226, val loss: 0.017630765214562416\n",
      "Epoch 130: train loss: 0.01755358426132182, val loss: 0.017589516937732697\n",
      "Epoch 131: train loss: 0.017519450629947232, val loss: 0.017564064357429743\n",
      "Epoch 132: train loss: 0.017483347546149802, val loss: 0.01752456557005644\n",
      "Epoch 133: train loss: 0.01744828558484642, val loss: 0.01750640571117401\n",
      "Epoch 134: train loss: 0.01741328810538509, val loss: 0.017454371321946383\n",
      "Epoch 135: train loss: 0.017381609415368333, val loss: 0.01743545988574624\n",
      "Epoch 136: train loss: 0.01734770605220724, val loss: 0.01740594068542123\n",
      "Epoch 137: train loss: 0.017315514837822194, val loss: 0.01736294198781252\n",
      "Epoch 138: train loss: 0.01728131986207644, val loss: 0.017333627212792635\n",
      "Epoch 139: train loss: 0.017253474560278727, val loss: 0.017297755926847458\n",
      "Epoch 140: train loss: 0.017219977151530702, val loss: 0.017264241352677345\n",
      "Epoch 141: train loss: 0.01718921528089206, val loss: 0.017238314263522625\n",
      "Epoch 142: train loss: 0.01716024132079741, val loss: 0.017209196463227272\n",
      "Epoch 143: train loss: 0.017131097959181094, val loss: 0.01718570990487933\n",
      "Epoch 144: train loss: 0.017102618053998483, val loss: 0.01714628329500556\n",
      "Epoch 145: train loss: 0.01707232592226399, val loss: 0.017131580039858818\n",
      "Epoch 146: train loss: 0.017044926066485857, val loss: 0.017097051721066236\n",
      "Epoch 147: train loss: 0.01701526383497247, val loss: 0.01706863334402442\n",
      "Epoch 148: train loss: 0.016989815649167263, val loss: 0.01704300381243229\n",
      "Epoch 149: train loss: 0.016964001826301815, val loss: 0.01701252581551671\n",
      "Epoch 150: train loss: 0.01693725458515777, val loss: 0.01699438178911805\n",
      "Epoch 151: train loss: 0.01691005480156561, val loss: 0.016967652831226587\n",
      "Epoch 152: train loss: 0.016883090593599877, val loss: 0.01694664265960455\n",
      "Epoch 153: train loss: 0.016861553147206387, val loss: 0.016903378069400787\n",
      "Epoch 154: train loss: 0.016834785463212362, val loss: 0.016883241944015026\n",
      "Epoch 155: train loss: 0.016809112892864343, val loss: 0.016854604240506887\n",
      "Epoch 156: train loss: 0.016783780666912197, val loss: 0.016828226391226053\n",
      "Epoch 157: train loss: 0.01675778165537522, val loss: 0.01682483684271574\n",
      "Epoch 158: train loss: 0.01673742706654061, val loss: 0.016803568229079247\n",
      "Epoch 159: train loss: 0.016713613819492795, val loss: 0.016776464879512787\n",
      "Epoch 160: train loss: 0.01668853033060188, val loss: 0.01676191110163927\n",
      "Epoch 161: train loss: 0.01666796421683894, val loss: 0.016722945496439934\n",
      "Epoch 162: train loss: 0.016645791874976376, val loss: 0.016697958577424288\n",
      "Epoch 163: train loss: 0.01662433367345466, val loss: 0.01668631751090288\n",
      "Epoch 164: train loss: 0.0166020878438212, val loss: 0.016659109853208065\n",
      "Epoch 165: train loss: 0.016583610361953726, val loss: 0.016640796326100826\n",
      "Epoch 166: train loss: 0.016560077583870635, val loss: 0.01663629710674286\n",
      "Epoch 167: train loss: 0.01653940406832108, val loss: 0.016619568690657616\n",
      "Epoch 168: train loss: 0.01651806106175698, val loss: 0.0165790393948555\n",
      "Epoch 169: train loss: 0.016500623159205474, val loss: 0.016575515270233154\n",
      "Epoch 170: train loss: 0.016477734923279974, val loss: 0.016540465876460075\n",
      "Epoch 171: train loss: 0.01645931853081852, val loss: 0.016519678756594658\n",
      "Epoch 172: train loss: 0.016439905426653918, val loss: 0.016497293021529913\n",
      "Epoch 173: train loss: 0.016420983822273298, val loss: 0.01647853758186102\n",
      "Epoch 174: train loss: 0.016404371544334165, val loss: 0.01645694673061371\n",
      "Epoch 175: train loss: 0.016384953918341365, val loss: 0.01643675658851862\n",
      "Epoch 176: train loss: 0.016365440001322413, val loss: 0.016428378876298666\n",
      "Epoch 177: train loss: 0.016347863866282186, val loss: 0.01640466321259737\n",
      "Epoch 178: train loss: 0.016331921139599956, val loss: 0.01639276696369052\n",
      "Epoch 179: train loss: 0.01631648521121909, val loss: 0.0163742839358747\n",
      "Epoch 180: train loss: 0.016294383307890306, val loss: 0.016363411210477352\n",
      "Epoch 181: train loss: 0.016280219030050194, val loss: 0.016334078274667263\n",
      "Epoch 182: train loss: 0.016261452421356126, val loss: 0.01633935235440731\n",
      "Epoch 183: train loss: 0.016248133230062335, val loss: 0.016313670203089714\n",
      "Epoch 184: train loss: 0.016228675715106836, val loss: 0.01629062369465828\n",
      "Epoch 185: train loss: 0.01621431621550365, val loss: 0.016277152579277754\n",
      "Epoch 186: train loss: 0.016195862247021086, val loss: 0.016255194321274757\n",
      "Epoch 187: train loss: 0.01618107753918086, val loss: 0.016244337428361177\n",
      "Epoch 188: train loss: 0.01616773860543061, val loss: 0.01624077931046486\n",
      "Epoch 189: train loss: 0.01615041331471726, val loss: 0.01622170675545931\n",
      "Epoch 190: train loss: 0.01613561970804572, val loss: 0.01620033895596862\n",
      "Epoch 191: train loss: 0.01612136556929095, val loss: 0.01618986902758479\n",
      "Epoch 192: train loss: 0.01610725723348171, val loss: 0.016173286363482475\n",
      "Epoch 193: train loss: 0.016091563103903708, val loss: 0.01616728212684393\n",
      "Epoch 194: train loss: 0.016078086239485055, val loss: 0.01615011179819703\n",
      "Epoch 195: train loss: 0.016065336123707637, val loss: 0.01612404966726899\n",
      "Epoch 196: train loss: 0.016048293490156323, val loss: 0.016135905869305134\n",
      "Epoch 197: train loss: 0.016036937556666745, val loss: 0.01610482670366764\n",
      "Epoch 198: train loss: 0.01602340118070515, val loss: 0.016084046103060246\n",
      "Epoch 199: train loss: 0.01601027313112356, val loss: 0.0160695631057024\n",
      "Epoch 200: train loss: 0.015995362303160406, val loss: 0.01608366472646594\n",
      "Epoch 201: train loss: 0.01598400211741199, val loss: 0.016049218364059925\n",
      "Epoch 202: train loss: 0.01597102891625785, val loss: 0.01604068512097001\n",
      "Epoch 203: train loss: 0.01595717782932962, val loss: 0.01601478084921837\n",
      "Epoch 204: train loss: 0.015945855586533064, val loss: 0.016020517796278\n",
      "Epoch 205: train loss: 0.01593289690196261, val loss: 0.015995079651474953\n",
      "Epoch 206: train loss: 0.01591941831607283, val loss: 0.01598724164068699\n",
      "Epoch 207: train loss: 0.015909327940126332, val loss: 0.01597913494333625\n",
      "Epoch 208: train loss: 0.01589633402016351, val loss: 0.0159513964317739\n",
      "Epoch 209: train loss: 0.015884964817837344, val loss: 0.015957994386553764\n",
      "Epoch 210: train loss: 0.01587231664126389, val loss: 0.015937944408506155\n",
      "Epoch 211: train loss: 0.015861752759135848, val loss: 0.015934084076434374\n",
      "Epoch 212: train loss: 0.015847590660365413, val loss: 0.01592059340327978\n",
      "Epoch 213: train loss: 0.015839032693718898, val loss: 0.015903292689472437\n",
      "Epoch 214: train loss: 0.01582683847376779, val loss: 0.01590195531025529\n",
      "Epoch 215: train loss: 0.015813587590618797, val loss: 0.015890529844909906\n",
      "Epoch 216: train loss: 0.015805031240779914, val loss: 0.015877369325608015\n",
      "Epoch 217: train loss: 0.015793456102982138, val loss: 0.015867032576352358\n",
      "Epoch 218: train loss: 0.015782218963170665, val loss: 0.015850914176553488\n",
      "Epoch 219: train loss: 0.01577148939203258, val loss: 0.015845746733248234\n",
      "Epoch 220: train loss: 0.015760591697849623, val loss: 0.01583394850604236\n",
      "Epoch 221: train loss: 0.015752226813449576, val loss: 0.015825956128537655\n",
      "Epoch 222: train loss: 0.015743758399203805, val loss: 0.015816843137145042\n",
      "Epoch 223: train loss: 0.015731452464356373, val loss: 0.015804068185389042\n",
      "Epoch 224: train loss: 0.01572171940107102, val loss: 0.015791382174938917\n",
      "Epoch 225: train loss: 0.01571244226049964, val loss: 0.015774673083797097\n",
      "Epoch 226: train loss: 0.01570030266941762, val loss: 0.015766615979373455\n",
      "Epoch 227: train loss: 0.01569494144240749, val loss: 0.015758392168208957\n",
      "Epoch 228: train loss: 0.015682100901019336, val loss: 0.015749697340652347\n",
      "Epoch 229: train loss: 0.015672735708629683, val loss: 0.01575857470743358\n",
      "Epoch 230: train loss: 0.015664762594304767, val loss: 0.01573551632463932\n",
      "Epoch 231: train loss: 0.01565408595125393, val loss: 0.015726611716672778\n",
      "Epoch 232: train loss: 0.01564408754957283, val loss: 0.01572965062223375\n",
      "Epoch 233: train loss: 0.015636057226301913, val loss: 0.01569943642243743\n",
      "Epoch 234: train loss: 0.01562583820340009, val loss: 0.015701618744060397\n",
      "Epoch 235: train loss: 0.015618993301030976, val loss: 0.015696556074544787\n",
      "Epoch 236: train loss: 0.015611543403789329, val loss: 0.015688011422753334\n",
      "Epoch 237: train loss: 0.015599099603383924, val loss: 0.015678578289225698\n",
      "Epoch 238: train loss: 0.01559278513835813, val loss: 0.015665011014789343\n",
      "Epoch 239: train loss: 0.015582560728632988, val loss: 0.015646682353690267\n",
      "Epoch 240: train loss: 0.015573407493212845, val loss: 0.015638835495337844\n",
      "Epoch 241: train loss: 0.01556545236579677, val loss: 0.015649283304810524\n",
      "Epoch 242: train loss: 0.01555855377196623, val loss: 0.015634371433407068\n",
      "Epoch 243: train loss: 0.015549452512317216, val loss: 0.015618041157722473\n",
      "Epoch 244: train loss: 0.015540970032392706, val loss: 0.015605102991685271\n",
      "Epoch 245: train loss: 0.015534054551228554, val loss: 0.01560802641324699\n",
      "Epoch 246: train loss: 0.01552699599354213, val loss: 0.015599640551954508\n",
      "Epoch 247: train loss: 0.01551632671557642, val loss: 0.015586990397423506\n",
      "Epoch 248: train loss: 0.015509661770225234, val loss: 0.015586314024403691\n",
      "Epoch 249: train loss: 0.015502931119320985, val loss: 0.01557631860487163\n",
      "Epoch 250: train loss: 0.015492641963356755, val loss: 0.015574833145365119\n",
      "Epoch 251: train loss: 0.015485923238310382, val loss: 0.015557155944406986\n",
      "Epoch 252: train loss: 0.015479517156213804, val loss: 0.01556394319050014\n",
      "Epoch 253: train loss: 0.015471580293795051, val loss: 0.015541840810328722\n",
      "Epoch 254: train loss: 0.015460864394326316, val loss: 0.015526496805250645\n",
      "Epoch 255: train loss: 0.015456384547451515, val loss: 0.01552238897420466\n",
      "Epoch 256: train loss: 0.015448598907965337, val loss: 0.015534881735220551\n",
      "Epoch 257: train loss: 0.015440020008619335, val loss: 0.015512436395511031\n",
      "Epoch 258: train loss: 0.015433428685519479, val loss: 0.015504352049902081\n",
      "Epoch 259: train loss: 0.015424865917130765, val loss: 0.015493151964619756\n",
      "Epoch 260: train loss: 0.015417613964120145, val loss: 0.015493569895625114\n",
      "Epoch 261: train loss: 0.01540989406730606, val loss: 0.015499618602916598\n",
      "Epoch 262: train loss: 0.015403811241463873, val loss: 0.015478566056117415\n",
      "Epoch 263: train loss: 0.015395248185250543, val loss: 0.015468554105609655\n",
      "Epoch 264: train loss: 0.015389449640375868, val loss: 0.015459647867828608\n",
      "Epoch 265: train loss: 0.01538514294008956, val loss: 0.015464184805750847\n",
      "Epoch 266: train loss: 0.015377890611324814, val loss: 0.015457332134246826\n",
      "Epoch 267: train loss: 0.01536809900402536, val loss: 0.015439846785739064\n",
      "Epoch 268: train loss: 0.0153623952900602, val loss: 0.015428431564942002\n",
      "Epoch 269: train loss: 0.015353626665610264, val loss: 0.015430036233738065\n",
      "Epoch 270: train loss: 0.01534524154342767, val loss: 0.01542017050087452\n",
      "Epoch 271: train loss: 0.01534108878306531, val loss: 0.015411889180541039\n",
      "Epoch 272: train loss: 0.015334829723974219, val loss: 0.015405040932819247\n",
      "Epoch 273: train loss: 0.01532751562123116, val loss: 0.01540127326734364\n",
      "Epoch 274: train loss: 0.015316640821275505, val loss: 0.015394740039482713\n",
      "Epoch 275: train loss: 0.015314788682941848, val loss: 0.015398981282487512\n",
      "Epoch 276: train loss: 0.01530813982929914, val loss: 0.015377923613414168\n",
      "Epoch 277: train loss: 0.01530089918329304, val loss: 0.015366669511422515\n",
      "Epoch 278: train loss: 0.015292176589788428, val loss: 0.015367248794063926\n",
      "Epoch 279: train loss: 0.015286819755906901, val loss: 0.015348432818427682\n",
      "Epoch 280: train loss: 0.015280102857907919, val loss: 0.015366922598332167\n",
      "Epoch 281: train loss: 0.015272240713968927, val loss: 0.015344304498285055\n",
      "Epoch 282: train loss: 0.015267503531500625, val loss: 0.015343293780460954\n",
      "Epoch 283: train loss: 0.015260612067566593, val loss: 0.015329310204833746\n",
      "Epoch 284: train loss: 0.01525429626138761, val loss: 0.015346631174907088\n",
      "Epoch 285: train loss: 0.01524898594312859, val loss: 0.015319089172407985\n",
      "Epoch 286: train loss: 0.015239019983784899, val loss: 0.015321270795539021\n",
      "Epoch 287: train loss: 0.015233111255326056, val loss: 0.015304258791729808\n",
      "Epoch 288: train loss: 0.01522770363317814, val loss: 0.015300342813134193\n",
      "Epoch 289: train loss: 0.015220233275744016, val loss: 0.01529332296922803\n",
      "Epoch 290: train loss: 0.015214265885220384, val loss: 0.015293100848793983\n",
      "Epoch 291: train loss: 0.0152091866034171, val loss: 0.015279718209058046\n",
      "Epoch 292: train loss: 0.01520186298724893, val loss: 0.015278256265446544\n",
      "Epoch 293: train loss: 0.015195000020081858, val loss: 0.0152745945379138\n",
      "Epoch 294: train loss: 0.01518878697581061, val loss: 0.015251524280756712\n",
      "Epoch 295: train loss: 0.015183650517616542, val loss: 0.015261706663295627\n",
      "Epoch 296: train loss: 0.01517812630337502, val loss: 0.015247234841808677\n",
      "Epoch 297: train loss: 0.01517033698727924, val loss: 0.015251086093485355\n",
      "Epoch 298: train loss: 0.015162077005520556, val loss: 0.015245911898091435\n",
      "Epoch 299: train loss: 0.015160065710822066, val loss: 0.015234194230288267\n",
      "Epoch 300: train loss: 0.015153449516145867, val loss: 0.01521749421954155\n",
      "Epoch 301: train loss: 0.015143234584552329, val loss: 0.015213459962978959\n",
      "Epoch 302: train loss: 0.015138697046488377, val loss: 0.015215436462312937\n",
      "Epoch 303: train loss: 0.01512950389715668, val loss: 0.015209183329716325\n",
      "Epoch 304: train loss: 0.015127399496670857, val loss: 0.015202788636088371\n",
      "Epoch 305: train loss: 0.015118219363644636, val loss: 0.01519305701367557\n",
      "Epoch 306: train loss: 0.01511599294042926, val loss: 0.015187317505478859\n",
      "Epoch 307: train loss: 0.015106783136822788, val loss: 0.015196081483736634\n",
      "Epoch 308: train loss: 0.015104402138096119, val loss: 0.015176121611148119\n",
      "Epoch 309: train loss: 0.015096225860361004, val loss: 0.015169379068538547\n",
      "Epoch 310: train loss: 0.015088772737110451, val loss: 0.01517464593052864\n",
      "Epoch 311: train loss: 0.015084153065279647, val loss: 0.015161177609115839\n",
      "Epoch 312: train loss: 0.015077042450890437, val loss: 0.015147087397053838\n",
      "Epoch 313: train loss: 0.015069875092760133, val loss: 0.01514901127666235\n",
      "Epoch 314: train loss: 0.015063592881434048, val loss: 0.0151509630959481\n",
      "Epoch 315: train loss: 0.015059073829560333, val loss: 0.015138169517740607\n",
      "Epoch 316: train loss: 0.015052291878633875, val loss: 0.015122324926778674\n",
      "Epoch 317: train loss: 0.015045983815119669, val loss: 0.015112714376300573\n",
      "Epoch 318: train loss: 0.015042343199416162, val loss: 0.015111379092559218\n",
      "Epoch 319: train loss: 0.015034112649093809, val loss: 0.015105828875675797\n",
      "Epoch 320: train loss: 0.01502757029147251, val loss: 0.015108921099454165\n",
      "Epoch 321: train loss: 0.015021810092078203, val loss: 0.015088851796463132\n",
      "Epoch 322: train loss: 0.015014971937766382, val loss: 0.015100449556484818\n",
      "Epoch 323: train loss: 0.015011919194717435, val loss: 0.015077334363013506\n",
      "Epoch 324: train loss: 0.0150037003426707, val loss: 0.01508553046733141\n",
      "Epoch 325: train loss: 0.014996726788782442, val loss: 0.015067648142576218\n",
      "Epoch 326: train loss: 0.014991075962134453, val loss: 0.015064459526911378\n",
      "Epoch 327: train loss: 0.014985811812343725, val loss: 0.015053431503474712\n",
      "Epoch 328: train loss: 0.014979544902881817, val loss: 0.015043742954730988\n",
      "Epoch 329: train loss: 0.014974930986934863, val loss: 0.015041093342006207\n",
      "Epoch 330: train loss: 0.01496843424038788, val loss: 0.015049085021018982\n",
      "Epoch 331: train loss: 0.014962416700227748, val loss: 0.015037595760077238\n",
      "Epoch 332: train loss: 0.014955281098949797, val loss: 0.015038517070934176\n",
      "Epoch 333: train loss: 0.014948420895325013, val loss: 0.01502368226647377\n",
      "Epoch 334: train loss: 0.014943835989117856, val loss: 0.015024007530882955\n",
      "Epoch 335: train loss: 0.014938781179404947, val loss: 0.015008834190666676\n",
      "Epoch 336: train loss: 0.014931701661940397, val loss: 0.01500332960858941\n",
      "Epoch 337: train loss: 0.014926150713477384, val loss: 0.015002063708379865\n",
      "Epoch 338: train loss: 0.014921880635374786, val loss: 0.014991145813837647\n",
      "Epoch 339: train loss: 0.014912236557664318, val loss: 0.014985661953687668\n",
      "Epoch 340: train loss: 0.014906310305371758, val loss: 0.014979895204305649\n",
      "Epoch 341: train loss: 0.01490258042874318, val loss: 0.014964895555749536\n",
      "Epoch 342: train loss: 0.014895000667528793, val loss: 0.014976335922256112\n",
      "Epoch 343: train loss: 0.014893299568358082, val loss: 0.014961346285417676\n",
      "Epoch 344: train loss: 0.014884153013755024, val loss: 0.014963441528379917\n",
      "Epoch 345: train loss: 0.014879770194506167, val loss: 0.014953231671825051\n",
      "Epoch 346: train loss: 0.01487160262635616, val loss: 0.014944209717214108\n",
      "Epoch 347: train loss: 0.014866766864564713, val loss: 0.01494647515937686\n",
      "Epoch 348: train loss: 0.014862679623551752, val loss: 0.014931565150618553\n",
      "Epoch 349: train loss: 0.014858770711795932, val loss: 0.014917911496013403\n",
      "Epoch 350: train loss: 0.014853309768768395, val loss: 0.01493961876258254\n",
      "Epoch 351: train loss: 0.014843450517207141, val loss: 0.014924233313649893\n",
      "Epoch 352: train loss: 0.014837746854356713, val loss: 0.014917404856532812\n",
      "Epoch 353: train loss: 0.014833566447922565, val loss: 0.014915975043550134\n",
      "Epoch 354: train loss: 0.014829528617208026, val loss: 0.014915228355675936\n",
      "Epoch 355: train loss: 0.014824242109894675, val loss: 0.014910346362739801\n",
      "Epoch 356: train loss: 0.01481826132031359, val loss: 0.014888482633978128\n",
      "Epoch 357: train loss: 0.014814456414865809, val loss: 0.014878517482429743\n",
      "Epoch 358: train loss: 0.014808360022626274, val loss: 0.014887484721839428\n",
      "Epoch 359: train loss: 0.014802505891089812, val loss: 0.014875797787681222\n",
      "Epoch 360: train loss: 0.014797943571813061, val loss: 0.014877155888825655\n",
      "Epoch 361: train loss: 0.014791027802824293, val loss: 0.014883928699418902\n",
      "Epoch 362: train loss: 0.014787745839413845, val loss: 0.014873642241582274\n",
      "Epoch 363: train loss: 0.014781042591786411, val loss: 0.014864644268527627\n",
      "Epoch 364: train loss: 0.014775276918247785, val loss: 0.01487446459941566\n",
      "Epoch 365: train loss: 0.014773790579750447, val loss: 0.014835964422672987\n",
      "Epoch 366: train loss: 0.014768897930305598, val loss: 0.014840715797618032\n",
      "Epoch 367: train loss: 0.01476141212800775, val loss: 0.014827741542831063\n",
      "Epoch 368: train loss: 0.014757118944829584, val loss: 0.014849708881229162\n",
      "Epoch 369: train loss: 0.014755574046192665, val loss: 0.014823259552940726\n",
      "Epoch 370: train loss: 0.014749371078825589, val loss: 0.014833084540441632\n",
      "Epoch 371: train loss: 0.014745166972802867, val loss: 0.0148091078735888\n",
      "Epoch 372: train loss: 0.01473631800907825, val loss: 0.014816638082265854\n",
      "Epoch 373: train loss: 0.014734781111113798, val loss: 0.014806198421865702\n",
      "Epoch 374: train loss: 0.014729603938210715, val loss: 0.01480225333943963\n",
      "Epoch 375: train loss: 0.014724827058155702, val loss: 0.014794234884902835\n",
      "Epoch 376: train loss: 0.01472261683618037, val loss: 0.014796344097703695\n",
      "Epoch 377: train loss: 0.014718814870742402, val loss: 0.014801379758864641\n",
      "Epoch 378: train loss: 0.014712489107839717, val loss: 0.014777601696550846\n",
      "Epoch 379: train loss: 0.014708470965728616, val loss: 0.01478189928457141\n",
      "Epoch 380: train loss: 0.01470301976462121, val loss: 0.014773833798244596\n",
      "Epoch 381: train loss: 0.014703042343032493, val loss: 0.014772791881114244\n",
      "Epoch 382: train loss: 0.014696852935977677, val loss: 0.014773426810279489\n",
      "Epoch 383: train loss: 0.014694480452113332, val loss: 0.014758001314476132\n",
      "Epoch 384: train loss: 0.014688561819262537, val loss: 0.014750557020306587\n",
      "Epoch 385: train loss: 0.014684211597216553, val loss: 0.014749328373000026\n",
      "Epoch 386: train loss: 0.014682089917821769, val loss: 0.014758397126570344\n",
      "Epoch 387: train loss: 0.014679330891276333, val loss: 0.01474675047211349\n",
      "Epoch 388: train loss: 0.014672862606507058, val loss: 0.014747047796845436\n",
      "Epoch 389: train loss: 0.014669158838553022, val loss: 0.014753005700185895\n",
      "Epoch 390: train loss: 0.01466994186664321, val loss: 0.014738454949110746\n",
      "Epoch 391: train loss: 0.014662923576107644, val loss: 0.014735209988430142\n",
      "Epoch 392: train loss: 0.014659964443299749, val loss: 0.014739152509719133\n",
      "Epoch 393: train loss: 0.014659698097822091, val loss: 0.01473440881818533\n",
      "Epoch 394: train loss: 0.014654025147318821, val loss: 0.01471921312622726\n",
      "Epoch 395: train loss: 0.014651641592551333, val loss: 0.014729262329638004\n",
      "Epoch 396: train loss: 0.014648829441370333, val loss: 0.014724337495863438\n",
      "Epoch 397: train loss: 0.0146460867949665, val loss: 0.014716811245307326\n",
      "Epoch 398: train loss: 0.01464271605500391, val loss: 0.014705928973853588\n",
      "Epoch 399: train loss: 0.014641644323304934, val loss: 0.014708885923027992\n",
      "Epoch 400: train loss: 0.014639892833832529, val loss: 0.014708296628668904\n",
      "Epoch 401: train loss: 0.014635434125333158, val loss: 0.014710452873259783\n",
      "Epoch 402: train loss: 0.014631444413965822, val loss: 0.01469086017459631\n",
      "Epoch 403: train loss: 0.014630893190269357, val loss: 0.01470384793356061\n",
      "Epoch 404: train loss: 0.014627338290927711, val loss: 0.014698462560772896\n",
      "Epoch 405: train loss: 0.014624228089444196, val loss: 0.01469493075273931\n",
      "Epoch 406: train loss: 0.014621990737108206, val loss: 0.01469668815843761\n",
      "Epoch 407: train loss: 0.014621165887735923, val loss: 0.014699583873152733\n",
      "Epoch 00408: reducing learning rate of group 0 to 3.0000e-02.\n",
      "Epoch 408: train loss: 0.014619481462616522, val loss: 0.014689994510263205\n",
      "Epoch 409: train loss: 0.014617036496846165, val loss: 0.014691503252834082\n",
      "Epoch 410: train loss: 0.01461422681621051, val loss: 0.014685724396258593\n",
      "Epoch 411: train loss: 0.014615978139560039, val loss: 0.014688786352053285\n",
      "Epoch 412: train loss: 0.014615125125362297, val loss: 0.014679842861369252\n",
      "Epoch 413: train loss: 0.014613792994836993, val loss: 0.014691543066874146\n",
      "Epoch 414: train loss: 0.014615110967494486, val loss: 0.014677600469440222\n",
      "Epoch 415: train loss: 0.014611748989448755, val loss: 0.014671690529212356\n",
      "Epoch 416: train loss: 0.014612805779824963, val loss: 0.014681193046271801\n",
      "Epoch 417: train loss: 0.01461362261270355, val loss: 0.014683143701404333\n",
      "Epoch 418: train loss: 0.014610680118044951, val loss: 0.014676810707896948\n",
      "Epoch 419: train loss: 0.014611223030729509, val loss: 0.014676833292469382\n",
      "Epoch 420: train loss: 0.014610120406260543, val loss: 0.0146764584351331\n",
      "Epoch 00421: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 421: train loss: 0.014609494480818306, val loss: 0.014694412238895893\n",
      "Epoch 422: train loss: 0.014608500776973688, val loss: 0.01468042517080903\n",
      "Epoch 423: train loss: 0.014611219715878334, val loss: 0.014671418815851212\n",
      "Epoch 424: train loss: 0.014609645047474243, val loss: 0.01467868429608643\n",
      "Epoch 425: train loss: 0.014609272812354102, val loss: 0.014675479382276535\n",
      "Epoch 426: train loss: 0.014608635725036668, val loss: 0.014689052710309625\n",
      "Epoch 00427: reducing learning rate of group 0 to 2.7000e-03.\n",
      "Epoch 427: train loss: 0.01460815169956512, val loss: 0.014688660856336355\n",
      "Epoch 428: train loss: 0.01460961053255618, val loss: 0.014676069375127554\n",
      "Epoch 429: train loss: 0.014608704051132472, val loss: 0.014683196321129799\n",
      "Epoch 430: train loss: 0.014607407301433119, val loss: 0.014685790985822678\n",
      "Epoch 431: train loss: 0.01460808777298726, val loss: 0.014686619630083442\n",
      "Epoch 432: train loss: 0.014606580192668011, val loss: 0.014700396452099085\n",
      "Epoch 00433: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 433: train loss: 0.014609647162346251, val loss: 0.014680563705042005\n",
      "Epoch 434: train loss: 0.014606839398208766, val loss: 0.014675250044092536\n",
      "Epoch 435: train loss: 0.014606895471069012, val loss: 0.014674898935481906\n",
      "Epoch 436: train loss: 0.014608245197840567, val loss: 0.014670506352558732\n",
      "Epoch 437: train loss: 0.014608760462323558, val loss: 0.014681922039017081\n",
      "Epoch 438: train loss: 0.014608723679797804, val loss: 0.014697054866701365\n",
      "Epoch 00439: reducing learning rate of group 0 to 2.4300e-04.\n",
      "Epoch 439: train loss: 0.014608443048969763, val loss: 0.014672915218397975\n",
      "Epoch 440: train loss: 0.014607949616562128, val loss: 0.014682553941383958\n",
      "Epoch 441: train loss: 0.014607358663636499, val loss: 0.01467318064533174\n",
      "Epoch 442: train loss: 0.014605537874758613, val loss: 0.014676443533971906\n",
      "Epoch 443: train loss: 0.014605759971308693, val loss: 0.014685989590361714\n",
      "Epoch 444: train loss: 0.014607587992921058, val loss: 0.014676929218694568\n",
      "Epoch 00445: reducing learning rate of group 0 to 7.2900e-05.\n",
      "Epoch 445: train loss: 0.01460819219034293, val loss: 0.014680906431749463\n",
      "Epoch 446: train loss: 0.014610263432276595, val loss: 0.014680291526019573\n",
      "Epoch 447: train loss: 0.014606716442026858, val loss: 0.014679583022370934\n",
      "Epoch 448: train loss: 0.01460719115579393, val loss: 0.014675319660454988\n",
      "Epoch 449: train loss: 0.014607498366282251, val loss: 0.01468594465404749\n",
      "Epoch 450: train loss: 0.014607862046074241, val loss: 0.014683241257444024\n",
      "Epoch 00451: reducing learning rate of group 0 to 2.1870e-05.\n",
      "Epoch 451: train loss: 0.01460762951268709, val loss: 0.01468964060768485\n",
      "Epoch 452: train loss: 0.0146076022438907, val loss: 0.014692190568894148\n",
      "Epoch 453: train loss: 0.014609464327386395, val loss: 0.014675578568130732\n",
      "Epoch 454: train loss: 0.014607903881352035, val loss: 0.014682112727314234\n",
      "Epoch 455: train loss: 0.014606328127368413, val loss: 0.01468055578880012\n",
      "Epoch 456: train loss: 0.01460837726157228, val loss: 0.014670776203274727\n",
      "Epoch 457: train loss: 0.014608043260271158, val loss: 0.01466988562606275\n",
      "Epoch 458: train loss: 0.01460970296502863, val loss: 0.014674075180664659\n",
      "Epoch 459: train loss: 0.01460767332713945, val loss: 0.014689374715089798\n",
      "Epoch 460: train loss: 0.014607615406846774, val loss: 0.014688626397401094\n",
      "Epoch 461: train loss: 0.014607925303292524, val loss: 0.01467513875104487\n",
      "Epoch 462: train loss: 0.014608126106601706, val loss: 0.01468600519001484\n",
      "Epoch 00463: reducing learning rate of group 0 to 6.5610e-06.\n",
      "Epoch 463: train loss: 0.014607424394138417, val loss: 0.014680399326607585\n",
      "Epoch 464: train loss: 0.014607173726887572, val loss: 0.014667231822386384\n",
      "Epoch 465: train loss: 0.014606122551221195, val loss: 0.014679403742775321\n",
      "Epoch 466: train loss: 0.014608638978732718, val loss: 0.014676878461614251\n",
      "Epoch 467: train loss: 0.01460650532814616, val loss: 0.014692997559905052\n",
      "Epoch 468: train loss: 0.014607384973423168, val loss: 0.014674563193693757\n",
      "Epoch 469: train loss: 0.014606822871416564, val loss: 0.014676850056275725\n",
      "Epoch 00470: reducing learning rate of group 0 to 1.9683e-06.\n",
      "Epoch 470: train loss: 0.014606536856199238, val loss: 0.014679949963465333\n",
      "Epoch 471: train loss: 0.014607999403831695, val loss: 0.014680178137496114\n",
      "Epoch 472: train loss: 0.014607852592891492, val loss: 0.014687036396935582\n",
      "Epoch 473: train loss: 0.014607261733980456, val loss: 0.014677495695650578\n",
      "Epoch 474: train loss: 0.014607520906661741, val loss: 0.014682404231280088\n",
      "Epoch 475: train loss: 0.014605538589147834, val loss: 0.014680975582450628\n",
      "Epoch 00476: reducing learning rate of group 0 to 5.9049e-07.\n",
      "Epoch 476: train loss: 0.014607171109387373, val loss: 0.014688749331980944\n",
      "Early stop at epoch 476\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_x = ae_model.encoder(torch.tensor(x_pretrain, dtype=torch.float).to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43e04bfc4654e5185883937b4ed650c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.2176157743546823, val loss: 0.5885423868894577\n",
      "Epoch 2: train loss: 0.5968789197930476, val loss: 0.5417300313711166\n",
      "Epoch 3: train loss: 0.5317910633341076, val loss: 0.49697350710630417\n",
      "Epoch 4: train loss: 0.4923697005919638, val loss: 0.5065941885113716\n",
      "Epoch 5: train loss: 0.4437599905621417, val loss: 0.42411258071660995\n",
      "Epoch 6: train loss: 0.40792531959958345, val loss: 0.3807792440056801\n",
      "Epoch 7: train loss: 0.3712322944054141, val loss: 0.3741005063056946\n",
      "Epoch 8: train loss: 0.3357777037347154, val loss: 0.30825547873973846\n",
      "Epoch 9: train loss: 0.30394409571119246, val loss: 0.3008662611246109\n",
      "Epoch 10: train loss: 0.2803601227829714, val loss: 0.2647411674261093\n",
      "Epoch 11: train loss: 0.25048334226231295, val loss: 0.21686721965670586\n",
      "Epoch 12: train loss: 0.22707930822576505, val loss: 0.22091049700975418\n",
      "Epoch 13: train loss: 0.20517777673698104, val loss: 0.19073177501559258\n",
      "Epoch 14: train loss: 0.1870312693022856, val loss: 0.16045453399419785\n",
      "Epoch 15: train loss: 0.16831760852798766, val loss: 0.15289628133177757\n",
      "Epoch 16: train loss: 0.1491166198275051, val loss: 0.140272356569767\n",
      "Epoch 17: train loss: 0.13518953004018888, val loss: 0.13224824517965317\n",
      "Epoch 18: train loss: 0.12034109666120846, val loss: 0.10890016704797745\n",
      "Epoch 19: train loss: 0.10726336367873732, val loss: 0.09376038797199726\n",
      "Epoch 20: train loss: 0.09524192383567012, val loss: 0.08857634663581848\n",
      "Epoch 21: train loss: 0.08516793819774175, val loss: 0.08016224578022957\n",
      "Epoch 22: train loss: 0.07412929448601234, val loss: 0.07273164857178926\n",
      "Epoch 23: train loss: 0.06464930809486617, val loss: 0.05709785036742687\n",
      "Epoch 24: train loss: 0.05773040528504415, val loss: 0.0561856497079134\n",
      "Epoch 25: train loss: 0.05034503507695919, val loss: 0.049182916060090065\n",
      "Epoch 26: train loss: 0.04434220193088425, val loss: 0.044284772127866745\n",
      "Epoch 27: train loss: 0.03926997939693659, val loss: 0.03615642059594393\n",
      "Epoch 28: train loss: 0.034999406775742606, val loss: 0.03183759283274412\n",
      "Epoch 29: train loss: 0.0313563106008962, val loss: 0.028274827636778355\n",
      "Epoch 30: train loss: 0.028042954854893554, val loss: 0.027349567972123623\n",
      "Epoch 31: train loss: 0.025476069062125524, val loss: 0.025318786036223173\n",
      "Epoch 32: train loss: 0.023369121007750293, val loss: 0.021639434155076742\n",
      "Epoch 33: train loss: 0.021931215176922907, val loss: 0.02081847982481122\n",
      "Epoch 34: train loss: 0.020352623711727445, val loss: 0.0206688498146832\n",
      "Epoch 35: train loss: 0.019786522169991456, val loss: 0.01940375054255128\n",
      "Epoch 36: train loss: 0.019016868844068505, val loss: 0.01917268056422472\n",
      "Epoch 37: train loss: 0.018697665423174303, val loss: 0.019122330471873283\n",
      "Epoch 38: train loss: 0.018564924725527003, val loss: 0.01916086208075285\n",
      "Epoch 39: train loss: 0.01823194727316202, val loss: 0.016092801932245493\n",
      "Epoch 40: train loss: 0.017968510812144192, val loss: 0.017987464554607868\n",
      "Epoch 41: train loss: 0.01790941208880912, val loss: 0.01741136284545064\n",
      "Epoch 42: train loss: 0.017791478878525457, val loss: 0.018699777778238058\n",
      "Epoch 43: train loss: 0.017599813419228984, val loss: 0.019203553907573223\n",
      "Epoch 44: train loss: 0.01755866313877107, val loss: 0.01731681078672409\n",
      "Epoch 00045: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 45: train loss: 0.017513460800665067, val loss: 0.017571180826053023\n",
      "Epoch 46: train loss: 0.01678083287670658, val loss: 0.01620093546807766\n",
      "Epoch 47: train loss: 0.016443276549672738, val loss: 0.017914763186126947\n",
      "Epoch 48: train loss: 0.016406752320346645, val loss: 0.01737270620651543\n",
      "Epoch 49: train loss: 0.016140746794292952, val loss: 0.018038279842585325\n",
      "Epoch 50: train loss: 0.016362286255868656, val loss: 0.016318807611241937\n",
      "Epoch 00051: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 51: train loss: 0.01620391993462426, val loss: 0.018163245171308517\n",
      "Epoch 52: train loss: 0.01605779885922001, val loss: 0.01722861686721444\n",
      "Epoch 53: train loss: 0.015840778214329405, val loss: 0.019134473521262407\n",
      "Epoch 54: train loss: 0.01575406206140708, val loss: 0.017860541585832834\n",
      "Epoch 55: train loss: 0.015871632320283675, val loss: 0.015357769094407558\n",
      "Epoch 56: train loss: 0.01579740911853621, val loss: 0.015852776123210788\n",
      "Epoch 57: train loss: 0.015952452637829625, val loss: 0.01635802211239934\n",
      "Epoch 58: train loss: 0.015842276333859547, val loss: 0.01754811708815396\n",
      "Epoch 59: train loss: 0.01575838681492968, val loss: 0.017115347553044558\n",
      "Epoch 60: train loss: 0.015797902578326736, val loss: 0.01749065099284053\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 61: train loss: 0.015687357851847817, val loss: 0.015754559775814414\n",
      "Epoch 62: train loss: 0.015492169481295361, val loss: 0.016606111777946353\n",
      "Epoch 63: train loss: 0.015585028952823106, val loss: 0.01729586534202099\n",
      "Epoch 64: train loss: 0.015420550631699377, val loss: 0.015625851461663842\n",
      "Epoch 65: train loss: 0.015472605954387129, val loss: 0.01677675638347864\n",
      "Epoch 66: train loss: 0.01573730471042861, val loss: 0.015369219705462456\n",
      "Epoch 00067: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 67: train loss: 0.01556983926631352, val loss: 0.016859472496435046\n",
      "Epoch 68: train loss: 0.015468265599309814, val loss: 0.016792410518974066\n",
      "Epoch 69: train loss: 0.015607195336472673, val loss: 0.014765082160010934\n",
      "Epoch 70: train loss: 0.01550088910406047, val loss: 0.01502303616143763\n",
      "Epoch 71: train loss: 0.015745931837132108, val loss: 0.01604775758460164\n",
      "Epoch 72: train loss: 0.015525683226372362, val loss: 0.015330820810049772\n",
      "Epoch 73: train loss: 0.015700938620345047, val loss: 0.016423190012574196\n",
      "Epoch 74: train loss: 0.01563964510985797, val loss: 0.016722839325666428\n",
      "Epoch 00075: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 75: train loss: 0.015482950467605366, val loss: 0.01616283878684044\n",
      "Epoch 76: train loss: 0.015600884639332785, val loss: 0.015658249612897635\n",
      "Epoch 77: train loss: 0.015098958017900269, val loss: 0.017631529364734888\n",
      "Epoch 78: train loss: 0.01552917328515607, val loss: 0.014849195722490549\n",
      "Epoch 79: train loss: 0.015331504455128942, val loss: 0.01566717168316245\n",
      "Epoch 80: train loss: 0.015442779596209799, val loss: 0.015735133783891797\n",
      "Epoch 81: train loss: 0.01513493426140472, val loss: 0.014560115290805697\n",
      "Epoch 82: train loss: 0.015644451760288666, val loss: 0.016711344942450523\n",
      "Epoch 83: train loss: 0.01535233112927727, val loss: 0.015247211558744311\n",
      "Epoch 84: train loss: 0.015355848634846165, val loss: 0.016783588333055377\n",
      "Epoch 85: train loss: 0.015419245361356866, val loss: 0.016275065951049328\n",
      "Epoch 86: train loss: 0.015398162139633811, val loss: 0.017215279396623373\n",
      "Epoch 00087: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 87: train loss: 0.015322965034610772, val loss: 0.015955439768731594\n",
      "Early stop at epoch 87\n"
     ]
    }
   ],
   "source": [
    "# model declaration\n",
    "nn_model = Net()\n",
    "nn_model.to(device)\n",
    "nn_model.train()\n",
    "    \n",
    "def train_nn():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(pretrained_x, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001, weight_decay=0.0001)    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_x_train = ae_model.encode(torch.tensor(x_train, dtype=torch.float).to(device))\n",
    "featured_x_train = nn_model.encode(featured_x_train).detach().cpu().numpy()\n",
    "# scaler = StandardScaler()\n",
    "# featured_x_train = scaler.fit_transform(featured_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class onelayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16, 1)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3255831335ae47f9a9164c8da6955957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: train loss: 3.647225856781006\n",
      "Epoch 200: train loss: 2.8522441387176514\n",
      "Epoch 300: train loss: 2.265383005142212\n",
      "Epoch 400: train loss: 1.838626503944397\n",
      "Epoch 500: train loss: 1.528767704963684\n",
      "Epoch 600: train loss: 1.2997362613677979\n",
      "Epoch 700: train loss: 1.1240910291671753\n",
      "Epoch 800: train loss: 0.9830479025840759\n",
      "Epoch 900: train loss: 0.865081787109375\n",
      "Epoch 1000: train loss: 0.7637797594070435\n",
      "Epoch 1100: train loss: 0.6757708787918091\n",
      "Epoch 1200: train loss: 0.5992141366004944\n",
      "Epoch 1300: train loss: 0.5329111218452454\n",
      "Epoch 1400: train loss: 0.4758740961551666\n",
      "Epoch 1500: train loss: 0.4271615743637085\n",
      "Epoch 1600: train loss: 0.38583385944366455\n",
      "Epoch 1700: train loss: 0.35095855593681335\n",
      "Epoch 1800: train loss: 0.32162806391716003\n",
      "Epoch 1900: train loss: 0.296980619430542\n",
      "Epoch 2000: train loss: 0.27621716260910034\n",
      "Epoch 2100: train loss: 0.25861501693725586\n",
      "Epoch 2200: train loss: 0.24353738129138947\n",
      "Epoch 2300: train loss: 0.23043735325336456\n",
      "Epoch 2400: train loss: 0.21885940432548523\n",
      "Epoch 2500: train loss: 0.2084348201751709\n",
      "Epoch 2600: train loss: 0.1988755315542221\n",
      "Epoch 2700: train loss: 0.18996448814868927\n",
      "Epoch 2800: train loss: 0.18154513835906982\n",
      "Epoch 2900: train loss: 0.17351043224334717\n",
      "Epoch 3000: train loss: 0.16579154133796692\n",
      "Epoch 3100: train loss: 0.15834906697273254\n",
      "Epoch 3200: train loss: 0.1511639505624771\n",
      "Epoch 3300: train loss: 0.14423136413097382\n",
      "Epoch 3400: train loss: 0.1375553160905838\n",
      "Epoch 3500: train loss: 0.131144717335701\n",
      "Epoch 3600: train loss: 0.12501081824302673\n",
      "Epoch 3700: train loss: 0.11916535347700119\n",
      "Epoch 3800: train loss: 0.11361924558877945\n",
      "Epoch 3900: train loss: 0.1083817183971405\n",
      "Epoch 4000: train loss: 0.1034599095582962\n",
      "Epoch 4100: train loss: 0.09885864704847336\n",
      "Epoch 4200: train loss: 0.09458008408546448\n",
      "Epoch 4300: train loss: 0.09062385559082031\n",
      "Epoch 4400: train loss: 0.08698680996894836\n",
      "Epoch 4500: train loss: 0.08366338908672333\n",
      "Epoch 4600: train loss: 0.08064554631710052\n",
      "Epoch 4700: train loss: 0.07792304456233978\n",
      "Epoch 4800: train loss: 0.07548332959413528\n",
      "Epoch 4900: train loss: 0.07331220805644989\n",
      "Epoch 5000: train loss: 0.07139381021261215\n",
      "Epoch 5100: train loss: 0.06971098482608795\n",
      "Epoch 5200: train loss: 0.06824559718370438\n",
      "Epoch 5300: train loss: 0.0669788271188736\n",
      "Epoch 5400: train loss: 0.0658915713429451\n",
      "Epoch 5500: train loss: 0.0649646520614624\n",
      "Epoch 5600: train loss: 0.06417927891016006\n",
      "Epoch 5700: train loss: 0.06351722776889801\n",
      "Epoch 5800: train loss: 0.06296117603778839\n",
      "Epoch 5900: train loss: 0.062494907528162\n",
      "Epoch 6000: train loss: 0.06210344284772873\n",
      "Epoch 6100: train loss: 0.061773259192705154\n",
      "Epoch 6200: train loss: 0.061492327600717545\n",
      "Epoch 6300: train loss: 0.061250101774930954\n",
      "Epoch 6400: train loss: 0.061037592589855194\n",
      "Epoch 6500: train loss: 0.060847196727991104\n",
      "Epoch 6600: train loss: 0.06067269295454025\n",
      "Epoch 6700: train loss: 0.060509052127599716\n",
      "Epoch 6800: train loss: 0.06035232916474342\n",
      "Epoch 6900: train loss: 0.06019949913024902\n",
      "Epoch 7000: train loss: 0.06004829332232475\n",
      "Epoch 7100: train loss: 0.059897102415561676\n",
      "Epoch 7200: train loss: 0.05974477529525757\n",
      "Epoch 7300: train loss: 0.05959058552980423\n",
      "Epoch 7400: train loss: 0.05943406745791435\n",
      "Epoch 7500: train loss: 0.059274982661008835\n",
      "Epoch 7600: train loss: 0.05911323428153992\n",
      "Epoch 7700: train loss: 0.05894884839653969\n",
      "Epoch 7800: train loss: 0.05878191813826561\n",
      "Epoch 7900: train loss: 0.058612603694200516\n",
      "Epoch 8000: train loss: 0.058441076427698135\n",
      "Epoch 8100: train loss: 0.05826758220791817\n",
      "Epoch 8200: train loss: 0.05809234455227852\n",
      "Epoch 8300: train loss: 0.05791565775871277\n",
      "Epoch 8400: train loss: 0.05773778259754181\n",
      "Epoch 8500: train loss: 0.057559020817279816\n",
      "Epoch 8600: train loss: 0.057379674166440964\n",
      "Epoch 8700: train loss: 0.057200077921152115\n",
      "Epoch 8800: train loss: 0.05702057108283043\n",
      "Epoch 8900: train loss: 0.056841474026441574\n",
      "Epoch 9000: train loss: 0.0566631443798542\n",
      "Epoch 9100: train loss: 0.056485917419195175\n",
      "Epoch 9200: train loss: 0.05631016567349434\n",
      "Epoch 9300: train loss: 0.05613621696829796\n",
      "Epoch 9400: train loss: 0.05596441030502319\n",
      "Epoch 9500: train loss: 0.05579511076211929\n",
      "Epoch 9600: train loss: 0.05562862381339073\n",
      "Epoch 9700: train loss: 0.055465277284383774\n",
      "Epoch 9800: train loss: 0.055305369198322296\n",
      "Epoch 9900: train loss: 0.05514919012784958\n",
      "Epoch 10000: train loss: 0.05499701201915741\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    # x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "one_model = get_regression_model(featured_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae-lr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999982968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "featured_x_train = ae_model.encode(torch.tensor(x_train, dtype=torch.float).to(device)).detach().cpu().numpy()\n",
    "lr = LinearRegression()\n",
    "lr.fit(featured_x_train, y_train)\n",
    "print(lr.score(featured_x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-ae-lr.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# featured_x_test = nn_model.encode(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device)))\n",
    "# featured_x_test = scaler.transform(featured_x_test.detach().cpu().numpy())\n",
    "# featured_x_test = torch.tensor(featured_x_test, dtype=torch.float).to(device)\n",
    "# y_pred = one_model(featured_x_test).squeeze(-1).detach().cpu().numpy()\n",
    "featured_x_test = ae_model.encode(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))\n",
    "y_pred = lr.predict(featured_x_test.detach().cpu().numpy())\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
