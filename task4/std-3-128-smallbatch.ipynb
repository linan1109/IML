{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_pretrain = scaler.transform(x_pretrain)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 256)\n",
    "        self.fc6 = nn.Linear(256, 128)\n",
    "        self.fc7 = nn.Linear(128, 64)\n",
    "        self.fc8 = nn.Linear(64, 1)\n",
    "\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "        nn.init.xavier_normal_(self.fc6.weight)\n",
    "        nn.init.xavier_normal_(self.fc7.weight)\n",
    "        nn.init.xavier_normal_(self.fc8.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc1 = nn.Linear(128, 64)\n",
    "            self.fc2 = nn.Linear(64, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-std-3-128-smallbatch.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876ef1f6d7f844b69954bbddccb2036b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.2625987841316632, val loss: 0.05352412757277489\n",
      "Epoch 2: train loss: 0.041118349831931446, val loss: 0.030246220111846925\n",
      "Epoch 3: train loss: 0.022570555646200568, val loss: 0.01906789383292198\n",
      "Epoch 4: train loss: 0.016902974290811286, val loss: 0.01704988967627287\n",
      "Epoch 5: train loss: 0.014900950730455165, val loss: 0.01796092203259468\n",
      "Epoch 6: train loss: 0.014766471628935969, val loss: 0.013808741204440594\n",
      "Epoch 7: train loss: 0.014635348415344345, val loss: 0.015781521677970888\n",
      "Epoch 8: train loss: 0.014455044816799309, val loss: 0.014000639252364635\n",
      "Epoch 9: train loss: 0.014025708252678112, val loss: 0.02092244955897331\n",
      "Epoch 10: train loss: 0.013507410859423025, val loss: 0.014035345688462257\n",
      "Epoch 11: train loss: 0.013411084550831998, val loss: 0.015268064357340336\n",
      "Epoch 00012: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 12: train loss: 0.013342080109429603, val loss: 0.01505603727698326\n",
      "Epoch 13: train loss: 0.00896723657968093, val loss: 0.008150315288454293\n",
      "Epoch 14: train loss: 0.007878939893476817, val loss: 0.0075541393607854845\n",
      "Epoch 15: train loss: 0.007474545179733208, val loss: 0.0070724252015352245\n",
      "Epoch 16: train loss: 0.007241841161813663, val loss: 0.008497293662279845\n",
      "Epoch 17: train loss: 0.007091143296993509, val loss: 0.007465872876346111\n",
      "Epoch 18: train loss: 0.006859880124427834, val loss: 0.006778296165168286\n",
      "Epoch 19: train loss: 0.0067855464889078725, val loss: 0.00732706817612052\n",
      "Epoch 20: train loss: 0.006745579061763627, val loss: 0.006757430549710989\n",
      "Epoch 21: train loss: 0.006659991191053877, val loss: 0.00737686251103878\n",
      "Epoch 22: train loss: 0.006558167258581641, val loss: 0.00649655931070447\n",
      "Epoch 23: train loss: 0.006439370261163127, val loss: 0.007490012962371111\n",
      "Epoch 24: train loss: 0.0064824784146431755, val loss: 0.006963797062635422\n",
      "Epoch 25: train loss: 0.0063942453011262175, val loss: 0.00645683104544878\n",
      "Epoch 26: train loss: 0.006415592632199429, val loss: 0.007206557653844357\n",
      "Epoch 27: train loss: 0.006394689186894316, val loss: 0.007006739735603332\n",
      "Epoch 28: train loss: 0.006314347866314406, val loss: 0.00608263685926795\n",
      "Epoch 29: train loss: 0.006264972861057946, val loss: 0.006611372336745262\n",
      "Epoch 30: train loss: 0.006324352665899359, val loss: 0.00656423320621252\n",
      "Epoch 31: train loss: 0.006197110782852586, val loss: 0.006701428074389696\n",
      "Epoch 32: train loss: 0.0061780484761966735, val loss: 0.006658911671489477\n",
      "Epoch 33: train loss: 0.0062488526623620064, val loss: 0.006260904852300882\n",
      "Epoch 00034: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 34: train loss: 0.006159637219595666, val loss: 0.006614874627441168\n",
      "Epoch 35: train loss: 0.0047578361990044315, val loss: 0.004960819881409406\n",
      "Epoch 36: train loss: 0.004232239042237705, val loss: 0.004970673024654388\n",
      "Epoch 37: train loss: 0.004149779283213524, val loss: 0.005067410912364722\n",
      "Epoch 38: train loss: 0.004051169836817651, val loss: 0.005006585142575204\n",
      "Epoch 39: train loss: 0.004064834185856946, val loss: 0.004622320048511029\n",
      "Epoch 40: train loss: 0.003951121958618869, val loss: 0.004772259514778852\n",
      "Epoch 41: train loss: 0.0038799055644063926, val loss: 0.004615696866065264\n",
      "Epoch 42: train loss: 0.0039032500033475913, val loss: 0.00500605496019125\n",
      "Epoch 43: train loss: 0.003898570098458048, val loss: 0.004742858750745654\n",
      "Epoch 44: train loss: 0.0038253413495436613, val loss: 0.0050758118107914925\n",
      "Epoch 45: train loss: 0.0037784024858657193, val loss: 0.004593262650072575\n",
      "Epoch 46: train loss: 0.003714965156792682, val loss: 0.004755233742296696\n",
      "Epoch 47: train loss: 0.003750392360346658, val loss: 0.005086119968444109\n",
      "Epoch 48: train loss: 0.003692270775990827, val loss: 0.004607914762571454\n",
      "Epoch 49: train loss: 0.0037296543987087754, val loss: 0.0050279578492045405\n",
      "Epoch 50: train loss: 0.0036257350025508477, val loss: 0.004590700821951032\n",
      "Epoch 51: train loss: 0.0036552475579843227, val loss: 0.00427060909382999\n",
      "Epoch 52: train loss: 0.003599264214651621, val loss: 0.00495584412664175\n",
      "Epoch 53: train loss: 0.0036361244331893266, val loss: 0.005016457978636027\n",
      "Epoch 54: train loss: 0.0035803603054765537, val loss: 0.005106932822614908\n",
      "Epoch 55: train loss: 0.0035840196147827164, val loss: 0.0047458609379827975\n",
      "Epoch 56: train loss: 0.003588834720815779, val loss: 0.004746940312907099\n",
      "Epoch 00057: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 57: train loss: 0.0035636758217398, val loss: 0.004451374813914299\n",
      "Epoch 58: train loss: 0.0030951649509180263, val loss: 0.004106802728027105\n",
      "Epoch 59: train loss: 0.0029018799853796256, val loss: 0.00383162409439683\n",
      "Epoch 60: train loss: 0.0028381339486175197, val loss: 0.004369134590029716\n",
      "Epoch 61: train loss: 0.002777243042668822, val loss: 0.0038225442171096803\n",
      "Epoch 62: train loss: 0.002753633611018256, val loss: 0.004130747412331402\n",
      "Epoch 63: train loss: 0.002717588645235008, val loss: 0.0037297163708135484\n",
      "Epoch 64: train loss: 0.002701464879801687, val loss: 0.004376604380086064\n",
      "Epoch 65: train loss: 0.0026782013955621085, val loss: 0.003736969918012619\n",
      "Epoch 66: train loss: 0.002623496612800019, val loss: 0.004264342661947012\n",
      "Epoch 67: train loss: 0.0025992875909622835, val loss: 0.003818872883915901\n",
      "Epoch 68: train loss: 0.0025700058369726247, val loss: 0.004136363405734301\n",
      "Epoch 69: train loss: 0.0025965242422356896, val loss: 0.0035431016236543655\n",
      "Epoch 70: train loss: 0.0025257211743796967, val loss: 0.003856859989464283\n",
      "Epoch 71: train loss: 0.0025365069405917, val loss: 0.0037273507378995418\n",
      "Epoch 72: train loss: 0.002524703940652235, val loss: 0.0037133336514234543\n",
      "Epoch 73: train loss: 0.002525340212547049, val loss: 0.0038630017787218093\n",
      "Epoch 74: train loss: 0.0024678488818890587, val loss: 0.004050788253545761\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 75: train loss: 0.002502173020466402, val loss: 0.0037284210063517095\n",
      "Epoch 76: train loss: 0.0023404040056927014, val loss: 0.00379060485959053\n",
      "Epoch 77: train loss: 0.0022740744622424245, val loss: 0.0034494582489132883\n",
      "Epoch 78: train loss: 0.002210482787359886, val loss: 0.0033286760849878193\n",
      "Epoch 79: train loss: 0.002202157857428704, val loss: 0.003482379022985697\n",
      "Epoch 80: train loss: 0.0021710121864262893, val loss: 0.0037247971724718807\n",
      "Epoch 81: train loss: 0.0021988953743320035, val loss: 0.0036320357210934164\n",
      "Epoch 82: train loss: 0.0021550403272505014, val loss: 0.0035289777517318725\n",
      "Epoch 83: train loss: 0.002136440072854867, val loss: 0.003522552395239472\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 84: train loss: 0.002140639844065418, val loss: 0.0034581359289586546\n",
      "Epoch 85: train loss: 0.002085187460618968, val loss: 0.0033361234776675703\n",
      "Epoch 86: train loss: 0.0020534233391208915, val loss: 0.003519174626097083\n",
      "Epoch 87: train loss: 0.002055895341370179, val loss: 0.0035619749333709477\n",
      "Epoch 88: train loss: 0.002024758702543165, val loss: 0.003411091677844524\n",
      "Epoch 89: train loss: 0.002061999436790998, val loss: 0.0037266083024442196\n",
      "Epoch 90: train loss: 0.0020255323977741812, val loss: 0.0032349613574333487\n",
      "Epoch 91: train loss: 0.00202210647029308, val loss: 0.003406998474150896\n",
      "Epoch 92: train loss: 0.0019776734308130585, val loss: 0.003498462997376919\n",
      "Epoch 93: train loss: 0.001984817900999012, val loss: 0.003352376889437437\n",
      "Epoch 94: train loss: 0.0019902051941548685, val loss: 0.0035186518244445324\n",
      "Epoch 95: train loss: 0.0019878996551911136, val loss: 0.003360404517501593\n",
      "Epoch 00096: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 96: train loss: 0.0019834724023215927, val loss: 0.0032372971549630165\n",
      "Early stop at epoch 96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57897bf9ad5644c9978e9a5e12ed2237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 2.518319606781006\n",
      "Epoch 20: train loss: 1.884042739868164\n",
      "Epoch 30: train loss: 1.1954892873764038\n",
      "Epoch 40: train loss: 0.6302603483200073\n",
      "Epoch 50: train loss: 0.3854168951511383\n",
      "Epoch 60: train loss: 0.26691901683807373\n",
      "Epoch 70: train loss: 0.16459307074546814\n",
      "Epoch 80: train loss: 0.11049054563045502\n",
      "Epoch 90: train loss: 0.08402150869369507\n",
      "Epoch 100: train loss: 0.07226655632257462\n",
      "Epoch 110: train loss: 0.06658436357975006\n",
      "Epoch 120: train loss: 0.06332142651081085\n",
      "Epoch 130: train loss: 0.06155552715063095\n",
      "Epoch 140: train loss: 0.06063281372189522\n",
      "Epoch 150: train loss: 0.05996851623058319\n",
      "Epoch 160: train loss: 0.059500496834516525\n",
      "Epoch 170: train loss: 0.059194087982177734\n",
      "Epoch 180: train loss: 0.05900268256664276\n",
      "Epoch 190: train loss: 0.05881177634000778\n",
      "Epoch 200: train loss: 0.058639008551836014\n",
      "Epoch 210: train loss: 0.05847782641649246\n",
      "Epoch 220: train loss: 0.05831592530012131\n",
      "Epoch 230: train loss: 0.058161020278930664\n",
      "Epoch 240: train loss: 0.05801581218838692\n",
      "Epoch 250: train loss: 0.05788101628422737\n",
      "Epoch 260: train loss: 0.05776861682534218\n",
      "Epoch 270: train loss: 0.05768398195505142\n",
      "Epoch 280: train loss: 0.05759873613715172\n",
      "Epoch 290: train loss: 0.05752701684832573\n",
      "Epoch 300: train loss: 0.057454340159893036\n",
      "Epoch 310: train loss: 0.057387880980968475\n",
      "Epoch 320: train loss: 0.05732062831521034\n",
      "Epoch 330: train loss: 0.05724091827869415\n",
      "Epoch 340: train loss: 0.05718303471803665\n",
      "Epoch 350: train loss: 0.05712500587105751\n",
      "Epoch 360: train loss: 0.05707032233476639\n",
      "Epoch 370: train loss: 0.057022225111722946\n",
      "Epoch 380: train loss: 0.05697500705718994\n",
      "Epoch 390: train loss: 0.0569259449839592\n",
      "Epoch 400: train loss: 0.05688812956213951\n",
      "Epoch 410: train loss: 0.05687316879630089\n",
      "Epoch 420: train loss: 0.05681346356868744\n",
      "Epoch 430: train loss: 0.056776970624923706\n",
      "Epoch 440: train loss: 0.05672904849052429\n",
      "Epoch 450: train loss: 0.05668535456061363\n",
      "Epoch 460: train loss: 0.05666772648692131\n",
      "Epoch 470: train loss: 0.056607142090797424\n",
      "Epoch 480: train loss: 0.05659469589591026\n",
      "Epoch 490: train loss: 0.05654992535710335\n",
      "Epoch 500: train loss: 0.056510619819164276\n",
      "Epoch 510: train loss: 0.05647588148713112\n",
      "Epoch 520: train loss: 0.05644768103957176\n",
      "Epoch 530: train loss: 0.05640270933508873\n",
      "Epoch 540: train loss: 0.05638353154063225\n",
      "Epoch 00549: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 550: train loss: 0.05635073408484459\n",
      "Epoch 560: train loss: 0.05633028224110603\n",
      "Epoch 00561: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 00568: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 570: train loss: 0.05631464719772339\n",
      "Epoch 00579: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 580: train loss: 0.056311096996068954\n",
      "Epoch 00585: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 590: train loss: 0.056310322135686874\n",
      "Epoch 00591: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 00597: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 600: train loss: 0.05630985274910927\n",
      "Epoch 00603: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 603, loss: 0.05630979314446449\n",
      "Predictions saved to results-std-3-128-smallbatch.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain, batch_size=32)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
