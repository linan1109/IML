{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-mm-1-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040408d4140d4792997cb4e129798c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.49257847465057764, val loss: 0.14317791998386384\n",
      "Epoch 2: train loss: 0.13635290726350277, val loss: 0.11798315453529358\n",
      "Epoch 3: train loss: 0.11850129731942197, val loss: 0.11595095205307007\n",
      "Epoch 4: train loss: 0.10760862651406503, val loss: 0.10729715186357498\n",
      "Epoch 5: train loss: 0.10164029089771971, val loss: 0.10621762508153916\n",
      "Epoch 6: train loss: 0.08885675914190254, val loss: 0.09259597218036651\n",
      "Epoch 7: train loss: 0.08252354934264203, val loss: 0.07464068818092347\n",
      "Epoch 8: train loss: 0.0739320835629288, val loss: 0.06986611187458039\n",
      "Epoch 9: train loss: 0.06720745329832545, val loss: 0.06062476634979248\n",
      "Epoch 10: train loss: 0.05994247107724754, val loss: 0.05702410027384758\n",
      "Epoch 11: train loss: 0.053684965072845925, val loss: 0.049988573849201204\n",
      "Epoch 12: train loss: 0.046816416672297884, val loss: 0.04539820459485054\n",
      "Epoch 13: train loss: 0.040394120841001975, val loss: 0.03236108975112438\n",
      "Epoch 14: train loss: 0.03467203601951502, val loss: 0.0318940018415451\n",
      "Epoch 15: train loss: 0.029112021481504245, val loss: 0.026410316467285155\n",
      "Epoch 16: train loss: 0.023199078245430577, val loss: 0.01998778213560581\n",
      "Epoch 17: train loss: 0.018015977178605234, val loss: 0.015738834112882613\n",
      "Epoch 18: train loss: 0.013640620768526379, val loss: 0.013432488732039928\n",
      "Epoch 19: train loss: 0.010474737942370834, val loss: 0.00991405788809061\n",
      "Epoch 20: train loss: 0.008220029016462516, val loss: 0.007554029550403357\n",
      "Epoch 21: train loss: 0.006780881815418905, val loss: 0.00666559050232172\n",
      "Epoch 22: train loss: 0.005842766130533145, val loss: 0.006583115827292204\n",
      "Epoch 23: train loss: 0.005340754136823269, val loss: 0.005899599798023701\n",
      "Epoch 24: train loss: 0.005043807857270752, val loss: 0.005956635903567075\n",
      "Epoch 25: train loss: 0.005011299680158192, val loss: 0.0052875221408903595\n",
      "Epoch 26: train loss: 0.004976699919192766, val loss: 0.005354331273585558\n",
      "Epoch 27: train loss: 0.0047494883353308755, val loss: 0.005158110573887825\n",
      "Epoch 28: train loss: 0.004747577165082401, val loss: 0.005847109775990247\n",
      "Epoch 29: train loss: 0.004658656041917144, val loss: 0.005091513488441706\n",
      "Epoch 30: train loss: 0.004642876137960322, val loss: 0.004851682674139738\n",
      "Epoch 31: train loss: 0.004578598205836452, val loss: 0.0057539108730852604\n",
      "Epoch 32: train loss: 0.0046379573834793904, val loss: 0.004772997088730335\n",
      "Epoch 33: train loss: 0.004593419185402442, val loss: 0.004849151838570833\n",
      "Epoch 34: train loss: 0.0045943906450149966, val loss: 0.005429397258907557\n",
      "Epoch 35: train loss: 0.004661969600967607, val loss: 0.005115976545959711\n",
      "Epoch 36: train loss: 0.004577295450227601, val loss: 0.005849706839770079\n",
      "Epoch 37: train loss: 0.004726284199299253, val loss: 0.00536097351461649\n",
      "Epoch 00038: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 38: train loss: 0.004650673370808363, val loss: 0.005047756470739842\n",
      "Epoch 39: train loss: 0.003483280118896949, val loss: 0.003835859563201666\n",
      "Epoch 40: train loss: 0.0030176504231153095, val loss: 0.004029397362843156\n",
      "Epoch 41: train loss: 0.0030078401969631714, val loss: 0.003413723470643163\n",
      "Epoch 42: train loss: 0.002991985334805688, val loss: 0.003977738561108709\n",
      "Epoch 43: train loss: 0.003009488292798704, val loss: 0.003637775843963027\n",
      "Epoch 44: train loss: 0.003006038099337293, val loss: 0.003677461141720414\n",
      "Epoch 45: train loss: 0.0030256585748676137, val loss: 0.0035743921734392643\n",
      "Epoch 46: train loss: 0.0029864682371024878, val loss: 0.0041148433070629835\n",
      "Epoch 47: train loss: 0.0030299750240329578, val loss: 0.00329858716391027\n",
      "Epoch 48: train loss: 0.0029467165288040225, val loss: 0.003696389876306057\n",
      "Epoch 49: train loss: 0.002985014596687896, val loss: 0.003604974934831262\n",
      "Epoch 50: train loss: 0.002965770514643922, val loss: 0.0037767587378621102\n",
      "Epoch 51: train loss: 0.0029011832013422128, val loss: 0.003699752377346158\n",
      "Epoch 52: train loss: 0.0029668929200923565, val loss: 0.004133069112896919\n",
      "Epoch 00053: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 53: train loss: 0.002979207807923762, val loss: 0.003676312666386366\n",
      "Epoch 54: train loss: 0.0024433862427059483, val loss: 0.0031173578631132843\n",
      "Epoch 55: train loss: 0.002240066130284448, val loss: 0.0031192378904670477\n",
      "Epoch 56: train loss: 0.002188663343613853, val loss: 0.003160643858835101\n",
      "Epoch 57: train loss: 0.0021855612268899473, val loss: 0.0030257202815264462\n",
      "Epoch 58: train loss: 0.002172503125606751, val loss: 0.0032239490188658237\n",
      "Epoch 59: train loss: 0.0021694467229654594, val loss: 0.0033416806254535913\n",
      "Epoch 60: train loss: 0.0021438459768055047, val loss: 0.003026162611320615\n",
      "Epoch 61: train loss: 0.0021159552934826636, val loss: 0.003055120214819908\n",
      "Epoch 62: train loss: 0.0021230084500370584, val loss: 0.002774990923702717\n",
      "Epoch 63: train loss: 0.002145409280928422, val loss: 0.0027727013006806373\n",
      "Epoch 64: train loss: 0.002086676318236456, val loss: 0.003417638571932912\n",
      "Epoch 65: train loss: 0.0021009741415820866, val loss: 0.0030373697243630884\n",
      "Epoch 66: train loss: 0.0021002243726837393, val loss: 0.0031085243690758945\n",
      "Epoch 67: train loss: 0.0020819099137977678, val loss: 0.0032985342498868704\n",
      "Epoch 68: train loss: 0.0020835594171848223, val loss: 0.003117164747789502\n",
      "Epoch 00069: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 69: train loss: 0.002079209272160518, val loss: 0.0030594481583684683\n",
      "Epoch 70: train loss: 0.0019406562345672628, val loss: 0.00319805028103292\n",
      "Epoch 71: train loss: 0.001874610554997106, val loss: 0.0029706995096057654\n",
      "Epoch 72: train loss: 0.0018357074066844522, val loss: 0.002926443871110678\n",
      "Epoch 73: train loss: 0.001857697326214794, val loss: 0.002870224302634597\n",
      "Epoch 74: train loss: 0.0018255919194495192, val loss: 0.002827758735045791\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 75: train loss: 0.0017785196578586284, val loss: 0.0030000727009028196\n",
      "Epoch 76: train loss: 0.0017638170495819377, val loss: 0.003000606210902333\n",
      "Epoch 77: train loss: 0.001745919977276757, val loss: 0.002823237646371126\n",
      "Epoch 78: train loss: 0.001710883640665181, val loss: 0.0029461904540657997\n",
      "Epoch 79: train loss: 0.0017420782978095266, val loss: 0.0026751869190484285\n",
      "Epoch 80: train loss: 0.0017246686612655009, val loss: 0.0028626714665442705\n",
      "Epoch 81: train loss: 0.001700897590960471, val loss: 0.0029235001634806395\n",
      "Epoch 82: train loss: 0.0017249370122290387, val loss: 0.00294114744476974\n",
      "Epoch 83: train loss: 0.0017209429785001035, val loss: 0.002772876553237438\n",
      "Epoch 84: train loss: 0.0017191397402131436, val loss: 0.0028425253201276065\n",
      "Epoch 00085: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 85: train loss: 0.0017214893141229237, val loss: 0.0027382873930037023\n",
      "Epoch 86: train loss: 0.0016999907701143197, val loss: 0.00299115789309144\n",
      "Epoch 87: train loss: 0.001684502456774365, val loss: 0.0029077468067407607\n",
      "Epoch 88: train loss: 0.0017073721419449668, val loss: 0.003083686087280512\n",
      "Epoch 89: train loss: 0.00168387988985193, val loss: 0.0030269033052027225\n",
      "Epoch 90: train loss: 0.0016804107543917336, val loss: 0.0030341654513031243\n",
      "Epoch 00091: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 91: train loss: 0.0016908847929111548, val loss: 0.003087354341521859\n",
      "Early stop at epoch 91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828291f27a1341a3958e3e12ffee0ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 5.013541221618652\n",
      "Epoch 20: train loss: 4.761620998382568\n",
      "Epoch 30: train loss: 4.5182037353515625\n",
      "Epoch 40: train loss: 4.283757209777832\n",
      "Epoch 50: train loss: 4.058452606201172\n",
      "Epoch 60: train loss: 3.842261791229248\n",
      "Epoch 70: train loss: 3.6350481510162354\n",
      "Epoch 80: train loss: 3.436619520187378\n",
      "Epoch 90: train loss: 3.2467617988586426\n",
      "Epoch 100: train loss: 3.0652482509613037\n",
      "Epoch 110: train loss: 2.8918511867523193\n",
      "Epoch 120: train loss: 2.7263388633728027\n",
      "Epoch 130: train loss: 2.568481683731079\n",
      "Epoch 140: train loss: 2.418048858642578\n",
      "Epoch 150: train loss: 2.274813413619995\n",
      "Epoch 160: train loss: 2.138546943664551\n",
      "Epoch 170: train loss: 2.009024143218994\n",
      "Epoch 180: train loss: 1.8860212564468384\n",
      "Epoch 190: train loss: 1.769315481185913\n",
      "Epoch 200: train loss: 1.6586861610412598\n",
      "Epoch 210: train loss: 1.553916335105896\n",
      "Epoch 220: train loss: 1.4547899961471558\n",
      "Epoch 230: train loss: 1.361094355583191\n",
      "Epoch 240: train loss: 1.2726194858551025\n",
      "Epoch 250: train loss: 1.189157247543335\n",
      "Epoch 260: train loss: 1.110503911972046\n",
      "Epoch 270: train loss: 1.0364595651626587\n",
      "Epoch 280: train loss: 0.9668263792991638\n",
      "Epoch 290: train loss: 0.9014112949371338\n",
      "Epoch 300: train loss: 0.8400248289108276\n",
      "Epoch 310: train loss: 0.7824816703796387\n",
      "Epoch 320: train loss: 0.7286006808280945\n",
      "Epoch 330: train loss: 0.6782047748565674\n",
      "Epoch 340: train loss: 0.6311220526695251\n",
      "Epoch 350: train loss: 0.5871851444244385\n",
      "Epoch 360: train loss: 0.5462309718132019\n",
      "Epoch 370: train loss: 0.5081015825271606\n",
      "Epoch 380: train loss: 0.47264334559440613\n",
      "Epoch 390: train loss: 0.4397089183330536\n",
      "Epoch 400: train loss: 0.40915435552597046\n",
      "Epoch 410: train loss: 0.38084226846694946\n",
      "Epoch 420: train loss: 0.35463979840278625\n",
      "Epoch 430: train loss: 0.3304193913936615\n",
      "Epoch 440: train loss: 0.3080579936504364\n",
      "Epoch 450: train loss: 0.2874388098716736\n",
      "Epoch 460: train loss: 0.2684493660926819\n",
      "Epoch 470: train loss: 0.2509826123714447\n",
      "Epoch 480: train loss: 0.23493610322475433\n",
      "Epoch 490: train loss: 0.22021295130252838\n",
      "Epoch 500: train loss: 0.20672059059143066\n",
      "Epoch 510: train loss: 0.1943715661764145\n",
      "Epoch 520: train loss: 0.18308278918266296\n",
      "Epoch 530: train loss: 0.17277587950229645\n",
      "Epoch 540: train loss: 0.1633770763874054\n",
      "Epoch 550: train loss: 0.15481676161289215\n",
      "Epoch 560: train loss: 0.14702945947647095\n",
      "Epoch 570: train loss: 0.13995376229286194\n",
      "Epoch 580: train loss: 0.13353225588798523\n",
      "Epoch 590: train loss: 0.12771104276180267\n",
      "Epoch 600: train loss: 0.12243999540805817\n",
      "Epoch 610: train loss: 0.11767236143350601\n",
      "Epoch 620: train loss: 0.11336461454629898\n",
      "Epoch 630: train loss: 0.10947637259960175\n",
      "Epoch 640: train loss: 0.10597031563520432\n",
      "Epoch 650: train loss: 0.10281167924404144\n",
      "Epoch 660: train loss: 0.09996861964464188\n",
      "Epoch 670: train loss: 0.0974116325378418\n",
      "Epoch 680: train loss: 0.09511356800794601\n",
      "Epoch 690: train loss: 0.0930495634675026\n",
      "Epoch 700: train loss: 0.09119672328233719\n",
      "Epoch 710: train loss: 0.08953425288200378\n",
      "Epoch 720: train loss: 0.0880429595708847\n",
      "Epoch 730: train loss: 0.08670549094676971\n",
      "Epoch 740: train loss: 0.08550594002008438\n",
      "Epoch 750: train loss: 0.08442991226911545\n",
      "Epoch 760: train loss: 0.0834643542766571\n",
      "Epoch 770: train loss: 0.08259738981723785\n",
      "Epoch 780: train loss: 0.08181831240653992\n",
      "Epoch 790: train loss: 0.08111749589443207\n",
      "Epoch 800: train loss: 0.08048615604639053\n",
      "Epoch 810: train loss: 0.07991645485162735\n",
      "Epoch 820: train loss: 0.0794014260172844\n",
      "Epoch 830: train loss: 0.07893471419811249\n",
      "Epoch 840: train loss: 0.07851067185401917\n",
      "Epoch 850: train loss: 0.07812431454658508\n",
      "Epoch 860: train loss: 0.07777109742164612\n",
      "Epoch 870: train loss: 0.07744701206684113\n",
      "Epoch 880: train loss: 0.07714851200580597\n",
      "Epoch 890: train loss: 0.07687246054410934\n",
      "Epoch 900: train loss: 0.076616071164608\n",
      "Epoch 910: train loss: 0.07637681812047958\n",
      "Epoch 920: train loss: 0.07615254819393158\n",
      "Epoch 930: train loss: 0.07594136148691177\n",
      "Epoch 940: train loss: 0.07574153691530228\n",
      "Epoch 950: train loss: 0.07555162906646729\n",
      "Epoch 960: train loss: 0.07537031173706055\n",
      "Epoch 970: train loss: 0.07519645243883133\n",
      "Epoch 980: train loss: 0.07502907514572144\n",
      "Epoch 990: train loss: 0.07486730068922043\n",
      "Epoch 1000: train loss: 0.07471039891242981\n",
      "Epoch 1010: train loss: 0.07455769181251526\n",
      "Epoch 1020: train loss: 0.0744086280465126\n",
      "Epoch 1030: train loss: 0.07426270097494125\n",
      "Epoch 1040: train loss: 0.07411950826644897\n",
      "Epoch 1050: train loss: 0.0739787146449089\n",
      "Epoch 1060: train loss: 0.07383996993303299\n",
      "Epoch 1070: train loss: 0.07370301336050034\n",
      "Epoch 1080: train loss: 0.07356764376163483\n",
      "Epoch 1090: train loss: 0.07343364506959915\n",
      "Epoch 1100: train loss: 0.07330085337162018\n",
      "Epoch 1110: train loss: 0.07316913455724716\n",
      "Epoch 1120: train loss: 0.07303838431835175\n",
      "Epoch 1130: train loss: 0.07290849834680557\n",
      "Epoch 1140: train loss: 0.07277937978506088\n",
      "Epoch 1150: train loss: 0.0726509764790535\n",
      "Epoch 1160: train loss: 0.07252322882413864\n",
      "Epoch 1170: train loss: 0.07239610701799393\n",
      "Epoch 1180: train loss: 0.07226956635713577\n",
      "Epoch 1190: train loss: 0.0721435546875\n",
      "Epoch 1200: train loss: 0.07201806455850601\n",
      "Epoch 1210: train loss: 0.07189308851957321\n",
      "Epoch 1220: train loss: 0.071768619120121\n",
      "Epoch 1230: train loss: 0.0716446116566658\n",
      "Epoch 1240: train loss: 0.07152111083269119\n",
      "Epoch 1250: train loss: 0.0713980570435524\n",
      "Epoch 1260: train loss: 0.0712754875421524\n",
      "Epoch 1270: train loss: 0.07115339487791061\n",
      "Epoch 1280: train loss: 0.07103178650140762\n",
      "Epoch 1290: train loss: 0.07091065496206284\n",
      "Epoch 1300: train loss: 0.07078998535871506\n",
      "Epoch 1310: train loss: 0.07066984474658966\n",
      "Epoch 1320: train loss: 0.07055017352104187\n",
      "Epoch 1330: train loss: 0.07043100148439407\n",
      "Epoch 1340: train loss: 0.07031235098838806\n",
      "Epoch 1350: train loss: 0.07019420713186264\n",
      "Epoch 1360: train loss: 0.070076584815979\n",
      "Epoch 1370: train loss: 0.06995950639247894\n",
      "Epoch 1380: train loss: 0.06984296441078186\n",
      "Epoch 1390: train loss: 0.06972696632146835\n",
      "Epoch 1400: train loss: 0.06961151957511902\n",
      "Epoch 1410: train loss: 0.06949663907289505\n",
      "Epoch 1420: train loss: 0.06938232481479645\n",
      "Epoch 1430: train loss: 0.0692686066031456\n",
      "Epoch 1440: train loss: 0.06915544718503952\n",
      "Epoch 1450: train loss: 0.06904286891222\n",
      "Epoch 1460: train loss: 0.06893091648817062\n",
      "Epoch 1470: train loss: 0.0688195675611496\n",
      "Epoch 1480: train loss: 0.06870880722999573\n",
      "Epoch 1490: train loss: 0.06859869509935379\n",
      "Epoch 1500: train loss: 0.0684892013669014\n",
      "Epoch 1510: train loss: 0.06838034838438034\n",
      "Epoch 1520: train loss: 0.06827211380004883\n",
      "Epoch 1530: train loss: 0.06816454976797104\n",
      "Epoch 1540: train loss: 0.06805761158466339\n",
      "Epoch 1550: train loss: 0.06795135140419006\n",
      "Epoch 1560: train loss: 0.06784574687480927\n",
      "Epoch 1570: train loss: 0.06774081289768219\n",
      "Epoch 1580: train loss: 0.06763655692338943\n",
      "Epoch 1590: train loss: 0.0675329864025116\n",
      "Epoch 1600: train loss: 0.06743010133504868\n",
      "Epoch 1610: train loss: 0.06732790172100067\n",
      "Epoch 1620: train loss: 0.06722639501094818\n",
      "Epoch 1630: train loss: 0.0671255886554718\n",
      "Epoch 1640: train loss: 0.06702549010515213\n",
      "Epoch 1650: train loss: 0.06692609935998917\n",
      "Epoch 1660: train loss: 0.06682741641998291\n",
      "Epoch 1670: train loss: 0.06672943383455276\n",
      "Epoch 1680: train loss: 0.0666322112083435\n",
      "Epoch 1690: train loss: 0.06653568893671036\n",
      "Epoch 1700: train loss: 0.0664399042725563\n",
      "Epoch 1710: train loss: 0.06634485721588135\n",
      "Epoch 1720: train loss: 0.0662505254149437\n",
      "Epoch 1730: train loss: 0.06615694612264633\n",
      "Epoch 1740: train loss: 0.06606409698724747\n",
      "Epoch 1750: train loss: 0.06597200036048889\n",
      "Epoch 1760: train loss: 0.0658806562423706\n",
      "Epoch 1770: train loss: 0.06579004228115082\n",
      "Epoch 1780: train loss: 0.06570018827915192\n",
      "Epoch 1790: train loss: 0.06561107933521271\n",
      "Epoch 1800: train loss: 0.06552274525165558\n",
      "Epoch 1810: train loss: 0.06543514877557755\n",
      "Epoch 1820: train loss: 0.065348319709301\n",
      "Epoch 1830: train loss: 0.06526224315166473\n",
      "Epoch 1840: train loss: 0.06517694145441055\n",
      "Epoch 1850: train loss: 0.06509236991405487\n",
      "Epoch 1860: train loss: 0.06500858813524246\n",
      "Epoch 1870: train loss: 0.06492556631565094\n",
      "Epoch 1880: train loss: 0.0648433119058609\n",
      "Epoch 1890: train loss: 0.06476182490587234\n",
      "Epoch 1900: train loss: 0.06468110531568527\n",
      "Epoch 1910: train loss: 0.06460114568471909\n",
      "Epoch 1920: train loss: 0.06452195346355438\n",
      "Epoch 1930: train loss: 0.06444352120161057\n",
      "Epoch 1940: train loss: 0.06436586380004883\n",
      "Epoch 1950: train loss: 0.06428896635770798\n",
      "Epoch 1960: train loss: 0.0642128437757492\n",
      "Epoch 1970: train loss: 0.06413749605417252\n",
      "Epoch 1980: train loss: 0.06406289339065552\n",
      "Epoch 1990: train loss: 0.06398905813694\n",
      "Epoch 2000: train loss: 0.06391599774360657\n",
      "Epoch 2010: train loss: 0.06384368985891342\n",
      "Epoch 2020: train loss: 0.06377213448286057\n",
      "Epoch 2030: train loss: 0.06370135396718979\n",
      "Epoch 2040: train loss: 0.0636313259601593\n",
      "Epoch 2050: train loss: 0.0635620579123497\n",
      "Epoch 2060: train loss: 0.06349354237318039\n",
      "Epoch 2070: train loss: 0.06342577189207077\n",
      "Epoch 2080: train loss: 0.06335877627134323\n",
      "Epoch 2090: train loss: 0.06329251080751419\n",
      "Epoch 2100: train loss: 0.06322699040174484\n",
      "Epoch 2110: train loss: 0.06316220760345459\n",
      "Epoch 2120: train loss: 0.06309819966554642\n",
      "Epoch 2130: train loss: 0.06303490698337555\n",
      "Epoch 2140: train loss: 0.06297235935926437\n",
      "Epoch 2150: train loss: 0.0629105493426323\n",
      "Epoch 2160: train loss: 0.06284946203231812\n",
      "Epoch 2170: train loss: 0.06278910487890244\n",
      "Epoch 2180: train loss: 0.06272949278354645\n",
      "Epoch 2190: train loss: 0.06267058104276657\n",
      "Epoch 2200: train loss: 0.06261239945888519\n",
      "Epoch 2210: train loss: 0.06255493313074112\n",
      "Epoch 2220: train loss: 0.06249817833304405\n",
      "Epoch 2230: train loss: 0.06244213134050369\n",
      "Epoch 2240: train loss: 0.06238679215312004\n",
      "Epoch 2250: train loss: 0.0623321533203125\n",
      "Epoch 2260: train loss: 0.06227821111679077\n",
      "Epoch 2270: train loss: 0.062224969267845154\n",
      "Epoch 2280: train loss: 0.062172405421733856\n",
      "Epoch 2290: train loss: 0.06212054193019867\n",
      "Epoch 2300: train loss: 0.062069352716207504\n",
      "Epoch 2310: train loss: 0.062018852680921555\n",
      "Epoch 2320: train loss: 0.06196901202201843\n",
      "Epoch 2330: train loss: 0.06191984936594963\n",
      "Epoch 2340: train loss: 0.06187134608626366\n",
      "Epoch 2350: train loss: 0.061823520809412\n",
      "Epoch 2360: train loss: 0.06177634000778198\n",
      "Epoch 2370: train loss: 0.0617297999560833\n",
      "Epoch 2380: train loss: 0.06168392673134804\n",
      "Epoch 2390: train loss: 0.061638668179512024\n",
      "Epoch 2400: train loss: 0.061594076454639435\n",
      "Epoch 2410: train loss: 0.06155009940266609\n",
      "Epoch 2420: train loss: 0.061506737023591995\n",
      "Epoch 2430: train loss: 0.06146402284502983\n",
      "Epoch 2440: train loss: 0.06142192706465721\n",
      "Epoch 2450: train loss: 0.06138042360544205\n",
      "Epoch 2460: train loss: 0.06133953109383583\n",
      "Epoch 2470: train loss: 0.06129923835396767\n",
      "Epoch 2480: train loss: 0.061259567737579346\n",
      "Epoch 2490: train loss: 0.061220452189445496\n",
      "Epoch 2500: train loss: 0.061181943863630295\n",
      "Epoch 2510: train loss: 0.06114400550723076\n",
      "Epoch 2520: train loss: 0.06110665574669838\n",
      "Epoch 2530: train loss: 0.06106986477971077\n",
      "Epoch 2540: train loss: 0.06103362888097763\n",
      "Epoch 2550: train loss: 0.060997962951660156\n",
      "Epoch 2560: train loss: 0.06096285209059715\n",
      "Epoch 2570: train loss: 0.06092827767133713\n",
      "Epoch 2580: train loss: 0.060894258320331573\n",
      "Epoch 2590: train loss: 0.06086078658699989\n",
      "Epoch 2600: train loss: 0.06082779914140701\n",
      "Epoch 2610: train loss: 0.060795363038778305\n",
      "Epoch 2620: train loss: 0.06076344475150108\n",
      "Epoch 2630: train loss: 0.06073204055428505\n",
      "Epoch 2640: train loss: 0.06070113927125931\n",
      "Epoch 2650: train loss: 0.06067075580358505\n",
      "Epoch 2660: train loss: 0.060640849173069\n",
      "Epoch 2670: train loss: 0.06061143800616264\n",
      "Epoch 2680: train loss: 0.060582492500543594\n",
      "Epoch 2690: train loss: 0.06055404990911484\n",
      "Epoch 2700: train loss: 0.06052606925368309\n",
      "Epoch 2710: train loss: 0.06049855425953865\n",
      "Epoch 2720: train loss: 0.06047150865197182\n",
      "Epoch 2730: train loss: 0.060444917529821396\n",
      "Epoch 2740: train loss: 0.060418758541345596\n",
      "Epoch 2750: train loss: 0.06039305403828621\n",
      "Epoch 2760: train loss: 0.06036778911948204\n",
      "Epoch 2770: train loss: 0.06034296005964279\n",
      "Epoch 2780: train loss: 0.060318537056446075\n",
      "Epoch 2790: train loss: 0.06029454246163368\n",
      "Epoch 2800: train loss: 0.06027097627520561\n",
      "Epoch 2810: train loss: 0.060247790068387985\n",
      "Epoch 2820: train loss: 0.06022502854466438\n",
      "Epoch 2830: train loss: 0.06020265445113182\n",
      "Epoch 2840: train loss: 0.06018068268895149\n",
      "Epoch 2850: train loss: 0.06015907973051071\n",
      "Epoch 2860: train loss: 0.06013786047697067\n",
      "Epoch 2870: train loss: 0.06011701375246048\n",
      "Epoch 2880: train loss: 0.060096535831689835\n",
      "Epoch 2890: train loss: 0.06007642671465874\n",
      "Epoch 2900: train loss: 0.060056667774915695\n",
      "Epoch 2910: train loss: 0.060037266463041306\n",
      "Epoch 2920: train loss: 0.06001820042729378\n",
      "Epoch 2930: train loss: 0.05999947339296341\n",
      "Epoch 2940: train loss: 0.0599810928106308\n",
      "Epoch 2950: train loss: 0.05996304005384445\n",
      "Epoch 2960: train loss: 0.059945303946733475\n",
      "Epoch 2970: train loss: 0.059927888214588165\n",
      "Epoch 2980: train loss: 0.059910792857408524\n",
      "Epoch 2990: train loss: 0.05989398807287216\n",
      "Epoch 3000: train loss: 0.059877488762140274\n",
      "Epoch 3010: train loss: 0.05986129119992256\n",
      "Epoch 3020: train loss: 0.05984538421034813\n",
      "Epoch 3030: train loss: 0.05982976779341698\n",
      "Epoch 3040: train loss: 0.05981443449854851\n",
      "Epoch 3050: train loss: 0.059799354523420334\n",
      "Epoch 3060: train loss: 0.05978456512093544\n",
      "Epoch 3070: train loss: 0.05977002903819084\n",
      "Epoch 3080: train loss: 0.05975576117634773\n",
      "Epoch 3090: train loss: 0.05974176153540611\n",
      "Epoch 3100: train loss: 0.0597279816865921\n",
      "Epoch 3110: train loss: 0.05971447005867958\n",
      "Epoch 3120: train loss: 0.05970119312405586\n",
      "Epoch 3130: train loss: 0.05968815088272095\n",
      "Epoch 3140: train loss: 0.059675347059965134\n",
      "Epoch 3150: train loss: 0.05966275930404663\n",
      "Epoch 3160: train loss: 0.05965040251612663\n",
      "Epoch 3170: train loss: 0.05963825061917305\n",
      "Epoch 3180: train loss: 0.05962632596492767\n",
      "Epoch 3190: train loss: 0.05961460992693901\n",
      "Epoch 3200: train loss: 0.05960308760404587\n",
      "Epoch 3210: train loss: 0.05959177017211914\n",
      "Epoch 3220: train loss: 0.05958065018057823\n",
      "Epoch 3230: train loss: 0.05956970527768135\n",
      "Epoch 3240: train loss: 0.05955896154046059\n",
      "Epoch 3250: train loss: 0.05954840034246445\n",
      "Epoch 3260: train loss: 0.059538014233112335\n",
      "Epoch 3270: train loss: 0.05952780693769455\n",
      "Epoch 3280: train loss: 0.059517763555049896\n",
      "Epoch 03289: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 3290: train loss: 0.05950788035988808\n",
      "Epoch 03296: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 3300: train loss: 0.05950557067990303\n",
      "Epoch 03302: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 03308: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 3310: train loss: 0.059505145996809006\n",
      "Epoch 03314: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 03320: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 3320: train loss: 0.059505097568035126\n",
      "Epoch 03326: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 3330: train loss: 0.05950508639216423\n",
      "Epoch 03332: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 3332, loss: 0.05950509011745453\n",
      "Predictions saved to results-mm-1-10.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
