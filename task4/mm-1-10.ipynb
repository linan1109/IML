{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "        nn.init.xavier_normal_(self.fc6.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-mm-1-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152eb05d2ae747ab869cfec189e39fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 2.4953139769301123, val loss: 1.6224374084472657\n",
      "Epoch 2: train loss: 1.5724829788402634, val loss: 1.4577139568328858\n",
      "Epoch 3: train loss: 1.2913417053417284, val loss: 1.0972143898010254\n",
      "Epoch 4: train loss: 1.111205592982623, val loss: 0.9747189378738403\n",
      "Epoch 5: train loss: 0.9829574831845809, val loss: 0.977591372013092\n",
      "Epoch 6: train loss: 0.8704035035055511, val loss: 0.8162348074913025\n",
      "Epoch 7: train loss: 0.7609953787667411, val loss: 0.7270260972976684\n",
      "Epoch 8: train loss: 0.6770785671837476, val loss: 0.6447800583839417\n",
      "Epoch 9: train loss: 0.5932145981204753, val loss: 0.5418333706855774\n",
      "Epoch 10: train loss: 0.5273310290550699, val loss: 0.4878260817527771\n",
      "Epoch 11: train loss: 0.4613488407329637, val loss: 0.4436122510433197\n",
      "Epoch 12: train loss: 0.40601700659187473, val loss: 0.3804196856021881\n",
      "Epoch 13: train loss: 0.35421021924213486, val loss: 0.3416312699317932\n",
      "Epoch 14: train loss: 0.3071942740897743, val loss: 0.27549313187599184\n",
      "Epoch 15: train loss: 0.2670630807706288, val loss: 0.24521034574508668\n",
      "Epoch 16: train loss: 0.22781989748623907, val loss: 0.20237963223457336\n",
      "Epoch 17: train loss: 0.19408473720599195, val loss: 0.17238807213306426\n",
      "Epoch 18: train loss: 0.163609727752452, val loss: 0.14173890125751495\n",
      "Epoch 19: train loss: 0.13665966901973803, val loss: 0.11609143185615539\n",
      "Epoch 20: train loss: 0.11387815189604857, val loss: 0.10241215163469315\n",
      "Epoch 21: train loss: 0.0954128120797021, val loss: 0.08425722169876099\n",
      "Epoch 22: train loss: 0.07834051210418039, val loss: 0.07324340677261353\n",
      "Epoch 23: train loss: 0.0661274480357462, val loss: 0.06057906821370125\n",
      "Epoch 24: train loss: 0.057295599366937365, val loss: 0.04713538938760758\n",
      "Epoch 25: train loss: 0.05010798099332926, val loss: 0.04726206487417221\n",
      "Epoch 26: train loss: 0.044506973578613634, val loss: 0.04111774265766144\n",
      "Epoch 27: train loss: 0.04132194142074001, val loss: 0.03863712242245674\n",
      "Epoch 28: train loss: 0.03904859714057981, val loss: 0.04057424423098564\n",
      "Epoch 29: train loss: 0.037445403501087306, val loss: 0.032871289640665054\n",
      "Epoch 30: train loss: 0.036392051749083464, val loss: 0.03327470037341118\n",
      "Epoch 31: train loss: 0.03618627579662265, val loss: 0.03333624067902565\n",
      "Epoch 32: train loss: 0.03564699141954889, val loss: 0.03431736299395561\n",
      "Epoch 33: train loss: 0.035361957522071136, val loss: 0.03102235209941864\n",
      "Epoch 34: train loss: 0.03565015235543251, val loss: 0.03289080607891083\n",
      "Epoch 35: train loss: 0.03531158771685192, val loss: 0.03603312599658966\n",
      "Epoch 36: train loss: 0.03496903752368324, val loss: 0.03145535463094711\n",
      "Epoch 37: train loss: 0.0346752087854001, val loss: 0.037027080267667774\n",
      "Epoch 38: train loss: 0.034985521136497964, val loss: 0.033573756873607634\n",
      "Epoch 00039: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 39: train loss: 0.03488832244946032, val loss: 0.03415360090136528\n",
      "Epoch 40: train loss: 0.03444789535476237, val loss: 0.0326240728199482\n",
      "Epoch 41: train loss: 0.033915787383305786, val loss: 0.03147017312049866\n",
      "Epoch 42: train loss: 0.033567814340700906, val loss: 0.03551959529519081\n",
      "Epoch 43: train loss: 0.033388917751458226, val loss: 0.03033022812008858\n",
      "Epoch 44: train loss: 0.03298305226649557, val loss: 0.03120693311095238\n",
      "Epoch 45: train loss: 0.032894861452433526, val loss: 0.031929259538650515\n",
      "Epoch 46: train loss: 0.033135193582092014, val loss: 0.029502499580383302\n",
      "Epoch 47: train loss: 0.03374362462029165, val loss: 0.03273990587890148\n",
      "Epoch 48: train loss: 0.033280357537829146, val loss: 0.030481246769428254\n",
      "Epoch 49: train loss: 0.03305490559217881, val loss: 0.030057107239961623\n",
      "Epoch 50: train loss: 0.032923635620243696, val loss: 0.032544450521469113\n",
      "Epoch 51: train loss: 0.03317677877448043, val loss: 0.03562963631749153\n",
      "Epoch 00052: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 52: train loss: 0.03308813450774368, val loss: 0.03291301366686821\n",
      "Epoch 53: train loss: 0.032372527128579666, val loss: 0.03291054347157479\n",
      "Epoch 54: train loss: 0.032370833932137, val loss: 0.03221322563290596\n",
      "Epoch 55: train loss: 0.032402243546685396, val loss: 0.0279912244528532\n",
      "Epoch 56: train loss: 0.03245817441447657, val loss: 0.031517614930868146\n",
      "Epoch 57: train loss: 0.031939859404247636, val loss: 0.03334731960296631\n",
      "Epoch 58: train loss: 0.03183676482220085, val loss: 0.03201046897470951\n",
      "Epoch 59: train loss: 0.03171183959257846, val loss: 0.032028713405132296\n",
      "Epoch 60: train loss: 0.03163679959032, val loss: 0.0331675044298172\n",
      "Epoch 00061: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 61: train loss: 0.032315214116050275, val loss: 0.03340434557199478\n",
      "Epoch 62: train loss: 0.03178369335434875, val loss: 0.03211897100508213\n",
      "Epoch 63: train loss: 0.03192112965760183, val loss: 0.03098913396894932\n",
      "Epoch 64: train loss: 0.03174294891801416, val loss: 0.031131826281547547\n",
      "Epoch 65: train loss: 0.03187451536101955, val loss: 0.029003337442874907\n",
      "Epoch 66: train loss: 0.03164693057172153, val loss: 0.034136334508657454\n",
      "Epoch 00067: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 67: train loss: 0.031188718273019306, val loss: 0.03734909987449646\n",
      "Epoch 68: train loss: 0.03151642133386768, val loss: 0.034447390854358675\n",
      "Epoch 69: train loss: 0.03179621173289358, val loss: 0.030008216202259064\n",
      "Epoch 70: train loss: 0.03153353203498587, val loss: 0.03047738753259182\n",
      "Epoch 71: train loss: 0.031487582186046914, val loss: 0.029467795222997667\n",
      "Epoch 72: train loss: 0.031235584387365654, val loss: 0.032433501422405246\n",
      "Epoch 00073: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 73: train loss: 0.031519610288191814, val loss: 0.03153254155814648\n",
      "Epoch 74: train loss: 0.03144861836396918, val loss: 0.033245472639799115\n",
      "Epoch 75: train loss: 0.03146957461870446, val loss: 0.029575714975595475\n",
      "Epoch 76: train loss: 0.0315802025113787, val loss: 0.030245198637247086\n",
      "Epoch 77: train loss: 0.031530867829614756, val loss: 0.029673809453845023\n",
      "Epoch 78: train loss: 0.0314928532875314, val loss: 0.03209533253312111\n",
      "Epoch 00079: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 79: train loss: 0.03122203098207104, val loss: 0.03270435792207718\n",
      "Early stop at epoch 79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e806d0309045718066958f3d2cd20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 3.796717643737793\n",
      "Epoch 20: train loss: 3.7042019367218018\n",
      "Epoch 30: train loss: 3.6133596897125244\n",
      "Epoch 40: train loss: 3.5242955684661865\n",
      "Epoch 50: train loss: 3.4370601177215576\n",
      "Epoch 60: train loss: 3.351666212081909\n",
      "Epoch 70: train loss: 3.268105983734131\n",
      "Epoch 80: train loss: 3.18636155128479\n",
      "Epoch 90: train loss: 3.1064114570617676\n",
      "Epoch 100: train loss: 3.0282301902770996\n",
      "Epoch 110: train loss: 2.9517931938171387\n",
      "Epoch 120: train loss: 2.8770742416381836\n",
      "Epoch 130: train loss: 2.804048776626587\n",
      "Epoch 140: train loss: 2.7326903343200684\n",
      "Epoch 150: train loss: 2.6629741191864014\n",
      "Epoch 160: train loss: 2.5948736667633057\n",
      "Epoch 170: train loss: 2.528364658355713\n",
      "Epoch 180: train loss: 2.46342134475708\n",
      "Epoch 190: train loss: 2.4000184535980225\n",
      "Epoch 200: train loss: 2.338130474090576\n",
      "Epoch 210: train loss: 2.2777328491210938\n",
      "Epoch 220: train loss: 2.2188003063201904\n",
      "Epoch 230: train loss: 2.1613080501556396\n",
      "Epoch 240: train loss: 2.105231285095215\n",
      "Epoch 250: train loss: 2.0505452156066895\n",
      "Epoch 260: train loss: 1.9972256422042847\n",
      "Epoch 270: train loss: 1.9452471733093262\n",
      "Epoch 280: train loss: 1.894586443901062\n",
      "Epoch 290: train loss: 1.8452187776565552\n",
      "Epoch 300: train loss: 1.7971197366714478\n",
      "Epoch 310: train loss: 1.7502657175064087\n",
      "Epoch 320: train loss: 1.7046329975128174\n",
      "Epoch 330: train loss: 1.660197377204895\n",
      "Epoch 340: train loss: 1.6169357299804688\n",
      "Epoch 350: train loss: 1.5748242139816284\n",
      "Epoch 360: train loss: 1.5338399410247803\n",
      "Epoch 370: train loss: 1.4939597845077515\n",
      "Epoch 380: train loss: 1.4551607370376587\n",
      "Epoch 390: train loss: 1.4174200296401978\n",
      "Epoch 400: train loss: 1.380715250968933\n",
      "Epoch 410: train loss: 1.3450238704681396\n",
      "Epoch 420: train loss: 1.3103241920471191\n",
      "Epoch 430: train loss: 1.276593804359436\n",
      "Epoch 440: train loss: 1.2438112497329712\n",
      "Epoch 450: train loss: 1.2119550704956055\n",
      "Epoch 460: train loss: 1.1810040473937988\n",
      "Epoch 470: train loss: 1.1509368419647217\n",
      "Epoch 480: train loss: 1.1217329502105713\n",
      "Epoch 490: train loss: 1.0933717489242554\n",
      "Epoch 500: train loss: 1.0658328533172607\n",
      "Epoch 510: train loss: 1.0390958786010742\n",
      "Epoch 520: train loss: 1.0131416320800781\n",
      "Epoch 530: train loss: 0.9879510402679443\n",
      "Epoch 540: train loss: 0.9635040760040283\n",
      "Epoch 550: train loss: 0.9397822618484497\n",
      "Epoch 560: train loss: 0.9167666435241699\n",
      "Epoch 570: train loss: 0.8944389224052429\n",
      "Epoch 580: train loss: 0.8727812170982361\n",
      "Epoch 590: train loss: 0.8517756462097168\n",
      "Epoch 600: train loss: 0.8314045667648315\n",
      "Epoch 610: train loss: 0.8116511106491089\n",
      "Epoch 620: train loss: 0.7924982905387878\n",
      "Epoch 630: train loss: 0.7739297151565552\n",
      "Epoch 640: train loss: 0.7559289932250977\n",
      "Epoch 650: train loss: 0.7384802103042603\n",
      "Epoch 660: train loss: 0.7215679883956909\n",
      "Epoch 670: train loss: 0.7051768898963928\n",
      "Epoch 680: train loss: 0.6892919540405273\n",
      "Epoch 690: train loss: 0.6738987565040588\n",
      "Epoch 700: train loss: 0.6589828133583069\n",
      "Epoch 710: train loss: 0.6445302963256836\n",
      "Epoch 720: train loss: 0.6305273175239563\n",
      "Epoch 730: train loss: 0.6169606447219849\n",
      "Epoch 740: train loss: 0.6038174629211426\n",
      "Epoch 750: train loss: 0.5910848379135132\n",
      "Epoch 760: train loss: 0.5787504315376282\n",
      "Epoch 770: train loss: 0.5668022036552429\n",
      "Epoch 780: train loss: 0.5552284717559814\n",
      "Epoch 790: train loss: 0.5440173745155334\n",
      "Epoch 800: train loss: 0.5331581234931946\n",
      "Epoch 810: train loss: 0.5226396322250366\n",
      "Epoch 820: train loss: 0.5124515295028687\n",
      "Epoch 830: train loss: 0.5025833249092102\n",
      "Epoch 840: train loss: 0.4930253028869629\n",
      "Epoch 850: train loss: 0.48376744985580444\n",
      "Epoch 860: train loss: 0.4748005270957947\n",
      "Epoch 870: train loss: 0.46611517667770386\n",
      "Epoch 880: train loss: 0.4577026665210724\n",
      "Epoch 890: train loss: 0.4495542049407959\n",
      "Epoch 900: train loss: 0.44166141748428345\n",
      "Epoch 910: train loss: 0.4340161681175232\n",
      "Epoch 920: train loss: 0.42661070823669434\n",
      "Epoch 930: train loss: 0.419437050819397\n",
      "Epoch 940: train loss: 0.41248801350593567\n",
      "Epoch 950: train loss: 0.4057563245296478\n",
      "Epoch 960: train loss: 0.3992348313331604\n",
      "Epoch 970: train loss: 0.39291685819625854\n",
      "Epoch 980: train loss: 0.3867959976196289\n",
      "Epoch 990: train loss: 0.3808656930923462\n",
      "Epoch 1000: train loss: 0.37511977553367615\n",
      "Epoch 1010: train loss: 0.36955228447914124\n",
      "Epoch 1020: train loss: 0.3641573190689087\n",
      "Epoch 1030: train loss: 0.3589293658733368\n",
      "Epoch 1040: train loss: 0.3538629412651062\n",
      "Epoch 1050: train loss: 0.3489527702331543\n",
      "Epoch 1060: train loss: 0.3441936671733856\n",
      "Epoch 1070: train loss: 0.3395806849002838\n",
      "Epoch 1080: train loss: 0.3351089358329773\n",
      "Epoch 1090: train loss: 0.3307739198207855\n",
      "Epoch 1100: train loss: 0.3265710473060608\n",
      "Epoch 1110: train loss: 0.32249587774276733\n",
      "Epoch 1120: train loss: 0.3185441792011261\n",
      "Epoch 1130: train loss: 0.3147118389606476\n",
      "Epoch 1140: train loss: 0.310994952917099\n",
      "Epoch 1150: train loss: 0.307389497756958\n",
      "Epoch 1160: train loss: 0.3038918375968933\n",
      "Epoch 1170: train loss: 0.30049824714660645\n",
      "Epoch 1180: train loss: 0.29720523953437805\n",
      "Epoch 1190: train loss: 0.29400932788848877\n",
      "Epoch 1200: train loss: 0.2909073233604431\n",
      "Epoch 1210: train loss: 0.2878958582878113\n",
      "Epoch 1220: train loss: 0.2849718928337097\n",
      "Epoch 1230: train loss: 0.2821323275566101\n",
      "Epoch 1240: train loss: 0.2793743312358856\n",
      "Epoch 1250: train loss: 0.2766948938369751\n",
      "Epoch 1260: train loss: 0.2740916311740875\n",
      "Epoch 1270: train loss: 0.27156153321266174\n",
      "Epoch 1280: train loss: 0.2691018581390381\n",
      "Epoch 1290: train loss: 0.2667103409767151\n",
      "Epoch 1300: train loss: 0.2643844485282898\n",
      "Epoch 1310: train loss: 0.26212191581726074\n",
      "Epoch 1320: train loss: 0.2599203884601593\n",
      "Epoch 1330: train loss: 0.257777601480484\n",
      "Epoch 1340: train loss: 0.25569140911102295\n",
      "Epoch 1350: train loss: 0.25365975499153137\n",
      "Epoch 1360: train loss: 0.2516806423664093\n",
      "Epoch 1370: train loss: 0.2497519999742508\n",
      "Epoch 1380: train loss: 0.2478720247745514\n",
      "Epoch 1390: train loss: 0.24603891372680664\n",
      "Epoch 1400: train loss: 0.24425071477890015\n",
      "Epoch 1410: train loss: 0.24250581860542297\n",
      "Epoch 1420: train loss: 0.24080252647399902\n",
      "Epoch 1430: train loss: 0.23913924396038055\n",
      "Epoch 1440: train loss: 0.23751439154148102\n",
      "Epoch 1450: train loss: 0.23592638969421387\n",
      "Epoch 1460: train loss: 0.23437385261058807\n",
      "Epoch 1470: train loss: 0.23285533487796783\n",
      "Epoch 1480: train loss: 0.2313695102930069\n",
      "Epoch 1490: train loss: 0.22991500794887543\n",
      "Epoch 1500: train loss: 0.22849056124687195\n",
      "Epoch 1510: train loss: 0.22709491848945618\n",
      "Epoch 1520: train loss: 0.2257269024848938\n",
      "Epoch 1530: train loss: 0.2243853658437729\n",
      "Epoch 1540: train loss: 0.223069429397583\n",
      "Epoch 1550: train loss: 0.22177749872207642\n",
      "Epoch 1560: train loss: 0.22050900757312775\n",
      "Epoch 1570: train loss: 0.2192627340555191\n",
      "Epoch 1580: train loss: 0.21803778409957886\n",
      "Epoch 1590: train loss: 0.21683326363563538\n",
      "Epoch 1600: train loss: 0.21564820408821106\n",
      "Epoch 1610: train loss: 0.21448183059692383\n",
      "Epoch 1620: train loss: 0.21333329379558563\n",
      "Epoch 1630: train loss: 0.21220186352729797\n",
      "Epoch 1640: train loss: 0.2110866755247116\n",
      "Epoch 1650: train loss: 0.20998705923557281\n",
      "Epoch 1660: train loss: 0.20890235900878906\n",
      "Epoch 1670: train loss: 0.20783185958862305\n",
      "Epoch 1680: train loss: 0.20677486062049866\n",
      "Epoch 1690: train loss: 0.2057308554649353\n",
      "Epoch 1700: train loss: 0.20469917356967926\n",
      "Epoch 1710: train loss: 0.20367926359176636\n",
      "Epoch 1720: train loss: 0.20267058908939362\n",
      "Epoch 1730: train loss: 0.20167265832424164\n",
      "Epoch 1740: train loss: 0.20068499445915222\n",
      "Epoch 1750: train loss: 0.19970710575580597\n",
      "Epoch 1760: train loss: 0.1987384557723999\n",
      "Epoch 1770: train loss: 0.19777870178222656\n",
      "Epoch 1780: train loss: 0.19682742655277252\n",
      "Epoch 1790: train loss: 0.19588419795036316\n",
      "Epoch 1800: train loss: 0.1949487328529358\n",
      "Epoch 1810: train loss: 0.1940205842256546\n",
      "Epoch 1820: train loss: 0.19309943914413452\n",
      "Epoch 1830: train loss: 0.19218501448631287\n",
      "Epoch 1840: train loss: 0.19127699732780457\n",
      "Epoch 1850: train loss: 0.19037507474422455\n",
      "Epoch 1860: train loss: 0.18947897851467133\n",
      "Epoch 1870: train loss: 0.1885884404182434\n",
      "Epoch 1880: train loss: 0.1877032369375229\n",
      "Epoch 1890: train loss: 0.18682315945625305\n",
      "Epoch 1900: train loss: 0.18594790995121002\n",
      "Epoch 1910: train loss: 0.18507735431194305\n",
      "Epoch 1920: train loss: 0.18421128392219543\n",
      "Epoch 1930: train loss: 0.18334949016571045\n",
      "Epoch 1940: train loss: 0.18249183893203735\n",
      "Epoch 1950: train loss: 0.18163810670375824\n",
      "Epoch 1960: train loss: 0.18078820407390594\n",
      "Epoch 1970: train loss: 0.17994199693202972\n",
      "Epoch 1980: train loss: 0.17909929156303406\n",
      "Epoch 1990: train loss: 0.17825999855995178\n",
      "Epoch 2000: train loss: 0.17742401361465454\n",
      "Epoch 2010: train loss: 0.1765911877155304\n",
      "Epoch 2020: train loss: 0.17576146125793457\n",
      "Epoch 2030: train loss: 0.17493470013141632\n",
      "Epoch 2040: train loss: 0.17411084473133087\n",
      "Epoch 2050: train loss: 0.17328979074954987\n",
      "Epoch 2060: train loss: 0.1724715232849121\n",
      "Epoch 2070: train loss: 0.17165592312812805\n",
      "Epoch 2080: train loss: 0.17084293067455292\n",
      "Epoch 2090: train loss: 0.1700325310230255\n",
      "Epoch 2100: train loss: 0.16922466456890106\n",
      "Epoch 2110: train loss: 0.16841912269592285\n",
      "Epoch 2120: train loss: 0.16761617362499237\n",
      "Epoch 2130: train loss: 0.1668156236410141\n",
      "Epoch 2140: train loss: 0.16601738333702087\n",
      "Epoch 2150: train loss: 0.1652214378118515\n",
      "Epoch 2160: train loss: 0.16442778706550598\n",
      "Epoch 2170: train loss: 0.16363643109798431\n",
      "Epoch 2180: train loss: 0.1628473848104477\n",
      "Epoch 2190: train loss: 0.16206054389476776\n",
      "Epoch 2200: train loss: 0.16127578914165497\n",
      "Epoch 2210: train loss: 0.16049326956272125\n",
      "Epoch 2220: train loss: 0.15971310436725616\n",
      "Epoch 2230: train loss: 0.15893502533435822\n",
      "Epoch 2240: train loss: 0.15815915167331696\n",
      "Epoch 2250: train loss: 0.15738554298877716\n",
      "Epoch 2260: train loss: 0.1566140204668045\n",
      "Epoch 2270: train loss: 0.1558448225259781\n",
      "Epoch 2280: train loss: 0.15507772564888\n",
      "Epoch 2290: train loss: 0.1543128788471222\n",
      "Epoch 2300: train loss: 0.15355023741722107\n",
      "Epoch 2310: train loss: 0.15278983116149902\n",
      "Epoch 2320: train loss: 0.15203171968460083\n",
      "Epoch 2330: train loss: 0.15127573907375336\n",
      "Epoch 2340: train loss: 0.15052209794521332\n",
      "Epoch 2350: train loss: 0.14977073669433594\n",
      "Epoch 2360: train loss: 0.14902165532112122\n",
      "Epoch 2370: train loss: 0.14827485382556915\n",
      "Epoch 2380: train loss: 0.1475304216146469\n",
      "Epoch 2390: train loss: 0.14678828418254852\n",
      "Epoch 2400: train loss: 0.14604850113391876\n",
      "Epoch 2410: train loss: 0.1453111469745636\n",
      "Epoch 2420: train loss: 0.14457617700099945\n",
      "Epoch 2430: train loss: 0.14384357631206512\n",
      "Epoch 2440: train loss: 0.14311343431472778\n",
      "Epoch 2450: train loss: 0.14238578081130981\n",
      "Epoch 2460: train loss: 0.14166055619716644\n",
      "Epoch 2470: train loss: 0.14093783497810364\n",
      "Epoch 2480: train loss: 0.1402176320552826\n",
      "Epoch 2490: train loss: 0.1394999921321869\n",
      "Epoch 2500: train loss: 0.13878488540649414\n",
      "Epoch 2510: train loss: 0.13807238638401031\n",
      "Epoch 2520: train loss: 0.13736248016357422\n",
      "Epoch 2530: train loss: 0.13665518164634705\n",
      "Epoch 2540: train loss: 0.13595058023929596\n",
      "Epoch 2550: train loss: 0.135248601436615\n",
      "Epoch 2560: train loss: 0.1345493346452713\n",
      "Epoch 2570: train loss: 0.1338527798652649\n",
      "Epoch 2580: train loss: 0.13315895199775696\n",
      "Epoch 2590: train loss: 0.13246791064739227\n",
      "Epoch 2600: train loss: 0.13177961111068726\n",
      "Epoch 2610: train loss: 0.13109414279460907\n",
      "Epoch 2620: train loss: 0.1304115206003189\n",
      "Epoch 2630: train loss: 0.1297316700220108\n",
      "Epoch 2640: train loss: 0.12905478477478027\n",
      "Epoch 2650: train loss: 0.12838073074817657\n",
      "Epoch 2660: train loss: 0.1277095526456833\n",
      "Epoch 2670: train loss: 0.12704138457775116\n",
      "Epoch 2680: train loss: 0.126376211643219\n",
      "Epoch 2690: train loss: 0.12571385502815247\n",
      "Epoch 2700: train loss: 0.1250545084476471\n",
      "Epoch 2710: train loss: 0.12439821660518646\n",
      "Epoch 2720: train loss: 0.12374497950077057\n",
      "Epoch 2730: train loss: 0.12309479713439941\n",
      "Epoch 2740: train loss: 0.1224476769566536\n",
      "Epoch 2750: train loss: 0.1218036636710167\n",
      "Epoch 2760: train loss: 0.12116273492574692\n",
      "Epoch 2770: train loss: 0.12052492797374725\n",
      "Epoch 2780: train loss: 0.1198902279138565\n",
      "Epoch 2790: train loss: 0.11925873160362244\n",
      "Epoch 2800: train loss: 0.11863049864768982\n",
      "Epoch 2810: train loss: 0.11800539493560791\n",
      "Epoch 2820: train loss: 0.11738346517086029\n",
      "Epoch 2830: train loss: 0.11676487326622009\n",
      "Epoch 2840: train loss: 0.11614947021007538\n",
      "Epoch 2850: train loss: 0.1155373752117157\n",
      "Epoch 2860: train loss: 0.11492850631475449\n",
      "Epoch 2870: train loss: 0.11432299017906189\n",
      "Epoch 2880: train loss: 0.11372074484825134\n",
      "Epoch 2890: train loss: 0.11312185227870941\n",
      "Epoch 2900: train loss: 0.1125262975692749\n",
      "Epoch 2910: train loss: 0.11193409562110901\n",
      "Epoch 2920: train loss: 0.11134526878595352\n",
      "Epoch 2930: train loss: 0.11075982451438904\n",
      "Epoch 2940: train loss: 0.11017778515815735\n",
      "Epoch 2950: train loss: 0.10959914326667786\n",
      "Epoch 2960: train loss: 0.10902392864227295\n",
      "Epoch 2970: train loss: 0.10845217108726501\n",
      "Epoch 2980: train loss: 0.10788386315107346\n",
      "Epoch 2990: train loss: 0.10731898993253708\n",
      "Epoch 3000: train loss: 0.10675760358572006\n",
      "Epoch 3010: train loss: 0.1061997041106224\n",
      "Epoch 3020: train loss: 0.10564528405666351\n",
      "Epoch 3030: train loss: 0.10509435087442398\n",
      "Epoch 3040: train loss: 0.10454696416854858\n",
      "Epoch 3050: train loss: 0.10400308668613434\n",
      "Epoch 3060: train loss: 0.10346273332834244\n",
      "Epoch 3070: train loss: 0.10292592644691467\n",
      "Epoch 3080: train loss: 0.10239265114068985\n",
      "Epoch 3090: train loss: 0.10186295211315155\n",
      "Epoch 3100: train loss: 0.10133680701255798\n",
      "Epoch 3110: train loss: 0.10081424564123154\n",
      "Epoch 3120: train loss: 0.10029524564743042\n",
      "Epoch 3130: train loss: 0.09977984428405762\n",
      "Epoch 3140: train loss: 0.09926801174879074\n",
      "Epoch 3150: train loss: 0.09875980019569397\n",
      "Epoch 3160: train loss: 0.09825519472360611\n",
      "Epoch 3170: train loss: 0.09775418788194656\n",
      "Epoch 3180: train loss: 0.09725680202245712\n",
      "Epoch 3190: train loss: 0.096763014793396\n",
      "Epoch 3200: train loss: 0.09627287834882736\n",
      "Epoch 3210: train loss: 0.09578634053468704\n",
      "Epoch 3220: train loss: 0.09530346840620041\n",
      "Epoch 3230: train loss: 0.0948241800069809\n",
      "Epoch 3240: train loss: 0.09434854984283447\n",
      "Epoch 3250: train loss: 0.09387657046318054\n",
      "Epoch 3260: train loss: 0.09340821951627731\n",
      "Epoch 3270: train loss: 0.09294351190328598\n",
      "Epoch 3280: train loss: 0.09248244017362595\n",
      "Epoch 3290: train loss: 0.0920250192284584\n",
      "Epoch 3300: train loss: 0.09157123416662216\n",
      "Epoch 3310: train loss: 0.09112109988927841\n",
      "Epoch 3320: train loss: 0.09067457914352417\n",
      "Epoch 3330: train loss: 0.09023173898458481\n",
      "Epoch 3340: train loss: 0.08979254215955734\n",
      "Epoch 3350: train loss: 0.08935700356960297\n",
      "Epoch 3360: train loss: 0.0889250710606575\n",
      "Epoch 3370: train loss: 0.08849676698446274\n",
      "Epoch 3380: train loss: 0.08807210624217987\n",
      "Epoch 3390: train loss: 0.0876510813832283\n",
      "Epoch 3400: train loss: 0.08723367750644684\n",
      "Epoch 3410: train loss: 0.08681991696357727\n",
      "Epoch 3420: train loss: 0.08640975505113602\n",
      "Epoch 3430: train loss: 0.08600321412086487\n",
      "Epoch 3440: train loss: 0.08560030907392502\n",
      "Epoch 3450: train loss: 0.08520099520683289\n",
      "Epoch 3460: train loss: 0.08480529487133026\n",
      "Epoch 3470: train loss: 0.08441318571567535\n",
      "Epoch 3480: train loss: 0.08402469754219055\n",
      "Epoch 3490: train loss: 0.08363978564739227\n",
      "Epoch 3500: train loss: 0.08325844258069992\n",
      "Epoch 3510: train loss: 0.08288070559501648\n",
      "Epoch 3520: train loss: 0.08250651508569717\n",
      "Epoch 3530: train loss: 0.0821358859539032\n",
      "Epoch 3540: train loss: 0.08176880329847336\n",
      "Epoch 3550: train loss: 0.08140528202056885\n",
      "Epoch 3560: train loss: 0.08104529976844788\n",
      "Epoch 3570: train loss: 0.08068883419036865\n",
      "Epoch 3580: train loss: 0.08033589273691177\n",
      "Epoch 3590: train loss: 0.07998647540807724\n",
      "Epoch 3600: train loss: 0.07964053004980087\n",
      "Epoch 3610: train loss: 0.07929808646440506\n",
      "Epoch 3620: train loss: 0.0789591372013092\n",
      "Epoch 3630: train loss: 0.07862366735935211\n",
      "Epoch 3640: train loss: 0.07829165458679199\n",
      "Epoch 3650: train loss: 0.07796309143304825\n",
      "Epoch 3660: train loss: 0.0776379406452179\n",
      "Epoch 3670: train loss: 0.07731624692678452\n",
      "Epoch 3680: train loss: 0.07699795067310333\n",
      "Epoch 3690: train loss: 0.07668305933475494\n",
      "Epoch 3700: train loss: 0.07637156546115875\n",
      "Epoch 3710: train loss: 0.07606344670057297\n",
      "Epoch 3720: train loss: 0.07575869560241699\n",
      "Epoch 3730: train loss: 0.07545729726552963\n",
      "Epoch 3740: train loss: 0.07515924423933029\n",
      "Epoch 3750: train loss: 0.07486449927091599\n",
      "Epoch 3760: train loss: 0.0745730921626091\n",
      "Epoch 3770: train loss: 0.07428497076034546\n",
      "Epoch 3780: train loss: 0.07400013506412506\n",
      "Epoch 3790: train loss: 0.07371857017278671\n",
      "Epoch 3800: train loss: 0.07344023883342743\n",
      "Epoch 3810: train loss: 0.0731651559472084\n",
      "Epoch 3820: train loss: 0.07289329171180725\n",
      "Epoch 3830: train loss: 0.07262464612722397\n",
      "Epoch 3840: train loss: 0.07235918939113617\n",
      "Epoch 3850: train loss: 0.07209690660238266\n",
      "Epoch 3860: train loss: 0.07183777540922165\n",
      "Epoch 3870: train loss: 0.07158178836107254\n",
      "Epoch 3880: train loss: 0.07132892310619354\n",
      "Epoch 3890: train loss: 0.07107917219400406\n",
      "Epoch 3900: train loss: 0.0708325207233429\n",
      "Epoch 3910: train loss: 0.07058892399072647\n",
      "Epoch 3920: train loss: 0.07034839689731598\n",
      "Epoch 3930: train loss: 0.07011090219020844\n",
      "Epoch 3940: train loss: 0.06987643241882324\n",
      "Epoch 3950: train loss: 0.06964495033025742\n",
      "Epoch 3960: train loss: 0.06941647082567215\n",
      "Epoch 3970: train loss: 0.06919094920158386\n",
      "Epoch 3980: train loss: 0.06896837055683136\n",
      "Epoch 3990: train loss: 0.06874871253967285\n",
      "Epoch 4000: train loss: 0.06853197515010834\n",
      "Epoch 4010: train loss: 0.06831812113523483\n",
      "Epoch 4020: train loss: 0.06810713559389114\n",
      "Epoch 4030: train loss: 0.06789899617433548\n",
      "Epoch 4040: train loss: 0.06769369542598724\n",
      "Epoch 4050: train loss: 0.06749118864536285\n",
      "Epoch 4060: train loss: 0.0672914981842041\n",
      "Epoch 4070: train loss: 0.06709454208612442\n",
      "Epoch 4080: train loss: 0.0669003576040268\n",
      "Epoch 4090: train loss: 0.06670890003442764\n",
      "Epoch 4100: train loss: 0.06652013957500458\n",
      "Epoch 4110: train loss: 0.0663340836763382\n",
      "Epoch 4120: train loss: 0.06615068018436432\n",
      "Epoch 4130: train loss: 0.06596993654966354\n",
      "Epoch 4140: train loss: 0.06579180061817169\n",
      "Epoch 4150: train loss: 0.06561627238988876\n",
      "Epoch 4160: train loss: 0.06544332951307297\n",
      "Epoch 4170: train loss: 0.06527294963598251\n",
      "Epoch 4180: train loss: 0.06510509550571442\n",
      "Epoch 4190: train loss: 0.06493977457284927\n",
      "Epoch 4200: train loss: 0.0647769421339035\n",
      "Epoch 4210: train loss: 0.06461659073829651\n",
      "Epoch 4220: train loss: 0.0644586831331253\n",
      "Epoch 4230: train loss: 0.0643032118678093\n",
      "Epoch 4240: train loss: 0.06415014714002609\n",
      "Epoch 4250: train loss: 0.0639994665980339\n",
      "Epoch 4260: train loss: 0.06385115534067154\n",
      "Epoch 4270: train loss: 0.06370517611503601\n",
      "Epoch 4280: train loss: 0.06356153637170792\n",
      "Epoch 4290: train loss: 0.06342016160488129\n",
      "Epoch 4300: train loss: 0.06328108161687851\n",
      "Epoch 4310: train loss: 0.063144251704216\n",
      "Epoch 4320: train loss: 0.06300965696573257\n",
      "Epoch 4330: train loss: 0.06287726014852524\n",
      "Epoch 4340: train loss: 0.0627470538020134\n",
      "Epoch 4350: train loss: 0.06261901557445526\n",
      "Epoch 4360: train loss: 0.062493111938238144\n",
      "Epoch 4370: train loss: 0.06236933544278145\n",
      "Epoch 4380: train loss: 0.06224765628576279\n",
      "Epoch 4390: train loss: 0.062128037214279175\n",
      "Epoch 4400: train loss: 0.06201048567891121\n",
      "Epoch 4410: train loss: 0.06189494952559471\n",
      "Epoch 4420: train loss: 0.06178142875432968\n",
      "Epoch 4430: train loss: 0.061669882386922836\n",
      "Epoch 4440: train loss: 0.06156030669808388\n",
      "Epoch 4450: train loss: 0.06145267188549042\n",
      "Epoch 4460: train loss: 0.06134694069623947\n",
      "Epoch 4470: train loss: 0.061243101954460144\n",
      "Epoch 4480: train loss: 0.06114114448428154\n",
      "Epoch 4490: train loss: 0.06104103848338127\n",
      "Epoch 4500: train loss: 0.06094276160001755\n",
      "Epoch 4510: train loss: 0.06084628775715828\n",
      "Epoch 4520: train loss: 0.060751598328351974\n",
      "Epoch 4530: train loss: 0.060658663511276245\n",
      "Epoch 4540: train loss: 0.06056748330593109\n",
      "Epoch 4550: train loss: 0.06047799438238144\n",
      "Epoch 4560: train loss: 0.06039021536707878\n",
      "Epoch 4570: train loss: 0.060304105281829834\n",
      "Epoch 4580: train loss: 0.060219649225473404\n",
      "Epoch 4590: train loss: 0.0601368211209774\n",
      "Epoch 4600: train loss: 0.060055606067180634\n",
      "Epoch 4610: train loss: 0.05997597426176071\n",
      "Epoch 4620: train loss: 0.059897903352975845\n",
      "Epoch 4630: train loss: 0.05982137843966484\n",
      "Epoch 4640: train loss: 0.059746384620666504\n",
      "Epoch 4650: train loss: 0.05967288836836815\n",
      "Epoch 4660: train loss: 0.059600867331027985\n",
      "Epoch 4670: train loss: 0.059530310332775116\n",
      "Epoch 4680: train loss: 0.05946120247244835\n",
      "Epoch 4690: train loss: 0.059393491595983505\n",
      "Epoch 4700: train loss: 0.05932719260454178\n",
      "Epoch 4710: train loss: 0.05926226079463959\n",
      "Epoch 4720: train loss: 0.059198688715696335\n",
      "Epoch 4730: train loss: 0.05913645774126053\n",
      "Epoch 4740: train loss: 0.05907553434371948\n",
      "Epoch 4750: train loss: 0.0590159110724926\n",
      "Epoch 4760: train loss: 0.05895756557583809\n",
      "Epoch 4770: train loss: 0.05890047550201416\n",
      "Epoch 4780: train loss: 0.05884462594985962\n",
      "Epoch 4790: train loss: 0.05878998711705208\n",
      "Epoch 4800: train loss: 0.05873654782772064\n",
      "Epoch 4810: train loss: 0.05868428945541382\n",
      "Epoch 4820: train loss: 0.058633193373680115\n",
      "Epoch 4830: train loss: 0.058583229780197144\n",
      "Epoch 4840: train loss: 0.05853438749909401\n",
      "Epoch 4850: train loss: 0.05848665162920952\n",
      "Epoch 4860: train loss: 0.05843999609351158\n",
      "Epoch 4870: train loss: 0.058394405990839005\n",
      "Epoch 4880: train loss: 0.05834987387061119\n",
      "Epoch 4890: train loss: 0.05830636993050575\n",
      "Epoch 4900: train loss: 0.0582638718187809\n",
      "Epoch 4910: train loss: 0.05822237953543663\n",
      "Epoch 4920: train loss: 0.058181848376989365\n",
      "Epoch 4930: train loss: 0.0581422857940197\n",
      "Epoch 4940: train loss: 0.058103665709495544\n",
      "Epoch 4950: train loss: 0.05806596577167511\n",
      "Epoch 4960: train loss: 0.0580291785299778\n",
      "Epoch 4970: train loss: 0.05799328535795212\n",
      "Epoch 4980: train loss: 0.057958267629146576\n",
      "Epoch 4990: train loss: 0.05792410299181938\n",
      "Epoch 5000: train loss: 0.05789078399538994\n",
      "Epoch 5010: train loss: 0.05785829573869705\n",
      "Epoch 5020: train loss: 0.05782661959528923\n",
      "Epoch 5030: train loss: 0.05779572203755379\n",
      "Epoch 5040: train loss: 0.05776561424136162\n",
      "Epoch 5050: train loss: 0.05773627758026123\n",
      "Epoch 5060: train loss: 0.057707689702510834\n",
      "Epoch 5070: train loss: 0.057679831981658936\n",
      "Epoch 5080: train loss: 0.05765269324183464\n",
      "Epoch 5090: train loss: 0.05762626603245735\n",
      "Epoch 5100: train loss: 0.05760051682591438\n",
      "Epoch 5110: train loss: 0.05757545307278633\n",
      "Epoch 5120: train loss: 0.057551059871912\n",
      "Epoch 5130: train loss: 0.057527292519807816\n",
      "Epoch 5140: train loss: 0.05750418081879616\n",
      "Epoch 5150: train loss: 0.05748168006539345\n",
      "Epoch 5160: train loss: 0.05745979771018028\n",
      "Epoch 5170: train loss: 0.057438503950834274\n",
      "Epoch 5180: train loss: 0.057417791336774826\n",
      "Epoch 5190: train loss: 0.057397644966840744\n",
      "Epoch 5200: train loss: 0.05737805366516113\n",
      "Epoch 5210: train loss: 0.05735902115702629\n",
      "Epoch 5220: train loss: 0.05734051391482353\n",
      "Epoch 5230: train loss: 0.05732252448797226\n",
      "Epoch 5240: train loss: 0.057305049151182175\n",
      "Epoch 5250: train loss: 0.05728806182742119\n",
      "Epoch 5260: train loss: 0.057271573692560196\n",
      "Epoch 5270: train loss: 0.05725554749369621\n",
      "Epoch 5280: train loss: 0.057239990681409836\n",
      "Epoch 5290: train loss: 0.057224880903959274\n",
      "Epoch 5300: train loss: 0.057210225611925125\n",
      "Epoch 5310: train loss: 0.05719598010182381\n",
      "Epoch 5320: train loss: 0.05718216672539711\n",
      "Epoch 5330: train loss: 0.057168759405612946\n",
      "Epoch 5340: train loss: 0.057155754417181015\n",
      "Epoch 5350: train loss: 0.05714314430952072\n",
      "Epoch 5360: train loss: 0.057130906730890274\n",
      "Epoch 5370: train loss: 0.05711904913187027\n",
      "Epoch 5380: train loss: 0.05710754171013832\n",
      "Epoch 5390: train loss: 0.05709639564156532\n",
      "Epoch 5400: train loss: 0.057085588574409485\n",
      "Epoch 5410: train loss: 0.05707511678338051\n",
      "Epoch 5420: train loss: 0.0570649690926075\n",
      "Epoch 5430: train loss: 0.057055141776800156\n",
      "Epoch 5440: train loss: 0.05704563111066818\n",
      "Epoch 05444: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 5450: train loss: 0.05703960731625557\n",
      "Epoch 05451: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 05457: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 5460: train loss: 0.05703851580619812\n",
      "Epoch 05463: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 05469: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 5470: train loss: 0.05703837424516678\n",
      "Epoch 05475: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 5480: train loss: 0.05703836306929588\n",
      "Epoch 05481: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 05487: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 5487, loss: 0.05703836306929588\n",
      "Predictions saved to results-mm-1-10.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
