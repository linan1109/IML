{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "        nn.init.xavier_normal_(self.fc6.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-mm-1-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883fb277b7545258b30d67f17dca67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 2.3327019556006605, val loss: 1.46075532913208\n",
      "Epoch 2: train loss: 1.4493130606437217, val loss: 1.3072930822372437\n",
      "Epoch 3: train loss: 1.2232311781474523, val loss: 1.1469740314483643\n",
      "Epoch 4: train loss: 1.0578087838912498, val loss: 0.9960399255752563\n",
      "Epoch 5: train loss: 0.9286522641863142, val loss: 0.9040435881614685\n",
      "Epoch 6: train loss: 0.8273859880408462, val loss: 0.7797326936721801\n",
      "Epoch 7: train loss: 0.7202136587318109, val loss: 0.6299484877586364\n",
      "Epoch 8: train loss: 0.6216190403870174, val loss: 0.5713677124977112\n",
      "Epoch 9: train loss: 0.5466155162830743, val loss: 0.4952908306121826\n",
      "Epoch 10: train loss: 0.47794085709902706, val loss: 0.4400307357311249\n",
      "Epoch 11: train loss: 0.4107710465752349, val loss: 0.3951287667751312\n",
      "Epoch 12: train loss: 0.3567962187358311, val loss: 0.3051853721141815\n",
      "Epoch 13: train loss: 0.30296527096203396, val loss: 0.27542999386787415\n",
      "Epoch 14: train loss: 0.2573024128797103, val loss: 0.22203066384792328\n",
      "Epoch 15: train loss: 0.21733523832048687, val loss: 0.19046132624149323\n",
      "Epoch 16: train loss: 0.18536675741842815, val loss: 0.16265373957157134\n",
      "Epoch 17: train loss: 0.1549326142267305, val loss: 0.14317112421989442\n",
      "Epoch 18: train loss: 0.12978757874941338, val loss: 0.11233496201038361\n",
      "Epoch 19: train loss: 0.10743760286058698, val loss: 0.09814975589513779\n",
      "Epoch 20: train loss: 0.0891650104121286, val loss: 0.07915912538766862\n",
      "Epoch 21: train loss: 0.07458739871273236, val loss: 0.06674556583166122\n",
      "Epoch 22: train loss: 0.06309665033038782, val loss: 0.05335772204399109\n",
      "Epoch 23: train loss: 0.05571041569113731, val loss: 0.04891112911701202\n",
      "Epoch 24: train loss: 0.047513908004578276, val loss: 0.041301495492458346\n",
      "Epoch 25: train loss: 0.04354145519435406, val loss: 0.037901722103357315\n",
      "Epoch 26: train loss: 0.03995535849916692, val loss: 0.0444698925614357\n",
      "Epoch 27: train loss: 0.038963476380523374, val loss: 0.03741595274209976\n",
      "Epoch 28: train loss: 0.037506290750844135, val loss: 0.03937808388471603\n",
      "Epoch 29: train loss: 0.0372131831956153, val loss: 0.0374038727581501\n",
      "Epoch 30: train loss: 0.03723721020988056, val loss: 0.033557832449674606\n",
      "Epoch 31: train loss: 0.03668136293486673, val loss: 0.03594218027591705\n",
      "Epoch 32: train loss: 0.03654380973504514, val loss: 0.0345135395526886\n",
      "Epoch 33: train loss: 0.03619977190604015, val loss: 0.0349528700709343\n",
      "Epoch 34: train loss: 0.03604426425391314, val loss: 0.03645180258154869\n",
      "Epoch 35: train loss: 0.03688533491078688, val loss: 0.034052751690149305\n",
      "Epoch 00036: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 36: train loss: 0.03630107187190834, val loss: 0.03930158686637879\n",
      "Epoch 37: train loss: 0.03519894156224874, val loss: 0.0330163805782795\n",
      "Epoch 38: train loss: 0.03483245219989699, val loss: 0.03231767472624779\n",
      "Epoch 39: train loss: 0.03485613727083012, val loss: 0.03232429051399231\n",
      "Epoch 40: train loss: 0.03437241129212233, val loss: 0.031268996194005014\n",
      "Epoch 41: train loss: 0.034702218798958524, val loss: 0.03415019837021828\n",
      "Epoch 42: train loss: 0.0345631165072626, val loss: 0.03206617894768715\n",
      "Epoch 43: train loss: 0.03444111380437199, val loss: 0.03435562315583229\n",
      "Epoch 44: train loss: 0.033929467506554664, val loss: 0.03347028568387032\n",
      "Epoch 45: train loss: 0.0344762273759258, val loss: 0.03297452998161316\n",
      "Epoch 00046: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 46: train loss: 0.034058758190699985, val loss: 0.03349480682611466\n",
      "Epoch 47: train loss: 0.03387655974894154, val loss: 0.02983453357219696\n",
      "Epoch 48: train loss: 0.03362199752671378, val loss: 0.03308631348609924\n",
      "Epoch 49: train loss: 0.033639633277849273, val loss: 0.031798229932785034\n",
      "Epoch 50: train loss: 0.03339481130972201, val loss: 0.03482807102799416\n",
      "Epoch 51: train loss: 0.03377097519988916, val loss: 0.03327642834186554\n",
      "Epoch 52: train loss: 0.03317306000176741, val loss: 0.033816085726022724\n",
      "Epoch 00053: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 53: train loss: 0.033388096313087305, val loss: 0.03442948578298092\n",
      "Epoch 54: train loss: 0.03306175906013469, val loss: 0.034271681010723115\n",
      "Epoch 55: train loss: 0.03348851533507814, val loss: 0.03349795174598694\n",
      "Epoch 56: train loss: 0.03354661526424544, val loss: 0.03167088875174522\n",
      "Epoch 57: train loss: 0.033282868181260264, val loss: 0.03785506647825241\n",
      "Epoch 58: train loss: 0.033349251771459774, val loss: 0.029899500414729118\n",
      "Epoch 00059: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 59: train loss: 0.03329371814946739, val loss: 0.03152783954143524\n",
      "Epoch 60: train loss: 0.03322016213803875, val loss: 0.03592285218834877\n",
      "Epoch 61: train loss: 0.033063473989160694, val loss: 0.033887713313102724\n",
      "Epoch 62: train loss: 0.03298860028903095, val loss: 0.03164674714207649\n",
      "Epoch 63: train loss: 0.03265815369207032, val loss: 0.0321346245110035\n",
      "Epoch 64: train loss: 0.0333154290257668, val loss: 0.03365369296073913\n",
      "Epoch 00065: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 65: train loss: 0.032707119611757145, val loss: 0.030958157196640967\n",
      "Epoch 66: train loss: 0.033049004858245655, val loss: 0.03605597439408302\n",
      "Epoch 67: train loss: 0.03338777686503469, val loss: 0.031795968666672704\n",
      "Epoch 68: train loss: 0.032787047703351294, val loss: 0.03335567831993103\n",
      "Epoch 69: train loss: 0.032973249204912965, val loss: 0.03273815095424652\n",
      "Epoch 70: train loss: 0.03297622250537483, val loss: 0.030910424306988715\n",
      "Epoch 00071: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 71: train loss: 0.03290944525903585, val loss: 0.03225052851438522\n",
      "Early stop at epoch 71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d65452a350845c2990d2e14b5586341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 8.910155296325684\n",
      "Epoch 20: train loss: 8.483434677124023\n",
      "Epoch 30: train loss: 8.072325706481934\n",
      "Epoch 40: train loss: 7.6776556968688965\n",
      "Epoch 50: train loss: 7.299696922302246\n",
      "Epoch 60: train loss: 6.938359260559082\n",
      "Epoch 70: train loss: 6.5933451652526855\n",
      "Epoch 80: train loss: 6.264257907867432\n",
      "Epoch 90: train loss: 5.950654029846191\n",
      "Epoch 100: train loss: 5.652071952819824\n",
      "Epoch 110: train loss: 5.368043899536133\n",
      "Epoch 120: train loss: 5.098097801208496\n",
      "Epoch 130: train loss: 4.841765880584717\n",
      "Epoch 140: train loss: 4.598578929901123\n",
      "Epoch 150: train loss: 4.368072509765625\n",
      "Epoch 160: train loss: 4.149783611297607\n",
      "Epoch 170: train loss: 3.943253755569458\n",
      "Epoch 180: train loss: 3.7480289936065674\n",
      "Epoch 190: train loss: 3.563660144805908\n",
      "Epoch 200: train loss: 3.3897013664245605\n",
      "Epoch 210: train loss: 3.225715160369873\n",
      "Epoch 220: train loss: 3.071268320083618\n",
      "Epoch 230: train loss: 2.925935983657837\n",
      "Epoch 240: train loss: 2.789299249649048\n",
      "Epoch 250: train loss: 2.6609485149383545\n",
      "Epoch 260: train loss: 2.540480852127075\n",
      "Epoch 270: train loss: 2.4275031089782715\n",
      "Epoch 280: train loss: 2.3216311931610107\n",
      "Epoch 290: train loss: 2.222491502761841\n",
      "Epoch 300: train loss: 2.129718780517578\n",
      "Epoch 310: train loss: 2.0429599285125732\n",
      "Epoch 320: train loss: 1.9618724584579468\n",
      "Epoch 330: train loss: 1.8861238956451416\n",
      "Epoch 340: train loss: 1.8153942823410034\n",
      "Epoch 350: train loss: 1.7493749856948853\n",
      "Epoch 360: train loss: 1.687768816947937\n",
      "Epoch 370: train loss: 1.6302911043167114\n",
      "Epoch 380: train loss: 1.576668620109558\n",
      "Epoch 390: train loss: 1.5266410112380981\n",
      "Epoch 400: train loss: 1.4799587726593018\n",
      "Epoch 410: train loss: 1.4363857507705688\n",
      "Epoch 420: train loss: 1.395696997642517\n",
      "Epoch 430: train loss: 1.3576796054840088\n",
      "Epoch 440: train loss: 1.322131872177124\n",
      "Epoch 450: train loss: 1.2888643741607666\n",
      "Epoch 460: train loss: 1.2576982975006104\n",
      "Epoch 470: train loss: 1.2284656763076782\n",
      "Epoch 480: train loss: 1.201008915901184\n",
      "Epoch 490: train loss: 1.1751821041107178\n",
      "Epoch 500: train loss: 1.1508479118347168\n",
      "Epoch 510: train loss: 1.1278786659240723\n",
      "Epoch 520: train loss: 1.106156349182129\n",
      "Epoch 530: train loss: 1.0855712890625\n",
      "Epoch 540: train loss: 1.0660226345062256\n",
      "Epoch 550: train loss: 1.0474166870117188\n",
      "Epoch 560: train loss: 1.029667854309082\n",
      "Epoch 570: train loss: 1.0126979351043701\n",
      "Epoch 580: train loss: 0.9964346289634705\n",
      "Epoch 590: train loss: 0.980812668800354\n",
      "Epoch 600: train loss: 0.9657721519470215\n",
      "Epoch 610: train loss: 0.9512585401535034\n",
      "Epoch 620: train loss: 0.9372227191925049\n",
      "Epoch 630: train loss: 0.923620343208313\n",
      "Epoch 640: train loss: 0.9104110598564148\n",
      "Epoch 650: train loss: 0.8975587487220764\n",
      "Epoch 660: train loss: 0.8850311040878296\n",
      "Epoch 670: train loss: 0.8727990388870239\n",
      "Epoch 680: train loss: 0.8608368635177612\n",
      "Epoch 690: train loss: 0.8491209149360657\n",
      "Epoch 700: train loss: 0.8376312255859375\n",
      "Epoch 710: train loss: 0.8263493180274963\n",
      "Epoch 720: train loss: 0.8152592778205872\n",
      "Epoch 730: train loss: 0.804347038269043\n",
      "Epoch 740: train loss: 0.7935999035835266\n",
      "Epoch 750: train loss: 0.7830070853233337\n",
      "Epoch 760: train loss: 0.7725591063499451\n",
      "Epoch 770: train loss: 0.7622476816177368\n",
      "Epoch 780: train loss: 0.7520655393600464\n",
      "Epoch 790: train loss: 0.7420061826705933\n",
      "Epoch 800: train loss: 0.7320645451545715\n",
      "Epoch 810: train loss: 0.7222356796264648\n",
      "Epoch 820: train loss: 0.7125159502029419\n",
      "Epoch 830: train loss: 0.7029016017913818\n",
      "Epoch 840: train loss: 0.6933901906013489\n",
      "Epoch 850: train loss: 0.6839789152145386\n",
      "Epoch 860: train loss: 0.6746661067008972\n",
      "Epoch 870: train loss: 0.6654495000839233\n",
      "Epoch 880: train loss: 0.656328022480011\n",
      "Epoch 890: train loss: 0.6473004221916199\n",
      "Epoch 900: train loss: 0.6383655667304993\n",
      "Epoch 910: train loss: 0.6295227408409119\n",
      "Epoch 920: train loss: 0.6207712292671204\n",
      "Epoch 930: train loss: 0.6121106743812561\n",
      "Epoch 940: train loss: 0.6035404801368713\n",
      "Epoch 950: train loss: 0.5950602293014526\n",
      "Epoch 960: train loss: 0.5866697430610657\n",
      "Epoch 970: train loss: 0.5783687233924866\n",
      "Epoch 980: train loss: 0.570156991481781\n",
      "Epoch 990: train loss: 0.5620344877243042\n",
      "Epoch 1000: train loss: 0.554000973701477\n",
      "Epoch 1010: train loss: 0.5460562109947205\n",
      "Epoch 1020: train loss: 0.538200318813324\n",
      "Epoch 1030: train loss: 0.5304329991340637\n",
      "Epoch 1040: train loss: 0.5227542519569397\n",
      "Epoch 1050: train loss: 0.5151638388633728\n",
      "Epoch 1060: train loss: 0.5076618194580078\n",
      "Epoch 1070: train loss: 0.5002478957176208\n",
      "Epoch 1080: train loss: 0.49292197823524475\n",
      "Epoch 1090: train loss: 0.48568400740623474\n",
      "Epoch 1100: train loss: 0.4785337746143341\n",
      "Epoch 1110: train loss: 0.47147098183631897\n",
      "Epoch 1120: train loss: 0.46449530124664307\n",
      "Epoch 1130: train loss: 0.45760688185691833\n",
      "Epoch 1140: train loss: 0.4508054256439209\n",
      "Epoch 1150: train loss: 0.4440905749797821\n",
      "Epoch 1160: train loss: 0.43746206164360046\n",
      "Epoch 1170: train loss: 0.4309195578098297\n",
      "Epoch 1180: train loss: 0.42446279525756836\n",
      "Epoch 1190: train loss: 0.41809144616127014\n",
      "Epoch 1200: train loss: 0.41180533170700073\n",
      "Epoch 1210: train loss: 0.40560370683670044\n",
      "Epoch 1220: train loss: 0.3994865417480469\n",
      "Epoch 1230: train loss: 0.39345335960388184\n",
      "Epoch 1240: train loss: 0.38750380277633667\n",
      "Epoch 1250: train loss: 0.38163724541664124\n",
      "Epoch 1260: train loss: 0.375853568315506\n",
      "Epoch 1270: train loss: 0.370152086019516\n",
      "Epoch 1280: train loss: 0.3645325303077698\n",
      "Epoch 1290: train loss: 0.3589942753314972\n",
      "Epoch 1300: train loss: 0.353536993265152\n",
      "Epoch 1310: train loss: 0.3481599986553192\n",
      "Epoch 1320: train loss: 0.34286272525787354\n",
      "Epoch 1330: train loss: 0.3376447856426239\n",
      "Epoch 1340: train loss: 0.3325057625770569\n",
      "Epoch 1350: train loss: 0.3274449408054352\n",
      "Epoch 1360: train loss: 0.3224618434906006\n",
      "Epoch 1370: train loss: 0.3175559639930725\n",
      "Epoch 1380: train loss: 0.31272652745246887\n",
      "Epoch 1390: train loss: 0.30797311663627625\n",
      "Epoch 1400: train loss: 0.30329492688179016\n",
      "Epoch 1410: train loss: 0.2986917495727539\n",
      "Epoch 1420: train loss: 0.2941628694534302\n",
      "Epoch 1430: train loss: 0.2897075414657593\n",
      "Epoch 1440: train loss: 0.2853250205516815\n",
      "Epoch 1450: train loss: 0.2810145914554596\n",
      "Epoch 1460: train loss: 0.27677568793296814\n",
      "Epoch 1470: train loss: 0.2726081311702728\n",
      "Epoch 1480: train loss: 0.26851069927215576\n",
      "Epoch 1490: train loss: 0.2644830346107483\n",
      "Epoch 1500: train loss: 0.2605243921279907\n",
      "Epoch 1510: train loss: 0.2566341161727905\n",
      "Epoch 1520: train loss: 0.2528114318847656\n",
      "Epoch 1530: train loss: 0.2490558624267578\n",
      "Epoch 1540: train loss: 0.24536654353141785\n",
      "Epoch 1550: train loss: 0.2417428344488144\n",
      "Epoch 1560: train loss: 0.23818418383598328\n",
      "Epoch 1570: train loss: 0.23468974232673645\n",
      "Epoch 1580: train loss: 0.23125888407230377\n",
      "Epoch 1590: train loss: 0.22789078950881958\n",
      "Epoch 1600: train loss: 0.2245849221944809\n",
      "Epoch 1610: train loss: 0.22134044766426086\n",
      "Epoch 1620: train loss: 0.21815676987171173\n",
      "Epoch 1630: train loss: 0.21503318846225739\n",
      "Epoch 1640: train loss: 0.21196885406970978\n",
      "Epoch 1650: train loss: 0.20896321535110474\n",
      "Epoch 1660: train loss: 0.20601533353328705\n",
      "Epoch 1670: train loss: 0.2031247615814209\n",
      "Epoch 1680: train loss: 0.20029060542583466\n",
      "Epoch 1690: train loss: 0.19751225411891937\n",
      "Epoch 1700: train loss: 0.19478896260261536\n",
      "Epoch 1710: train loss: 0.19211995601654053\n",
      "Epoch 1720: train loss: 0.18950462341308594\n",
      "Epoch 1730: train loss: 0.1869421899318695\n",
      "Epoch 1740: train loss: 0.18443191051483154\n",
      "Epoch 1750: train loss: 0.1819731444120407\n",
      "Epoch 1760: train loss: 0.1795651614665985\n",
      "Epoch 1770: train loss: 0.17720721662044525\n",
      "Epoch 1780: train loss: 0.1748986840248108\n",
      "Epoch 1790: train loss: 0.17263881862163544\n",
      "Epoch 1800: train loss: 0.17042690515518188\n",
      "Epoch 1810: train loss: 0.16826222836971283\n",
      "Epoch 1820: train loss: 0.16614411771297455\n",
      "Epoch 1830: train loss: 0.16407184302806854\n",
      "Epoch 1840: train loss: 0.16204479336738586\n",
      "Epoch 1850: train loss: 0.16006220877170563\n",
      "Epoch 1860: train loss: 0.15812347829341888\n",
      "Epoch 1870: train loss: 0.15622775256633759\n",
      "Epoch 1880: train loss: 0.1543746292591095\n",
      "Epoch 1890: train loss: 0.1525631994009018\n",
      "Epoch 1900: train loss: 0.15079285204410553\n",
      "Epoch 1910: train loss: 0.14906291663646698\n",
      "Epoch 1920: train loss: 0.14737269282341003\n",
      "Epoch 1930: train loss: 0.14572162926197052\n",
      "Epoch 1940: train loss: 0.14410904049873352\n",
      "Epoch 1950: train loss: 0.14253421127796173\n",
      "Epoch 1960: train loss: 0.14099648594856262\n",
      "Epoch 1970: train loss: 0.13949529826641083\n",
      "Epoch 1980: train loss: 0.1380300223827362\n",
      "Epoch 1990: train loss: 0.13659992814064026\n",
      "Epoch 2000: train loss: 0.1352045089006424\n",
      "Epoch 2010: train loss: 0.13384301960468292\n",
      "Epoch 2020: train loss: 0.13251489400863647\n",
      "Epoch 2030: train loss: 0.1312195360660553\n",
      "Epoch 2040: train loss: 0.12995631992816925\n",
      "Epoch 2050: train loss: 0.12872467935085297\n",
      "Epoch 2060: train loss: 0.12752395868301392\n",
      "Epoch 2070: train loss: 0.12635359168052673\n",
      "Epoch 2080: train loss: 0.12521296739578247\n",
      "Epoch 2090: train loss: 0.12410157173871994\n",
      "Epoch 2100: train loss: 0.12301883101463318\n",
      "Epoch 2110: train loss: 0.12196412682533264\n",
      "Epoch 2120: train loss: 0.12093692272901535\n",
      "Epoch 2130: train loss: 0.11993668973445892\n",
      "Epoch 2140: train loss: 0.11896286904811859\n",
      "Epoch 2150: train loss: 0.118014857172966\n",
      "Epoch 2160: train loss: 0.11709223687648773\n",
      "Epoch 2170: train loss: 0.11619433015584946\n",
      "Epoch 2180: train loss: 0.11532077193260193\n",
      "Epoch 2190: train loss: 0.11447092890739441\n",
      "Epoch 2200: train loss: 0.1136443093419075\n",
      "Epoch 2210: train loss: 0.1128404438495636\n",
      "Epoch 2220: train loss: 0.11205882579088211\n",
      "Epoch 2230: train loss: 0.11129895597696304\n",
      "Epoch 2240: train loss: 0.11056039482355118\n",
      "Epoch 2250: train loss: 0.10984260588884354\n",
      "Epoch 2260: train loss: 0.10914512723684311\n",
      "Epoch 2270: train loss: 0.1084674745798111\n",
      "Epoch 2280: train loss: 0.10780931264162064\n",
      "Epoch 2290: train loss: 0.1071699857711792\n",
      "Epoch 2300: train loss: 0.10654924064874649\n",
      "Epoch 2310: train loss: 0.10594659298658371\n",
      "Epoch 2320: train loss: 0.10536157339811325\n",
      "Epoch 2330: train loss: 0.1047937348484993\n",
      "Epoch 2340: train loss: 0.1042427122592926\n",
      "Epoch 2350: train loss: 0.10370811820030212\n",
      "Epoch 2360: train loss: 0.10318940132856369\n",
      "Epoch 2370: train loss: 0.10268628597259521\n",
      "Epoch 2380: train loss: 0.10219837725162506\n",
      "Epoch 2390: train loss: 0.10172530263662338\n",
      "Epoch 2400: train loss: 0.10126660019159317\n",
      "Epoch 2410: train loss: 0.10082198679447174\n",
      "Epoch 2420: train loss: 0.10039108246564865\n",
      "Epoch 2430: train loss: 0.09997352212667465\n",
      "Epoch 2440: train loss: 0.09956894814968109\n",
      "Epoch 2450: train loss: 0.09917698800563812\n",
      "Epoch 2460: train loss: 0.09879735857248306\n",
      "Epoch 2470: train loss: 0.09842967987060547\n",
      "Epoch 2480: train loss: 0.09807363897562027\n",
      "Epoch 2490: train loss: 0.09772895276546478\n",
      "Epoch 2500: train loss: 0.09739526361227036\n",
      "Epoch 2510: train loss: 0.09707227349281311\n",
      "Epoch 2520: train loss: 0.09675969928503036\n",
      "Epoch 2530: train loss: 0.09645721316337585\n",
      "Epoch 2540: train loss: 0.09616456925868988\n",
      "Epoch 2550: train loss: 0.09588142484426498\n",
      "Epoch 2560: train loss: 0.09560757130384445\n",
      "Epoch 2570: train loss: 0.0953427106142044\n",
      "Epoch 2580: train loss: 0.09508655220270157\n",
      "Epoch 2590: train loss: 0.09483883529901505\n",
      "Epoch 2600: train loss: 0.09459934383630753\n",
      "Epoch 2610: train loss: 0.09436782449483871\n",
      "Epoch 2620: train loss: 0.09414401650428772\n",
      "Epoch 2630: train loss: 0.09392768889665604\n",
      "Epoch 2640: train loss: 0.09371858090162277\n",
      "Epoch 2650: train loss: 0.0935165211558342\n",
      "Epoch 2660: train loss: 0.09332124143838882\n",
      "Epoch 2670: train loss: 0.09313257783651352\n",
      "Epoch 2680: train loss: 0.09295028448104858\n",
      "Epoch 2690: train loss: 0.09277412295341492\n",
      "Epoch 2700: train loss: 0.09260398894548416\n",
      "Epoch 2710: train loss: 0.09243959188461304\n",
      "Epoch 2720: train loss: 0.09228077530860901\n",
      "Epoch 2730: train loss: 0.09212735295295715\n",
      "Epoch 2740: train loss: 0.09197916090488434\n",
      "Epoch 2750: train loss: 0.09183599054813385\n",
      "Epoch 2760: train loss: 0.09169767796993256\n",
      "Epoch 2770: train loss: 0.09156407415866852\n",
      "Epoch 2780: train loss: 0.091435007750988\n",
      "Epoch 2790: train loss: 0.09131030738353729\n",
      "Epoch 2800: train loss: 0.09118986129760742\n",
      "Epoch 2810: train loss: 0.09107346087694168\n",
      "Epoch 2820: train loss: 0.09096099436283112\n",
      "Epoch 2830: train loss: 0.09085231274366379\n",
      "Epoch 2840: train loss: 0.09074730426073074\n",
      "Epoch 2850: train loss: 0.09064579010009766\n",
      "Epoch 2860: train loss: 0.09054765850305557\n",
      "Epoch 2870: train loss: 0.09045278280973434\n",
      "Epoch 2880: train loss: 0.09036105871200562\n",
      "Epoch 2890: train loss: 0.09027233719825745\n",
      "Epoch 2900: train loss: 0.09018652886152267\n",
      "Epoch 2910: train loss: 0.09010350704193115\n",
      "Epoch 2920: train loss: 0.09002315253019333\n",
      "Epoch 2930: train loss: 0.08994539827108383\n",
      "Epoch 2940: train loss: 0.08987011015415192\n",
      "Epoch 2950: train loss: 0.08979719132184982\n",
      "Epoch 2960: train loss: 0.08972655981779099\n",
      "Epoch 2970: train loss: 0.08965810388326645\n",
      "Epoch 2980: train loss: 0.08959177136421204\n",
      "Epoch 2990: train loss: 0.08952740579843521\n",
      "Epoch 3000: train loss: 0.08946502208709717\n",
      "Epoch 3010: train loss: 0.08940449357032776\n",
      "Epoch 3020: train loss: 0.08934572339057922\n",
      "Epoch 3030: train loss: 0.0892886221408844\n",
      "Epoch 3040: train loss: 0.08923313021659851\n",
      "Epoch 3050: train loss: 0.08917921036481857\n",
      "Epoch 3060: train loss: 0.08912675827741623\n",
      "Epoch 3070: train loss: 0.0890757143497467\n",
      "Epoch 3080: train loss: 0.08902602642774582\n",
      "Epoch 3090: train loss: 0.0889776423573494\n",
      "Epoch 3100: train loss: 0.08893048018217087\n",
      "Epoch 3110: train loss: 0.08888448774814606\n",
      "Epoch 3120: train loss: 0.08883961290121078\n",
      "Epoch 3130: train loss: 0.08879581838846207\n",
      "Epoch 3140: train loss: 0.08875302970409393\n",
      "Epoch 3150: train loss: 0.0887112021446228\n",
      "Epoch 3160: train loss: 0.08867029845714569\n",
      "Epoch 3170: train loss: 0.08863027393817902\n",
      "Epoch 3180: train loss: 0.088591068983078\n",
      "Epoch 3190: train loss: 0.08855266124010086\n",
      "Epoch 3200: train loss: 0.08851499110460281\n",
      "Epoch 3210: train loss: 0.08847802877426147\n",
      "Epoch 3220: train loss: 0.08844175189733505\n",
      "Epoch 3230: train loss: 0.08840609341859818\n",
      "Epoch 3240: train loss: 0.08837103843688965\n",
      "Epoch 3250: train loss: 0.08833654969930649\n",
      "Epoch 3260: train loss: 0.0883025974035263\n",
      "Epoch 3270: train loss: 0.08826915174722672\n",
      "Epoch 3280: train loss: 0.08823617547750473\n",
      "Epoch 3290: train loss: 0.08820365369319916\n",
      "Epoch 3300: train loss: 0.08817154914140701\n",
      "Epoch 3310: train loss: 0.0881398543715477\n",
      "Epoch 3320: train loss: 0.08810850232839584\n",
      "Epoch 3330: train loss: 0.08807751536369324\n",
      "Epoch 3340: train loss: 0.0880468413233757\n",
      "Epoch 3350: train loss: 0.08801648765802383\n",
      "Epoch 3360: train loss: 0.08798640221357346\n",
      "Epoch 3370: train loss: 0.08795659989118576\n",
      "Epoch 3380: train loss: 0.08792701363563538\n",
      "Epoch 3390: train loss: 0.0878976583480835\n",
      "Epoch 3400: train loss: 0.08786852657794952\n",
      "Epoch 3410: train loss: 0.08783958107233047\n",
      "Epoch 3420: train loss: 0.08781079947948456\n",
      "Epoch 3430: train loss: 0.08778218179941177\n",
      "Epoch 3440: train loss: 0.08775371313095093\n",
      "Epoch 3450: train loss: 0.08772537112236023\n",
      "Epoch 3460: train loss: 0.08769714087247849\n",
      "Epoch 3470: train loss: 0.08766904473304749\n",
      "Epoch 3480: train loss: 0.08764103055000305\n",
      "Epoch 3490: train loss: 0.08761309087276459\n",
      "Epoch 3500: train loss: 0.08758524060249329\n",
      "Epoch 3510: train loss: 0.08755744993686676\n",
      "Epoch 3520: train loss: 0.08752969652414322\n",
      "Epoch 3530: train loss: 0.08750201016664505\n",
      "Epoch 3540: train loss: 0.08747434616088867\n",
      "Epoch 3550: train loss: 0.08744671940803528\n",
      "Epoch 3560: train loss: 0.08741910755634308\n",
      "Epoch 3570: train loss: 0.08739151060581207\n",
      "Epoch 3580: train loss: 0.08736389875411987\n",
      "Epoch 3590: train loss: 0.08733628690242767\n",
      "Epoch 3600: train loss: 0.08730868250131607\n",
      "Epoch 3610: train loss: 0.08728106319904327\n",
      "Epoch 3620: train loss: 0.08725341409444809\n",
      "Epoch 3630: train loss: 0.08722574263811111\n",
      "Epoch 3640: train loss: 0.08719804883003235\n",
      "Epoch 3650: train loss: 0.08717029541730881\n",
      "Epoch 3660: train loss: 0.08714251220226288\n",
      "Epoch 3670: train loss: 0.08711469173431396\n",
      "Epoch 3680: train loss: 0.08708681166172028\n",
      "Epoch 3690: train loss: 0.087058886885643\n",
      "Epoch 3700: train loss: 0.08703088760375977\n",
      "Epoch 3710: train loss: 0.08700283616781235\n",
      "Epoch 3720: train loss: 0.08697471767663956\n",
      "Epoch 3730: train loss: 0.0869465246796608\n",
      "Epoch 3740: train loss: 0.08691827207803726\n",
      "Epoch 3750: train loss: 0.08688995242118835\n",
      "Epoch 3760: train loss: 0.08686152845621109\n",
      "Epoch 3770: train loss: 0.08683304488658905\n",
      "Epoch 3780: train loss: 0.08680447190999985\n",
      "Epoch 3790: train loss: 0.08677582442760468\n",
      "Epoch 3800: train loss: 0.08674708008766174\n",
      "Epoch 3810: train loss: 0.08671825379133224\n",
      "Epoch 3820: train loss: 0.08668933808803558\n",
      "Epoch 3830: train loss: 0.08666031807661057\n",
      "Epoch 3840: train loss: 0.08663120120763779\n",
      "Epoch 3850: train loss: 0.08660200983285904\n",
      "Epoch 3860: train loss: 0.08657269924879074\n",
      "Epoch 3870: train loss: 0.08654329925775528\n",
      "Epoch 3880: train loss: 0.08651380240917206\n",
      "Epoch 3890: train loss: 0.08648420125246048\n",
      "Epoch 3900: train loss: 0.08645449578762054\n",
      "Epoch 3910: train loss: 0.08642468601465225\n",
      "Epoch 3920: train loss: 0.086394764482975\n",
      "Epoch 3930: train loss: 0.0863647311925888\n",
      "Epoch 3940: train loss: 0.08633460849523544\n",
      "Epoch 3950: train loss: 0.08630437403917313\n",
      "Epoch 3960: train loss: 0.08627405017614365\n",
      "Epoch 3970: train loss: 0.08624359965324402\n",
      "Epoch 3980: train loss: 0.08621301501989365\n",
      "Epoch 3990: train loss: 0.08618232607841492\n",
      "Epoch 4000: train loss: 0.08615154027938843\n",
      "Epoch 4010: train loss: 0.0861206203699112\n",
      "Epoch 4020: train loss: 0.0860896110534668\n",
      "Epoch 4030: train loss: 0.08605846017599106\n",
      "Epoch 4040: train loss: 0.08602721989154816\n",
      "Epoch 4050: train loss: 0.08599584549665451\n",
      "Epoch 4060: train loss: 0.08596435189247131\n",
      "Epoch 4070: train loss: 0.08593276143074036\n",
      "Epoch 4080: train loss: 0.08590103685855865\n",
      "Epoch 4090: train loss: 0.0858692079782486\n",
      "Epoch 4100: train loss: 0.08583724498748779\n",
      "Epoch 4110: train loss: 0.08580516278743744\n",
      "Epoch 4120: train loss: 0.08577299118041992\n",
      "Epoch 4130: train loss: 0.08574066311120987\n",
      "Epoch 4140: train loss: 0.08570823818445206\n",
      "Epoch 4150: train loss: 0.0856756791472435\n",
      "Epoch 4160: train loss: 0.0856429934501648\n",
      "Epoch 4170: train loss: 0.08561019599437714\n",
      "Epoch 4180: train loss: 0.08557728677988052\n",
      "Epoch 4190: train loss: 0.08554424345493317\n",
      "Epoch 4200: train loss: 0.08551107347011566\n",
      "Epoch 4210: train loss: 0.0854777917265892\n",
      "Epoch 4220: train loss: 0.0854443907737732\n",
      "Epoch 4230: train loss: 0.08541084080934525\n",
      "Epoch 4240: train loss: 0.08537719398736954\n",
      "Epoch 4250: train loss: 0.08534340560436249\n",
      "Epoch 4260: train loss: 0.08530950546264648\n",
      "Epoch 4270: train loss: 0.08527547866106033\n",
      "Epoch 4280: train loss: 0.08524131774902344\n",
      "Epoch 4290: train loss: 0.0852070301771164\n",
      "Epoch 4300: train loss: 0.085172638297081\n",
      "Epoch 4310: train loss: 0.08513811230659485\n",
      "Epoch 4320: train loss: 0.08510345965623856\n",
      "Epoch 4330: train loss: 0.08506868034601212\n",
      "Epoch 4340: train loss: 0.08503376692533493\n",
      "Epoch 4350: train loss: 0.08499874174594879\n",
      "Epoch 4360: train loss: 0.0849635899066925\n",
      "Epoch 4370: train loss: 0.08492830395698547\n",
      "Epoch 4380: train loss: 0.0848928838968277\n",
      "Epoch 4390: train loss: 0.08485734462738037\n",
      "Epoch 4400: train loss: 0.0848216786980629\n",
      "Epoch 4410: train loss: 0.08478588610887527\n",
      "Epoch 4420: train loss: 0.0847499668598175\n",
      "Epoch 4430: train loss: 0.08471392095088959\n",
      "Epoch 4440: train loss: 0.08467774093151093\n",
      "Epoch 4450: train loss: 0.08464144170284271\n",
      "Epoch 4460: train loss: 0.08460500836372375\n",
      "Epoch 4470: train loss: 0.08456844091415405\n",
      "Epoch 4480: train loss: 0.0845317617058754\n",
      "Epoch 4490: train loss: 0.084494948387146\n",
      "Epoch 4500: train loss: 0.08445798605680466\n",
      "Epoch 4510: train loss: 0.08442091941833496\n",
      "Epoch 4520: train loss: 0.08438371121883392\n",
      "Epoch 4530: train loss: 0.08434637635946274\n",
      "Epoch 4540: train loss: 0.0843089297413826\n",
      "Epoch 4550: train loss: 0.08427133411169052\n",
      "Epoch 4560: train loss: 0.0842335969209671\n",
      "Epoch 4570: train loss: 0.08419575542211533\n",
      "Epoch 4580: train loss: 0.08415777236223221\n",
      "Epoch 4590: train loss: 0.08411966264247894\n",
      "Epoch 4600: train loss: 0.08408141881227493\n",
      "Epoch 4610: train loss: 0.08404304087162018\n",
      "Epoch 4620: train loss: 0.08400453627109528\n",
      "Epoch 4630: train loss: 0.08396591246128082\n",
      "Epoch 4640: train loss: 0.08392713963985443\n",
      "Epoch 4650: train loss: 0.08388825505971909\n",
      "Epoch 4660: train loss: 0.083849236369133\n",
      "Epoch 4670: train loss: 0.08381007611751556\n",
      "Epoch 4680: train loss: 0.08377078920602798\n",
      "Epoch 4690: train loss: 0.08373137563467026\n",
      "Epoch 4700: train loss: 0.08369182050228119\n",
      "Epoch 4710: train loss: 0.08365215361118317\n",
      "Epoch 4720: train loss: 0.0836123451590538\n",
      "Epoch 4730: train loss: 0.0835724025964737\n",
      "Epoch 4740: train loss: 0.08353234082460403\n",
      "Epoch 4750: train loss: 0.08349212259054184\n",
      "Epoch 4760: train loss: 0.08345180004835129\n",
      "Epoch 4770: train loss: 0.0834113284945488\n",
      "Epoch 4780: train loss: 0.08337073028087616\n",
      "Epoch 4790: train loss: 0.08333001285791397\n",
      "Epoch 4800: train loss: 0.08328915387392044\n",
      "Epoch 4810: train loss: 0.08324815332889557\n",
      "Epoch 4820: train loss: 0.08320703357458115\n",
      "Epoch 4830: train loss: 0.08316577970981598\n",
      "Epoch 4840: train loss: 0.08312439918518066\n",
      "Epoch 4850: train loss: 0.0830828845500946\n",
      "Epoch 4860: train loss: 0.0830412358045578\n",
      "Epoch 4870: train loss: 0.08299945294857025\n",
      "Epoch 4880: train loss: 0.08295754343271255\n",
      "Epoch 4890: train loss: 0.08291549235582352\n",
      "Epoch 4900: train loss: 0.08287332206964493\n",
      "Epoch 4910: train loss: 0.0828310176730156\n",
      "Epoch 4920: train loss: 0.08278857916593552\n",
      "Epoch 4930: train loss: 0.08274601399898529\n",
      "Epoch 4940: train loss: 0.08270332217216492\n",
      "Epoch 4950: train loss: 0.0826604813337326\n",
      "Epoch 4960: train loss: 0.08261752873659134\n",
      "Epoch 4970: train loss: 0.08257443457841873\n",
      "Epoch 4980: train loss: 0.08253119885921478\n",
      "Epoch 4990: train loss: 0.08248785138130188\n",
      "Epoch 5000: train loss: 0.08244436234235764\n",
      "Epoch 5010: train loss: 0.08240074664354324\n",
      "Epoch 5020: train loss: 0.08235698193311691\n",
      "Epoch 5030: train loss: 0.08231309801340103\n",
      "Epoch 5040: train loss: 0.0822690948843956\n",
      "Epoch 5050: train loss: 0.08222495764493942\n",
      "Epoch 5060: train loss: 0.0821806788444519\n",
      "Epoch 5070: train loss: 0.08213627338409424\n",
      "Epoch 5080: train loss: 0.08209174871444702\n",
      "Epoch 5090: train loss: 0.08204708248376846\n",
      "Epoch 5100: train loss: 0.08200228214263916\n",
      "Epoch 5110: train loss: 0.08195735514163971\n",
      "Epoch 5120: train loss: 0.08191230893135071\n",
      "Epoch 5130: train loss: 0.08186712116003036\n",
      "Epoch 5140: train loss: 0.08182181417942047\n",
      "Epoch 5150: train loss: 0.08177635818719864\n",
      "Epoch 5160: train loss: 0.08173079043626785\n",
      "Epoch 5170: train loss: 0.08168509602546692\n",
      "Epoch 5180: train loss: 0.08163925260305405\n",
      "Epoch 5190: train loss: 0.08159330487251282\n",
      "Epoch 5200: train loss: 0.08154720813035965\n",
      "Epoch 5210: train loss: 0.08150097727775574\n",
      "Epoch 5220: train loss: 0.08145463466644287\n",
      "Epoch 5230: train loss: 0.08140815794467926\n",
      "Epoch 5240: train loss: 0.0813615620136261\n",
      "Epoch 5250: train loss: 0.08131483942270279\n",
      "Epoch 5260: train loss: 0.08126796782016754\n",
      "Epoch 5270: train loss: 0.08122098445892334\n",
      "Epoch 5280: train loss: 0.08117387443780899\n",
      "Epoch 5290: train loss: 0.0811266303062439\n",
      "Epoch 5300: train loss: 0.08107927441596985\n",
      "Epoch 5310: train loss: 0.08103176951408386\n",
      "Epoch 5320: train loss: 0.08098414540290833\n",
      "Epoch 5330: train loss: 0.08093640953302383\n",
      "Epoch 5340: train loss: 0.0808885246515274\n",
      "Epoch 5350: train loss: 0.08084052801132202\n",
      "Epoch 5360: train loss: 0.08079241216182709\n",
      "Epoch 5370: train loss: 0.080744169652462\n",
      "Epoch 5380: train loss: 0.08069580048322678\n",
      "Epoch 5390: train loss: 0.08064728230237961\n",
      "Epoch 5400: train loss: 0.08059867471456528\n",
      "Epoch 5410: train loss: 0.0805499255657196\n",
      "Epoch 5420: train loss: 0.08050105720758438\n",
      "Epoch 5430: train loss: 0.08045206218957901\n",
      "Epoch 5440: train loss: 0.08040294796228409\n",
      "Epoch 5450: train loss: 0.08035371452569962\n",
      "Epoch 5460: train loss: 0.0803043469786644\n",
      "Epoch 5470: train loss: 0.08025486022233963\n",
      "Epoch 5480: train loss: 0.08020525425672531\n",
      "Epoch 5490: train loss: 0.08015554398298264\n",
      "Epoch 5500: train loss: 0.08010568469762802\n",
      "Epoch 5510: train loss: 0.08005572110414505\n",
      "Epoch 5520: train loss: 0.08000562340021133\n",
      "Epoch 5530: train loss: 0.07995543628931046\n",
      "Epoch 5540: train loss: 0.07990510016679764\n",
      "Epoch 5550: train loss: 0.07985465973615646\n",
      "Epoch 5560: train loss: 0.07980409264564514\n",
      "Epoch 5570: train loss: 0.07975339889526367\n",
      "Epoch 5580: train loss: 0.07970260828733444\n",
      "Epoch 5590: train loss: 0.07965169101953506\n",
      "Epoch 5600: train loss: 0.07960065454244614\n",
      "Epoch 5610: train loss: 0.07954950630664825\n",
      "Epoch 5620: train loss: 0.07949823886156082\n",
      "Epoch 5630: train loss: 0.07944684475660324\n",
      "Epoch 5640: train loss: 0.0793953612446785\n",
      "Epoch 5650: train loss: 0.0793437510728836\n",
      "Epoch 5660: train loss: 0.07929202169179916\n",
      "Epoch 5670: train loss: 0.07924018800258636\n",
      "Epoch 5680: train loss: 0.07918822765350342\n",
      "Epoch 5690: train loss: 0.07913616299629211\n",
      "Epoch 5700: train loss: 0.07908397912979126\n",
      "Epoch 5710: train loss: 0.07903169095516205\n",
      "Epoch 5720: train loss: 0.07897929102182388\n",
      "Epoch 5730: train loss: 0.07892677932977676\n",
      "Epoch 5740: train loss: 0.07887416332960129\n",
      "Epoch 5750: train loss: 0.07882143557071686\n",
      "Epoch 5760: train loss: 0.07876860350370407\n",
      "Epoch 5770: train loss: 0.07871565967798233\n",
      "Epoch 5780: train loss: 0.07866259664297104\n",
      "Epoch 5790: train loss: 0.07860944420099258\n",
      "Epoch 5800: train loss: 0.07855616509914398\n",
      "Epoch 5810: train loss: 0.07850278913974762\n",
      "Epoch 5820: train loss: 0.0784493163228035\n",
      "Epoch 5830: train loss: 0.07839573919773102\n",
      "Epoch 5840: train loss: 0.07834204286336899\n",
      "Epoch 5850: train loss: 0.0782882571220398\n",
      "Epoch 5860: train loss: 0.07823435962200165\n",
      "Epoch 5870: train loss: 0.07818035781383514\n",
      "Epoch 5880: train loss: 0.07812625914812088\n",
      "Epoch 5890: train loss: 0.07807206362485886\n",
      "Epoch 5900: train loss: 0.07801777869462967\n",
      "Epoch 5910: train loss: 0.07796336710453033\n",
      "Epoch 5920: train loss: 0.07790887355804443\n",
      "Epoch 5930: train loss: 0.07785427570343018\n",
      "Epoch 5940: train loss: 0.07779958844184875\n",
      "Epoch 5950: train loss: 0.07774478942155838\n",
      "Epoch 5960: train loss: 0.07768991589546204\n",
      "Epoch 5970: train loss: 0.07763493061065674\n",
      "Epoch 5980: train loss: 0.07757985591888428\n",
      "Epoch 5990: train loss: 0.07752468436956406\n",
      "Epoch 6000: train loss: 0.07746942341327667\n",
      "Epoch 6010: train loss: 0.07741408050060272\n",
      "Epoch 6020: train loss: 0.07735863327980042\n",
      "Epoch 6030: train loss: 0.07730309665203094\n",
      "Epoch 6040: train loss: 0.07724747806787491\n",
      "Epoch 6050: train loss: 0.07719176262617111\n",
      "Epoch 6060: train loss: 0.07713596522808075\n",
      "Epoch 6070: train loss: 0.07708007097244263\n",
      "Epoch 6080: train loss: 0.07702409476041794\n",
      "Epoch 6090: train loss: 0.07696803659200668\n",
      "Epoch 6100: train loss: 0.07691190391778946\n",
      "Epoch 6110: train loss: 0.07685566693544388\n",
      "Epoch 6120: train loss: 0.07679936289787292\n",
      "Epoch 6130: train loss: 0.07674296200275421\n",
      "Epoch 6140: train loss: 0.07668648660182953\n",
      "Epoch 6150: train loss: 0.07662992924451828\n",
      "Epoch 6160: train loss: 0.07657330483198166\n",
      "Epoch 6170: train loss: 0.07651659846305847\n",
      "Epoch 6180: train loss: 0.07645981013774872\n",
      "Epoch 6190: train loss: 0.0764029324054718\n",
      "Epoch 6200: train loss: 0.07634599506855011\n",
      "Epoch 6210: train loss: 0.07628898322582245\n",
      "Epoch 6220: train loss: 0.07623188942670822\n",
      "Epoch 6230: train loss: 0.07617472112178802\n",
      "Epoch 6240: train loss: 0.07611749321222305\n",
      "Epoch 6250: train loss: 0.07606019079685211\n",
      "Epoch 6260: train loss: 0.0760028138756752\n",
      "Epoch 6270: train loss: 0.07594536244869232\n",
      "Epoch 6280: train loss: 0.07588786631822586\n",
      "Epoch 6290: train loss: 0.07583027333021164\n",
      "Epoch 6300: train loss: 0.07577263563871384\n",
      "Epoch 6310: train loss: 0.07571493089199066\n",
      "Epoch 6320: train loss: 0.07565716654062271\n",
      "Epoch 6330: train loss: 0.07559932768344879\n",
      "Epoch 6340: train loss: 0.0755414366722107\n",
      "Epoch 6350: train loss: 0.07548348605632782\n",
      "Epoch 6360: train loss: 0.07542546838521957\n",
      "Epoch 6370: train loss: 0.07536739110946655\n",
      "Epoch 6380: train loss: 0.07530926167964935\n",
      "Epoch 6390: train loss: 0.07525106519460678\n",
      "Epoch 6400: train loss: 0.07519283890724182\n",
      "Epoch 6410: train loss: 0.07513454556465149\n",
      "Epoch 6420: train loss: 0.07507620006799698\n",
      "Epoch 6430: train loss: 0.07501779496669769\n",
      "Epoch 6440: train loss: 0.07495935261249542\n",
      "Epoch 6450: train loss: 0.07490085065364838\n",
      "Epoch 6460: train loss: 0.07484231144189835\n",
      "Epoch 6470: train loss: 0.07478372752666473\n",
      "Epoch 6480: train loss: 0.07472508400678635\n",
      "Epoch 6490: train loss: 0.07466639578342438\n",
      "Epoch 6500: train loss: 0.07460768520832062\n",
      "Epoch 6510: train loss: 0.07454891502857208\n",
      "Epoch 6520: train loss: 0.07449011504650116\n",
      "Epoch 6530: train loss: 0.07443127036094666\n",
      "Epoch 6540: train loss: 0.07437238842248917\n",
      "Epoch 6550: train loss: 0.07431346923112869\n",
      "Epoch 6560: train loss: 0.07425451278686523\n",
      "Epoch 6570: train loss: 0.07419553399085999\n",
      "Epoch 6580: train loss: 0.07413651049137115\n",
      "Epoch 6590: train loss: 0.07407744973897934\n",
      "Epoch 6600: train loss: 0.07401836663484573\n",
      "Epoch 6610: train loss: 0.07395925372838974\n",
      "Epoch 6620: train loss: 0.07390011847019196\n",
      "Epoch 6630: train loss: 0.07384095340967178\n",
      "Epoch 6640: train loss: 0.07378175854682922\n",
      "Epoch 6650: train loss: 0.07372254878282547\n",
      "Epoch 6660: train loss: 0.07366330176591873\n",
      "Epoch 6670: train loss: 0.0736040472984314\n",
      "Epoch 6680: train loss: 0.07354477792978287\n",
      "Epoch 6690: train loss: 0.07348547875881195\n",
      "Epoch 6700: train loss: 0.07342617213726044\n",
      "Epoch 6710: train loss: 0.07336684316396713\n",
      "Epoch 6720: train loss: 0.07330749928951263\n",
      "Epoch 6730: train loss: 0.07324814796447754\n",
      "Epoch 6740: train loss: 0.07318876683712006\n",
      "Epoch 6750: train loss: 0.07312940061092377\n",
      "Epoch 6760: train loss: 0.07307002693414688\n",
      "Epoch 6770: train loss: 0.0730106309056282\n",
      "Epoch 6780: train loss: 0.07295124232769012\n",
      "Epoch 6790: train loss: 0.07289183139801025\n",
      "Epoch 6800: train loss: 0.07283244282007217\n",
      "Epoch 6810: train loss: 0.0727730318903923\n",
      "Epoch 6820: train loss: 0.07271362841129303\n",
      "Epoch 6830: train loss: 0.07265423983335495\n",
      "Epoch 6840: train loss: 0.07259482890367508\n",
      "Epoch 6850: train loss: 0.0725354477763176\n",
      "Epoch 6860: train loss: 0.07247605174779892\n",
      "Epoch 6870: train loss: 0.07241667807102203\n",
      "Epoch 6880: train loss: 0.07235731184482574\n",
      "Epoch 6890: train loss: 0.07229796051979065\n",
      "Epoch 6900: train loss: 0.07223861664533615\n",
      "Epoch 6910: train loss: 0.07217929512262344\n",
      "Epoch 6920: train loss: 0.07211996614933014\n",
      "Epoch 6930: train loss: 0.07206066697835922\n",
      "Epoch 6940: train loss: 0.0720013976097107\n",
      "Epoch 6950: train loss: 0.07194213569164276\n",
      "Epoch 6960: train loss: 0.07188290357589722\n",
      "Epoch 6970: train loss: 0.07182369381189346\n",
      "Epoch 6980: train loss: 0.0717644989490509\n",
      "Epoch 6990: train loss: 0.07170534133911133\n",
      "Epoch 7000: train loss: 0.07164620608091354\n",
      "Epoch 7010: train loss: 0.07158711552619934\n",
      "Epoch 7020: train loss: 0.07152803242206573\n",
      "Epoch 7030: train loss: 0.07146899402141571\n",
      "Epoch 7040: train loss: 0.07140999287366867\n",
      "Epoch 7050: train loss: 0.07135102897882462\n",
      "Epoch 7060: train loss: 0.07129209488630295\n",
      "Epoch 7070: train loss: 0.07123320549726486\n",
      "Epoch 7080: train loss: 0.07117434591054916\n",
      "Epoch 7090: train loss: 0.07111554592847824\n",
      "Epoch 7100: train loss: 0.0710567757487297\n",
      "Epoch 7110: train loss: 0.07099807262420654\n",
      "Epoch 7120: train loss: 0.07093940675258636\n",
      "Epoch 7130: train loss: 0.07088078558444977\n",
      "Epoch 7140: train loss: 0.07082221657037735\n",
      "Epoch 7150: train loss: 0.07076369971036911\n",
      "Epoch 7160: train loss: 0.07070524245500565\n",
      "Epoch 7170: train loss: 0.07064682990312576\n",
      "Epoch 7180: train loss: 0.07058849185705185\n",
      "Epoch 7190: train loss: 0.07053019106388092\n",
      "Epoch 7200: train loss: 0.07047197222709656\n",
      "Epoch 7210: train loss: 0.07041380554437637\n",
      "Epoch 7220: train loss: 0.07035570591688156\n",
      "Epoch 7230: train loss: 0.07029767334461212\n",
      "Epoch 7240: train loss: 0.07023970782756805\n",
      "Epoch 7250: train loss: 0.07018180936574936\n",
      "Epoch 7260: train loss: 0.07012397795915604\n",
      "Epoch 7270: train loss: 0.07006622105836868\n",
      "Epoch 7280: train loss: 0.07000855356454849\n",
      "Epoch 7290: train loss: 0.06995094567537308\n",
      "Epoch 7300: train loss: 0.06989341974258423\n",
      "Epoch 7310: train loss: 0.06983598321676254\n",
      "Epoch 7320: train loss: 0.06977861374616623\n",
      "Epoch 7330: train loss: 0.06972133368253708\n",
      "Epoch 7340: train loss: 0.0696641355752945\n",
      "Epoch 7350: train loss: 0.06960702687501907\n",
      "Epoch 7360: train loss: 0.06955001503229141\n",
      "Epoch 7370: train loss: 0.06949307024478912\n",
      "Epoch 7380: train loss: 0.06943624466657639\n",
      "Epoch 7390: train loss: 0.06937949359416962\n",
      "Epoch 7400: train loss: 0.06932283937931061\n",
      "Epoch 7410: train loss: 0.06926628202199936\n",
      "Epoch 7420: train loss: 0.06920984387397766\n",
      "Epoch 7430: train loss: 0.06915347278118134\n",
      "Epoch 7440: train loss: 0.06909722089767456\n",
      "Epoch 7450: train loss: 0.06904106587171555\n",
      "Epoch 7460: train loss: 0.06898502260446548\n",
      "Epoch 7470: train loss: 0.06892909109592438\n",
      "Epoch 7480: train loss: 0.06887324899435043\n",
      "Epoch 7490: train loss: 0.06881754100322723\n",
      "Epoch 7500: train loss: 0.0687619298696518\n",
      "Epoch 7510: train loss: 0.0687064379453659\n",
      "Epoch 7520: train loss: 0.06865105777978897\n",
      "Epoch 7530: train loss: 0.06859578937292099\n",
      "Epoch 7540: train loss: 0.06854065507650375\n",
      "Epoch 7550: train loss: 0.06848563253879547\n",
      "Epoch 7560: train loss: 0.06843073666095734\n",
      "Epoch 7570: train loss: 0.06837595999240875\n",
      "Epoch 7580: train loss: 0.06832130998373032\n",
      "Epoch 7590: train loss: 0.06826677918434143\n",
      "Epoch 7600: train loss: 0.06821238994598389\n",
      "Epoch 7610: train loss: 0.06815812736749649\n",
      "Epoch 7620: train loss: 0.06810399144887924\n",
      "Epoch 7630: train loss: 0.06804998964071274\n",
      "Epoch 7640: train loss: 0.06799612939357758\n",
      "Epoch 7650: train loss: 0.06794240325689316\n",
      "Epoch 7660: train loss: 0.06788881123065948\n",
      "Epoch 7670: train loss: 0.06783536821603775\n",
      "Epoch 7680: train loss: 0.06778206676244736\n",
      "Epoch 7690: train loss: 0.0677289068698883\n",
      "Epoch 7700: train loss: 0.0676758885383606\n",
      "Epoch 7710: train loss: 0.06762301921844482\n",
      "Epoch 7720: train loss: 0.06757030636072159\n",
      "Epoch 7730: train loss: 0.0675177350640297\n",
      "Epoch 7740: train loss: 0.06746532022953033\n",
      "Epoch 7750: train loss: 0.06741305440664291\n",
      "Epoch 7760: train loss: 0.06736094504594803\n",
      "Epoch 7770: train loss: 0.06730899959802628\n",
      "Epoch 7780: train loss: 0.06725719571113586\n",
      "Epoch 7790: train loss: 0.06720556318759918\n",
      "Epoch 7800: train loss: 0.06715409457683563\n",
      "Epoch 7810: train loss: 0.06710277497768402\n",
      "Epoch 7820: train loss: 0.06705162674188614\n",
      "Epoch 7830: train loss: 0.06700065732002258\n",
      "Epoch 7840: train loss: 0.06694983690977097\n",
      "Epoch 7850: train loss: 0.06689920276403427\n",
      "Epoch 7860: train loss: 0.06684871762990952\n",
      "Epoch 7870: train loss: 0.06679841130971909\n",
      "Epoch 7880: train loss: 0.06674829125404358\n",
      "Epoch 7890: train loss: 0.06669832766056061\n",
      "Epoch 7900: train loss: 0.06664855033159256\n",
      "Epoch 7910: train loss: 0.06659895926713943\n",
      "Epoch 7920: train loss: 0.06654953956604004\n",
      "Epoch 7930: train loss: 0.06650029122829437\n",
      "Epoch 7940: train loss: 0.06645123660564423\n",
      "Epoch 7950: train loss: 0.0664023607969284\n",
      "Epoch 7960: train loss: 0.06635367125272751\n",
      "Epoch 7970: train loss: 0.06630516797304153\n",
      "Epoch 7980: train loss: 0.06625684350728989\n",
      "Epoch 7990: train loss: 0.06620872020721436\n",
      "Epoch 8000: train loss: 0.06616078317165375\n",
      "Epoch 8010: train loss: 0.06611304730176926\n",
      "Epoch 8020: train loss: 0.0660654827952385\n",
      "Epoch 8030: train loss: 0.06601814180612564\n",
      "Epoch 8040: train loss: 0.06597096472978592\n",
      "Epoch 8050: train loss: 0.0659240111708641\n",
      "Epoch 8060: train loss: 0.06587724387645721\n",
      "Epoch 8070: train loss: 0.06583067029714584\n",
      "Epoch 8080: train loss: 0.06578431278467178\n",
      "Epoch 8090: train loss: 0.06573814898729324\n",
      "Epoch 8100: train loss: 0.06569219380617142\n",
      "Epoch 8110: train loss: 0.06564643234014511\n",
      "Epoch 8120: train loss: 0.06560087949037552\n",
      "Epoch 8130: train loss: 0.06555554270744324\n",
      "Epoch 8140: train loss: 0.06551039963960648\n",
      "Epoch 8150: train loss: 0.06546548008918762\n",
      "Epoch 8160: train loss: 0.06542076170444489\n",
      "Epoch 8170: train loss: 0.06537625938653946\n",
      "Epoch 8180: train loss: 0.06533197313547134\n",
      "Epoch 8190: train loss: 0.06528789550065994\n",
      "Epoch 8200: train loss: 0.06524404138326645\n",
      "Epoch 8210: train loss: 0.06520039588212967\n",
      "Epoch 8220: train loss: 0.0651569738984108\n",
      "Epoch 8230: train loss: 0.06511376798152924\n",
      "Epoch 8240: train loss: 0.06507077068090439\n",
      "Epoch 8250: train loss: 0.06502800434827805\n",
      "Epoch 8260: train loss: 0.06498544663190842\n",
      "Epoch 8270: train loss: 0.06494311988353729\n",
      "Epoch 8280: train loss: 0.06490101665258408\n",
      "Epoch 8290: train loss: 0.06485914438962936\n",
      "Epoch 8300: train loss: 0.06481747329235077\n",
      "Epoch 8310: train loss: 0.06477604806423187\n",
      "Epoch 8320: train loss: 0.06473483890295029\n",
      "Epoch 8330: train loss: 0.0646938681602478\n",
      "Epoch 8340: train loss: 0.06465312093496323\n",
      "Epoch 8350: train loss: 0.06461260467767715\n",
      "Epoch 8360: train loss: 0.06457231193780899\n",
      "Epoch 8370: train loss: 0.06453225761651993\n",
      "Epoch 8380: train loss: 0.06449242681264877\n",
      "Epoch 8390: train loss: 0.06445282697677612\n",
      "Epoch 8400: train loss: 0.06441346555948257\n",
      "Epoch 8410: train loss: 0.06437435001134872\n",
      "Epoch 8420: train loss: 0.06433545798063278\n",
      "Epoch 8430: train loss: 0.06429679691791534\n",
      "Epoch 8440: train loss: 0.06425837427377701\n",
      "Epoch 8450: train loss: 0.06422018259763718\n",
      "Epoch 8460: train loss: 0.06418222934007645\n",
      "Epoch 8470: train loss: 0.06414452195167542\n",
      "Epoch 8480: train loss: 0.06410705298185349\n",
      "Epoch 8490: train loss: 0.06406981498003006\n",
      "Epoch 8500: train loss: 0.06403282284736633\n",
      "Epoch 8510: train loss: 0.06399606913328171\n",
      "Epoch 8520: train loss: 0.06395954638719559\n",
      "Epoch 8530: train loss: 0.06392328441143036\n",
      "Epoch 8540: train loss: 0.06388724595308304\n",
      "Epoch 8550: train loss: 0.06385146826505661\n",
      "Epoch 8560: train loss: 0.06381591409444809\n",
      "Epoch 8570: train loss: 0.06378060579299927\n",
      "Epoch 8580: train loss: 0.06374554336071014\n",
      "Epoch 8590: train loss: 0.06371072679758072\n",
      "Epoch 8600: train loss: 0.06367615610361099\n",
      "Epoch 8610: train loss: 0.06364182382822037\n",
      "Epoch 8620: train loss: 0.06360773742198944\n",
      "Epoch 8630: train loss: 0.06357388943433762\n",
      "Epoch 8640: train loss: 0.06354029476642609\n",
      "Epoch 8650: train loss: 0.06350694596767426\n",
      "Epoch 8660: train loss: 0.06347383558750153\n",
      "Epoch 8670: train loss: 0.0634409710764885\n",
      "Epoch 8680: train loss: 0.06340835988521576\n",
      "Epoch 8690: train loss: 0.06337599456310272\n",
      "Epoch 8700: train loss: 0.06334387511014938\n",
      "Epoch 8710: train loss: 0.06331198662519455\n",
      "Epoch 8720: train loss: 0.06328035145998001\n",
      "Epoch 8730: train loss: 0.06324896961450577\n",
      "Epoch 8740: train loss: 0.06321784108877182\n",
      "Epoch 8750: train loss: 0.06318694353103638\n",
      "Epoch 8760: train loss: 0.06315629929304123\n",
      "Epoch 8770: train loss: 0.06312589347362518\n",
      "Epoch 8780: train loss: 0.06309573352336884\n",
      "Epoch 8790: train loss: 0.06306583434343338\n",
      "Epoch 8800: train loss: 0.06303617358207703\n",
      "Epoch 8810: train loss: 0.06300675123929977\n",
      "Epoch 8820: train loss: 0.06297758221626282\n",
      "Epoch 8830: train loss: 0.06294866651296616\n",
      "Epoch 8840: train loss: 0.0629199892282486\n",
      "Epoch 8850: train loss: 0.06289155036211014\n",
      "Epoch 8860: train loss: 0.06286336481571198\n",
      "Epoch 8870: train loss: 0.06283541768789291\n",
      "Epoch 8880: train loss: 0.06280770897865295\n",
      "Epoch 8890: train loss: 0.06278026103973389\n",
      "Epoch 8900: train loss: 0.06275305896997452\n",
      "Epoch 8910: train loss: 0.06272608786821365\n",
      "Epoch 8920: train loss: 0.06269935518503189\n",
      "Epoch 8930: train loss: 0.06267287582159042\n",
      "Epoch 8940: train loss: 0.06264663487672806\n",
      "Epoch 8950: train loss: 0.0626206323504448\n",
      "Epoch 8960: train loss: 0.06259488314390182\n",
      "Epoch 8970: train loss: 0.06256937235593796\n",
      "Epoch 8980: train loss: 0.06254409998655319\n",
      "Epoch 8990: train loss: 0.06251907348632812\n",
      "Epoch 9000: train loss: 0.06249427795410156\n",
      "Epoch 9010: train loss: 0.0624697245657444\n",
      "Epoch 9020: train loss: 0.06244540959596634\n",
      "Epoch 9030: train loss: 0.06242132559418678\n",
      "Epoch 9040: train loss: 0.062397487461566925\n",
      "Epoch 9050: train loss: 0.06237388402223587\n",
      "Epoch 9060: train loss: 0.062350522726774216\n",
      "Epoch 9070: train loss: 0.062327392399311066\n",
      "Epoch 9080: train loss: 0.06230449676513672\n",
      "Epoch 9090: train loss: 0.062281835824251175\n",
      "Epoch 9100: train loss: 0.062259405851364136\n",
      "Epoch 9110: train loss: 0.0622372142970562\n",
      "Epoch 9120: train loss: 0.06221524998545647\n",
      "Epoch 9130: train loss: 0.06219351664185524\n",
      "Epoch 9140: train loss: 0.06217202544212341\n",
      "Epoch 9150: train loss: 0.0621507428586483\n",
      "Epoch 9160: train loss: 0.062129706144332886\n",
      "Epoch 9170: train loss: 0.06210888549685478\n",
      "Epoch 9180: train loss: 0.06208830326795578\n",
      "Epoch 9190: train loss: 0.062067944556474686\n",
      "Epoch 9200: train loss: 0.0620478130877018\n",
      "Epoch 9210: train loss: 0.06202790141105652\n",
      "Epoch 9220: train loss: 0.06200820952653885\n",
      "Epoch 9230: train loss: 0.061988748610019684\n",
      "Epoch 9240: train loss: 0.06196950003504753\n",
      "Epoch 9250: train loss: 0.06195048242807388\n",
      "Epoch 9260: train loss: 0.061931684613227844\n",
      "Epoch 9270: train loss: 0.06191309168934822\n",
      "Epoch 9280: train loss: 0.061894722282886505\n",
      "Epoch 9290: train loss: 0.0618765763938427\n",
      "Epoch 9300: train loss: 0.0618586428463459\n",
      "Epoch 9310: train loss: 0.06184092536568642\n",
      "Epoch 9320: train loss: 0.06182341277599335\n",
      "Epoch 9330: train loss: 0.06180611625313759\n",
      "Epoch 9340: train loss: 0.06178903579711914\n",
      "Epoch 9350: train loss: 0.06177216395735741\n",
      "Epoch 9360: train loss: 0.06175549328327179\n",
      "Epoch 9370: train loss: 0.06173903867602348\n",
      "Epoch 9380: train loss: 0.06172279268503189\n",
      "Epoch 9390: train loss: 0.06170674040913582\n",
      "Epoch 9400: train loss: 0.06169090047478676\n",
      "Epoch 9410: train loss: 0.06167525053024292\n",
      "Epoch 9420: train loss: 0.06165982037782669\n",
      "Epoch 9430: train loss: 0.061644576489925385\n",
      "Epoch 9440: train loss: 0.06162954121828079\n",
      "Epoch 9450: train loss: 0.06161469966173172\n",
      "Epoch 9460: train loss: 0.061600055545568466\n",
      "Epoch 9470: train loss: 0.06158561259508133\n",
      "Epoch 9480: train loss: 0.06157134845852852\n",
      "Epoch 9490: train loss: 0.06155729293823242\n",
      "Epoch 9500: train loss: 0.06154341623187065\n",
      "Epoch 9510: train loss: 0.061529725790023804\n",
      "Epoch 9520: train loss: 0.06151624396443367\n",
      "Epoch 9530: train loss: 0.061502937227487564\n",
      "Epoch 9540: train loss: 0.06148981302976608\n",
      "Epoch 9550: train loss: 0.06147687882184982\n",
      "Epoch 9560: train loss: 0.06146412715315819\n",
      "Epoch 9570: train loss: 0.06145154684782028\n",
      "Epoch 9580: train loss: 0.061439160257577896\n",
      "Epoch 9590: train loss: 0.06142694875597954\n",
      "Epoch 9600: train loss: 0.06141491234302521\n",
      "Epoch 9610: train loss: 0.0614030547440052\n",
      "Epoch 9620: train loss: 0.06139137223362923\n",
      "Epoch 9630: train loss: 0.06137986108660698\n",
      "Epoch 9640: train loss: 0.06136851757764816\n",
      "Epoch 9650: train loss: 0.061357345432043076\n",
      "Epoch 9660: train loss: 0.06134634464979172\n",
      "Epoch 9670: train loss: 0.06133550405502319\n",
      "Epoch 9680: train loss: 0.0613248385488987\n",
      "Epoch 9690: train loss: 0.061314333230257034\n",
      "Epoch 9700: train loss: 0.061303991824388504\n",
      "Epoch 09708: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 9710: train loss: 0.061294522136449814\n",
      "Epoch 09715: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 9720: train loss: 0.06129233539104462\n",
      "Epoch 09721: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 09727: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 9730: train loss: 0.061291977763175964\n",
      "Epoch 09733: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 09739: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 9740: train loss: 0.06129193305969238\n",
      "Epoch 09745: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 9750: train loss: 0.06129192188382149\n",
      "Epoch 09751: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 9751, loss: 0.06129192188382149\n",
      "Predictions saved to results-mm-1-10.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
