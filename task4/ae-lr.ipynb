{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "#     x_train = scaler.transform(x_train)\n",
    "#     x_test_transed = scaler.transform(x_test)\n",
    "#     x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(256, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747f697561c54a1ab74fbacacd70faee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.04505546429935767, val loss: 0.03530123350024223\n",
      "Epoch 2: train loss: 0.03440218642962222, val loss: 0.033902372807264325\n",
      "Epoch 3: train loss: 0.03327326464896299, val loss: 0.03287018379569054\n",
      "Epoch 4: train loss: 0.03244164434622745, val loss: 0.032234469830989834\n",
      "Epoch 5: train loss: 0.031941096130682495, val loss: 0.03181082752346993\n",
      "Epoch 6: train loss: 0.03155850038844712, val loss: 0.03137721699476242\n",
      "Epoch 7: train loss: 0.031229984399615503, val loss: 0.031096578821539878\n",
      "Epoch 8: train loss: 0.03095118919197394, val loss: 0.030863363310694693\n",
      "Epoch 9: train loss: 0.030726539033712173, val loss: 0.030610124066472053\n",
      "Epoch 10: train loss: 0.030495185870297103, val loss: 0.03046785219013691\n",
      "Epoch 11: train loss: 0.03030294287356795, val loss: 0.030390532821416856\n",
      "Epoch 12: train loss: 0.03012161080508816, val loss: 0.03020090335607529\n",
      "Epoch 13: train loss: 0.02995653474087618, val loss: 0.030020905196666718\n",
      "Epoch 14: train loss: 0.029797275884723178, val loss: 0.029915626376867295\n",
      "Epoch 15: train loss: 0.029612163609387922, val loss: 0.029661662980914116\n",
      "Epoch 16: train loss: 0.029461797922849656, val loss: 0.029478737235069274\n",
      "Epoch 17: train loss: 0.029306536429998826, val loss: 0.029339764058589936\n",
      "Epoch 18: train loss: 0.02912507470195391, val loss: 0.02925619027018547\n",
      "Epoch 19: train loss: 0.02895550905593804, val loss: 0.028996546164155007\n",
      "Epoch 20: train loss: 0.02882044805647159, val loss: 0.028878862649202346\n",
      "Epoch 21: train loss: 0.028696468344148324, val loss: 0.028874368980526924\n",
      "Epoch 22: train loss: 0.028593450521023907, val loss: 0.028748261734843253\n",
      "Epoch 23: train loss: 0.02852031471413009, val loss: 0.02862290947139263\n",
      "Epoch 24: train loss: 0.028440117774265153, val loss: 0.028510715305805207\n",
      "Epoch 25: train loss: 0.02836456271489056, val loss: 0.02832130941748619\n",
      "Epoch 26: train loss: 0.028234066340388084, val loss: 0.028500963270664214\n",
      "Epoch 27: train loss: 0.028178005991237504, val loss: 0.028199834570288657\n",
      "Epoch 28: train loss: 0.028093052831839542, val loss: 0.02810166820883751\n",
      "Epoch 29: train loss: 0.028048995747858164, val loss: 0.028048367232084273\n",
      "Epoch 30: train loss: 0.02796031625812151, val loss: 0.02798549197614193\n",
      "Epoch 31: train loss: 0.02791407341525263, val loss: 0.028021679371595384\n",
      "Epoch 32: train loss: 0.027840824257050242, val loss: 0.027966273784637452\n",
      "Epoch 33: train loss: 0.027820028231156115, val loss: 0.02792875415086746\n",
      "Epoch 34: train loss: 0.027780269334814988, val loss: 0.027751903980970383\n",
      "Epoch 35: train loss: 0.027757890422125252, val loss: 0.027771061062812807\n",
      "Epoch 36: train loss: 0.02769691737969311, val loss: 0.02777830131351948\n",
      "Epoch 37: train loss: 0.027672473820800684, val loss: 0.027749746844172477\n",
      "Epoch 38: train loss: 0.027638645460410994, val loss: 0.02772318372130394\n",
      "Epoch 39: train loss: 0.02757309824927729, val loss: 0.027839908942580222\n",
      "Epoch 40: train loss: 0.027550728530299908, val loss: 0.027768272206187248\n",
      "Epoch 41: train loss: 0.027529219984095923, val loss: 0.02773444412648678\n",
      "Epoch 42: train loss: 0.027497964508983553, val loss: 0.02763498158752918\n",
      "Epoch 43: train loss: 0.027496460089574053, val loss: 0.027691603749990464\n",
      "Epoch 44: train loss: 0.027453434589261912, val loss: 0.02774352490901947\n",
      "Epoch 45: train loss: 0.027428233506424086, val loss: 0.027871005788445473\n",
      "Epoch 46: train loss: 0.027405966253304968, val loss: 0.02750419610738754\n",
      "Epoch 47: train loss: 0.027371290444111337, val loss: 0.027693946808576585\n",
      "Epoch 48: train loss: 0.027374843050022513, val loss: 0.027389160498976706\n",
      "Epoch 49: train loss: 0.027334155017624095, val loss: 0.027532351449131964\n",
      "Epoch 50: train loss: 0.027328588698591505, val loss: 0.027363585487008096\n",
      "Epoch 51: train loss: 0.02731636576993125, val loss: 0.027478274762630463\n",
      "Epoch 52: train loss: 0.02727085513363079, val loss: 0.027407621785998343\n",
      "Epoch 53: train loss: 0.02727730998700979, val loss: 0.027466256305575372\n",
      "Epoch 54: train loss: 0.027255016851181885, val loss: 0.027427545696496962\n",
      "Epoch 55: train loss: 0.027236318105641675, val loss: 0.02756204725801945\n",
      "Epoch 00056: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 56: train loss: 0.02720891882144675, val loss: 0.027439037516713144\n",
      "Epoch 57: train loss: 0.027068423757443623, val loss: 0.02723501719534397\n",
      "Epoch 58: train loss: 0.027005315985910747, val loss: 0.027058698698878288\n",
      "Epoch 59: train loss: 0.02697303758318327, val loss: 0.027268131256103514\n",
      "Epoch 60: train loss: 0.026970397633861522, val loss: 0.02725782097876072\n",
      "Epoch 61: train loss: 0.026955823483819863, val loss: 0.02719669134914875\n",
      "Epoch 62: train loss: 0.026940666372678716, val loss: 0.027108078464865685\n",
      "Epoch 63: train loss: 0.026921821333620012, val loss: 0.027158119201660155\n",
      "Epoch 64: train loss: 0.026902423512570713, val loss: 0.026984148710966112\n",
      "Epoch 65: train loss: 0.026907042383539432, val loss: 0.02717936633527279\n",
      "Epoch 66: train loss: 0.02689948523074997, val loss: 0.027164357826113702\n",
      "Epoch 67: train loss: 0.026857878115408275, val loss: 0.026991427421569823\n",
      "Epoch 68: train loss: 0.02686981685125098, val loss: 0.027013646975159646\n",
      "Epoch 69: train loss: 0.026869343797771297, val loss: 0.027040325030684473\n",
      "Epoch 00070: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 70: train loss: 0.026871524411196612, val loss: 0.02707017342746258\n",
      "Epoch 71: train loss: 0.02682583126881901, val loss: 0.026967465996742247\n",
      "Epoch 72: train loss: 0.026776516633678457, val loss: 0.02691356647014618\n",
      "Epoch 73: train loss: 0.026770168381990218, val loss: 0.02675234878063202\n",
      "Epoch 74: train loss: 0.026773285055951196, val loss: 0.02696090732514858\n",
      "Epoch 75: train loss: 0.02676218214935186, val loss: 0.026914580628275873\n",
      "Epoch 76: train loss: 0.02674392711234336, val loss: 0.026947606191039086\n",
      "Epoch 77: train loss: 0.026769913944662833, val loss: 0.026914550736546517\n",
      "Epoch 78: train loss: 0.026758277777202275, val loss: 0.026863093286752703\n",
      "Epoch 00079: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 79: train loss: 0.026766554592823497, val loss: 0.026994220718741416\n",
      "Epoch 80: train loss: 0.026743545414841904, val loss: 0.02680421708524227\n",
      "Epoch 81: train loss: 0.026741162098792133, val loss: 0.026940567538142203\n",
      "Epoch 82: train loss: 0.02672145005087463, val loss: 0.026889850988984106\n",
      "Epoch 83: train loss: 0.026727584340134447, val loss: 0.02694757503271103\n",
      "Epoch 84: train loss: 0.026730163323940064, val loss: 0.02694278109073639\n",
      "Epoch 00085: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 85: train loss: 0.02671366885365272, val loss: 0.026781334310770035\n",
      "Epoch 86: train loss: 0.026703877491610392, val loss: 0.026941864177584648\n",
      "Epoch 87: train loss: 0.026731792269920816, val loss: 0.026882256507873535\n",
      "Epoch 88: train loss: 0.026713422520124184, val loss: 0.026963888868689535\n",
      "Epoch 89: train loss: 0.02670711434130766, val loss: 0.026940694332122803\n",
      "Epoch 90: train loss: 0.026687890502263088, val loss: 0.026876579761505127\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 91: train loss: 0.026703704883553545, val loss: 0.026795502245426178\n",
      "Epoch 92: train loss: 0.026702367241285285, val loss: 0.026888436138629915\n",
      "Epoch 93: train loss: 0.02672272477010075, val loss: 0.02688411955535412\n",
      "Epoch 94: train loss: 0.02670478431363495, val loss: 0.026978947177529335\n",
      "Epoch 95: train loss: 0.026718704610758898, val loss: 0.02684691086411476\n",
      "Epoch 96: train loss: 0.026729974123896386, val loss: 0.02683952747285366\n",
      "Epoch 00097: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 97: train loss: 0.02670274164846965, val loss: 0.02689834924042225\n",
      "Epoch 98: train loss: 0.02669916187196362, val loss: 0.027007239148020744\n",
      "Epoch 99: train loss: 0.026711327194863438, val loss: 0.026792143821716307\n",
      "Epoch 100: train loss: 0.026703121685251897, val loss: 0.02691049861907959\n",
      "Epoch 101: train loss: 0.026703240160735286, val loss: 0.02686536718904972\n",
      "Epoch 102: train loss: 0.026691573204130542, val loss: 0.026886032015085222\n",
      "Epoch 00103: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 103: train loss: 0.02672676181458697, val loss: 0.027044261440634727\n",
      "Epoch 104: train loss: 0.026712815949807362, val loss: 0.02680667698383331\n",
      "Epoch 105: train loss: 0.02672912106921478, val loss: 0.0270081138163805\n",
      "Epoch 106: train loss: 0.026722119883615142, val loss: 0.027294295951724053\n",
      "Epoch 107: train loss: 0.026699551418423652, val loss: 0.02695759989321232\n",
      "Epoch 108: train loss: 0.026702410517298446, val loss: 0.026965546980500223\n",
      "Epoch 00109: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 109: train loss: 0.02671247406668809, val loss: 0.02697974145412445\n",
      "Early stop at epoch 109\n"
     ]
    }
   ],
   "source": [
    "eval_size = 1000\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(ae_model.parameters(), lr=learning_rate)\n",
    "    # optimizer = torch.optim.SGD(ae_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 1000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        \n",
    "        # if(epoch % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, HuberRegressor, ElasticNet\n",
    "def get_regression_model(X, y):\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # model = HuberRegressor(alpha=0.001, max_iter=10000)\n",
    "    model = ElasticNet(alpha=0.01, l1_ratio=0.05, max_iter=10000)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae-lr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "0.4548999383664605\n",
      "Predictions saved to results-ae-lr.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "featured_x_train = ae_model.encode(torch.tensor(x_train, dtype=torch.float).to(device)).detach().cpu().numpy()\n",
    "scaler = StandardScaler()\n",
    "featured_x_train = scaler.fit_transform(featured_x_train)\n",
    "print(featured_x_train.shape)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "featured_x_test = ae_model.encode(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))\n",
    "featured_x_test = scaler.transform(featured_x_test.detach().cpu().numpy())\n",
    "# featured_x_test = torch.tensor(featured_x_test, dtype=torch.float).to(device)\n",
    "# y_pred = one_model(featured_x_test).squeeze(-1).detach().cpu().numpy()\n",
    "lr = get_regression_model(featured_x_train, y_train)\n",
    "print(lr.score(featured_x_train, y_train))\n",
    "y_pred = lr.predict(featured_x_test)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
