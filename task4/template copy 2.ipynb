{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-dropout.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7486ef4f8d2b4aa3bcb1c41acde5685e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.3895806704370343, val loss: 0.17438548016548155\n",
      "Epoch 2: train loss: 0.1425675984730526, val loss: 0.12250461745262146\n",
      "Epoch 3: train loss: 0.09546364850535685, val loss: 0.07982212203741074\n",
      "Epoch 4: train loss: 0.07276069616602392, val loss: 0.06883284384012223\n",
      "Epoch 5: train loss: 0.05831798754541241, val loss: 0.05449769654870033\n",
      "Epoch 6: train loss: 0.04779409138158876, val loss: 0.04496195492148399\n",
      "Epoch 7: train loss: 0.041052279514317606, val loss: 0.03731858330965042\n",
      "Epoch 8: train loss: 0.0336964361436513, val loss: 0.0322950539290905\n",
      "Epoch 9: train loss: 0.027481426903179715, val loss: 0.02525399889051914\n",
      "Epoch 10: train loss: 0.02389327717192319, val loss: 0.021084985703229905\n",
      "Epoch 11: train loss: 0.019330606977246246, val loss: 0.01774333779513836\n",
      "Epoch 12: train loss: 0.015945944154445005, val loss: 0.015687924817204477\n",
      "Epoch 13: train loss: 0.014340256430664841, val loss: 0.014401729799807072\n",
      "Epoch 14: train loss: 0.011852574076275437, val loss: 0.01287369817495346\n",
      "Epoch 15: train loss: 0.010100011672976673, val loss: 0.009950989864766597\n",
      "Epoch 16: train loss: 0.008809007068677824, val loss: 0.010544200584292411\n",
      "Epoch 17: train loss: 0.007964886101913086, val loss: 0.008709823563694954\n",
      "Epoch 18: train loss: 0.0071563345811196735, val loss: 0.007072680659592151\n",
      "Epoch 19: train loss: 0.006605711516144933, val loss: 0.007148400768637657\n",
      "Epoch 20: train loss: 0.00599623462945527, val loss: 0.006752253647893667\n",
      "Epoch 21: train loss: 0.005807182065230243, val loss: 0.0067347723841667175\n",
      "Epoch 22: train loss: 0.00569081217681571, val loss: 0.00687308980897069\n",
      "Epoch 23: train loss: 0.005509231349035185, val loss: 0.006041734337806702\n",
      "Epoch 24: train loss: 0.005343803365619815, val loss: 0.006186413999646902\n",
      "Epoch 25: train loss: 0.005276865564970945, val loss: 0.006147614903748036\n",
      "Epoch 26: train loss: 0.00520934050347732, val loss: 0.005682171501219272\n",
      "Epoch 27: train loss: 0.005189013794900811, val loss: 0.0052213951759040355\n",
      "Epoch 28: train loss: 0.005160364189014143, val loss: 0.005546162281185389\n",
      "Epoch 29: train loss: 0.005128110036786113, val loss: 0.005627250421792269\n",
      "Epoch 30: train loss: 0.005056816198920109, val loss: 0.005992441818118095\n",
      "Epoch 31: train loss: 0.005115103645440267, val loss: 0.00564050604775548\n",
      "Epoch 32: train loss: 0.0049618455452882515, val loss: 0.005946625612676144\n",
      "Epoch 00033: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 33: train loss: 0.005205462182586899, val loss: 0.0054271074607968334\n",
      "Epoch 34: train loss: 0.0033805835665184623, val loss: 0.0035561834089457988\n",
      "Epoch 35: train loss: 0.0027656777758075265, val loss: 0.0035830539483577015\n",
      "Epoch 36: train loss: 0.0025387924928218127, val loss: 0.003592621432617307\n",
      "Epoch 37: train loss: 0.0025541929607184564, val loss: 0.003682778729125857\n",
      "Epoch 38: train loss: 0.0027296181840403954, val loss: 0.004110633175820113\n",
      "Epoch 39: train loss: 0.0027712023257068833, val loss: 0.0035172994546592237\n",
      "Epoch 40: train loss: 0.0028326031249776788, val loss: 0.0036344464011490345\n",
      "Epoch 41: train loss: 0.0028224256369000188, val loss: 0.003655530363321304\n",
      "Epoch 42: train loss: 0.0028802494020866497, val loss: 0.00375394019857049\n",
      "Epoch 43: train loss: 0.0028740989847907, val loss: 0.0038208577819168568\n",
      "Epoch 44: train loss: 0.002827798173401733, val loss: 0.0037707400731742382\n",
      "Epoch 00045: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 45: train loss: 0.002889164367576643, val loss: 0.0036922118458896877\n",
      "Epoch 46: train loss: 0.0021425282445983315, val loss: 0.0028104454930871726\n",
      "Epoch 47: train loss: 0.0017685518453695944, val loss: 0.0027135255243629216\n",
      "Epoch 48: train loss: 0.001696795006263621, val loss: 0.0028456256426870824\n",
      "Epoch 49: train loss: 0.0016723961298905161, val loss: 0.002846282308921218\n",
      "Epoch 50: train loss: 0.0016233800255066278, val loss: 0.0026425288897007704\n",
      "Epoch 51: train loss: 0.0016023979651912743, val loss: 0.002563657663762569\n",
      "Epoch 52: train loss: 0.0016156495260834997, val loss: 0.0026108430530875923\n",
      "Epoch 53: train loss: 0.0016246378920706254, val loss: 0.002624919692054391\n",
      "Epoch 54: train loss: 0.0016240747537920061, val loss: 0.0028614821061491965\n",
      "Epoch 55: train loss: 0.001620443521295579, val loss: 0.0029779682736843826\n",
      "Epoch 56: train loss: 0.0016217295814077465, val loss: 0.0027331044804304837\n",
      "Epoch 00057: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 57: train loss: 0.0016000287813913762, val loss: 0.002719866069033742\n",
      "Epoch 58: train loss: 0.0013754782399010598, val loss: 0.0023536938689649105\n",
      "Epoch 59: train loss: 0.0012607504069539054, val loss: 0.0025769206918776037\n",
      "Epoch 60: train loss: 0.001226180087546913, val loss: 0.0023662279061973093\n",
      "Epoch 61: train loss: 0.0012104980991012892, val loss: 0.0024369656182825564\n",
      "Epoch 62: train loss: 0.0012038469818249648, val loss: 0.0024390265848487614\n",
      "Epoch 63: train loss: 0.0011979247925194855, val loss: 0.0025694154128432273\n",
      "Epoch 00064: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 64: train loss: 0.0011689822035366479, val loss: 0.0024283122960478067\n",
      "Epoch 65: train loss: 0.001116621243273269, val loss: 0.0025025971550494434\n",
      "Epoch 66: train loss: 0.0010860486773355883, val loss: 0.0024722764249891045\n",
      "Epoch 67: train loss: 0.001061493356731169, val loss: 0.002536510219797492\n",
      "Epoch 68: train loss: 0.0010662933385227713, val loss: 0.0024609514400362967\n",
      "Epoch 69: train loss: 0.0010447708009552134, val loss: 0.002432066973298788\n",
      "Epoch 00070: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 70: train loss: 0.0010474359765458776, val loss: 0.0025165036655962468\n",
      "Epoch 71: train loss: 0.0010317586363217204, val loss: 0.0025289222411811352\n",
      "Epoch 72: train loss: 0.001032157537936024, val loss: 0.002499517301097512\n",
      "Epoch 73: train loss: 0.0010172319095961902, val loss: 0.002499807425774634\n",
      "Epoch 74: train loss: 0.0010158660555144353, val loss: 0.0024130549877882\n",
      "Epoch 75: train loss: 0.0010232893696853092, val loss: 0.0024674397762864827\n",
      "Epoch 00076: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 76: train loss: 0.0010158317347380276, val loss: 0.002535220924764872\n",
      "Early stop at epoch 76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462d54463a5945a6a65133e18637be75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 0.6707581877708435\n",
      "Epoch 20: train loss: 0.5371580719947815\n",
      "Epoch 30: train loss: 0.4468727111816406\n",
      "Epoch 40: train loss: 0.38909950852394104\n",
      "Epoch 50: train loss: 0.3505701720714569\n",
      "Epoch 60: train loss: 0.3210780918598175\n",
      "Epoch 70: train loss: 0.2955169975757599\n",
      "Epoch 80: train loss: 0.2723051905632019\n",
      "Epoch 90: train loss: 0.2511240243911743\n",
      "Epoch 100: train loss: 0.2318057417869568\n",
      "Epoch 110: train loss: 0.21415798366069794\n",
      "Epoch 120: train loss: 0.19802328944206238\n",
      "Epoch 130: train loss: 0.18328702449798584\n",
      "Epoch 140: train loss: 0.1698535531759262\n",
      "Epoch 150: train loss: 0.15763375163078308\n",
      "Epoch 160: train loss: 0.14654341340065002\n",
      "Epoch 170: train loss: 0.13650290668010712\n",
      "Epoch 180: train loss: 0.12743651866912842\n",
      "Epoch 190: train loss: 0.11927194148302078\n",
      "Epoch 200: train loss: 0.11193986982107162\n",
      "Epoch 210: train loss: 0.10537409782409668\n",
      "Epoch 220: train loss: 0.0995115339756012\n",
      "Epoch 230: train loss: 0.09429208189249039\n",
      "Epoch 240: train loss: 0.08965883404016495\n",
      "Epoch 250: train loss: 0.0855579525232315\n",
      "Epoch 260: train loss: 0.08193892985582352\n",
      "Epoch 270: train loss: 0.07875437289476395\n",
      "Epoch 280: train loss: 0.07596009224653244\n",
      "Epoch 290: train loss: 0.07351512461900711\n",
      "Epoch 300: train loss: 0.0713815912604332\n",
      "Epoch 310: train loss: 0.06952470541000366\n",
      "Epoch 320: train loss: 0.06791257113218307\n",
      "Epoch 330: train loss: 0.0665162056684494\n",
      "Epoch 340: train loss: 0.0653093010187149\n",
      "Epoch 350: train loss: 0.06426816433668137\n",
      "Epoch 360: train loss: 0.06337146461009979\n",
      "Epoch 370: train loss: 0.06260020285844803\n",
      "Epoch 380: train loss: 0.06193747743964195\n",
      "Epoch 390: train loss: 0.06136826425790787\n",
      "Epoch 400: train loss: 0.0608794204890728\n",
      "Epoch 410: train loss: 0.06045934930443764\n",
      "Epoch 420: train loss: 0.060097988694906235\n",
      "Epoch 430: train loss: 0.059786539524793625\n",
      "Epoch 440: train loss: 0.05951743945479393\n",
      "Epoch 450: train loss: 0.05928413197398186\n",
      "Epoch 460: train loss: 0.0590810589492321\n",
      "Epoch 470: train loss: 0.058903396129608154\n",
      "Epoch 480: train loss: 0.05874709412455559\n",
      "Epoch 490: train loss: 0.05860874429345131\n",
      "Epoch 500: train loss: 0.058485399931669235\n",
      "Epoch 510: train loss: 0.05837468057870865\n",
      "Epoch 520: train loss: 0.0582745186984539\n",
      "Epoch 530: train loss: 0.05818326398730278\n",
      "Epoch 540: train loss: 0.05809947848320007\n",
      "Epoch 550: train loss: 0.058021992444992065\n",
      "Epoch 560: train loss: 0.05794987082481384\n",
      "Epoch 570: train loss: 0.057882364839315414\n",
      "Epoch 580: train loss: 0.057818762958049774\n",
      "Epoch 590: train loss: 0.057758595794439316\n",
      "Epoch 600: train loss: 0.05770140513777733\n",
      "Epoch 610: train loss: 0.057646870613098145\n",
      "Epoch 620: train loss: 0.05759467929601669\n",
      "Epoch 630: train loss: 0.05754464492201805\n",
      "Epoch 640: train loss: 0.05749654769897461\n",
      "Epoch 650: train loss: 0.05745023488998413\n",
      "Epoch 660: train loss: 0.05740562453866005\n",
      "Epoch 670: train loss: 0.057362575083971024\n",
      "Epoch 680: train loss: 0.05732100456953049\n",
      "Epoch 690: train loss: 0.057280853390693665\n",
      "Epoch 700: train loss: 0.05724205821752548\n",
      "Epoch 710: train loss: 0.05720457062125206\n",
      "Epoch 720: train loss: 0.05716833844780922\n",
      "Epoch 730: train loss: 0.05713333934545517\n",
      "Epoch 740: train loss: 0.057099517434835434\n",
      "Epoch 750: train loss: 0.05706685036420822\n",
      "Epoch 760: train loss: 0.057035285979509354\n",
      "Epoch 770: train loss: 0.057004813104867935\n",
      "Epoch 780: train loss: 0.05697540566325188\n",
      "Epoch 790: train loss: 0.05694703757762909\n",
      "Epoch 800: train loss: 0.05691966786980629\n",
      "Epoch 810: train loss: 0.056893281638622284\n",
      "Epoch 820: train loss: 0.05686785653233528\n",
      "Epoch 830: train loss: 0.05684332549571991\n",
      "Epoch 840: train loss: 0.05681972950696945\n",
      "Epoch 850: train loss: 0.05679700896143913\n",
      "Epoch 860: train loss: 0.05677511915564537\n",
      "Epoch 870: train loss: 0.05675407126545906\n",
      "Epoch 880: train loss: 0.05673383176326752\n",
      "Epoch 890: train loss: 0.056714367121458054\n",
      "Epoch 900: train loss: 0.05669567361474037\n",
      "Epoch 910: train loss: 0.056677691638469696\n",
      "Epoch 920: train loss: 0.05666043236851692\n",
      "Epoch 930: train loss: 0.056643858551979065\n",
      "Epoch 940: train loss: 0.05662793666124344\n",
      "Epoch 950: train loss: 0.056612662971019745\n",
      "Epoch 960: train loss: 0.05659801512956619\n",
      "Epoch 970: train loss: 0.0565839521586895\n",
      "Epoch 980: train loss: 0.05657048150897026\n",
      "Epoch 990: train loss: 0.05655756965279579\n",
      "Epoch 1000: train loss: 0.05654517933726311\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
