{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-dropout.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39abc5670fc47568670ba2e63c91732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.5299376805607153, val loss: 0.2206245869398117\n",
      "Epoch 2: train loss: 0.1614212184268601, val loss: 0.1345952731370926\n",
      "Epoch 3: train loss: 0.1201144617504003, val loss: 0.10979283207654952\n",
      "Epoch 4: train loss: 0.08451679548925282, val loss: 0.07685105448961257\n",
      "Epoch 5: train loss: 0.06582379409731651, val loss: 0.061735263168811796\n",
      "Epoch 6: train loss: 0.053258515170642305, val loss: 0.05043203201889992\n",
      "Epoch 7: train loss: 0.04526872478334271, val loss: 0.04513170823454857\n",
      "Epoch 8: train loss: 0.037403369969859414, val loss: 0.03456162703037262\n",
      "Epoch 9: train loss: 0.030935327980591325, val loss: 0.028008030846714972\n",
      "Epoch 10: train loss: 0.025745277534942237, val loss: 0.023738642528653146\n",
      "Epoch 11: train loss: 0.02181300890202425, val loss: 0.02286740490794182\n",
      "Epoch 12: train loss: 0.017591764844497856, val loss: 0.02004387937486172\n",
      "Epoch 13: train loss: 0.014472463403885461, val loss: 0.014837646588683128\n",
      "Epoch 14: train loss: 0.012163019356070733, val loss: 0.012141805298626423\n",
      "Epoch 15: train loss: 0.010287634839664916, val loss: 0.010942114286124707\n",
      "Epoch 16: train loss: 0.008941521018591463, val loss: 0.009845114424824714\n",
      "Epoch 17: train loss: 0.00819995940537477, val loss: 0.008967193104326724\n",
      "Epoch 18: train loss: 0.007296135129216982, val loss: 0.008345135517418384\n",
      "Epoch 19: train loss: 0.006653411948711288, val loss: 0.007487236741930246\n",
      "Epoch 20: train loss: 0.006485071442945271, val loss: 0.007073670249432325\n",
      "Epoch 21: train loss: 0.00623159816481021, val loss: 0.006646210562437773\n",
      "Epoch 22: train loss: 0.005755574447159864, val loss: 0.006081439159810543\n",
      "Epoch 23: train loss: 0.005448380886291971, val loss: 0.0062142348960041996\n",
      "Epoch 24: train loss: 0.005673297734132835, val loss: 0.007541649289429188\n",
      "Epoch 25: train loss: 0.0054293726857523525, val loss: 0.005929680027067661\n",
      "Epoch 26: train loss: 0.005288593037548114, val loss: 0.0054085197411477565\n",
      "Epoch 27: train loss: 0.005042267187137385, val loss: 0.00568416254222393\n",
      "Epoch 28: train loss: 0.005042184107385728, val loss: 0.005466840852051973\n",
      "Epoch 29: train loss: 0.0051804927088776416, val loss: 0.006776267319917679\n",
      "Epoch 30: train loss: 0.005259978825188413, val loss: 0.005814818117767572\n",
      "Epoch 31: train loss: 0.005180156915771718, val loss: 0.005551080889999866\n",
      "Epoch 00032: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 32: train loss: 0.005137417872037206, val loss: 0.006291445292532444\n",
      "Epoch 33: train loss: 0.003530907274775055, val loss: 0.003845156032592058\n",
      "Epoch 34: train loss: 0.0027991006138297367, val loss: 0.00352263749204576\n",
      "Epoch 35: train loss: 0.0026872695250322624, val loss: 0.003208055129274726\n",
      "Epoch 36: train loss: 0.0026763734238184228, val loss: 0.003658526998013258\n",
      "Epoch 37: train loss: 0.0027758827247865952, val loss: 0.003718331938609481\n",
      "Epoch 38: train loss: 0.002740375433269204, val loss: 0.0036690718270838262\n",
      "Epoch 39: train loss: 0.0028922686774511728, val loss: 0.003772085651755333\n",
      "Epoch 40: train loss: 0.002852573420472291, val loss: 0.0036489775814116\n",
      "Epoch 00041: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 41: train loss: 0.002932881064713001, val loss: 0.003965088764205575\n",
      "Epoch 42: train loss: 0.0022727975724835176, val loss: 0.0030785135105252264\n",
      "Epoch 43: train loss: 0.001905962332246863, val loss: 0.0028496382217854262\n",
      "Epoch 44: train loss: 0.0018270877839199134, val loss: 0.0030166216585785152\n",
      "Epoch 45: train loss: 0.0017682724644296935, val loss: 0.0027478666342794895\n",
      "Epoch 46: train loss: 0.0017472009485093307, val loss: 0.0030184025187045336\n",
      "Epoch 47: train loss: 0.0017548299171220587, val loss: 0.0027734176628291607\n",
      "Epoch 48: train loss: 0.0017342102333561195, val loss: 0.0032730361111462116\n",
      "Epoch 49: train loss: 0.0017371610563476476, val loss: 0.002913346691057086\n",
      "Epoch 50: train loss: 0.0017514264989948395, val loss: 0.003020053459331393\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 51: train loss: 0.0017736554364388695, val loss: 0.00293842613697052\n",
      "Epoch 52: train loss: 0.0015378493679953473, val loss: 0.0027517704349011183\n",
      "Epoch 53: train loss: 0.0014090101188428852, val loss: 0.002716542411595583\n",
      "Epoch 54: train loss: 0.0013638060893673373, val loss: 0.0028357304260134696\n",
      "Epoch 55: train loss: 0.0013481968277114995, val loss: 0.0026609210167080166\n",
      "Epoch 56: train loss: 0.001317451865811433, val loss: 0.0027824367862194777\n",
      "Epoch 57: train loss: 0.0012951754776152726, val loss: 0.002852472227066755\n",
      "Epoch 58: train loss: 0.0012920751000697515, val loss: 0.002822142319753766\n",
      "Epoch 59: train loss: 0.001270074632493969, val loss: 0.00266307033598423\n",
      "Epoch 60: train loss: 0.0012650811359172268, val loss: 0.002954114766791463\n",
      "Epoch 00061: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 61: train loss: 0.0012513279604098322, val loss: 0.0027799906358122826\n",
      "Epoch 62: train loss: 0.0012035425032051851, val loss: 0.0026897031590342522\n",
      "Epoch 63: train loss: 0.0011464431086289032, val loss: 0.002717854844406247\n",
      "Epoch 64: train loss: 0.001152034745266547, val loss: 0.0026792393419891596\n",
      "Epoch 65: train loss: 0.0011351121165504565, val loss: 0.0025607329811900854\n",
      "Epoch 66: train loss: 0.0011199243230060958, val loss: 0.002585299437865615\n",
      "Epoch 67: train loss: 0.0011341612990565446, val loss: 0.00246599037386477\n",
      "Epoch 68: train loss: 0.0011230070559421971, val loss: 0.0025788754634559156\n",
      "Epoch 69: train loss: 0.0011304262165569377, val loss: 0.0025538254100829364\n",
      "Epoch 70: train loss: 0.001105108066614033, val loss: 0.0024377432372421027\n",
      "Epoch 71: train loss: 0.001126024512488547, val loss: 0.0024765398669987916\n",
      "Epoch 72: train loss: 0.0010906130738499366, val loss: 0.0025248559433966875\n",
      "Epoch 73: train loss: 0.0010965371702284534, val loss: 0.0026914241164922715\n",
      "Epoch 74: train loss: 0.0010977135821986868, val loss: 0.0026066839108243585\n",
      "Epoch 75: train loss: 0.0011095284100773992, val loss: 0.0025619806181639434\n",
      "Epoch 00076: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 76: train loss: 0.0010825299602762169, val loss: 0.002518755109980702\n",
      "Epoch 77: train loss: 0.0010640048560287272, val loss: 0.0025817469973117113\n",
      "Epoch 78: train loss: 0.0010605873406047419, val loss: 0.0026415283661335707\n",
      "Epoch 79: train loss: 0.0010670886693750414, val loss: 0.002512817095965147\n",
      "Epoch 80: train loss: 0.0010673636749804931, val loss: 0.0024047352615743874\n",
      "Epoch 81: train loss: 0.0010423704675227709, val loss: 0.002551260630600154\n",
      "Epoch 82: train loss: 0.0010430770079149123, val loss: 0.0027073981650173663\n",
      "Epoch 83: train loss: 0.001040382604627889, val loss: 0.0025949511602520944\n",
      "Epoch 84: train loss: 0.0010507368819636046, val loss: 0.0025392530541867016\n",
      "Epoch 85: train loss: 0.001057715898854848, val loss: 0.0025063925515860317\n",
      "Epoch 00086: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 86: train loss: 0.0010314062790070869, val loss: 0.002625538818538189\n",
      "Early stop at epoch 86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb274bbd137424a837cfba7a9a7c6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 3.9018046855926514\n",
      "Epoch 20: train loss: 3.2708616256713867\n",
      "Epoch 30: train loss: 2.722972869873047\n",
      "Epoch 40: train loss: 2.2577524185180664\n",
      "Epoch 50: train loss: 1.8703242540359497\n",
      "Epoch 60: train loss: 1.553064227104187\n",
      "Epoch 70: train loss: 1.297062635421753\n",
      "Epoch 80: train loss: 1.0931044816970825\n",
      "Epoch 90: train loss: 0.9322864413261414\n",
      "Epoch 100: train loss: 0.8063956499099731\n",
      "Epoch 110: train loss: 0.7081441879272461\n",
      "Epoch 120: train loss: 0.6312915086746216\n",
      "Epoch 130: train loss: 0.5706750154495239\n",
      "Epoch 140: train loss: 0.5221635103225708\n",
      "Epoch 150: train loss: 0.48255422711372375\n",
      "Epoch 160: train loss: 0.4494367241859436\n",
      "Epoch 170: train loss: 0.42104631662368774\n",
      "Epoch 180: train loss: 0.3961225152015686\n",
      "Epoch 190: train loss: 0.3737846612930298\n",
      "Epoch 200: train loss: 0.3534284234046936\n",
      "Epoch 210: train loss: 0.3346450626850128\n",
      "Epoch 220: train loss: 0.3171598017215729\n",
      "Epoch 230: train loss: 0.3007880449295044\n",
      "Epoch 240: train loss: 0.28540390729904175\n",
      "Epoch 250: train loss: 0.27091896533966064\n",
      "Epoch 260: train loss: 0.2572686970233917\n",
      "Epoch 270: train loss: 0.2444029003381729\n",
      "Epoch 280: train loss: 0.23228026926517487\n",
      "Epoch 290: train loss: 0.22086495161056519\n",
      "Epoch 300: train loss: 0.21012422442436218\n",
      "Epoch 310: train loss: 0.20002761483192444\n",
      "Epoch 320: train loss: 0.19054582715034485\n",
      "Epoch 330: train loss: 0.18165060877799988\n",
      "Epoch 340: train loss: 0.17331454157829285\n",
      "Epoch 350: train loss: 0.16551093757152557\n",
      "Epoch 360: train loss: 0.15821361541748047\n",
      "Epoch 370: train loss: 0.15139727294445038\n",
      "Epoch 380: train loss: 0.14503709971904755\n",
      "Epoch 390: train loss: 0.1391090303659439\n",
      "Epoch 400: train loss: 0.1335897296667099\n",
      "Epoch 410: train loss: 0.12845653295516968\n",
      "Epoch 420: train loss: 0.12368755042552948\n",
      "Epoch 430: train loss: 0.11926161497831345\n",
      "Epoch 440: train loss: 0.11515834182500839\n",
      "Epoch 450: train loss: 0.1113581657409668\n",
      "Epoch 460: train loss: 0.10784217715263367\n",
      "Epoch 470: train loss: 0.10459236055612564\n",
      "Epoch 480: train loss: 0.10159139335155487\n",
      "Epoch 490: train loss: 0.09882280230522156\n",
      "Epoch 500: train loss: 0.0962708443403244\n",
      "Epoch 510: train loss: 0.0939205214381218\n",
      "Epoch 520: train loss: 0.09175767749547958\n",
      "Epoch 530: train loss: 0.08976876735687256\n",
      "Epoch 540: train loss: 0.08794108778238297\n",
      "Epoch 550: train loss: 0.08626256883144379\n",
      "Epoch 560: train loss: 0.08472184836864471\n",
      "Epoch 570: train loss: 0.08330829441547394\n",
      "Epoch 580: train loss: 0.08201181888580322\n",
      "Epoch 590: train loss: 0.08082306385040283\n",
      "Epoch 600: train loss: 0.07973319292068481\n",
      "Epoch 610: train loss: 0.07873401790857315\n",
      "Epoch 620: train loss: 0.0778178796172142\n",
      "Epoch 630: train loss: 0.07697765529155731\n",
      "Epoch 640: train loss: 0.07620669901371002\n",
      "Epoch 650: train loss: 0.07549891620874405\n",
      "Epoch 660: train loss: 0.07484861463308334\n",
      "Epoch 670: train loss: 0.07425056397914886\n",
      "Epoch 680: train loss: 0.0736999362707138\n",
      "Epoch 690: train loss: 0.07319226115942001\n",
      "Epoch 700: train loss: 0.07272348552942276\n",
      "Epoch 710: train loss: 0.0722898468375206\n",
      "Epoch 720: train loss: 0.07188791781663895\n",
      "Epoch 730: train loss: 0.07151461392641068\n",
      "Epoch 740: train loss: 0.07116702198982239\n",
      "Epoch 750: train loss: 0.07084261626005173\n",
      "Epoch 760: train loss: 0.0705389752984047\n",
      "Epoch 770: train loss: 0.07025396823883057\n",
      "Epoch 780: train loss: 0.06998571008443832\n",
      "Epoch 790: train loss: 0.0697324275970459\n",
      "Epoch 800: train loss: 0.06949251890182495\n",
      "Epoch 810: train loss: 0.06926460564136505\n",
      "Epoch 820: train loss: 0.06904738396406174\n",
      "Epoch 830: train loss: 0.06883971393108368\n",
      "Epoch 840: train loss: 0.06864059716463089\n",
      "Epoch 850: train loss: 0.06844908744096756\n",
      "Epoch 860: train loss: 0.06826438009738922\n",
      "Epoch 870: train loss: 0.06808576732873917\n",
      "Epoch 880: train loss: 0.06791258603334427\n",
      "Epoch 890: train loss: 0.06774427741765976\n",
      "Epoch 900: train loss: 0.06758031249046326\n",
      "Epoch 910: train loss: 0.06742028146982193\n",
      "Epoch 920: train loss: 0.06726374477148056\n",
      "Epoch 930: train loss: 0.06711041927337646\n",
      "Epoch 940: train loss: 0.0669599398970604\n",
      "Epoch 950: train loss: 0.06681206077337265\n",
      "Epoch 960: train loss: 0.06666658073663712\n",
      "Epoch 970: train loss: 0.0665232464671135\n",
      "Epoch 980: train loss: 0.06638193875551224\n",
      "Epoch 990: train loss: 0.06624246388673782\n",
      "Epoch 1000: train loss: 0.06610468775033951\n",
      "Epoch 1010: train loss: 0.06596852838993073\n",
      "Epoch 1020: train loss: 0.06583385914564133\n",
      "Epoch 1030: train loss: 0.06570064276456833\n",
      "Epoch 1040: train loss: 0.06556873768568039\n",
      "Epoch 1050: train loss: 0.0654381513595581\n",
      "Epoch 1060: train loss: 0.06530880182981491\n",
      "Epoch 1070: train loss: 0.06518061459064484\n",
      "Epoch 1080: train loss: 0.06505362689495087\n",
      "Epoch 1090: train loss: 0.06492774933576584\n",
      "Epoch 1100: train loss: 0.06480298936367035\n",
      "Epoch 1110: train loss: 0.06467932462692261\n",
      "Epoch 1120: train loss: 0.06455669552087784\n",
      "Epoch 1130: train loss: 0.06443515419960022\n",
      "Epoch 1140: train loss: 0.06431464850902557\n",
      "Epoch 1150: train loss: 0.0641951709985733\n",
      "Epoch 1160: train loss: 0.06407676637172699\n",
      "Epoch 1170: train loss: 0.06395935267210007\n",
      "Epoch 1180: train loss: 0.0638430044054985\n",
      "Epoch 1190: train loss: 0.06372763961553574\n",
      "Epoch 1200: train loss: 0.06361331045627594\n",
      "Epoch 1210: train loss: 0.06349999457597733\n",
      "Epoch 1220: train loss: 0.06338772922754288\n",
      "Epoch 1230: train loss: 0.06327646225690842\n",
      "Epoch 1240: train loss: 0.06316621601581573\n",
      "Epoch 1250: train loss: 0.06305702030658722\n",
      "Epoch 1260: train loss: 0.06294883787631989\n",
      "Epoch 1270: train loss: 0.06284168362617493\n",
      "Epoch 1280: train loss: 0.06273555755615234\n",
      "Epoch 1290: train loss: 0.06263046711683273\n",
      "Epoch 1300: train loss: 0.0625264048576355\n",
      "Epoch 1310: train loss: 0.062423400580883026\n",
      "Epoch 1320: train loss: 0.06232139468193054\n",
      "Epoch 1330: train loss: 0.06222042441368103\n",
      "Epoch 1340: train loss: 0.06212051212787628\n",
      "Epoch 1350: train loss: 0.06202162429690361\n",
      "Epoch 1360: train loss: 0.061923760920763016\n",
      "Epoch 1370: train loss: 0.06182694435119629\n",
      "Epoch 1380: train loss: 0.06173115596175194\n",
      "Epoch 1390: train loss: 0.06163639947772026\n",
      "Epoch 1400: train loss: 0.06154267117381096\n",
      "Epoch 1410: train loss: 0.06144999340176582\n",
      "Epoch 1420: train loss: 0.06135833263397217\n",
      "Epoch 1430: train loss: 0.06126768887042999\n",
      "Epoch 1440: train loss: 0.06117808818817139\n",
      "Epoch 1450: train loss: 0.06108948588371277\n",
      "Epoch 1460: train loss: 0.06100191920995712\n",
      "Epoch 1470: train loss: 0.06091538444161415\n",
      "Epoch 1480: train loss: 0.06082986667752266\n",
      "Epoch 1490: train loss: 0.06074534356594086\n",
      "Epoch 1500: train loss: 0.060661833733320236\n",
      "Epoch 1510: train loss: 0.0605793222784996\n",
      "Epoch 1520: train loss: 0.06049780547618866\n",
      "Epoch 1530: train loss: 0.0604172945022583\n",
      "Epoch 1540: train loss: 0.06033778563141823\n",
      "Epoch 1550: train loss: 0.06025923416018486\n",
      "Epoch 1560: train loss: 0.060181692242622375\n",
      "Epoch 1570: train loss: 0.06010512635111809\n",
      "Epoch 1580: train loss: 0.060029514133930206\n",
      "Epoch 1590: train loss: 0.05995488911867142\n",
      "Epoch 1600: train loss: 0.05988122895359993\n",
      "Epoch 1610: train loss: 0.05980851128697395\n",
      "Epoch 1620: train loss: 0.059736721217632294\n",
      "Epoch 1630: train loss: 0.05966591835021973\n",
      "Epoch 1640: train loss: 0.05959603562951088\n",
      "Epoch 1650: train loss: 0.059527091681957245\n",
      "Epoch 1660: train loss: 0.059459056705236435\n",
      "Epoch 1670: train loss: 0.05939193069934845\n",
      "Epoch 1680: train loss: 0.05932573601603508\n",
      "Epoch 1690: train loss: 0.05926045402884483\n",
      "Epoch 1700: train loss: 0.059196047484874725\n",
      "Epoch 1710: train loss: 0.05913253501057625\n",
      "Epoch 1720: train loss: 0.0590699277818203\n",
      "Epoch 1730: train loss: 0.0590081587433815\n",
      "Epoch 1740: train loss: 0.058947283774614334\n",
      "Epoch 1750: train loss: 0.058887261897325516\n",
      "Epoch 1760: train loss: 0.05882808938622475\n",
      "Epoch 1770: train loss: 0.058769769966602325\n",
      "Epoch 1780: train loss: 0.058712270110845566\n",
      "Epoch 1790: train loss: 0.05865561217069626\n",
      "Epoch 1800: train loss: 0.058599770069122314\n",
      "Epoch 1810: train loss: 0.05854472890496254\n",
      "Epoch 1820: train loss: 0.05849049612879753\n",
      "Epoch 1830: train loss: 0.05843706801533699\n",
      "Epoch 1840: train loss: 0.058384425938129425\n",
      "Epoch 1850: train loss: 0.05833253636956215\n",
      "Epoch 1860: train loss: 0.05828144773840904\n",
      "Epoch 1870: train loss: 0.05823110416531563\n",
      "Epoch 1880: train loss: 0.0581815168261528\n",
      "Epoch 1890: train loss: 0.05813268572092056\n",
      "Epoch 1900: train loss: 0.05808456614613533\n",
      "Epoch 1910: train loss: 0.058037180453538895\n",
      "Epoch 1920: train loss: 0.057990510016679764\n",
      "Epoch 1930: train loss: 0.057944562286138535\n",
      "Epoch 1940: train loss: 0.05789930373430252\n",
      "Epoch 1950: train loss: 0.057854726910591125\n",
      "Epoch 1960: train loss: 0.057810839265584946\n",
      "Epoch 1970: train loss: 0.05776762217283249\n",
      "Epoch 1980: train loss: 0.057725075632333755\n",
      "Epoch 1990: train loss: 0.057683177292346954\n",
      "Epoch 2000: train loss: 0.05764193832874298\n",
      "Epoch 2010: train loss: 0.057601314038038254\n",
      "Epoch 2020: train loss: 0.057561345398426056\n",
      "Epoch 2030: train loss: 0.05752198025584221\n",
      "Epoch 2040: train loss: 0.05748322233557701\n",
      "Epoch 2050: train loss: 0.05744507536292076\n",
      "Epoch 2060: train loss: 0.05740752071142197\n",
      "Epoch 2070: train loss: 0.05737053602933884\n",
      "Epoch 2080: train loss: 0.057334139943122864\n",
      "Epoch 2090: train loss: 0.05729831010103226\n",
      "Epoch 2100: train loss: 0.05726303905248642\n",
      "Epoch 2110: train loss: 0.057228315621614456\n",
      "Epoch 2120: train loss: 0.057194147258996964\n",
      "Epoch 2130: train loss: 0.05716048553586006\n",
      "Epoch 2140: train loss: 0.05712735652923584\n",
      "Epoch 2150: train loss: 0.0570947527885437\n",
      "Epoch 2160: train loss: 0.05706263706088066\n",
      "Epoch 2170: train loss: 0.05703103914856911\n",
      "Epoch 2180: train loss: 0.05699992552399635\n",
      "Epoch 2190: train loss: 0.056969285011291504\n",
      "Epoch 2200: train loss: 0.05693914368748665\n",
      "Epoch 2210: train loss: 0.056909434497356415\n",
      "Epoch 2220: train loss: 0.05688020586967468\n",
      "Epoch 2230: train loss: 0.05685139819979668\n",
      "Epoch 2240: train loss: 0.05682307109236717\n",
      "Epoch 2250: train loss: 0.0567951537668705\n",
      "Epoch 2260: train loss: 0.056767672300338745\n",
      "Epoch 2270: train loss: 0.056740596890449524\n",
      "Epoch 2280: train loss: 0.05671395733952522\n",
      "Epoch 2290: train loss: 0.056687697768211365\n",
      "Epoch 2300: train loss: 0.05666184425354004\n",
      "Epoch 2310: train loss: 0.056636370718479156\n",
      "Epoch 2320: train loss: 0.05661129951477051\n",
      "Epoch 2330: train loss: 0.056586578488349915\n",
      "Epoch 2340: train loss: 0.05656222999095917\n",
      "Epoch 2350: train loss: 0.05653824284672737\n",
      "Epoch 2360: train loss: 0.05651460587978363\n",
      "Epoch 2370: train loss: 0.05649132654070854\n",
      "Epoch 2380: train loss: 0.056468382477760315\n",
      "Epoch 2390: train loss: 0.056445762515068054\n",
      "Epoch 2400: train loss: 0.05642347037792206\n",
      "Epoch 2410: train loss: 0.05640150606632233\n",
      "Epoch 2420: train loss: 0.05637983977794647\n",
      "Epoch 2430: train loss: 0.05635850876569748\n",
      "Epoch 2440: train loss: 0.05633744224905968\n",
      "Epoch 2450: train loss: 0.056316688656806946\n",
      "Epoch 2460: train loss: 0.056296221911907196\n",
      "Epoch 2470: train loss: 0.05627603456377983\n",
      "Epoch 2480: train loss: 0.05625611171126366\n",
      "Epoch 2490: train loss: 0.056236498057842255\n",
      "Epoch 2500: train loss: 0.05621710047125816\n",
      "Epoch 2510: train loss: 0.056197986006736755\n",
      "Epoch 2520: train loss: 0.056179121136665344\n",
      "Epoch 2530: train loss: 0.05616050586104393\n",
      "Epoch 2540: train loss: 0.056142136454582214\n",
      "Epoch 2550: train loss: 0.056123990565538406\n",
      "Epoch 2560: train loss: 0.05610610917210579\n",
      "Epoch 2570: train loss: 0.05608843266963959\n",
      "Epoch 2580: train loss: 0.0560709647834301\n",
      "Epoch 2590: train loss: 0.05605372413992882\n",
      "Epoch 2600: train loss: 0.05603671073913574\n",
      "Epoch 2610: train loss: 0.05601990967988968\n",
      "Epoch 2620: train loss: 0.05600328743457794\n",
      "Epoch 2630: train loss: 0.05598687008023262\n",
      "Epoch 2640: train loss: 0.05597067251801491\n",
      "Epoch 2650: train loss: 0.05595463514328003\n",
      "Epoch 2660: train loss: 0.055938806384801865\n",
      "Epoch 2670: train loss: 0.055923156440258026\n",
      "Epoch 2680: train loss: 0.05590766668319702\n",
      "Epoch 2690: train loss: 0.05589236319065094\n",
      "Epoch 2700: train loss: 0.05587724596261978\n",
      "Epoch 2710: train loss: 0.05586228147149086\n",
      "Epoch 2720: train loss: 0.055847473442554474\n",
      "Epoch 2730: train loss: 0.05583283305168152\n",
      "Epoch 2740: train loss: 0.0558183453977108\n",
      "Epoch 2750: train loss: 0.05580401420593262\n",
      "Epoch 2760: train loss: 0.05578983202576637\n",
      "Epoch 2770: train loss: 0.05577578768134117\n",
      "Epoch 2780: train loss: 0.055761899799108505\n",
      "Epoch 2790: train loss: 0.055748146027326584\n",
      "Epoch 2800: train loss: 0.0557345375418663\n",
      "Epoch 2810: train loss: 0.05572104826569557\n",
      "Epoch 2820: train loss: 0.05570768192410469\n",
      "Epoch 2830: train loss: 0.05569445341825485\n",
      "Epoch 2840: train loss: 0.05568136274814606\n",
      "Epoch 2850: train loss: 0.05566838011145592\n",
      "Epoch 2860: train loss: 0.05565553158521652\n",
      "Epoch 2870: train loss: 0.05564279109239578\n",
      "Epoch 2880: train loss: 0.05563017353415489\n",
      "Epoch 2890: train loss: 0.05561766400933266\n",
      "Epoch 2900: train loss: 0.05560525879263878\n",
      "Epoch 2910: train loss: 0.05559295415878296\n",
      "Epoch 2920: train loss: 0.05558077618479729\n",
      "Epoch 2930: train loss: 0.055568695068359375\n",
      "Epoch 2940: train loss: 0.05555671080946922\n",
      "Epoch 2950: train loss: 0.05554482713341713\n",
      "Epoch 2960: train loss: 0.0555330365896225\n",
      "Epoch 2970: train loss: 0.055521368980407715\n",
      "Epoch 2980: train loss: 0.0555097758769989\n",
      "Epoch 2990: train loss: 0.055498283356428146\n",
      "Epoch 3000: train loss: 0.055486857891082764\n",
      "Epoch 3010: train loss: 0.05547554790973663\n",
      "Epoch 3020: train loss: 0.05546432361006737\n",
      "Epoch 3030: train loss: 0.05545315518975258\n",
      "Epoch 3040: train loss: 0.05544210970401764\n",
      "Epoch 3050: train loss: 0.055431120097637177\n",
      "Epoch 3060: train loss: 0.05542023852467537\n",
      "Epoch 3070: train loss: 0.05540940538048744\n",
      "Epoch 3080: train loss: 0.05539870262145996\n",
      "Epoch 3090: train loss: 0.055388033390045166\n",
      "Epoch 3100: train loss: 0.05537744238972664\n",
      "Epoch 3110: train loss: 0.05536693334579468\n",
      "Epoch 3120: train loss: 0.05535650998353958\n",
      "Epoch 3130: train loss: 0.055346157401800156\n",
      "Epoch 3140: train loss: 0.055335864424705505\n",
      "Epoch 3150: train loss: 0.055325645953416824\n",
      "Epoch 3160: train loss: 0.055315498262643814\n",
      "Epoch 3170: train loss: 0.055305421352386475\n",
      "Epoch 3180: train loss: 0.055295418947935104\n",
      "Epoch 3190: train loss: 0.05528547242283821\n",
      "Epoch 3200: train loss: 0.05527559667825699\n",
      "Epoch 3210: train loss: 0.055265799164772034\n",
      "Epoch 3220: train loss: 0.05525605008006096\n",
      "Epoch 3230: train loss: 0.055246368050575256\n",
      "Epoch 3240: train loss: 0.055236753076314926\n",
      "Epoch 3250: train loss: 0.05522720143198967\n",
      "Epoch 3260: train loss: 0.05521771311759949\n",
      "Epoch 3270: train loss: 0.05520827695727348\n",
      "Epoch 3280: train loss: 0.05519890785217285\n",
      "Epoch 3290: train loss: 0.055189598351716995\n",
      "Epoch 3300: train loss: 0.05518035590648651\n",
      "Epoch 03309: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 3310: train loss: 0.05517114698886871\n",
      "Epoch 03316: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 3320: train loss: 0.0551689937710762\n",
      "Epoch 03322: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 03328: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 3330: train loss: 0.05516858026385307\n",
      "Epoch 03334: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 03340: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 3340: train loss: 0.055168524384498596\n",
      "Epoch 03346: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 3350: train loss: 0.055168524384498596\n",
      "Epoch 03352: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 3352, loss: 0.055168524384498596\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
