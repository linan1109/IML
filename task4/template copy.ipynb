{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 1000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8083d0ea075c45c5805912e41bb303e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.22172975515072443, val loss: 0.028617265075445174\n",
      "Epoch 2: train loss: 0.016385001859494618, val loss: 0.017981045350432396\n",
      "Epoch 3: train loss: 0.011280111132835856, val loss: 0.019214819803833962\n",
      "Epoch 4: train loss: 0.01313225786600794, val loss: 0.021417522490024567\n",
      "Epoch 5: train loss: 0.013379970808190351, val loss: 0.01715828260779381\n",
      "Epoch 6: train loss: 0.01229264594796969, val loss: 0.016820656806230544\n",
      "Epoch 7: train loss: 0.011458797817783696, val loss: 0.016914474219083785\n",
      "Epoch 8: train loss: 0.011887369862472524, val loss: 0.011819722503423691\n",
      "Epoch 9: train loss: 0.010222963360043205, val loss: 0.013717557489871978\n",
      "Epoch 10: train loss: 0.010240353781805963, val loss: 0.013595572598278522\n",
      "Epoch 11: train loss: 0.010367008017186, val loss: 0.01895168496668339\n",
      "Epoch 12: train loss: 0.013771282754990519, val loss: 0.020211945101618767\n",
      "Epoch 13: train loss: 0.011134911150348429, val loss: 0.016097051672637464\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 14: train loss: 0.010508851380220481, val loss: 0.01218103152513504\n",
      "Epoch 15: train loss: 0.004164806962013244, val loss: 0.005913694612681866\n",
      "Epoch 16: train loss: 0.0019565709611515003, val loss: 0.005837265886366367\n",
      "Epoch 17: train loss: 0.001322801887627919, val loss: 0.005575311284512281\n",
      "Epoch 18: train loss: 0.0010292844040965547, val loss: 0.005564275808632374\n",
      "Epoch 19: train loss: 0.0009025925833883943, val loss: 0.005511152300983668\n",
      "Epoch 20: train loss: 0.000813155750848581, val loss: 0.005056186579167843\n",
      "Epoch 21: train loss: 0.0008338405651287461, val loss: 0.005576166812330485\n",
      "Epoch 22: train loss: 0.0010065264273434878, val loss: 0.005347482647746801\n",
      "Epoch 23: train loss: 0.0015421062250792676, val loss: 0.007206785045564174\n",
      "Epoch 24: train loss: 0.0027492770204054456, val loss: 0.005905467711389065\n",
      "Epoch 25: train loss: 0.00413189267108635, val loss: 0.008744110077619553\n",
      "Epoch 00026: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 26: train loss: 0.0043638165669933875, val loss: 0.0063771833963692185\n",
      "Epoch 27: train loss: 0.0019614834222866566, val loss: 0.0042149370759725574\n",
      "Epoch 28: train loss: 0.0009240868794731796, val loss: 0.0042681492604315285\n",
      "Epoch 29: train loss: 0.0006272248975951604, val loss: 0.00437968085706234\n",
      "Epoch 30: train loss: 0.00048357087894038733, val loss: 0.004356159599497914\n",
      "Epoch 31: train loss: 0.00041149935744968905, val loss: 0.004392591618001461\n",
      "Epoch 32: train loss: 0.0003641667689995042, val loss: 0.00440447386726737\n",
      "Epoch 00033: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 33: train loss: 0.00033515337963911646, val loss: 0.004391226436942816\n",
      "Epoch 34: train loss: 0.00022810664182832958, val loss: 0.0043650283925235275\n",
      "Epoch 35: train loss: 0.00020055646866046805, val loss: 0.004385123433545232\n",
      "Epoch 36: train loss: 0.00019299178562668743, val loss: 0.004339731009677053\n",
      "Epoch 37: train loss: 0.00018725479957029908, val loss: 0.004349981838837266\n",
      "Epoch 38: train loss: 0.00018076533331934895, val loss: 0.004353864315897227\n",
      "Epoch 00039: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 39: train loss: 0.00017827775259977397, val loss: 0.004378611750900745\n",
      "Epoch 40: train loss: 0.00014792522005926893, val loss: 0.004341434098780155\n",
      "Epoch 41: train loss: 0.0001429186167898683, val loss: 0.004350450685247779\n",
      "Epoch 42: train loss: 0.00014153608444089793, val loss: 0.004356081422418356\n",
      "Epoch 43: train loss: 0.00014008077268716783, val loss: 0.004352807398885488\n",
      "Epoch 44: train loss: 0.00013837722906063558, val loss: 0.004348485637456179\n",
      "Epoch 00045: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 45: train loss: 0.00013657743664344355, val loss: 0.004361186541616917\n",
      "Epoch 46: train loss: 0.00012567808929526684, val loss: 0.004349362835288048\n",
      "Epoch 47: train loss: 0.0001241837633994636, val loss: 0.004359187478199601\n",
      "Epoch 48: train loss: 0.00012361970299151631, val loss: 0.004345825964584947\n",
      "Epoch 49: train loss: 0.0001231467480669083, val loss: 0.004353674702346325\n",
      "Epoch 50: train loss: 0.00012289164584529188, val loss: 0.00435950725339353\n",
      "Epoch 00051: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 51: train loss: 0.00012191840052980055, val loss: 0.004364922687411309\n",
      "Early stop at epoch 51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d926a4c66fe74a32972a4e1de8cf25d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 2.121460199356079\n",
      "Epoch 20: train loss: 1.4736509323120117\n",
      "Epoch 30: train loss: 1.0016741752624512\n",
      "Epoch 40: train loss: 0.6867256164550781\n",
      "Epoch 50: train loss: 0.4960576593875885\n",
      "Epoch 60: train loss: 0.3925231993198395\n",
      "Epoch 70: train loss: 0.342799574136734\n",
      "Epoch 80: train loss: 0.32201364636421204\n",
      "Epoch 90: train loss: 0.3145182430744171\n",
      "Epoch 100: train loss: 0.3120565414428711\n",
      "Epoch 110: train loss: 0.31104913353919983\n",
      "Epoch 120: train loss: 0.310285747051239\n",
      "Epoch 130: train loss: 0.309492826461792\n",
      "Epoch 140: train loss: 0.3086583912372589\n",
      "Epoch 150: train loss: 0.3078019618988037\n",
      "Epoch 160: train loss: 0.3069305121898651\n",
      "Epoch 170: train loss: 0.3060436248779297\n",
      "Epoch 180: train loss: 0.3051401376724243\n",
      "Epoch 190: train loss: 0.3042202293872833\n",
      "Epoch 200: train loss: 0.3032849431037903\n",
      "Epoch 210: train loss: 0.30233535170555115\n",
      "Epoch 220: train loss: 0.30137214064598083\n",
      "Epoch 230: train loss: 0.3003963828086853\n",
      "Epoch 240: train loss: 0.2994089126586914\n",
      "Epoch 250: train loss: 0.2984103262424469\n",
      "Epoch 260: train loss: 0.2974015772342682\n",
      "Epoch 270: train loss: 0.29638317227363586\n",
      "Epoch 280: train loss: 0.29535573720932007\n",
      "Epoch 290: train loss: 0.2943199574947357\n",
      "Epoch 300: train loss: 0.29327648878097534\n",
      "Epoch 310: train loss: 0.2922256886959076\n",
      "Epoch 320: train loss: 0.2911681830883026\n",
      "Epoch 330: train loss: 0.290104478597641\n",
      "Epoch 340: train loss: 0.28903505206108093\n",
      "Epoch 350: train loss: 0.28796032071113586\n",
      "Epoch 360: train loss: 0.2868807017803192\n",
      "Epoch 370: train loss: 0.2857967019081116\n",
      "Epoch 380: train loss: 0.28470858931541443\n",
      "Epoch 390: train loss: 0.2836167514324188\n",
      "Epoch 400: train loss: 0.28252166509628296\n",
      "Epoch 410: train loss: 0.28142356872558594\n",
      "Epoch 420: train loss: 0.2803228199481964\n",
      "Epoch 430: train loss: 0.27921968698501587\n",
      "Epoch 440: train loss: 0.27811458706855774\n",
      "Epoch 450: train loss: 0.2770077586174011\n",
      "Epoch 460: train loss: 0.2758994400501251\n",
      "Epoch 470: train loss: 0.2747898995876312\n",
      "Epoch 480: train loss: 0.2736794352531433\n",
      "Epoch 490: train loss: 0.2725682258605957\n",
      "Epoch 500: train loss: 0.27145659923553467\n",
      "Epoch 510: train loss: 0.2703447937965393\n",
      "Epoch 520: train loss: 0.26923292875289917\n",
      "Epoch 530: train loss: 0.26812124252319336\n",
      "Epoch 540: train loss: 0.26701000332832336\n",
      "Epoch 550: train loss: 0.26589930057525635\n",
      "Epoch 560: train loss: 0.2647893726825714\n",
      "Epoch 570: train loss: 0.26368045806884766\n",
      "Epoch 580: train loss: 0.26257261633872986\n",
      "Epoch 590: train loss: 0.26146602630615234\n",
      "Epoch 600: train loss: 0.2603609263896942\n",
      "Epoch 610: train loss: 0.2592574656009674\n",
      "Epoch 620: train loss: 0.2581557333469391\n",
      "Epoch 630: train loss: 0.2570558488368988\n",
      "Epoch 640: train loss: 0.25595802068710327\n",
      "Epoch 650: train loss: 0.25486230850219727\n",
      "Epoch 660: train loss: 0.2537688910961151\n",
      "Epoch 670: train loss: 0.25267791748046875\n",
      "Epoch 680: train loss: 0.25158941745758057\n",
      "Epoch 690: train loss: 0.2505035102367401\n",
      "Epoch 700: train loss: 0.2494203746318817\n",
      "Epoch 710: train loss: 0.2483399510383606\n",
      "Epoch 720: train loss: 0.24726252257823944\n",
      "Epoch 730: train loss: 0.24618813395500183\n",
      "Epoch 740: train loss: 0.24511680006980896\n",
      "Epoch 750: train loss: 0.244048610329628\n",
      "Epoch 760: train loss: 0.24298377335071564\n",
      "Epoch 770: train loss: 0.24192224442958832\n",
      "Epoch 780: train loss: 0.24086406826972961\n",
      "Epoch 790: train loss: 0.23980943858623505\n",
      "Epoch 800: train loss: 0.23875835537910461\n",
      "Epoch 810: train loss: 0.2377108782529831\n",
      "Epoch 820: train loss: 0.23666705191135406\n",
      "Epoch 830: train loss: 0.23562701046466827\n",
      "Epoch 840: train loss: 0.23459067940711975\n",
      "Epoch 850: train loss: 0.23355822265148163\n",
      "Epoch 860: train loss: 0.2325296401977539\n",
      "Epoch 870: train loss: 0.23150502145290375\n",
      "Epoch 880: train loss: 0.23048436641693115\n",
      "Epoch 890: train loss: 0.22946768999099731\n",
      "Epoch 900: train loss: 0.22845514118671417\n",
      "Epoch 910: train loss: 0.22744666039943695\n",
      "Epoch 920: train loss: 0.22644230723381042\n",
      "Epoch 930: train loss: 0.22544211149215698\n",
      "Epoch 940: train loss: 0.2244461327791214\n",
      "Epoch 950: train loss: 0.2234543114900589\n",
      "Epoch 960: train loss: 0.222466841340065\n",
      "Epoch 970: train loss: 0.2214835286140442\n",
      "Epoch 980: train loss: 0.2205045521259308\n",
      "Epoch 990: train loss: 0.2195299118757248\n",
      "Epoch 1000: train loss: 0.2185596078634262\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
