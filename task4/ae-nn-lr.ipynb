{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "#     x_train = scaler.transform(x_train)\n",
    "#     x_test_transed = scaler.transform(x_test)\n",
    "#     x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3fe8140f494d9581bd4366bb4fae2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.0469012905122066, val loss: 0.031096543431282042\n",
      "Epoch 2: train loss: 0.02780783176695814, val loss: 0.025113112807273866\n",
      "Epoch 3: train loss: 0.02329130178264209, val loss: 0.03144400544464588\n",
      "Epoch 4: train loss: 0.019720219261488137, val loss: 0.01859737168252468\n",
      "Epoch 5: train loss: 0.01733484455152434, val loss: 0.016893097937107086\n",
      "Epoch 6: train loss: 0.015799177887488386, val loss: 0.015091578602790832\n",
      "Epoch 7: train loss: 0.014519363838342987, val loss: 0.014138679437339307\n",
      "Epoch 8: train loss: 0.0136059241332874, val loss: 0.013433186240494251\n",
      "Epoch 9: train loss: 0.012916220368779435, val loss: 0.012732396207749843\n",
      "Epoch 10: train loss: 0.012306574097397376, val loss: 0.01217357062548399\n",
      "Epoch 11: train loss: 0.011893344141694965, val loss: 0.011822087474167346\n",
      "Epoch 12: train loss: 0.011431326527072459, val loss: 0.01122866529226303\n",
      "Epoch 13: train loss: 0.011053578493090308, val loss: 0.010876182943582535\n",
      "Epoch 14: train loss: 0.010694082257546941, val loss: 0.010645002208650112\n",
      "Epoch 15: train loss: 0.010452074854501656, val loss: 0.010489404842257499\n",
      "Epoch 16: train loss: 0.010187612695657476, val loss: 0.010132905222475529\n",
      "Epoch 17: train loss: 0.009948079449333706, val loss: 0.009969291381537914\n",
      "Epoch 18: train loss: 0.009749748649493772, val loss: 0.009793907716870307\n",
      "Epoch 19: train loss: 0.009554835556417096, val loss: 0.009523732982575894\n",
      "Epoch 20: train loss: 0.009381099184100725, val loss: 0.009479434013366698\n",
      "Epoch 21: train loss: 0.009220321876662118, val loss: 0.009427709609270096\n",
      "Epoch 22: train loss: 0.009085588679934035, val loss: 0.009080259159207344\n",
      "Epoch 23: train loss: 0.008944652908918809, val loss: 0.008949849300086498\n",
      "Epoch 24: train loss: 0.008841817305100207, val loss: 0.008869983330368996\n",
      "Epoch 25: train loss: 0.008728243009320328, val loss: 0.00874451070278883\n",
      "Epoch 26: train loss: 0.008615208795180126, val loss: 0.008804517179727554\n",
      "Epoch 27: train loss: 0.008526274099946022, val loss: 0.008619023732841015\n",
      "Epoch 28: train loss: 0.008428988217851338, val loss: 0.008531093545258045\n",
      "Epoch 29: train loss: 0.008351648750962044, val loss: 0.00846649669110775\n",
      "Epoch 30: train loss: 0.00828443789847043, val loss: 0.00838675633072853\n",
      "Epoch 31: train loss: 0.008214910913790976, val loss: 0.008393155902624131\n",
      "Epoch 32: train loss: 0.008129642615664978, val loss: 0.00834573670476675\n",
      "Epoch 33: train loss: 0.008086349635708088, val loss: 0.008223866641521454\n",
      "Epoch 34: train loss: 0.008018114393614993, val loss: 0.008203204303979873\n",
      "Epoch 35: train loss: 0.007950458725192108, val loss: 0.00801655299961567\n",
      "Epoch 36: train loss: 0.007904076988295632, val loss: 0.0080622940659523\n",
      "Epoch 37: train loss: 0.007896203962667865, val loss: 0.008121857039630413\n",
      "Epoch 38: train loss: 0.007822217165055323, val loss: 0.00797949694097042\n",
      "Epoch 39: train loss: 0.007774166534445724, val loss: 0.007995062440633773\n",
      "Epoch 40: train loss: 0.007750992306337065, val loss: 0.007767502039670944\n",
      "Epoch 41: train loss: 0.007693190074088622, val loss: 0.007757235571742058\n",
      "Epoch 42: train loss: 0.007652831862167436, val loss: 0.007913565717637539\n",
      "Epoch 43: train loss: 0.007622636170411597, val loss: 0.00765286361053586\n",
      "Epoch 44: train loss: 0.007611066417882637, val loss: 0.0077491700835525985\n",
      "Epoch 45: train loss: 0.007571824979569231, val loss: 0.0077600951828062535\n",
      "Epoch 46: train loss: 0.007551927245241038, val loss: 0.0076717837043106555\n",
      "Epoch 47: train loss: 0.007514458254891999, val loss: 0.007697872437536717\n",
      "Epoch 48: train loss: 0.007489797831037823, val loss: 0.0075547613948583605\n",
      "Epoch 49: train loss: 0.007457899831235409, val loss: 0.007626056551933289\n",
      "Epoch 50: train loss: 0.007417946329074247, val loss: 0.00756771007925272\n",
      "Epoch 51: train loss: 0.007413369952567986, val loss: 0.007652821362018585\n",
      "Epoch 52: train loss: 0.0074192840107241455, val loss: 0.007488698862493038\n",
      "Epoch 53: train loss: 0.007358843402290831, val loss: 0.007560013670474291\n",
      "Epoch 54: train loss: 0.0073569581469102785, val loss: 0.0074273099973797795\n",
      "Epoch 55: train loss: 0.007306756385278945, val loss: 0.007490650933235884\n",
      "Epoch 56: train loss: 0.007304709674448383, val loss: 0.007434236787259579\n",
      "Epoch 57: train loss: 0.0072946891672149, val loss: 0.007530290696769953\n",
      "Epoch 58: train loss: 0.007273629921887602, val loss: 0.007371317598968744\n",
      "Epoch 59: train loss: 0.007262290192653938, val loss: 0.007460272502154112\n",
      "Epoch 60: train loss: 0.007235251736428056, val loss: 0.0073816291801631455\n",
      "Epoch 61: train loss: 0.007203951686620713, val loss: 0.007484361633658409\n",
      "Epoch 62: train loss: 0.007199895604684645, val loss: 0.007468550730496645\n",
      "Epoch 63: train loss: 0.007166644044828658, val loss: 0.007329484488815069\n",
      "Epoch 64: train loss: 0.007184880879460549, val loss: 0.007448689803481102\n",
      "Epoch 65: train loss: 0.007145312536583871, val loss: 0.007366571824997664\n",
      "Epoch 66: train loss: 0.007134700172713825, val loss: 0.007392173051834107\n",
      "Epoch 67: train loss: 0.007111583614987986, val loss: 0.0071404541619122025\n",
      "Epoch 68: train loss: 0.0070798615606463685, val loss: 0.0073528547212481496\n",
      "Epoch 69: train loss: 0.007112848847192161, val loss: 0.007313946418464183\n",
      "Epoch 70: train loss: 0.0070771142142463704, val loss: 0.00719283377751708\n",
      "Epoch 71: train loss: 0.0070644869168802185, val loss: 0.007267828196287155\n",
      "Epoch 72: train loss: 0.0070554604586593956, val loss: 0.007193887088447809\n",
      "Epoch 00073: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 73: train loss: 0.00706119509816778, val loss: 0.00724559872597456\n",
      "Epoch 74: train loss: 0.006868436271818925, val loss: 0.006929340917617083\n",
      "Epoch 75: train loss: 0.006803827017469673, val loss: 0.007052438642829657\n",
      "Epoch 76: train loss: 0.006750532613420973, val loss: 0.0069196811132133\n",
      "Epoch 77: train loss: 0.006723813196834252, val loss: 0.00699244624003768\n",
      "Epoch 78: train loss: 0.006721526811241495, val loss: 0.006882498387247324\n",
      "Epoch 79: train loss: 0.0066976111026442785, val loss: 0.006847699034959078\n",
      "Epoch 80: train loss: 0.006677151864280506, val loss: 0.006841430515050888\n",
      "Epoch 81: train loss: 0.0066566855250572675, val loss: 0.00695074412971735\n",
      "Epoch 82: train loss: 0.006672360452918373, val loss: 0.007072821844369173\n",
      "Epoch 83: train loss: 0.006637853989719737, val loss: 0.00674835966527462\n",
      "Epoch 84: train loss: 0.006628037598060102, val loss: 0.006866133838891983\n",
      "Epoch 85: train loss: 0.006616244386349406, val loss: 0.006735007114708424\n",
      "Epoch 86: train loss: 0.006623412217412676, val loss: 0.006880276031792164\n",
      "Epoch 87: train loss: 0.006613319417043608, val loss: 0.006816360291093588\n",
      "Epoch 88: train loss: 0.006606617604135251, val loss: 0.006749978292733431\n",
      "Epoch 89: train loss: 0.006602974255094114, val loss: 0.006759032521396875\n",
      "Epoch 90: train loss: 0.006590780737220633, val loss: 0.0068302281498909\n",
      "Epoch 00091: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 91: train loss: 0.006608600339795254, val loss: 0.006824346397072077\n",
      "Epoch 92: train loss: 0.006539716640297248, val loss: 0.006867225471884012\n",
      "Epoch 93: train loss: 0.006511002846518342, val loss: 0.006726172015070915\n",
      "Epoch 94: train loss: 0.006516921327354349, val loss: 0.006611935965716839\n",
      "Epoch 95: train loss: 0.006497810773171332, val loss: 0.006767438981682062\n",
      "Epoch 96: train loss: 0.006496362904048696, val loss: 0.006679709922522307\n",
      "Epoch 97: train loss: 0.006498606497003716, val loss: 0.0067314358167350295\n",
      "Epoch 98: train loss: 0.006494533659624202, val loss: 0.006801636427640915\n",
      "Epoch 99: train loss: 0.00648701890793686, val loss: 0.006729872930794955\n",
      "Epoch 00100: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 100: train loss: 0.006463697097055158, val loss: 0.0067594250664114956\n",
      "Epoch 101: train loss: 0.006443518539852634, val loss: 0.006721606444567442\n",
      "Epoch 102: train loss: 0.0064651033681418215, val loss: 0.006685799367725849\n",
      "Epoch 103: train loss: 0.006460018842424057, val loss: 0.006597794000059366\n",
      "Epoch 104: train loss: 0.006435433907928515, val loss: 0.00681792014092207\n",
      "Epoch 105: train loss: 0.006453398003109864, val loss: 0.006694670155644416\n",
      "Epoch 106: train loss: 0.006463103898705877, val loss: 0.006580753702670336\n",
      "Epoch 107: train loss: 0.006460384529768205, val loss: 0.006668709192425013\n",
      "Epoch 108: train loss: 0.00642414042893417, val loss: 0.006664627231657505\n",
      "Epoch 109: train loss: 0.006437126350874196, val loss: 0.006689981196075678\n",
      "Epoch 110: train loss: 0.006448327786460215, val loss: 0.006722320567816496\n",
      "Epoch 111: train loss: 0.006442438765523993, val loss: 0.006754656154662371\n",
      "Epoch 00112: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 112: train loss: 0.006423617571592331, val loss: 0.0066058412715792655\n",
      "Epoch 113: train loss: 0.006433453320468567, val loss: 0.006641214922070504\n",
      "Epoch 114: train loss: 0.00642693356363749, val loss: 0.006642553210258484\n",
      "Epoch 115: train loss: 0.006433490895875254, val loss: 0.006605368185788393\n",
      "Epoch 116: train loss: 0.0064386641823363545, val loss: 0.006659500945359468\n",
      "Epoch 117: train loss: 0.006434940034181488, val loss: 0.006580899570137262\n",
      "Epoch 00118: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 118: train loss: 0.006428486623022021, val loss: 0.006720255710184574\n",
      "Epoch 119: train loss: 0.00642720886190631, val loss: 0.006727414332330227\n",
      "Epoch 120: train loss: 0.006436456439902588, val loss: 0.006770875696092844\n",
      "Epoch 121: train loss: 0.0064332267270252414, val loss: 0.006786808125674724\n",
      "Epoch 122: train loss: 0.006435384593295808, val loss: 0.006695361398160457\n",
      "Epoch 123: train loss: 0.006427456451313836, val loss: 0.006705842852592468\n",
      "Epoch 00124: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 124: train loss: 0.00642325260512987, val loss: 0.006708526831120253\n",
      "Epoch 125: train loss: 0.0064278794652497286, val loss: 0.006715653978288174\n",
      "Epoch 126: train loss: 0.006429642828675557, val loss: 0.006749392062425613\n",
      "Epoch 127: train loss: 0.006414026724744816, val loss: 0.006697491426020861\n",
      "Epoch 128: train loss: 0.006433675039787682, val loss: 0.006563794478774071\n",
      "Epoch 129: train loss: 0.00642102749417631, val loss: 0.006730808094143868\n",
      "Epoch 130: train loss: 0.006428087685257196, val loss: 0.006584529150277376\n",
      "Epoch 131: train loss: 0.006411299957152532, val loss: 0.006604758284986019\n",
      "Epoch 132: train loss: 0.006428648252192201, val loss: 0.0067538366131484505\n",
      "Epoch 133: train loss: 0.00643660344928503, val loss: 0.006641007702797652\n",
      "Epoch 00134: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 134: train loss: 0.006404328998634402, val loss: 0.00659010948240757\n",
      "Epoch 135: train loss: 0.00642369503999243, val loss: 0.006670597244054079\n",
      "Epoch 136: train loss: 0.00639466364240768, val loss: 0.006757073659449816\n",
      "Epoch 137: train loss: 0.006407704571756173, val loss: 0.006609272476285696\n",
      "Epoch 138: train loss: 0.006401557351861681, val loss: 0.006713495820760727\n",
      "Epoch 139: train loss: 0.00641994814833208, val loss: 0.00678346599265933\n",
      "Epoch 00140: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 140: train loss: 0.006411751720294052, val loss: 0.00669024945050478\n",
      "Early stop at epoch 140\n"
     ]
    }
   ],
   "source": [
    "eval_size = 1000\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(ae_model.parameters(), lr=learning_rate)\n",
    "    # optimizer = torch.optim.SGD(ae_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 1000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        \n",
    "        # if(epoch % 10 == 0):\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.seq[:-3](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aed_x_pretrain = ae_model.encode(torch.tensor(x_pretrain, dtype=torch.float).to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026e8a1e41f1472392f8e5c31e90ec21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.5522916644422375, val loss: 0.097227508187294\n",
      "Epoch 2: train loss: 0.06730460220271227, val loss: 0.05047814780473709\n",
      "Epoch 3: train loss: 0.04896574281247295, val loss: 0.04429624062776565\n",
      "Epoch 4: train loss: 0.04463283661555271, val loss: 0.0445116383433342\n",
      "Epoch 5: train loss: 0.041710337719442894, val loss: 0.03859437975287437\n",
      "Epoch 6: train loss: 0.03938714456071659, val loss: 0.03859195524454117\n",
      "Epoch 7: train loss: 0.03886460914569242, val loss: 0.04096921890974045\n",
      "Epoch 8: train loss: 0.038402497321975475, val loss: 0.04493274086713791\n",
      "Epoch 9: train loss: 0.03774038075488441, val loss: 0.036222328051924706\n",
      "Epoch 10: train loss: 0.03642653064885918, val loss: 0.035131525963544845\n",
      "Epoch 11: train loss: 0.037953362908898565, val loss: 0.04176260051131248\n",
      "Epoch 12: train loss: 0.03550053739791014, val loss: 0.034019233465194705\n",
      "Epoch 13: train loss: 0.03675932344490168, val loss: 0.03461115227639675\n",
      "Epoch 14: train loss: 0.035345912370754747, val loss: 0.031668058022856715\n",
      "Epoch 15: train loss: 0.0346910058831682, val loss: 0.04582371509075165\n",
      "Epoch 16: train loss: 0.03387172825725711, val loss: 0.030020295992493628\n",
      "Epoch 17: train loss: 0.032661586165428164, val loss: 0.030623690620064734\n",
      "Epoch 18: train loss: 0.03364931443941836, val loss: 0.034184048786759375\n",
      "Epoch 19: train loss: 0.03327818321086923, val loss: 0.037543128550052644\n",
      "Epoch 20: train loss: 0.033891522020101546, val loss: 0.02719837284088135\n",
      "Epoch 21: train loss: 0.031404952974951994, val loss: 0.03101609690487385\n",
      "Epoch 22: train loss: 0.028461585050334737, val loss: 0.04738345587253571\n",
      "Epoch 23: train loss: 0.02971771472023458, val loss: 0.02921913167834282\n",
      "Epoch 24: train loss: 0.029641828013622033, val loss: 0.029392483547329904\n",
      "Epoch 25: train loss: 0.030610362625851924, val loss: 0.03235441166162491\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.5000e-02.\n",
      "Epoch 26: train loss: 0.02787107412425839, val loss: 0.030058344930410385\n",
      "Epoch 27: train loss: 0.025050444090548827, val loss: 0.02416369765996933\n",
      "Epoch 28: train loss: 0.024917873253931806, val loss: 0.029339070305228233\n",
      "Epoch 29: train loss: 0.025012795737203285, val loss: 0.02573771610856056\n",
      "Epoch 30: train loss: 0.02452457640730605, val loss: 0.02359799674153328\n",
      "Epoch 31: train loss: 0.024946643977444997, val loss: 0.02322266198694706\n",
      "Epoch 32: train loss: 0.02454928434472911, val loss: 0.025029932364821435\n",
      "Epoch 33: train loss: 0.024757480118955885, val loss: 0.022617049351334573\n",
      "Epoch 34: train loss: 0.02499507412770573, val loss: 0.024306975647807122\n",
      "Epoch 35: train loss: 0.024663012568439755, val loss: 0.022093851402401925\n",
      "Epoch 36: train loss: 0.02446556275474782, val loss: 0.0278630141466856\n",
      "Epoch 37: train loss: 0.02559802176331987, val loss: 0.023836378172039985\n",
      "Epoch 38: train loss: 0.024513677940684923, val loss: 0.024427844032645224\n",
      "Epoch 39: train loss: 0.02452198005391627, val loss: 0.02345077207684517\n",
      "Epoch 40: train loss: 0.023936829979322394, val loss: 0.030172371968626977\n",
      "Epoch 00041: reducing learning rate of group 0 to 4.5000e-03.\n",
      "Epoch 41: train loss: 0.024529777565476845, val loss: 0.024594392478466034\n",
      "Epoch 42: train loss: 0.023175026324938754, val loss: 0.02223470212519169\n",
      "Epoch 43: train loss: 0.02269400535158965, val loss: 0.022437700912356376\n",
      "Epoch 44: train loss: 0.02302141646523865, val loss: 0.02257571317255497\n",
      "Epoch 45: train loss: 0.023083633831569125, val loss: 0.024708631440997122\n",
      "Epoch 46: train loss: 0.022756584543962868, val loss: 0.022791695192456246\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.3500e-03.\n",
      "Epoch 47: train loss: 0.022499980243797204, val loss: 0.02732960692048073\n",
      "Epoch 48: train loss: 0.02220788813884161, val loss: 0.02349023263156414\n",
      "Epoch 49: train loss: 0.02228619776757396, val loss: 0.023901681423187257\n",
      "Epoch 50: train loss: 0.02226779344282588, val loss: 0.023531472235918047\n",
      "Epoch 51: train loss: 0.02239934776845027, val loss: 0.023879475474357606\n",
      "Epoch 52: train loss: 0.02186852106269525, val loss: 0.02142817260324955\n",
      "Epoch 53: train loss: 0.02163621495268783, val loss: 0.024157589808106422\n",
      "Epoch 54: train loss: 0.022362874062389744, val loss: 0.022042619556188584\n",
      "Epoch 55: train loss: 0.021812129776392665, val loss: 0.022167041778564454\n",
      "Epoch 56: train loss: 0.022076201614676688, val loss: 0.02312609513103962\n",
      "Epoch 57: train loss: 0.022019083249933867, val loss: 0.023752282321453093\n",
      "Epoch 00058: reducing learning rate of group 0 to 4.0500e-04.\n",
      "Epoch 58: train loss: 0.022109061588134086, val loss: 0.02258029079437256\n",
      "Epoch 59: train loss: 0.021705967253872326, val loss: 0.02250432728230953\n",
      "Epoch 60: train loss: 0.021633703097092862, val loss: 0.022874222710728646\n",
      "Epoch 61: train loss: 0.02188867214991122, val loss: 0.023842923060059546\n",
      "Epoch 62: train loss: 0.02156619522766191, val loss: 0.021435072883963586\n",
      "Epoch 63: train loss: 0.0218161726694326, val loss: 0.021158071860671042\n",
      "Epoch 64: train loss: 0.021918854172740664, val loss: 0.02248699280619621\n",
      "Epoch 65: train loss: 0.021785139731302552, val loss: 0.024280831143260004\n",
      "Epoch 66: train loss: 0.021635163901411757, val loss: 0.02199640055000782\n",
      "Epoch 67: train loss: 0.021335748826970858, val loss: 0.022202528461813927\n",
      "Epoch 68: train loss: 0.021540534118000342, val loss: 0.022173801884055138\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.2150e-04.\n",
      "Epoch 69: train loss: 0.021918117174384546, val loss: 0.022117542997002602\n",
      "Epoch 70: train loss: 0.0217761198464705, val loss: 0.023144784957170487\n",
      "Epoch 71: train loss: 0.021345052089009966, val loss: 0.021888973325490953\n",
      "Epoch 72: train loss: 0.02150400306375659, val loss: 0.02150222547352314\n",
      "Epoch 73: train loss: 0.02163683134256577, val loss: 0.022855564296245575\n",
      "Epoch 74: train loss: 0.021654153171546606, val loss: 0.02220557953417301\n",
      "Epoch 00075: reducing learning rate of group 0 to 3.6450e-05.\n",
      "Epoch 75: train loss: 0.02137367008170303, val loss: 0.022449584603309633\n",
      "Epoch 76: train loss: 0.0215400163850614, val loss: 0.022661506742238997\n",
      "Epoch 77: train loss: 0.021625366360253216, val loss: 0.022658660069108008\n",
      "Epoch 78: train loss: 0.021798287545844, val loss: 0.022343082770705222\n",
      "Epoch 79: train loss: 0.021512724985881727, val loss: 0.021819108858704566\n",
      "Epoch 80: train loss: 0.021381489036034564, val loss: 0.022458637788891792\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0935e-05.\n",
      "Epoch 81: train loss: 0.02141067992302836, val loss: 0.021410183414816857\n",
      "Epoch 82: train loss: 0.021694709350564043, val loss: 0.021968096897006035\n",
      "Epoch 83: train loss: 0.02148480301274329, val loss: 0.023957420840859413\n",
      "Epoch 84: train loss: 0.021531309890503787, val loss: 0.022295635566115378\n",
      "Epoch 85: train loss: 0.021530428458233268, val loss: 0.022242736458778382\n",
      "Epoch 86: train loss: 0.0216505026945046, val loss: 0.022086364597082138\n",
      "Epoch 00087: reducing learning rate of group 0 to 3.2805e-06.\n",
      "Epoch 87: train loss: 0.02162853419659089, val loss: 0.021813387677073477\n",
      "Epoch 88: train loss: 0.021502678276020655, val loss: 0.021800103202462198\n",
      "Epoch 89: train loss: 0.021517232741628374, val loss: 0.0216405837982893\n",
      "Epoch 90: train loss: 0.02144567154195844, val loss: 0.020313144326210023\n",
      "Epoch 91: train loss: 0.021380096419125187, val loss: 0.022347526952624323\n",
      "Epoch 92: train loss: 0.021443516752245474, val loss: 0.022346145674586294\n",
      "Epoch 93: train loss: 0.021450457282212315, val loss: 0.02304882191121578\n",
      "Epoch 94: train loss: 0.021558357815961448, val loss: 0.022170445069670676\n",
      "Epoch 95: train loss: 0.02148356641099161, val loss: 0.0212250514626503\n",
      "Epoch 00096: reducing learning rate of group 0 to 9.8415e-07.\n",
      "Epoch 96: train loss: 0.021431337343490852, val loss: 0.02213962608575821\n",
      "Early stop at epoch 96\n"
     ]
    }
   ],
   "source": [
    "# model declaration\n",
    "nn_model = Net()\n",
    "nn_model.to(device)\n",
    "nn_model.train()\n",
    "batch_size = 256\n",
    "eval_size = 1000\n",
    "learning_rate = 0.05\n",
    "weight_decay = 0.0001\n",
    "\n",
    "def train_nn():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(aed_x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = nn_model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def get_regression_model(X, y):\n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ae-nn-lr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742937044897616\n",
      "Predictions saved to results-ae-nn-lr.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "featured_x_train = ae_model.encode(torch.tensor(x_train, dtype=torch.float).to(device))\n",
    "featured_x_train = nn_model.encode(featured_x_train).detach().cpu().numpy()\n",
    "scaler = StandardScaler()\n",
    "featured_x_train = scaler.fit_transform(featured_x_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "featured_x_test = nn_model.encode(ae_model.encode(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device)))\n",
    "featured_x_test = scaler.transform(featured_x_test.detach().cpu().numpy())\n",
    "# featured_x_test = torch.tensor(featured_x_test, dtype=torch.float).to(device)\n",
    "# y_pred = one_model(featured_x_test).squeeze(-1).detach().cpu().numpy()\n",
    "lr = get_regression_model(featured_x_train, y_train)\n",
    "print(lr.score(featured_x_train, y_train))\n",
    "y_pred = lr.predict(featured_x_test)\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
