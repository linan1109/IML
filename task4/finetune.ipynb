{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc = nn.ModuleList()\n",
    "        self.fc.append(nn.Linear(1000, 1000))\n",
    "        self.fc.append(nn.Linear(1000, 512))\n",
    "        self.fc.append(nn.Linear(512, 512))\n",
    "        self.fc.append(nn.Linear(512, 512))\n",
    "        self.fc.append(nn.Linear(512, 256))\n",
    "        self.fc.append(nn.Linear(256, 256))\n",
    "        self.fc.append(nn.Linear(256, 128))\n",
    "        self.fc.append(nn.Linear(128, 64))\n",
    "        self.fc.append(nn.Linear(64, 1))\n",
    "\n",
    "        self.dropout = nn.ModuleList()\n",
    "        self.dropout.append(nn.Dropout(0.2))\n",
    "        self.dropout.append(nn.Dropout(0.2))\n",
    "        self.dropout.append(nn.Dropout(0.3))\n",
    "        self.dropout.append(nn.Dropout(0.3))\n",
    "        self.dropout.append(nn.Dropout(0.4))\n",
    "        self.dropout.append(nn.Dropout(0.4))\n",
    "        self.dropout.append(nn.Dropout(0.5))\n",
    "\n",
    "        for fc in self.fc:\n",
    "            nn.init.xavier_normal_(fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc[0](x))\n",
    "        x = self.dropout[0](x)\n",
    "        x = torch.relu(self.fc[1](x))\n",
    "        x = self.dropout[1](x)\n",
    "        x = torch.relu(self.fc[2](x))\n",
    "        x = self.dropout[2](x)\n",
    "        x = torch.relu(self.fc[3](x))\n",
    "        x = self.dropout[3](x)\n",
    "        x = torch.relu(self.fc[4](x))\n",
    "        x = self.dropout[4](x)\n",
    "        x = torch.relu(self.fc[5](x))\n",
    "        x = self.dropout[5](x)\n",
    "        x = torch.relu(self.fc[6](x))\n",
    "        x = self.dropout[6](x)\n",
    "        x = torch.relu(self.fc[7](x))\n",
    "        x = self.fc[8](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, X, y, tune_layers=2):\n",
    "    # frozen layers and not frozen last n layers\n",
    "    num_layers = len(model.fc)\n",
    "    for i in range(num_layers - tune_layers):\n",
    "        for param in model.fc[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    for i in range(num_layers - tune_layers, num_layers):\n",
    "        for param in model.fc[i].parameters():\n",
    "            param.requires_grad = True\n",
    "        nn.init.xavier_normal_(model.fc[i].weight)\n",
    "\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-ft-2-128.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e43c52bc98b4467ac3036e2f9f95462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.7509917573150323, val loss: 0.24481031668186187\n",
      "Epoch 2: train loss: 0.1856592956075863, val loss: 0.1351518316268921\n",
      "Epoch 3: train loss: 0.11355318463578516, val loss: 0.08567215746641159\n",
      "Epoch 4: train loss: 0.07297525085721697, val loss: 0.058291280835866925\n",
      "Epoch 5: train loss: 0.04919610972307166, val loss: 0.039477434039115904\n",
      "Epoch 6: train loss: 0.03377175368946426, val loss: 0.027582309380173683\n",
      "Epoch 7: train loss: 0.023817637523826287, val loss: 0.022008428007364272\n",
      "Epoch 8: train loss: 0.0184417150285171, val loss: 0.01608578810095787\n",
      "Epoch 9: train loss: 0.015224994518775114, val loss: 0.015083551578223706\n",
      "Epoch 10: train loss: 0.0133196210344227, val loss: 0.013720905534923077\n",
      "Epoch 11: train loss: 0.011722442549254213, val loss: 0.012380805291235447\n",
      "Epoch 12: train loss: 0.010464945681393147, val loss: 0.010750147394835948\n",
      "Epoch 13: train loss: 0.009986051261425018, val loss: 0.010044511444866656\n",
      "Epoch 14: train loss: 0.009011310397742354, val loss: 0.010090007543563843\n",
      "Epoch 15: train loss: 0.008651717272340036, val loss: 0.009932028993964195\n",
      "Epoch 16: train loss: 0.008153917890726304, val loss: 0.008692802213132381\n",
      "Epoch 17: train loss: 0.007753310131099151, val loss: 0.009624974768608808\n",
      "Epoch 18: train loss: 0.007516495921173874, val loss: 0.008849795386195182\n",
      "Epoch 19: train loss: 0.0072771391673963895, val loss: 0.00857275914028287\n",
      "Epoch 20: train loss: 0.007186470130268408, val loss: 0.007609351582825184\n",
      "Epoch 21: train loss: 0.0069561367419605355, val loss: 0.007344483006745577\n",
      "Epoch 22: train loss: 0.007121850351974064, val loss: 0.008077206149697303\n",
      "Epoch 23: train loss: 0.006827542165484355, val loss: 0.009071408748626709\n",
      "Epoch 24: train loss: 0.0067220702326419406, val loss: 0.007220707125961781\n",
      "Epoch 25: train loss: 0.006637318184333188, val loss: 0.007401632767170667\n",
      "Epoch 26: train loss: 0.006616244815061895, val loss: 0.006575011532753706\n",
      "Epoch 27: train loss: 0.006494489714123156, val loss: 0.006215705893933773\n",
      "Epoch 28: train loss: 0.006326245485747955, val loss: 0.00734181747213006\n",
      "Epoch 29: train loss: 0.00637471830806866, val loss: 0.007167746964842081\n",
      "Epoch 30: train loss: 0.0069761095211974215, val loss: 0.006849336951971054\n",
      "Epoch 31: train loss: 0.006384333428375575, val loss: 0.006659748062491417\n",
      "Epoch 32: train loss: 0.006452424007182826, val loss: 0.006031798042356968\n",
      "Epoch 33: train loss: 0.006394826726797892, val loss: 0.007405447132885456\n",
      "Epoch 34: train loss: 0.006285847556530213, val loss: 0.007853358156979084\n",
      "Epoch 35: train loss: 0.006475058130539802, val loss: 0.006573596514761448\n",
      "Epoch 36: train loss: 0.006190514606480696, val loss: 0.007004277799278497\n",
      "Epoch 37: train loss: 0.006618622726931864, val loss: 0.006164818737655878\n",
      "Epoch 00038: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 38: train loss: 0.0065157461874187, val loss: 0.008200863637030124\n",
      "Epoch 39: train loss: 0.004549745237720864, val loss: 0.004597869154065847\n",
      "Epoch 40: train loss: 0.003670125411405247, val loss: 0.004603787500411272\n",
      "Epoch 41: train loss: 0.00350544533844353, val loss: 0.00457207552343607\n",
      "Epoch 42: train loss: 0.0033967634165773587, val loss: 0.0048902384266257285\n",
      "Epoch 43: train loss: 0.0035706936152157735, val loss: 0.004759437549859286\n",
      "Epoch 44: train loss: 0.0035856743866235627, val loss: 0.004552970618009567\n",
      "Epoch 45: train loss: 0.0036175895181237436, val loss: 0.0067204957455396655\n",
      "Epoch 46: train loss: 0.0036678313759288617, val loss: 0.004922029249370098\n",
      "Epoch 47: train loss: 0.003638049964317862, val loss: 0.004119865518063307\n",
      "Epoch 48: train loss: 0.003640818902690496, val loss: 0.004382585097104311\n",
      "Epoch 49: train loss: 0.0036019129196219906, val loss: 0.004882459599524737\n",
      "Epoch 50: train loss: 0.0037216544069577843, val loss: 0.005136755719780922\n",
      "Epoch 51: train loss: 0.003600774527014214, val loss: 0.0047228336036205295\n",
      "Epoch 52: train loss: 0.003475708008496737, val loss: 0.004441590379923582\n",
      "Epoch 00053: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 53: train loss: 0.003389302746677885, val loss: 0.004275142733007669\n",
      "Epoch 54: train loss: 0.0026812534091271915, val loss: 0.0033358335588127373\n",
      "Epoch 55: train loss: 0.002334698452177096, val loss: 0.0033825081884860993\n",
      "Epoch 56: train loss: 0.0022719833206917557, val loss: 0.0032603073604404926\n",
      "Epoch 57: train loss: 0.002202600027795653, val loss: 0.0035989740677177906\n",
      "Epoch 58: train loss: 0.0021441718015667737, val loss: 0.003968353733420372\n",
      "Epoch 59: train loss: 0.0021683342579029955, val loss: 0.003624155569821596\n",
      "Epoch 60: train loss: 0.0021915532339516343, val loss: 0.0032980827055871485\n",
      "Epoch 61: train loss: 0.002122968816369468, val loss: 0.0033407801184803247\n",
      "Epoch 00062: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 62: train loss: 0.002104340238876793, val loss: 0.0033914784975349905\n",
      "Epoch 63: train loss: 0.001915699915371227, val loss: 0.0032095248885452747\n",
      "Epoch 64: train loss: 0.001827888054621159, val loss: 0.0033119445573538543\n",
      "Epoch 65: train loss: 0.001778062097614213, val loss: 0.0030144686065614224\n",
      "Epoch 66: train loss: 0.0017546260015544842, val loss: 0.002910277223214507\n",
      "Epoch 67: train loss: 0.0017176066339776223, val loss: 0.003027074459940195\n",
      "Epoch 68: train loss: 0.0017315882602516486, val loss: 0.003247236926108599\n",
      "Epoch 69: train loss: 0.001698753303487082, val loss: 0.0028570823278278113\n",
      "Epoch 70: train loss: 0.0017025654175489837, val loss: 0.0028943964764475824\n",
      "Epoch 71: train loss: 0.0016828254480675167, val loss: 0.0030446113254874946\n",
      "Epoch 72: train loss: 0.0016742303660329508, val loss: 0.0031708759143948555\n",
      "Epoch 73: train loss: 0.0016503326109401425, val loss: 0.0030973517522215844\n",
      "Epoch 74: train loss: 0.0016660802210586108, val loss: 0.002766465052962303\n",
      "Epoch 75: train loss: 0.0016574679567877735, val loss: 0.0030857662092894315\n",
      "Epoch 76: train loss: 0.0016642524396278421, val loss: 0.0029830456990748645\n",
      "Epoch 77: train loss: 0.0016324735844735893, val loss: 0.0029395194221287966\n",
      "Epoch 78: train loss: 0.0016200816039832271, val loss: 0.0030367871895432474\n",
      "Epoch 79: train loss: 0.0016090445046940325, val loss: 0.0031920243389904498\n",
      "Epoch 00080: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 80: train loss: 0.0016054299348509128, val loss: 0.0030341879799962042\n",
      "Epoch 81: train loss: 0.0015674647508303122, val loss: 0.002959615655243397\n",
      "Epoch 82: train loss: 0.0015361848475449547, val loss: 0.0029361027032136916\n",
      "Epoch 83: train loss: 0.0015347903519298653, val loss: 0.002832581140100956\n",
      "Epoch 84: train loss: 0.0015120433549757819, val loss: 0.003056019918993115\n",
      "Epoch 85: train loss: 0.001505755722598762, val loss: 0.002890135366469622\n",
      "Epoch 00086: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 86: train loss: 0.00150134120911968, val loss: 0.0031412840839475393\n",
      "Epoch 87: train loss: 0.0015095530821542654, val loss: 0.0030261259730905293\n",
      "Epoch 88: train loss: 0.0014802645178967897, val loss: 0.002778916893526912\n",
      "Epoch 89: train loss: 0.0014483059575135003, val loss: 0.002782816668972373\n",
      "Epoch 90: train loss: 0.001480308416660647, val loss: 0.0027083252146840094\n",
      "Epoch 91: train loss: 0.0014545154907379529, val loss: 0.002925644937902689\n",
      "Epoch 92: train loss: 0.0014552824363986752, val loss: 0.0028943628538399937\n",
      "Epoch 93: train loss: 0.0014651380633289109, val loss: 0.003160382395610213\n",
      "Epoch 94: train loss: 0.0014499868539218999, val loss: 0.0028165468480437993\n",
      "Epoch 95: train loss: 0.0014463984063465376, val loss: 0.00272643481194973\n",
      "Epoch 00096: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 96: train loss: 0.0014650696084967682, val loss: 0.002767721448093653\n",
      "Early stop at epoch 96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f96dc0513684e70b5f260a56f927bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 4.663094997406006\n",
      "Epoch 20: train loss: 4.425008296966553\n",
      "Epoch 30: train loss: 4.299380779266357\n",
      "Epoch 40: train loss: 4.115776062011719\n",
      "Epoch 50: train loss: 3.8876583576202393\n",
      "Epoch 60: train loss: 3.7343132495880127\n",
      "Epoch 00070: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 70: train loss: 3.6495964527130127\n",
      "Epoch 00080: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Epoch 80: train loss: 3.627293109893799\n",
      "Epoch 90: train loss: 3.4836437702178955\n",
      "Epoch 00096: reducing learning rate of group 0 to 2.7000e-06.\n",
      "Epoch 100: train loss: 3.5559816360473633\n",
      "Epoch 00102: reducing learning rate of group 0 to 8.1000e-07.\n",
      "Epoch 110: train loss: 3.5351343154907227\n",
      "Epoch 00113: reducing learning rate of group 0 to 2.4300e-07.\n",
      "Epoch 120: train loss: 3.5370030403137207\n",
      "Epoch 00121: reducing learning rate of group 0 to 7.2900e-08.\n",
      "Early stop at epoch 121, loss: 3.5777339935302734\n",
      "Predictions saved to results-ft-2-128.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "\n",
    "regression_model = finetune(feature_extractor, x_train, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "x_test_tensor = torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device)\n",
    "y_pred = regression_model(x_test_tensor).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
