{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "        nn.init.xavier_normal_(self.fc6.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 500\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-std-1-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf8bd03e48a40188146fa98519ad1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 3.872801798956735, val loss: 2.794013801574707\n",
      "Epoch 2: train loss: 2.3834540414226297, val loss: 1.9419426383972167\n",
      "Epoch 3: train loss: 1.9359621873388484, val loss: 1.72244140625\n",
      "Epoch 4: train loss: 1.7147272632170696, val loss: 1.531053394317627\n",
      "Epoch 5: train loss: 1.5214055871768875, val loss: 1.404157434463501\n",
      "Epoch 6: train loss: 1.3514733100618634, val loss: 1.2330084409713744\n",
      "Epoch 7: train loss: 1.2009372901721878, val loss: 1.1207122826576232\n",
      "Epoch 8: train loss: 1.0659056862811653, val loss: 0.9800570659637451\n",
      "Epoch 9: train loss: 0.9544341612543379, val loss: 0.8415515904426575\n",
      "Epoch 10: train loss: 0.8451037280900138, val loss: 0.7707172102928161\n",
      "Epoch 11: train loss: 0.7485488836424692, val loss: 0.6641493744850159\n",
      "Epoch 12: train loss: 0.6567813282694135, val loss: 0.5922441849708557\n",
      "Epoch 13: train loss: 0.5747074261295552, val loss: 0.5119609470367432\n",
      "Epoch 14: train loss: 0.46152196311950683, val loss: 0.4244204113483429\n",
      "Epoch 15: train loss: 0.386204185344735, val loss: 0.35696578526496886\n",
      "Epoch 16: train loss: 0.3385650303217829, val loss: 0.32430844926834107\n",
      "Epoch 17: train loss: 0.29355898521384416, val loss: 0.286835524559021\n",
      "Epoch 18: train loss: 0.255699221888367, val loss: 0.22847676873207093\n",
      "Epoch 19: train loss: 0.21846364478189118, val loss: 0.1969830609560013\n",
      "Epoch 20: train loss: 0.1852617643512025, val loss: 0.17099513840675354\n",
      "Epoch 21: train loss: 0.15749061686408763, val loss: 0.1384953179359436\n",
      "Epoch 22: train loss: 0.1301740725162078, val loss: 0.11649517595767975\n",
      "Epoch 23: train loss: 0.11086793474761807, val loss: 0.1016980698108673\n",
      "Epoch 24: train loss: 0.09553020407958907, val loss: 0.07573153740167618\n",
      "Epoch 25: train loss: 0.08065444438676445, val loss: 0.07855818438529968\n",
      "Epoch 26: train loss: 0.06917266777705174, val loss: 0.05912519943714142\n",
      "Epoch 27: train loss: 0.061543021204520244, val loss: 0.06048313143849373\n",
      "Epoch 28: train loss: 0.05511857041777397, val loss: 0.0548370113670826\n",
      "Epoch 29: train loss: 0.05111592528406455, val loss: 0.05128822234272957\n",
      "Epoch 30: train loss: 0.04810586300857213, val loss: 0.04488445591926575\n",
      "Epoch 31: train loss: 0.04689791928687874, val loss: 0.040937908053398135\n",
      "Epoch 32: train loss: 0.04499248470092306, val loss: 0.040999989420175555\n",
      "Epoch 33: train loss: 0.04475203327317627, val loss: 0.04088614758849144\n",
      "Epoch 34: train loss: 0.043756725931654174, val loss: 0.039788260847330094\n",
      "Epoch 35: train loss: 0.04340363612892676, val loss: 0.040618288397789\n",
      "Epoch 36: train loss: 0.04154896123251137, val loss: 0.04016675114631653\n",
      "Epoch 37: train loss: 0.042729697025552085, val loss: 0.04253723141551018\n",
      "Epoch 38: train loss: 0.04196803264107023, val loss: 0.04369828873872757\n",
      "Epoch 39: train loss: 0.04254638172898974, val loss: 0.035668603107333184\n",
      "Epoch 40: train loss: 0.042048606126892325, val loss: 0.04003405764698982\n",
      "Epoch 41: train loss: 0.04201417971022275, val loss: 0.04102828758955002\n",
      "Epoch 42: train loss: 0.04228205043016648, val loss: 0.0428481035232544\n",
      "Epoch 43: train loss: 0.042357769732572596, val loss: 0.04180182239413261\n",
      "Epoch 44: train loss: 0.042101747771915124, val loss: 0.041576829105615616\n",
      "Epoch 00045: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 45: train loss: 0.04152976192199454, val loss: 0.04617063727974892\n",
      "Epoch 46: train loss: 0.04039183589451167, val loss: 0.04049696043133735\n",
      "Epoch 47: train loss: 0.04000290442851125, val loss: 0.038572981178760526\n",
      "Epoch 48: train loss: 0.03963895127907091, val loss: 0.03983412846922874\n",
      "Epoch 49: train loss: 0.039420083348240174, val loss: 0.0390460461974144\n",
      "Epoch 50: train loss: 0.03945425798394242, val loss: 0.0361294047832489\n",
      "Epoch 00051: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 51: train loss: 0.03942731028247853, val loss: 0.03693724358081817\n",
      "Epoch 52: train loss: 0.03898690191002525, val loss: 0.039193665713071826\n",
      "Epoch 53: train loss: 0.03879688095134132, val loss: 0.03756533244252205\n",
      "Epoch 54: train loss: 0.03873280493489334, val loss: 0.035517803251743314\n",
      "Epoch 55: train loss: 0.03868786029426419, val loss: 0.03585663166642189\n",
      "Epoch 56: train loss: 0.03822885956387131, val loss: 0.03588947212696075\n",
      "Epoch 57: train loss: 0.03841060224966127, val loss: 0.03731957098841667\n",
      "Epoch 58: train loss: 0.03846271139444137, val loss: 0.042467788338661196\n",
      "Epoch 59: train loss: 0.03808595413212874, val loss: 0.03804171907901764\n",
      "Epoch 00060: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 60: train loss: 0.03795323573569862, val loss: 0.03834472405910492\n",
      "Epoch 61: train loss: 0.03801586020175292, val loss: 0.03567425063252449\n",
      "Epoch 62: train loss: 0.038251036285137645, val loss: 0.03860063537955284\n",
      "Epoch 63: train loss: 0.03772702354436018, val loss: 0.03714610087871552\n",
      "Epoch 64: train loss: 0.03707351854504371, val loss: 0.04320976808667183\n",
      "Epoch 65: train loss: 0.03814502865167297, val loss: 0.03625090476870537\n",
      "Epoch 00066: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 66: train loss: 0.03799636105159108, val loss: 0.0369755274951458\n",
      "Epoch 67: train loss: 0.03772042492153693, val loss: 0.03794810140132904\n",
      "Epoch 68: train loss: 0.03797743249669367, val loss: 0.03247438822686672\n",
      "Epoch 69: train loss: 0.03784084903099099, val loss: 0.03575919809937477\n",
      "Epoch 70: train loss: 0.03786645513590501, val loss: 0.03569658195972442\n",
      "Epoch 71: train loss: 0.037914503717300846, val loss: 0.03639192590117454\n",
      "Epoch 72: train loss: 0.037989442384364656, val loss: 0.03974024048447609\n",
      "Epoch 73: train loss: 0.03770440154355399, val loss: 0.03381346619129181\n",
      "Epoch 00074: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 74: train loss: 0.037893685872457464, val loss: 0.03911643859744072\n",
      "Epoch 75: train loss: 0.0375116455263021, val loss: 0.04080743408203125\n",
      "Epoch 76: train loss: 0.03809031307028264, val loss: 0.0370643150806427\n",
      "Epoch 77: train loss: 0.037935825498736635, val loss: 0.03483352844417095\n",
      "Epoch 78: train loss: 0.03764630509061473, val loss: 0.035928607344627383\n",
      "Epoch 79: train loss: 0.03746508911191201, val loss: 0.0346677038371563\n",
      "Epoch 00080: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 80: train loss: 0.03758718726038933, val loss: 0.03792748239636421\n",
      "Early stop at epoch 80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f068d81e57c4220a4ca03b854fa91b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 1.8371461629867554\n",
      "Epoch 20: train loss: 1.7824550867080688\n",
      "Epoch 30: train loss: 1.7290624380111694\n",
      "Epoch 40: train loss: 1.677046775817871\n",
      "Epoch 50: train loss: 1.6264424324035645\n",
      "Epoch 60: train loss: 1.5772539377212524\n",
      "Epoch 70: train loss: 1.5294709205627441\n",
      "Epoch 80: train loss: 1.4830738306045532\n",
      "Epoch 90: train loss: 1.4380401372909546\n",
      "Epoch 100: train loss: 1.3943454027175903\n",
      "Epoch 110: train loss: 1.3519651889801025\n",
      "Epoch 120: train loss: 1.3108739852905273\n",
      "Epoch 130: train loss: 1.27104651927948\n",
      "Epoch 140: train loss: 1.2324579954147339\n",
      "Epoch 150: train loss: 1.1950833797454834\n",
      "Epoch 160: train loss: 1.1588975191116333\n",
      "Epoch 170: train loss: 1.1238752603530884\n",
      "Epoch 180: train loss: 1.0899919271469116\n",
      "Epoch 190: train loss: 1.0572227239608765\n",
      "Epoch 200: train loss: 1.0255430936813354\n",
      "Epoch 210: train loss: 0.9949285387992859\n",
      "Epoch 220: train loss: 0.9653547406196594\n",
      "Epoch 230: train loss: 0.9367973208427429\n",
      "Epoch 240: train loss: 0.9092319011688232\n",
      "Epoch 250: train loss: 0.8826348185539246\n",
      "Epoch 260: train loss: 0.8569819331169128\n",
      "Epoch 270: train loss: 0.8322497606277466\n",
      "Epoch 280: train loss: 0.8084145784378052\n",
      "Epoch 290: train loss: 0.7854531407356262\n",
      "Epoch 300: train loss: 0.7633424401283264\n",
      "Epoch 310: train loss: 0.742059051990509\n",
      "Epoch 320: train loss: 0.7215808033943176\n",
      "Epoch 330: train loss: 0.7018847465515137\n",
      "Epoch 340: train loss: 0.6829485893249512\n",
      "Epoch 350: train loss: 0.6647505164146423\n",
      "Epoch 360: train loss: 0.6472683548927307\n",
      "Epoch 370: train loss: 0.6304807662963867\n",
      "Epoch 380: train loss: 0.6143664717674255\n",
      "Epoch 390: train loss: 0.5989044904708862\n",
      "Epoch 400: train loss: 0.5840737819671631\n",
      "Epoch 410: train loss: 0.5698537826538086\n",
      "Epoch 420: train loss: 0.556225061416626\n",
      "Epoch 430: train loss: 0.5431673526763916\n",
      "Epoch 440: train loss: 0.5306611657142639\n",
      "Epoch 450: train loss: 0.5186874270439148\n",
      "Epoch 460: train loss: 0.5072271823883057\n",
      "Epoch 470: train loss: 0.49626195430755615\n",
      "Epoch 480: train loss: 0.4857734441757202\n",
      "Epoch 490: train loss: 0.4757441580295563\n",
      "Epoch 500: train loss: 0.4661564528942108\n",
      "Epoch 510: train loss: 0.4569932520389557\n",
      "Epoch 520: train loss: 0.44823822379112244\n",
      "Epoch 530: train loss: 0.43987470865249634\n",
      "Epoch 540: train loss: 0.43188703060150146\n",
      "Epoch 550: train loss: 0.4242595434188843\n",
      "Epoch 560: train loss: 0.4169771075248718\n",
      "Epoch 570: train loss: 0.41002508997917175\n",
      "Epoch 580: train loss: 0.40338918566703796\n",
      "Epoch 590: train loss: 0.3970552682876587\n",
      "Epoch 600: train loss: 0.3910099267959595\n",
      "Epoch 610: train loss: 0.38523992896080017\n",
      "Epoch 620: train loss: 0.3797326982021332\n",
      "Epoch 630: train loss: 0.3744758665561676\n",
      "Epoch 640: train loss: 0.3694574236869812\n",
      "Epoch 650: train loss: 0.36466583609580994\n",
      "Epoch 660: train loss: 0.36008983850479126\n",
      "Epoch 670: train loss: 0.3557188808917999\n",
      "Epoch 680: train loss: 0.35154253244400024\n",
      "Epoch 690: train loss: 0.34755071997642517\n",
      "Epoch 700: train loss: 0.34373390674591064\n",
      "Epoch 710: train loss: 0.34008270502090454\n",
      "Epoch 720: train loss: 0.33658847212791443\n",
      "Epoch 730: train loss: 0.33324241638183594\n",
      "Epoch 740: train loss: 0.3300364911556244\n",
      "Epoch 750: train loss: 0.3269628882408142\n",
      "Epoch 760: train loss: 0.3240140974521637\n",
      "Epoch 770: train loss: 0.3211829662322998\n",
      "Epoch 780: train loss: 0.3184625804424286\n",
      "Epoch 790: train loss: 0.31584659218788147\n",
      "Epoch 800: train loss: 0.31332865357398987\n",
      "Epoch 810: train loss: 0.3109028935432434\n",
      "Epoch 820: train loss: 0.3085636496543884\n",
      "Epoch 830: train loss: 0.30630558729171753\n",
      "Epoch 840: train loss: 0.30412358045578003\n",
      "Epoch 850: train loss: 0.30201292037963867\n",
      "Epoch 860: train loss: 0.29996904730796814\n",
      "Epoch 870: train loss: 0.29798755049705505\n",
      "Epoch 880: train loss: 0.2960643768310547\n",
      "Epoch 890: train loss: 0.2941957116127014\n",
      "Epoch 900: train loss: 0.29237791895866394\n",
      "Epoch 910: train loss: 0.2906074821949005\n",
      "Epoch 920: train loss: 0.2888812720775604\n",
      "Epoch 930: train loss: 0.2871960997581482\n",
      "Epoch 940: train loss: 0.28554925322532654\n",
      "Epoch 950: train loss: 0.2839379608631134\n",
      "Epoch 960: train loss: 0.2823598086833954\n",
      "Epoch 970: train loss: 0.2808123528957367\n",
      "Epoch 980: train loss: 0.2792934477329254\n",
      "Epoch 990: train loss: 0.27780094742774963\n",
      "Epoch 1000: train loss: 0.2763330042362213\n",
      "Epoch 1010: train loss: 0.27488765120506287\n",
      "Epoch 1020: train loss: 0.2734634280204773\n",
      "Epoch 1030: train loss: 0.2720586359500885\n",
      "Epoch 1040: train loss: 0.2706718146800995\n",
      "Epoch 1050: train loss: 0.2693016827106476\n",
      "Epoch 1060: train loss: 0.26794689893722534\n",
      "Epoch 1070: train loss: 0.26660633087158203\n",
      "Epoch 1080: train loss: 0.2652789354324341\n",
      "Epoch 1090: train loss: 0.2639636695384979\n",
      "Epoch 1100: train loss: 0.26265960931777954\n",
      "Epoch 1110: train loss: 0.2613658905029297\n",
      "Epoch 1120: train loss: 0.2600817084312439\n",
      "Epoch 1130: train loss: 0.25880637764930725\n",
      "Epoch 1140: train loss: 0.25753918290138245\n",
      "Epoch 1150: train loss: 0.2562795877456665\n",
      "Epoch 1160: train loss: 0.25502699613571167\n",
      "Epoch 1170: train loss: 0.2537808418273926\n",
      "Epoch 1180: train loss: 0.2525407075881958\n",
      "Epoch 1190: train loss: 0.2513061463832855\n",
      "Epoch 1200: train loss: 0.2500767409801483\n",
      "Epoch 1210: train loss: 0.2488521933555603\n",
      "Epoch 1220: train loss: 0.24763211607933044\n",
      "Epoch 1230: train loss: 0.24641624093055725\n",
      "Epoch 1240: train loss: 0.24520431458950043\n",
      "Epoch 1250: train loss: 0.24399608373641968\n",
      "Epoch 1260: train loss: 0.2427913248538971\n",
      "Epoch 1270: train loss: 0.24158982932567596\n",
      "Epoch 1280: train loss: 0.24039146304130554\n",
      "Epoch 1290: train loss: 0.2391960322856903\n",
      "Epoch 1300: train loss: 0.2380034625530243\n",
      "Epoch 1310: train loss: 0.23681358993053436\n",
      "Epoch 1320: train loss: 0.23562632501125336\n",
      "Epoch 1330: train loss: 0.23444156348705292\n",
      "Epoch 1340: train loss: 0.23325923085212708\n",
      "Epoch 1350: train loss: 0.23207929730415344\n",
      "Epoch 1360: train loss: 0.23090164363384247\n",
      "Epoch 1370: train loss: 0.22972625494003296\n",
      "Epoch 1380: train loss: 0.2285531610250473\n",
      "Epoch 1390: train loss: 0.22738231718540192\n",
      "Epoch 1400: train loss: 0.22621366381645203\n",
      "Epoch 1410: train loss: 0.2250470370054245\n",
      "Epoch 1420: train loss: 0.22388264536857605\n",
      "Epoch 1430: train loss: 0.22272036969661713\n",
      "Epoch 1440: train loss: 0.2215602993965149\n",
      "Epoch 1450: train loss: 0.22040240466594696\n",
      "Epoch 1460: train loss: 0.21924668550491333\n",
      "Epoch 1470: train loss: 0.2180931568145752\n",
      "Epoch 1480: train loss: 0.21694183349609375\n",
      "Epoch 1490: train loss: 0.21579280495643616\n",
      "Epoch 1500: train loss: 0.21464602649211884\n",
      "Epoch 1510: train loss: 0.21350158751010895\n",
      "Epoch 1520: train loss: 0.2123594582080841\n",
      "Epoch 1530: train loss: 0.2112196832895279\n",
      "Epoch 1540: train loss: 0.21008235216140747\n",
      "Epoch 1550: train loss: 0.20894740521907806\n",
      "Epoch 1560: train loss: 0.2078150063753128\n",
      "Epoch 1570: train loss: 0.2066851258277893\n",
      "Epoch 1580: train loss: 0.20555774867534637\n",
      "Epoch 1590: train loss: 0.20443299412727356\n",
      "Epoch 1600: train loss: 0.20331089198589325\n",
      "Epoch 1610: train loss: 0.20219148695468903\n",
      "Epoch 1620: train loss: 0.20107482373714447\n",
      "Epoch 1630: train loss: 0.19996093213558197\n",
      "Epoch 1640: train loss: 0.1988498717546463\n",
      "Epoch 1650: train loss: 0.19774165749549866\n",
      "Epoch 1660: train loss: 0.1966363936662674\n",
      "Epoch 1670: train loss: 0.19553406536579132\n",
      "Epoch 1680: train loss: 0.19443479180335999\n",
      "Epoch 1690: train loss: 0.193338543176651\n",
      "Epoch 1700: train loss: 0.19224542379379272\n",
      "Epoch 1710: train loss: 0.19115543365478516\n",
      "Epoch 1720: train loss: 0.19006866216659546\n",
      "Epoch 1730: train loss: 0.18898515403270721\n",
      "Epoch 1740: train loss: 0.18790492415428162\n",
      "Epoch 1750: train loss: 0.18682803213596344\n",
      "Epoch 1760: train loss: 0.18575456738471985\n",
      "Epoch 1770: train loss: 0.18468450009822845\n",
      "Epoch 1780: train loss: 0.1836179494857788\n",
      "Epoch 1790: train loss: 0.18255488574504852\n",
      "Epoch 1800: train loss: 0.18149539828300476\n",
      "Epoch 1810: train loss: 0.1804395616054535\n",
      "Epoch 1820: train loss: 0.17938736081123352\n",
      "Epoch 1830: train loss: 0.17833884060382843\n",
      "Epoch 1840: train loss: 0.1772940754890442\n",
      "Epoch 1850: train loss: 0.17625312507152557\n",
      "Epoch 1860: train loss: 0.17521600425243378\n",
      "Epoch 1870: train loss: 0.17418272793293\n",
      "Epoch 1880: train loss: 0.1731533706188202\n",
      "Epoch 1890: train loss: 0.17212800681591034\n",
      "Epoch 1900: train loss: 0.17110663652420044\n",
      "Epoch 1910: train loss: 0.17008930444717407\n",
      "Epoch 1920: train loss: 0.169076070189476\n",
      "Epoch 1930: train loss: 0.16806690394878387\n",
      "Epoch 1940: train loss: 0.1670619547367096\n",
      "Epoch 1950: train loss: 0.16606108844280243\n",
      "Epoch 1960: train loss: 0.16506454348564148\n",
      "Epoch 1970: train loss: 0.16407230496406555\n",
      "Epoch 1980: train loss: 0.16308432817459106\n",
      "Epoch 1990: train loss: 0.16210073232650757\n",
      "Epoch 2000: train loss: 0.16112147271633148\n",
      "Epoch 2010: train loss: 0.16014665365219116\n",
      "Epoch 2020: train loss: 0.15917626023292542\n",
      "Epoch 2030: train loss: 0.15821033716201782\n",
      "Epoch 2040: train loss: 0.15724894404411316\n",
      "Epoch 2050: train loss: 0.156292125582695\n",
      "Epoch 2060: train loss: 0.15533986687660217\n",
      "Epoch 2070: train loss: 0.15439222753047943\n",
      "Epoch 2080: train loss: 0.15344922244548798\n",
      "Epoch 2090: train loss: 0.15251098573207855\n",
      "Epoch 2100: train loss: 0.15157729387283325\n",
      "Epoch 2110: train loss: 0.15064838528633118\n",
      "Epoch 2120: train loss: 0.14972424507141113\n",
      "Epoch 2130: train loss: 0.14880496263504028\n",
      "Epoch 2140: train loss: 0.14789046347141266\n",
      "Epoch 2150: train loss: 0.14698080718517303\n",
      "Epoch 2160: train loss: 0.1460759937763214\n",
      "Epoch 2170: train loss: 0.1451760232448578\n",
      "Epoch 2180: train loss: 0.1442810744047165\n",
      "Epoch 2190: train loss: 0.14339104294776917\n",
      "Epoch 2200: train loss: 0.1425059735774994\n",
      "Epoch 2210: train loss: 0.14162586629390717\n",
      "Epoch 2220: train loss: 0.14075082540512085\n",
      "Epoch 2230: train loss: 0.13988079130649567\n",
      "Epoch 2240: train loss: 0.1390158087015152\n",
      "Epoch 2250: train loss: 0.13815592229366302\n",
      "Epoch 2260: train loss: 0.13730113208293915\n",
      "Epoch 2270: train loss: 0.13645145297050476\n",
      "Epoch 2280: train loss: 0.13560692965984344\n",
      "Epoch 2290: train loss: 0.134767547249794\n",
      "Epoch 2300: train loss: 0.13393333554267883\n",
      "Epoch 2310: train loss: 0.1331043243408203\n",
      "Epoch 2320: train loss: 0.13228052854537964\n",
      "Epoch 2330: train loss: 0.13146191835403442\n",
      "Epoch 2340: train loss: 0.13064856827259064\n",
      "Epoch 2350: train loss: 0.12984047830104828\n",
      "Epoch 2360: train loss: 0.12903764843940735\n",
      "Epoch 2370: train loss: 0.12824006378650665\n",
      "Epoch 2380: train loss: 0.12744782865047455\n",
      "Epoch 2390: train loss: 0.12666088342666626\n",
      "Epoch 2400: train loss: 0.1258792281150818\n",
      "Epoch 2410: train loss: 0.1251029223203659\n",
      "Epoch 2420: train loss: 0.12433195114135742\n",
      "Epoch 2430: train loss: 0.12356632947921753\n",
      "Epoch 2440: train loss: 0.12280608713626862\n",
      "Epoch 2450: train loss: 0.1220511794090271\n",
      "Epoch 2460: train loss: 0.12130168825387955\n",
      "Epoch 2470: train loss: 0.12055755406618118\n",
      "Epoch 2480: train loss: 0.1198188066482544\n",
      "Epoch 2490: train loss: 0.11908546090126038\n",
      "Epoch 2500: train loss: 0.11835752427577972\n",
      "Epoch 2510: train loss: 0.11763501167297363\n",
      "Epoch 2520: train loss: 0.11691789329051971\n",
      "Epoch 2530: train loss: 0.11620620638132095\n",
      "Epoch 2540: train loss: 0.11549993604421616\n",
      "Epoch 2550: train loss: 0.11479905992746353\n",
      "Epoch 2560: train loss: 0.11410363763570786\n",
      "Epoch 2570: train loss: 0.11341365426778793\n",
      "Epoch 2580: train loss: 0.11272908747196198\n",
      "Epoch 2590: train loss: 0.11204993724822998\n",
      "Epoch 2600: train loss: 0.11137624830007553\n",
      "Epoch 2610: train loss: 0.11070798337459564\n",
      "Epoch 2620: train loss: 0.11004513502120972\n",
      "Epoch 2630: train loss: 0.10938774049282074\n",
      "Epoch 2640: train loss: 0.10873576998710632\n",
      "Epoch 2650: train loss: 0.10808917880058289\n",
      "Epoch 2660: train loss: 0.10744806379079819\n",
      "Epoch 2670: train loss: 0.10681232810020447\n",
      "Epoch 2680: train loss: 0.1061820387840271\n",
      "Epoch 2690: train loss: 0.1055571660399437\n",
      "Epoch 2700: train loss: 0.10493766516447067\n",
      "Epoch 2710: train loss: 0.10432358831167221\n",
      "Epoch 2720: train loss: 0.10371488332748413\n",
      "Epoch 2730: train loss: 0.10311158001422882\n",
      "Epoch 2740: train loss: 0.1025136411190033\n",
      "Epoch 2750: train loss: 0.10192109644412994\n",
      "Epoch 2760: train loss: 0.10133393853902817\n",
      "Epoch 2770: train loss: 0.10075214505195618\n",
      "Epoch 2780: train loss: 0.10017566382884979\n",
      "Epoch 2790: train loss: 0.09960450232028961\n",
      "Epoch 2800: train loss: 0.09903867542743683\n",
      "Epoch 2810: train loss: 0.09847814589738846\n",
      "Epoch 2820: train loss: 0.09792294353246689\n",
      "Epoch 2830: train loss: 0.09737302362918854\n",
      "Epoch 2840: train loss: 0.09682837873697281\n",
      "Epoch 2850: train loss: 0.0962890163064003\n",
      "Epoch 2860: train loss: 0.09575491398572922\n",
      "Epoch 2870: train loss: 0.09522603452205658\n",
      "Epoch 2880: train loss: 0.09470239281654358\n",
      "Epoch 2890: train loss: 0.09418395906686783\n",
      "Epoch 2900: train loss: 0.09367071837186813\n",
      "Epoch 2910: train loss: 0.0931626707315445\n",
      "Epoch 2920: train loss: 0.09265978634357452\n",
      "Epoch 2930: train loss: 0.09216205030679703\n",
      "Epoch 2940: train loss: 0.091669462621212\n",
      "Epoch 2950: train loss: 0.09118197858333588\n",
      "Epoch 2960: train loss: 0.09069961309432983\n",
      "Epoch 2970: train loss: 0.0902223214507103\n",
      "Epoch 2980: train loss: 0.08975009620189667\n",
      "Epoch 2990: train loss: 0.08928290754556656\n",
      "Epoch 3000: train loss: 0.08882074803113937\n",
      "Epoch 3010: train loss: 0.08836359530687332\n",
      "Epoch 3020: train loss: 0.0879114419221878\n",
      "Epoch 3030: train loss: 0.08746423572301865\n",
      "Epoch 3040: train loss: 0.08702199906110764\n",
      "Epoch 3050: train loss: 0.08658467233181\n",
      "Epoch 3060: train loss: 0.08615227788686752\n",
      "Epoch 3070: train loss: 0.08572474122047424\n",
      "Epoch 3080: train loss: 0.08530206233263016\n",
      "Epoch 3090: train loss: 0.08488424122333527\n",
      "Epoch 3100: train loss: 0.08447122573852539\n",
      "Epoch 3110: train loss: 0.08406299352645874\n",
      "Epoch 3120: train loss: 0.08365954458713531\n",
      "Epoch 3130: train loss: 0.08326081931591034\n",
      "Epoch 3140: train loss: 0.0828668400645256\n",
      "Epoch 3150: train loss: 0.08247753232717514\n",
      "Epoch 3160: train loss: 0.08209290355443954\n",
      "Epoch 3170: train loss: 0.08171293139457703\n",
      "Epoch 3180: train loss: 0.081337571144104\n",
      "Epoch 3190: train loss: 0.08096681535243988\n",
      "Epoch 3200: train loss: 0.08060061186552048\n",
      "Epoch 3210: train loss: 0.0802389532327652\n",
      "Epoch 3220: train loss: 0.07988183200359344\n",
      "Epoch 3230: train loss: 0.07952918112277985\n",
      "Epoch 3240: train loss: 0.0791810005903244\n",
      "Epoch 3250: train loss: 0.07883725315332413\n",
      "Epoch 3260: train loss: 0.07849791646003723\n",
      "Epoch 3270: train loss: 0.07816295325756073\n",
      "Epoch 3280: train loss: 0.07783235609531403\n",
      "Epoch 3290: train loss: 0.07750606536865234\n",
      "Epoch 3300: train loss: 0.07718408107757568\n",
      "Epoch 3310: train loss: 0.07686633616685867\n",
      "Epoch 3320: train loss: 0.0765528529882431\n",
      "Epoch 3330: train loss: 0.0762435793876648\n",
      "Epoch 3340: train loss: 0.07593846321105957\n",
      "Epoch 3350: train loss: 0.07563751190900803\n",
      "Epoch 3360: train loss: 0.07534067332744598\n",
      "Epoch 3370: train loss: 0.07504791021347046\n",
      "Epoch 3380: train loss: 0.07475923001766205\n",
      "Epoch 3390: train loss: 0.07447455823421478\n",
      "Epoch 3400: train loss: 0.07419388741254807\n",
      "Epoch 3410: train loss: 0.0739171952009201\n",
      "Epoch 3420: train loss: 0.07364442199468613\n",
      "Epoch 3430: train loss: 0.07337556034326553\n",
      "Epoch 3440: train loss: 0.07311056554317474\n",
      "Epoch 3450: train loss: 0.07284940779209137\n",
      "Epoch 3460: train loss: 0.07259206473827362\n",
      "Epoch 3470: train loss: 0.07233850657939911\n",
      "Epoch 3480: train loss: 0.07208866626024246\n",
      "Epoch 3490: train loss: 0.07184257358312607\n",
      "Epoch 3500: train loss: 0.07160013914108276\n",
      "Epoch 3510: train loss: 0.07136137038469315\n",
      "Epoch 3520: train loss: 0.07112620770931244\n",
      "Epoch 3530: train loss: 0.07089461386203766\n",
      "Epoch 3540: train loss: 0.07066661864519119\n",
      "Epoch 3550: train loss: 0.0704420953989029\n",
      "Epoch 3560: train loss: 0.07022105902433395\n",
      "Epoch 3570: train loss: 0.07000350207090378\n",
      "Epoch 3580: train loss: 0.06978937238454819\n",
      "Epoch 3590: train loss: 0.06957864761352539\n",
      "Epoch 3600: train loss: 0.06937123835086823\n",
      "Epoch 3610: train loss: 0.0691671296954155\n",
      "Epoch 3620: train loss: 0.06896631419658661\n",
      "Epoch 3630: train loss: 0.06876875460147858\n",
      "Epoch 3640: train loss: 0.06857442855834961\n",
      "Epoch 3650: train loss: 0.06838331371545792\n",
      "Epoch 3660: train loss: 0.0681953951716423\n",
      "Epoch 3670: train loss: 0.06801058351993561\n",
      "Epoch 3680: train loss: 0.06782885640859604\n",
      "Epoch 3690: train loss: 0.06765017658472061\n",
      "Epoch 3700: train loss: 0.06747450679540634\n",
      "Epoch 3710: train loss: 0.06730182468891144\n",
      "Epoch 3720: train loss: 0.0671321377158165\n",
      "Epoch 3730: train loss: 0.06696537137031555\n",
      "Epoch 3740: train loss: 0.06680147349834442\n",
      "Epoch 3750: train loss: 0.0666404515504837\n",
      "Epoch 3760: train loss: 0.06648225337266922\n",
      "Epoch 3770: train loss: 0.06632684916257858\n",
      "Epoch 3780: train loss: 0.0661742091178894\n",
      "Epoch 3790: train loss: 0.0660242959856987\n",
      "Epoch 3800: train loss: 0.06587707996368408\n",
      "Epoch 3810: train loss: 0.06573252379894257\n",
      "Epoch 3820: train loss: 0.06559060513973236\n",
      "Epoch 3830: train loss: 0.06545129418373108\n",
      "Epoch 3840: train loss: 0.06531453132629395\n",
      "Epoch 3850: train loss: 0.06518031656742096\n",
      "Epoch 3860: train loss: 0.06504859775304794\n",
      "Epoch 3870: train loss: 0.06491934508085251\n",
      "Epoch 3880: train loss: 0.06479253619909286\n",
      "Epoch 3890: train loss: 0.06466812640428543\n",
      "Epoch 3900: train loss: 0.06454610824584961\n",
      "Epoch 3910: train loss: 0.06442642211914062\n",
      "Epoch 3920: train loss: 0.06430905312299728\n",
      "Epoch 3930: train loss: 0.0641939640045166\n",
      "Epoch 3940: train loss: 0.0640811175107956\n",
      "Epoch 3950: train loss: 0.06397049129009247\n",
      "Epoch 3960: train loss: 0.06386205554008484\n",
      "Epoch 3970: train loss: 0.06375576555728912\n",
      "Epoch 3980: train loss: 0.06365161389112473\n",
      "Epoch 3990: train loss: 0.06354955583810806\n",
      "Epoch 4000: train loss: 0.06344955414533615\n",
      "Epoch 4010: train loss: 0.0633515939116478\n",
      "Epoch 4020: train loss: 0.06325563788414001\n",
      "Epoch 4030: train loss: 0.06316164880990982\n",
      "Epoch 4040: train loss: 0.06306961178779602\n",
      "Epoch 4050: train loss: 0.06297949701547623\n",
      "Epoch 4060: train loss: 0.06289125978946686\n",
      "Epoch 4070: train loss: 0.06280488520860672\n",
      "Epoch 4080: train loss: 0.06272033601999283\n",
      "Epoch 4090: train loss: 0.06263759732246399\n",
      "Epoch 4100: train loss: 0.06255661696195602\n",
      "Epoch 4110: train loss: 0.06247737631201744\n",
      "Epoch 4120: train loss: 0.06239987164735794\n",
      "Epoch 4130: train loss: 0.06232403591275215\n",
      "Epoch 4140: train loss: 0.06224988028407097\n",
      "Epoch 4150: train loss: 0.06217734143137932\n",
      "Epoch 4160: train loss: 0.062106408178806305\n",
      "Epoch 4170: train loss: 0.062037065625190735\n",
      "Epoch 4180: train loss: 0.06196926161646843\n",
      "Epoch 4190: train loss: 0.061902984976768494\n",
      "Epoch 4200: train loss: 0.06183820962905884\n",
      "Epoch 4210: train loss: 0.06177492067217827\n",
      "Epoch 4220: train loss: 0.061713069677352905\n",
      "Epoch 4230: train loss: 0.06165263056755066\n",
      "Epoch 4240: train loss: 0.06159360334277153\n",
      "Epoch 4250: train loss: 0.06153592839837074\n",
      "Epoch 4260: train loss: 0.06147962436079979\n",
      "Epoch 4270: train loss: 0.061424631625413895\n",
      "Epoch 4280: train loss: 0.06137092411518097\n",
      "Epoch 4290: train loss: 0.06131850928068161\n",
      "Epoch 4300: train loss: 0.06126733496785164\n",
      "Epoch 4310: train loss: 0.06121738255023956\n",
      "Epoch 4320: train loss: 0.06116865202784538\n",
      "Epoch 4330: train loss: 0.06112109497189522\n",
      "Epoch 4340: train loss: 0.06107468530535698\n",
      "Epoch 4350: train loss: 0.06102941557765007\n",
      "Epoch 4360: train loss: 0.06098524108529091\n",
      "Epoch 4370: train loss: 0.060942161828279495\n",
      "Epoch 4380: train loss: 0.06090017035603523\n",
      "Epoch 4390: train loss: 0.06085923686623573\n",
      "Epoch 4400: train loss: 0.06081930920481682\n",
      "Epoch 4410: train loss: 0.06078037992119789\n",
      "Epoch 4420: train loss: 0.06074242293834686\n",
      "Epoch 4430: train loss: 0.06070544198155403\n",
      "Epoch 4440: train loss: 0.060669414699077606\n",
      "Epoch 4450: train loss: 0.060634296387434006\n",
      "Epoch 4460: train loss: 0.06060009449720383\n",
      "Epoch 4470: train loss: 0.060566775500774384\n",
      "Epoch 4480: train loss: 0.060534317046403885\n",
      "Epoch 4490: train loss: 0.06050271540880203\n",
      "Epoch 4500: train loss: 0.06047193333506584\n",
      "Epoch 4510: train loss: 0.06044195964932442\n",
      "Epoch 4520: train loss: 0.06041278690099716\n",
      "Epoch 4530: train loss: 0.06038438156247139\n",
      "Epoch 4540: train loss: 0.06035672873258591\n",
      "Epoch 4550: train loss: 0.060329828411340714\n",
      "Epoch 4560: train loss: 0.060303643345832825\n",
      "Epoch 4570: train loss: 0.06027815863490105\n",
      "Epoch 4580: train loss: 0.06025336682796478\n",
      "Epoch 4590: train loss: 0.06022924184799194\n",
      "Epoch 4600: train loss: 0.06020578369498253\n",
      "Epoch 4610: train loss: 0.060182951390743256\n",
      "Epoch 4620: train loss: 0.06016075983643532\n",
      "Epoch 4630: train loss: 0.06013917177915573\n",
      "Epoch 4640: train loss: 0.0601181834936142\n",
      "Epoch 4650: train loss: 0.060097768902778625\n",
      "Epoch 4660: train loss: 0.06007792428135872\n",
      "Epoch 4670: train loss: 0.06005863472819328\n",
      "Epoch 4680: train loss: 0.060039885342121124\n",
      "Epoch 4690: train loss: 0.060021646320819855\n",
      "Epoch 4700: train loss: 0.06000392884016037\n",
      "Epoch 4710: train loss: 0.05998670309782028\n",
      "Epoch 4720: train loss: 0.05996997654438019\n",
      "Epoch 4730: train loss: 0.059953708201646805\n",
      "Epoch 4740: train loss: 0.05993790552020073\n",
      "Epoch 4750: train loss: 0.05992254987359047\n",
      "Epoch 4760: train loss: 0.05990762636065483\n",
      "Epoch 4770: train loss: 0.059893134981393814\n",
      "Epoch 4780: train loss: 0.05987904965877533\n",
      "Epoch 4790: train loss: 0.05986536666750908\n",
      "Epoch 4800: train loss: 0.05985207483172417\n",
      "Epoch 4810: train loss: 0.0598391517996788\n",
      "Epoch 4820: train loss: 0.05982661247253418\n",
      "Epoch 4830: train loss: 0.05981441214680672\n",
      "Epoch 4840: train loss: 0.0598025768995285\n",
      "Epoch 4850: train loss: 0.05979107692837715\n",
      "Epoch 4860: train loss: 0.059779904782772064\n",
      "Epoch 4870: train loss: 0.05976904556155205\n",
      "Epoch 4880: train loss: 0.059758491814136505\n",
      "Epoch 4890: train loss: 0.059748243540525436\n",
      "Epoch 4900: train loss: 0.05973828583955765\n",
      "Epoch 04904: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 4910: train loss: 0.059731971472501755\n",
      "Epoch 04911: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 04917: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 4920: train loss: 0.05973082408308983\n",
      "Epoch 04923: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 04929: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 4930: train loss: 0.05973067507147789\n",
      "Epoch 04935: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 4940: train loss: 0.05973066762089729\n",
      "Epoch 04941: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 04947: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 4947, loss: 0.059730660170316696\n",
      "Predictions saved to results-std-1-10.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
