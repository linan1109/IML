{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc2 = nn.Linear(256, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-dropout-2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab13b1086083429ba7fb30f22865d5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.42069579099392407, val loss: 0.1514379599094391\n",
      "Epoch 2: train loss: 0.1401553646496364, val loss: 0.12674615061283112\n",
      "Epoch 3: train loss: 0.1013151730359817, val loss: 0.09192320120334625\n",
      "Epoch 4: train loss: 0.0763195397434186, val loss: 0.07998404687643051\n",
      "Epoch 5: train loss: 0.06189837190204737, val loss: 0.05781873804330826\n",
      "Epoch 6: train loss: 0.05036012614928946, val loss: 0.04506524929404259\n",
      "Epoch 7: train loss: 0.041445595910962744, val loss: 0.041797691017389296\n",
      "Epoch 8: train loss: 0.03348705738570009, val loss: 0.0338082275390625\n",
      "Epoch 9: train loss: 0.027858714104915153, val loss: 0.02414726699888706\n",
      "Epoch 10: train loss: 0.02218524452374906, val loss: 0.020385963335633278\n",
      "Epoch 11: train loss: 0.01778050575603028, val loss: 0.017010319858789445\n",
      "Epoch 12: train loss: 0.014390188972864832, val loss: 0.012958048731088639\n",
      "Epoch 13: train loss: 0.011752959267369339, val loss: 0.0112060821428895\n",
      "Epoch 14: train loss: 0.009589919789409151, val loss: 0.011407350786030293\n",
      "Epoch 15: train loss: 0.00850707221548168, val loss: 0.00922357401996851\n",
      "Epoch 16: train loss: 0.007467616651055156, val loss: 0.007429724551737308\n",
      "Epoch 17: train loss: 0.006643552125868749, val loss: 0.007139920961111784\n",
      "Epoch 18: train loss: 0.006237939432491454, val loss: 0.006600741907954216\n",
      "Epoch 19: train loss: 0.005999860143859168, val loss: 0.010365643542259932\n",
      "Epoch 20: train loss: 0.005734437126514255, val loss: 0.006013733077794314\n",
      "Epoch 21: train loss: 0.005376424590619851, val loss: 0.006282721284776926\n",
      "Epoch 22: train loss: 0.005429012007022999, val loss: 0.006226348992437124\n",
      "Epoch 23: train loss: 0.00527236846940858, val loss: 0.0054886193051934245\n",
      "Epoch 24: train loss: 0.005159197079207824, val loss: 0.006112518575042486\n",
      "Epoch 25: train loss: 0.005107966461047835, val loss: 0.005560417283326387\n",
      "Epoch 26: train loss: 0.0051603354758449965, val loss: 0.005355294413864613\n",
      "Epoch 27: train loss: 0.005124067043847575, val loss: 0.0066394336596131325\n",
      "Epoch 28: train loss: 0.005029906332416802, val loss: 0.006235408030450344\n",
      "Epoch 29: train loss: 0.00497015327831008, val loss: 0.006268786415457726\n",
      "Epoch 30: train loss: 0.005086693908791153, val loss: 0.005701425794512034\n",
      "Epoch 31: train loss: 0.004976894395768035, val loss: 0.0069641596153378485\n",
      "Epoch 32: train loss: 0.0052541739274652634, val loss: 0.005209110893309116\n",
      "Epoch 33: train loss: 0.004992828236094543, val loss: 0.005521405320614577\n",
      "Epoch 34: train loss: 0.005455483001637824, val loss: 0.00650585126131773\n",
      "Epoch 35: train loss: 0.005174960527949187, val loss: 0.0047535735070705415\n",
      "Epoch 36: train loss: 0.005311980049524988, val loss: 0.005763900000602007\n",
      "Epoch 37: train loss: 0.005176559773634891, val loss: 0.005932222478091716\n",
      "Epoch 38: train loss: 0.005307212243426819, val loss: 0.006309579111635685\n",
      "Epoch 39: train loss: 0.005400779750502231, val loss: 0.00686710936948657\n",
      "Epoch 40: train loss: 0.005506299603046203, val loss: 0.006023613020777702\n",
      "Epoch 00041: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 41: train loss: 0.005287020451332234, val loss: 0.006095860850065946\n",
      "Epoch 42: train loss: 0.003544758796653881, val loss: 0.00407638905197382\n",
      "Epoch 43: train loss: 0.002834712198156179, val loss: 0.0036553432308137415\n",
      "Epoch 44: train loss: 0.0027432805534300148, val loss: 0.0036665740087628365\n",
      "Epoch 45: train loss: 0.0027606776160549146, val loss: 0.0034473838564008474\n",
      "Epoch 46: train loss: 0.002750275778185044, val loss: 0.003553289845585823\n",
      "Epoch 47: train loss: 0.002865960182850154, val loss: 0.003919544445350766\n",
      "Epoch 48: train loss: 0.002889672640749082, val loss: 0.0039012546464800833\n",
      "Epoch 49: train loss: 0.0028212218790182046, val loss: 0.0035375662967562674\n",
      "Epoch 50: train loss: 0.0027957014481023866, val loss: 0.003963870409876108\n",
      "Epoch 00051: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 51: train loss: 0.0028768327569551008, val loss: 0.003768045548349619\n",
      "Epoch 52: train loss: 0.0021917986245635823, val loss: 0.0030878293402493\n",
      "Epoch 53: train loss: 0.0018423506297475221, val loss: 0.0029298759195953606\n",
      "Epoch 54: train loss: 0.0017547910847530072, val loss: 0.002884769957512617\n",
      "Epoch 55: train loss: 0.0017111163766180375, val loss: 0.003087229374796152\n",
      "Epoch 56: train loss: 0.0016852839692324704, val loss: 0.0029140053912997247\n",
      "Epoch 57: train loss: 0.0016774599386037005, val loss: 0.002990325141698122\n",
      "Epoch 58: train loss: 0.0016752446605918967, val loss: 0.0031677213218063117\n",
      "Epoch 59: train loss: 0.0016785701067776096, val loss: 0.002977192612364888\n",
      "Epoch 00060: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 60: train loss: 0.0016685705460151848, val loss: 0.003051147807389498\n",
      "Epoch 61: train loss: 0.0014802211443038316, val loss: 0.002682288287207484\n",
      "Epoch 62: train loss: 0.0013808891148477489, val loss: 0.002875157793983817\n",
      "Epoch 63: train loss: 0.0013328576758313848, val loss: 0.002877434317022562\n",
      "Epoch 64: train loss: 0.0013375580518943618, val loss: 0.0028904252722859384\n",
      "Epoch 65: train loss: 0.0013069920738284686, val loss: 0.002718353372067213\n",
      "Epoch 66: train loss: 0.0013166477611972665, val loss: 0.0027065781094133853\n",
      "Epoch 00067: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 67: train loss: 0.0012944829141987222, val loss: 0.002925014132633805\n",
      "Epoch 68: train loss: 0.0012367781568888804, val loss: 0.0027667460981756447\n",
      "Epoch 69: train loss: 0.0012118373912645085, val loss: 0.0026804288029670716\n",
      "Epoch 70: train loss: 0.0012086444939961847, val loss: 0.0026202965136617423\n",
      "Epoch 71: train loss: 0.0011834837125272166, val loss: 0.0027712403628975153\n",
      "Epoch 72: train loss: 0.0011793787746633194, val loss: 0.0027769663762301205\n",
      "Epoch 73: train loss: 0.0011715566136171014, val loss: 0.0025888046212494374\n",
      "Epoch 74: train loss: 0.0011663018603029908, val loss: 0.0026972413063049315\n",
      "Epoch 75: train loss: 0.001158339021214265, val loss: 0.0025661156121641396\n",
      "Epoch 76: train loss: 0.0011508409062172382, val loss: 0.00272663520462811\n",
      "Epoch 77: train loss: 0.0011580483829982731, val loss: 0.0027252810392528774\n",
      "Epoch 78: train loss: 0.0011672202653194568, val loss: 0.0026415226198732854\n",
      "Epoch 79: train loss: 0.0011343275431771667, val loss: 0.0025572061259299516\n",
      "Epoch 80: train loss: 0.001143077940242935, val loss: 0.002671673469245434\n",
      "Epoch 81: train loss: 0.0011285228109253304, val loss: 0.0026001764982938767\n",
      "Epoch 82: train loss: 0.001127847178706101, val loss: 0.0025628353171050547\n",
      "Epoch 83: train loss: 0.0011441455974946825, val loss: 0.0025246942099183796\n",
      "Epoch 84: train loss: 0.0011301047745141753, val loss: 0.0026491091810166836\n",
      "Epoch 85: train loss: 0.001126547022836701, val loss: 0.002599590718746185\n",
      "Epoch 86: train loss: 0.0011244799407313065, val loss: 0.00269893167540431\n",
      "Epoch 87: train loss: 0.0011096755531046312, val loss: 0.0025668158531188963\n",
      "Epoch 88: train loss: 0.0011079921160577512, val loss: 0.0026313897110521794\n",
      "Epoch 89: train loss: 0.0010939059132348975, val loss: 0.0024886500723659993\n",
      "Epoch 90: train loss: 0.001102593411862546, val loss: 0.002519739791750908\n",
      "Epoch 91: train loss: 0.0011030250295274416, val loss: 0.0026620791032910345\n",
      "Epoch 92: train loss: 0.0010866831169102568, val loss: 0.002614529773592949\n",
      "Epoch 93: train loss: 0.0010943823071310715, val loss: 0.002547826457768679\n",
      "Epoch 94: train loss: 0.0010875620213337243, val loss: 0.0026798207256942988\n",
      "Epoch 00095: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 95: train loss: 0.0011070898134945607, val loss: 0.0027224637009203435\n",
      "Epoch 96: train loss: 0.0010916565015474905, val loss: 0.002625032834708691\n",
      "Epoch 97: train loss: 0.001051454760029684, val loss: 0.0024992185477167366\n",
      "Epoch 98: train loss: 0.0010638924740558984, val loss: 0.002653464773669839\n",
      "Epoch 99: train loss: 0.0010505910632390605, val loss: 0.0026821880172938108\n",
      "Epoch 100: train loss: 0.0010596807068593951, val loss: 0.002752316154539585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611352b882ae429c8dd602dc75286b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 2.1715126037597656\n",
      "Epoch 20: train loss: 1.684804916381836\n",
      "Epoch 30: train loss: 1.4197808504104614\n",
      "Epoch 40: train loss: 1.117823600769043\n",
      "Epoch 50: train loss: 0.8563810586929321\n",
      "Epoch 60: train loss: 0.631883442401886\n",
      "Epoch 70: train loss: 0.45204222202301025\n",
      "Epoch 80: train loss: 0.3252784013748169\n",
      "Epoch 90: train loss: 0.24778829514980316\n",
      "Epoch 100: train loss: 0.20546025037765503\n",
      "Epoch 110: train loss: 0.18106813728809357\n",
      "Epoch 120: train loss: 0.16327592730522156\n",
      "Epoch 130: train loss: 0.14777204394340515\n",
      "Epoch 140: train loss: 0.13396327197551727\n",
      "Epoch 150: train loss: 0.12196128815412521\n",
      "Epoch 160: train loss: 0.11172839999198914\n",
      "Epoch 170: train loss: 0.1030426174402237\n",
      "Epoch 180: train loss: 0.0957895815372467\n",
      "Epoch 190: train loss: 0.08977731317281723\n",
      "Epoch 200: train loss: 0.08486223220825195\n",
      "Epoch 210: train loss: 0.08084699511528015\n",
      "Epoch 220: train loss: 0.07756154984235764\n",
      "Epoch 230: train loss: 0.07486870884895325\n",
      "Epoch 240: train loss: 0.07258762419223785\n",
      "Epoch 250: train loss: 0.0708007887005806\n",
      "Epoch 260: train loss: 0.06933394074440002\n",
      "Epoch 270: train loss: 0.06814605742692947\n",
      "Epoch 280: train loss: 0.0671684518456459\n",
      "Epoch 290: train loss: 0.0663667693734169\n",
      "Epoch 300: train loss: 0.06565927714109421\n",
      "Epoch 310: train loss: 0.06500368565320969\n",
      "Epoch 320: train loss: 0.06438950449228287\n",
      "Epoch 330: train loss: 0.0638434886932373\n",
      "Epoch 340: train loss: 0.0633612796664238\n",
      "Epoch 350: train loss: 0.06292309612035751\n",
      "Epoch 360: train loss: 0.06250761449337006\n",
      "Epoch 370: train loss: 0.06211715564131737\n",
      "Epoch 380: train loss: 0.06174090877175331\n",
      "Epoch 390: train loss: 0.06138385459780693\n",
      "Epoch 400: train loss: 0.061039503663778305\n",
      "Epoch 410: train loss: 0.06070825085043907\n",
      "Epoch 420: train loss: 0.06037943810224533\n",
      "Epoch 430: train loss: 0.060062017291784286\n",
      "Epoch 440: train loss: 0.05975694581866264\n",
      "Epoch 450: train loss: 0.059465400874614716\n",
      "Epoch 460: train loss: 0.05918554961681366\n",
      "Epoch 470: train loss: 0.05891024321317673\n",
      "Epoch 480: train loss: 0.05864676460623741\n",
      "Epoch 490: train loss: 0.05840088054537773\n",
      "Epoch 500: train loss: 0.058165863156318665\n",
      "Epoch 510: train loss: 0.05794256925582886\n",
      "Epoch 520: train loss: 0.057731322944164276\n",
      "Epoch 530: train loss: 0.057530611753463745\n",
      "Epoch 540: train loss: 0.05733886733651161\n",
      "Epoch 550: train loss: 0.05715479701757431\n",
      "Epoch 560: train loss: 0.05698089301586151\n",
      "Epoch 570: train loss: 0.056817471981048584\n",
      "Epoch 580: train loss: 0.0566597580909729\n",
      "Epoch 590: train loss: 0.05651259422302246\n",
      "Epoch 600: train loss: 0.056373339146375656\n",
      "Epoch 610: train loss: 0.05624294653534889\n",
      "Epoch 620: train loss: 0.05611772462725639\n",
      "Epoch 630: train loss: 0.056000299751758575\n",
      "Epoch 640: train loss: 0.05589057505130768\n",
      "Epoch 650: train loss: 0.05576333776116371\n",
      "Epoch 660: train loss: 0.0556461475789547\n",
      "Epoch 670: train loss: 0.05553967505693436\n",
      "Epoch 680: train loss: 0.05544465780258179\n",
      "Epoch 690: train loss: 0.05535989627242088\n",
      "Epoch 700: train loss: 0.05527632683515549\n",
      "Epoch 710: train loss: 0.055198878049850464\n",
      "Epoch 720: train loss: 0.055127520114183426\n",
      "Epoch 730: train loss: 0.05505337193608284\n",
      "Epoch 740: train loss: 0.05498478189110756\n",
      "Epoch 750: train loss: 0.05492307245731354\n",
      "Epoch 760: train loss: 0.054856300354003906\n",
      "Epoch 770: train loss: 0.054797809571027756\n",
      "Epoch 780: train loss: 0.05473931506276131\n",
      "Epoch 790: train loss: 0.05468083918094635\n",
      "Epoch 800: train loss: 0.054628219455480576\n",
      "Epoch 810: train loss: 0.054572004824876785\n",
      "Epoch 820: train loss: 0.054519519209861755\n",
      "Epoch 830: train loss: 0.054472874850034714\n",
      "Epoch 840: train loss: 0.054424308240413666\n",
      "Epoch 850: train loss: 0.05437403544783592\n",
      "Epoch 860: train loss: 0.054328128695487976\n",
      "Epoch 870: train loss: 0.05427977442741394\n",
      "Epoch 880: train loss: 0.05423441156744957\n",
      "Epoch 890: train loss: 0.054190464317798615\n",
      "Epoch 900: train loss: 0.05415014922618866\n",
      "Epoch 910: train loss: 0.05410671979188919\n",
      "Epoch 920: train loss: 0.05406888946890831\n",
      "Epoch 930: train loss: 0.05402160435914993\n",
      "Epoch 940: train loss: 0.05398186668753624\n",
      "Epoch 950: train loss: 0.05393931642174721\n",
      "Epoch 960: train loss: 0.05389830842614174\n",
      "Epoch 970: train loss: 0.05386245623230934\n",
      "Epoch 980: train loss: 0.053816404193639755\n",
      "Epoch 990: train loss: 0.053777143359184265\n",
      "Epoch 1000: train loss: 0.0537370890378952\n",
      "Epoch 1010: train loss: 0.05369449406862259\n",
      "Epoch 1020: train loss: 0.053653523325920105\n",
      "Epoch 1030: train loss: 0.05360966548323631\n",
      "Epoch 1040: train loss: 0.0535716712474823\n",
      "Epoch 1050: train loss: 0.05352945625782013\n",
      "Epoch 1060: train loss: 0.05349188670516014\n",
      "Epoch 1070: train loss: 0.05344954878091812\n",
      "Epoch 1080: train loss: 0.053409356623888016\n",
      "Epoch 1090: train loss: 0.05336853861808777\n",
      "Epoch 1100: train loss: 0.05332392081618309\n",
      "Epoch 1110: train loss: 0.05328531190752983\n",
      "Epoch 1120: train loss: 0.05324545130133629\n",
      "Epoch 1130: train loss: 0.05320579558610916\n",
      "Epoch 1140: train loss: 0.053165003657341\n",
      "Epoch 1150: train loss: 0.05312572419643402\n",
      "Epoch 1160: train loss: 0.05308258533477783\n",
      "Epoch 1170: train loss: 0.0530402809381485\n",
      "Epoch 1180: train loss: 0.05299929529428482\n",
      "Epoch 1190: train loss: 0.052962854504585266\n",
      "Epoch 1200: train loss: 0.05292489379644394\n",
      "Epoch 1210: train loss: 0.05288078263401985\n",
      "Epoch 1220: train loss: 0.052838876843452454\n",
      "Epoch 1230: train loss: 0.05279726907610893\n",
      "Epoch 1240: train loss: 0.05275876075029373\n",
      "Epoch 1250: train loss: 0.052720069885253906\n",
      "Epoch 1260: train loss: 0.05268184095621109\n",
      "Epoch 1270: train loss: 0.05263994634151459\n",
      "Epoch 1280: train loss: 0.05259755998849869\n",
      "Epoch 1290: train loss: 0.05256276950240135\n",
      "Epoch 1300: train loss: 0.05252404138445854\n",
      "Epoch 1310: train loss: 0.052486080676317215\n",
      "Epoch 1320: train loss: 0.052445847541093826\n",
      "Epoch 1330: train loss: 0.05240728333592415\n",
      "Epoch 1340: train loss: 0.05236467719078064\n",
      "Epoch 1350: train loss: 0.05232271924614906\n",
      "Epoch 1360: train loss: 0.05228748172521591\n",
      "Epoch 1370: train loss: 0.052250344306230545\n",
      "Epoch 1380: train loss: 0.05221317708492279\n",
      "Epoch 1390: train loss: 0.052174873650074005\n",
      "Epoch 1400: train loss: 0.05213714390993118\n",
      "Epoch 1410: train loss: 0.052104294300079346\n",
      "Epoch 1420: train loss: 0.05206774175167084\n",
      "Epoch 1430: train loss: 0.05202898755669594\n",
      "Epoch 1440: train loss: 0.051991213113069534\n",
      "Epoch 1450: train loss: 0.05195411667227745\n",
      "Epoch 1460: train loss: 0.05191374197602272\n",
      "Epoch 1470: train loss: 0.05188213288784027\n",
      "Epoch 1480: train loss: 0.05184381827712059\n",
      "Epoch 1490: train loss: 0.051817119121551514\n",
      "Epoch 1500: train loss: 0.0517730712890625\n",
      "Epoch 1510: train loss: 0.05173114314675331\n",
      "Epoch 1520: train loss: 0.05168988183140755\n",
      "Epoch 1530: train loss: 0.05165913328528404\n",
      "Epoch 1540: train loss: 0.05162215977907181\n",
      "Epoch 1550: train loss: 0.051586415618658066\n",
      "Epoch 1560: train loss: 0.05155785381793976\n",
      "Epoch 1570: train loss: 0.051513079553842545\n",
      "Epoch 1580: train loss: 0.05147522687911987\n",
      "Epoch 1590: train loss: 0.05143772065639496\n",
      "Epoch 1600: train loss: 0.051403675228357315\n",
      "Epoch 1610: train loss: 0.05136454477906227\n",
      "Epoch 1620: train loss: 0.0513310432434082\n",
      "Epoch 1630: train loss: 0.05128630995750427\n",
      "Epoch 1640: train loss: 0.051256053149700165\n",
      "Epoch 1650: train loss: 0.05121776834130287\n",
      "Epoch 1660: train loss: 0.05118676647543907\n",
      "Epoch 1670: train loss: 0.05114780366420746\n",
      "Epoch 1680: train loss: 0.051108911633491516\n",
      "Epoch 1690: train loss: 0.05107235163450241\n",
      "Epoch 1700: train loss: 0.05103594809770584\n",
      "Epoch 1710: train loss: 0.05100596323609352\n",
      "Epoch 1720: train loss: 0.05097325146198273\n",
      "Epoch 1730: train loss: 0.050936274230480194\n",
      "Epoch 1740: train loss: 0.05090001970529556\n",
      "Epoch 1750: train loss: 0.05085976794362068\n",
      "Epoch 1760: train loss: 0.050836607813835144\n",
      "Epoch 1770: train loss: 0.05079152062535286\n",
      "Epoch 1780: train loss: 0.05075204744935036\n",
      "Epoch 1790: train loss: 0.05071892589330673\n",
      "Epoch 1800: train loss: 0.05068981647491455\n",
      "Epoch 1810: train loss: 0.050652921199798584\n",
      "Epoch 1820: train loss: 0.05061555281281471\n",
      "Epoch 1830: train loss: 0.050573885440826416\n",
      "Epoch 1840: train loss: 0.050539758056402206\n",
      "Epoch 1850: train loss: 0.05049966648221016\n",
      "Epoch 1860: train loss: 0.05046813189983368\n",
      "Epoch 1870: train loss: 0.05042950436472893\n",
      "Epoch 1880: train loss: 0.050397928804159164\n",
      "Epoch 1890: train loss: 0.05036415904760361\n",
      "Epoch 1900: train loss: 0.05032281577587128\n",
      "Epoch 1910: train loss: 0.05029222369194031\n",
      "Epoch 1920: train loss: 0.05025357007980347\n",
      "Epoch 1930: train loss: 0.05021684616804123\n",
      "Epoch 1940: train loss: 0.05017992854118347\n",
      "Epoch 1950: train loss: 0.05014316737651825\n",
      "Epoch 1960: train loss: 0.050105009227991104\n",
      "Epoch 1970: train loss: 0.050070419907569885\n",
      "Epoch 1980: train loss: 0.05003712698817253\n",
      "Epoch 1990: train loss: 0.05000082775950432\n",
      "Epoch 2000: train loss: 0.049964889883995056\n",
      "Epoch 2010: train loss: 0.049925267696380615\n",
      "Epoch 2020: train loss: 0.04988674074411392\n",
      "Epoch 2030: train loss: 0.04985283315181732\n",
      "Epoch 2040: train loss: 0.04981604591012001\n",
      "Epoch 2050: train loss: 0.049775753170251846\n",
      "Epoch 2060: train loss: 0.04974135383963585\n",
      "Epoch 2070: train loss: 0.04970323294401169\n",
      "Epoch 2080: train loss: 0.04966927319765091\n",
      "Epoch 2090: train loss: 0.049630504101514816\n",
      "Epoch 2100: train loss: 0.04959700629115105\n",
      "Epoch 2110: train loss: 0.04955700784921646\n",
      "Epoch 2120: train loss: 0.04952100291848183\n",
      "Epoch 2130: train loss: 0.049482058733701706\n",
      "Epoch 2140: train loss: 0.04944045469164848\n",
      "Epoch 2150: train loss: 0.04940532147884369\n",
      "Epoch 2160: train loss: 0.0493725948035717\n",
      "Epoch 2170: train loss: 0.049334682524204254\n",
      "Epoch 2180: train loss: 0.04930073767900467\n",
      "Epoch 2190: train loss: 0.04926188290119171\n",
      "Epoch 2200: train loss: 0.049218062311410904\n",
      "Epoch 2210: train loss: 0.04917734116315842\n",
      "Epoch 2220: train loss: 0.04914327338337898\n",
      "Epoch 2230: train loss: 0.04910774156451225\n",
      "Epoch 2240: train loss: 0.04906676337122917\n",
      "Epoch 2250: train loss: 0.04903273656964302\n",
      "Epoch 2260: train loss: 0.04899328202009201\n",
      "Epoch 2270: train loss: 0.04895706847310066\n",
      "Epoch 2280: train loss: 0.04891306906938553\n",
      "Epoch 2290: train loss: 0.048874035477638245\n",
      "Epoch 2300: train loss: 0.048837851732969284\n",
      "Epoch 2310: train loss: 0.04879871383309364\n",
      "Epoch 2320: train loss: 0.0487557053565979\n",
      "Epoch 2330: train loss: 0.048719149082899094\n",
      "Epoch 2340: train loss: 0.04867617040872574\n",
      "Epoch 2350: train loss: 0.04863996431231499\n",
      "Epoch 2360: train loss: 0.04859679192304611\n",
      "Epoch 2370: train loss: 0.048559464514255524\n",
      "Epoch 2380: train loss: 0.0485164076089859\n",
      "Epoch 2390: train loss: 0.0484711155295372\n",
      "Epoch 2400: train loss: 0.048437003046274185\n",
      "Epoch 2410: train loss: 0.048395756632089615\n",
      "Epoch 2420: train loss: 0.04834981635212898\n",
      "Epoch 2430: train loss: 0.04830503836274147\n",
      "Epoch 2440: train loss: 0.04827158898115158\n",
      "Epoch 2450: train loss: 0.04821909964084625\n",
      "Epoch 2460: train loss: 0.048176269978284836\n",
      "Epoch 2470: train loss: 0.048136550933122635\n",
      "Epoch 2480: train loss: 0.048095185309648514\n",
      "Epoch 2490: train loss: 0.04805281385779381\n",
      "Epoch 2500: train loss: 0.04800672456622124\n",
      "Epoch 2510: train loss: 0.04795832931995392\n",
      "Epoch 2520: train loss: 0.0479198694229126\n",
      "Epoch 2530: train loss: 0.04787497594952583\n",
      "Epoch 2540: train loss: 0.04782797768712044\n",
      "Epoch 2550: train loss: 0.04777959734201431\n",
      "Epoch 2560: train loss: 0.04773547872900963\n",
      "Epoch 2570: train loss: 0.04768548533320427\n",
      "Epoch 2580: train loss: 0.04763707146048546\n",
      "Epoch 2590: train loss: 0.047593917697668076\n",
      "Epoch 2600: train loss: 0.04754950478672981\n",
      "Epoch 2610: train loss: 0.04750652238726616\n",
      "Epoch 2620: train loss: 0.047452762722969055\n",
      "Epoch 2630: train loss: 0.047402914613485336\n",
      "Epoch 2640: train loss: 0.047357335686683655\n",
      "Epoch 2650: train loss: 0.04732047766447067\n",
      "Epoch 2660: train loss: 0.04727032408118248\n",
      "Epoch 2670: train loss: 0.047215551137924194\n",
      "Epoch 2680: train loss: 0.04716950282454491\n",
      "Epoch 2690: train loss: 0.047118764370679855\n",
      "Epoch 2700: train loss: 0.04707341268658638\n",
      "Epoch 2710: train loss: 0.04701985791325569\n",
      "Epoch 2720: train loss: 0.046974003314971924\n",
      "Epoch 2730: train loss: 0.046930473297834396\n",
      "Epoch 2740: train loss: 0.04687770828604698\n",
      "Epoch 2750: train loss: 0.04682353883981705\n",
      "Epoch 2760: train loss: 0.046780116856098175\n",
      "Epoch 2770: train loss: 0.04673584923148155\n",
      "Epoch 2780: train loss: 0.04668160155415535\n",
      "Epoch 2790: train loss: 0.04662781208753586\n",
      "Epoch 2800: train loss: 0.04657367616891861\n",
      "Epoch 2810: train loss: 0.04652699828147888\n",
      "Epoch 2820: train loss: 0.046476658433675766\n",
      "Epoch 2830: train loss: 0.04643021151423454\n",
      "Epoch 2840: train loss: 0.04637159779667854\n",
      "Epoch 2850: train loss: 0.04632735997438431\n",
      "Epoch 2860: train loss: 0.046267736703157425\n",
      "Epoch 2870: train loss: 0.04621350020170212\n",
      "Epoch 2880: train loss: 0.04617218300700188\n",
      "Epoch 2890: train loss: 0.04611331969499588\n",
      "Epoch 2900: train loss: 0.04605792835354805\n",
      "Epoch 2910: train loss: 0.04600255563855171\n",
      "Epoch 2920: train loss: 0.04595867916941643\n",
      "Epoch 2930: train loss: 0.04590796306729317\n",
      "Epoch 2940: train loss: 0.045842934399843216\n",
      "Epoch 2950: train loss: 0.045787062495946884\n",
      "Epoch 2960: train loss: 0.04574205353856087\n",
      "Epoch 2970: train loss: 0.04567842558026314\n",
      "Epoch 2980: train loss: 0.04563059285283089\n",
      "Epoch 2990: train loss: 0.045581549406051636\n",
      "Epoch 3000: train loss: 0.045522455126047134\n",
      "Epoch 3010: train loss: 0.04545770585536957\n",
      "Epoch 3020: train loss: 0.04541093856096268\n",
      "Epoch 3030: train loss: 0.045354947447776794\n",
      "Epoch 3040: train loss: 0.04530330374836922\n",
      "Epoch 3050: train loss: 0.045242685824632645\n",
      "Epoch 3060: train loss: 0.04519306868314743\n",
      "Epoch 3070: train loss: 0.045133646577596664\n",
      "Epoch 3080: train loss: 0.045096505433321\n",
      "Epoch 3090: train loss: 0.04502429813146591\n",
      "Epoch 3100: train loss: 0.044963035732507706\n",
      "Epoch 3110: train loss: 0.04490843787789345\n",
      "Epoch 3120: train loss: 0.044854454696178436\n",
      "Epoch 3130: train loss: 0.04479976370930672\n",
      "Epoch 3140: train loss: 0.044740088284015656\n",
      "Epoch 3150: train loss: 0.04468124359846115\n",
      "Epoch 3160: train loss: 0.044623907655477524\n",
      "Epoch 3170: train loss: 0.044558342546224594\n",
      "Epoch 3180: train loss: 0.04450402781367302\n",
      "Epoch 3190: train loss: 0.0444391630589962\n",
      "Epoch 3200: train loss: 0.044381845742464066\n",
      "Epoch 3210: train loss: 0.04432879388332367\n",
      "Epoch 3220: train loss: 0.04428962618112564\n",
      "Epoch 3230: train loss: 0.0442253015935421\n",
      "Epoch 3240: train loss: 0.04416225850582123\n",
      "Epoch 3250: train loss: 0.044103000313043594\n",
      "Epoch 3260: train loss: 0.04404681548476219\n",
      "Epoch 3270: train loss: 0.04398885741829872\n",
      "Epoch 3280: train loss: 0.043926604092121124\n",
      "Epoch 3290: train loss: 0.0438430979847908\n",
      "Epoch 3300: train loss: 0.043775953352451324\n",
      "Epoch 3310: train loss: 0.043687839061021805\n",
      "Epoch 3320: train loss: 0.04362979903817177\n",
      "Epoch 3330: train loss: 0.04356774315237999\n",
      "Epoch 3340: train loss: 0.04352295771241188\n",
      "Epoch 3350: train loss: 0.04345802217721939\n",
      "Epoch 3360: train loss: 0.04338378459215164\n",
      "Epoch 3370: train loss: 0.0433218888938427\n",
      "Epoch 3380: train loss: 0.043267421424388885\n",
      "Epoch 3390: train loss: 0.04320504143834114\n",
      "Epoch 3400: train loss: 0.04312742128968239\n",
      "Epoch 3410: train loss: 0.043061207979917526\n",
      "Epoch 3420: train loss: 0.04299973323941231\n",
      "Epoch 3430: train loss: 0.04293942451477051\n",
      "Epoch 3440: train loss: 0.042871780693531036\n",
      "Epoch 3450: train loss: 0.042824745178222656\n",
      "Epoch 3460: train loss: 0.04273859038949013\n",
      "Epoch 3470: train loss: 0.04267243295907974\n",
      "Epoch 3480: train loss: 0.04260484129190445\n",
      "Epoch 3490: train loss: 0.042546749114990234\n",
      "Epoch 3500: train loss: 0.04247079789638519\n",
      "Epoch 3510: train loss: 0.04241020232439041\n",
      "Epoch 3520: train loss: 0.042364709079265594\n",
      "Epoch 3530: train loss: 0.042285576462745667\n",
      "Epoch 3540: train loss: 0.04223000258207321\n",
      "Epoch 3550: train loss: 0.042162127792835236\n",
      "Epoch 3560: train loss: 0.04209985211491585\n",
      "Epoch 3570: train loss: 0.04201831296086311\n",
      "Epoch 3580: train loss: 0.041949909180402756\n",
      "Epoch 3590: train loss: 0.04188017174601555\n",
      "Epoch 3600: train loss: 0.04180469363927841\n",
      "Epoch 3610: train loss: 0.041758425533771515\n",
      "Epoch 3620: train loss: 0.041680578142404556\n",
      "Epoch 3630: train loss: 0.04161082208156586\n",
      "Epoch 3640: train loss: 0.04153615981340408\n",
      "Epoch 3650: train loss: 0.041477739810943604\n",
      "Epoch 3660: train loss: 0.04139980301260948\n",
      "Epoch 3670: train loss: 0.041331902146339417\n",
      "Epoch 3680: train loss: 0.041263941675424576\n",
      "Epoch 3690: train loss: 0.04119322821497917\n",
      "Epoch 3700: train loss: 0.041132524609565735\n",
      "Epoch 3710: train loss: 0.041065067052841187\n",
      "Epoch 3720: train loss: 0.040998425334692\n",
      "Epoch 3730: train loss: 0.04093349725008011\n",
      "Epoch 3740: train loss: 0.0408562570810318\n",
      "Epoch 3750: train loss: 0.04078562557697296\n",
      "Epoch 3760: train loss: 0.04071368649601936\n",
      "Epoch 3770: train loss: 0.04064364731311798\n",
      "Epoch 3780: train loss: 0.04057856649160385\n",
      "Epoch 3790: train loss: 0.04049420356750488\n",
      "Epoch 3800: train loss: 0.04044384881854057\n",
      "Epoch 3810: train loss: 0.04036124795675278\n",
      "Epoch 3820: train loss: 0.040288668125867844\n",
      "Epoch 3830: train loss: 0.04022069275379181\n",
      "Epoch 3840: train loss: 0.040144357830286026\n",
      "Epoch 3850: train loss: 0.040077533572912216\n",
      "Epoch 3860: train loss: 0.040012381970882416\n",
      "Epoch 3870: train loss: 0.039933379739522934\n",
      "Epoch 3880: train loss: 0.03986562788486481\n",
      "Epoch 3890: train loss: 0.0398004874587059\n",
      "Epoch 3900: train loss: 0.039745867252349854\n",
      "Epoch 3910: train loss: 0.03966386988759041\n",
      "Epoch 3920: train loss: 0.039595168083906174\n",
      "Epoch 3930: train loss: 0.03952454775571823\n",
      "Epoch 3940: train loss: 0.03944377601146698\n",
      "Epoch 3950: train loss: 0.03937507048249245\n",
      "Epoch 3960: train loss: 0.03932468220591545\n",
      "Epoch 3970: train loss: 0.03924829512834549\n",
      "Epoch 3980: train loss: 0.03917898237705231\n",
      "Epoch 3990: train loss: 0.039097778499126434\n",
      "Epoch 4000: train loss: 0.03903597220778465\n",
      "Epoch 4010: train loss: 0.038954999297857285\n",
      "Epoch 4020: train loss: 0.03888624534010887\n",
      "Epoch 4030: train loss: 0.03882194682955742\n",
      "Epoch 4040: train loss: 0.038760434836149216\n",
      "Epoch 4050: train loss: 0.03868318349123001\n",
      "Epoch 4060: train loss: 0.038604121655225754\n",
      "Epoch 4070: train loss: 0.03854238986968994\n",
      "Epoch 4080: train loss: 0.03845696523785591\n",
      "Epoch 4090: train loss: 0.03838586062192917\n",
      "Epoch 4100: train loss: 0.03832598775625229\n",
      "Epoch 4110: train loss: 0.038245052099227905\n",
      "Epoch 4120: train loss: 0.03819243237376213\n",
      "Epoch 4130: train loss: 0.03812458738684654\n",
      "Epoch 4140: train loss: 0.03805003687739372\n",
      "Epoch 4150: train loss: 0.03797488659620285\n",
      "Epoch 4160: train loss: 0.03789374604821205\n",
      "Epoch 4170: train loss: 0.03785451501607895\n",
      "Epoch 4180: train loss: 0.037763774394989014\n",
      "Epoch 4190: train loss: 0.037692852318286896\n",
      "Epoch 4200: train loss: 0.037617143243551254\n",
      "Epoch 4210: train loss: 0.03755903244018555\n",
      "Epoch 4220: train loss: 0.03748113662004471\n",
      "Epoch 4230: train loss: 0.03740601986646652\n",
      "Epoch 4240: train loss: 0.03734413534402847\n",
      "Epoch 4250: train loss: 0.037278395146131516\n",
      "Epoch 4260: train loss: 0.03721052408218384\n",
      "Epoch 4270: train loss: 0.03713501989841461\n",
      "Epoch 4280: train loss: 0.03706347197294235\n",
      "Epoch 4290: train loss: 0.03699224069714546\n",
      "Epoch 4300: train loss: 0.03692275285720825\n",
      "Epoch 4310: train loss: 0.03688471391797066\n",
      "Epoch 4320: train loss: 0.036778539419174194\n",
      "Epoch 4330: train loss: 0.036713965237140656\n",
      "Epoch 4340: train loss: 0.03663678094744682\n",
      "Epoch 4350: train loss: 0.03657010570168495\n",
      "Epoch 4360: train loss: 0.036498937755823135\n",
      "Epoch 4370: train loss: 0.03644876927137375\n",
      "Epoch 4380: train loss: 0.03641553223133087\n",
      "Epoch 4390: train loss: 0.036343980580568314\n",
      "Epoch 4400: train loss: 0.03623340651392937\n",
      "Epoch 4410: train loss: 0.036183521151542664\n",
      "Epoch 4420: train loss: 0.036094166338443756\n",
      "Epoch 4430: train loss: 0.03604365140199661\n",
      "Epoch 4440: train loss: 0.03596247360110283\n",
      "Epoch 4450: train loss: 0.035925157368183136\n",
      "Epoch 4460: train loss: 0.03583237901329994\n",
      "Epoch 4470: train loss: 0.03575938940048218\n",
      "Epoch 4480: train loss: 0.03569108247756958\n",
      "Epoch 4490: train loss: 0.035627882927656174\n",
      "Epoch 4500: train loss: 0.03558645397424698\n",
      "Epoch 4510: train loss: 0.035508159548044205\n",
      "Epoch 4520: train loss: 0.03542589023709297\n",
      "Epoch 4530: train loss: 0.035377901047468185\n",
      "Epoch 4540: train loss: 0.03533206507563591\n",
      "Epoch 4550: train loss: 0.0352359376847744\n",
      "Epoch 4560: train loss: 0.035168495029211044\n",
      "Epoch 4570: train loss: 0.035105060786008835\n",
      "Epoch 04579: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 4580: train loss: 0.03509160131216049\n",
      "Epoch 4590: train loss: 0.03498072922229767\n",
      "Epoch 4600: train loss: 0.03495590761303902\n",
      "Epoch 4610: train loss: 0.034928783774375916\n",
      "Epoch 4620: train loss: 0.034908123314380646\n",
      "Epoch 4630: train loss: 0.03488835319876671\n",
      "Epoch 4640: train loss: 0.03486606478691101\n",
      "Epoch 4650: train loss: 0.034841157495975494\n",
      "Epoch 4660: train loss: 0.034821540117263794\n",
      "Epoch 4670: train loss: 0.034800946712493896\n",
      "Epoch 4680: train loss: 0.034780099987983704\n",
      "Epoch 4690: train loss: 0.034760743379592896\n",
      "Epoch 4700: train loss: 0.03474460542201996\n",
      "Epoch 4710: train loss: 0.03471541032195091\n",
      "Epoch 4720: train loss: 0.03470327705144882\n",
      "Epoch 4730: train loss: 0.03466753289103508\n",
      "Epoch 4740: train loss: 0.034643061459064484\n",
      "Epoch 4750: train loss: 0.034622080624103546\n",
      "Epoch 4760: train loss: 0.03459188714623451\n",
      "Epoch 4770: train loss: 0.03457403928041458\n",
      "Epoch 4780: train loss: 0.03454449027776718\n",
      "Epoch 4790: train loss: 0.03452325612306595\n",
      "Epoch 4800: train loss: 0.034496136009693146\n",
      "Epoch 4810: train loss: 0.034473177045583725\n",
      "Epoch 4820: train loss: 0.03444278985261917\n",
      "Epoch 4830: train loss: 0.03441593423485756\n",
      "Epoch 4840: train loss: 0.03439223766326904\n",
      "Epoch 4850: train loss: 0.03436894714832306\n",
      "Epoch 4860: train loss: 0.0343472920358181\n",
      "Epoch 4870: train loss: 0.034323979169130325\n",
      "Epoch 4880: train loss: 0.03430385887622833\n",
      "Epoch 4890: train loss: 0.03428206965327263\n",
      "Epoch 4900: train loss: 0.034256745129823685\n",
      "Epoch 4910: train loss: 0.034238316118717194\n",
      "Epoch 4920: train loss: 0.03421584889292717\n",
      "Epoch 4930: train loss: 0.03418798744678497\n",
      "Epoch 4940: train loss: 0.034163448959589005\n",
      "Epoch 4950: train loss: 0.034140463918447495\n",
      "Epoch 4960: train loss: 0.03411861136555672\n",
      "Epoch 4970: train loss: 0.03409931808710098\n",
      "Epoch 4980: train loss: 0.034073103219270706\n",
      "Epoch 4990: train loss: 0.034050870686769485\n",
      "Epoch 5000: train loss: 0.03402424231171608\n",
      "Epoch 5010: train loss: 0.03400729224085808\n",
      "Epoch 5020: train loss: 0.03398438170552254\n",
      "Epoch 5030: train loss: 0.03395763784646988\n",
      "Epoch 5040: train loss: 0.033933740109205246\n",
      "Epoch 5050: train loss: 0.03391478955745697\n",
      "Epoch 5060: train loss: 0.03388722240924835\n",
      "Epoch 5070: train loss: 0.03386626020073891\n",
      "Epoch 5080: train loss: 0.03384426608681679\n",
      "Epoch 5090: train loss: 0.03382066264748573\n",
      "Epoch 5100: train loss: 0.03379417955875397\n",
      "Epoch 5110: train loss: 0.03377527743577957\n",
      "Epoch 5120: train loss: 0.033750299364328384\n",
      "Epoch 5130: train loss: 0.0337262824177742\n",
      "Epoch 5140: train loss: 0.03369956091046333\n",
      "Epoch 5150: train loss: 0.03367578610777855\n",
      "Epoch 5160: train loss: 0.03365841135382652\n",
      "Epoch 5170: train loss: 0.03363792970776558\n",
      "Epoch 5180: train loss: 0.03361184522509575\n",
      "Epoch 5190: train loss: 0.03358462452888489\n",
      "Epoch 5200: train loss: 0.033568281680345535\n",
      "Epoch 5210: train loss: 0.03354332223534584\n",
      "Epoch 5220: train loss: 0.033513691276311874\n",
      "Epoch 5230: train loss: 0.03348872438073158\n",
      "Epoch 5240: train loss: 0.033465828746557236\n",
      "Epoch 5250: train loss: 0.03344196826219559\n",
      "Epoch 5260: train loss: 0.03341970592737198\n",
      "Epoch 5270: train loss: 0.03339819982647896\n",
      "Epoch 5280: train loss: 0.03337555751204491\n",
      "Epoch 5290: train loss: 0.033346693962812424\n",
      "Epoch 5300: train loss: 0.03332364931702614\n",
      "Epoch 5310: train loss: 0.03330427035689354\n",
      "Epoch 5320: train loss: 0.033278241753578186\n",
      "Epoch 5330: train loss: 0.033257246017456055\n",
      "Epoch 5340: train loss: 0.03323761001229286\n",
      "Epoch 5350: train loss: 0.03320588171482086\n",
      "Epoch 5360: train loss: 0.03318411856889725\n",
      "Epoch 5370: train loss: 0.03315403312444687\n",
      "Epoch 5380: train loss: 0.03313139081001282\n",
      "Epoch 5390: train loss: 0.03310484439134598\n",
      "Epoch 5400: train loss: 0.033081479370594025\n",
      "Epoch 5410: train loss: 0.03305744007229805\n",
      "Epoch 5420: train loss: 0.03303297981619835\n",
      "Epoch 5430: train loss: 0.03300776332616806\n",
      "Epoch 5440: train loss: 0.032983243465423584\n",
      "Epoch 5450: train loss: 0.032958369702100754\n",
      "Epoch 5460: train loss: 0.03293601796030998\n",
      "Epoch 5470: train loss: 0.03290591016411781\n",
      "Epoch 5480: train loss: 0.0328810028731823\n",
      "Epoch 5490: train loss: 0.032858770340681076\n",
      "Epoch 5500: train loss: 0.03283848986029625\n",
      "Epoch 5510: train loss: 0.032807279378175735\n",
      "Epoch 5520: train loss: 0.03278883546590805\n",
      "Epoch 5530: train loss: 0.032767076045274734\n",
      "Epoch 5540: train loss: 0.03274070844054222\n",
      "Epoch 5550: train loss: 0.03271620348095894\n",
      "Epoch 5560: train loss: 0.03268793225288391\n",
      "Epoch 5570: train loss: 0.03266744315624237\n",
      "Epoch 5580: train loss: 0.032644517719745636\n",
      "Epoch 5590: train loss: 0.03261752799153328\n",
      "Epoch 5600: train loss: 0.03259475156664848\n",
      "Epoch 5610: train loss: 0.032569922506809235\n",
      "Epoch 5620: train loss: 0.032555922865867615\n",
      "Epoch 5630: train loss: 0.032522670924663544\n",
      "Epoch 5640: train loss: 0.03249194473028183\n",
      "Epoch 5650: train loss: 0.0324726402759552\n",
      "Epoch 5660: train loss: 0.03244621679186821\n",
      "Epoch 5670: train loss: 0.032427020370960236\n",
      "Epoch 5680: train loss: 0.03240232914686203\n",
      "Epoch 5690: train loss: 0.03237847238779068\n",
      "Epoch 5700: train loss: 0.032351743429899216\n",
      "Epoch 5710: train loss: 0.03232982009649277\n",
      "Epoch 5720: train loss: 0.032306838780641556\n",
      "Epoch 5730: train loss: 0.03228885680437088\n",
      "Epoch 5740: train loss: 0.03225655481219292\n",
      "Epoch 5750: train loss: 0.03223143145442009\n",
      "Epoch 5760: train loss: 0.03220488131046295\n",
      "Epoch 5770: train loss: 0.03217947483062744\n",
      "Epoch 5780: train loss: 0.032154083251953125\n",
      "Epoch 5790: train loss: 0.03213126212358475\n",
      "Epoch 5800: train loss: 0.032112929970026016\n",
      "Epoch 5810: train loss: 0.03208378329873085\n",
      "Epoch 5820: train loss: 0.03206503763794899\n",
      "Epoch 5830: train loss: 0.03203725814819336\n",
      "Epoch 5840: train loss: 0.0320172943174839\n",
      "Epoch 5850: train loss: 0.031992197036743164\n",
      "Epoch 5860: train loss: 0.03196762129664421\n",
      "Epoch 5870: train loss: 0.031945910304784775\n",
      "Epoch 5880: train loss: 0.031912777572870255\n",
      "Epoch 5890: train loss: 0.031892918050289154\n",
      "Epoch 5900: train loss: 0.03186856210231781\n",
      "Epoch 5910: train loss: 0.03184890374541283\n",
      "Epoch 5920: train loss: 0.031827036291360855\n",
      "Epoch 5930: train loss: 0.03180037811398506\n",
      "Epoch 5940: train loss: 0.0317743644118309\n",
      "Epoch 5950: train loss: 0.03174548223614693\n",
      "Epoch 5960: train loss: 0.03171742707490921\n",
      "Epoch 5970: train loss: 0.031693458557128906\n",
      "Epoch 5980: train loss: 0.03167644515633583\n",
      "Epoch 5990: train loss: 0.03165436163544655\n",
      "Epoch 6000: train loss: 0.03162330389022827\n",
      "Epoch 6010: train loss: 0.031601596623659134\n",
      "Epoch 6020: train loss: 0.03157561272382736\n",
      "Epoch 6030: train loss: 0.03155070170760155\n",
      "Epoch 6040: train loss: 0.0315200611948967\n",
      "Epoch 6050: train loss: 0.03149636089801788\n",
      "Epoch 6060: train loss: 0.03147457540035248\n",
      "Epoch 6070: train loss: 0.03144478425383568\n",
      "Epoch 6080: train loss: 0.03142193332314491\n",
      "Epoch 6090: train loss: 0.031394995748996735\n",
      "Epoch 6100: train loss: 0.03137970715761185\n",
      "Epoch 6110: train loss: 0.031354453414678574\n",
      "Epoch 6120: train loss: 0.0313180573284626\n",
      "Epoch 6130: train loss: 0.03129547834396362\n",
      "Epoch 6140: train loss: 0.03126776963472366\n",
      "Epoch 6150: train loss: 0.03124953620135784\n",
      "Epoch 6160: train loss: 0.031215276569128036\n",
      "Epoch 6170: train loss: 0.03119543567299843\n",
      "Epoch 6180: train loss: 0.03116430900990963\n",
      "Epoch 6190: train loss: 0.031142303720116615\n",
      "Epoch 6200: train loss: 0.03111669421195984\n",
      "Epoch 6210: train loss: 0.031086573377251625\n",
      "Epoch 6220: train loss: 0.031060442328453064\n",
      "Epoch 6230: train loss: 0.031037716194987297\n",
      "Epoch 6240: train loss: 0.031008996069431305\n",
      "Epoch 6250: train loss: 0.030988119542598724\n",
      "Epoch 6260: train loss: 0.030957907438278198\n",
      "Epoch 6270: train loss: 0.03092658519744873\n",
      "Epoch 6280: train loss: 0.030894476920366287\n",
      "Epoch 6290: train loss: 0.03087274543941021\n",
      "Epoch 6300: train loss: 0.03083939477801323\n",
      "Epoch 6310: train loss: 0.030813783407211304\n",
      "Epoch 6320: train loss: 0.030786149203777313\n",
      "Epoch 6330: train loss: 0.03075585514307022\n",
      "Epoch 6340: train loss: 0.03073306754231453\n",
      "Epoch 6350: train loss: 0.03070337139070034\n",
      "Epoch 6360: train loss: 0.030682358890771866\n",
      "Epoch 6370: train loss: 0.030654972419142723\n",
      "Epoch 6380: train loss: 0.030624035745859146\n",
      "Epoch 6390: train loss: 0.030599774792790413\n",
      "Epoch 6400: train loss: 0.030568286776542664\n",
      "Epoch 6410: train loss: 0.030547723174095154\n",
      "Epoch 6420: train loss: 0.030515288934111595\n",
      "Epoch 6430: train loss: 0.030482109636068344\n",
      "Epoch 6440: train loss: 0.030451776459813118\n",
      "Epoch 6450: train loss: 0.030430210754275322\n",
      "Epoch 6460: train loss: 0.03039734810590744\n",
      "Epoch 6470: train loss: 0.030369609594345093\n",
      "Epoch 6480: train loss: 0.03034469112753868\n",
      "Epoch 6490: train loss: 0.030324557796120644\n",
      "Epoch 6500: train loss: 0.03029768355190754\n",
      "Epoch 6510: train loss: 0.03027241863310337\n",
      "Epoch 6520: train loss: 0.03025604970753193\n",
      "Epoch 6530: train loss: 0.030212201178073883\n",
      "Epoch 6540: train loss: 0.030190162360668182\n",
      "Epoch 6550: train loss: 0.03016643412411213\n",
      "Epoch 6560: train loss: 0.030142175033688545\n",
      "Epoch 6570: train loss: 0.030108056962490082\n",
      "Epoch 6580: train loss: 0.03008672595024109\n",
      "Epoch 6590: train loss: 0.03005594201385975\n",
      "Epoch 6600: train loss: 0.030030814930796623\n",
      "Epoch 6610: train loss: 0.0300065316259861\n",
      "Epoch 6620: train loss: 0.02998252399265766\n",
      "Epoch 6630: train loss: 0.029962832108139992\n",
      "Epoch 6640: train loss: 0.029934989288449287\n",
      "Epoch 6650: train loss: 0.02990945801138878\n",
      "Epoch 6660: train loss: 0.029879000037908554\n",
      "Epoch 6670: train loss: 0.029848305508494377\n",
      "Epoch 6680: train loss: 0.029748691245913506\n",
      "Epoch 6690: train loss: 0.029725050553679466\n",
      "Epoch 6700: train loss: 0.029679972678422928\n",
      "Epoch 6710: train loss: 0.029648184776306152\n",
      "Epoch 6720: train loss: 0.02961319126188755\n",
      "Epoch 6730: train loss: 0.029574330896139145\n",
      "Epoch 6740: train loss: 0.029537515714764595\n",
      "Epoch 6750: train loss: 0.029500341042876244\n",
      "Epoch 6760: train loss: 0.02946239896118641\n",
      "Epoch 6770: train loss: 0.029428662732243538\n",
      "Epoch 6780: train loss: 0.029395971447229385\n",
      "Epoch 6790: train loss: 0.029362086206674576\n",
      "Epoch 6800: train loss: 0.02932067960500717\n",
      "Epoch 6810: train loss: 0.02929074876010418\n",
      "Epoch 6820: train loss: 0.02925257943570614\n",
      "Epoch 6830: train loss: 0.02921687625348568\n",
      "Epoch 6840: train loss: 0.02918749302625656\n",
      "Epoch 6850: train loss: 0.029151836410164833\n",
      "Epoch 6860: train loss: 0.029119519516825676\n",
      "Epoch 6870: train loss: 0.02908450923860073\n",
      "Epoch 6880: train loss: 0.02904978208243847\n",
      "Epoch 6890: train loss: 0.029017308726906776\n",
      "Epoch 6900: train loss: 0.028988901525735855\n",
      "Epoch 6910: train loss: 0.0289579089730978\n",
      "Epoch 6920: train loss: 0.028919698670506477\n",
      "Epoch 6930: train loss: 0.028887880966067314\n",
      "Epoch 6940: train loss: 0.028863420709967613\n",
      "Epoch 6950: train loss: 0.028703419491648674\n",
      "Epoch 06958: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 6960: train loss: 0.0286572203040123\n",
      "Epoch 6970: train loss: 0.028616564348340034\n",
      "Epoch 6980: train loss: 0.028589634224772453\n",
      "Epoch 06983: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 6990: train loss: 0.028571659699082375\n",
      "Epoch 06996: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 7000: train loss: 0.028569718822836876\n",
      "Epoch 07002: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 7010: train loss: 0.02856825292110443\n",
      "Epoch 07012: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 07018: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 7020: train loss: 0.028567714616656303\n",
      "Epoch 07024: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 7024, loss: 0.02856764756143093\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
