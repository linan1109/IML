{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1)\n",
    "        \n",
    "        self.nomal1 = nn.BatchNorm1d(1000)\n",
    "        self.nomal2 = nn.BatchNorm1d(1000)\n",
    "        self.nomal3 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = nn.LeakyReLU(0.01)(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.nomal1(x)\n",
    "        x = nn.LeakyReLU(0.01)(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.nomal2(x)\n",
    "        x = nn.LeakyReLU(0.01)(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.nomal3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = nn.LeakyReLU(0.01)(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.nomal1(x)\n",
    "        x = nn.LeakyReLU(0.01)(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.nomal2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.nomal3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.4, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 200\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc1 = nn.Linear(1000, 100)\n",
    "            self.fc2 = nn.Linear(100, 1)\n",
    "            \n",
    "            self.nomal1 = nn.BatchNorm1d(100)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc1(x)\n",
    "            x = nn.LeakyReLU(0.01)(x)\n",
    "            x = nn.Dropout(0.6)(x)\n",
    "            x = self.nomal1(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=10, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "    \n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.4, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr = loss.item()\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val = loss.item()\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-8):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-upgrade.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a38b77889c44be89163d2a25d57237a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 3.480101949302518, val loss: 0.928158323764801\n",
      "Epoch 2: train loss: 0.6245866422555885, val loss: 0.42885990715026856\n",
      "Epoch 3: train loss: 0.3180882890224457, val loss: 0.3188017817735672\n",
      "Epoch 4: train loss: 0.19367666848460022, val loss: 0.16699293076992036\n",
      "Epoch 5: train loss: 0.12112748398342911, val loss: 0.1035601778626442\n",
      "Epoch 6: train loss: 0.07294600957996991, val loss: 0.055067546904087064\n",
      "Epoch 7: train loss: 0.0466218267594065, val loss: 0.03991509318351746\n",
      "Epoch 8: train loss: 0.03237390867727143, val loss: 0.03167793908715248\n",
      "Epoch 9: train loss: 0.02492805113871487, val loss: 0.0236653855741024\n",
      "Epoch 10: train loss: 0.020639013133486924, val loss: 0.018734809696674348\n",
      "Epoch 11: train loss: 0.01838031056979481, val loss: 0.02046343372762203\n",
      "Epoch 12: train loss: 0.016749416017258652, val loss: 0.017000468522310255\n",
      "Epoch 13: train loss: 0.015639829860962167, val loss: 0.01476034316420555\n",
      "Epoch 14: train loss: 0.014392104922964865, val loss: 0.014746605694293976\n",
      "Epoch 15: train loss: 0.013961675506161184, val loss: 0.013571087367832661\n",
      "Epoch 16: train loss: 0.013640719950807338, val loss: 0.014343340195715428\n",
      "Epoch 17: train loss: 0.013096051455152278, val loss: 0.01217966404557228\n",
      "Epoch 18: train loss: 0.012363406339470221, val loss: 0.012595026470720768\n",
      "Epoch 19: train loss: 0.012467184220953864, val loss: 0.010816617526113986\n",
      "Epoch 20: train loss: 0.011700251929918115, val loss: 0.0120211543738842\n",
      "Epoch 21: train loss: 0.011687267420243244, val loss: 0.013651325851678848\n",
      "Epoch 22: train loss: 0.011046541399645561, val loss: 0.010271491512656212\n",
      "Epoch 23: train loss: 0.009843968877378775, val loss: 0.012641806691884994\n",
      "Epoch 24: train loss: 0.010164591090259504, val loss: 0.011858515188097954\n",
      "Epoch 25: train loss: 0.009587242991872589, val loss: 0.01203469754755497\n",
      "Epoch 26: train loss: 0.010076122510798124, val loss: 0.014489679291844368\n",
      "Epoch 27: train loss: 0.009047947914168544, val loss: 0.013127404391765594\n",
      "Epoch 00028: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 28: train loss: 0.00852826999127865, val loss: 0.016997936964035034\n",
      "Epoch 29: train loss: 0.007103492365275719, val loss: 0.009422563463449479\n",
      "Epoch 30: train loss: 0.0070791814848020366, val loss: 0.008530580397695303\n",
      "Epoch 31: train loss: 0.006439055330139033, val loss: 0.008416415899991989\n",
      "Epoch 32: train loss: 0.006393981980578023, val loss: 0.0074475110024213795\n",
      "Epoch 33: train loss: 0.006469368154525148, val loss: 0.006642725117504597\n",
      "Epoch 34: train loss: 0.006072043870480693, val loss: 0.007676897831261158\n",
      "Epoch 35: train loss: 0.006023746419698, val loss: 0.0074823774956166746\n",
      "Epoch 36: train loss: 0.005602387750787394, val loss: 0.008762709967792034\n",
      "Epoch 37: train loss: 0.006124024403490583, val loss: 0.007608903743326664\n",
      "Epoch 38: train loss: 0.005792718334161505, val loss: 0.008025277085602283\n",
      "Epoch 00039: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 39: train loss: 0.005729011363216809, val loss: 0.007087738897651434\n",
      "Epoch 40: train loss: 0.004983780073876284, val loss: 0.006137811336666346\n",
      "Epoch 41: train loss: 0.00483448533394507, val loss: 0.0060440987721085545\n",
      "Epoch 42: train loss: 0.00469646805296747, val loss: 0.0070841524563729765\n",
      "Epoch 43: train loss: 0.00457749407555984, val loss: 0.006540280379354954\n",
      "Epoch 44: train loss: 0.004569893738292918, val loss: 0.005626126404851675\n",
      "Epoch 45: train loss: 0.004452441910699922, val loss: 0.006300219550728798\n",
      "Epoch 46: train loss: 0.004576814957191142, val loss: 0.006360160812735558\n",
      "Epoch 47: train loss: 0.004436575695720254, val loss: 0.005835709322243929\n",
      "Epoch 48: train loss: 0.0044209232616181275, val loss: 0.006088543701916933\n",
      "Epoch 49: train loss: 0.004262527799499887, val loss: 0.005866321310400963\n",
      "Epoch 00050: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 50: train loss: 0.004350178399560403, val loss: 0.007040960032492876\n",
      "Epoch 51: train loss: 0.0039051655366712686, val loss: 0.0054519855193793775\n",
      "Epoch 52: train loss: 0.0038269952821488283, val loss: 0.005436697218567133\n",
      "Epoch 53: train loss: 0.0038318354149101948, val loss: 0.005338414069265127\n",
      "Epoch 54: train loss: 0.0037493139446085812, val loss: 0.005251578222960233\n",
      "Epoch 55: train loss: 0.0039532803733434, val loss: 0.0050454728342592715\n",
      "Epoch 56: train loss: 0.003875041684021755, val loss: 0.005086987871676683\n",
      "Epoch 57: train loss: 0.003676127443584252, val loss: 0.005628908779472113\n",
      "Epoch 58: train loss: 0.003824480521131535, val loss: 0.004539718795567751\n",
      "Epoch 59: train loss: 0.003620501208138101, val loss: 0.005499705046415329\n",
      "Epoch 60: train loss: 0.0036159493830055, val loss: 0.006348051358014345\n",
      "Epoch 61: train loss: 0.0035407289657361654, val loss: 0.0051269115880131725\n",
      "Epoch 62: train loss: 0.0035453800986692007, val loss: 0.004901026405394077\n",
      "Epoch 63: train loss: 0.0034671989515119668, val loss: 0.006370246607810259\n",
      "Epoch 00064: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 64: train loss: 0.0035523097130412957, val loss: 0.004540134847164154\n",
      "Epoch 65: train loss: 0.0034119175918856447, val loss: 0.005248637523502111\n",
      "Epoch 66: train loss: 0.0032899211310610478, val loss: 0.005786878075450658\n",
      "Epoch 67: train loss: 0.0032257986797818118, val loss: 0.005457678783684969\n",
      "Epoch 68: train loss: 0.0032988473496266776, val loss: 0.004954121220856905\n",
      "Epoch 69: train loss: 0.0033421967014366266, val loss: 0.004678049191832542\n",
      "Epoch 00070: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 70: train loss: 0.003325306767711834, val loss: 0.005795295156538487\n",
      "Epoch 71: train loss: 0.003308361328263976, val loss: 0.005141457423567772\n",
      "Epoch 72: train loss: 0.0031861380343230403, val loss: 0.005277845654636622\n",
      "Epoch 73: train loss: 0.0031373674129421005, val loss: 0.005699196718633175\n",
      "Epoch 74: train loss: 0.003203520185219086, val loss: 0.0048610099032521245\n",
      "Epoch 75: train loss: 0.0032462834572533563, val loss: 0.005600350633263588\n",
      "Epoch 00076: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 76: train loss: 0.003193874365707137, val loss: 0.004823734041303396\n",
      "Early stop at epoch 76\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ad75067d6344bbb03df0a133d880ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 6.150294303894043, val loss: 5.138032913208008\n",
      "Epoch 2: train loss: 4.456811428070068, val loss: 3.914276123046875\n",
      "Epoch 3: train loss: 3.407897472381592, val loss: 2.6705219745635986\n",
      "Epoch 4: train loss: 3.0515992641448975, val loss: 2.6292550563812256\n",
      "Epoch 5: train loss: 3.0407309532165527, val loss: 2.6841471195220947\n",
      "Epoch 6: train loss: 2.4987878799438477, val loss: 2.041243076324463\n",
      "Epoch 7: train loss: 2.0317957401275635, val loss: 2.7088139057159424\n",
      "Epoch 8: train loss: 2.040835380554199, val loss: 2.1694347858428955\n",
      "Epoch 9: train loss: 2.129324436187744, val loss: 2.4382879734039307\n",
      "Epoch 10: train loss: 1.7219696044921875, val loss: 1.3037515878677368\n",
      "Epoch 11: train loss: 1.5998972654342651, val loss: 0.9132499098777771\n",
      "Epoch 12: train loss: 1.737145185470581, val loss: 1.3306528329849243\n",
      "Epoch 13: train loss: 1.5977648496627808, val loss: 2.138814687728882\n",
      "Epoch 14: train loss: 1.1486908197402954, val loss: 1.628670334815979\n",
      "Epoch 15: train loss: 1.0953648090362549, val loss: 0.9478117227554321\n",
      "Epoch 16: train loss: 1.343849539756775, val loss: 1.3836930990219116\n",
      "Epoch 17: train loss: 1.0547174215316772, val loss: 0.8531884551048279\n",
      "Epoch 18: train loss: 0.799841582775116, val loss: 0.9811305403709412\n",
      "Epoch 19: train loss: 0.9386355876922607, val loss: 0.593447208404541\n",
      "Epoch 20: train loss: 1.0886857509613037, val loss: 1.2095310688018799\n",
      "Epoch 21: train loss: 0.8542772531509399, val loss: 1.0053024291992188\n",
      "Epoch 22: train loss: 0.7761526703834534, val loss: 0.4313008487224579\n",
      "Epoch 23: train loss: 0.9787263870239258, val loss: 1.786028265953064\n",
      "Epoch 24: train loss: 1.1706348657608032, val loss: 0.6005922555923462\n",
      "Epoch 25: train loss: 0.7230015993118286, val loss: 1.10521399974823\n",
      "Epoch 26: train loss: 0.6762586832046509, val loss: 0.6146420836448669\n",
      "Epoch 27: train loss: 0.8388609290122986, val loss: 0.47090402245521545\n",
      "Epoch 28: train loss: 0.6989553570747375, val loss: 0.6791372895240784\n",
      "Epoch 29: train loss: 0.7451510429382324, val loss: 0.7110385894775391\n",
      "Epoch 30: train loss: 0.6398413777351379, val loss: 0.3893176019191742\n",
      "Epoch 31: train loss: 0.6863560080528259, val loss: 1.0663931369781494\n",
      "Epoch 32: train loss: 0.6379185914993286, val loss: 0.5156218409538269\n",
      "Epoch 33: train loss: 0.5457218885421753, val loss: 0.6265224814414978\n",
      "Epoch 34: train loss: 0.524294376373291, val loss: 0.26120275259017944\n",
      "Epoch 35: train loss: 0.4672984778881073, val loss: 0.48605281114578247\n",
      "Epoch 36: train loss: 0.8836948275566101, val loss: 0.6300618648529053\n",
      "Epoch 37: train loss: 0.4315564036369324, val loss: 0.5716304183006287\n",
      "Epoch 38: train loss: 0.6364439129829407, val loss: 0.6277187466621399\n",
      "Epoch 39: train loss: 0.6990942358970642, val loss: 0.45106419920921326\n",
      "Epoch 40: train loss: 0.47707635164260864, val loss: 0.844542920589447\n",
      "Epoch 41: train loss: 0.6467686295509338, val loss: 0.39793452620506287\n",
      "Epoch 42: train loss: 0.4797390401363373, val loss: 0.19913776218891144\n",
      "Epoch 43: train loss: 0.617285966873169, val loss: 0.38158437609672546\n",
      "Epoch 44: train loss: 0.44540590047836304, val loss: 0.48322397470474243\n",
      "Epoch 45: train loss: 0.4365268349647522, val loss: 0.9051352739334106\n",
      "Epoch 46: train loss: 0.3608066737651825, val loss: 0.5752847194671631\n",
      "Epoch 47: train loss: 0.5686137080192566, val loss: 0.737700879573822\n",
      "Epoch 48: train loss: 0.5648804306983948, val loss: 0.4738022983074188\n",
      "Epoch 49: train loss: 0.5161729454994202, val loss: 0.19965068995952606\n",
      "Epoch 50: train loss: 0.5409112572669983, val loss: 0.6065438389778137\n",
      "Epoch 51: train loss: 0.5032402873039246, val loss: 0.391973614692688\n",
      "Epoch 52: train loss: 0.5062145590782166, val loss: 0.4156525731086731\n",
      "Epoch 53: train loss: 0.4720070958137512, val loss: 0.36344724893569946\n",
      "Epoch 54: train loss: 0.40177592635154724, val loss: 0.20274825394153595\n",
      "Epoch 55: train loss: 0.3012213408946991, val loss: 0.34260573983192444\n",
      "Epoch 56: train loss: 0.3948284983634949, val loss: 0.4289562404155731\n",
      "Epoch 57: train loss: 0.408770889043808, val loss: 0.47926220297813416\n",
      "Epoch 58: train loss: 0.3849860429763794, val loss: 0.20637468993663788\n",
      "Epoch 59: train loss: 0.22112058103084564, val loss: 0.5529016852378845\n",
      "Epoch 60: train loss: 0.2564026713371277, val loss: 0.13922321796417236\n",
      "Epoch 61: train loss: 0.3280496895313263, val loss: 0.24077430367469788\n",
      "Epoch 62: train loss: 0.2664034068584442, val loss: 0.26351213455200195\n",
      "Epoch 63: train loss: 0.31232190132141113, val loss: 0.22394190728664398\n",
      "Epoch 64: train loss: 0.3798556923866272, val loss: 0.14810574054718018\n",
      "Epoch 65: train loss: 0.3993235230445862, val loss: 0.28286248445510864\n",
      "Epoch 66: train loss: 0.47882506251335144, val loss: 0.2762978971004486\n",
      "Epoch 67: train loss: 0.2515428364276886, val loss: 0.20593085885047913\n",
      "Epoch 68: train loss: 0.3525283634662628, val loss: 0.16097061336040497\n",
      "Epoch 69: train loss: 0.3617512583732605, val loss: 0.4073708653450012\n",
      "Epoch 70: train loss: 0.303372323513031, val loss: 0.2507933974266052\n",
      "Epoch 71: train loss: 0.22734040021896362, val loss: 0.24619488418102264\n",
      "Epoch 72: train loss: 0.32709866762161255, val loss: 0.38204437494277954\n",
      "Epoch 73: train loss: 0.27810022234916687, val loss: 0.3045765161514282\n",
      "Epoch 74: train loss: 0.34841257333755493, val loss: 0.1654377430677414\n",
      "Epoch 75: train loss: 0.2916426360607147, val loss: 0.48325395584106445\n",
      "Epoch 76: train loss: 0.27223634719848633, val loss: 0.2143702358007431\n",
      "Epoch 77: train loss: 0.18231426179409027, val loss: 0.3276605010032654\n",
      "Epoch 78: train loss: 0.33448657393455505, val loss: 0.34891220927238464\n",
      "Epoch 79: train loss: 0.22828207910060883, val loss: 0.3851633667945862\n",
      "Epoch 80: train loss: 0.26491501927375793, val loss: 0.2658853232860565\n",
      "Epoch 81: train loss: 0.19849754869937897, val loss: 0.23115339875221252\n",
      "Epoch 82: train loss: 0.23965416848659515, val loss: 0.153424933552742\n",
      "Epoch 83: train loss: 0.24743245542049408, val loss: 0.4098111689090729\n",
      "Epoch 84: train loss: 0.2474556416273117, val loss: 0.5444676280021667\n",
      "Epoch 85: train loss: 0.21725744009017944, val loss: 0.42454347014427185\n",
      "Epoch 86: train loss: 0.23209553956985474, val loss: 0.16828127205371857\n",
      "Epoch 87: train loss: 0.21305158734321594, val loss: 0.27468425035476685\n",
      "Epoch 88: train loss: 0.16835446655750275, val loss: 0.15815794467926025\n",
      "Epoch 89: train loss: 0.19036975502967834, val loss: 0.3278273642063141\n",
      "Epoch 90: train loss: 0.18509621918201447, val loss: 0.1745336353778839\n",
      "Epoch 91: train loss: 0.2240423858165741, val loss: 0.3795911371707916\n",
      "Epoch 92: train loss: 0.14354628324508667, val loss: 0.33619585633277893\n",
      "Epoch 93: train loss: 0.21078157424926758, val loss: 0.1968727558851242\n",
      "Epoch 94: train loss: 0.2265366017818451, val loss: 0.2713277041912079\n",
      "Epoch 95: train loss: 0.1858796626329422, val loss: 0.6921277046203613\n",
      "Epoch 96: train loss: 0.21032817661762238, val loss: 0.276409387588501\n",
      "Epoch 97: train loss: 0.19645114243030548, val loss: 0.23960308730602264\n",
      "Epoch 98: train loss: 0.20777378976345062, val loss: 0.19662223756313324\n",
      "Epoch 99: train loss: 0.1980232298374176, val loss: 0.19915257394313812\n",
      "Epoch 100: train loss: 0.18307538330554962, val loss: 0.21007151901721954\n",
      "Epoch 101: train loss: 0.18145696818828583, val loss: 0.20639291405677795\n",
      "Epoch 102: train loss: 0.21499201655387878, val loss: 0.2784425616264343\n",
      "Epoch 103: train loss: 0.18737176060676575, val loss: 0.15104106068611145\n",
      "Epoch 104: train loss: 0.15886545181274414, val loss: 0.18535156548023224\n",
      "Epoch 105: train loss: 0.14751294255256653, val loss: 0.1569100320339203\n",
      "Epoch 106: train loss: 0.2092568576335907, val loss: 0.374157190322876\n",
      "Epoch 107: train loss: 0.1748300939798355, val loss: 0.22102618217468262\n",
      "Epoch 108: train loss: 0.1562148928642273, val loss: 0.24936366081237793\n",
      "Epoch 109: train loss: 0.1449284702539444, val loss: 0.38319796323776245\n",
      "Epoch 110: train loss: 0.1045423224568367, val loss: 0.22191046178340912\n",
      "Epoch 111: train loss: 0.13089294731616974, val loss: 0.181865856051445\n",
      "Epoch 112: train loss: 0.22377118468284607, val loss: 0.23683153092861176\n",
      "Epoch 113: train loss: 0.24702000617980957, val loss: 0.07539927959442139\n",
      "Epoch 114: train loss: 0.15451419353485107, val loss: 0.14651353657245636\n",
      "Epoch 115: train loss: 0.15124204754829407, val loss: 0.24617700278759003\n",
      "Epoch 116: train loss: 0.18599213659763336, val loss: 0.22803619503974915\n",
      "Epoch 117: train loss: 0.15646305680274963, val loss: 0.1775265485048294\n",
      "Epoch 118: train loss: 0.1471318155527115, val loss: 0.4076741337776184\n",
      "Epoch 119: train loss: 0.1465347707271576, val loss: 0.15468327701091766\n",
      "Epoch 120: train loss: 0.18404965102672577, val loss: 0.10729598253965378\n",
      "Epoch 121: train loss: 0.14496101438999176, val loss: 0.21786943078041077\n",
      "Epoch 122: train loss: 0.1454971581697464, val loss: 0.11137552559375763\n",
      "Epoch 123: train loss: 0.153423011302948, val loss: 0.2427365630865097\n",
      "Epoch 124: train loss: 0.10556723177433014, val loss: 0.07747621834278107\n",
      "Epoch 125: train loss: 0.12761425971984863, val loss: 0.111496202647686\n",
      "Epoch 126: train loss: 0.12255143374204636, val loss: 0.18624328076839447\n",
      "Epoch 127: train loss: 0.12637904286384583, val loss: 0.14966069161891937\n",
      "Epoch 128: train loss: 0.11261893808841705, val loss: 0.10260558128356934\n",
      "Epoch 129: train loss: 0.14021123945713043, val loss: 0.17659598588943481\n",
      "Epoch 130: train loss: 0.13715794682502747, val loss: 0.1840657889842987\n",
      "Epoch 131: train loss: 0.11642749607563019, val loss: 0.15286779403686523\n",
      "Epoch 132: train loss: 0.092061847448349, val loss: 0.15332069993019104\n",
      "Epoch 133: train loss: 0.10134599357843399, val loss: 0.19488953053951263\n",
      "Epoch 134: train loss: 0.1248248964548111, val loss: 0.1074531078338623\n",
      "Epoch 135: train loss: 0.11630421131849289, val loss: 0.35711726546287537\n",
      "Epoch 136: train loss: 0.11776218563318253, val loss: 0.0836583599448204\n",
      "Epoch 137: train loss: 0.1398961842060089, val loss: 0.12074365466833115\n",
      "Epoch 138: train loss: 0.12969782948493958, val loss: 0.19419841468334198\n",
      "Epoch 139: train loss: 0.11945398151874542, val loss: 0.20417849719524384\n",
      "Epoch 140: train loss: 0.12657201290130615, val loss: 0.2034677267074585\n",
      "Epoch 141: train loss: 0.11355587840080261, val loss: 0.11559700220823288\n",
      "Epoch 142: train loss: 0.14558495581150055, val loss: 0.1207372322678566\n",
      "Epoch 143: train loss: 0.09644493460655212, val loss: 0.08789675682783127\n",
      "Epoch 144: train loss: 0.10394781082868576, val loss: 0.12292778491973877\n",
      "Epoch 145: train loss: 0.12556485831737518, val loss: 0.13130661845207214\n",
      "Epoch 146: train loss: 0.14195960760116577, val loss: 0.11371999233961105\n",
      "Epoch 147: train loss: 0.1088501438498497, val loss: 0.08102726191282272\n",
      "Epoch 148: train loss: 0.10351403802633286, val loss: 0.1321093887090683\n",
      "Epoch 149: train loss: 0.11743710935115814, val loss: 0.3326817750930786\n",
      "Epoch 150: train loss: 0.096349336206913, val loss: 0.14576341211795807\n",
      "Epoch 151: train loss: 0.16395039856433868, val loss: 0.21336530148983002\n",
      "Epoch 152: train loss: 0.10441415756940842, val loss: 0.15065430104732513\n",
      "Epoch 153: train loss: 0.11549122631549835, val loss: 0.08807028084993362\n",
      "Epoch 154: train loss: 0.1162632629275322, val loss: 0.09764943271875381\n",
      "Epoch 155: train loss: 0.11409959942102432, val loss: 0.12051410973072052\n",
      "Epoch 156: train loss: 0.1140584871172905, val loss: 0.08065885305404663\n",
      "Epoch 157: train loss: 0.11036420613527298, val loss: 0.07698845863342285\n",
      "Epoch 158: train loss: 0.14894713461399078, val loss: 0.13092052936553955\n",
      "Epoch 159: train loss: 0.11254458874464035, val loss: 0.11129868030548096\n",
      "Epoch 160: train loss: 0.10812999308109283, val loss: 0.17402829229831696\n",
      "Epoch 161: train loss: 0.10159186273813248, val loss: 0.19947181642055511\n",
      "Epoch 162: train loss: 0.0837576687335968, val loss: 0.16850411891937256\n",
      "Epoch 163: train loss: 0.10168900340795517, val loss: 0.09475834667682648\n",
      "Epoch 164: train loss: 0.10602010041475296, val loss: 0.14933982491493225\n",
      "Epoch 165: train loss: 0.11083237826824188, val loss: 0.19065812230110168\n",
      "Epoch 166: train loss: 0.10865757614374161, val loss: 0.1498171091079712\n",
      "Epoch 167: train loss: 0.09811056405305862, val loss: 0.08724834024906158\n",
      "Epoch 168: train loss: 0.11901741474866867, val loss: 0.19320134818553925\n",
      "Epoch 169: train loss: 0.08647962659597397, val loss: 0.15524566173553467\n",
      "Epoch 170: train loss: 0.11915263533592224, val loss: 0.14297251403331757\n",
      "Epoch 171: train loss: 0.0869273766875267, val loss: 0.12266390770673752\n",
      "Epoch 172: train loss: 0.09783031046390533, val loss: 0.2048175185918808\n",
      "Epoch 173: train loss: 0.11593297868967056, val loss: 0.14948952198028564\n",
      "Epoch 174: train loss: 0.08325079083442688, val loss: 0.06747578084468842\n",
      "Epoch 175: train loss: 0.09347443282604218, val loss: 0.12636736035346985\n",
      "Epoch 176: train loss: 0.10171869397163391, val loss: 0.16965678334236145\n",
      "Epoch 177: train loss: 0.0894666463136673, val loss: 0.20083259046077728\n",
      "Epoch 178: train loss: 0.10675226897001266, val loss: 0.15013618767261505\n",
      "Epoch 179: train loss: 0.11182671040296555, val loss: 0.14917369186878204\n",
      "Epoch 180: train loss: 0.08875579386949539, val loss: 0.11290673166513443\n",
      "Epoch 181: train loss: 0.08841673284769058, val loss: 0.15003672242164612\n",
      "Epoch 182: train loss: 0.09540695697069168, val loss: 0.12317445129156113\n",
      "Epoch 183: train loss: 0.09264689683914185, val loss: 0.09466133266687393\n",
      "Epoch 184: train loss: 0.10787657648324966, val loss: 0.09914608299732208\n",
      "Epoch 185: train loss: 0.09230605512857437, val loss: 0.09092976897954941\n",
      "Epoch 186: train loss: 0.097007617354393, val loss: 0.12099113315343857\n",
      "Epoch 187: train loss: 0.07506589591503143, val loss: 0.187821164727211\n",
      "Epoch 188: train loss: 0.07142259925603867, val loss: 0.14511823654174805\n",
      "Epoch 189: train loss: 0.09946707636117935, val loss: 0.18914957344532013\n",
      "Epoch 190: train loss: 0.0892263874411583, val loss: 0.09836231917142868\n",
      "Epoch 191: train loss: 0.0813346579670906, val loss: 0.12883572280406952\n",
      "Epoch 192: train loss: 0.08166225999593735, val loss: 0.10510437935590744\n",
      "Epoch 193: train loss: 0.0773995965719223, val loss: 0.1147695928812027\n",
      "Epoch 194: train loss: 0.09348125755786896, val loss: 0.20035724341869354\n",
      "Epoch 195: train loss: 0.0847005844116211, val loss: 0.13956062495708466\n",
      "Epoch 196: train loss: 0.07581091672182083, val loss: 0.1258329153060913\n",
      "Epoch 197: train loss: 0.08527256548404694, val loss: 0.11879930645227432\n",
      "Epoch 198: train loss: 0.08939225971698761, val loss: 0.09959429502487183\n",
      "Epoch 199: train loss: 0.08712481707334518, val loss: 0.11885479837656021\n",
      "Epoch 200: train loss: 0.10366915911436081, val loss: 0.11716447025537491\n",
      "Epoch 201: train loss: 0.10303425788879395, val loss: 0.13216061890125275\n",
      "Epoch 202: train loss: 0.07075190544128418, val loss: 0.10190623998641968\n",
      "Epoch 203: train loss: 0.06977775692939758, val loss: 0.12567676603794098\n",
      "Epoch 204: train loss: 0.0752909779548645, val loss: 0.14815793931484222\n",
      "Epoch 205: train loss: 0.08480515331029892, val loss: 0.1188487783074379\n",
      "Epoch 206: train loss: 0.07522112876176834, val loss: 0.12598273158073425\n",
      "Epoch 207: train loss: 0.06798100471496582, val loss: 0.08171844482421875\n",
      "Epoch 208: train loss: 0.08703583478927612, val loss: 0.08210377395153046\n",
      "Epoch 209: train loss: 0.08154800534248352, val loss: 0.17895767092704773\n",
      "Epoch 210: train loss: 0.07976116985082626, val loss: 0.09473221749067307\n",
      "Epoch 211: train loss: 0.08413635939359665, val loss: 0.13549137115478516\n",
      "Epoch 212: train loss: 0.07506416738033295, val loss: 0.12123256176710129\n",
      "Epoch 213: train loss: 0.07715079933404922, val loss: 0.1771748661994934\n",
      "Epoch 214: train loss: 0.07939441502094269, val loss: 0.09107454121112823\n",
      "Epoch 215: train loss: 0.08303332328796387, val loss: 0.06911575049161911\n",
      "Epoch 216: train loss: 0.08384748548269272, val loss: 0.10502555221319199\n",
      "Epoch 217: train loss: 0.08278421312570572, val loss: 0.11729300022125244\n",
      "Epoch 218: train loss: 0.06455032527446747, val loss: 0.1298084259033203\n",
      "Epoch 219: train loss: 0.07686012983322144, val loss: 0.1507430374622345\n",
      "Epoch 220: train loss: 0.07629851251840591, val loss: 0.11411561816930771\n",
      "Epoch 221: train loss: 0.07525917887687683, val loss: 0.13875217735767365\n",
      "Epoch 222: train loss: 0.07393508404493332, val loss: 0.12402861565351486\n",
      "Epoch 223: train loss: 0.06924232840538025, val loss: 0.08662613481283188\n",
      "Epoch 224: train loss: 0.08104703575372696, val loss: 0.13077476620674133\n",
      "Epoch 225: train loss: 0.0775657594203949, val loss: 0.0933750793337822\n",
      "Epoch 226: train loss: 0.0843578577041626, val loss: 0.11594414710998535\n",
      "Epoch 227: train loss: 0.06492147594690323, val loss: 0.1044546589255333\n",
      "Epoch 228: train loss: 0.07204500585794449, val loss: 0.09143473207950592\n",
      "Epoch 229: train loss: 0.07784400880336761, val loss: 0.11679134517908096\n",
      "Epoch 230: train loss: 0.07595986127853394, val loss: 0.1497282236814499\n",
      "Epoch 231: train loss: 0.06937598437070847, val loss: 0.16796384751796722\n",
      "Epoch 232: train loss: 0.09094185382127762, val loss: 0.06494105607271194\n",
      "Epoch 233: train loss: 0.07388792186975479, val loss: 0.12624810636043549\n",
      "Epoch 234: train loss: 0.06496544927358627, val loss: 0.09152956306934357\n",
      "Epoch 235: train loss: 0.06973229348659515, val loss: 0.11441733688116074\n",
      "Epoch 236: train loss: 0.06977837532758713, val loss: 0.1299028843641281\n",
      "Epoch 237: train loss: 0.07862607389688492, val loss: 0.09209206700325012\n",
      "Epoch 238: train loss: 0.07319821417331696, val loss: 0.08519580215215683\n",
      "Epoch 239: train loss: 0.07248561084270477, val loss: 0.09927300363779068\n",
      "Epoch 240: train loss: 0.08046364784240723, val loss: 0.1274424046278\n",
      "Epoch 241: train loss: 0.07370571792125702, val loss: 0.12192761898040771\n",
      "Epoch 242: train loss: 0.06375475227832794, val loss: 0.09428834915161133\n",
      "Epoch 243: train loss: 0.08320118486881256, val loss: 0.09033256024122238\n",
      "Epoch 244: train loss: 0.07026553899049759, val loss: 0.057667918503284454\n",
      "Epoch 245: train loss: 0.06062089279294014, val loss: 0.08395092934370041\n",
      "Epoch 246: train loss: 0.0689292848110199, val loss: 0.06585025787353516\n",
      "Epoch 247: train loss: 0.08153392374515533, val loss: 0.1181291863322258\n",
      "Epoch 248: train loss: 0.07366273552179337, val loss: 0.1307087242603302\n",
      "Epoch 249: train loss: 0.07566489279270172, val loss: 0.12077879905700684\n",
      "Epoch 250: train loss: 0.058854710310697556, val loss: 0.10266251862049103\n",
      "Epoch 251: train loss: 0.07332593947649002, val loss: 0.07040218263864517\n",
      "Epoch 252: train loss: 0.07253290712833405, val loss: 0.07571719586849213\n",
      "Epoch 253: train loss: 0.05673753097653389, val loss: 0.07494572550058365\n",
      "Epoch 254: train loss: 0.087057925760746, val loss: 0.09594929218292236\n",
      "Epoch 255: train loss: 0.06538169085979462, val loss: 0.1076413169503212\n",
      "Epoch 256: train loss: 0.07855397462844849, val loss: 0.10011676698923111\n",
      "Epoch 257: train loss: 0.06986255943775177, val loss: 0.08247768878936768\n",
      "Epoch 258: train loss: 0.07274137437343597, val loss: 0.08700086176395416\n",
      "Epoch 259: train loss: 0.07373299449682236, val loss: 0.09044793993234634\n",
      "Epoch 260: train loss: 0.06578942388296127, val loss: 0.10707707703113556\n",
      "Epoch 261: train loss: 0.07311096042394638, val loss: 0.08831065893173218\n",
      "Epoch 262: train loss: 0.0682859942317009, val loss: 0.054898619651794434\n",
      "Epoch 263: train loss: 0.05882645770907402, val loss: 0.13827316462993622\n",
      "Epoch 264: train loss: 0.07590518891811371, val loss: 0.09864338487386703\n",
      "Epoch 265: train loss: 0.07766179740428925, val loss: 0.12051896005868912\n",
      "Epoch 266: train loss: 0.07423833757638931, val loss: 0.09279175847768784\n",
      "Epoch 267: train loss: 0.06550709903240204, val loss: 0.10129137337207794\n",
      "Epoch 268: train loss: 0.07136841863393784, val loss: 0.10727987438440323\n",
      "Epoch 269: train loss: 0.058281730860471725, val loss: 0.1366579383611679\n",
      "Epoch 270: train loss: 0.05379423126578331, val loss: 0.13305211067199707\n",
      "Epoch 271: train loss: 0.06757955253124237, val loss: 0.08832906931638718\n",
      "Epoch 272: train loss: 0.06569698452949524, val loss: 0.14724601805210114\n",
      "Epoch 273: train loss: 0.06713856756687164, val loss: 0.11813825368881226\n",
      "Epoch 274: train loss: 0.0689612552523613, val loss: 0.09304709732532501\n",
      "Epoch 275: train loss: 0.06898059695959091, val loss: 0.0707566887140274\n",
      "Epoch 276: train loss: 0.07019556313753128, val loss: 0.1005123183131218\n",
      "Epoch 277: train loss: 0.06079244613647461, val loss: 0.0925457626581192\n",
      "Epoch 278: train loss: 0.062392257153987885, val loss: 0.07690751552581787\n",
      "Epoch 279: train loss: 0.06367301940917969, val loss: 0.09655071049928665\n",
      "Epoch 280: train loss: 0.07416124641895294, val loss: 0.12208855152130127\n",
      "Epoch 281: train loss: 0.06792519241571426, val loss: 0.09990083426237106\n",
      "Epoch 282: train loss: 0.0604160837829113, val loss: 0.09650091081857681\n",
      "Epoch 283: train loss: 0.06892136484384537, val loss: 0.0921412855386734\n",
      "Epoch 284: train loss: 0.06767987459897995, val loss: 0.09791085869073868\n",
      "Epoch 285: train loss: 0.06428086012601852, val loss: 0.08789284527301788\n",
      "Epoch 286: train loss: 0.0639626681804657, val loss: 0.08473861962556839\n",
      "Epoch 287: train loss: 0.05078840255737305, val loss: 0.0760604664683342\n",
      "Epoch 288: train loss: 0.06489112973213196, val loss: 0.08863737434148788\n",
      "Epoch 289: train loss: 0.06178245693445206, val loss: 0.09942621737718582\n",
      "Epoch 290: train loss: 0.06745772063732147, val loss: 0.13199864327907562\n",
      "Epoch 291: train loss: 0.07791139930486679, val loss: 0.08323287963867188\n",
      "Epoch 292: train loss: 0.06163199990987778, val loss: 0.09823188185691833\n",
      "Epoch 293: train loss: 0.056925442069768906, val loss: 0.06573522835969925\n",
      "Epoch 294: train loss: 0.073450468480587, val loss: 0.08143573254346848\n",
      "Epoch 295: train loss: 0.06629697978496552, val loss: 0.10860200226306915\n",
      "Epoch 296: train loss: 0.06723806262016296, val loss: 0.09135100245475769\n",
      "Epoch 297: train loss: 0.07159555703401566, val loss: 0.10467851161956787\n",
      "Epoch 298: train loss: 0.05788816511631012, val loss: 0.08471279591321945\n",
      "Epoch 299: train loss: 0.07008175551891327, val loss: 0.09513063728809357\n",
      "Epoch 300: train loss: 0.06830129027366638, val loss: 0.11473678797483444\n",
      "Epoch 301: train loss: 0.06550778448581696, val loss: 0.10194642841815948\n",
      "Epoch 302: train loss: 0.0762033462524414, val loss: 0.11975989490747452\n",
      "Epoch 303: train loss: 0.06696736812591553, val loss: 0.1167505532503128\n",
      "Epoch 304: train loss: 0.0574917271733284, val loss: 0.10951844602823257\n",
      "Epoch 305: train loss: 0.0544540211558342, val loss: 0.09250079840421677\n",
      "Epoch 306: train loss: 0.05760942026972771, val loss: 0.11122202128171921\n",
      "Epoch 307: train loss: 0.07051652669906616, val loss: 0.08128586411476135\n",
      "Epoch 308: train loss: 0.0587599016726017, val loss: 0.09125574678182602\n",
      "Epoch 309: train loss: 0.06148591637611389, val loss: 0.08650744706392288\n",
      "Epoch 310: train loss: 0.06920261681079865, val loss: 0.11934369057416916\n",
      "Epoch 311: train loss: 0.05846920236945152, val loss: 0.09655449539422989\n",
      "Epoch 312: train loss: 0.06245919317007065, val loss: 0.07209110260009766\n",
      "Epoch 313: train loss: 0.05581352487206459, val loss: 0.07195552438497543\n",
      "Epoch 314: train loss: 0.06075827777385712, val loss: 0.09574475884437561\n",
      "Epoch 315: train loss: 0.06875402480363846, val loss: 0.0844634398818016\n",
      "Epoch 316: train loss: 0.05702337995171547, val loss: 0.12835510075092316\n",
      "Epoch 317: train loss: 0.05951954796910286, val loss: 0.13750584423542023\n",
      "Epoch 318: train loss: 0.06829063594341278, val loss: 0.07613636553287506\n",
      "Epoch 319: train loss: 0.06688418239355087, val loss: 0.08407620340585709\n",
      "Epoch 320: train loss: 0.06283187866210938, val loss: 0.11590921878814697\n",
      "Epoch 321: train loss: 0.06283388286828995, val loss: 0.09535332769155502\n",
      "Epoch 322: train loss: 0.06589395552873611, val loss: 0.07083044201135635\n",
      "Epoch 323: train loss: 0.0641254186630249, val loss: 0.09564433246850967\n",
      "Epoch 324: train loss: 0.06273367255926132, val loss: 0.09986110031604767\n",
      "Epoch 325: train loss: 0.06462790817022324, val loss: 0.07621369510889053\n",
      "Epoch 326: train loss: 0.05915052816271782, val loss: 0.08868284523487091\n",
      "Epoch 327: train loss: 0.06395678222179413, val loss: 0.08619289845228195\n",
      "Epoch 328: train loss: 0.05368504300713539, val loss: 0.088228240609169\n",
      "Epoch 329: train loss: 0.060585279017686844, val loss: 0.14873960614204407\n",
      "Epoch 330: train loss: 0.06119808182120323, val loss: 0.11164791882038116\n",
      "Epoch 331: train loss: 0.06772489100694656, val loss: 0.08616744726896286\n",
      "Epoch 332: train loss: 0.06489261984825134, val loss: 0.07781781256198883\n",
      "Epoch 333: train loss: 0.06916191428899765, val loss: 0.10182330757379532\n",
      "Epoch 334: train loss: 0.06200816109776497, val loss: 0.10658945888280869\n",
      "Epoch 335: train loss: 0.05860019475221634, val loss: 0.12094936519861221\n",
      "Epoch 336: train loss: 0.06297927349805832, val loss: 0.0871983990073204\n",
      "Epoch 337: train loss: 0.050196193158626556, val loss: 0.07562386244535446\n",
      "Epoch 338: train loss: 0.05731289088726044, val loss: 0.07528196275234222\n",
      "Epoch 339: train loss: 0.06328459084033966, val loss: 0.08587783575057983\n",
      "Epoch 340: train loss: 0.06419511884450912, val loss: 0.09462936967611313\n",
      "Epoch 341: train loss: 0.06442800164222717, val loss: 0.0688353106379509\n",
      "Epoch 342: train loss: 0.06071675196290016, val loss: 0.08279441297054291\n",
      "Epoch 343: train loss: 0.06533946096897125, val loss: 0.0838802382349968\n",
      "Epoch 344: train loss: 0.05457979068160057, val loss: 0.08205723017454147\n",
      "Epoch 345: train loss: 0.06320478767156601, val loss: 0.0732092633843422\n",
      "Epoch 346: train loss: 0.06695353239774704, val loss: 0.09717106819152832\n",
      "Epoch 347: train loss: 0.0616246722638607, val loss: 0.06953277438879013\n",
      "Epoch 348: train loss: 0.054214637726545334, val loss: 0.10779736191034317\n",
      "Epoch 349: train loss: 0.049605805426836014, val loss: 0.09928623586893082\n",
      "Epoch 350: train loss: 0.05484531447291374, val loss: 0.08689271658658981\n",
      "Epoch 351: train loss: 0.055547282099723816, val loss: 0.1197669729590416\n",
      "Epoch 352: train loss: 0.05488136038184166, val loss: 0.11048131436109543\n",
      "Epoch 353: train loss: 0.05467185005545616, val loss: 0.07721501588821411\n",
      "Epoch 354: train loss: 0.06380338966846466, val loss: 0.06145159527659416\n",
      "Epoch 355: train loss: 0.07487242668867111, val loss: 0.12110894173383713\n",
      "Epoch 356: train loss: 0.06275633722543716, val loss: 0.10143976658582687\n",
      "Epoch 357: train loss: 0.0648529902100563, val loss: 0.07247272878885269\n",
      "Epoch 358: train loss: 0.05710461363196373, val loss: 0.08589150011539459\n",
      "Epoch 359: train loss: 0.05895943194627762, val loss: 0.1029813289642334\n",
      "Epoch 360: train loss: 0.058846116065979004, val loss: 0.0973285660147667\n",
      "Epoch 361: train loss: 0.06310560554265976, val loss: 0.0639895349740982\n",
      "Epoch 362: train loss: 0.05180152505636215, val loss: 0.08646221458911896\n",
      "Epoch 363: train loss: 0.06047392636537552, val loss: 0.07884053885936737\n",
      "Epoch 364: train loss: 0.05550120770931244, val loss: 0.11328325420618057\n",
      "Epoch 365: train loss: 0.06071826070547104, val loss: 0.11301622539758682\n",
      "Epoch 366: train loss: 0.05701229348778725, val loss: 0.09870308637619019\n",
      "Epoch 367: train loss: 0.06047875061631203, val loss: 0.06861593574285507\n",
      "Epoch 368: train loss: 0.06719312071800232, val loss: 0.09776115417480469\n",
      "Epoch 369: train loss: 0.06155751645565033, val loss: 0.10449208319187164\n",
      "Epoch 370: train loss: 0.05716864764690399, val loss: 0.06434125453233719\n",
      "Epoch 371: train loss: 0.05504776909947395, val loss: 0.08634168654680252\n",
      "Epoch 372: train loss: 0.046950437128543854, val loss: 0.11251071840524673\n",
      "Epoch 373: train loss: 0.05704529210925102, val loss: 0.09376364201307297\n",
      "Epoch 374: train loss: 0.05408814176917076, val loss: 0.07791773229837418\n",
      "Epoch 375: train loss: 0.05386563390493393, val loss: 0.08013156801462173\n",
      "Epoch 376: train loss: 0.057356011122465134, val loss: 0.07935097068548203\n",
      "Epoch 377: train loss: 0.05559471994638443, val loss: 0.09202006459236145\n",
      "Epoch 378: train loss: 0.060927677899599075, val loss: 0.0891382098197937\n",
      "Epoch 379: train loss: 0.05805841088294983, val loss: 0.06486063450574875\n",
      "Epoch 380: train loss: 0.05531292036175728, val loss: 0.09764484316110611\n",
      "Epoch 381: train loss: 0.05641515552997589, val loss: 0.09574177116155624\n",
      "Epoch 382: train loss: 0.06115249544382095, val loss: 0.06573257595300674\n",
      "Epoch 383: train loss: 0.06029239296913147, val loss: 0.11625788360834122\n",
      "Epoch 384: train loss: 0.059837449342012405, val loss: 0.10128140449523926\n",
      "Epoch 385: train loss: 0.06315682828426361, val loss: 0.07020384073257446\n",
      "Epoch 386: train loss: 0.05406533554196358, val loss: 0.0843036025762558\n",
      "Epoch 387: train loss: 0.061946604400873184, val loss: 0.09967094659805298\n",
      "Epoch 388: train loss: 0.05925988778471947, val loss: 0.1113445982336998\n",
      "Epoch 389: train loss: 0.057856082916259766, val loss: 0.09673892706632614\n",
      "Epoch 390: train loss: 0.050820332020521164, val loss: 0.10311275720596313\n",
      "Epoch 391: train loss: 0.06034873425960541, val loss: 0.10794997215270996\n",
      "Epoch 392: train loss: 0.058375827968120575, val loss: 0.07649823278188705\n",
      "Epoch 393: train loss: 0.050304170697927475, val loss: 0.09390618652105331\n",
      "Epoch 394: train loss: 0.05339394882321358, val loss: 0.106450654566288\n",
      "Epoch 395: train loss: 0.0585629902780056, val loss: 0.09629920870065689\n",
      "Epoch 396: train loss: 0.050117410719394684, val loss: 0.056033145636320114\n",
      "Epoch 397: train loss: 0.06538482755422592, val loss: 0.08532705903053284\n",
      "Epoch 398: train loss: 0.05761375650763512, val loss: 0.07177256047725677\n",
      "Epoch 399: train loss: 0.05528527498245239, val loss: 0.09876137971878052\n",
      "Epoch 400: train loss: 0.06098891422152519, val loss: 0.083893783390522\n",
      "Epoch 401: train loss: 0.05998321622610092, val loss: 0.10648341476917267\n",
      "Epoch 402: train loss: 0.056232135742902756, val loss: 0.11734984070062637\n",
      "Epoch 403: train loss: 0.055729374289512634, val loss: 0.10108859837055206\n",
      "Epoch 404: train loss: 0.05929134413599968, val loss: 0.1075960323214531\n",
      "Epoch 405: train loss: 0.05327397957444191, val loss: 0.10004409402608871\n",
      "Epoch 406: train loss: 0.051553282886743546, val loss: 0.1116892620921135\n",
      "Epoch 407: train loss: 0.05453962832689285, val loss: 0.10295744240283966\n",
      "Epoch 408: train loss: 0.0556672178208828, val loss: 0.10072534531354904\n",
      "Epoch 409: train loss: 0.06257829070091248, val loss: 0.07914550602436066\n",
      "Epoch 410: train loss: 0.05383719503879547, val loss: 0.09257257729768753\n",
      "Epoch 411: train loss: 0.06996676325798035, val loss: 0.07989693433046341\n",
      "Epoch 412: train loss: 0.05815577134490013, val loss: 0.0970086008310318\n",
      "Epoch 413: train loss: 0.04448787495493889, val loss: 0.09489738196134567\n",
      "Epoch 414: train loss: 0.05354989320039749, val loss: 0.09906832873821259\n",
      "Epoch 415: train loss: 0.05687889829277992, val loss: 0.07628317922353745\n",
      "Epoch 416: train loss: 0.059217654168605804, val loss: 0.09447049349546432\n",
      "Epoch 417: train loss: 0.06429339200258255, val loss: 0.08577519655227661\n",
      "Epoch 418: train loss: 0.04543619975447655, val loss: 0.09652579575777054\n",
      "Epoch 419: train loss: 0.05064397677779198, val loss: 0.10965084284543991\n",
      "Epoch 420: train loss: 0.05875493213534355, val loss: 0.07473017275333405\n",
      "Epoch 421: train loss: 0.05839027836918831, val loss: 0.11934830993413925\n",
      "Epoch 422: train loss: 0.05511984974145889, val loss: 0.06337936967611313\n",
      "Epoch 423: train loss: 0.05646951124072075, val loss: 0.12324166297912598\n",
      "Epoch 424: train loss: 0.058882925659418106, val loss: 0.08496952801942825\n",
      "Epoch 425: train loss: 0.05995061993598938, val loss: 0.05980755016207695\n",
      "Epoch 426: train loss: 0.061634309589862823, val loss: 0.06268924474716187\n",
      "Epoch 427: train loss: 0.06637409329414368, val loss: 0.09954055398702621\n",
      "Epoch 428: train loss: 0.05457615107297897, val loss: 0.07438306510448456\n",
      "Epoch 429: train loss: 0.05267564207315445, val loss: 0.08007674664258957\n",
      "Epoch 430: train loss: 0.05930441990494728, val loss: 0.13115732371807098\n",
      "Epoch 431: train loss: 0.06508955359458923, val loss: 0.09033343940973282\n",
      "Epoch 432: train loss: 0.057505324482917786, val loss: 0.09401249140501022\n",
      "Epoch 433: train loss: 0.05864681676030159, val loss: 0.09276868402957916\n",
      "Epoch 434: train loss: 0.05869204178452492, val loss: 0.10043468326330185\n",
      "Epoch 435: train loss: 0.05436195805668831, val loss: 0.08651133626699448\n",
      "Epoch 436: train loss: 0.0582919679582119, val loss: 0.0862857773900032\n",
      "Epoch 437: train loss: 0.05821531265974045, val loss: 0.09200365841388702\n",
      "Epoch 438: train loss: 0.05963347852230072, val loss: 0.08170672506093979\n",
      "Epoch 439: train loss: 0.06560508161783218, val loss: 0.11657673120498657\n",
      "Epoch 440: train loss: 0.055230751633644104, val loss: 0.08663811534643173\n",
      "Epoch 441: train loss: 0.05475378781557083, val loss: 0.10277105867862701\n",
      "Epoch 442: train loss: 0.059271566569805145, val loss: 0.08308696001768112\n",
      "Epoch 443: train loss: 0.053909219801425934, val loss: 0.11441894620656967\n",
      "Epoch 444: train loss: 0.05923891067504883, val loss: 0.11530708521604538\n",
      "Epoch 445: train loss: 0.05411568284034729, val loss: 0.09878791123628616\n",
      "Epoch 446: train loss: 0.05501126870512962, val loss: 0.09362474828958511\n",
      "Epoch 447: train loss: 0.048417914658784866, val loss: 0.08141257613897324\n",
      "Epoch 448: train loss: 0.04724321886897087, val loss: 0.11523205041885376\n",
      "Epoch 449: train loss: 0.05172933638095856, val loss: 0.1323407143354416\n",
      "Epoch 450: train loss: 0.04949409142136574, val loss: 0.09269583225250244\n",
      "Epoch 451: train loss: 0.05696452781558037, val loss: 0.10220400243997574\n",
      "Epoch 452: train loss: 0.057568736374378204, val loss: 0.06995123624801636\n",
      "Epoch 453: train loss: 0.052872102707624435, val loss: 0.10513883084058762\n",
      "Epoch 454: train loss: 0.0611693300306797, val loss: 0.07065553963184357\n",
      "Epoch 455: train loss: 0.05039975047111511, val loss: 0.10135593265295029\n",
      "Epoch 456: train loss: 0.0519329197704792, val loss: 0.08304315060377121\n",
      "Epoch 457: train loss: 0.050948016345500946, val loss: 0.06805931776762009\n",
      "Epoch 458: train loss: 0.057443663477897644, val loss: 0.10062481462955475\n",
      "Epoch 459: train loss: 0.06268034875392914, val loss: 0.06948351860046387\n",
      "Epoch 460: train loss: 0.05164547264575958, val loss: 0.10365910828113556\n",
      "Epoch 461: train loss: 0.05951587110757828, val loss: 0.12833495438098907\n",
      "Epoch 462: train loss: 0.05564874783158302, val loss: 0.10441721975803375\n",
      "Epoch 463: train loss: 0.05986413732171059, val loss: 0.09719914197921753\n",
      "Epoch 464: train loss: 0.055739276111125946, val loss: 0.10465093702077866\n",
      "Epoch 465: train loss: 0.05610731989145279, val loss: 0.09597083926200867\n",
      "Epoch 466: train loss: 0.05311236158013344, val loss: 0.07663335651159286\n",
      "Epoch 467: train loss: 0.05045390874147415, val loss: 0.08176630735397339\n",
      "Epoch 468: train loss: 0.053898315876722336, val loss: 0.0846027359366417\n",
      "Epoch 469: train loss: 0.052203986793756485, val loss: 0.08869554102420807\n",
      "Epoch 470: train loss: 0.05120239406824112, val loss: 0.10280998051166534\n",
      "Epoch 471: train loss: 0.05786808952689171, val loss: 0.08168439567089081\n",
      "Epoch 472: train loss: 0.055803485214710236, val loss: 0.08847131580114365\n",
      "Epoch 473: train loss: 0.05933956801891327, val loss: 0.08672019094228745\n",
      "Epoch 474: train loss: 0.051903873682022095, val loss: 0.0767975002527237\n",
      "Epoch 475: train loss: 0.05587774142622948, val loss: 0.0973268523812294\n",
      "Epoch 476: train loss: 0.0477837473154068, val loss: 0.115796759724617\n",
      "Epoch 477: train loss: 0.05667092278599739, val loss: 0.06742225587368011\n",
      "Epoch 478: train loss: 0.054826293140649796, val loss: 0.10735543072223663\n",
      "Epoch 479: train loss: 0.05000065267086029, val loss: 0.1157122477889061\n",
      "Epoch 480: train loss: 0.05249187722802162, val loss: 0.113900326192379\n",
      "Epoch 481: train loss: 0.048923198133707047, val loss: 0.09951506555080414\n",
      "Epoch 482: train loss: 0.056384023278951645, val loss: 0.08167797327041626\n",
      "Epoch 483: train loss: 0.05114622414112091, val loss: 0.08191923052072525\n",
      "Epoch 484: train loss: 0.048101283609867096, val loss: 0.08465205878019333\n",
      "Epoch 485: train loss: 0.05600641295313835, val loss: 0.08369822800159454\n",
      "Epoch 486: train loss: 0.06039327010512352, val loss: 0.11840679496526718\n",
      "Epoch 487: train loss: 0.052503470331430435, val loss: 0.0504232756793499\n",
      "Epoch 488: train loss: 0.058655284345149994, val loss: 0.10916052013635635\n",
      "Epoch 489: train loss: 0.051127057522535324, val loss: 0.10881228744983673\n",
      "Epoch 490: train loss: 0.055605825036764145, val loss: 0.09015463292598724\n",
      "Epoch 491: train loss: 0.05757515877485275, val loss: 0.09970881789922714\n",
      "Epoch 492: train loss: 0.05603720620274544, val loss: 0.08288991451263428\n",
      "Epoch 493: train loss: 0.056480105966329575, val loss: 0.10780476778745651\n",
      "Epoch 494: train loss: 0.053742069751024246, val loss: 0.07882630079984665\n",
      "Epoch 495: train loss: 0.060820240527391434, val loss: 0.07293607294559479\n",
      "Epoch 496: train loss: 0.059189606457948685, val loss: 0.08178102225065231\n",
      "Epoch 497: train loss: 0.05398376286029816, val loss: 0.08613280206918716\n",
      "Epoch 498: train loss: 0.05556580424308777, val loss: 0.07866352051496506\n",
      "Epoch 499: train loss: 0.062180403620004654, val loss: 0.10060452669858932\n",
      "Epoch 500: train loss: 0.05043746158480644, val loss: 0.11745727062225342\n",
      "Epoch 501: train loss: 0.05084061250090599, val loss: 0.11858528852462769\n",
      "Epoch 502: train loss: 0.05460217967629433, val loss: 0.07871802151203156\n",
      "Epoch 503: train loss: 0.0514974482357502, val loss: 0.10205725580453873\n",
      "Epoch 504: train loss: 0.056996870785951614, val loss: 0.1001395732164383\n",
      "Epoch 505: train loss: 0.04476083815097809, val loss: 0.058602701872587204\n",
      "Epoch 506: train loss: 0.05591881647706032, val loss: 0.06151566654443741\n",
      "Epoch 507: train loss: 0.05523977056145668, val loss: 0.09030189365148544\n",
      "Epoch 508: train loss: 0.060708969831466675, val loss: 0.055928636342287064\n",
      "Epoch 509: train loss: 0.053905095905065536, val loss: 0.09531380236148834\n",
      "Epoch 510: train loss: 0.05440765991806984, val loss: 0.0548480860888958\n",
      "Epoch 511: train loss: 0.0541878379881382, val loss: 0.09364724159240723\n",
      "Epoch 512: train loss: 0.04419732093811035, val loss: 0.07040379196405411\n",
      "Epoch 513: train loss: 0.05496910959482193, val loss: 0.08266344666481018\n",
      "Epoch 514: train loss: 0.057952314615249634, val loss: 0.08899161964654922\n",
      "Epoch 515: train loss: 0.05532963201403618, val loss: 0.09312354773283005\n",
      "Epoch 516: train loss: 0.06170785799622536, val loss: 0.10584479570388794\n",
      "Epoch 517: train loss: 0.05609055235981941, val loss: 0.085664764046669\n",
      "Epoch 518: train loss: 0.047267235815525055, val loss: 0.10892784595489502\n",
      "Epoch 519: train loss: 0.049333032220602036, val loss: 0.08005346357822418\n",
      "Epoch 520: train loss: 0.05962572619318962, val loss: 0.08145113289356232\n",
      "Epoch 521: train loss: 0.05420314520597458, val loss: 0.07697466015815735\n",
      "Epoch 522: train loss: 0.049789298325777054, val loss: 0.08530670404434204\n",
      "Epoch 523: train loss: 0.05673179402947426, val loss: 0.09233557432889938\n",
      "Epoch 524: train loss: 0.048731692135334015, val loss: 0.07084740698337555\n",
      "Epoch 525: train loss: 0.059698380529880524, val loss: 0.10061071068048477\n",
      "Epoch 526: train loss: 0.061417900025844574, val loss: 0.09422551095485687\n",
      "Epoch 527: train loss: 0.04757634550333023, val loss: 0.06802576035261154\n",
      "Epoch 528: train loss: 0.050780341029167175, val loss: 0.08569608628749847\n",
      "Epoch 529: train loss: 0.051024358719587326, val loss: 0.09897719323635101\n",
      "Epoch 530: train loss: 0.05128917843103409, val loss: 0.08472398668527603\n",
      "Epoch 531: train loss: 0.05067555233836174, val loss: 0.09517689794301987\n",
      "Epoch 532: train loss: 0.05865431949496269, val loss: 0.08320499956607819\n",
      "Epoch 533: train loss: 0.05418030545115471, val loss: 0.07956527918577194\n",
      "Epoch 534: train loss: 0.057315096259117126, val loss: 0.11208231747150421\n",
      "Epoch 535: train loss: 0.05602795258164406, val loss: 0.08069705218076706\n",
      "Epoch 536: train loss: 0.046802714467048645, val loss: 0.10575368255376816\n",
      "Epoch 537: train loss: 0.05476441979408264, val loss: 0.10154276341199875\n",
      "Epoch 538: train loss: 0.058094847947359085, val loss: 0.07793974876403809\n",
      "Epoch 539: train loss: 0.05110876262187958, val loss: 0.10151692479848862\n",
      "Epoch 540: train loss: 0.05494813621044159, val loss: 0.08600828796625137\n",
      "Epoch 541: train loss: 0.06622433662414551, val loss: 0.07154693454504013\n",
      "Epoch 542: train loss: 0.05844971537590027, val loss: 0.08500532060861588\n",
      "Epoch 543: train loss: 0.05317015200853348, val loss: 0.06975143402814865\n",
      "Epoch 544: train loss: 0.049939289689064026, val loss: 0.11605037748813629\n",
      "Epoch 545: train loss: 0.05098466947674751, val loss: 0.07959895581007004\n",
      "Epoch 546: train loss: 0.0587717704474926, val loss: 0.07178285717964172\n",
      "Epoch 547: train loss: 0.050353243947029114, val loss: 0.09364920854568481\n",
      "Epoch 548: train loss: 0.057604577392339706, val loss: 0.1090097650885582\n",
      "Epoch 549: train loss: 0.054110750555992126, val loss: 0.09923555701971054\n",
      "Epoch 550: train loss: 0.045611556619405746, val loss: 0.13613756000995636\n",
      "Epoch 551: train loss: 0.0595722533762455, val loss: 0.09912479668855667\n",
      "Epoch 552: train loss: 0.0539078414440155, val loss: 0.10857230424880981\n",
      "Epoch 553: train loss: 0.051957130432128906, val loss: 0.09387242794036865\n",
      "Epoch 554: train loss: 0.05266802757978439, val loss: 0.06222444772720337\n",
      "Epoch 555: train loss: 0.05318678915500641, val loss: 0.08623620122671127\n",
      "Epoch 556: train loss: 0.05576283112168312, val loss: 0.08003046363592148\n",
      "Epoch 557: train loss: 0.052952803671360016, val loss: 0.10146134346723557\n",
      "Epoch 558: train loss: 0.05924002826213837, val loss: 0.09360881894826889\n",
      "Epoch 559: train loss: 0.05302054062485695, val loss: 0.09637842327356339\n",
      "Epoch 560: train loss: 0.055557094514369965, val loss: 0.08140891790390015\n",
      "Epoch 561: train loss: 0.057617928832769394, val loss: 0.08678215742111206\n",
      "Epoch 562: train loss: 0.06021815538406372, val loss: 0.0908086895942688\n",
      "Epoch 563: train loss: 0.0485713817179203, val loss: 0.08914202451705933\n",
      "Epoch 564: train loss: 0.05562537536025047, val loss: 0.08618228882551193\n",
      "Epoch 565: train loss: 0.0520414374768734, val loss: 0.08057297766208649\n",
      "Epoch 566: train loss: 0.045540884137153625, val loss: 0.07376763969659805\n",
      "Epoch 567: train loss: 0.05746620148420334, val loss: 0.06105349212884903\n",
      "Epoch 568: train loss: 0.05536116659641266, val loss: 0.05604667589068413\n",
      "Epoch 569: train loss: 0.05026179552078247, val loss: 0.07952500879764557\n",
      "Epoch 570: train loss: 0.05019411817193031, val loss: 0.10628222674131393\n",
      "Epoch 571: train loss: 0.0520344153046608, val loss: 0.07378580421209335\n",
      "Epoch 572: train loss: 0.05410375818610191, val loss: 0.1014285609126091\n",
      "Epoch 573: train loss: 0.0493631586432457, val loss: 0.08964268118143082\n",
      "Epoch 574: train loss: 0.05075574666261673, val loss: 0.08231031894683838\n",
      "Epoch 575: train loss: 0.05512554943561554, val loss: 0.07847044616937637\n",
      "Epoch 576: train loss: 0.04667631536722183, val loss: 0.09070921689271927\n",
      "Epoch 577: train loss: 0.057370975613594055, val loss: 0.09159430861473083\n",
      "Epoch 578: train loss: 0.052317336201667786, val loss: 0.08033803850412369\n",
      "Epoch 579: train loss: 0.05427589640021324, val loss: 0.07123929262161255\n",
      "Epoch 580: train loss: 0.04998248443007469, val loss: 0.08088560402393341\n",
      "Epoch 581: train loss: 0.054981403052806854, val loss: 0.08179860562086105\n",
      "Epoch 582: train loss: 0.06436781585216522, val loss: 0.08407550305128098\n",
      "Epoch 583: train loss: 0.06356384605169296, val loss: 0.06045135483145714\n",
      "Epoch 584: train loss: 0.05239075794816017, val loss: 0.08417439460754395\n",
      "Epoch 585: train loss: 0.04818592220544815, val loss: 0.1100175753235817\n",
      "Epoch 586: train loss: 0.06213301792740822, val loss: 0.0768180638551712\n",
      "Epoch 587: train loss: 0.04551544412970543, val loss: 0.09110308438539505\n",
      "Epoch 588: train loss: 0.05157940834760666, val loss: 0.10419050604104996\n",
      "Epoch 589: train loss: 0.04308554530143738, val loss: 0.04545316472649574\n",
      "Epoch 590: train loss: 0.05760933831334114, val loss: 0.10662008821964264\n",
      "Epoch 591: train loss: 0.05013196915388107, val loss: 0.11851353943347931\n",
      "Epoch 592: train loss: 0.05162454769015312, val loss: 0.07197095453739166\n",
      "Epoch 593: train loss: 0.052155908197164536, val loss: 0.10342621803283691\n",
      "Epoch 594: train loss: 0.058871425688266754, val loss: 0.11095326393842697\n",
      "Epoch 595: train loss: 0.0490485318005085, val loss: 0.1057242900133133\n",
      "Epoch 596: train loss: 0.04930408298969269, val loss: 0.1015588566660881\n",
      "Epoch 597: train loss: 0.048669006675481796, val loss: 0.09065338224172592\n",
      "Epoch 598: train loss: 0.04662097990512848, val loss: 0.0752628743648529\n",
      "Epoch 599: train loss: 0.04812502861022949, val loss: 0.08142673969268799\n",
      "Epoch 600: train loss: 0.0451924167573452, val loss: 0.0891713872551918\n",
      "Epoch 601: train loss: 0.04562291502952576, val loss: 0.10068874806165695\n",
      "Epoch 602: train loss: 0.05026884749531746, val loss: 0.09035616368055344\n",
      "Epoch 603: train loss: 0.04600820690393448, val loss: 0.0985226035118103\n",
      "Epoch 604: train loss: 0.05002027377486229, val loss: 0.10832083225250244\n",
      "Epoch 605: train loss: 0.05123913288116455, val loss: 0.1002788171172142\n",
      "Epoch 606: train loss: 0.04700196534395218, val loss: 0.07700406014919281\n",
      "Epoch 607: train loss: 0.03884545713663101, val loss: 0.09102192521095276\n",
      "Epoch 608: train loss: 0.0471864752471447, val loss: 0.09512128680944443\n",
      "Epoch 609: train loss: 0.046868886798620224, val loss: 0.1079113632440567\n",
      "Epoch 610: train loss: 0.05248643085360527, val loss: 0.10137063264846802\n",
      "Epoch 611: train loss: 0.045798420906066895, val loss: 0.06928455084562302\n",
      "Epoch 612: train loss: 0.04872548580169678, val loss: 0.07337739318609238\n",
      "Epoch 613: train loss: 0.055437296628952026, val loss: 0.059078581631183624\n",
      "Epoch 614: train loss: 0.05377482622861862, val loss: 0.11398710310459137\n",
      "Epoch 615: train loss: 0.047232188284397125, val loss: 0.12144072353839874\n",
      "Epoch 616: train loss: 0.04833107069134712, val loss: 0.08541630953550339\n",
      "Epoch 617: train loss: 0.0466267392039299, val loss: 0.09436915069818497\n",
      "Epoch 618: train loss: 0.041986994445323944, val loss: 0.08912310749292374\n",
      "Epoch 619: train loss: 0.04915929585695267, val loss: 0.1088453158736229\n",
      "Epoch 620: train loss: 0.05350438877940178, val loss: 0.0905764251947403\n",
      "Epoch 621: train loss: 0.050549544394016266, val loss: 0.09597528725862503\n",
      "Epoch 622: train loss: 0.04969663918018341, val loss: 0.08218159526586533\n",
      "Epoch 623: train loss: 0.05470810458064079, val loss: 0.08433591574430466\n",
      "Epoch 624: train loss: 0.0441090427339077, val loss: 0.08192974328994751\n",
      "Epoch 625: train loss: 0.044837646186351776, val loss: 0.09676000475883484\n",
      "Epoch 626: train loss: 0.048046961426734924, val loss: 0.08833011239767075\n",
      "Epoch 627: train loss: 0.042229991406202316, val loss: 0.11006858199834824\n",
      "Epoch 628: train loss: 0.048271577805280685, val loss: 0.08141098916530609\n",
      "Epoch 629: train loss: 0.052590444684028625, val loss: 0.08712735027074814\n",
      "Epoch 630: train loss: 0.04875047132372856, val loss: 0.08585017919540405\n",
      "Epoch 631: train loss: 0.04947047308087349, val loss: 0.12275969237089157\n",
      "Epoch 632: train loss: 0.05259499326348305, val loss: 0.10368233174085617\n",
      "Epoch 633: train loss: 0.05449953302741051, val loss: 0.1271810084581375\n",
      "Epoch 634: train loss: 0.05169994756579399, val loss: 0.12396752834320068\n",
      "Epoch 635: train loss: 0.057967659085989, val loss: 0.09570717066526413\n",
      "Epoch 636: train loss: 0.05360182747244835, val loss: 0.11204385757446289\n",
      "Epoch 637: train loss: 0.04844498261809349, val loss: 0.08306609839200974\n",
      "Epoch 638: train loss: 0.05712945759296417, val loss: 0.09204914420843124\n",
      "Epoch 639: train loss: 0.051290787756443024, val loss: 0.08533825725317001\n",
      "Epoch 640: train loss: 0.05071325600147247, val loss: 0.0894632339477539\n",
      "Epoch 641: train loss: 0.05372164025902748, val loss: 0.05607575178146362\n",
      "Epoch 642: train loss: 0.04176636412739754, val loss: 0.0759279653429985\n",
      "Epoch 643: train loss: 0.05165160074830055, val loss: 0.07742433995008469\n",
      "Epoch 644: train loss: 0.046496424823999405, val loss: 0.0755479708313942\n",
      "Epoch 645: train loss: 0.05042200908064842, val loss: 0.08939560502767563\n",
      "Epoch 646: train loss: 0.05485846474766731, val loss: 0.08954192698001862\n",
      "Epoch 647: train loss: 0.05573250725865364, val loss: 0.05261652544140816\n",
      "Epoch 648: train loss: 0.05350042134523392, val loss: 0.08044464886188507\n",
      "Epoch 649: train loss: 0.04853281378746033, val loss: 0.10311847925186157\n",
      "Epoch 650: train loss: 0.04851897060871124, val loss: 0.10955255478620529\n",
      "Epoch 651: train loss: 0.060185085982084274, val loss: 0.1191551461815834\n",
      "Epoch 652: train loss: 0.05456157028675079, val loss: 0.08096162229776382\n",
      "Epoch 653: train loss: 0.05115652084350586, val loss: 0.07782164961099625\n",
      "Epoch 654: train loss: 0.047282204031944275, val loss: 0.08258211612701416\n",
      "Epoch 655: train loss: 0.05350317060947418, val loss: 0.08652340620756149\n",
      "Epoch 656: train loss: 0.060285527259111404, val loss: 0.0913265272974968\n",
      "Epoch 657: train loss: 0.05607358366250992, val loss: 0.05666656419634819\n",
      "Epoch 658: train loss: 0.051347363740205765, val loss: 0.07568491995334625\n",
      "Epoch 659: train loss: 0.05164238065481186, val loss: 0.06876567751169205\n",
      "Epoch 660: train loss: 0.05741322413086891, val loss: 0.12335597723722458\n",
      "Epoch 661: train loss: 0.05956839770078659, val loss: 0.09594978392124176\n",
      "Epoch 662: train loss: 0.0544842928647995, val loss: 0.07811450958251953\n",
      "Epoch 663: train loss: 0.06121128425002098, val loss: 0.09772878140211105\n",
      "Epoch 664: train loss: 0.05954702943563461, val loss: 0.06365108489990234\n",
      "Epoch 665: train loss: 0.056613579392433167, val loss: 0.09896557778120041\n",
      "Epoch 666: train loss: 0.0552871935069561, val loss: 0.08175317198038101\n",
      "Epoch 667: train loss: 0.05654427409172058, val loss: 0.0905950665473938\n",
      "Epoch 668: train loss: 0.056075140833854675, val loss: 0.1008448377251625\n",
      "Epoch 669: train loss: 0.053215302526950836, val loss: 0.05677802488207817\n",
      "Epoch 670: train loss: 0.060240816324949265, val loss: 0.08412852883338928\n",
      "Epoch 671: train loss: 0.059810344129800797, val loss: 0.07918550074100494\n",
      "Epoch 672: train loss: 0.05957518890500069, val loss: 0.0844399556517601\n",
      "Epoch 673: train loss: 0.053560521453619, val loss: 0.08086103200912476\n",
      "Epoch 674: train loss: 0.04691731184720993, val loss: 0.08250925689935684\n",
      "Epoch 675: train loss: 0.062494486570358276, val loss: 0.08309032768011093\n",
      "Epoch 676: train loss: 0.05547849088907242, val loss: 0.09496648609638214\n",
      "Epoch 677: train loss: 0.058866631239652634, val loss: 0.10491307824850082\n",
      "Epoch 678: train loss: 0.052203644067049026, val loss: 0.10896308720111847\n",
      "Epoch 679: train loss: 0.056116487830877304, val loss: 0.10287733376026154\n",
      "Epoch 680: train loss: 0.04616749286651611, val loss: 0.06291084736585617\n",
      "Epoch 681: train loss: 0.047585468739271164, val loss: 0.10790570825338364\n",
      "Epoch 682: train loss: 0.061170946806669235, val loss: 0.06859162449836731\n",
      "Epoch 683: train loss: 0.054080624133348465, val loss: 0.08558548241853714\n",
      "Epoch 684: train loss: 0.05892593786120415, val loss: 0.08890970796346664\n",
      "Epoch 685: train loss: 0.04826875403523445, val loss: 0.11528929322957993\n",
      "Epoch 686: train loss: 0.05499868094921112, val loss: 0.07519730180501938\n",
      "Epoch 687: train loss: 0.058730095624923706, val loss: 0.11203575134277344\n",
      "Epoch 688: train loss: 0.049798689782619476, val loss: 0.07952194660902023\n",
      "Epoch 689: train loss: 0.04813021048903465, val loss: 0.08143117278814316\n",
      "Epoch 690: train loss: 0.05205409973859787, val loss: 0.08361553400754929\n",
      "Epoch 691: train loss: 0.05874631926417351, val loss: 0.11870360374450684\n",
      "Epoch 692: train loss: 0.05497608333826065, val loss: 0.09547276794910431\n",
      "Epoch 693: train loss: 0.05909157544374466, val loss: 0.08764363825321198\n",
      "Epoch 694: train loss: 0.054992884397506714, val loss: 0.07500012964010239\n",
      "Epoch 695: train loss: 0.054102931171655655, val loss: 0.09532883018255234\n",
      "Epoch 696: train loss: 0.059274859726428986, val loss: 0.09401317685842514\n",
      "Epoch 697: train loss: 0.05187840014696121, val loss: 0.1215251088142395\n",
      "Epoch 698: train loss: 0.05392410233616829, val loss: 0.09841250628232956\n",
      "Epoch 699: train loss: 0.05336892977356911, val loss: 0.08352918922901154\n",
      "Epoch 700: train loss: 0.05631621181964874, val loss: 0.07667698711156845\n",
      "Epoch 701: train loss: 0.05358690768480301, val loss: 0.1025296002626419\n",
      "Epoch 702: train loss: 0.04740992188453674, val loss: 0.08190476149320602\n",
      "Epoch 703: train loss: 0.051387641578912735, val loss: 0.08137499541044235\n",
      "Epoch 704: train loss: 0.04527654871344566, val loss: 0.0918518677353859\n",
      "Epoch 705: train loss: 0.052288543432950974, val loss: 0.09096696972846985\n",
      "Epoch 706: train loss: 0.06308179348707199, val loss: 0.07109319418668747\n",
      "Epoch 707: train loss: 0.059451717883348465, val loss: 0.11918666213750839\n",
      "Epoch 708: train loss: 0.05499810352921486, val loss: 0.11344072967767715\n",
      "Epoch 709: train loss: 0.04915487766265869, val loss: 0.1062360554933548\n",
      "Epoch 710: train loss: 0.054370298981666565, val loss: 0.07179036736488342\n",
      "Epoch 711: train loss: 0.04967178776860237, val loss: 0.08153694868087769\n",
      "Epoch 712: train loss: 0.04097699001431465, val loss: 0.07758438587188721\n",
      "Epoch 713: train loss: 0.04303208738565445, val loss: 0.08701955527067184\n",
      "Epoch 714: train loss: 0.04314274340867996, val loss: 0.10781190544366837\n",
      "Epoch 715: train loss: 0.04827749729156494, val loss: 0.09942116588354111\n",
      "Epoch 716: train loss: 0.054599519819021225, val loss: 0.09893857687711716\n",
      "Epoch 717: train loss: 0.04864586889743805, val loss: 0.0683058500289917\n",
      "Epoch 718: train loss: 0.053490184247493744, val loss: 0.0986107662320137\n",
      "Epoch 719: train loss: 0.051749229431152344, val loss: 0.04924094304442406\n",
      "Epoch 720: train loss: 0.045735932886600494, val loss: 0.06202542781829834\n",
      "Epoch 721: train loss: 0.049730826169252396, val loss: 0.0666700080037117\n",
      "Epoch 722: train loss: 0.05403158441185951, val loss: 0.11052770912647247\n",
      "Epoch 723: train loss: 0.04796478524804115, val loss: 0.0864139124751091\n",
      "Epoch 724: train loss: 0.05020665004849434, val loss: 0.11539477109909058\n",
      "Epoch 725: train loss: 0.055563945323228836, val loss: 0.09683815389871597\n",
      "Epoch 726: train loss: 0.06092941015958786, val loss: 0.09401128441095352\n",
      "Epoch 727: train loss: 0.050241079181432724, val loss: 0.095347560942173\n",
      "Epoch 728: train loss: 0.04458942636847496, val loss: 0.052215367555618286\n",
      "Epoch 729: train loss: 0.05374683439731598, val loss: 0.06493577361106873\n",
      "Epoch 730: train loss: 0.05830863490700722, val loss: 0.09997144341468811\n",
      "Epoch 731: train loss: 0.0526181124150753, val loss: 0.09801321476697922\n",
      "Epoch 732: train loss: 0.04974326118826866, val loss: 0.08523014932870865\n",
      "Epoch 733: train loss: 0.04698755592107773, val loss: 0.07735718041658401\n",
      "Epoch 734: train loss: 0.0505298413336277, val loss: 0.10002896934747696\n",
      "Epoch 735: train loss: 0.048914190381765366, val loss: 0.06943609565496445\n",
      "Epoch 736: train loss: 0.05715421214699745, val loss: 0.05467259883880615\n",
      "Epoch 737: train loss: 0.053992029279470444, val loss: 0.07654930651187897\n",
      "Epoch 738: train loss: 0.05151017755270004, val loss: 0.097553551197052\n",
      "Epoch 739: train loss: 0.047135353088378906, val loss: 0.13531112670898438\n",
      "Epoch 740: train loss: 0.06902004778385162, val loss: 0.08754706382751465\n",
      "Epoch 741: train loss: 0.052340567111968994, val loss: 0.07049655169248581\n",
      "Epoch 742: train loss: 0.05487757921218872, val loss: 0.06707759946584702\n",
      "Epoch 743: train loss: 0.055455565452575684, val loss: 0.07804989814758301\n",
      "Epoch 744: train loss: 0.04668443650007248, val loss: 0.07908309996128082\n",
      "Epoch 745: train loss: 0.04694581776857376, val loss: 0.06662679463624954\n",
      "Epoch 746: train loss: 0.047384243458509445, val loss: 0.09027925878763199\n",
      "Epoch 747: train loss: 0.05413057655096054, val loss: 0.058256279677152634\n",
      "Epoch 748: train loss: 0.05221034213900566, val loss: 0.09819158166646957\n",
      "Epoch 749: train loss: 0.04663975536823273, val loss: 0.06894543766975403\n",
      "Epoch 750: train loss: 0.049171920865774155, val loss: 0.08351045101881027\n",
      "Epoch 751: train loss: 0.04680611565709114, val loss: 0.07287338376045227\n",
      "Epoch 752: train loss: 0.05741135776042938, val loss: 0.13068169355392456\n",
      "Epoch 753: train loss: 0.04857145994901657, val loss: 0.07460514456033707\n",
      "Epoch 754: train loss: 0.05192585289478302, val loss: 0.09050699323415756\n",
      "Epoch 755: train loss: 0.04625190049409866, val loss: 0.10276048630475998\n",
      "Epoch 756: train loss: 0.04814732074737549, val loss: 0.10375570505857468\n",
      "Epoch 757: train loss: 0.045865003019571304, val loss: 0.08730634301900864\n",
      "Epoch 758: train loss: 0.05175197497010231, val loss: 0.059945810586214066\n",
      "Epoch 759: train loss: 0.05070176720619202, val loss: 0.07013919204473495\n",
      "Epoch 760: train loss: 0.05144200846552849, val loss: 0.09594958275556564\n",
      "Epoch 761: train loss: 0.05415308475494385, val loss: 0.08640708029270172\n",
      "Epoch 762: train loss: 0.0472351610660553, val loss: 0.07044366747140884\n",
      "Epoch 763: train loss: 0.04978656768798828, val loss: 0.06597401946783066\n",
      "Epoch 764: train loss: 0.044605553150177, val loss: 0.07173948734998703\n",
      "Epoch 765: train loss: 0.05392739176750183, val loss: 0.07427697628736496\n",
      "Epoch 766: train loss: 0.036693934351205826, val loss: 0.060406364500522614\n",
      "Epoch 767: train loss: 0.0458390899002552, val loss: 0.11595147103071213\n",
      "Epoch 768: train loss: 0.0481419675052166, val loss: 0.05830981954932213\n",
      "Epoch 769: train loss: 0.042944639921188354, val loss: 0.07748325914144516\n",
      "Epoch 770: train loss: 0.047201007604599, val loss: 0.10794011503458023\n",
      "Epoch 771: train loss: 0.045282747596502304, val loss: 0.08158230781555176\n",
      "Epoch 772: train loss: 0.05772773176431656, val loss: 0.10147949308156967\n",
      "Epoch 773: train loss: 0.04653013497591019, val loss: 0.07140367478132248\n",
      "Epoch 774: train loss: 0.047137193381786346, val loss: 0.10562644153833389\n",
      "Epoch 775: train loss: 0.04371330514550209, val loss: 0.06468271464109421\n",
      "Epoch 776: train loss: 0.055238787084817886, val loss: 0.07956374436616898\n",
      "Epoch 777: train loss: 0.04791444167494774, val loss: 0.06859668344259262\n",
      "Epoch 778: train loss: 0.050238918513059616, val loss: 0.11766767501831055\n",
      "Epoch 779: train loss: 0.049861375242471695, val loss: 0.12586741149425507\n",
      "Epoch 780: train loss: 0.04869319126009941, val loss: 0.10365679115056992\n",
      "Epoch 781: train loss: 0.04978040233254433, val loss: 0.0660528838634491\n",
      "Epoch 782: train loss: 0.04277645796537399, val loss: 0.1022874116897583\n",
      "Epoch 783: train loss: 0.04798663407564163, val loss: 0.08131036907434464\n",
      "Epoch 784: train loss: 0.043282218277454376, val loss: 0.08017607778310776\n",
      "Epoch 785: train loss: 0.04762975126504898, val loss: 0.0872846469283104\n",
      "Epoch 786: train loss: 0.0560702346265316, val loss: 0.08603211492300034\n",
      "Epoch 787: train loss: 0.047164466232061386, val loss: 0.09880590438842773\n",
      "Epoch 788: train loss: 0.046865321695804596, val loss: 0.10218226909637451\n",
      "Epoch 789: train loss: 0.05400404706597328, val loss: 0.09812247008085251\n",
      "Epoch 790: train loss: 0.048529595136642456, val loss: 0.09499611705541611\n",
      "Epoch 791: train loss: 0.04796978086233139, val loss: 0.07902213931083679\n",
      "Epoch 792: train loss: 0.04194369539618492, val loss: 0.058869313448667526\n",
      "Epoch 793: train loss: 0.04220433160662651, val loss: 0.06745892018079758\n",
      "Epoch 794: train loss: 0.049575090408325195, val loss: 0.09459485113620758\n",
      "Epoch 795: train loss: 0.05893097072839737, val loss: 0.08336638659238815\n",
      "Epoch 796: train loss: 0.05356723070144653, val loss: 0.11861034482717514\n",
      "Epoch 797: train loss: 0.04693812504410744, val loss: 0.08255936205387115\n",
      "Epoch 798: train loss: 0.04685739055275917, val loss: 0.07323628664016724\n",
      "Epoch 799: train loss: 0.049725864082574844, val loss: 0.0827612653374672\n",
      "Epoch 800: train loss: 0.04548194259405136, val loss: 0.08262434601783752\n",
      "Epoch 801: train loss: 0.05052266642451286, val loss: 0.056377310305833817\n",
      "Epoch 802: train loss: 0.05070003122091293, val loss: 0.09358908981084824\n",
      "Epoch 803: train loss: 0.041954297572374344, val loss: 0.08281601965427399\n",
      "Epoch 804: train loss: 0.05497840419411659, val loss: 0.08615650236606598\n",
      "Epoch 805: train loss: 0.05398569256067276, val loss: 0.08157400041818619\n",
      "Epoch 806: train loss: 0.04873734712600708, val loss: 0.08085012435913086\n",
      "Epoch 807: train loss: 0.05511606112122536, val loss: 0.08344481140375137\n",
      "Epoch 808: train loss: 0.05305064469575882, val loss: 0.077226921916008\n",
      "Epoch 809: train loss: 0.057327792048454285, val loss: 0.08116400241851807\n",
      "Epoch 810: train loss: 0.04323289915919304, val loss: 0.08596079051494598\n",
      "Epoch 811: train loss: 0.04956623166799545, val loss: 0.08078646659851074\n",
      "Epoch 812: train loss: 0.05730587616562843, val loss: 0.08114001899957657\n",
      "Epoch 813: train loss: 0.056383971124887466, val loss: 0.10348784923553467\n",
      "Epoch 814: train loss: 0.05291067436337471, val loss: 0.11658857017755508\n",
      "Epoch 815: train loss: 0.05447746068239212, val loss: 0.07556896656751633\n",
      "Epoch 816: train loss: 0.0467195138335228, val loss: 0.12594692409038544\n",
      "Epoch 817: train loss: 0.04665890336036682, val loss: 0.12298556417226791\n",
      "Epoch 818: train loss: 0.05214302986860275, val loss: 0.09350550174713135\n",
      "Epoch 819: train loss: 0.047979481518268585, val loss: 0.10237755626440048\n",
      "Epoch 820: train loss: 0.04748547449707985, val loss: 0.0796874463558197\n",
      "Epoch 821: train loss: 0.06084795296192169, val loss: 0.1316247582435608\n",
      "Epoch 822: train loss: 0.051363036036491394, val loss: 0.11214064806699753\n",
      "Epoch 823: train loss: 0.05737987160682678, val loss: 0.08282151818275452\n",
      "Epoch 824: train loss: 0.04940878599882126, val loss: 0.0831717699766159\n",
      "Epoch 825: train loss: 0.058799199759960175, val loss: 0.08135180920362473\n",
      "Epoch 826: train loss: 0.04955562204122543, val loss: 0.09557286649942398\n",
      "Epoch 827: train loss: 0.04743476212024689, val loss: 0.07401486486196518\n",
      "Epoch 828: train loss: 0.05333384498953819, val loss: 0.07111182063817978\n",
      "Epoch 829: train loss: 0.054085832089185715, val loss: 0.1022665724158287\n",
      "Epoch 830: train loss: 0.04915858060121536, val loss: 0.07131489366292953\n",
      "Epoch 831: train loss: 0.04619517922401428, val loss: 0.07577315717935562\n",
      "Epoch 832: train loss: 0.04907361418008804, val loss: 0.07435392588376999\n",
      "Epoch 833: train loss: 0.0477171316742897, val loss: 0.10316324234008789\n",
      "Epoch 834: train loss: 0.0407591313123703, val loss: 0.09565773606300354\n",
      "Epoch 835: train loss: 0.05158470198512077, val loss: 0.1048213467001915\n",
      "Epoch 836: train loss: 0.04125659167766571, val loss: 0.09679257869720459\n",
      "Epoch 837: train loss: 0.04526623710989952, val loss: 0.09782005846500397\n",
      "Epoch 838: train loss: 0.04852929338812828, val loss: 0.10060322284698486\n",
      "Epoch 839: train loss: 0.04701414704322815, val loss: 0.08511241525411606\n",
      "Epoch 840: train loss: 0.05129016935825348, val loss: 0.095051109790802\n",
      "Epoch 841: train loss: 0.05137743055820465, val loss: 0.048193808645009995\n",
      "Epoch 842: train loss: 0.04614274948835373, val loss: 0.09033330529928207\n",
      "Epoch 843: train loss: 0.048934757709503174, val loss: 0.11473830789327621\n",
      "Epoch 844: train loss: 0.04908275604248047, val loss: 0.07497534900903702\n",
      "Epoch 845: train loss: 0.04058971628546715, val loss: 0.08150682598352432\n",
      "Epoch 846: train loss: 0.05266944319009781, val loss: 0.06048957258462906\n",
      "Epoch 847: train loss: 0.04383208230137825, val loss: 0.06906523555517197\n",
      "Epoch 848: train loss: 0.047327637672424316, val loss: 0.07856737822294235\n",
      "Epoch 849: train loss: 0.04450616240501404, val loss: 0.10382810980081558\n",
      "Epoch 850: train loss: 0.043446335941553116, val loss: 0.09301751852035522\n",
      "Epoch 851: train loss: 0.05894659087061882, val loss: 0.10509663075208664\n",
      "Epoch 852: train loss: 0.04585721716284752, val loss: 0.09419702738523483\n",
      "Epoch 853: train loss: 0.054121751338243484, val loss: 0.1125941276550293\n",
      "Epoch 854: train loss: 0.04250223934650421, val loss: 0.0858931764960289\n",
      "Epoch 855: train loss: 0.056862618774175644, val loss: 0.09762445092201233\n",
      "Epoch 856: train loss: 0.055836789309978485, val loss: 0.09305961430072784\n",
      "Epoch 857: train loss: 0.04395333677530289, val loss: 0.12755893170833588\n",
      "Epoch 858: train loss: 0.050236597657203674, val loss: 0.1065877228975296\n",
      "Epoch 859: train loss: 0.05987170711159706, val loss: 0.09392505139112473\n",
      "Epoch 860: train loss: 0.04802227020263672, val loss: 0.07449512928724289\n",
      "Epoch 861: train loss: 0.054719746112823486, val loss: 0.08822741359472275\n",
      "Epoch 862: train loss: 0.0420924611389637, val loss: 0.06408794969320297\n",
      "Epoch 863: train loss: 0.05365148186683655, val loss: 0.08880045264959335\n",
      "Epoch 864: train loss: 0.05672799423336983, val loss: 0.11690797656774521\n",
      "Epoch 865: train loss: 0.05329330265522003, val loss: 0.06783405691385269\n",
      "Epoch 866: train loss: 0.04764248803257942, val loss: 0.08765705674886703\n",
      "Epoch 867: train loss: 0.0416719950735569, val loss: 0.09528215229511261\n",
      "Epoch 868: train loss: 0.041184332221746445, val loss: 0.11787300556898117\n",
      "Epoch 869: train loss: 0.051597777754068375, val loss: 0.09739135205745697\n",
      "Epoch 870: train loss: 0.04998449236154556, val loss: 0.0925573855638504\n",
      "Epoch 871: train loss: 0.04792024940252304, val loss: 0.14662957191467285\n",
      "Epoch 872: train loss: 0.04907163605093956, val loss: 0.10763581097126007\n",
      "Epoch 873: train loss: 0.045271482318639755, val loss: 0.08677656203508377\n",
      "Epoch 874: train loss: 0.03831181302666664, val loss: 0.055540721863508224\n",
      "Epoch 875: train loss: 0.04342501237988472, val loss: 0.051275987178087234\n",
      "Epoch 876: train loss: 0.041672904044389725, val loss: 0.11031562089920044\n",
      "Epoch 877: train loss: 0.04429211840033531, val loss: 0.06555169820785522\n",
      "Epoch 878: train loss: 0.04194733500480652, val loss: 0.13263407349586487\n",
      "Epoch 879: train loss: 0.04985986277461052, val loss: 0.07713302969932556\n",
      "Epoch 880: train loss: 0.045666202902793884, val loss: 0.06300479173660278\n",
      "Epoch 881: train loss: 0.040482647716999054, val loss: 0.10510315746068954\n",
      "Epoch 882: train loss: 0.03725996986031532, val loss: 0.09713824093341827\n",
      "Epoch 883: train loss: 0.037866830825805664, val loss: 0.08703682571649551\n",
      "Epoch 884: train loss: 0.04862448573112488, val loss: 0.08218268305063248\n",
      "Epoch 885: train loss: 0.04630538076162338, val loss: 0.08108119666576385\n",
      "Epoch 886: train loss: 0.038866110146045685, val loss: 0.048104654997587204\n",
      "Epoch 887: train loss: 0.04995354637503624, val loss: 0.10494408756494522\n",
      "Epoch 888: train loss: 0.04177301749587059, val loss: 0.11486782878637314\n",
      "Epoch 889: train loss: 0.046199649572372437, val loss: 0.06839801371097565\n",
      "Epoch 890: train loss: 0.04381647706031799, val loss: 0.07943005859851837\n",
      "Epoch 891: train loss: 0.04454825446009636, val loss: 0.05367637798190117\n",
      "Epoch 892: train loss: 0.04132726415991783, val loss: 0.08373203128576279\n",
      "Epoch 893: train loss: 0.04973195493221283, val loss: 0.07151156663894653\n",
      "Epoch 894: train loss: 0.04778112471103668, val loss: 0.07407454401254654\n",
      "Epoch 895: train loss: 0.05612285062670708, val loss: 0.08288019895553589\n",
      "Epoch 896: train loss: 0.05397316813468933, val loss: 0.0795115977525711\n",
      "Epoch 897: train loss: 0.04928593710064888, val loss: 0.09719877690076828\n",
      "Epoch 898: train loss: 0.039232440292835236, val loss: 0.09895969927310944\n",
      "Epoch 899: train loss: 0.059397101402282715, val loss: 0.10299112647771835\n",
      "Epoch 900: train loss: 0.054754357784986496, val loss: 0.11994018405675888\n",
      "Epoch 901: train loss: 0.04904429242014885, val loss: 0.0742371529340744\n",
      "Epoch 902: train loss: 0.04779665917158127, val loss: 0.07667668163776398\n",
      "Epoch 903: train loss: 0.051676586270332336, val loss: 0.09712617844343185\n",
      "Epoch 904: train loss: 0.05345103144645691, val loss: 0.10044202953577042\n",
      "Epoch 905: train loss: 0.0469488687813282, val loss: 0.07247089594602585\n",
      "Epoch 906: train loss: 0.04324193298816681, val loss: 0.09926820546388626\n",
      "Epoch 907: train loss: 0.050939999520778656, val loss: 0.0820111557841301\n",
      "Epoch 908: train loss: 0.05163697898387909, val loss: 0.12391989678144455\n",
      "Epoch 909: train loss: 0.056291837245225906, val loss: 0.09525718539953232\n",
      "Epoch 910: train loss: 0.054270341992378235, val loss: 0.08055116981267929\n",
      "Epoch 911: train loss: 0.047373514622449875, val loss: 0.08925538510084152\n",
      "Epoch 912: train loss: 0.04715850576758385, val loss: 0.06519129127264023\n",
      "Epoch 913: train loss: 0.04545196518301964, val loss: 0.08172593265771866\n",
      "Epoch 914: train loss: 0.05003729090094566, val loss: 0.07779427617788315\n",
      "Epoch 915: train loss: 0.05358562991023064, val loss: 0.07196589559316635\n",
      "Epoch 916: train loss: 0.05702424794435501, val loss: 0.11647825688123703\n",
      "Epoch 917: train loss: 0.04863452911376953, val loss: 0.09136541932821274\n",
      "Epoch 918: train loss: 0.05914851278066635, val loss: 0.07207395881414413\n",
      "Epoch 919: train loss: 0.05408687889575958, val loss: 0.0754183679819107\n",
      "Epoch 920: train loss: 0.05062105506658554, val loss: 0.06929263472557068\n",
      "Epoch 921: train loss: 0.05092548951506615, val loss: 0.08597905933856964\n",
      "Epoch 922: train loss: 0.0495288111269474, val loss: 0.09756521135568619\n",
      "Epoch 923: train loss: 0.05328914523124695, val loss: 0.09926803410053253\n",
      "Epoch 924: train loss: 0.053351953625679016, val loss: 0.10647695511579514\n",
      "Epoch 925: train loss: 0.05364569276571274, val loss: 0.09588988125324249\n",
      "Epoch 926: train loss: 0.03851170465350151, val loss: 0.10308264940977097\n",
      "Epoch 927: train loss: 0.038678232580423355, val loss: 0.10686137527227402\n",
      "Epoch 928: train loss: 0.052361562848091125, val loss: 0.10365261137485504\n",
      "Epoch 929: train loss: 0.047043927013874054, val loss: 0.1256633847951889\n",
      "Epoch 930: train loss: 0.044955216348171234, val loss: 0.08859595656394958\n",
      "Epoch 931: train loss: 0.04291268438100815, val loss: 0.08719388395547867\n",
      "Epoch 932: train loss: 0.04077615216374397, val loss: 0.06381344050168991\n",
      "Epoch 933: train loss: 0.04628317803144455, val loss: 0.100777268409729\n",
      "Epoch 934: train loss: 0.04921318218111992, val loss: 0.07424817979335785\n",
      "Epoch 935: train loss: 0.05078456923365593, val loss: 0.058312635868787766\n",
      "Epoch 936: train loss: 0.0476241372525692, val loss: 0.07084103673696518\n",
      "Epoch 937: train loss: 0.04842729866504669, val loss: 0.07194771617650986\n",
      "Epoch 938: train loss: 0.04124040529131889, val loss: 0.08429531008005142\n",
      "Epoch 939: train loss: 0.03804189711809158, val loss: 0.05613070726394653\n",
      "Epoch 940: train loss: 0.04377363249659538, val loss: 0.09097997844219208\n",
      "Epoch 941: train loss: 0.04651450738310814, val loss: 0.09320821613073349\n",
      "Epoch 942: train loss: 0.04108918458223343, val loss: 0.07437251508235931\n",
      "Epoch 943: train loss: 0.0496487095952034, val loss: 0.11492490023374557\n",
      "Epoch 944: train loss: 0.046804655343294144, val loss: 0.10826420038938522\n",
      "Epoch 945: train loss: 0.04097897186875343, val loss: 0.0921822115778923\n",
      "Epoch 946: train loss: 0.04739106446504593, val loss: 0.1065429076552391\n",
      "Epoch 947: train loss: 0.048504576086997986, val loss: 0.1158231720328331\n",
      "Epoch 948: train loss: 0.03794340044260025, val loss: 0.09617265313863754\n",
      "Epoch 949: train loss: 0.04386936500668526, val loss: 0.08915575593709946\n",
      "Epoch 950: train loss: 0.04197334870696068, val loss: 0.07762134075164795\n",
      "Epoch 951: train loss: 0.03994549438357353, val loss: 0.10304760932922363\n",
      "Epoch 952: train loss: 0.04928309842944145, val loss: 0.09234132617712021\n",
      "Epoch 953: train loss: 0.04678725078701973, val loss: 0.08929309993982315\n",
      "Epoch 954: train loss: 0.044494301080703735, val loss: 0.09337764233350754\n",
      "Epoch 955: train loss: 0.04242919012904167, val loss: 0.10792676359415054\n",
      "Epoch 956: train loss: 0.05754037946462631, val loss: 0.09438882768154144\n",
      "Epoch 957: train loss: 0.050182852894067764, val loss: 0.12491227686405182\n",
      "Epoch 958: train loss: 0.05170440673828125, val loss: 0.0737837478518486\n",
      "Epoch 959: train loss: 0.06047670543193817, val loss: 0.08246864378452301\n",
      "Epoch 960: train loss: 0.050168219953775406, val loss: 0.07578586786985397\n",
      "Epoch 961: train loss: 0.05420329049229622, val loss: 0.09127344191074371\n",
      "Epoch 962: train loss: 0.05017567425966263, val loss: 0.08555945008993149\n",
      "Epoch 963: train loss: 0.051616109907627106, val loss: 0.09665533155202866\n",
      "Epoch 964: train loss: 0.04692943021655083, val loss: 0.08333505690097809\n",
      "Epoch 965: train loss: 0.05175676569342613, val loss: 0.08585753291845322\n",
      "Epoch 966: train loss: 0.04565075784921646, val loss: 0.09827514737844467\n",
      "Epoch 967: train loss: 0.04822135716676712, val loss: 0.07925887405872345\n",
      "Epoch 968: train loss: 0.04579809680581093, val loss: 0.08026915788650513\n",
      "Epoch 969: train loss: 0.05267002433538437, val loss: 0.08785602450370789\n",
      "Epoch 970: train loss: 0.049182817339897156, val loss: 0.11348666995763779\n",
      "Epoch 971: train loss: 0.04494685307145119, val loss: 0.09061279147863388\n",
      "Epoch 972: train loss: 0.04973196983337402, val loss: 0.10182899236679077\n",
      "Epoch 973: train loss: 0.04492989927530289, val loss: 0.08725135773420334\n",
      "Epoch 974: train loss: 0.04513741657137871, val loss: 0.10196318477392197\n",
      "Epoch 975: train loss: 0.04612799733877182, val loss: 0.10786803066730499\n",
      "Epoch 976: train loss: 0.052180513739585876, val loss: 0.11290564388036728\n",
      "Epoch 977: train loss: 0.045552223920822144, val loss: 0.10767896473407745\n",
      "Epoch 978: train loss: 0.04944164678454399, val loss: 0.08613302558660507\n",
      "Epoch 979: train loss: 0.06197066977620125, val loss: 0.07771610468626022\n",
      "Epoch 980: train loss: 0.04675411805510521, val loss: 0.07840172201395035\n",
      "Epoch 981: train loss: 0.046341992914676666, val loss: 0.0709243193268776\n",
      "Epoch 982: train loss: 0.057387109845876694, val loss: 0.08262362331151962\n",
      "Epoch 983: train loss: 0.045262232422828674, val loss: 0.10229531675577164\n",
      "Epoch 984: train loss: 0.041821204125881195, val loss: 0.0648733600974083\n",
      "Epoch 985: train loss: 0.05754207819700241, val loss: 0.0716143250465393\n",
      "Epoch 986: train loss: 0.04114273563027382, val loss: 0.0809163972735405\n",
      "Epoch 987: train loss: 0.05200370401144028, val loss: 0.10471650213003159\n",
      "Epoch 988: train loss: 0.04574066027998924, val loss: 0.10468193143606186\n",
      "Epoch 989: train loss: 0.04257291182875633, val loss: 0.07898371666669846\n",
      "Epoch 990: train loss: 0.049907244741916656, val loss: 0.09158319234848022\n",
      "Epoch 991: train loss: 0.03956897184252739, val loss: 0.09637816995382309\n",
      "Epoch 992: train loss: 0.04771391674876213, val loss: 0.10678404569625854\n",
      "Epoch 993: train loss: 0.0445040725171566, val loss: 0.07993859797716141\n",
      "Epoch 994: train loss: 0.049991630017757416, val loss: 0.0977272167801857\n",
      "Epoch 995: train loss: 0.039029110223054886, val loss: 0.09648263454437256\n",
      "Epoch 996: train loss: 0.05160597339272499, val loss: 0.10622622817754745\n",
      "Epoch 997: train loss: 0.05738303065299988, val loss: 0.10097736120223999\n",
      "Epoch 998: train loss: 0.046284839510917664, val loss: 0.07781614363193512\n",
      "Epoch 999: train loss: 0.050435036420822144, val loss: 0.06773710250854492\n",
      "Epoch 1000: train loss: 0.043396253138780594, val loss: 0.08279450982809067\n",
      "Epoch 1001: train loss: 0.05027363821864128, val loss: 0.07877375930547714\n",
      "Epoch 1002: train loss: 0.04813385009765625, val loss: 0.10841679573059082\n",
      "Epoch 1003: train loss: 0.054083798080682755, val loss: 0.09404287487268448\n",
      "Epoch 1004: train loss: 0.04742363840341568, val loss: 0.057983845472335815\n",
      "Epoch 1005: train loss: 0.047218047082424164, val loss: 0.08689841628074646\n",
      "Epoch 1006: train loss: 0.04253004863858223, val loss: 0.06317012757062912\n",
      "Epoch 1007: train loss: 0.0527750663459301, val loss: 0.0928397998213768\n",
      "Epoch 1008: train loss: 0.05786539986729622, val loss: 0.07613082975149155\n",
      "Epoch 1009: train loss: 0.05568603053689003, val loss: 0.06871157139539719\n",
      "Epoch 1010: train loss: 0.06054198741912842, val loss: 0.04652455076575279\n",
      "Epoch 1011: train loss: 0.04893786832690239, val loss: 0.09803444147109985\n",
      "Epoch 1012: train loss: 0.05299513041973114, val loss: 0.131862074136734\n",
      "Epoch 1013: train loss: 0.04487232118844986, val loss: 0.10480590164661407\n",
      "Epoch 1014: train loss: 0.042261406779289246, val loss: 0.06507193297147751\n",
      "Epoch 1015: train loss: 0.043775565922260284, val loss: 0.09020038694143295\n",
      "Epoch 1016: train loss: 0.05055820941925049, val loss: 0.06436546891927719\n",
      "Epoch 1017: train loss: 0.036092609167099, val loss: 0.05844951421022415\n",
      "Epoch 1018: train loss: 0.04533855617046356, val loss: 0.09649132937192917\n",
      "Epoch 1019: train loss: 0.041855622082948685, val loss: 0.08940938860177994\n",
      "Epoch 1020: train loss: 0.039597317576408386, val loss: 0.09386038780212402\n",
      "Epoch 1021: train loss: 0.04583948105573654, val loss: 0.09879333525896072\n",
      "Epoch 1022: train loss: 0.04993775859475136, val loss: 0.08195702731609344\n",
      "Epoch 1023: train loss: 0.049581509083509445, val loss: 0.09458837658166885\n",
      "Epoch 1024: train loss: 0.04146648570895195, val loss: 0.09569375962018967\n",
      "Epoch 1025: train loss: 0.040124718099832535, val loss: 0.07962359488010406\n",
      "Epoch 1026: train loss: 0.0391898974776268, val loss: 0.08438219875097275\n",
      "Epoch 1027: train loss: 0.049572091549634933, val loss: 0.09196885675191879\n",
      "Epoch 1028: train loss: 0.04806514456868172, val loss: 0.0817878395318985\n",
      "Epoch 1029: train loss: 0.042139679193496704, val loss: 0.09764512628316879\n",
      "Epoch 1030: train loss: 0.04722407087683678, val loss: 0.10316134989261627\n",
      "Epoch 1031: train loss: 0.03845331072807312, val loss: 0.09245121479034424\n",
      "Epoch 1032: train loss: 0.04907079041004181, val loss: 0.07210031896829605\n",
      "Epoch 1033: train loss: 0.04334151744842529, val loss: 0.09597595781087875\n",
      "Epoch 1034: train loss: 0.044758379459381104, val loss: 0.10348117351531982\n",
      "Epoch 1035: train loss: 0.04028145596385002, val loss: 0.11445688456296921\n",
      "Epoch 1036: train loss: 0.04976958408951759, val loss: 0.08793037384748459\n",
      "Epoch 1037: train loss: 0.050240349024534225, val loss: 0.10326609760522842\n",
      "Epoch 1038: train loss: 0.04864668846130371, val loss: 0.08842658996582031\n",
      "Epoch 1039: train loss: 0.03645225614309311, val loss: 0.07406540215015411\n",
      "Epoch 1040: train loss: 0.04802573472261429, val loss: 0.14893634617328644\n",
      "Epoch 1041: train loss: 0.04670534282922745, val loss: 0.15195779502391815\n",
      "Epoch 1042: train loss: 0.05942321568727493, val loss: 0.11509990692138672\n",
      "Epoch 1043: train loss: 0.0605386383831501, val loss: 0.11565382778644562\n",
      "Epoch 1044: train loss: 0.050213225185871124, val loss: 0.09283974021673203\n",
      "Epoch 1045: train loss: 0.04760819673538208, val loss: 0.07029222697019577\n",
      "Epoch 1046: train loss: 0.05153093487024307, val loss: 0.07953321933746338\n",
      "Epoch 1047: train loss: 0.04847332835197449, val loss: 0.052572477608919144\n",
      "Epoch 1048: train loss: 0.04874369502067566, val loss: 0.08460625261068344\n",
      "Epoch 1049: train loss: 0.04596241936087608, val loss: 0.09021767228841782\n",
      "Epoch 1050: train loss: 0.055648207664489746, val loss: 0.07708390802145004\n",
      "Epoch 1051: train loss: 0.05635073035955429, val loss: 0.09297048300504684\n",
      "Epoch 1052: train loss: 0.04698776826262474, val loss: 0.07549647241830826\n",
      "Epoch 1053: train loss: 0.05309136211872101, val loss: 0.0936615988612175\n",
      "Epoch 1054: train loss: 0.052234046161174774, val loss: 0.12790712714195251\n",
      "Epoch 1055: train loss: 0.04592214897274971, val loss: 0.09902173280715942\n",
      "Epoch 1056: train loss: 0.048827532678842545, val loss: 0.08811628818511963\n",
      "Epoch 1057: train loss: 0.04042982682585716, val loss: 0.09348775446414948\n",
      "Epoch 1058: train loss: 0.04611882194876671, val loss: 0.08198200166225433\n",
      "Epoch 1059: train loss: 0.053157880902290344, val loss: 0.08442062139511108\n",
      "Epoch 1060: train loss: 0.04852075129747391, val loss: 0.10706157982349396\n",
      "Epoch 1061: train loss: 0.037069760262966156, val loss: 0.06424109637737274\n",
      "Epoch 1062: train loss: 0.04899337515234947, val loss: 0.07663507014513016\n",
      "Epoch 1063: train loss: 0.055200036615133286, val loss: 0.07248225063085556\n",
      "Epoch 1064: train loss: 0.04396248608827591, val loss: 0.09076284617185593\n",
      "Epoch 1065: train loss: 0.0409473180770874, val loss: 0.08351925760507584\n",
      "Epoch 1066: train loss: 0.04515231028199196, val loss: 0.0892237201333046\n",
      "Epoch 1067: train loss: 0.04649019241333008, val loss: 0.09523457288742065\n",
      "Epoch 1068: train loss: 0.053124163299798965, val loss: 0.13312751054763794\n",
      "Epoch 1069: train loss: 0.043303247541189194, val loss: 0.08958252519369125\n",
      "Epoch 1070: train loss: 0.04989602416753769, val loss: 0.10556458681821823\n",
      "Epoch 1071: train loss: 0.04414742439985275, val loss: 0.08649658411741257\n",
      "Epoch 1072: train loss: 0.044464148581027985, val loss: 0.11222254484891891\n",
      "Epoch 1073: train loss: 0.04305681586265564, val loss: 0.1112004891037941\n",
      "Epoch 1074: train loss: 0.041533660143613815, val loss: 0.07159923017024994\n",
      "Epoch 1075: train loss: 0.05428910255432129, val loss: 0.06208108738064766\n",
      "Epoch 1076: train loss: 0.0437421053647995, val loss: 0.07977180927991867\n",
      "Epoch 1077: train loss: 0.04842584952712059, val loss: 0.08847615122795105\n",
      "Epoch 1078: train loss: 0.046111226081848145, val loss: 0.09457244724035263\n",
      "Epoch 1079: train loss: 0.042483989149332047, val loss: 0.06686919182538986\n",
      "Epoch 1080: train loss: 0.039174772799015045, val loss: 0.07215487211942673\n",
      "Epoch 1081: train loss: 0.046982161700725555, val loss: 0.07321762293577194\n",
      "Epoch 1082: train loss: 0.040272362530231476, val loss: 0.07411094754934311\n",
      "Epoch 1083: train loss: 0.03966793045401573, val loss: 0.07751157879829407\n",
      "Epoch 1084: train loss: 0.046277403831481934, val loss: 0.10293847322463989\n",
      "Epoch 1085: train loss: 0.04582827538251877, val loss: 0.06504752486944199\n",
      "Epoch 1086: train loss: 0.04778794199228287, val loss: 0.08796558529138565\n",
      "Epoch 1087: train loss: 0.03999077156186104, val loss: 0.08420641720294952\n",
      "Epoch 1088: train loss: 0.03632368892431259, val loss: 0.05041520670056343\n",
      "Epoch 1089: train loss: 0.04714098572731018, val loss: 0.06079542636871338\n",
      "Epoch 1090: train loss: 0.03656281158328056, val loss: 0.06699002534151077\n",
      "Epoch 1091: train loss: 0.045517031103372574, val loss: 0.08493431657552719\n",
      "Epoch 1092: train loss: 0.05376303941011429, val loss: 0.10541119426488876\n",
      "Epoch 1093: train loss: 0.05253947153687477, val loss: 0.10143562406301498\n",
      "Epoch 1094: train loss: 0.04528561979532242, val loss: 0.10832424461841583\n",
      "Epoch 1095: train loss: 0.050648774951696396, val loss: 0.08365901559591293\n",
      "Epoch 1096: train loss: 0.04521447792649269, val loss: 0.07279752939939499\n",
      "Epoch 1097: train loss: 0.04650828242301941, val loss: 0.08719026297330856\n",
      "Epoch 1098: train loss: 0.044151708483695984, val loss: 0.09037178754806519\n",
      "Epoch 1099: train loss: 0.048017896711826324, val loss: 0.08728821575641632\n",
      "Epoch 1100: train loss: 0.042828287929296494, val loss: 0.12053364515304565\n",
      "Epoch 1101: train loss: 0.03992198035120964, val loss: 0.0703379213809967\n",
      "Epoch 1102: train loss: 0.04914745315909386, val loss: 0.07818865031003952\n",
      "Epoch 1103: train loss: 0.04275181517004967, val loss: 0.06942453235387802\n",
      "Epoch 1104: train loss: 0.03953332453966141, val loss: 0.08598913997411728\n",
      "Epoch 1105: train loss: 0.03648727759718895, val loss: 0.08911122381687164\n",
      "Epoch 1106: train loss: 0.04827464371919632, val loss: 0.07935618609189987\n",
      "Epoch 1107: train loss: 0.044438742101192474, val loss: 0.0750674158334732\n",
      "Epoch 1108: train loss: 0.05258886516094208, val loss: 0.07062884420156479\n",
      "Epoch 1109: train loss: 0.04428274556994438, val loss: 0.11652245372533798\n",
      "Epoch 1110: train loss: 0.04881203919649124, val loss: 0.12715208530426025\n",
      "Epoch 1111: train loss: 0.05481529235839844, val loss: 0.10623849928379059\n",
      "Epoch 1112: train loss: 0.04978291317820549, val loss: 0.08052212744951248\n",
      "Epoch 1113: train loss: 0.04615725576877594, val loss: 0.09796571731567383\n",
      "Epoch 1114: train loss: 0.04398383945226669, val loss: 0.10109873861074448\n",
      "Epoch 1115: train loss: 0.04542037099599838, val loss: 0.1050359234213829\n",
      "Epoch 1116: train loss: 0.04119178652763367, val loss: 0.10688773542642593\n",
      "Epoch 1117: train loss: 0.049219176173210144, val loss: 0.0818549394607544\n",
      "Epoch 1118: train loss: 0.046838440001010895, val loss: 0.1213669553399086\n",
      "Epoch 1119: train loss: 0.047667212784290314, val loss: 0.07836220413446426\n",
      "Epoch 1120: train loss: 0.045339155942201614, val loss: 0.09113083779811859\n",
      "Epoch 1121: train loss: 0.04878407344222069, val loss: 0.06955588608980179\n",
      "Epoch 1122: train loss: 0.04272132366895676, val loss: 0.12260174751281738\n",
      "Epoch 1123: train loss: 0.04982432350516319, val loss: 0.11205919831991196\n",
      "Epoch 1124: train loss: 0.04958290234208107, val loss: 0.09509634226560593\n",
      "Epoch 1125: train loss: 0.05162151902914047, val loss: 0.11654925346374512\n",
      "Epoch 1126: train loss: 0.04799490049481392, val loss: 0.08417610079050064\n",
      "Epoch 1127: train loss: 0.040197812020778656, val loss: 0.09041028469800949\n",
      "Epoch 1128: train loss: 0.04391631484031677, val loss: 0.0775868147611618\n",
      "Epoch 1129: train loss: 0.04377752169966698, val loss: 0.09939591586589813\n",
      "Epoch 1130: train loss: 0.04584931209683418, val loss: 0.08117695897817612\n",
      "Epoch 1131: train loss: 0.045438364148139954, val loss: 0.0993727594614029\n",
      "Epoch 1132: train loss: 0.04931318014860153, val loss: 0.08034852892160416\n",
      "Epoch 1133: train loss: 0.04446416720747948, val loss: 0.08771901577711105\n",
      "Epoch 1134: train loss: 0.044858742505311966, val loss: 0.05106986314058304\n",
      "Epoch 1135: train loss: 0.05401644855737686, val loss: 0.063175730407238\n",
      "Epoch 1136: train loss: 0.052814919501543045, val loss: 0.11170624941587448\n",
      "Epoch 1137: train loss: 0.04922368749976158, val loss: 0.1138533502817154\n",
      "Epoch 1138: train loss: 0.05558134242892265, val loss: 0.09694252163171768\n",
      "Epoch 1139: train loss: 0.045702628791332245, val loss: 0.1094689592719078\n",
      "Epoch 1140: train loss: 0.04928971454501152, val loss: 0.11122699081897736\n",
      "Epoch 1141: train loss: 0.044657908380031586, val loss: 0.09112434089183807\n",
      "Epoch 1142: train loss: 0.04910722374916077, val loss: 0.08273676782846451\n",
      "Epoch 1143: train loss: 0.0398838184773922, val loss: 0.08472941070795059\n",
      "Epoch 1144: train loss: 0.04932467266917229, val loss: 0.10359852761030197\n",
      "Epoch 1145: train loss: 0.04858916252851486, val loss: 0.07944249361753464\n",
      "Epoch 1146: train loss: 0.04383239150047302, val loss: 0.08365096151828766\n",
      "Epoch 1147: train loss: 0.043543741106987, val loss: 0.08195244520902634\n",
      "Epoch 1148: train loss: 0.04765492305159569, val loss: 0.0892300009727478\n",
      "Epoch 1149: train loss: 0.052879758179187775, val loss: 0.10480030626058578\n",
      "Epoch 1150: train loss: 0.03821234405040741, val loss: 0.10485948622226715\n",
      "Epoch 1151: train loss: 0.03917921334505081, val loss: 0.13474462926387787\n",
      "Epoch 1152: train loss: 0.035305511206388474, val loss: 0.08082306385040283\n",
      "Epoch 1153: train loss: 0.04984121397137642, val loss: 0.08568510413169861\n",
      "Epoch 1154: train loss: 0.045705508440732956, val loss: 0.06765072792768478\n",
      "Epoch 1155: train loss: 0.04677028954029083, val loss: 0.08782079070806503\n",
      "Epoch 1156: train loss: 0.0424155555665493, val loss: 0.10667150467634201\n",
      "Epoch 1157: train loss: 0.04335849732160568, val loss: 0.1331293284893036\n",
      "Epoch 1158: train loss: 0.04358565807342529, val loss: 0.06398817151784897\n",
      "Epoch 1159: train loss: 0.04981553927063942, val loss: 0.07860352098941803\n",
      "Epoch 1160: train loss: 0.03609396144747734, val loss: 0.08902955055236816\n",
      "Epoch 1161: train loss: 0.04031930863857269, val loss: 0.08943890780210495\n",
      "Epoch 1162: train loss: 0.047490138560533524, val loss: 0.12736530601978302\n",
      "Epoch 1163: train loss: 0.051682114601135254, val loss: 0.1215028390288353\n",
      "Epoch 1164: train loss: 0.041185036301612854, val loss: 0.06254085153341293\n",
      "Epoch 1165: train loss: 0.04864617437124252, val loss: 0.10671090334653854\n",
      "Epoch 1166: train loss: 0.047144949436187744, val loss: 0.0923914909362793\n",
      "Epoch 1167: train loss: 0.04992595314979553, val loss: 0.10605809837579727\n",
      "Epoch 1168: train loss: 0.050649210810661316, val loss: 0.08374632894992828\n",
      "Epoch 1169: train loss: 0.04137786477804184, val loss: 0.08774135261774063\n",
      "Epoch 1170: train loss: 0.04154040291905403, val loss: 0.10919683426618576\n",
      "Epoch 1171: train loss: 0.05127320811152458, val loss: 0.10449247807264328\n",
      "Epoch 1172: train loss: 0.04408665746450424, val loss: 0.11210497468709946\n",
      "Epoch 1173: train loss: 0.041562456637620926, val loss: 0.11075889319181442\n",
      "Epoch 1174: train loss: 0.05766677483916283, val loss: 0.08141164481639862\n",
      "Epoch 1175: train loss: 0.04398779943585396, val loss: 0.09427057951688766\n",
      "Epoch 1176: train loss: 0.05008212849497795, val loss: 0.10503906011581421\n",
      "Epoch 1177: train loss: 0.041104305535554886, val loss: 0.10021068900823593\n",
      "Epoch 1178: train loss: 0.03693390637636185, val loss: 0.09508160501718521\n",
      "Epoch 1179: train loss: 0.04798530414700508, val loss: 0.1222275048494339\n",
      "Epoch 1180: train loss: 0.05226760730147362, val loss: 0.08874117583036423\n",
      "Epoch 1181: train loss: 0.04479951784014702, val loss: 0.11095122247934341\n",
      "Epoch 1182: train loss: 0.04546868056058884, val loss: 0.08452912420034409\n",
      "Epoch 1183: train loss: 0.05221588537096977, val loss: 0.07288622856140137\n",
      "Epoch 1184: train loss: 0.040987201035022736, val loss: 0.07683859765529633\n",
      "Epoch 1185: train loss: 0.0449753999710083, val loss: 0.08370557427406311\n",
      "Epoch 1186: train loss: 0.04071858525276184, val loss: 0.09075404703617096\n",
      "Epoch 1187: train loss: 0.05439498648047447, val loss: 0.07716982811689377\n",
      "Epoch 1188: train loss: 0.04846229776740074, val loss: 0.12288662046194077\n",
      "Epoch 1189: train loss: 0.04354175180196762, val loss: 0.12501774728298187\n",
      "Epoch 1190: train loss: 0.041628409177064896, val loss: 0.1003866121172905\n",
      "Epoch 1191: train loss: 0.04778248071670532, val loss: 0.09308146685361862\n",
      "Epoch 1192: train loss: 0.04856334254145622, val loss: 0.08972596377134323\n",
      "Epoch 1193: train loss: 0.039369601756334305, val loss: 0.10901365429162979\n",
      "Epoch 1194: train loss: 0.04650420323014259, val loss: 0.12193625420331955\n",
      "Epoch 1195: train loss: 0.03561651334166527, val loss: 0.0484195314347744\n",
      "Epoch 1196: train loss: 0.04563504084944725, val loss: 0.07233390212059021\n",
      "Epoch 1197: train loss: 0.038828056305646896, val loss: 0.1035538911819458\n",
      "Epoch 1198: train loss: 0.04820328950881958, val loss: 0.0893675908446312\n",
      "Epoch 1199: train loss: 0.045584119856357574, val loss: 0.08611397445201874\n",
      "Epoch 1200: train loss: 0.036469899117946625, val loss: 0.09918875992298126\n",
      "Epoch 1201: train loss: 0.03969810530543327, val loss: 0.09236481785774231\n",
      "Epoch 1202: train loss: 0.04834295064210892, val loss: 0.10805147141218185\n",
      "Epoch 1203: train loss: 0.039842888712882996, val loss: 0.11179212480783463\n",
      "Epoch 1204: train loss: 0.046996843069791794, val loss: 0.09055805206298828\n",
      "Epoch 1205: train loss: 0.05436597391963005, val loss: 0.0870373472571373\n",
      "Epoch 1206: train loss: 0.05035436898469925, val loss: 0.10341665893793106\n",
      "Epoch 1207: train loss: 0.04615640640258789, val loss: 0.06151555851101875\n",
      "Epoch 1208: train loss: 0.04674577713012695, val loss: 0.09347888827323914\n",
      "Epoch 1209: train loss: 0.04249788820743561, val loss: 0.09623666107654572\n",
      "Epoch 1210: train loss: 0.04391847923398018, val loss: 0.09233712404966354\n",
      "Epoch 1211: train loss: 0.03846967965364456, val loss: 0.08972282707691193\n",
      "Epoch 1212: train loss: 0.04339911788702011, val loss: 0.11848993599414825\n",
      "Epoch 1213: train loss: 0.04718746617436409, val loss: 0.10630228370428085\n",
      "Epoch 1214: train loss: 0.037720438092947006, val loss: 0.10150120407342911\n",
      "Epoch 1215: train loss: 0.035234712064266205, val loss: 0.06634975969791412\n",
      "Epoch 1216: train loss: 0.0538107231259346, val loss: 0.09998676180839539\n",
      "Epoch 1217: train loss: 0.04281197860836983, val loss: 0.10391151160001755\n",
      "Epoch 1218: train loss: 0.044000186026096344, val loss: 0.12405140697956085\n",
      "Epoch 1219: train loss: 0.04561470076441765, val loss: 0.06664606928825378\n",
      "Epoch 1220: train loss: 0.04477434232831001, val loss: 0.08749395608901978\n",
      "Epoch 1221: train loss: 0.05243450403213501, val loss: 0.07792381197214127\n",
      "Epoch 1222: train loss: 0.03977710008621216, val loss: 0.11332215368747711\n",
      "Epoch 1223: train loss: 0.041122470051050186, val loss: 0.09035719186067581\n",
      "Epoch 1224: train loss: 0.04265974834561348, val loss: 0.07231058925390244\n",
      "Epoch 1225: train loss: 0.04782501980662346, val loss: 0.11682871729135513\n",
      "Epoch 1226: train loss: 0.04373634606599808, val loss: 0.0652972087264061\n",
      "Epoch 1227: train loss: 0.04499082639813423, val loss: 0.06430673599243164\n",
      "Epoch 1228: train loss: 0.040222615003585815, val loss: 0.0767887756228447\n",
      "Epoch 1229: train loss: 0.04234654828906059, val loss: 0.13332206010818481\n",
      "Epoch 1230: train loss: 0.03618040308356285, val loss: 0.12127088755369186\n",
      "Epoch 1231: train loss: 0.043495673686265945, val loss: 0.07494465261697769\n",
      "Epoch 1232: train loss: 0.04194331541657448, val loss: 0.1139722391963005\n",
      "Epoch 1233: train loss: 0.048728447407484055, val loss: 0.11648368835449219\n",
      "Epoch 1234: train loss: 0.04083218798041344, val loss: 0.07332181930541992\n",
      "Epoch 1235: train loss: 0.04345592111349106, val loss: 0.09059084951877594\n",
      "Epoch 1236: train loss: 0.045721109956502914, val loss: 0.07893450558185577\n",
      "Epoch 1237: train loss: 0.0437026284635067, val loss: 0.09435193985700607\n",
      "Epoch 1238: train loss: 0.03371147811412811, val loss: 0.10222368687391281\n",
      "Epoch 1239: train loss: 0.0487048365175724, val loss: 0.10913515090942383\n",
      "Epoch 1240: train loss: 0.04897927865386009, val loss: 0.08963700383901596\n",
      "Epoch 1241: train loss: 0.039428554475307465, val loss: 0.09321113675832748\n",
      "Epoch 1242: train loss: 0.02882947213947773, val loss: 0.10257434099912643\n",
      "Epoch 1243: train loss: 0.031276654452085495, val loss: 0.10014616698026657\n",
      "Epoch 1244: train loss: 0.03414423391222954, val loss: 0.11507735401391983\n",
      "Epoch 1245: train loss: 0.05477871373295784, val loss: 0.07502149790525436\n",
      "Epoch 1246: train loss: 0.03958380967378616, val loss: 0.07632394880056381\n",
      "Epoch 1247: train loss: 0.03827406466007233, val loss: 0.0799408107995987\n",
      "Epoch 1248: train loss: 0.03387882560491562, val loss: 0.10815383493900299\n",
      "Epoch 1249: train loss: 0.04390080273151398, val loss: 0.08390524238348007\n",
      "Epoch 1250: train loss: 0.03279072418808937, val loss: 0.07442442327737808\n",
      "Epoch 1251: train loss: 0.03994494304060936, val loss: 0.08408446609973907\n",
      "Epoch 1252: train loss: 0.05643820762634277, val loss: 0.10357467085123062\n",
      "Epoch 1253: train loss: 0.044925421476364136, val loss: 0.097636878490448\n",
      "Epoch 1254: train loss: 0.040175341069698334, val loss: 0.04646957665681839\n",
      "Epoch 1255: train loss: 0.040802713483572006, val loss: 0.07776092737913132\n",
      "Epoch 1256: train loss: 0.052467066794633865, val loss: 0.11985798925161362\n",
      "Epoch 1257: train loss: 0.0425197035074234, val loss: 0.04225052148103714\n",
      "Epoch 1258: train loss: 0.028933266177773476, val loss: 0.09495026618242264\n",
      "Epoch 1259: train loss: 0.04231544956564903, val loss: 0.114324189722538\n",
      "Epoch 1260: train loss: 0.041688647121191025, val loss: 0.10438495129346848\n",
      "Epoch 1261: train loss: 0.041162848472595215, val loss: 0.10099853575229645\n",
      "Epoch 1262: train loss: 0.04026896879076958, val loss: 0.11678357422351837\n",
      "Epoch 1263: train loss: 0.04109300673007965, val loss: 0.10720622539520264\n",
      "Epoch 1264: train loss: 0.037759680300951004, val loss: 0.09629891067743301\n",
      "Epoch 1265: train loss: 0.046084214001894, val loss: 0.09235336631536484\n",
      "Epoch 1266: train loss: 0.042601171880960464, val loss: 0.08392234891653061\n",
      "Epoch 1267: train loss: 0.03734580799937248, val loss: 0.07974720001220703\n",
      "Epoch 1268: train loss: 0.0422118604183197, val loss: 0.10833793133497238\n",
      "Epoch 1269: train loss: 0.0423838347196579, val loss: 0.08337301015853882\n",
      "Epoch 1270: train loss: 0.043940138071775436, val loss: 0.08991994708776474\n",
      "Epoch 1271: train loss: 0.03775837644934654, val loss: 0.08926011621952057\n",
      "Epoch 1272: train loss: 0.0407383106648922, val loss: 0.056064773350954056\n",
      "Epoch 1273: train loss: 0.04105899855494499, val loss: 0.09172945469617844\n",
      "Epoch 1274: train loss: 0.03694209083914757, val loss: 0.09011055529117584\n",
      "Epoch 1275: train loss: 0.04610653594136238, val loss: 0.11095278710126877\n",
      "Epoch 1276: train loss: 0.0385424867272377, val loss: 0.06945103406906128\n",
      "Epoch 1277: train loss: 0.030654849484562874, val loss: 0.05754745006561279\n",
      "Epoch 1278: train loss: 0.03596656769514084, val loss: 0.08921199291944504\n",
      "Epoch 1279: train loss: 0.04254215583205223, val loss: 0.11611821502447128\n",
      "Epoch 1280: train loss: 0.05217461287975311, val loss: 0.09033538401126862\n",
      "Epoch 1281: train loss: 0.03667096793651581, val loss: 0.06478521227836609\n",
      "Epoch 1282: train loss: 0.04138188064098358, val loss: 0.12174121290445328\n",
      "Epoch 1283: train loss: 0.04317668452858925, val loss: 0.0930667445063591\n",
      "Epoch 1284: train loss: 0.04735137149691582, val loss: 0.09259996563196182\n",
      "Epoch 1285: train loss: 0.04033312946557999, val loss: 0.07217364758253098\n",
      "Epoch 1286: train loss: 0.042948026210069656, val loss: 0.11195039749145508\n",
      "Epoch 1287: train loss: 0.03878268599510193, val loss: 0.0735926628112793\n",
      "Epoch 1288: train loss: 0.05074978619813919, val loss: 0.06801559031009674\n",
      "Epoch 1289: train loss: 0.050120413303375244, val loss: 0.06518086791038513\n",
      "Epoch 1290: train loss: 0.04347776994109154, val loss: 0.09307011216878891\n",
      "Epoch 1291: train loss: 0.05053214728832245, val loss: 0.05435585603117943\n",
      "Epoch 1292: train loss: 0.04979213699698448, val loss: 0.10249834507703781\n",
      "Epoch 1293: train loss: 0.044234927743673325, val loss: 0.14888954162597656\n",
      "Epoch 1294: train loss: 0.04951322451233864, val loss: 0.08977379649877548\n",
      "Epoch 1295: train loss: 0.03765422850847244, val loss: 0.08189892023801804\n",
      "Epoch 1296: train loss: 0.05227438732981682, val loss: 0.09360133856534958\n",
      "Epoch 1297: train loss: 0.04520687460899353, val loss: 0.08103131502866745\n",
      "Epoch 1298: train loss: 0.04258541762828827, val loss: 0.05991582199931145\n",
      "Epoch 1299: train loss: 0.046019814908504486, val loss: 0.08718793094158173\n",
      "Epoch 1300: train loss: 0.04731719195842743, val loss: 0.09415128082036972\n",
      "Epoch 1301: train loss: 0.03794419765472412, val loss: 0.11314067989587784\n",
      "Epoch 1302: train loss: 0.04193945229053497, val loss: 0.0884147435426712\n",
      "Epoch 1303: train loss: 0.041098203510046005, val loss: 0.06988241523504257\n",
      "Epoch 1304: train loss: 0.03797360509634018, val loss: 0.07192959636449814\n",
      "Epoch 1305: train loss: 0.04448620602488518, val loss: 0.07861816138029099\n",
      "Epoch 1306: train loss: 0.03697611764073372, val loss: 0.07610388845205307\n",
      "Epoch 1307: train loss: 0.03883181884884834, val loss: 0.07201404869556427\n",
      "Epoch 1308: train loss: 0.039108023047447205, val loss: 0.09588287770748138\n",
      "Epoch 1309: train loss: 0.040029507130384445, val loss: 0.08946961164474487\n",
      "Epoch 1310: train loss: 0.041214317083358765, val loss: 0.10158532857894897\n",
      "Epoch 1311: train loss: 0.04204670339822769, val loss: 0.10164044052362442\n",
      "Epoch 1312: train loss: 0.04061223566532135, val loss: 0.09545724838972092\n",
      "Epoch 1313: train loss: 0.04611578583717346, val loss: 0.07284928858280182\n",
      "Epoch 1314: train loss: 0.04123500734567642, val loss: 0.08628671616315842\n",
      "Epoch 1315: train loss: 0.05084097385406494, val loss: 0.060528840869665146\n",
      "Epoch 1316: train loss: 0.04212519899010658, val loss: 0.1077478751540184\n",
      "Epoch 1317: train loss: 0.03381054103374481, val loss: 0.10029270499944687\n",
      "Epoch 1318: train loss: 0.03708851709961891, val loss: 0.13916033506393433\n",
      "Epoch 1319: train loss: 0.04270406439900398, val loss: 0.11235590279102325\n",
      "Epoch 1320: train loss: 0.038372598588466644, val loss: 0.14258284866809845\n",
      "Epoch 1321: train loss: 0.03733978793025017, val loss: 0.08906888216733932\n",
      "Epoch 1322: train loss: 0.042510416358709335, val loss: 0.08670505881309509\n",
      "Epoch 1323: train loss: 0.04536179453134537, val loss: 0.11678065359592438\n",
      "Epoch 1324: train loss: 0.04180264472961426, val loss: 0.09571359306573868\n",
      "Epoch 1325: train loss: 0.050840072333812714, val loss: 0.09162034839391708\n",
      "Epoch 1326: train loss: 0.04512976482510567, val loss: 0.09767688810825348\n",
      "Epoch 1327: train loss: 0.04426170140504837, val loss: 0.11139519512653351\n",
      "Epoch 1328: train loss: 0.04944946989417076, val loss: 0.10717232525348663\n",
      "Epoch 1329: train loss: 0.039365582168102264, val loss: 0.08807966858148575\n",
      "Epoch 1330: train loss: 0.06078258901834488, val loss: 0.06369084864854813\n",
      "Epoch 1331: train loss: 0.04870512709021568, val loss: 0.062138527631759644\n",
      "Epoch 1332: train loss: 0.04674411565065384, val loss: 0.06830734759569168\n",
      "Epoch 1333: train loss: 0.04145036265254021, val loss: 0.11474283039569855\n",
      "Epoch 1334: train loss: 0.05505450814962387, val loss: 0.07273045927286148\n",
      "Epoch 1335: train loss: 0.05541674792766571, val loss: 0.12286766618490219\n",
      "Epoch 1336: train loss: 0.05313440412282944, val loss: 0.08043912798166275\n",
      "Epoch 1337: train loss: 0.04803554713726044, val loss: 0.0734480544924736\n",
      "Epoch 1338: train loss: 0.0446443073451519, val loss: 0.08862738311290741\n",
      "Epoch 1339: train loss: 0.0566713884472847, val loss: 0.11422882229089737\n",
      "Epoch 1340: train loss: 0.04423404484987259, val loss: 0.09837879985570908\n",
      "Epoch 1341: train loss: 0.05179206281900406, val loss: 0.10102956742048264\n",
      "Epoch 1342: train loss: 0.05484117567539215, val loss: 0.06681030243635178\n",
      "Epoch 1343: train loss: 0.05199966952204704, val loss: 0.07619413733482361\n",
      "Epoch 1344: train loss: 0.037028975784778595, val loss: 0.10065954178571701\n",
      "Epoch 1345: train loss: 0.04072393476963043, val loss: 0.07949363440275192\n",
      "Epoch 1346: train loss: 0.04614231362938881, val loss: 0.10297556221485138\n",
      "Epoch 1347: train loss: 0.03531396761536598, val loss: 0.07972314953804016\n",
      "Epoch 1348: train loss: 0.037102699279785156, val loss: 0.08037746697664261\n",
      "Epoch 1349: train loss: 0.03749776631593704, val loss: 0.06829273700714111\n",
      "Epoch 1350: train loss: 0.03326428309082985, val loss: 0.08183826506137848\n",
      "Epoch 1351: train loss: 0.03190743550658226, val loss: 0.08892346173524857\n",
      "Epoch 1352: train loss: 0.03050374612212181, val loss: 0.10764477401971817\n",
      "Epoch 1353: train loss: 0.036197781562805176, val loss: 0.10874191671609879\n",
      "Epoch 1354: train loss: 0.045339882373809814, val loss: 0.06907112896442413\n",
      "Epoch 1355: train loss: 0.03794405609369278, val loss: 0.09040284156799316\n",
      "Epoch 1356: train loss: 0.04754685238003731, val loss: 0.09181980788707733\n",
      "Epoch 1357: train loss: 0.042462851852178574, val loss: 0.06468086689710617\n",
      "Epoch 1358: train loss: 0.04292251542210579, val loss: 0.13281898200511932\n",
      "Epoch 1359: train loss: 0.03317618742585182, val loss: 0.09263298660516739\n",
      "Epoch 1360: train loss: 0.042888399213552475, val loss: 0.06244231387972832\n",
      "Epoch 1361: train loss: 0.050097741186618805, val loss: 0.09784233570098877\n",
      "Epoch 1362: train loss: 0.03655543550848961, val loss: 0.07601529359817505\n",
      "Epoch 1363: train loss: 0.045393265783786774, val loss: 0.0655336007475853\n",
      "Epoch 1364: train loss: 0.03822016716003418, val loss: 0.06358999013900757\n",
      "Epoch 1365: train loss: 0.03652289882302284, val loss: 0.08040009438991547\n",
      "Epoch 1366: train loss: 0.04611239954829216, val loss: 0.09645289927721024\n",
      "Epoch 1367: train loss: 0.042406048625707626, val loss: 0.0968794897198677\n",
      "Epoch 1368: train loss: 0.051655642688274384, val loss: 0.10821979492902756\n",
      "Epoch 1369: train loss: 0.042448971420526505, val loss: 0.09272563457489014\n",
      "Epoch 1370: train loss: 0.04349645972251892, val loss: 0.09900390356779099\n",
      "Epoch 1371: train loss: 0.055873192846775055, val loss: 0.09544094651937485\n",
      "Epoch 1372: train loss: 0.048700153827667236, val loss: 0.11087353527545929\n",
      "Epoch 1373: train loss: 0.04195338860154152, val loss: 0.10331427305936813\n",
      "Epoch 1374: train loss: 0.028015173971652985, val loss: 0.06256890296936035\n",
      "Epoch 1375: train loss: 0.036681897938251495, val loss: 0.1171649917960167\n",
      "Epoch 1376: train loss: 0.03858973830938339, val loss: 0.09889717400074005\n",
      "Epoch 1377: train loss: 0.04863690212368965, val loss: 0.090889111161232\n",
      "Epoch 1378: train loss: 0.03969861939549446, val loss: 0.10308678448200226\n",
      "Epoch 1379: train loss: 0.05349031835794449, val loss: 0.06900627911090851\n",
      "Epoch 1380: train loss: 0.0385783351957798, val loss: 0.12334006279706955\n",
      "Epoch 1381: train loss: 0.03609338030219078, val loss: 0.05773533135652542\n",
      "Epoch 1382: train loss: 0.04371287673711777, val loss: 0.06737103313207626\n",
      "Epoch 1383: train loss: 0.03733358159661293, val loss: 0.07166007906198502\n",
      "Epoch 1384: train loss: 0.042645953595638275, val loss: 0.07576867938041687\n",
      "Epoch 1385: train loss: 0.04903828352689743, val loss: 0.0877818763256073\n",
      "Epoch 1386: train loss: 0.04520554468035698, val loss: 0.09260495752096176\n",
      "Epoch 1387: train loss: 0.043795838952064514, val loss: 0.10268664360046387\n",
      "Epoch 1388: train loss: 0.038637783378362656, val loss: 0.05415314435958862\n",
      "Epoch 1389: train loss: 0.04897932708263397, val loss: 0.06451930850744247\n",
      "Epoch 1390: train loss: 0.048087142407894135, val loss: 0.07385869324207306\n",
      "Epoch 1391: train loss: 0.035039134323596954, val loss: 0.08869412541389465\n",
      "Epoch 1392: train loss: 0.05133480578660965, val loss: 0.08582551777362823\n",
      "Epoch 1393: train loss: 0.041390806436538696, val loss: 0.06781608611345291\n",
      "Epoch 1394: train loss: 0.03563375025987625, val loss: 0.12600994110107422\n",
      "Epoch 1395: train loss: 0.06014276668429375, val loss: 0.10656583309173584\n",
      "Epoch 1396: train loss: 0.050716400146484375, val loss: 0.1240372434258461\n",
      "Epoch 1397: train loss: 0.050310004502534866, val loss: 0.09397687762975693\n",
      "Epoch 1398: train loss: 0.04210033640265465, val loss: 0.09812591969966888\n",
      "Epoch 1399: train loss: 0.04075276851654053, val loss: 0.07662483304738998\n",
      "Epoch 1400: train loss: 0.03727448359131813, val loss: 0.0662706047296524\n",
      "Epoch 1401: train loss: 0.041042301803827286, val loss: 0.0891595110297203\n",
      "Epoch 1402: train loss: 0.052348192781209946, val loss: 0.06608028709888458\n",
      "Epoch 1403: train loss: 0.04665933921933174, val loss: 0.055974215269088745\n",
      "Epoch 1404: train loss: 0.05755162239074707, val loss: 0.09112036973237991\n",
      "Epoch 1405: train loss: 0.05481649190187454, val loss: 0.09611772745847702\n",
      "Epoch 1406: train loss: 0.044258687645196915, val loss: 0.06222786381840706\n",
      "Epoch 1407: train loss: 0.039986323565244675, val loss: 0.0594869926571846\n",
      "Epoch 1408: train loss: 0.03784944862127304, val loss: 0.0983872264623642\n",
      "Epoch 1409: train loss: 0.05101188272237778, val loss: 0.09740620106458664\n",
      "Epoch 1410: train loss: 0.04485310986638069, val loss: 0.11627712100744247\n",
      "Epoch 1411: train loss: 0.05203033238649368, val loss: 0.0819481760263443\n",
      "Epoch 1412: train loss: 0.043378353118896484, val loss: 0.07770003378391266\n",
      "Epoch 1413: train loss: 0.05042196065187454, val loss: 0.10437466204166412\n",
      "Epoch 1414: train loss: 0.03719271346926689, val loss: 0.09133090823888779\n",
      "Epoch 1415: train loss: 0.04758176952600479, val loss: 0.09946316480636597\n",
      "Epoch 1416: train loss: 0.044309552758932114, val loss: 0.07973340898752213\n",
      "Epoch 1417: train loss: 0.046235837042331696, val loss: 0.08819571137428284\n",
      "Epoch 1418: train loss: 0.05505630746483803, val loss: 0.09956524521112442\n",
      "Epoch 1419: train loss: 0.04573057219386101, val loss: 0.058451712131500244\n",
      "Epoch 1420: train loss: 0.051348090171813965, val loss: 0.0725812241435051\n",
      "Epoch 1421: train loss: 0.04508095607161522, val loss: 0.08940193057060242\n",
      "Epoch 1422: train loss: 0.040764521807432175, val loss: 0.11190932244062424\n",
      "Epoch 1423: train loss: 0.0415298230946064, val loss: 0.10615295171737671\n",
      "Epoch 1424: train loss: 0.04483439400792122, val loss: 0.12262924760580063\n",
      "Epoch 1425: train loss: 0.036631837487220764, val loss: 0.07945050299167633\n",
      "Epoch 1426: train loss: 0.031170718371868134, val loss: 0.07678570598363876\n",
      "Epoch 1427: train loss: 0.04601673781871796, val loss: 0.09781266748905182\n",
      "Epoch 1428: train loss: 0.03594602644443512, val loss: 0.087507463991642\n",
      "Epoch 1429: train loss: 0.04167569801211357, val loss: 0.0855395495891571\n",
      "Epoch 1430: train loss: 0.0508788637816906, val loss: 0.07488569617271423\n",
      "Epoch 1431: train loss: 0.05012672394514084, val loss: 0.07120997458696365\n",
      "Epoch 1432: train loss: 0.04022067040205002, val loss: 0.07892350107431412\n",
      "Epoch 1433: train loss: 0.040096502751111984, val loss: 0.12343460321426392\n",
      "Epoch 1434: train loss: 0.042867597192525864, val loss: 0.05845986679196358\n",
      "Epoch 1435: train loss: 0.04619424790143967, val loss: 0.09954076260328293\n",
      "Epoch 1436: train loss: 0.044654130935668945, val loss: 0.0788695216178894\n",
      "Epoch 1437: train loss: 0.039543766528367996, val loss: 0.0799674317240715\n",
      "Epoch 1438: train loss: 0.0402802936732769, val loss: 0.077323317527771\n",
      "Epoch 1439: train loss: 0.040139902383089066, val loss: 0.09102244675159454\n",
      "Epoch 1440: train loss: 0.04767848923802376, val loss: 0.040513452142477036\n",
      "Epoch 1441: train loss: 0.03679675981402397, val loss: 0.06691531091928482\n",
      "Epoch 1442: train loss: 0.0388437919318676, val loss: 0.0932624340057373\n",
      "Epoch 1443: train loss: 0.04179053008556366, val loss: 0.1167900487780571\n",
      "Epoch 1444: train loss: 0.04860870540142059, val loss: 0.0773899257183075\n",
      "Epoch 1445: train loss: 0.05029347911477089, val loss: 0.08097226917743683\n",
      "Epoch 1446: train loss: 0.04586190730333328, val loss: 0.12987880408763885\n",
      "Epoch 1447: train loss: 0.03762710094451904, val loss: 0.07311280816793442\n",
      "Epoch 1448: train loss: 0.04528362676501274, val loss: 0.0759691670536995\n",
      "Epoch 1449: train loss: 0.04790777713060379, val loss: 0.061124272644519806\n",
      "Epoch 1450: train loss: 0.0435786098241806, val loss: 0.09126901626586914\n",
      "Epoch 1451: train loss: 0.04598831757903099, val loss: 0.08097377419471741\n",
      "Epoch 1452: train loss: 0.048837896436452866, val loss: 0.09754224866628647\n",
      "Epoch 1453: train loss: 0.041458867490291595, val loss: 0.11258067935705185\n",
      "Epoch 1454: train loss: 0.03943217545747757, val loss: 0.07259928435087204\n",
      "Epoch 1455: train loss: 0.03973386064171791, val loss: 0.0863715186715126\n",
      "Epoch 1456: train loss: 0.036722972989082336, val loss: 0.08588213473558426\n",
      "Epoch 1457: train loss: 0.04293720796704292, val loss: 0.06282933056354523\n",
      "Epoch 1458: train loss: 0.04375949501991272, val loss: 0.11503344774246216\n",
      "Epoch 1459: train loss: 0.04511843994259834, val loss: 0.07202088087797165\n",
      "Epoch 1460: train loss: 0.04863294959068298, val loss: 0.05023366957902908\n",
      "Epoch 1461: train loss: 0.042057037353515625, val loss: 0.06616134941577911\n",
      "Epoch 1462: train loss: 0.0445549376308918, val loss: 0.08181170374155045\n",
      "Epoch 1463: train loss: 0.04432827606797218, val loss: 0.09171803295612335\n",
      "Epoch 1464: train loss: 0.058603107929229736, val loss: 0.1040482297539711\n",
      "Epoch 1465: train loss: 0.04283370077610016, val loss: 0.08210068941116333\n",
      "Epoch 1466: train loss: 0.05167458951473236, val loss: 0.12466461956501007\n",
      "Epoch 1467: train loss: 0.04416772723197937, val loss: 0.10080816596746445\n",
      "Epoch 1468: train loss: 0.0628650113940239, val loss: 0.068466916680336\n",
      "Epoch 1469: train loss: 0.052114952355623245, val loss: 0.0646468997001648\n",
      "Epoch 1470: train loss: 0.03968922421336174, val loss: 0.12635652720928192\n",
      "Epoch 1471: train loss: 0.04278433322906494, val loss: 0.06941135972738266\n",
      "Epoch 1472: train loss: 0.03442772105336189, val loss: 0.07238682359457016\n",
      "Epoch 1473: train loss: 0.04421599581837654, val loss: 0.11333005875349045\n",
      "Epoch 1474: train loss: 0.04833253473043442, val loss: 0.11173608154058456\n",
      "Epoch 1475: train loss: 0.04463082551956177, val loss: 0.09380131214857101\n",
      "Epoch 1476: train loss: 0.03739949315786362, val loss: 0.06853631883859634\n",
      "Epoch 1477: train loss: 0.045290544629096985, val loss: 0.09810870885848999\n",
      "Epoch 1478: train loss: 0.0526064895093441, val loss: 0.07435580343008041\n",
      "Epoch 1479: train loss: 0.042223766446113586, val loss: 0.08904942125082016\n",
      "Epoch 1480: train loss: 0.04175543785095215, val loss: 0.0866333469748497\n",
      "Epoch 1481: train loss: 0.04275549203157425, val loss: 0.08353500068187714\n",
      "Epoch 1482: train loss: 0.047018252313137054, val loss: 0.06410140544176102\n",
      "Epoch 1483: train loss: 0.04188237339258194, val loss: 0.07929126918315887\n",
      "Epoch 1484: train loss: 0.04481153562664986, val loss: 0.06830984354019165\n",
      "Epoch 1485: train loss: 0.04293615743517876, val loss: 0.061185140162706375\n",
      "Epoch 1486: train loss: 0.047873783856630325, val loss: 0.11969020217657089\n",
      "Epoch 1487: train loss: 0.04296990483999252, val loss: 0.08790691196918488\n",
      "Epoch 1488: train loss: 0.04620097950100899, val loss: 0.08946582674980164\n",
      "Epoch 1489: train loss: 0.0492272712290287, val loss: 0.07144960016012192\n",
      "Epoch 1490: train loss: 0.052621521055698395, val loss: 0.08677490800619125\n",
      "Epoch 1491: train loss: 0.04564555734395981, val loss: 0.09404908865690231\n",
      "Epoch 1492: train loss: 0.03899091109633446, val loss: 0.0929023027420044\n",
      "Epoch 1493: train loss: 0.04612419381737709, val loss: 0.08422760665416718\n",
      "Epoch 1494: train loss: 0.035342130810022354, val loss: 0.07162465900182724\n",
      "Epoch 1495: train loss: 0.04085021838545799, val loss: 0.09038859605789185\n",
      "Epoch 1496: train loss: 0.03992936760187149, val loss: 0.10763036459684372\n",
      "Epoch 1497: train loss: 0.04157432168722153, val loss: 0.061105914413928986\n",
      "Epoch 1498: train loss: 0.04257434234023094, val loss: 0.0653756856918335\n",
      "Epoch 1499: train loss: 0.04598379135131836, val loss: 0.04947490617632866\n",
      "Epoch 1500: train loss: 0.04554896056652069, val loss: 0.0813499242067337\n",
      "Epoch 1501: train loss: 0.03580055385828018, val loss: 0.10165562480688095\n",
      "Epoch 1502: train loss: 0.043879829347133636, val loss: 0.09861468523740768\n",
      "Epoch 1503: train loss: 0.047382134944200516, val loss: 0.11272483319044113\n",
      "Epoch 1504: train loss: 0.05355483293533325, val loss: 0.078225277364254\n",
      "Epoch 1505: train loss: 0.03608371689915657, val loss: 0.07358737289905548\n",
      "Epoch 1506: train loss: 0.03831101208925247, val loss: 0.11556785553693771\n",
      "Epoch 1507: train loss: 0.04343738034367561, val loss: 0.06539230793714523\n",
      "Epoch 1508: train loss: 0.043140482157468796, val loss: 0.07907097041606903\n",
      "Epoch 1509: train loss: 0.0400225929915905, val loss: 0.07857457548379898\n",
      "Epoch 1510: train loss: 0.04805980995297432, val loss: 0.09974086284637451\n",
      "Epoch 1511: train loss: 0.04933366924524307, val loss: 0.09177838265895844\n",
      "Epoch 1512: train loss: 0.048268239945173264, val loss: 0.12069590389728546\n",
      "Epoch 1513: train loss: 0.03826675936579704, val loss: 0.09294498711824417\n",
      "Epoch 1514: train loss: 0.04581880569458008, val loss: 0.06930483877658844\n",
      "Epoch 1515: train loss: 0.04381493479013443, val loss: 0.06634508073329926\n",
      "Epoch 1516: train loss: 0.04606664553284645, val loss: 0.12555356323719025\n",
      "Epoch 1517: train loss: 0.0448111966252327, val loss: 0.10405254364013672\n",
      "Epoch 1518: train loss: 0.0413568839430809, val loss: 0.09674816578626633\n",
      "Epoch 1519: train loss: 0.03671931102871895, val loss: 0.08276009559631348\n",
      "Epoch 1520: train loss: 0.0500713586807251, val loss: 0.0790669247508049\n",
      "Epoch 1521: train loss: 0.05019984766840935, val loss: 0.0688067227602005\n",
      "Epoch 1522: train loss: 0.043327946215867996, val loss: 0.07640506327152252\n",
      "Epoch 1523: train loss: 0.04458098113536835, val loss: 0.10314180701971054\n",
      "Epoch 1524: train loss: 0.03926880657672882, val loss: 0.08250212669372559\n",
      "Epoch 1525: train loss: 0.03715496510267258, val loss: 0.07989190518856049\n",
      "Epoch 1526: train loss: 0.03066621907055378, val loss: 0.08921655267477036\n",
      "Epoch 1527: train loss: 0.03691769763827324, val loss: 0.07199554890394211\n",
      "Epoch 1528: train loss: 0.04498909041285515, val loss: 0.08902058005332947\n",
      "Epoch 1529: train loss: 0.033496808260679245, val loss: 0.08098767697811127\n",
      "Epoch 1530: train loss: 0.03880295157432556, val loss: 0.06438641250133514\n",
      "Epoch 1531: train loss: 0.03637510538101196, val loss: 0.07427617162466049\n",
      "Epoch 1532: train loss: 0.040978606790304184, val loss: 0.09225823730230331\n",
      "Epoch 1533: train loss: 0.0370117723941803, val loss: 0.09279920905828476\n",
      "Epoch 1534: train loss: 0.04235977679491043, val loss: 0.11256015300750732\n",
      "Epoch 1535: train loss: 0.04356658458709717, val loss: 0.09986519068479538\n",
      "Epoch 1536: train loss: 0.03485851734876633, val loss: 0.08212963491678238\n",
      "Epoch 1537: train loss: 0.0435633510351181, val loss: 0.0919409990310669\n",
      "Epoch 1538: train loss: 0.05592634528875351, val loss: 0.09440898895263672\n",
      "Epoch 1539: train loss: 0.048848893493413925, val loss: 0.09319531172513962\n",
      "Epoch 1540: train loss: 0.045682430267333984, val loss: 0.10884270817041397\n",
      "Epoch 1541: train loss: 0.046887923032045364, val loss: 0.09422838687896729\n",
      "Epoch 1542: train loss: 0.04573678597807884, val loss: 0.06584185361862183\n",
      "Epoch 1543: train loss: 0.039229776710271835, val loss: 0.07780148833990097\n",
      "Epoch 1544: train loss: 0.03958004340529442, val loss: 0.08907020092010498\n",
      "Epoch 1545: train loss: 0.0460437573492527, val loss: 0.08801417797803879\n",
      "Epoch 1546: train loss: 0.04329845309257507, val loss: 0.07368882745504379\n",
      "Epoch 1547: train loss: 0.0363481305539608, val loss: 0.11300535500049591\n",
      "Epoch 1548: train loss: 0.03970405459403992, val loss: 0.09500040858983994\n",
      "Epoch 1549: train loss: 0.03992348164319992, val loss: 0.07318305969238281\n",
      "Epoch 1550: train loss: 0.0365842841565609, val loss: 0.12145145237445831\n",
      "Epoch 1551: train loss: 0.03677885979413986, val loss: 0.09405418485403061\n",
      "Epoch 1552: train loss: 0.05182233080267906, val loss: 0.0638103112578392\n",
      "Epoch 1553: train loss: 0.04014056175947189, val loss: 0.08462704718112946\n",
      "Epoch 1554: train loss: 0.03150192275643349, val loss: 0.12803292274475098\n",
      "Epoch 1555: train loss: 0.04202296584844589, val loss: 0.06901607662439346\n",
      "Epoch 1556: train loss: 0.03958498314023018, val loss: 0.0835084542632103\n",
      "Epoch 1557: train loss: 0.04184283688664436, val loss: 0.08441280573606491\n",
      "Epoch 1558: train loss: 0.02867983840405941, val loss: 0.11761084944009781\n",
      "Epoch 1559: train loss: 0.05190584436058998, val loss: 0.09909351170063019\n",
      "Epoch 1560: train loss: 0.037729911506175995, val loss: 0.11215589195489883\n",
      "Epoch 1561: train loss: 0.04947512596845627, val loss: 0.07654347270727158\n",
      "Epoch 1562: train loss: 0.03834995999932289, val loss: 0.08062190562486649\n",
      "Epoch 1563: train loss: 0.0367513969540596, val loss: 0.0749271884560585\n",
      "Epoch 1564: train loss: 0.038497116416692734, val loss: 0.09932146221399307\n",
      "Epoch 1565: train loss: 0.04643629491329193, val loss: 0.10729960352182388\n",
      "Epoch 1566: train loss: 0.04127403721213341, val loss: 0.09183569252490997\n",
      "Epoch 1567: train loss: 0.04095633327960968, val loss: 0.11586179584264755\n",
      "Epoch 1568: train loss: 0.036205291748046875, val loss: 0.09591380506753922\n",
      "Epoch 1569: train loss: 0.04115354269742966, val loss: 0.08228809386491776\n",
      "Epoch 1570: train loss: 0.04390520974993706, val loss: 0.08005024492740631\n",
      "Epoch 1571: train loss: 0.05440596491098404, val loss: 0.064103864133358\n",
      "Epoch 1572: train loss: 0.04698110371828079, val loss: 0.10494917631149292\n",
      "Epoch 1573: train loss: 0.04103115573525429, val loss: 0.08932425081729889\n",
      "Epoch 1574: train loss: 0.03867539018392563, val loss: 0.06695975363254547\n",
      "Epoch 1575: train loss: 0.03588099405169487, val loss: 0.0641908347606659\n",
      "Epoch 1576: train loss: 0.045637112110853195, val loss: 0.13981299102306366\n",
      "Epoch 1577: train loss: 0.05962079390883446, val loss: 0.09884940832853317\n",
      "Epoch 1578: train loss: 0.049214739352464676, val loss: 0.10135656595230103\n",
      "Epoch 1579: train loss: 0.04912944138050079, val loss: 0.07889672368764877\n",
      "Epoch 1580: train loss: 0.0436265803873539, val loss: 0.09417453408241272\n",
      "Epoch 1581: train loss: 0.04829009622335434, val loss: 0.09718507528305054\n",
      "Epoch 1582: train loss: 0.0362376905977726, val loss: 0.10783430188894272\n",
      "Epoch 1583: train loss: 0.043636295944452286, val loss: 0.07550900429487228\n",
      "Epoch 1584: train loss: 0.048944830894470215, val loss: 0.14002031087875366\n",
      "Epoch 1585: train loss: 0.04122007265686989, val loss: 0.08962632715702057\n",
      "Epoch 1586: train loss: 0.03436381742358208, val loss: 0.0804414227604866\n",
      "Epoch 1587: train loss: 0.039950210601091385, val loss: 0.09186047315597534\n",
      "Epoch 1588: train loss: 0.03452633321285248, val loss: 0.08235736936330795\n",
      "Epoch 1589: train loss: 0.036144692450761795, val loss: 0.13038308918476105\n",
      "Epoch 1590: train loss: 0.05112805962562561, val loss: 0.10261974483728409\n",
      "Epoch 1591: train loss: 0.030827848240733147, val loss: 0.10357655584812164\n",
      "Epoch 1592: train loss: 0.03368294984102249, val loss: 0.09739144891500473\n",
      "Epoch 1593: train loss: 0.03702552616596222, val loss: 0.09556840360164642\n",
      "Epoch 1594: train loss: 0.045661572366952896, val loss: 0.11789431422948837\n",
      "Epoch 1595: train loss: 0.03946250304579735, val loss: 0.06274264305830002\n",
      "Epoch 1596: train loss: 0.03962206467986107, val loss: 0.04872661456465721\n",
      "Epoch 1597: train loss: 0.03615681827068329, val loss: 0.06909944862127304\n",
      "Epoch 1598: train loss: 0.03631320968270302, val loss: 0.09227479994297028\n",
      "Epoch 1599: train loss: 0.041261736303567886, val loss: 0.07718532532453537\n",
      "Epoch 1600: train loss: 0.03786049410700798, val loss: 0.09067654609680176\n",
      "Epoch 1601: train loss: 0.03810802474617958, val loss: 0.04372662305831909\n",
      "Epoch 1602: train loss: 0.04181533306837082, val loss: 0.10252765566110611\n",
      "Epoch 1603: train loss: 0.03513118624687195, val loss: 0.08519156277179718\n",
      "Epoch 1604: train loss: 0.03628259152173996, val loss: 0.07889478653669357\n",
      "Epoch 1605: train loss: 0.04404713958501816, val loss: 0.11892614513635635\n",
      "Epoch 1606: train loss: 0.029848719015717506, val loss: 0.10536756366491318\n",
      "Epoch 1607: train loss: 0.04431900382041931, val loss: 0.05355088785290718\n",
      "Epoch 1608: train loss: 0.03321737423539162, val loss: 0.0949959084391594\n",
      "Epoch 1609: train loss: 0.04380327835679054, val loss: 0.10387692600488663\n",
      "Epoch 1610: train loss: 0.03227584809064865, val loss: 0.09200000017881393\n",
      "Epoch 1611: train loss: 0.041433513164520264, val loss: 0.11661507189273834\n",
      "Epoch 1612: train loss: 0.0465991236269474, val loss: 0.10927647352218628\n",
      "Epoch 1613: train loss: 0.043569158762693405, val loss: 0.07028006762266159\n",
      "Epoch 1614: train loss: 0.038137827068567276, val loss: 0.1004018783569336\n",
      "Epoch 1615: train loss: 0.040744680911302567, val loss: 0.09411483258008957\n",
      "Epoch 1616: train loss: 0.038695450872182846, val loss: 0.08020573109388351\n",
      "Epoch 1617: train loss: 0.043675296008586884, val loss: 0.053705956786870956\n",
      "Epoch 1618: train loss: 0.04166083037853241, val loss: 0.09087520092725754\n",
      "Epoch 1619: train loss: 0.04121783748269081, val loss: 0.08809296041727066\n",
      "Epoch 1620: train loss: 0.04853501915931702, val loss: 0.10685737431049347\n",
      "Epoch 1621: train loss: 0.046558964997529984, val loss: 0.10428036749362946\n",
      "Epoch 1622: train loss: 0.03975091502070427, val loss: 0.0879741758108139\n",
      "Epoch 1623: train loss: 0.04537465050816536, val loss: 0.0669146403670311\n",
      "Epoch 1624: train loss: 0.043251872062683105, val loss: 0.08762338757514954\n",
      "Epoch 1625: train loss: 0.04489947110414505, val loss: 0.07489614933729172\n",
      "Epoch 1626: train loss: 0.04128257930278778, val loss: 0.07292606681585312\n",
      "Epoch 1627: train loss: 0.03833375871181488, val loss: 0.10516228526830673\n",
      "Epoch 1628: train loss: 0.04222462326288223, val loss: 0.11982611566781998\n",
      "Epoch 1629: train loss: 0.0461755245923996, val loss: 0.07101418823003769\n",
      "Epoch 1630: train loss: 0.04425452649593353, val loss: 0.09932031482458115\n",
      "Epoch 1631: train loss: 0.035482559353113174, val loss: 0.04233613610267639\n",
      "Epoch 1632: train loss: 0.042243387550115585, val loss: 0.11257829517126083\n",
      "Epoch 1633: train loss: 0.04609258100390434, val loss: 0.11533772945404053\n",
      "Epoch 1634: train loss: 0.04156910255551338, val loss: 0.11137344688177109\n",
      "Epoch 1635: train loss: 0.04379531741142273, val loss: 0.07208044826984406\n",
      "Epoch 1636: train loss: 0.04149125516414642, val loss: 0.0761050432920456\n",
      "Epoch 1637: train loss: 0.03894989937543869, val loss: 0.1048615500330925\n",
      "Epoch 1638: train loss: 0.04271792620420456, val loss: 0.09466955065727234\n",
      "Epoch 1639: train loss: 0.03730744868516922, val loss: 0.11167093366384506\n",
      "Epoch 1640: train loss: 0.044070128351449966, val loss: 0.08889801055192947\n",
      "Epoch 1641: train loss: 0.03812344744801521, val loss: 0.09381111711263657\n",
      "Epoch 1642: train loss: 0.04320554807782173, val loss: 0.1344306915998459\n",
      "Epoch 1643: train loss: 0.03510083258152008, val loss: 0.11705159395933151\n",
      "Epoch 1644: train loss: 0.04403660446405411, val loss: 0.10224628448486328\n",
      "Epoch 1645: train loss: 0.03880678117275238, val loss: 0.07433902472257614\n",
      "Epoch 1646: train loss: 0.04015686735510826, val loss: 0.09940172731876373\n",
      "Epoch 1647: train loss: 0.03551505133509636, val loss: 0.1110919713973999\n",
      "Epoch 1648: train loss: 0.03747636079788208, val loss: 0.1595654934644699\n",
      "Epoch 1649: train loss: 0.032090771943330765, val loss: 0.10282092541456223\n",
      "Epoch 1650: train loss: 0.027857664972543716, val loss: 0.07371196895837784\n",
      "Epoch 1651: train loss: 0.03584621846675873, val loss: 0.09196043759584427\n",
      "Epoch 1652: train loss: 0.032753024250268936, val loss: 0.05709819868206978\n",
      "Epoch 1653: train loss: 0.0292604248970747, val loss: 0.07613920420408249\n",
      "Epoch 1654: train loss: 0.04206926003098488, val loss: 0.05447658523917198\n",
      "Epoch 1655: train loss: 0.0369853675365448, val loss: 0.08626657724380493\n",
      "Epoch 1656: train loss: 0.03282995894551277, val loss: 0.11276020109653473\n",
      "Epoch 1657: train loss: 0.03567732870578766, val loss: 0.0850018858909607\n",
      "Epoch 1658: train loss: 0.038866493850946426, val loss: 0.07481636852025986\n",
      "Epoch 1659: train loss: 0.030275393277406693, val loss: 0.06307407468557358\n",
      "Epoch 1660: train loss: 0.04087528958916664, val loss: 0.06319227069616318\n",
      "Epoch 1661: train loss: 0.0365198478102684, val loss: 0.04309071972966194\n",
      "Epoch 1662: train loss: 0.038478974252939224, val loss: 0.08069705963134766\n",
      "Epoch 1663: train loss: 0.0455598346889019, val loss: 0.05579965189099312\n",
      "Epoch 1664: train loss: 0.03498883172869682, val loss: 0.07769184559583664\n",
      "Epoch 1665: train loss: 0.04704614356160164, val loss: 0.08370710164308548\n",
      "Epoch 1666: train loss: 0.03986823558807373, val loss: 0.07671327143907547\n",
      "Epoch 1667: train loss: 0.036326829344034195, val loss: 0.07661856710910797\n",
      "Epoch 1668: train loss: 0.04181580990552902, val loss: 0.0840587317943573\n",
      "Epoch 1669: train loss: 0.04329035058617592, val loss: 0.10929826647043228\n",
      "Epoch 1670: train loss: 0.03960419073700905, val loss: 0.09532757103443146\n",
      "Epoch 1671: train loss: 0.052149053663015366, val loss: 0.13363304734230042\n",
      "Epoch 1672: train loss: 0.0396115742623806, val loss: 0.09682346135377884\n",
      "Epoch 1673: train loss: 0.03221053630113602, val loss: 0.0889071673154831\n",
      "Epoch 1674: train loss: 0.039997924119234085, val loss: 0.07397420704364777\n",
      "Epoch 1675: train loss: 0.03597915172576904, val loss: 0.06850150227546692\n",
      "Epoch 1676: train loss: 0.04182783514261246, val loss: 0.04579247161746025\n",
      "Epoch 1677: train loss: 0.036863453686237335, val loss: 0.07820253819227219\n",
      "Epoch 1678: train loss: 0.039217982441186905, val loss: 0.10104920715093613\n",
      "Epoch 1679: train loss: 0.04343923181295395, val loss: 0.08211418241262436\n",
      "Epoch 1680: train loss: 0.04277762398123741, val loss: 0.09915526211261749\n",
      "Epoch 1681: train loss: 0.04989852011203766, val loss: 0.0678180381655693\n",
      "Epoch 1682: train loss: 0.04347561299800873, val loss: 0.07173414528369904\n",
      "Epoch 1683: train loss: 0.04357496649026871, val loss: 0.10362482070922852\n",
      "Epoch 1684: train loss: 0.04170931130647659, val loss: 0.06193355470895767\n",
      "Epoch 1685: train loss: 0.04261774942278862, val loss: 0.0944196805357933\n",
      "Epoch 1686: train loss: 0.03647555410861969, val loss: 0.11205561459064484\n",
      "Epoch 1687: train loss: 0.03949122875928879, val loss: 0.07786476612091064\n",
      "Epoch 1688: train loss: 0.039246637374162674, val loss: 0.10142280906438828\n",
      "Epoch 1689: train loss: 0.03974160924553871, val loss: 0.08642703294754028\n",
      "Epoch 1690: train loss: 0.037901002913713455, val loss: 0.06515651941299438\n",
      "Epoch 1691: train loss: 0.04168909415602684, val loss: 0.09447910636663437\n",
      "Epoch 1692: train loss: 0.03675425797700882, val loss: 0.08586341142654419\n",
      "Epoch 1693: train loss: 0.046719394624233246, val loss: 0.0825817734003067\n",
      "Epoch 1694: train loss: 0.04240231215953827, val loss: 0.07628824561834335\n",
      "Epoch 1695: train loss: 0.03835907205939293, val loss: 0.055026825517416\n",
      "Epoch 1696: train loss: 0.04329819604754448, val loss: 0.0750439390540123\n",
      "Epoch 1697: train loss: 0.03732914477586746, val loss: 0.0946655422449112\n",
      "Epoch 1698: train loss: 0.04566601291298866, val loss: 0.10405649244785309\n",
      "Epoch 1699: train loss: 0.0398910790681839, val loss: 0.08809448778629303\n",
      "Epoch 1700: train loss: 0.03232092410326004, val loss: 0.10356521606445312\n",
      "Epoch 1701: train loss: 0.04228930175304413, val loss: 0.07838396728038788\n",
      "Epoch 1702: train loss: 0.044138796627521515, val loss: 0.10822109133005142\n",
      "Epoch 1703: train loss: 0.03216530382633209, val loss: 0.10024472326040268\n",
      "Epoch 1704: train loss: 0.03613147884607315, val loss: 0.10226231813430786\n",
      "Epoch 1705: train loss: 0.04420856758952141, val loss: 0.08592228591442108\n",
      "Epoch 1706: train loss: 0.04057858884334564, val loss: 0.08147591352462769\n",
      "Epoch 1707: train loss: 0.03615496680140495, val loss: 0.07022705674171448\n",
      "Epoch 1708: train loss: 0.039100535213947296, val loss: 0.12604038417339325\n",
      "Epoch 1709: train loss: 0.037315450608730316, val loss: 0.061557210981845856\n",
      "Epoch 1710: train loss: 0.039500851184129715, val loss: 0.11668022722005844\n",
      "Epoch 1711: train loss: 0.03406381234526634, val loss: 0.09242914617061615\n",
      "Epoch 1712: train loss: 0.03656316548585892, val loss: 0.07244042307138443\n",
      "Epoch 1713: train loss: 0.03920324891805649, val loss: 0.07970882207155228\n",
      "Epoch 1714: train loss: 0.039712242782115936, val loss: 0.09372784942388535\n",
      "Epoch 1715: train loss: 0.04573514312505722, val loss: 0.11838094145059586\n",
      "Epoch 1716: train loss: 0.037179119884967804, val loss: 0.08046569675207138\n",
      "Epoch 1717: train loss: 0.041974686086177826, val loss: 0.09047292917966843\n",
      "Epoch 1718: train loss: 0.04518507048487663, val loss: 0.08889390528202057\n",
      "Epoch 1719: train loss: 0.04294426739215851, val loss: 0.058283474296331406\n",
      "Epoch 1720: train loss: 0.04649229347705841, val loss: 0.07934834808111191\n",
      "Epoch 1721: train loss: 0.039898861199617386, val loss: 0.10650570690631866\n",
      "Epoch 1722: train loss: 0.04248517006635666, val loss: 0.07055502384901047\n",
      "Epoch 1723: train loss: 0.04747592285275459, val loss: 0.11685102432966232\n",
      "Epoch 1724: train loss: 0.03285003453493118, val loss: 0.09635075181722641\n",
      "Epoch 1725: train loss: 0.0418790839612484, val loss: 0.08840376138687134\n",
      "Epoch 1726: train loss: 0.034982118755578995, val loss: 0.09247729182243347\n",
      "Epoch 1727: train loss: 0.03699134662747383, val loss: 0.09838686138391495\n",
      "Epoch 1728: train loss: 0.03327174112200737, val loss: 0.10248058289289474\n",
      "Epoch 1729: train loss: 0.037056151777505875, val loss: 0.08005678653717041\n",
      "Epoch 1730: train loss: 0.04329296201467514, val loss: 0.09801223129034042\n",
      "Epoch 1731: train loss: 0.03604172542691231, val loss: 0.07733998447656631\n",
      "Epoch 1732: train loss: 0.042867422103881836, val loss: 0.07919382303953171\n",
      "Epoch 1733: train loss: 0.032780345529317856, val loss: 0.10559885948896408\n",
      "Epoch 1734: train loss: 0.04265354573726654, val loss: 0.11228962242603302\n",
      "Epoch 1735: train loss: 0.03224200755357742, val loss: 0.07286311686038971\n",
      "Epoch 1736: train loss: 0.03384806215763092, val loss: 0.05692177638411522\n",
      "Epoch 1737: train loss: 0.03230690956115723, val loss: 0.06784573197364807\n",
      "Epoch 1738: train loss: 0.03576195240020752, val loss: 0.06361608952283859\n",
      "Epoch 1739: train loss: 0.04142904281616211, val loss: 0.10055593401193619\n",
      "Epoch 1740: train loss: 0.03730706125497818, val loss: 0.08606382459402084\n",
      "Epoch 1741: train loss: 0.0325968861579895, val loss: 0.10445556789636612\n",
      "Epoch 1742: train loss: 0.03371177986264229, val loss: 0.11362924426794052\n",
      "Epoch 1743: train loss: 0.04383745789527893, val loss: 0.08240092545747757\n",
      "Epoch 1744: train loss: 0.05047578364610672, val loss: 0.1090397834777832\n",
      "Epoch 1745: train loss: 0.04928024113178253, val loss: 0.08738891035318375\n",
      "Epoch 1746: train loss: 0.04450115188956261, val loss: 0.08597906678915024\n",
      "Epoch 1747: train loss: 0.0417470820248127, val loss: 0.09155023097991943\n",
      "Epoch 1748: train loss: 0.04436212778091431, val loss: 0.08427976071834564\n",
      "Epoch 1749: train loss: 0.03669694438576698, val loss: 0.1004137173295021\n",
      "Epoch 1750: train loss: 0.03380411118268967, val loss: 0.09888695925474167\n",
      "Epoch 1751: train loss: 0.036628883332014084, val loss: 0.08914955705404282\n",
      "Epoch 1752: train loss: 0.04291718825697899, val loss: 0.06747660040855408\n",
      "Epoch 1753: train loss: 0.035009078681468964, val loss: 0.09231369197368622\n",
      "Epoch 1754: train loss: 0.03443152830004692, val loss: 0.06595814973115921\n",
      "Epoch 1755: train loss: 0.036866288632154465, val loss: 0.1371334344148636\n",
      "Epoch 1756: train loss: 0.03310679271817207, val loss: 0.09994353353977203\n",
      "Epoch 1757: train loss: 0.040835190564394, val loss: 0.05397411063313484\n",
      "Epoch 1758: train loss: 0.052389953285455704, val loss: 0.06908940523862839\n",
      "Epoch 1759: train loss: 0.03396395221352577, val loss: 0.09476110339164734\n",
      "Epoch 1760: train loss: 0.033972978591918945, val loss: 0.13046303391456604\n",
      "Epoch 1761: train loss: 0.035672131925821304, val loss: 0.067202627658844\n",
      "Epoch 1762: train loss: 0.04527493193745613, val loss: 0.07147566229104996\n",
      "Epoch 1763: train loss: 0.04145069792866707, val loss: 0.09774996340274811\n",
      "Epoch 1764: train loss: 0.04673301428556442, val loss: 0.17078997194766998\n",
      "Epoch 1765: train loss: 0.04097549617290497, val loss: 0.05997144803404808\n",
      "Epoch 1766: train loss: 0.03586602583527565, val loss: 0.07614176720380783\n",
      "Epoch 1767: train loss: 0.041888996958732605, val loss: 0.09750741720199585\n",
      "Epoch 1768: train loss: 0.040652479976415634, val loss: 0.08486055582761765\n",
      "Epoch 1769: train loss: 0.04319598153233528, val loss: 0.09833909571170807\n",
      "Epoch 1770: train loss: 0.03530719503760338, val loss: 0.07548308372497559\n",
      "Epoch 1771: train loss: 0.026123203337192535, val loss: 0.07228102535009384\n",
      "Epoch 1772: train loss: 0.03996466472744942, val loss: 0.08912952244281769\n",
      "Epoch 1773: train loss: 0.03858295455574989, val loss: 0.05864182859659195\n",
      "Epoch 1774: train loss: 0.03383484110236168, val loss: 0.055669236928224564\n",
      "Epoch 1775: train loss: 0.03367280960083008, val loss: 0.07961823791265488\n",
      "Epoch 1776: train loss: 0.04388885945081711, val loss: 0.07031197100877762\n",
      "Epoch 1777: train loss: 0.03351341187953949, val loss: 0.08508936315774918\n",
      "Epoch 1778: train loss: 0.0284793172031641, val loss: 0.09477750211954117\n",
      "Epoch 1779: train loss: 0.03240620717406273, val loss: 0.09300047904253006\n",
      "Epoch 1780: train loss: 0.03549237921833992, val loss: 0.07528530806303024\n",
      "Epoch 1781: train loss: 0.04351899027824402, val loss: 0.07385088503360748\n",
      "Epoch 1782: train loss: 0.03728143498301506, val loss: 0.07620885223150253\n",
      "Epoch 1783: train loss: 0.03880460187792778, val loss: 0.08362855017185211\n",
      "Epoch 1784: train loss: 0.03499974310398102, val loss: 0.07814006507396698\n",
      "Epoch 1785: train loss: 0.04225398972630501, val loss: 0.08968338370323181\n",
      "Epoch 1786: train loss: 0.031104782596230507, val loss: 0.07643856108188629\n",
      "Epoch 1787: train loss: 0.0465354286134243, val loss: 0.08327265828847885\n",
      "Epoch 1788: train loss: 0.03700205683708191, val loss: 0.08078666776418686\n",
      "Epoch 1789: train loss: 0.03245522826910019, val loss: 0.11032011359930038\n",
      "Epoch 1790: train loss: 0.03858175873756409, val loss: 0.08646362274885178\n",
      "Epoch 1791: train loss: 0.0469200573861599, val loss: 0.047622788697481155\n",
      "Epoch 1792: train loss: 0.03681984543800354, val loss: 0.08537191897630692\n",
      "Epoch 1793: train loss: 0.03895118832588196, val loss: 0.08893793821334839\n",
      "Epoch 1794: train loss: 0.038888975977897644, val loss: 0.09119732677936554\n",
      "Epoch 1795: train loss: 0.033615872263908386, val loss: 0.09972976893186569\n",
      "Epoch 1796: train loss: 0.027970420196652412, val loss: 0.11840371042490005\n",
      "Epoch 1797: train loss: 0.0424790121614933, val loss: 0.08357444405555725\n",
      "Epoch 1798: train loss: 0.03589692339301109, val loss: 0.11111515015363693\n",
      "Epoch 1799: train loss: 0.04063740372657776, val loss: 0.09077689796686172\n",
      "Epoch 1800: train loss: 0.032890960574150085, val loss: 0.06289020925760269\n",
      "Epoch 1801: train loss: 0.039348453283309937, val loss: 0.0796729251742363\n",
      "Epoch 1802: train loss: 0.03556888923048973, val loss: 0.08971768617630005\n",
      "Epoch 1803: train loss: 0.03891771659255028, val loss: 0.08296096324920654\n",
      "Epoch 1804: train loss: 0.038646500557661057, val loss: 0.07937770336866379\n",
      "Epoch 1805: train loss: 0.04390830174088478, val loss: 0.11210479587316513\n",
      "Epoch 1806: train loss: 0.04374569281935692, val loss: 0.07746927440166473\n",
      "Epoch 1807: train loss: 0.03805689513683319, val loss: 0.09837561845779419\n",
      "Epoch 1808: train loss: 0.03159124031662941, val loss: 0.10443129390478134\n",
      "Epoch 1809: train loss: 0.04857345297932625, val loss: 0.08922600001096725\n",
      "Epoch 1810: train loss: 0.050527628511190414, val loss: 0.10054226964712143\n",
      "Epoch 1811: train loss: 0.0470191165804863, val loss: 0.08762388676404953\n",
      "Epoch 1812: train loss: 0.03665563836693764, val loss: 0.06331472843885422\n",
      "Epoch 1813: train loss: 0.033269669860601425, val loss: 0.07846356183290482\n",
      "Epoch 1814: train loss: 0.04837249219417572, val loss: 0.08246088027954102\n",
      "Epoch 1815: train loss: 0.04366638883948326, val loss: 0.09137628227472305\n",
      "Epoch 1816: train loss: 0.03922031447291374, val loss: 0.0917380228638649\n",
      "Epoch 1817: train loss: 0.03765310347080231, val loss: 0.09201107174158096\n",
      "Epoch 1818: train loss: 0.05098195746541023, val loss: 0.08541491627693176\n",
      "Epoch 1819: train loss: 0.03360798582434654, val loss: 0.09128894656896591\n",
      "Epoch 1820: train loss: 0.03304370492696762, val loss: 0.05903565511107445\n",
      "Epoch 1821: train loss: 0.03882402181625366, val loss: 0.07914341241121292\n",
      "Epoch 1822: train loss: 0.041192762553691864, val loss: 0.10660286247730255\n",
      "Epoch 1823: train loss: 0.05098915100097656, val loss: 0.0976719781756401\n",
      "Epoch 1824: train loss: 0.03373602777719498, val loss: 0.09914080053567886\n",
      "Epoch 1825: train loss: 0.04427103325724602, val loss: 0.09643679112195969\n",
      "Epoch 1826: train loss: 0.03592512011528015, val loss: 0.06774457544088364\n",
      "Epoch 1827: train loss: 0.03304034844040871, val loss: 0.14046825468540192\n",
      "Epoch 1828: train loss: 0.049979258328676224, val loss: 0.11387842148542404\n",
      "Epoch 1829: train loss: 0.04560490697622299, val loss: 0.09593753516674042\n",
      "Epoch 1830: train loss: 0.04070639610290527, val loss: 0.08695726096630096\n",
      "Epoch 1831: train loss: 0.04275182634592056, val loss: 0.06549469381570816\n",
      "Epoch 1832: train loss: 0.03748532384634018, val loss: 0.06686940789222717\n",
      "Epoch 1833: train loss: 0.044907618314027786, val loss: 0.07189583778381348\n",
      "Epoch 1834: train loss: 0.039457809180021286, val loss: 0.056313276290893555\n",
      "Epoch 1835: train loss: 0.04185798391699791, val loss: 0.09532870352268219\n",
      "Epoch 1836: train loss: 0.038021158427000046, val loss: 0.09776981174945831\n",
      "Epoch 1837: train loss: 0.03758368268609047, val loss: 0.08093985170125961\n",
      "Epoch 1838: train loss: 0.028697920963168144, val loss: 0.1077437624335289\n",
      "Epoch 1839: train loss: 0.037011533975601196, val loss: 0.05207444354891777\n",
      "Epoch 1840: train loss: 0.04305397346615791, val loss: 0.07972671836614609\n",
      "Epoch 1841: train loss: 0.04582880809903145, val loss: 0.03697291016578674\n",
      "Epoch 1842: train loss: 0.036364179104566574, val loss: 0.07601699233055115\n",
      "Epoch 1843: train loss: 0.041403282433748245, val loss: 0.07216797769069672\n",
      "Epoch 1844: train loss: 0.03813119977712631, val loss: 0.07302066683769226\n",
      "Epoch 1845: train loss: 0.03983822464942932, val loss: 0.06717662513256073\n",
      "Epoch 1846: train loss: 0.03995935246348381, val loss: 0.13109703361988068\n",
      "Epoch 1847: train loss: 0.033293090760707855, val loss: 0.1051284670829773\n",
      "Epoch 1848: train loss: 0.03306244686245918, val loss: 0.11484529823064804\n",
      "Epoch 1849: train loss: 0.03593924269080162, val loss: 0.10328172892332077\n",
      "Epoch 1850: train loss: 0.03646266460418701, val loss: 0.09051733464002609\n",
      "Epoch 1851: train loss: 0.042142629623413086, val loss: 0.08902214467525482\n",
      "Epoch 1852: train loss: 0.041392531245946884, val loss: 0.09059358388185501\n",
      "Epoch 1853: train loss: 0.04647848382592201, val loss: 0.11435172706842422\n",
      "Epoch 1854: train loss: 0.032206494361162186, val loss: 0.1370583027601242\n",
      "Epoch 1855: train loss: 0.036378417164087296, val loss: 0.11469881981611252\n",
      "Epoch 1856: train loss: 0.034980278462171555, val loss: 0.08762361854314804\n",
      "Epoch 1857: train loss: 0.037447988986968994, val loss: 0.07785660028457642\n",
      "Epoch 1858: train loss: 0.04579540342092514, val loss: 0.09776061028242111\n",
      "Epoch 1859: train loss: 0.03351205959916115, val loss: 0.10908370465040207\n",
      "Epoch 1860: train loss: 0.03761164844036102, val loss: 0.08350693434476852\n",
      "Epoch 1861: train loss: 0.03640604019165039, val loss: 0.0629391148686409\n",
      "Epoch 1862: train loss: 0.02963813953101635, val loss: 0.0727015882730484\n",
      "Epoch 1863: train loss: 0.04335513338446617, val loss: 0.11056168377399445\n",
      "Epoch 1864: train loss: 0.03143010288476944, val loss: 0.026767505332827568\n",
      "Epoch 1865: train loss: 0.041243914514780045, val loss: 0.07736877351999283\n",
      "Epoch 1866: train loss: 0.03593149036169052, val loss: 0.10303252190351486\n",
      "Epoch 1867: train loss: 0.03525123745203018, val loss: 0.06400178372859955\n",
      "Epoch 1868: train loss: 0.024914050474762917, val loss: 0.06762798875570297\n",
      "Epoch 1869: train loss: 0.03336509317159653, val loss: 0.10721450299024582\n",
      "Epoch 1870: train loss: 0.035127945244312286, val loss: 0.09754911810159683\n",
      "Epoch 1871: train loss: 0.04276355728507042, val loss: 0.07338517904281616\n",
      "Epoch 1872: train loss: 0.0322747603058815, val loss: 0.08290683478116989\n",
      "Epoch 1873: train loss: 0.03847565874457359, val loss: 0.07367335259914398\n",
      "Epoch 1874: train loss: 0.04459716007113457, val loss: 0.06715234369039536\n",
      "Epoch 1875: train loss: 0.0468897707760334, val loss: 0.13135062158107758\n",
      "Epoch 1876: train loss: 0.035877808928489685, val loss: 0.11120779812335968\n",
      "Epoch 1877: train loss: 0.0446830652654171, val loss: 0.09464738517999649\n",
      "Epoch 1878: train loss: 0.03029598295688629, val loss: 0.09843428432941437\n",
      "Epoch 1879: train loss: 0.032424889504909515, val loss: 0.04980885609984398\n",
      "Epoch 1880: train loss: 0.03380410373210907, val loss: 0.055023130029439926\n",
      "Epoch 1881: train loss: 0.03674550727009773, val loss: 0.09302052110433578\n",
      "Epoch 1882: train loss: 0.03837541118264198, val loss: 0.08718717098236084\n",
      "Epoch 1883: train loss: 0.037688590586185455, val loss: 0.0676601231098175\n",
      "Epoch 1884: train loss: 0.036626189947128296, val loss: 0.0670061707496643\n",
      "Epoch 1885: train loss: 0.034287940710783005, val loss: 0.08090441673994064\n",
      "Epoch 1886: train loss: 0.033636607229709625, val loss: 0.12732334434986115\n",
      "Epoch 1887: train loss: 0.03615359216928482, val loss: 0.09920064359903336\n",
      "Epoch 1888: train loss: 0.03618388995528221, val loss: 0.05942239984869957\n",
      "Epoch 1889: train loss: 0.029936663806438446, val loss: 0.08638330549001694\n",
      "Epoch 1890: train loss: 0.031058503314852715, val loss: 0.08788104355335236\n",
      "Epoch 1891: train loss: 0.029917575418949127, val loss: 0.08963153511285782\n",
      "Epoch 1892: train loss: 0.030161889269948006, val loss: 0.07016099244356155\n",
      "Epoch 1893: train loss: 0.03798636794090271, val loss: 0.0800584927201271\n",
      "Epoch 1894: train loss: 0.03522154688835144, val loss: 0.09129517525434494\n",
      "Epoch 1895: train loss: 0.03078959323465824, val loss: 0.0882069393992424\n",
      "Epoch 1896: train loss: 0.041870489716529846, val loss: 0.1071290597319603\n",
      "Epoch 1897: train loss: 0.03667151927947998, val loss: 0.08159910142421722\n",
      "Epoch 1898: train loss: 0.040641993284225464, val loss: 0.07725103199481964\n",
      "Epoch 1899: train loss: 0.03076961636543274, val loss: 0.10166837275028229\n",
      "Epoch 1900: train loss: 0.040080804377794266, val loss: 0.07790616154670715\n",
      "Epoch 1901: train loss: 0.045216597616672516, val loss: 0.06162795051932335\n",
      "Epoch 1902: train loss: 0.034881092607975006, val loss: 0.10826589912176132\n",
      "Epoch 1903: train loss: 0.031445179134607315, val loss: 0.08495696634054184\n",
      "Epoch 1904: train loss: 0.03567144647240639, val loss: 0.07291558384895325\n",
      "Epoch 1905: train loss: 0.029341286048293114, val loss: 0.09910009056329727\n",
      "Epoch 1906: train loss: 0.03981376439332962, val loss: 0.07463651150465012\n",
      "Epoch 1907: train loss: 0.03634580224752426, val loss: 0.1019928976893425\n",
      "Epoch 1908: train loss: 0.03707590326666832, val loss: 0.09839246422052383\n",
      "Epoch 1909: train loss: 0.0375061072409153, val loss: 0.09924028813838959\n",
      "Epoch 1910: train loss: 0.03980068862438202, val loss: 0.06824960559606552\n",
      "Epoch 1911: train loss: 0.043694522231817245, val loss: 0.09306416660547256\n",
      "Epoch 1912: train loss: 0.05055329203605652, val loss: 0.10918750613927841\n",
      "Epoch 1913: train loss: 0.03581917658448219, val loss: 0.11538279056549072\n",
      "Epoch 1914: train loss: 0.033764541149139404, val loss: 0.09345785528421402\n",
      "Epoch 1915: train loss: 0.03402696177363396, val loss: 0.10285991430282593\n",
      "Epoch 1916: train loss: 0.029210178181529045, val loss: 0.11942746490240097\n",
      "Epoch 1917: train loss: 0.037844084203243256, val loss: 0.09772365540266037\n",
      "Epoch 1918: train loss: 0.03562341630458832, val loss: 0.12970969080924988\n",
      "Epoch 1919: train loss: 0.037540800869464874, val loss: 0.13358552753925323\n",
      "Epoch 1920: train loss: 0.04437309131026268, val loss: 0.08981329202651978\n",
      "Epoch 1921: train loss: 0.031108759343624115, val loss: 0.09148269146680832\n",
      "Epoch 1922: train loss: 0.03926314786076546, val loss: 0.09287291020154953\n",
      "Epoch 1923: train loss: 0.03372252732515335, val loss: 0.08980552852153778\n",
      "Epoch 1924: train loss: 0.051353272050619125, val loss: 0.04664284735918045\n",
      "Epoch 1925: train loss: 0.031632788479328156, val loss: 0.07402801513671875\n",
      "Epoch 1926: train loss: 0.027261167764663696, val loss: 0.05757669359445572\n",
      "Epoch 1927: train loss: 0.03247438371181488, val loss: 0.14090776443481445\n",
      "Epoch 1928: train loss: 0.04204491525888443, val loss: 0.0867750272154808\n",
      "Epoch 1929: train loss: 0.036564119160175323, val loss: 0.08505082130432129\n",
      "Epoch 1930: train loss: 0.03390270844101906, val loss: 0.08665330708026886\n",
      "Epoch 1931: train loss: 0.03057904914021492, val loss: 0.09701883047819138\n",
      "Epoch 1932: train loss: 0.04345592483878136, val loss: 0.11390870064496994\n",
      "Epoch 1933: train loss: 0.03270775452256203, val loss: 0.0807538852095604\n",
      "Epoch 1934: train loss: 0.030225330963730812, val loss: 0.09780170768499374\n",
      "Epoch 1935: train loss: 0.03648221120238304, val loss: 0.1199469119310379\n",
      "Epoch 1936: train loss: 0.04592098668217659, val loss: 0.11152507364749908\n",
      "Epoch 1937: train loss: 0.03742488473653793, val loss: 0.07899697870016098\n",
      "Epoch 1938: train loss: 0.04412929713726044, val loss: 0.13322755694389343\n",
      "Epoch 1939: train loss: 0.03651019185781479, val loss: 0.10270541161298752\n",
      "Epoch 1940: train loss: 0.03609069436788559, val loss: 0.09628249704837799\n",
      "Epoch 1941: train loss: 0.04731382802128792, val loss: 0.05388781055808067\n",
      "Epoch 1942: train loss: 0.04508242756128311, val loss: 0.1046048179268837\n",
      "Epoch 1943: train loss: 0.039126016199588776, val loss: 0.08664057403802872\n",
      "Epoch 1944: train loss: 0.03770482540130615, val loss: 0.07749245315790176\n",
      "Epoch 1945: train loss: 0.036684300750494, val loss: 0.08665596693754196\n",
      "Epoch 1946: train loss: 0.03429115563631058, val loss: 0.0720895454287529\n",
      "Epoch 1947: train loss: 0.03582620620727539, val loss: 0.13160672783851624\n",
      "Epoch 1948: train loss: 0.03605606406927109, val loss: 0.10872109979391098\n",
      "Epoch 1949: train loss: 0.03904437646269798, val loss: 0.09501229226589203\n",
      "Epoch 1950: train loss: 0.041113387793302536, val loss: 0.08246532827615738\n",
      "Epoch 1951: train loss: 0.0333997942507267, val loss: 0.09664280712604523\n",
      "Epoch 1952: train loss: 0.04425594210624695, val loss: 0.05759989842772484\n",
      "Epoch 1953: train loss: 0.0501028448343277, val loss: 0.11317332834005356\n",
      "Epoch 1954: train loss: 0.04412960633635521, val loss: 0.0828593298792839\n",
      "Epoch 1955: train loss: 0.036206185817718506, val loss: 0.14910995960235596\n",
      "Epoch 1956: train loss: 0.045145269483327866, val loss: 0.10684128850698471\n",
      "Epoch 1957: train loss: 0.034894879907369614, val loss: 0.0798865482211113\n",
      "Epoch 1958: train loss: 0.03343130648136139, val loss: 0.07057135552167892\n",
      "Epoch 1959: train loss: 0.03032051771879196, val loss: 0.10473370552062988\n",
      "Epoch 1960: train loss: 0.0412493497133255, val loss: 0.09426996111869812\n",
      "Epoch 1961: train loss: 0.03357909992337227, val loss: 0.07567080855369568\n",
      "Epoch 1962: train loss: 0.04119133949279785, val loss: 0.0919279158115387\n",
      "Epoch 1963: train loss: 0.04256768152117729, val loss: 0.06919678300619125\n",
      "Epoch 1964: train loss: 0.04889075085520744, val loss: 0.08974527567625046\n",
      "Epoch 1965: train loss: 0.033288512378931046, val loss: 0.07189164310693741\n",
      "Epoch 1966: train loss: 0.0391993373632431, val loss: 0.08688586205244064\n",
      "Epoch 1967: train loss: 0.04426301643252373, val loss: 0.08371571451425552\n",
      "Epoch 1968: train loss: 0.03881784528493881, val loss: 0.08401595056056976\n",
      "Epoch 1969: train loss: 0.03617856279015541, val loss: 0.09004265069961548\n",
      "Epoch 1970: train loss: 0.03563365712761879, val loss: 0.12578140199184418\n",
      "Epoch 1971: train loss: 0.03759312629699707, val loss: 0.07983364909887314\n",
      "Epoch 1972: train loss: 0.044427551329135895, val loss: 0.07811414450407028\n",
      "Epoch 1973: train loss: 0.03838730975985527, val loss: 0.12342915683984756\n",
      "Epoch 1974: train loss: 0.04423850402235985, val loss: 0.07549317926168442\n",
      "Epoch 1975: train loss: 0.04522347077727318, val loss: 0.07596717774868011\n",
      "Epoch 1976: train loss: 0.04322642460465431, val loss: 0.10205745697021484\n",
      "Epoch 1977: train loss: 0.04382004961371422, val loss: 0.06549569219350815\n",
      "Epoch 1978: train loss: 0.0403168685734272, val loss: 0.07030462473630905\n",
      "Epoch 1979: train loss: 0.04179671034216881, val loss: 0.06974917650222778\n",
      "Epoch 1980: train loss: 0.040181562304496765, val loss: 0.06289661675691605\n",
      "Epoch 1981: train loss: 0.04543434455990791, val loss: 0.09851094335317612\n",
      "Epoch 1982: train loss: 0.05049297586083412, val loss: 0.10533102601766586\n",
      "Epoch 1983: train loss: 0.041240379214286804, val loss: 0.0975397452712059\n",
      "Epoch 1984: train loss: 0.0462958887219429, val loss: 0.08769440650939941\n",
      "Epoch 1985: train loss: 0.04371471703052521, val loss: 0.11223571747541428\n",
      "Epoch 1986: train loss: 0.043545354157686234, val loss: 0.06995450705289841\n",
      "Epoch 1987: train loss: 0.04336154833436012, val loss: 0.08595901727676392\n",
      "Epoch 1988: train loss: 0.0349823534488678, val loss: 0.08970947563648224\n",
      "Epoch 1989: train loss: 0.036209218204021454, val loss: 0.0974995493888855\n",
      "Epoch 1990: train loss: 0.04038562253117561, val loss: 0.07629939168691635\n",
      "Epoch 1991: train loss: 0.037416644394397736, val loss: 0.07085768133401871\n",
      "Epoch 1992: train loss: 0.04152047634124756, val loss: 0.09213070571422577\n",
      "Epoch 1993: train loss: 0.0405728705227375, val loss: 0.0913703665137291\n",
      "Epoch 1994: train loss: 0.040395598858594894, val loss: 0.12113723903894424\n",
      "Epoch 1995: train loss: 0.03804255276918411, val loss: 0.06467336416244507\n",
      "Epoch 1996: train loss: 0.034660376608371735, val loss: 0.05515604093670845\n",
      "Epoch 1997: train loss: 0.0325389988720417, val loss: 0.08293882757425308\n",
      "Epoch 1998: train loss: 0.03806564211845398, val loss: 0.07312015444040298\n",
      "Epoch 1999: train loss: 0.03784479200839996, val loss: 0.09611468762159348\n",
      "Epoch 2000: train loss: 0.0335271954536438, val loss: 0.087315633893013\n",
      "Epoch 2001: train loss: 0.03630831092596054, val loss: 0.10754811018705368\n",
      "Epoch 2002: train loss: 0.04661691561341286, val loss: 0.13342776894569397\n",
      "Epoch 2003: train loss: 0.03856772184371948, val loss: 0.08602233976125717\n",
      "Epoch 2004: train loss: 0.03483171388506889, val loss: 0.05983671173453331\n",
      "Epoch 2005: train loss: 0.03478456661105156, val loss: 0.10697245597839355\n",
      "Epoch 2006: train loss: 0.04254601150751114, val loss: 0.06538789719343185\n",
      "Epoch 2007: train loss: 0.041668858379125595, val loss: 0.06110674887895584\n",
      "Epoch 2008: train loss: 0.04390358552336693, val loss: 0.0891154333949089\n",
      "Epoch 2009: train loss: 0.0398755706846714, val loss: 0.1330684870481491\n",
      "Epoch 2010: train loss: 0.04210856929421425, val loss: 0.07772844284772873\n",
      "Epoch 2011: train loss: 0.03477482870221138, val loss: 0.05568852648139\n",
      "Epoch 2012: train loss: 0.04731370881199837, val loss: 0.05018466338515282\n",
      "Epoch 2013: train loss: 0.03406500816345215, val loss: 0.1399887651205063\n",
      "Epoch 2014: train loss: 0.035673629492521286, val loss: 0.10285532474517822\n",
      "Epoch 2015: train loss: 0.04156384989619255, val loss: 0.10010721534490585\n",
      "Epoch 2016: train loss: 0.04336288198828697, val loss: 0.0529974065721035\n",
      "Epoch 2017: train loss: 0.031715162098407745, val loss: 0.10867605358362198\n",
      "Epoch 2018: train loss: 0.033122070133686066, val loss: 0.11092720180749893\n",
      "Epoch 2019: train loss: 0.03315909951925278, val loss: 0.12559282779693604\n",
      "Epoch 2020: train loss: 0.0328415222465992, val loss: 0.07181760668754578\n",
      "Epoch 2021: train loss: 0.039557941257953644, val loss: 0.10292583703994751\n",
      "Epoch 2022: train loss: 0.028311045840382576, val loss: 0.08606477081775665\n",
      "Epoch 2023: train loss: 0.028176281601190567, val loss: 0.0994265004992485\n",
      "Epoch 2024: train loss: 0.041297830641269684, val loss: 0.07394818216562271\n",
      "Epoch 2025: train loss: 0.047945622354745865, val loss: 0.08520438522100449\n",
      "Epoch 2026: train loss: 0.0472572036087513, val loss: 0.07012512534856796\n",
      "Epoch 2027: train loss: 0.041456542909145355, val loss: 0.10558588802814484\n",
      "Epoch 2028: train loss: 0.04350650683045387, val loss: 0.07996462285518646\n",
      "Epoch 2029: train loss: 0.0373661182820797, val loss: 0.11220159381628036\n",
      "Epoch 2030: train loss: 0.0387006513774395, val loss: 0.11953820288181305\n",
      "Epoch 2031: train loss: 0.037584275007247925, val loss: 0.09873727709054947\n",
      "Epoch 2032: train loss: 0.03542439639568329, val loss: 0.11999139934778214\n",
      "Epoch 2033: train loss: 0.033472537994384766, val loss: 0.07556389272212982\n",
      "Epoch 2034: train loss: 0.03545309230685234, val loss: 0.06868309527635574\n",
      "Epoch 2035: train loss: 0.04026246815919876, val loss: 0.10386376827955246\n",
      "Epoch 2036: train loss: 0.037331148982048035, val loss: 0.0741942748427391\n",
      "Epoch 2037: train loss: 0.03444940224289894, val loss: 0.07258881628513336\n",
      "Epoch 2038: train loss: 0.04858145862817764, val loss: 0.08636019378900528\n",
      "Epoch 2039: train loss: 0.03296545892953873, val loss: 0.05863837152719498\n",
      "Epoch 2040: train loss: 0.04044880345463753, val loss: 0.06369980424642563\n",
      "Epoch 2041: train loss: 0.030224313959479332, val loss: 0.09102306514978409\n",
      "Epoch 2042: train loss: 0.02935991808772087, val loss: 0.08803091943264008\n",
      "Epoch 2043: train loss: 0.03071444295346737, val loss: 0.07533261924982071\n",
      "Epoch 2044: train loss: 0.031320177018642426, val loss: 0.10456950962543488\n",
      "Epoch 2045: train loss: 0.04185549542307854, val loss: 0.07662926614284515\n",
      "Epoch 2046: train loss: 0.03973464295268059, val loss: 0.06897813081741333\n",
      "Epoch 2047: train loss: 0.04459748789668083, val loss: 0.08098172396421432\n",
      "Epoch 2048: train loss: 0.029499467462301254, val loss: 0.032958291471004486\n",
      "Epoch 2049: train loss: 0.036539725959300995, val loss: 0.06293728947639465\n",
      "Epoch 2050: train loss: 0.033342111855745316, val loss: 0.10990773886442184\n",
      "Epoch 2051: train loss: 0.04427831992506981, val loss: 0.14206591248512268\n",
      "Epoch 2052: train loss: 0.035032786428928375, val loss: 0.11778872460126877\n",
      "Epoch 2053: train loss: 0.037889063358306885, val loss: 0.08884917944669724\n",
      "Epoch 2054: train loss: 0.04632805287837982, val loss: 0.049723681062459946\n",
      "Epoch 2055: train loss: 0.036401957273483276, val loss: 0.09505089372396469\n",
      "Epoch 2056: train loss: 0.03594089299440384, val loss: 0.06834471970796585\n",
      "Epoch 2057: train loss: 0.03412772715091705, val loss: 0.0443042516708374\n",
      "Epoch 2058: train loss: 0.03627096489071846, val loss: 0.069227434694767\n",
      "Epoch 2059: train loss: 0.026802053675055504, val loss: 0.11831610649824142\n",
      "Epoch 2060: train loss: 0.03406602889299393, val loss: 0.10683727264404297\n",
      "Epoch 2061: train loss: 0.03746689483523369, val loss: 0.11259008944034576\n",
      "Epoch 2062: train loss: 0.04835197702050209, val loss: 0.08553745597600937\n",
      "Epoch 2063: train loss: 0.034530818462371826, val loss: 0.07962450385093689\n",
      "Epoch 2064: train loss: 0.03533901274204254, val loss: 0.10213214159011841\n",
      "Epoch 2065: train loss: 0.038181837648153305, val loss: 0.08394155651330948\n",
      "Epoch 2066: train loss: 0.05424386262893677, val loss: 0.13383448123931885\n",
      "Epoch 2067: train loss: 0.043419741094112396, val loss: 0.06967625766992569\n",
      "Epoch 2068: train loss: 0.039912059903144836, val loss: 0.07950203865766525\n",
      "Epoch 2069: train loss: 0.04165061190724373, val loss: 0.0826517641544342\n",
      "Epoch 2070: train loss: 0.032320670783519745, val loss: 0.0821753442287445\n",
      "Epoch 2071: train loss: 0.041980016976594925, val loss: 0.11003486067056656\n",
      "Epoch 2072: train loss: 0.02754305861890316, val loss: 0.0772605910897255\n",
      "Epoch 2073: train loss: 0.03492232412099838, val loss: 0.0505044125020504\n",
      "Epoch 2074: train loss: 0.02937590330839157, val loss: 0.08138596266508102\n",
      "Epoch 2075: train loss: 0.04937122389674187, val loss: 0.08963846415281296\n",
      "Epoch 2076: train loss: 0.029154926538467407, val loss: 0.14301715791225433\n",
      "Epoch 2077: train loss: 0.045504264533519745, val loss: 0.11565472930669785\n",
      "Epoch 2078: train loss: 0.03188129514455795, val loss: 0.1062004566192627\n",
      "Epoch 2079: train loss: 0.03738241270184517, val loss: 0.11823096126317978\n",
      "Epoch 2080: train loss: 0.03469378128647804, val loss: 0.14756250381469727\n",
      "Epoch 2081: train loss: 0.030459946021437645, val loss: 0.10726305097341537\n",
      "Epoch 2082: train loss: 0.03922298550605774, val loss: 0.11522412300109863\n",
      "Epoch 2083: train loss: 0.04313564673066139, val loss: 0.09403086453676224\n",
      "Epoch 2084: train loss: 0.03927667438983917, val loss: 0.10332802683115005\n",
      "Epoch 2085: train loss: 0.034417733550071716, val loss: 0.0693700984120369\n",
      "Epoch 2086: train loss: 0.03404412791132927, val loss: 0.09079022705554962\n",
      "Epoch 2087: train loss: 0.027481429278850555, val loss: 0.1015242338180542\n",
      "Epoch 2088: train loss: 0.0334697850048542, val loss: 0.0731968805193901\n",
      "Epoch 2089: train loss: 0.03447675704956055, val loss: 0.1056915745139122\n",
      "Epoch 2090: train loss: 0.03812350705265999, val loss: 0.07516759634017944\n",
      "Epoch 2091: train loss: 0.04193587228655815, val loss: 0.09260499477386475\n",
      "Epoch 2092: train loss: 0.04272410273551941, val loss: 0.08357559889554977\n",
      "Epoch 2093: train loss: 0.03782966360449791, val loss: 0.07339968532323837\n",
      "Epoch 2094: train loss: 0.042353902012109756, val loss: 0.07945215702056885\n",
      "Epoch 2095: train loss: 0.041665948927402496, val loss: 0.10882877558469772\n",
      "Epoch 2096: train loss: 0.040166739374399185, val loss: 0.08946529775857925\n",
      "Epoch 2097: train loss: 0.04351849481463432, val loss: 0.09219013154506683\n",
      "Epoch 2098: train loss: 0.043113768100738525, val loss: 0.0704156681895256\n",
      "Epoch 2099: train loss: 0.03585333377122879, val loss: 0.07965978235006332\n",
      "Epoch 2100: train loss: 0.03100866824388504, val loss: 0.077170230448246\n",
      "Epoch 2101: train loss: 0.03802957758307457, val loss: 0.0817796140909195\n",
      "Epoch 2102: train loss: 0.03086782619357109, val loss: 0.10633950680494308\n",
      "Epoch 2103: train loss: 0.03258048743009567, val loss: 0.09053955972194672\n",
      "Epoch 2104: train loss: 0.047139037400484085, val loss: 0.07498650997877121\n",
      "Epoch 2105: train loss: 0.04204605892300606, val loss: 0.09650107473134995\n",
      "Epoch 2106: train loss: 0.05233406648039818, val loss: 0.05928832292556763\n",
      "Epoch 2107: train loss: 0.03863752260804176, val loss: 0.09991707652807236\n",
      "Epoch 2108: train loss: 0.040652208030223846, val loss: 0.1049579307436943\n",
      "Epoch 2109: train loss: 0.04019387811422348, val loss: 0.11114891618490219\n",
      "Epoch 2110: train loss: 0.04964922368526459, val loss: 0.08989468216896057\n",
      "Epoch 2111: train loss: 0.04975185543298721, val loss: 0.10130635648965836\n",
      "Epoch 2112: train loss: 0.03744581714272499, val loss: 0.0667613297700882\n",
      "Epoch 2113: train loss: 0.033578284084796906, val loss: 0.058766920119524\n",
      "Epoch 2114: train loss: 0.03424302861094475, val loss: 0.05956379324197769\n",
      "Epoch 2115: train loss: 0.033659677952528, val loss: 0.10366830974817276\n",
      "Epoch 2116: train loss: 0.03700106218457222, val loss: 0.06598023325204849\n",
      "Epoch 2117: train loss: 0.04148620739579201, val loss: 0.08817916363477707\n",
      "Epoch 2118: train loss: 0.03988198935985565, val loss: 0.08351602405309677\n",
      "Epoch 2119: train loss: 0.03156203776597977, val loss: 0.08659905195236206\n",
      "Epoch 2120: train loss: 0.04555623233318329, val loss: 0.09117159992456436\n",
      "Epoch 2121: train loss: 0.039832353591918945, val loss: 0.07898740470409393\n",
      "Epoch 2122: train loss: 0.03677745908498764, val loss: 0.13201063871383667\n",
      "Epoch 2123: train loss: 0.039277128875255585, val loss: 0.09957291930913925\n",
      "Epoch 2124: train loss: 0.03301580622792244, val loss: 0.09984033554792404\n",
      "Epoch 2125: train loss: 0.038903120905160904, val loss: 0.10138054937124252\n",
      "Epoch 2126: train loss: 0.045611388981342316, val loss: 0.08821988105773926\n",
      "Epoch 2127: train loss: 0.03384074196219444, val loss: 0.0701630488038063\n",
      "Epoch 2128: train loss: 0.0357990525662899, val loss: 0.05562293156981468\n",
      "Epoch 2129: train loss: 0.032862886786460876, val loss: 0.061463117599487305\n",
      "Epoch 2130: train loss: 0.03896789997816086, val loss: 0.06413333863019943\n",
      "Epoch 2131: train loss: 0.047046855092048645, val loss: 0.11467750370502472\n",
      "Epoch 2132: train loss: 0.032551366835832596, val loss: 0.09456924349069595\n",
      "Epoch 2133: train loss: 0.03370868042111397, val loss: 0.08296044915914536\n",
      "Epoch 2134: train loss: 0.02734316699206829, val loss: 0.08513915538787842\n",
      "Epoch 2135: train loss: 0.035562917590141296, val loss: 0.062489088624715805\n",
      "Epoch 2136: train loss: 0.0296885184943676, val loss: 0.11553799360990524\n",
      "Epoch 2137: train loss: 0.029716968536376953, val loss: 0.08947619050741196\n",
      "Epoch 2138: train loss: 0.033061180263757706, val loss: 0.08116593211889267\n",
      "Epoch 2139: train loss: 0.03642157465219498, val loss: 0.0858558788895607\n",
      "Epoch 2140: train loss: 0.03388567641377449, val loss: 0.07998902350664139\n",
      "Epoch 2141: train loss: 0.02865995094180107, val loss: 0.06588619202375412\n",
      "Epoch 2142: train loss: 0.03884636238217354, val loss: 0.07818886637687683\n",
      "Epoch 2143: train loss: 0.02927445061504841, val loss: 0.10768666118383408\n",
      "Epoch 2144: train loss: 0.02939915657043457, val loss: 0.04956768453121185\n",
      "Epoch 2145: train loss: 0.03704628348350525, val loss: 0.06807983666658401\n",
      "Epoch 2146: train loss: 0.03272968530654907, val loss: 0.09790293127298355\n",
      "Epoch 2147: train loss: 0.044631704688072205, val loss: 0.08669836819171906\n",
      "Epoch 2148: train loss: 0.023554034531116486, val loss: 0.07862493395805359\n",
      "Epoch 2149: train loss: 0.03490586206316948, val loss: 0.06263293325901031\n",
      "Epoch 2150: train loss: 0.0328323058784008, val loss: 0.08000297844409943\n",
      "Epoch 2151: train loss: 0.03326153755187988, val loss: 0.08981489390134811\n",
      "Epoch 2152: train loss: 0.028223061934113503, val loss: 0.10357463359832764\n",
      "Epoch 2153: train loss: 0.03176230937242508, val loss: 0.08082972466945648\n",
      "Epoch 2154: train loss: 0.03435343876481056, val loss: 0.09474381059408188\n",
      "Epoch 2155: train loss: 0.03190404921770096, val loss: 0.07817595452070236\n",
      "Epoch 2156: train loss: 0.040598221123218536, val loss: 0.07900740951299667\n",
      "Epoch 2157: train loss: 0.03514934703707695, val loss: 0.0560055747628212\n",
      "Epoch 2158: train loss: 0.036198511719703674, val loss: 0.10907713323831558\n",
      "Epoch 2159: train loss: 0.034134186804294586, val loss: 0.06792952120304108\n",
      "Epoch 2160: train loss: 0.035144928842782974, val loss: 0.06320308148860931\n",
      "Epoch 2161: train loss: 0.035084255039691925, val loss: 0.05813102051615715\n",
      "Epoch 2162: train loss: 0.02786141075193882, val loss: 0.07559924572706223\n",
      "Epoch 2163: train loss: 0.0323491171002388, val loss: 0.039230089634656906\n",
      "Epoch 2164: train loss: 0.03513520583510399, val loss: 0.09112328290939331\n",
      "Epoch 2165: train loss: 0.032533708959817886, val loss: 0.06284494698047638\n",
      "Epoch 2166: train loss: 0.03385205194354057, val loss: 0.1254219263792038\n",
      "Epoch 2167: train loss: 0.024295950308442116, val loss: 0.0945623517036438\n",
      "Epoch 2168: train loss: 0.035992059856653214, val loss: 0.07588481158018112\n",
      "Epoch 2169: train loss: 0.03617280721664429, val loss: 0.0755896344780922\n",
      "Epoch 2170: train loss: 0.03581399470567703, val loss: 0.07578641176223755\n",
      "Epoch 2171: train loss: 0.03644683212041855, val loss: 0.07082703709602356\n",
      "Epoch 2172: train loss: 0.036214664578437805, val loss: 0.0371839664876461\n",
      "Epoch 2173: train loss: 0.034550365060567856, val loss: 0.09457935392856598\n",
      "Epoch 2174: train loss: 0.03895565867424011, val loss: 0.09682665765285492\n",
      "Epoch 2175: train loss: 0.03352247551083565, val loss: 0.11740590631961823\n",
      "Epoch 2176: train loss: 0.031196709722280502, val loss: 0.09344359487295151\n",
      "Epoch 2177: train loss: 0.03905699402093887, val loss: 0.09033030271530151\n",
      "Epoch 2178: train loss: 0.036695074290037155, val loss: 0.06456483900547028\n",
      "Epoch 2179: train loss: 0.030788784846663475, val loss: 0.09258400648832321\n",
      "Epoch 2180: train loss: 0.03676345571875572, val loss: 0.12339041382074356\n",
      "Epoch 2181: train loss: 0.03819449245929718, val loss: 0.08503475040197372\n",
      "Epoch 2182: train loss: 0.035582516342401505, val loss: 0.12626612186431885\n",
      "Epoch 2183: train loss: 0.037210509181022644, val loss: 0.0738363265991211\n",
      "Epoch 2184: train loss: 0.030427128076553345, val loss: 0.13412687182426453\n",
      "Epoch 2185: train loss: 0.027338635176420212, val loss: 0.14052055776119232\n",
      "Epoch 2186: train loss: 0.04346513748168945, val loss: 0.08267048746347427\n",
      "Epoch 2187: train loss: 0.0408792607486248, val loss: 0.13262726366519928\n",
      "Epoch 2188: train loss: 0.03587222471833229, val loss: 0.06964895874261856\n",
      "Epoch 2189: train loss: 0.04193643108010292, val loss: 0.0730975866317749\n",
      "Epoch 2190: train loss: 0.04248236492276192, val loss: 0.08960387855768204\n",
      "Epoch 2191: train loss: 0.037631869316101074, val loss: 0.06483455747365952\n",
      "Epoch 2192: train loss: 0.042121466249227524, val loss: 0.07956130802631378\n",
      "Epoch 2193: train loss: 0.03670723736286163, val loss: 0.057384975254535675\n",
      "Epoch 2194: train loss: 0.032851967960596085, val loss: 0.06584230810403824\n",
      "Epoch 2195: train loss: 0.04493650421500206, val loss: 0.09745100885629654\n",
      "Epoch 2196: train loss: 0.032557614147663116, val loss: 0.12696442008018494\n",
      "Epoch 2197: train loss: 0.03449448198080063, val loss: 0.08588297665119171\n",
      "Epoch 2198: train loss: 0.0384732149541378, val loss: 0.1205926463007927\n",
      "Epoch 2199: train loss: 0.03800441697239876, val loss: 0.0836203321814537\n",
      "Epoch 2200: train loss: 0.0359564833343029, val loss: 0.06462258845567703\n",
      "Epoch 2201: train loss: 0.04740964621305466, val loss: 0.08784331381320953\n",
      "Epoch 2202: train loss: 0.03431976959109306, val loss: 0.0815134271979332\n",
      "Epoch 2203: train loss: 0.03724342957139015, val loss: 0.1090054139494896\n",
      "Epoch 2204: train loss: 0.038525138050317764, val loss: 0.04655187204480171\n",
      "Epoch 2205: train loss: 0.028608277440071106, val loss: 0.1123386025428772\n",
      "Epoch 2206: train loss: 0.02896302007138729, val loss: 0.07435484230518341\n",
      "Epoch 2207: train loss: 0.028175344690680504, val loss: 0.09222420305013657\n",
      "Epoch 2208: train loss: 0.03186728060245514, val loss: 0.08872824162244797\n",
      "Epoch 2209: train loss: 0.03642017021775246, val loss: 0.08019090443849564\n",
      "Epoch 2210: train loss: 0.028032701462507248, val loss: 0.06472563743591309\n",
      "Epoch 2211: train loss: 0.034258175641298294, val loss: 0.11482001841068268\n",
      "Epoch 2212: train loss: 0.03205818310379982, val loss: 0.09414718300104141\n",
      "Epoch 2213: train loss: 0.026653386652469635, val loss: 0.10194945335388184\n",
      "Epoch 2214: train loss: 0.042403969913721085, val loss: 0.08084181696176529\n",
      "Epoch 2215: train loss: 0.0468369759619236, val loss: 0.10604458302259445\n",
      "Epoch 2216: train loss: 0.03629390522837639, val loss: 0.09265270829200745\n",
      "Epoch 2217: train loss: 0.032021280378103256, val loss: 0.09675970673561096\n",
      "Epoch 2218: train loss: 0.04073847085237503, val loss: 0.12243705987930298\n",
      "Epoch 2219: train loss: 0.02992330491542816, val loss: 0.0886867418885231\n",
      "Epoch 2220: train loss: 0.041693318635225296, val loss: 0.14393781125545502\n",
      "Epoch 2221: train loss: 0.048831094056367874, val loss: 0.08387361466884613\n",
      "Epoch 2222: train loss: 0.0440913625061512, val loss: 0.09725018590688705\n",
      "Epoch 2223: train loss: 0.037727054208517075, val loss: 0.06418152153491974\n",
      "Epoch 2224: train loss: 0.033926770091056824, val loss: 0.05626006796956062\n",
      "Epoch 2225: train loss: 0.03878464177250862, val loss: 0.08113142102956772\n",
      "Epoch 2226: train loss: 0.03973295912146568, val loss: 0.10431002825498581\n",
      "Epoch 2227: train loss: 0.03582088649272919, val loss: 0.11378472298383713\n",
      "Epoch 2228: train loss: 0.042756337672472, val loss: 0.10137828439474106\n",
      "Epoch 2229: train loss: 0.04839282110333443, val loss: 0.09167931973934174\n",
      "Epoch 2230: train loss: 0.0410887748003006, val loss: 0.06909594684839249\n",
      "Epoch 2231: train loss: 0.04001561924815178, val loss: 0.09909378737211227\n",
      "Epoch 2232: train loss: 0.03547624498605728, val loss: 0.04832448065280914\n",
      "Epoch 2233: train loss: 0.03710298240184784, val loss: 0.10503484308719635\n",
      "Epoch 2234: train loss: 0.03721589595079422, val loss: 0.08750972151756287\n",
      "Epoch 2235: train loss: 0.0393860824406147, val loss: 0.07205165922641754\n",
      "Epoch 2236: train loss: 0.03531383350491524, val loss: 0.08105164021253586\n",
      "Epoch 2237: train loss: 0.03374219685792923, val loss: 0.10337131470441818\n",
      "Epoch 2238: train loss: 0.04633781313896179, val loss: 0.043168991804122925\n",
      "Epoch 2239: train loss: 0.037967097014188766, val loss: 0.05059955269098282\n",
      "Epoch 2240: train loss: 0.036684077233076096, val loss: 0.10957960039377213\n",
      "Epoch 2241: train loss: 0.04870375618338585, val loss: 0.12193351238965988\n",
      "Epoch 2242: train loss: 0.04391154274344444, val loss: 0.06709655374288559\n",
      "Epoch 2243: train loss: 0.039739739149808884, val loss: 0.06135156750679016\n",
      "Epoch 2244: train loss: 0.03341274335980415, val loss: 0.11289133876562119\n",
      "Epoch 2245: train loss: 0.029975568875670433, val loss: 0.08934062719345093\n",
      "Epoch 2246: train loss: 0.038646068423986435, val loss: 0.0710538849234581\n",
      "Epoch 2247: train loss: 0.03538627550005913, val loss: 0.061866071075201035\n",
      "Epoch 2248: train loss: 0.04937472939491272, val loss: 0.07696006447076797\n",
      "Epoch 2249: train loss: 0.037106022238731384, val loss: 0.1008106991648674\n",
      "Epoch 2250: train loss: 0.036482278257608414, val loss: 0.08433400094509125\n",
      "Epoch 2251: train loss: 0.044257860630750656, val loss: 0.10404589027166367\n",
      "Epoch 2252: train loss: 0.03563745319843292, val loss: 0.09198659658432007\n",
      "Epoch 2253: train loss: 0.03498319163918495, val loss: 0.08410006016492844\n",
      "Epoch 2254: train loss: 0.03354775905609131, val loss: 0.08577539771795273\n",
      "Epoch 2255: train loss: 0.03830909729003906, val loss: 0.05312714725732803\n",
      "Epoch 2256: train loss: 0.03357696533203125, val loss: 0.11361527442932129\n",
      "Epoch 2257: train loss: 0.03246792405843735, val loss: 0.10223487764596939\n",
      "Epoch 2258: train loss: 0.039565153419971466, val loss: 0.08047378808259964\n",
      "Epoch 2259: train loss: 0.037306223064661026, val loss: 0.07963991165161133\n",
      "Epoch 2260: train loss: 0.038633450865745544, val loss: 0.10782436281442642\n",
      "Epoch 2261: train loss: 0.03341176360845566, val loss: 0.12102089077234268\n",
      "Epoch 2262: train loss: 0.03990871459245682, val loss: 0.10700984299182892\n",
      "Epoch 2263: train loss: 0.037334173917770386, val loss: 0.09831976890563965\n",
      "Epoch 2264: train loss: 0.03410257771611214, val loss: 0.08797960728406906\n",
      "Epoch 2265: train loss: 0.03349704667925835, val loss: 0.10278695076704025\n",
      "Epoch 2266: train loss: 0.039858024567365646, val loss: 0.11257729679346085\n",
      "Epoch 2267: train loss: 0.03977985307574272, val loss: 0.059775639325380325\n",
      "Epoch 2268: train loss: 0.032672762870788574, val loss: 0.08768420666456223\n",
      "Epoch 2269: train loss: 0.04489466920495033, val loss: 0.1278250515460968\n",
      "Epoch 2270: train loss: 0.0380675233900547, val loss: 0.08010131120681763\n",
      "Epoch 2271: train loss: 0.03714299201965332, val loss: 0.07268104702234268\n",
      "Epoch 2272: train loss: 0.045462608337402344, val loss: 0.07344568520784378\n",
      "Epoch 2273: train loss: 0.03917030990123749, val loss: 0.12115547806024551\n",
      "Epoch 2274: train loss: 0.027565697208046913, val loss: 0.08812879770994186\n",
      "Epoch 2275: train loss: 0.03367359936237335, val loss: 0.1001729965209961\n",
      "Epoch 2276: train loss: 0.046576183289289474, val loss: 0.0785861685872078\n",
      "Epoch 2277: train loss: 0.03580451011657715, val loss: 0.081016905605793\n",
      "Epoch 2278: train loss: 0.03956536203622818, val loss: 0.15238122642040253\n",
      "Epoch 2279: train loss: 0.037997130304574966, val loss: 0.10708340257406235\n",
      "Epoch 2280: train loss: 0.029458491131663322, val loss: 0.10079383850097656\n",
      "Epoch 2281: train loss: 0.04444126784801483, val loss: 0.04982517287135124\n",
      "Epoch 2282: train loss: 0.03614930063486099, val loss: 0.07254057377576828\n",
      "Epoch 2283: train loss: 0.035795100033283234, val loss: 0.10313576459884644\n",
      "Epoch 2284: train loss: 0.03528059646487236, val loss: 0.07319246977567673\n",
      "Epoch 2285: train loss: 0.039707306772470474, val loss: 0.11089477688074112\n",
      "Epoch 2286: train loss: 0.04884178563952446, val loss: 0.11865568161010742\n",
      "Epoch 2287: train loss: 0.037221819162368774, val loss: 0.07194577157497406\n",
      "Epoch 2288: train loss: 0.03450452908873558, val loss: 0.08051025122404099\n",
      "Epoch 2289: train loss: 0.043387431651353836, val loss: 0.05760059505701065\n",
      "Epoch 2290: train loss: 0.03790917992591858, val loss: 0.10440432280302048\n",
      "Epoch 2291: train loss: 0.03945232555270195, val loss: 0.11166578531265259\n",
      "Epoch 2292: train loss: 0.045212965458631516, val loss: 0.09938111901283264\n",
      "Epoch 2293: train loss: 0.03421406447887421, val loss: 0.05077120289206505\n",
      "Epoch 2294: train loss: 0.043364983052015305, val loss: 0.0783592015504837\n",
      "Epoch 2295: train loss: 0.052147019654512405, val loss: 0.057728733867406845\n",
      "Epoch 2296: train loss: 0.036081552505493164, val loss: 0.09073445945978165\n",
      "Epoch 2297: train loss: 0.05072805657982826, val loss: 0.07494538277387619\n",
      "Epoch 2298: train loss: 0.04712174832820892, val loss: 0.06123482063412666\n",
      "Epoch 2299: train loss: 0.030610905960202217, val loss: 0.06817098706960678\n",
      "Epoch 2300: train loss: 0.02881116420030594, val loss: 0.11906739324331284\n",
      "Epoch 2301: train loss: 0.030767977237701416, val loss: 0.07412209361791611\n",
      "Epoch 2302: train loss: 0.038396961987018585, val loss: 0.09577663242816925\n",
      "Epoch 2303: train loss: 0.03602324798703194, val loss: 0.07835862785577774\n",
      "Epoch 2304: train loss: 0.030703650787472725, val loss: 0.126665398478508\n",
      "Epoch 2305: train loss: 0.02554185688495636, val loss: 0.10884201526641846\n",
      "Epoch 2306: train loss: 0.03437666967511177, val loss: 0.10095036029815674\n",
      "Epoch 2307: train loss: 0.04131917655467987, val loss: 0.09839751571416855\n",
      "Epoch 2308: train loss: 0.04948082193732262, val loss: 0.08205203711986542\n",
      "Epoch 2309: train loss: 0.03167921304702759, val loss: 0.11071290820837021\n",
      "Epoch 2310: train loss: 0.039367206394672394, val loss: 0.0803004801273346\n",
      "Epoch 2311: train loss: 0.03668462112545967, val loss: 0.06629134714603424\n",
      "Epoch 2312: train loss: 0.03877314180135727, val loss: 0.11440026015043259\n",
      "Epoch 2313: train loss: 0.04131871834397316, val loss: 0.06535013020038605\n",
      "Epoch 2314: train loss: 0.03148811310529709, val loss: 0.11495691537857056\n",
      "Epoch 2315: train loss: 0.033250290900468826, val loss: 0.1087622195482254\n",
      "Epoch 2316: train loss: 0.04084295779466629, val loss: 0.06957599520683289\n",
      "Epoch 2317: train loss: 0.03343164175748825, val loss: 0.08111222833395004\n",
      "Epoch 2318: train loss: 0.04881865158677101, val loss: 0.09859069436788559\n",
      "Epoch 2319: train loss: 0.041345492005348206, val loss: 0.10399609804153442\n",
      "Epoch 2320: train loss: 0.035955216735601425, val loss: 0.07668570429086685\n",
      "Epoch 2321: train loss: 0.04811079427599907, val loss: 0.09463238716125488\n",
      "Epoch 2322: train loss: 0.03572691231966019, val loss: 0.08926375955343246\n",
      "Epoch 2323: train loss: 0.0357772521674633, val loss: 0.09322316199541092\n",
      "Epoch 2324: train loss: 0.039760787039995193, val loss: 0.10142762959003448\n",
      "Epoch 2325: train loss: 0.03456399589776993, val loss: 0.05647599697113037\n",
      "Epoch 2326: train loss: 0.032232675701379776, val loss: 0.05765452980995178\n",
      "Epoch 2327: train loss: 0.038080692291259766, val loss: 0.09476415067911148\n",
      "Epoch 2328: train loss: 0.034037474542856216, val loss: 0.10442926734685898\n",
      "Epoch 2329: train loss: 0.026606902480125427, val loss: 0.046331506222486496\n",
      "Epoch 2330: train loss: 0.033761005848646164, val loss: 0.10380373150110245\n",
      "Epoch 2331: train loss: 0.03546959161758423, val loss: 0.0755922719836235\n",
      "Epoch 2332: train loss: 0.034656815230846405, val loss: 0.10449258238077164\n",
      "Epoch 2333: train loss: 0.03466796875, val loss: 0.10715029388666153\n",
      "Epoch 2334: train loss: 0.032385073602199554, val loss: 0.09061301499605179\n",
      "Epoch 2335: train loss: 0.03283331170678139, val loss: 0.07237078249454498\n",
      "Epoch 2336: train loss: 0.03538661450147629, val loss: 0.07301180809736252\n",
      "Epoch 2337: train loss: 0.03634686395525932, val loss: 0.08664552867412567\n",
      "Epoch 2338: train loss: 0.030832739546895027, val loss: 0.09062572568655014\n",
      "Epoch 2339: train loss: 0.03276589512825012, val loss: 0.08188724517822266\n",
      "Epoch 2340: train loss: 0.024709852412343025, val loss: 0.08736889064311981\n",
      "Epoch 2341: train loss: 0.03283877298235893, val loss: 0.11687945574522018\n",
      "Epoch 2342: train loss: 0.026402298361063004, val loss: 0.07398659735918045\n",
      "Epoch 2343: train loss: 0.03270183876156807, val loss: 0.123865507543087\n",
      "Epoch 2344: train loss: 0.0304899699985981, val loss: 0.08397725969552994\n",
      "Epoch 2345: train loss: 0.02954058162868023, val loss: 0.14903400838375092\n",
      "Epoch 2346: train loss: 0.043077319860458374, val loss: 0.05692638084292412\n",
      "Epoch 2347: train loss: 0.031194470822811127, val loss: 0.11163604259490967\n",
      "Epoch 2348: train loss: 0.034926142543554306, val loss: 0.04295385628938675\n",
      "Epoch 2349: train loss: 0.029866058379411697, val loss: 0.10248208045959473\n",
      "Epoch 2350: train loss: 0.026500064879655838, val loss: 0.154474139213562\n",
      "Epoch 2351: train loss: 0.03116365522146225, val loss: 0.13496823608875275\n",
      "Epoch 2352: train loss: 0.03109641745686531, val loss: 0.09899992495775223\n",
      "Epoch 2353: train loss: 0.046696171164512634, val loss: 0.0902966633439064\n",
      "Epoch 2354: train loss: 0.03924626484513283, val loss: 0.06619449704885483\n",
      "Epoch 2355: train loss: 0.03351588174700737, val loss: 0.09301679581403732\n",
      "Epoch 2356: train loss: 0.024954255670309067, val loss: 0.10547752678394318\n",
      "Epoch 2357: train loss: 0.03136845678091049, val loss: 0.09454502165317535\n",
      "Epoch 2358: train loss: 0.02931336872279644, val loss: 0.08854027837514877\n",
      "Epoch 2359: train loss: 0.0293355043977499, val loss: 0.05733649060130119\n",
      "Epoch 2360: train loss: 0.02889481745660305, val loss: 0.11695154011249542\n",
      "Epoch 2361: train loss: 0.028177116066217422, val loss: 0.06515312939882278\n",
      "Epoch 2362: train loss: 0.027322378009557724, val loss: 0.16025769710540771\n",
      "Epoch 2363: train loss: 0.04208819940686226, val loss: 0.07466506958007812\n",
      "Epoch 2364: train loss: 0.02230503410100937, val loss: 0.07612530142068863\n",
      "Epoch 2365: train loss: 0.043941106647253036, val loss: 0.0640600249171257\n",
      "Epoch 2366: train loss: 0.02659844420850277, val loss: 0.11654786020517349\n",
      "Epoch 2367: train loss: 0.03292698785662651, val loss: 0.07516901940107346\n",
      "Epoch 2368: train loss: 0.040197961032390594, val loss: 0.09025802463293076\n",
      "Epoch 2369: train loss: 0.027971487492322922, val loss: 0.09580915421247482\n",
      "Epoch 2370: train loss: 0.03605595603585243, val loss: 0.10324607044458389\n",
      "Epoch 2371: train loss: 0.03477848321199417, val loss: 0.09179190546274185\n",
      "Epoch 2372: train loss: 0.028391938656568527, val loss: 0.09585907310247421\n",
      "Epoch 2373: train loss: 0.03279602527618408, val loss: 0.09697244316339493\n",
      "Epoch 2374: train loss: 0.03633645549416542, val loss: 0.10181958973407745\n",
      "Epoch 2375: train loss: 0.02375730499625206, val loss: 0.10033807903528214\n",
      "Epoch 2376: train loss: 0.027207370847463608, val loss: 0.08798874169588089\n",
      "Epoch 2377: train loss: 0.02657710574567318, val loss: 0.09714063256978989\n",
      "Epoch 2378: train loss: 0.027232445776462555, val loss: 0.10013687610626221\n",
      "Epoch 2379: train loss: 0.027548404410481453, val loss: 0.09055972099304199\n",
      "Epoch 2380: train loss: 0.03084234520792961, val loss: 0.074850894510746\n",
      "Epoch 2381: train loss: 0.027815496549010277, val loss: 0.09704219549894333\n",
      "Epoch 2382: train loss: 0.027019547298550606, val loss: 0.11715774983167648\n",
      "Epoch 2383: train loss: 0.029428625479340553, val loss: 0.09263533353805542\n",
      "Epoch 2384: train loss: 0.02653791382908821, val loss: 0.07065784931182861\n",
      "Epoch 2385: train loss: 0.028647974133491516, val loss: 0.12400525063276291\n",
      "Epoch 2386: train loss: 0.038091786205768585, val loss: 0.055483248084783554\n",
      "Epoch 2387: train loss: 0.03264046832919121, val loss: 0.11383843421936035\n",
      "Epoch 2388: train loss: 0.029920123517513275, val loss: 0.08623465895652771\n",
      "Epoch 2389: train loss: 0.03585020825266838, val loss: 0.09600593149662018\n",
      "Epoch 2390: train loss: 0.04076076298952103, val loss: 0.07564174383878708\n",
      "Epoch 2391: train loss: 0.04363568499684334, val loss: 0.10755106061697006\n",
      "Epoch 2392: train loss: 0.04313916340470314, val loss: 0.0790669247508049\n",
      "Epoch 2393: train loss: 0.04351933300495148, val loss: 0.08201050758361816\n",
      "Epoch 2394: train loss: 0.034661728888750076, val loss: 0.064972423017025\n",
      "Epoch 2395: train loss: 0.03533710539340973, val loss: 0.08145307749509811\n",
      "Epoch 2396: train loss: 0.027572007849812508, val loss: 0.09326770156621933\n",
      "Epoch 2397: train loss: 0.029496977105736732, val loss: 0.09858067333698273\n",
      "Epoch 2398: train loss: 0.029642412438988686, val loss: 0.0745425745844841\n",
      "Epoch 2399: train loss: 0.030919790267944336, val loss: 0.07456368952989578\n",
      "Epoch 2400: train loss: 0.02912232093513012, val loss: 0.09272202849388123\n",
      "Epoch 2401: train loss: 0.034052036702632904, val loss: 0.14140963554382324\n",
      "Epoch 2402: train loss: 0.028755925595760345, val loss: 0.04812038689851761\n",
      "Epoch 2403: train loss: 0.031411685049533844, val loss: 0.11869335174560547\n",
      "Epoch 2404: train loss: 0.031463220715522766, val loss: 0.10540556907653809\n",
      "Epoch 2405: train loss: 0.029169917106628418, val loss: 0.08485543727874756\n",
      "Epoch 2406: train loss: 0.03060862235724926, val loss: 0.08807650208473206\n",
      "Epoch 2407: train loss: 0.0347551666200161, val loss: 0.08625625818967819\n",
      "Epoch 2408: train loss: 0.0312831737101078, val loss: 0.09264862537384033\n",
      "Epoch 2409: train loss: 0.03782696649432182, val loss: 0.08235680311918259\n",
      "Epoch 2410: train loss: 0.035540226846933365, val loss: 0.06407563388347626\n",
      "Epoch 2411: train loss: 0.028227953240275383, val loss: 0.07591310888528824\n",
      "Epoch 2412: train loss: 0.032548751682043076, val loss: 0.11657046526670456\n",
      "Epoch 2413: train loss: 0.035328399389982224, val loss: 0.1321059912443161\n",
      "Epoch 2414: train loss: 0.03511934354901314, val loss: 0.1376316398382187\n",
      "Epoch 2415: train loss: 0.03782431781291962, val loss: 0.07262523472309113\n",
      "Epoch 2416: train loss: 0.042415957897901535, val loss: 0.055847764015197754\n",
      "Epoch 2417: train loss: 0.044515419751405716, val loss: 0.047612905502319336\n",
      "Epoch 2418: train loss: 0.033011190593242645, val loss: 0.11925394833087921\n",
      "Epoch 2419: train loss: 0.039914488792419434, val loss: 0.06985773891210556\n",
      "Epoch 2420: train loss: 0.04751051962375641, val loss: 0.08482746034860611\n",
      "Epoch 2421: train loss: 0.03628552705049515, val loss: 0.08779522031545639\n",
      "Epoch 2422: train loss: 0.03599591553211212, val loss: 0.08371459692716599\n",
      "Epoch 2423: train loss: 0.036483924835920334, val loss: 0.07740428298711777\n",
      "Epoch 2424: train loss: 0.03862684592604637, val loss: 0.09892339259386063\n",
      "Epoch 2425: train loss: 0.030686432495713234, val loss: 0.05838630720973015\n",
      "Epoch 2426: train loss: 0.03733574599027634, val loss: 0.10342469066381454\n",
      "Epoch 2427: train loss: 0.030376361683011055, val loss: 0.05731093883514404\n",
      "Epoch 2428: train loss: 0.030261855572462082, val loss: 0.11053655296564102\n",
      "Epoch 2429: train loss: 0.027366789057850838, val loss: 0.1028296947479248\n",
      "Epoch 2430: train loss: 0.02935398556292057, val loss: 0.04115939512848854\n",
      "Epoch 2431: train loss: 0.03130742907524109, val loss: 0.08182930201292038\n",
      "Epoch 2432: train loss: 0.037374433130025864, val loss: 0.08617476373910904\n",
      "Epoch 2433: train loss: 0.030895095318555832, val loss: 0.09301867336034775\n",
      "Epoch 2434: train loss: 0.025170819833874702, val loss: 0.09030652791261673\n",
      "Epoch 2435: train loss: 0.02968778647482395, val loss: 0.12100551277399063\n",
      "Epoch 2436: train loss: 0.03156585246324539, val loss: 0.08800562471151352\n",
      "Epoch 2437: train loss: 0.030439047142863274, val loss: 0.08479543775320053\n",
      "Epoch 2438: train loss: 0.0337064228951931, val loss: 0.059130728244781494\n",
      "Epoch 2439: train loss: 0.02453288622200489, val loss: 0.06192551925778389\n",
      "Epoch 2440: train loss: 0.03165474534034729, val loss: 0.07253561913967133\n",
      "Epoch 2441: train loss: 0.03784855827689171, val loss: 0.05556577444076538\n",
      "Epoch 2442: train loss: 0.03288921341300011, val loss: 0.073168084025383\n",
      "Epoch 2443: train loss: 0.03154955431818962, val loss: 0.099361352622509\n",
      "Epoch 2444: train loss: 0.030618861317634583, val loss: 0.09441041946411133\n",
      "Epoch 2445: train loss: 0.02774072252213955, val loss: 0.04689933732151985\n",
      "Epoch 2446: train loss: 0.03399611636996269, val loss: 0.059566743671894073\n",
      "Epoch 2447: train loss: 0.03678525239229202, val loss: 0.07258369773626328\n",
      "Epoch 2448: train loss: 0.035093218088150024, val loss: 0.13185332715511322\n",
      "Epoch 2449: train loss: 0.031268082559108734, val loss: 0.07963557541370392\n",
      "Epoch 2450: train loss: 0.04227924346923828, val loss: 0.052623867988586426\n",
      "Epoch 2451: train loss: 0.03502423316240311, val loss: 0.1143142580986023\n",
      "Epoch 2452: train loss: 0.03337598592042923, val loss: 0.07524340599775314\n",
      "Epoch 2453: train loss: 0.028902335092425346, val loss: 0.0824950560927391\n",
      "Epoch 2454: train loss: 0.033382102847099304, val loss: 0.04831321910023689\n",
      "Epoch 2455: train loss: 0.033642858266830444, val loss: 0.08760219067335129\n",
      "Epoch 2456: train loss: 0.03401903435587883, val loss: 0.0919724628329277\n",
      "Epoch 2457: train loss: 0.03368398919701576, val loss: 0.11662738770246506\n",
      "Epoch 2458: train loss: 0.04022341966629028, val loss: 0.08995270729064941\n",
      "Epoch 2459: train loss: 0.039784397929906845, val loss: 0.13718365132808685\n",
      "Epoch 2460: train loss: 0.03961272910237312, val loss: 0.12297254800796509\n",
      "Epoch 2461: train loss: 0.034551721066236496, val loss: 0.11316485702991486\n",
      "Epoch 2462: train loss: 0.03831174597144127, val loss: 0.08252866566181183\n",
      "Epoch 2463: train loss: 0.03535491228103638, val loss: 0.06837362051010132\n",
      "Epoch 2464: train loss: 0.03011697717010975, val loss: 0.10313472896814346\n",
      "Epoch 2465: train loss: 0.03489570692181587, val loss: 0.11413254588842392\n",
      "Epoch 2466: train loss: 0.0363309346139431, val loss: 0.04769767448306084\n",
      "Epoch 2467: train loss: 0.03438694030046463, val loss: 0.09111952036619186\n",
      "Epoch 2468: train loss: 0.03904017433524132, val loss: 0.08445863425731659\n",
      "Epoch 2469: train loss: 0.031049445271492004, val loss: 0.06206923723220825\n",
      "Epoch 2470: train loss: 0.028242217376828194, val loss: 0.12467018514871597\n",
      "Epoch 2471: train loss: 0.035636045038700104, val loss: 0.09613612294197083\n",
      "Epoch 2472: train loss: 0.025937139987945557, val loss: 0.05668974667787552\n",
      "Epoch 2473: train loss: 0.035588160157203674, val loss: 0.054712921380996704\n",
      "Epoch 2474: train loss: 0.04194182902574539, val loss: 0.055955853313207626\n",
      "Epoch 2475: train loss: 0.023309161886572838, val loss: 0.05457195267081261\n",
      "Epoch 2476: train loss: 0.03702174127101898, val loss: 0.09919720888137817\n",
      "Epoch 2477: train loss: 0.03472040593624115, val loss: 0.10115089267492294\n",
      "Epoch 2478: train loss: 0.028337566182017326, val loss: 0.06759201735258102\n",
      "Epoch 2479: train loss: 0.032170429825782776, val loss: 0.047102127224206924\n",
      "Epoch 2480: train loss: 0.03161654248833656, val loss: 0.0719444677233696\n",
      "Epoch 2481: train loss: 0.02636033669114113, val loss: 0.06306333839893341\n",
      "Epoch 2482: train loss: 0.03310244902968407, val loss: 0.08050639927387238\n",
      "Epoch 2483: train loss: 0.028597503900527954, val loss: 0.08939085155725479\n",
      "Epoch 2484: train loss: 0.03119475394487381, val loss: 0.08621750771999359\n",
      "Epoch 2485: train loss: 0.03193683177232742, val loss: 0.05240669846534729\n",
      "Epoch 2486: train loss: 0.02693854458630085, val loss: 0.09402275085449219\n",
      "Epoch 2487: train loss: 0.034667473286390305, val loss: 0.06152994558215141\n",
      "Epoch 2488: train loss: 0.040526147931814194, val loss: 0.07685070484876633\n",
      "Epoch 2489: train loss: 0.03366200253367424, val loss: 0.06345272064208984\n",
      "Epoch 2490: train loss: 0.0403161384165287, val loss: 0.06508809328079224\n",
      "Epoch 2491: train loss: 0.035088978707790375, val loss: 0.06536100059747696\n",
      "Epoch 2492: train loss: 0.030399572104215622, val loss: 0.09047261625528336\n",
      "Epoch 2493: train loss: 0.021757865324616432, val loss: 0.09088940918445587\n",
      "Epoch 2494: train loss: 0.03223183751106262, val loss: 0.0943952277302742\n",
      "Epoch 2495: train loss: 0.03255602717399597, val loss: 0.08024419844150543\n",
      "Epoch 2496: train loss: 0.028609663248062134, val loss: 0.08275996148586273\n",
      "Epoch 2497: train loss: 0.031617023050785065, val loss: 0.09297970682382584\n",
      "Epoch 2498: train loss: 0.038470927625894547, val loss: 0.07241465896368027\n",
      "Epoch 2499: train loss: 0.027820279821753502, val loss: 0.0777469277381897\n",
      "Epoch 2500: train loss: 0.02475854568183422, val loss: 0.04783020541071892\n",
      "Epoch 2501: train loss: 0.03182126209139824, val loss: 0.052110087126493454\n",
      "Epoch 2502: train loss: 0.037813734263181686, val loss: 0.11814580112695694\n",
      "Epoch 2503: train loss: 0.03217889368534088, val loss: 0.07300617545843124\n",
      "Epoch 2504: train loss: 0.030600296333432198, val loss: 0.08575648069381714\n",
      "Epoch 2505: train loss: 0.03200668469071388, val loss: 0.09205038100481033\n",
      "Epoch 2506: train loss: 0.03673558682203293, val loss: 0.0987507626414299\n",
      "Epoch 2507: train loss: 0.03825199976563454, val loss: 0.09436320513486862\n",
      "Epoch 2508: train loss: 0.029507802799344063, val loss: 0.07688528299331665\n",
      "Epoch 2509: train loss: 0.025688165798783302, val loss: 0.13552914559841156\n",
      "Epoch 2510: train loss: 0.03317728266119957, val loss: 0.07831636816263199\n",
      "Epoch 2511: train loss: 0.03203333169221878, val loss: 0.11010197550058365\n",
      "Epoch 2512: train loss: 0.028173409402370453, val loss: 0.09324429929256439\n",
      "Epoch 2513: train loss: 0.030678391456604004, val loss: 0.034750279039144516\n",
      "Epoch 2514: train loss: 0.036887459456920624, val loss: 0.08727560937404633\n",
      "Epoch 2515: train loss: 0.03259531408548355, val loss: 0.06906194239854813\n",
      "Epoch 2516: train loss: 0.032020144164562225, val loss: 0.05073493346571922\n",
      "Epoch 2517: train loss: 0.0361720509827137, val loss: 0.07726605236530304\n",
      "Epoch 2518: train loss: 0.03560575842857361, val loss: 0.06274989992380142\n",
      "Epoch 2519: train loss: 0.041460566222667694, val loss: 0.06769346445798874\n",
      "Epoch 2520: train loss: 0.03592483699321747, val loss: 0.06919851154088974\n",
      "Epoch 2521: train loss: 0.03805304691195488, val loss: 0.05906998738646507\n",
      "Epoch 2522: train loss: 0.037414636462926865, val loss: 0.12872271239757538\n",
      "Epoch 2523: train loss: 0.037715405225753784, val loss: 0.0659361258149147\n",
      "Epoch 2524: train loss: 0.04141053929924965, val loss: 0.09940183907747269\n",
      "Epoch 2525: train loss: 0.03938090056180954, val loss: 0.08054719120264053\n",
      "Epoch 2526: train loss: 0.04100613668560982, val loss: 0.12815670669078827\n",
      "Epoch 2527: train loss: 0.032666828483343124, val loss: 0.0956214889883995\n",
      "Epoch 2528: train loss: 0.034306757152080536, val loss: 0.09192910045385361\n",
      "Epoch 2529: train loss: 0.03938370198011398, val loss: 0.07308720797300339\n",
      "Epoch 2530: train loss: 0.04191042482852936, val loss: 0.08920931816101074\n",
      "Epoch 2531: train loss: 0.04056893289089203, val loss: 0.0893315002322197\n",
      "Epoch 2532: train loss: 0.03738221153616905, val loss: 0.08392155170440674\n",
      "Epoch 2533: train loss: 0.037480439990758896, val loss: 0.12078273296356201\n",
      "Epoch 2534: train loss: 0.04071884974837303, val loss: 0.0766930803656578\n",
      "Epoch 2535: train loss: 0.03621690720319748, val loss: 0.09057798236608505\n",
      "Epoch 2536: train loss: 0.04479986056685448, val loss: 0.10174877941608429\n",
      "Epoch 2537: train loss: 0.042492736130952835, val loss: 0.10707875341176987\n",
      "Epoch 2538: train loss: 0.05237240344285965, val loss: 0.08655047416687012\n",
      "Epoch 2539: train loss: 0.03893852233886719, val loss: 0.08571451157331467\n",
      "Epoch 2540: train loss: 0.04475216567516327, val loss: 0.0839972048997879\n",
      "Epoch 2541: train loss: 0.03737213462591171, val loss: 0.10986713320016861\n",
      "Epoch 2542: train loss: 0.03797002509236336, val loss: 0.11303301155567169\n",
      "Epoch 2543: train loss: 0.03606083616614342, val loss: 0.0927451029419899\n",
      "Epoch 2544: train loss: 0.039398193359375, val loss: 0.06711122393608093\n",
      "Epoch 2545: train loss: 0.044069815427064896, val loss: 0.05023328587412834\n",
      "Epoch 2546: train loss: 0.04396871104836464, val loss: 0.04681425914168358\n",
      "Epoch 2547: train loss: 0.036488424986600876, val loss: 0.1022343635559082\n",
      "Epoch 2548: train loss: 0.03324538841843605, val loss: 0.07260096073150635\n",
      "Epoch 2549: train loss: 0.04040415212512016, val loss: 0.08209740370512009\n",
      "Epoch 2550: train loss: 0.03596211224794388, val loss: 0.07327961176633835\n",
      "Epoch 2551: train loss: 0.0430530309677124, val loss: 0.09456970542669296\n",
      "Epoch 2552: train loss: 0.041190724819898605, val loss: 0.05678568035364151\n",
      "Epoch 2553: train loss: 0.03527713567018509, val loss: 0.04471619054675102\n",
      "Epoch 2554: train loss: 0.032813411206007004, val loss: 0.08492455631494522\n",
      "Epoch 2555: train loss: 0.02487458474934101, val loss: 0.046886611729860306\n",
      "Epoch 2556: train loss: 0.028344430029392242, val loss: 0.12530656158924103\n",
      "Epoch 2557: train loss: 0.045573458075523376, val loss: 0.08901631087064743\n",
      "Epoch 2558: train loss: 0.030222855508327484, val loss: 0.09372158348560333\n",
      "Epoch 2559: train loss: 0.028590679168701172, val loss: 0.09300904721021652\n",
      "Epoch 2560: train loss: 0.03333273157477379, val loss: 0.08862155675888062\n",
      "Epoch 2561: train loss: 0.048616282641887665, val loss: 0.07607487589120865\n",
      "Epoch 2562: train loss: 0.036339376121759415, val loss: 0.07277829200029373\n",
      "Epoch 2563: train loss: 0.045898694545030594, val loss: 0.0329037569463253\n",
      "Epoch 2564: train loss: 0.03613395616412163, val loss: 0.05517486482858658\n",
      "Epoch 2565: train loss: 0.02972126007080078, val loss: 0.10016965866088867\n",
      "Epoch 2566: train loss: 0.03022575192153454, val loss: 0.07380300015211105\n",
      "Epoch 2567: train loss: 0.030756451189517975, val loss: 0.07754947245121002\n",
      "Epoch 2568: train loss: 0.03206230700016022, val loss: 0.0757751390337944\n",
      "Epoch 2569: train loss: 0.04854844510555267, val loss: 0.11061930656433105\n",
      "Epoch 2570: train loss: 0.03094545006752014, val loss: 0.09289112687110901\n",
      "Epoch 2571: train loss: 0.026083944365382195, val loss: 0.09341776371002197\n",
      "Epoch 2572: train loss: 0.027829207479953766, val loss: 0.09759792685508728\n",
      "Epoch 2573: train loss: 0.03786996379494667, val loss: 0.08389035612344742\n",
      "Epoch 2574: train loss: 0.03423409163951874, val loss: 0.09277722984552383\n",
      "Epoch 2575: train loss: 0.04024127125740051, val loss: 0.0781930610537529\n",
      "Epoch 2576: train loss: 0.026133881881833076, val loss: 0.08624168485403061\n",
      "Epoch 2577: train loss: 0.030816609039902687, val loss: 0.10876863449811935\n",
      "Epoch 2578: train loss: 0.03496202826499939, val loss: 0.04477546736598015\n",
      "Epoch 2579: train loss: 0.03164973482489586, val loss: 0.10582409054040909\n",
      "Epoch 2580: train loss: 0.02385620027780533, val loss: 0.08365390449762344\n",
      "Epoch 2581: train loss: 0.030300365760922432, val loss: 0.07858874648809433\n",
      "Epoch 2582: train loss: 0.0378882959485054, val loss: 0.0636461153626442\n",
      "Epoch 2583: train loss: 0.037811439484357834, val loss: 0.12477560341358185\n",
      "Epoch 2584: train loss: 0.02912452258169651, val loss: 0.10647400468587875\n",
      "Epoch 2585: train loss: 0.036958806216716766, val loss: 0.06176461651921272\n",
      "Epoch 2586: train loss: 0.03202608600258827, val loss: 0.08036502450704575\n",
      "Epoch 2587: train loss: 0.03150350973010063, val loss: 0.07507652044296265\n",
      "Epoch 2588: train loss: 0.03211122751235962, val loss: 0.10037966817617416\n",
      "Epoch 2589: train loss: 0.029476014897227287, val loss: 0.08021330088376999\n",
      "Epoch 2590: train loss: 0.03025352954864502, val loss: 0.07938583940267563\n",
      "Epoch 2591: train loss: 0.03454805165529251, val loss: 0.06048620492219925\n",
      "Epoch 2592: train loss: 0.03398946300148964, val loss: 0.08613603562116623\n",
      "Epoch 2593: train loss: 0.039303261786699295, val loss: 0.07859738916158676\n",
      "Epoch 2594: train loss: 0.029493598267436028, val loss: 0.08128584176301956\n",
      "Epoch 2595: train loss: 0.04073858633637428, val loss: 0.08362641930580139\n",
      "Epoch 2596: train loss: 0.028647931292653084, val loss: 0.08276994526386261\n",
      "Epoch 2597: train loss: 0.04146387428045273, val loss: 0.11049274355173111\n",
      "Epoch 2598: train loss: 0.034437332302331924, val loss: 0.06971092522144318\n",
      "Epoch 2599: train loss: 0.03178835287690163, val loss: 0.08226107805967331\n",
      "Epoch 2600: train loss: 0.0298461876809597, val loss: 0.14262932538986206\n",
      "Epoch 2601: train loss: 0.026472052559256554, val loss: 0.07934549450874329\n",
      "Epoch 2602: train loss: 0.030874677002429962, val loss: 0.06047050282359123\n",
      "Epoch 2603: train loss: 0.028804276138544083, val loss: 0.07021508365869522\n",
      "Epoch 2604: train loss: 0.03086843527853489, val loss: 0.051490169018507004\n",
      "Epoch 2605: train loss: 0.030970802530646324, val loss: 0.0578685887157917\n",
      "Epoch 2606: train loss: 0.02883146144449711, val loss: 0.08305191993713379\n",
      "Epoch 2607: train loss: 0.04091804847121239, val loss: 0.102913998067379\n",
      "Epoch 2608: train loss: 0.03270265460014343, val loss: 0.05982373282313347\n",
      "Epoch 2609: train loss: 0.03515641391277313, val loss: 0.12707507610321045\n",
      "Epoch 2610: train loss: 0.039576541632413864, val loss: 0.10023870319128036\n",
      "Epoch 2611: train loss: 0.029068538919091225, val loss: 0.0712781473994255\n",
      "Epoch 2612: train loss: 0.04142710939049721, val loss: 0.11499488353729248\n",
      "Epoch 2613: train loss: 0.026271440088748932, val loss: 0.0539206862449646\n",
      "Epoch 2614: train loss: 0.03180641680955887, val loss: 0.09933992475271225\n",
      "Epoch 2615: train loss: 0.03112706169486046, val loss: 0.06954331696033478\n",
      "Epoch 2616: train loss: 0.030208028852939606, val loss: 0.1016743928194046\n",
      "Epoch 2617: train loss: 0.029469946399331093, val loss: 0.08596207946538925\n",
      "Epoch 2618: train loss: 0.03803113475441933, val loss: 0.08593417704105377\n",
      "Epoch 2619: train loss: 0.03066493198275566, val loss: 0.0569414384663105\n",
      "Epoch 2620: train loss: 0.03624752536416054, val loss: 0.1101759821176529\n",
      "Epoch 2621: train loss: 0.045397642999887466, val loss: 0.09971307218074799\n",
      "Epoch 2622: train loss: 0.026993567124009132, val loss: 0.10803887993097305\n",
      "Epoch 2623: train loss: 0.02872263640165329, val loss: 0.1194298043847084\n",
      "Epoch 2624: train loss: 0.03429121524095535, val loss: 0.07666101306676865\n",
      "Epoch 2625: train loss: 0.03367391601204872, val loss: 0.09942054748535156\n",
      "Epoch 2626: train loss: 0.03123774565756321, val loss: 0.05917253717780113\n",
      "Epoch 2627: train loss: 0.03797546401619911, val loss: 0.09537829458713531\n",
      "Epoch 2628: train loss: 0.03811010345816612, val loss: 0.054266221821308136\n",
      "Epoch 2629: train loss: 0.0350116528570652, val loss: 0.05343211814761162\n",
      "Epoch 2630: train loss: 0.024597786366939545, val loss: 0.07094939798116684\n",
      "Epoch 2631: train loss: 0.025861388072371483, val loss: 0.05975716933608055\n",
      "Epoch 2632: train loss: 0.029811862856149673, val loss: 0.06960002332925797\n",
      "Epoch 2633: train loss: 0.031387876719236374, val loss: 0.06207188591361046\n",
      "Epoch 2634: train loss: 0.03153447061777115, val loss: 0.058897990733385086\n",
      "Epoch 2635: train loss: 0.033122725784778595, val loss: 0.10883580893278122\n",
      "Epoch 2636: train loss: 0.023660864681005478, val loss: 0.0435476079583168\n",
      "Epoch 2637: train loss: 0.029922958463430405, val loss: 0.08691950142383575\n",
      "Epoch 2638: train loss: 0.02976025454699993, val loss: 0.09587530046701431\n",
      "Epoch 2639: train loss: 0.028969813138246536, val loss: 0.041934311389923096\n",
      "Epoch 2640: train loss: 0.03850538656115532, val loss: 0.08744686841964722\n",
      "Epoch 2641: train loss: 0.02842434123158455, val loss: 0.06987789273262024\n",
      "Epoch 2642: train loss: 0.03413926064968109, val loss: 0.0957706943154335\n",
      "Epoch 2643: train loss: 0.029497019946575165, val loss: 0.12117408961057663\n",
      "Epoch 2644: train loss: 0.03013170138001442, val loss: 0.08297943323850632\n",
      "Epoch 2645: train loss: 0.039148714393377304, val loss: 0.09493990987539291\n",
      "Epoch 2646: train loss: 0.031281836330890656, val loss: 0.07630949467420578\n",
      "Epoch 2647: train loss: 0.043499186635017395, val loss: 0.04065032675862312\n",
      "Epoch 2648: train loss: 0.030505266040563583, val loss: 0.11671608686447144\n",
      "Epoch 2649: train loss: 0.02884609065949917, val loss: 0.07025006413459778\n",
      "Epoch 2650: train loss: 0.02781037800014019, val loss: 0.12423143535852432\n",
      "Epoch 2651: train loss: 0.03566168621182442, val loss: 0.07199125736951828\n",
      "Epoch 2652: train loss: 0.03229844197630882, val loss: 0.07315782457590103\n",
      "Epoch 2653: train loss: 0.040004562586545944, val loss: 0.09796233475208282\n",
      "Epoch 2654: train loss: 0.0390772819519043, val loss: 0.1003040298819542\n",
      "Epoch 2655: train loss: 0.046205658465623856, val loss: 0.0817715972661972\n",
      "Epoch 2656: train loss: 0.04576056823134422, val loss: 0.07421959936618805\n",
      "Epoch 2657: train loss: 0.039006754755973816, val loss: 0.10113765299320221\n",
      "Epoch 2658: train loss: 0.03702573478221893, val loss: 0.0888746827840805\n",
      "Epoch 2659: train loss: 0.04301389306783676, val loss: 0.11435208469629288\n",
      "Epoch 2660: train loss: 0.034765735268592834, val loss: 0.09756149351596832\n",
      "Epoch 2661: train loss: 0.031752850860357285, val loss: 0.07246240228414536\n",
      "Epoch 2662: train loss: 0.034358762204647064, val loss: 0.051226306706666946\n",
      "Epoch 2663: train loss: 0.031958162784576416, val loss: 0.07873894274234772\n",
      "Epoch 2664: train loss: 0.039562344551086426, val loss: 0.09805043041706085\n",
      "Epoch 2665: train loss: 0.03142612800002098, val loss: 0.06610093265771866\n",
      "Epoch 2666: train loss: 0.03018203191459179, val loss: 0.0748608261346817\n",
      "Epoch 2667: train loss: 0.033545125275850296, val loss: 0.07447593659162521\n",
      "Epoch 2668: train loss: 0.029173176735639572, val loss: 0.12499222904443741\n",
      "Epoch 2669: train loss: 0.03444047272205353, val loss: 0.09819727391004562\n",
      "Epoch 2670: train loss: 0.034950241446495056, val loss: 0.07776691019535065\n",
      "Epoch 2671: train loss: 0.026908503845334053, val loss: 0.057895727455616\n",
      "Epoch 2672: train loss: 0.04159535467624664, val loss: 0.06511596590280533\n",
      "Epoch 2673: train loss: 0.031086545437574387, val loss: 0.07155712693929672\n",
      "Epoch 2674: train loss: 0.024827517569065094, val loss: 0.07107650488615036\n",
      "Epoch 2675: train loss: 0.04335181787610054, val loss: 0.08481466770172119\n",
      "Epoch 2676: train loss: 0.04219026118516922, val loss: 0.08826329559087753\n",
      "Epoch 2677: train loss: 0.030457712709903717, val loss: 0.08902277052402496\n",
      "Epoch 2678: train loss: 0.03247151896357536, val loss: 0.12286348640918732\n",
      "Epoch 2679: train loss: 0.048170484602451324, val loss: 0.08224067836999893\n",
      "Epoch 2680: train loss: 0.03505405783653259, val loss: 0.06310700625181198\n",
      "Epoch 2681: train loss: 0.036061953753232956, val loss: 0.0929756686091423\n",
      "Epoch 2682: train loss: 0.028716325759887695, val loss: 0.13096129894256592\n",
      "Epoch 2683: train loss: 0.04697796702384949, val loss: 0.07681921869516373\n",
      "Epoch 2684: train loss: 0.03207880258560181, val loss: 0.10514888912439346\n",
      "Epoch 2685: train loss: 0.02788587659597397, val loss: 0.11611957848072052\n",
      "Epoch 2686: train loss: 0.029074842110276222, val loss: 0.07802774012088776\n",
      "Epoch 2687: train loss: 0.03617352992296219, val loss: 0.07518868893384933\n",
      "Epoch 2688: train loss: 0.028629912063479424, val loss: 0.0545661561191082\n",
      "Epoch 2689: train loss: 0.03226285055279732, val loss: 0.06335589289665222\n",
      "Epoch 2690: train loss: 0.030633285641670227, val loss: 0.08809282630681992\n",
      "Epoch 2691: train loss: 0.03160178288817406, val loss: 0.06228729709982872\n",
      "Epoch 2692: train loss: 0.03126543015241623, val loss: 0.08880198001861572\n",
      "Epoch 2693: train loss: 0.03696854040026665, val loss: 0.10758310556411743\n",
      "Epoch 2694: train loss: 0.03016015514731407, val loss: 0.06342308968305588\n",
      "Epoch 2695: train loss: 0.037244006991386414, val loss: 0.08620836585760117\n",
      "Epoch 2696: train loss: 0.04244920611381531, val loss: 0.09304720163345337\n",
      "Epoch 2697: train loss: 0.023433469235897064, val loss: 0.09238957613706589\n",
      "Epoch 2698: train loss: 0.03537416458129883, val loss: 0.09251981973648071\n",
      "Epoch 2699: train loss: 0.03518647700548172, val loss: 0.06860074400901794\n",
      "Epoch 2700: train loss: 0.029798444360494614, val loss: 0.07002415508031845\n",
      "Epoch 2701: train loss: 0.02815920114517212, val loss: 0.0713321790099144\n",
      "Epoch 2702: train loss: 0.03818356990814209, val loss: 0.10825777053833008\n",
      "Epoch 2703: train loss: 0.027197854593396187, val loss: 0.12723498046398163\n",
      "Epoch 2704: train loss: 0.035313889384269714, val loss: 0.08633935451507568\n",
      "Epoch 2705: train loss: 0.03190159425139427, val loss: 0.10444766283035278\n",
      "Epoch 2706: train loss: 0.034520506858825684, val loss: 0.08979741483926773\n",
      "Epoch 2707: train loss: 0.027293391525745392, val loss: 0.08436431735754013\n",
      "Epoch 2708: train loss: 0.03389304131269455, val loss: 0.0740782618522644\n",
      "Epoch 2709: train loss: 0.03357253596186638, val loss: 0.037543006241321564\n",
      "Epoch 2710: train loss: 0.03507118299603462, val loss: 0.0818023607134819\n",
      "Epoch 2711: train loss: 0.03126032277941704, val loss: 0.11369355022907257\n",
      "Epoch 2712: train loss: 0.03594774752855301, val loss: 0.08710034936666489\n",
      "Epoch 2713: train loss: 0.036457426846027374, val loss: 0.0956936925649643\n",
      "Epoch 2714: train loss: 0.029830440878868103, val loss: 0.10883364826440811\n",
      "Epoch 2715: train loss: 0.026486985385417938, val loss: 0.06239992007613182\n",
      "Epoch 2716: train loss: 0.031708039343357086, val loss: 0.06657106429338455\n",
      "Epoch 2717: train loss: 0.02101867087185383, val loss: 0.15234270691871643\n",
      "Epoch 2718: train loss: 0.028285527601838112, val loss: 0.07597392797470093\n",
      "Epoch 2719: train loss: 0.02952343225479126, val loss: 0.07725939899682999\n",
      "Epoch 2720: train loss: 0.02753172256052494, val loss: 0.14726971089839935\n",
      "Epoch 2721: train loss: 0.032784923911094666, val loss: 0.0870392695069313\n",
      "Epoch 2722: train loss: 0.030329251661896706, val loss: 0.0783347636461258\n",
      "Epoch 2723: train loss: 0.02947128191590309, val loss: 0.08428525924682617\n",
      "Epoch 2724: train loss: 0.02725248411297798, val loss: 0.08389216661453247\n",
      "Epoch 2725: train loss: 0.025492645800113678, val loss: 0.0792946070432663\n",
      "Epoch 2726: train loss: 0.025844566524028778, val loss: 0.12535549700260162\n",
      "Epoch 2727: train loss: 0.028278324753046036, val loss: 0.13965320587158203\n",
      "Epoch 2728: train loss: 0.03438761085271835, val loss: 0.07261457294225693\n",
      "Epoch 2729: train loss: 0.028829842805862427, val loss: 0.07479825615882874\n",
      "Epoch 2730: train loss: 0.03616052120923996, val loss: 0.14428162574768066\n",
      "Epoch 2731: train loss: 0.02700021117925644, val loss: 0.06937068700790405\n",
      "Epoch 2732: train loss: 0.028839757665991783, val loss: 0.13938096165657043\n",
      "Epoch 2733: train loss: 0.030119631439447403, val loss: 0.09705914556980133\n",
      "Epoch 2734: train loss: 0.03204626962542534, val loss: 0.05284978076815605\n",
      "Epoch 2735: train loss: 0.030906200408935547, val loss: 0.0835556909441948\n",
      "Epoch 2736: train loss: 0.03046618588268757, val loss: 0.09595604985952377\n",
      "Epoch 2737: train loss: 0.030100928619503975, val loss: 0.11909039318561554\n",
      "Epoch 2738: train loss: 0.035600949078798294, val loss: 0.10138795524835587\n",
      "Epoch 2739: train loss: 0.03790290281176567, val loss: 0.060851313173770905\n",
      "Epoch 2740: train loss: 0.02911185286939144, val loss: 0.07052066177129745\n",
      "Epoch 2741: train loss: 0.025610426440835, val loss: 0.08625812828540802\n",
      "Epoch 2742: train loss: 0.027624202892184258, val loss: 0.09789805859327316\n",
      "Epoch 2743: train loss: 0.0271915290504694, val loss: 0.10333243757486343\n",
      "Epoch 2744: train loss: 0.03873313218355179, val loss: 0.0834878534078598\n",
      "Epoch 2745: train loss: 0.034952398389577866, val loss: 0.0678592175245285\n",
      "Epoch 2746: train loss: 0.037101276218891144, val loss: 0.09340967983007431\n",
      "Epoch 2747: train loss: 0.03525230288505554, val loss: 0.10701584070920944\n",
      "Epoch 2748: train loss: 0.03856275603175163, val loss: 0.09708826243877411\n",
      "Epoch 2749: train loss: 0.027122583240270615, val loss: 0.135849267244339\n",
      "Epoch 2750: train loss: 0.029376473277807236, val loss: 0.0617138147354126\n",
      "Epoch 2751: train loss: 0.02817550301551819, val loss: 0.05097419023513794\n",
      "Epoch 2752: train loss: 0.03067341446876526, val loss: 0.09344840049743652\n",
      "Epoch 2753: train loss: 0.030463807284832, val loss: 0.09791996330022812\n",
      "Epoch 2754: train loss: 0.03307587280869484, val loss: 0.06804599612951279\n",
      "Epoch 2755: train loss: 0.029977666214108467, val loss: 0.07813488692045212\n",
      "Epoch 2756: train loss: 0.028996441513299942, val loss: 0.08168055862188339\n",
      "Epoch 2757: train loss: 0.0276882816106081, val loss: 0.06225866079330444\n",
      "Epoch 2758: train loss: 0.030038386583328247, val loss: 0.024381646886467934\n",
      "Epoch 2759: train loss: 0.026084471493959427, val loss: 0.07889243215322495\n",
      "Epoch 2760: train loss: 0.03916141390800476, val loss: 0.061463333666324615\n",
      "Epoch 2761: train loss: 0.030999187380075455, val loss: 0.07160796970129013\n",
      "Epoch 2762: train loss: 0.03927488252520561, val loss: 0.07200469076633453\n",
      "Epoch 2763: train loss: 0.036966271698474884, val loss: 0.06662287563085556\n",
      "Epoch 2764: train loss: 0.034340377897024155, val loss: 0.053055018186569214\n",
      "Epoch 2765: train loss: 0.02942628227174282, val loss: 0.07220278680324554\n",
      "Epoch 2766: train loss: 0.024591078981757164, val loss: 0.06623824685811996\n",
      "Epoch 2767: train loss: 0.03564687818288803, val loss: 0.07402273267507553\n",
      "Epoch 2768: train loss: 0.03788457065820694, val loss: 0.09668531268835068\n",
      "Epoch 2769: train loss: 0.02889854647219181, val loss: 0.0728846937417984\n",
      "Epoch 2770: train loss: 0.02113800123333931, val loss: 0.07927235215902328\n",
      "Epoch 2771: train loss: 0.03021250292658806, val loss: 0.10319095849990845\n",
      "Epoch 2772: train loss: 0.023635756224393845, val loss: 0.08061893284320831\n",
      "Epoch 2773: train loss: 0.03225759044289589, val loss: 0.08232957869768143\n",
      "Epoch 2774: train loss: 0.04773276671767235, val loss: 0.08558502048254013\n",
      "Epoch 2775: train loss: 0.02535567432641983, val loss: 0.10655144602060318\n",
      "Epoch 2776: train loss: 0.03943182900547981, val loss: 0.06358984857797623\n",
      "Epoch 2777: train loss: 0.03199372440576553, val loss: 0.11855202168226242\n",
      "Epoch 2778: train loss: 0.0325988344848156, val loss: 0.06656461209058762\n",
      "Epoch 2779: train loss: 0.0245766993612051, val loss: 0.08072181791067123\n",
      "Epoch 2780: train loss: 0.029490837827324867, val loss: 0.09165053069591522\n",
      "Epoch 2781: train loss: 0.027033325284719467, val loss: 0.06381855905056\n",
      "Epoch 2782: train loss: 0.03321358934044838, val loss: 0.08673220127820969\n",
      "Epoch 2783: train loss: 0.028027351945638657, val loss: 0.1118789091706276\n",
      "Epoch 2784: train loss: 0.022044440731406212, val loss: 0.09683701395988464\n",
      "Epoch 2785: train loss: 0.030349282547831535, val loss: 0.10903918743133545\n",
      "Epoch 2786: train loss: 0.02448754385113716, val loss: 0.06656980514526367\n",
      "Epoch 2787: train loss: 0.03248417750000954, val loss: 0.10107570141553879\n",
      "Epoch 2788: train loss: 0.029576662927865982, val loss: 0.05829169973731041\n",
      "Epoch 2789: train loss: 0.03870418667793274, val loss: 0.13524556159973145\n",
      "Epoch 2790: train loss: 0.029568888247013092, val loss: 0.07103151828050613\n",
      "Epoch 2791: train loss: 0.026031475514173508, val loss: 0.09052705764770508\n",
      "Epoch 2792: train loss: 0.03604776784777641, val loss: 0.11147302389144897\n",
      "Epoch 2793: train loss: 0.037058111280202866, val loss: 0.08786597102880478\n",
      "Epoch 2794: train loss: 0.035770248621702194, val loss: 0.05973522737622261\n",
      "Epoch 2795: train loss: 0.03371088206768036, val loss: 0.1281745433807373\n",
      "Epoch 2796: train loss: 0.029413452371954918, val loss: 0.10113044828176498\n",
      "Epoch 2797: train loss: 0.046470146626234055, val loss: 0.13471783697605133\n",
      "Epoch 2798: train loss: 0.03738177567720413, val loss: 0.06658494472503662\n",
      "Epoch 2799: train loss: 0.033929698169231415, val loss: 0.06316109001636505\n",
      "Epoch 2800: train loss: 0.026002461090683937, val loss: 0.11501938104629517\n",
      "Epoch 2801: train loss: 0.04145797714591026, val loss: 0.0707155093550682\n",
      "Epoch 2802: train loss: 0.03357754275202751, val loss: 0.09798602014780045\n",
      "Epoch 2803: train loss: 0.032619595527648926, val loss: 0.11925417184829712\n",
      "Epoch 2804: train loss: 0.035361938178539276, val loss: 0.07596679776906967\n",
      "Epoch 2805: train loss: 0.03282133489847183, val loss: 0.059868376702070236\n",
      "Epoch 2806: train loss: 0.02867951989173889, val loss: 0.0924646183848381\n",
      "Epoch 2807: train loss: 0.03629498556256294, val loss: 0.0761580765247345\n",
      "Epoch 2808: train loss: 0.03675997629761696, val loss: 0.10482557117938995\n",
      "Epoch 2809: train loss: 0.02998417802155018, val loss: 0.13486994802951813\n",
      "Epoch 2810: train loss: 0.029052650555968285, val loss: 0.06106537580490112\n",
      "Epoch 2811: train loss: 0.024586031213402748, val loss: 0.09021963179111481\n",
      "Epoch 2812: train loss: 0.028258705511689186, val loss: 0.10867436230182648\n",
      "Epoch 2813: train loss: 0.027690310031175613, val loss: 0.0777248963713646\n",
      "Epoch 2814: train loss: 0.03326793387532234, val loss: 0.09108009189367294\n",
      "Epoch 2815: train loss: 0.03890211507678032, val loss: 0.08298133313655853\n",
      "Epoch 2816: train loss: 0.031587082892656326, val loss: 0.0943986102938652\n",
      "Epoch 2817: train loss: 0.03823162242770195, val loss: 0.08047953993082047\n",
      "Epoch 2818: train loss: 0.03229410573840141, val loss: 0.09084465354681015\n",
      "Epoch 2819: train loss: 0.02930874191224575, val loss: 0.09634781628847122\n",
      "Epoch 2820: train loss: 0.03209025040268898, val loss: 0.0764346569776535\n",
      "Epoch 2821: train loss: 0.03104155696928501, val loss: 0.07387653738260269\n",
      "Epoch 2822: train loss: 0.027489347383379936, val loss: 0.08796878904104233\n",
      "Epoch 2823: train loss: 0.022285718470811844, val loss: 0.040310971438884735\n",
      "Epoch 2824: train loss: 0.024441543966531754, val loss: 0.0404491126537323\n",
      "Epoch 2825: train loss: 0.024641668424010277, val loss: 0.10066001862287521\n",
      "Epoch 2826: train loss: 0.036448098719120026, val loss: 0.06421913206577301\n",
      "Epoch 2827: train loss: 0.03383523225784302, val loss: 0.08175221085548401\n",
      "Epoch 2828: train loss: 0.02848554588854313, val loss: 0.09657402336597443\n",
      "Epoch 2829: train loss: 0.02518603391945362, val loss: 0.12601225078105927\n",
      "Epoch 2830: train loss: 0.029388226568698883, val loss: 0.06779363006353378\n",
      "Epoch 2831: train loss: 0.027421792969107628, val loss: 0.11924105882644653\n",
      "Epoch 2832: train loss: 0.02395373024046421, val loss: 0.09241070598363876\n",
      "Epoch 2833: train loss: 0.0268750861287117, val loss: 0.09434141963720322\n",
      "Epoch 2834: train loss: 0.02857256308197975, val loss: 0.07897619158029556\n",
      "Epoch 2835: train loss: 0.02211434580385685, val loss: 0.06783849000930786\n",
      "Epoch 2836: train loss: 0.028922727331519127, val loss: 0.10499759018421173\n",
      "Epoch 2837: train loss: 0.026795437559485435, val loss: 0.07287291437387466\n",
      "Epoch 2838: train loss: 0.03011380322277546, val loss: 0.09392979741096497\n",
      "Epoch 2839: train loss: 0.03813319653272629, val loss: 0.07972466200590134\n",
      "Epoch 2840: train loss: 0.02873236872255802, val loss: 0.10891380161046982\n",
      "Epoch 2841: train loss: 0.02436487190425396, val loss: 0.07807417958974838\n",
      "Epoch 2842: train loss: 0.022586844861507416, val loss: 0.10449869930744171\n",
      "Epoch 2843: train loss: 0.04524800553917885, val loss: 0.13974876701831818\n",
      "Epoch 2844: train loss: 0.03186879679560661, val loss: 0.07904734462499619\n",
      "Epoch 2845: train loss: 0.02406308799982071, val loss: 0.06557509303092957\n",
      "Epoch 2846: train loss: 0.03633583337068558, val loss: 0.04775772988796234\n",
      "Epoch 2847: train loss: 0.033182911574840546, val loss: 0.043984923511743546\n",
      "Epoch 2848: train loss: 0.023101838305592537, val loss: 0.0769338384270668\n",
      "Epoch 2849: train loss: 0.03907735273241997, val loss: 0.07055055350065231\n",
      "Epoch 2850: train loss: 0.025234965607523918, val loss: 0.0839768648147583\n",
      "Epoch 2851: train loss: 0.028796939179301262, val loss: 0.1177196055650711\n",
      "Epoch 2852: train loss: 0.03262920305132866, val loss: 0.09381774812936783\n",
      "Epoch 2853: train loss: 0.034617822617292404, val loss: 0.13935668766498566\n",
      "Epoch 2854: train loss: 0.030625732615590096, val loss: 0.09298186004161835\n",
      "Epoch 2855: train loss: 0.02514558471739292, val loss: 0.15059931576251984\n",
      "Epoch 2856: train loss: 0.01822294294834137, val loss: 0.06614390015602112\n",
      "Epoch 2857: train loss: 0.030407020822167397, val loss: 0.10444976389408112\n",
      "Epoch 2858: train loss: 0.030923357233405113, val loss: 0.07368621230125427\n",
      "Epoch 2859: train loss: 0.028887521475553513, val loss: 0.10146355628967285\n",
      "Epoch 2860: train loss: 0.024301093071699142, val loss: 0.05645887181162834\n",
      "Epoch 2861: train loss: 0.02914821356534958, val loss: 0.08790769428014755\n",
      "Epoch 2862: train loss: 0.030285455286502838, val loss: 0.0762835144996643\n",
      "Epoch 2863: train loss: 0.030292892828583717, val loss: 0.10126568377017975\n",
      "Epoch 2864: train loss: 0.03248879313468933, val loss: 0.07337380945682526\n",
      "Epoch 2865: train loss: 0.036677129566669464, val loss: 0.09799300879240036\n",
      "Epoch 2866: train loss: 0.026693645864725113, val loss: 0.10313763469457626\n",
      "Epoch 2867: train loss: 0.029129482805728912, val loss: 0.0622897632420063\n",
      "Epoch 2868: train loss: 0.029904788359999657, val loss: 0.05068184807896614\n",
      "Epoch 2869: train loss: 0.029306719079613686, val loss: 0.09348388761281967\n",
      "Epoch 2870: train loss: 0.032254088670015335, val loss: 0.07944514602422714\n",
      "Epoch 2871: train loss: 0.028595125302672386, val loss: 0.11964422464370728\n",
      "Epoch 2872: train loss: 0.03557252883911133, val loss: 0.07698246091604233\n",
      "Epoch 2873: train loss: 0.03099212236702442, val loss: 0.06746984273195267\n",
      "Epoch 2874: train loss: 0.024988260120153427, val loss: 0.09000753611326218\n",
      "Epoch 2875: train loss: 0.03336362913250923, val loss: 0.09353476017713547\n",
      "Epoch 2876: train loss: 0.03213375061750412, val loss: 0.06603077054023743\n",
      "Epoch 2877: train loss: 0.024702997878193855, val loss: 0.060174643993377686\n",
      "Epoch 2878: train loss: 0.033764783293008804, val loss: 0.09187139570713043\n",
      "Epoch 2879: train loss: 0.02781693823635578, val loss: 0.0701029822230339\n",
      "Epoch 2880: train loss: 0.035030610859394073, val loss: 0.059590380638837814\n",
      "Epoch 2881: train loss: 0.03346097469329834, val loss: 0.0744946226477623\n",
      "Epoch 2882: train loss: 0.03988055884838104, val loss: 0.13016493618488312\n",
      "Epoch 2883: train loss: 0.03167049586772919, val loss: 0.07864465564489365\n",
      "Epoch 2884: train loss: 0.026690011844038963, val loss: 0.10833876579999924\n",
      "Epoch 2885: train loss: 0.03500351682305336, val loss: 0.07145640254020691\n",
      "Epoch 2886: train loss: 0.02861449308693409, val loss: 0.07226254045963287\n",
      "Epoch 2887: train loss: 0.022723376750946045, val loss: 0.1356855034828186\n",
      "Epoch 2888: train loss: 0.029651934280991554, val loss: 0.08074235171079636\n",
      "Epoch 2889: train loss: 0.028020404279232025, val loss: 0.10436685383319855\n",
      "Epoch 2890: train loss: 0.032304879277944565, val loss: 0.09105075895786285\n",
      "Epoch 2891: train loss: 0.028292924165725708, val loss: 0.059870537370443344\n",
      "Epoch 2892: train loss: 0.02318163588643074, val loss: 0.037115924060344696\n",
      "Epoch 2893: train loss: 0.027317849919199944, val loss: 0.0797639861702919\n",
      "Epoch 2894: train loss: 0.03314616531133652, val loss: 0.10308928787708282\n",
      "Epoch 2895: train loss: 0.030206773430109024, val loss: 0.0639873668551445\n",
      "Epoch 2896: train loss: 0.026059383526444435, val loss: 0.1258517950773239\n",
      "Epoch 2897: train loss: 0.02594049647450447, val loss: 0.07700153440237045\n",
      "Epoch 2898: train loss: 0.031726595014333725, val loss: 0.1013593003153801\n",
      "Epoch 2899: train loss: 0.030165143311023712, val loss: 0.09720762819051743\n",
      "Epoch 2900: train loss: 0.026972563937306404, val loss: 0.06028338894248009\n",
      "Epoch 2901: train loss: 0.02406812645494938, val loss: 0.06849777698516846\n",
      "Epoch 2902: train loss: 0.027265729382634163, val loss: 0.08427774906158447\n",
      "Epoch 2903: train loss: 0.028033554553985596, val loss: 0.11941482126712799\n",
      "Epoch 2904: train loss: 0.030363788828253746, val loss: 0.07479687035083771\n",
      "Epoch 2905: train loss: 0.028716348111629486, val loss: 0.08345427364110947\n",
      "Epoch 2906: train loss: 0.029708120971918106, val loss: 0.0555104985833168\n",
      "Epoch 2907: train loss: 0.02162511646747589, val loss: 0.10056184977293015\n",
      "Epoch 2908: train loss: 0.034851159900426865, val loss: 0.06483426690101624\n",
      "Epoch 2909: train loss: 0.03284240514039993, val loss: 0.1622643917798996\n",
      "Epoch 2910: train loss: 0.03181353211402893, val loss: 0.11733629554510117\n",
      "Epoch 2911: train loss: 0.0319858193397522, val loss: 0.08941429853439331\n",
      "Epoch 2912: train loss: 0.024391988292336464, val loss: 0.10850023478269577\n",
      "Epoch 2913: train loss: 0.034006066620349884, val loss: 0.08782783150672913\n",
      "Epoch 2914: train loss: 0.03273172304034233, val loss: 0.08502203971147537\n",
      "Epoch 2915: train loss: 0.03671424835920334, val loss: 0.08985904604196548\n",
      "Epoch 2916: train loss: 0.02683825045824051, val loss: 0.10980337858200073\n",
      "Epoch 2917: train loss: 0.022533154115080833, val loss: 0.06703997403383255\n",
      "Epoch 2918: train loss: 0.030003579333424568, val loss: 0.08515222370624542\n",
      "Epoch 2919: train loss: 0.03704097121953964, val loss: 0.07482896000146866\n",
      "Epoch 2920: train loss: 0.02933942899107933, val loss: 0.06098230555653572\n",
      "Epoch 2921: train loss: 0.03814872354269028, val loss: 0.05495358631014824\n",
      "Epoch 2922: train loss: 0.03139759227633476, val loss: 0.07574160397052765\n",
      "Epoch 2923: train loss: 0.028277631849050522, val loss: 0.06798814982175827\n",
      "Epoch 2924: train loss: 0.025420427322387695, val loss: 0.06609310954809189\n",
      "Epoch 2925: train loss: 0.028672372922301292, val loss: 0.041923560202121735\n",
      "Epoch 2926: train loss: 0.03676736354827881, val loss: 0.1321050375699997\n",
      "Epoch 2927: train loss: 0.03863830119371414, val loss: 0.06094987317919731\n",
      "Epoch 2928: train loss: 0.026585670188069344, val loss: 0.05729333311319351\n",
      "Epoch 2929: train loss: 0.03335628658533096, val loss: 0.08243605494499207\n",
      "Epoch 2930: train loss: 0.030238714069128036, val loss: 0.0702638179063797\n",
      "Epoch 2931: train loss: 0.026085229590535164, val loss: 0.10119004547595978\n",
      "Epoch 2932: train loss: 0.024625558406114578, val loss: 0.08446351438760757\n",
      "Epoch 2933: train loss: 0.023404156789183617, val loss: 0.09750427305698395\n",
      "Epoch 2934: train loss: 0.03570644184947014, val loss: 0.11422187089920044\n",
      "Epoch 2935: train loss: 0.03079112432897091, val loss: 0.14537177979946136\n",
      "Epoch 2936: train loss: 0.03164803609251976, val loss: 0.06604845076799393\n",
      "Epoch 2937: train loss: 0.02409730851650238, val loss: 0.07906769216060638\n",
      "Epoch 2938: train loss: 0.03126106038689613, val loss: 0.07110045850276947\n",
      "Epoch 2939: train loss: 0.032320763915777206, val loss: 0.09971709549427032\n",
      "Epoch 2940: train loss: 0.02264723740518093, val loss: 0.05246264860033989\n",
      "Epoch 2941: train loss: 0.030288008973002434, val loss: 0.03480398654937744\n",
      "Epoch 2942: train loss: 0.025298887863755226, val loss: 0.13387997448444366\n",
      "Epoch 2943: train loss: 0.03333279490470886, val loss: 0.07833247631788254\n",
      "Epoch 2944: train loss: 0.03267236799001694, val loss: 0.1485554724931717\n",
      "Epoch 2945: train loss: 0.021960556507110596, val loss: 0.10259093344211578\n",
      "Epoch 2946: train loss: 0.02609679289162159, val loss: 0.06662360578775406\n",
      "Epoch 2947: train loss: 0.02733241580426693, val loss: 0.043712932616472244\n",
      "Epoch 2948: train loss: 0.02105763368308544, val loss: 0.06273474544286728\n",
      "Epoch 2949: train loss: 0.027893058955669403, val loss: 0.08515329658985138\n",
      "Epoch 2950: train loss: 0.0363742895424366, val loss: 0.08940543979406357\n",
      "Epoch 2951: train loss: 0.023838520050048828, val loss: 0.10094199329614639\n",
      "Epoch 2952: train loss: 0.024297503754496574, val loss: 0.12271726131439209\n",
      "Epoch 2953: train loss: 0.02931595966219902, val loss: 0.07710343599319458\n",
      "Epoch 2954: train loss: 0.026826610788702965, val loss: 0.13702552020549774\n",
      "Epoch 2955: train loss: 0.020711982622742653, val loss: 0.10486098378896713\n",
      "Epoch 2956: train loss: 0.02454644814133644, val loss: 0.05306185036897659\n",
      "Epoch 2957: train loss: 0.04103913530707359, val loss: 0.11002891510725021\n",
      "Epoch 2958: train loss: 0.027666643261909485, val loss: 0.10043187439441681\n",
      "Epoch 2959: train loss: 0.031965263187885284, val loss: 0.06402646750211716\n",
      "Epoch 2960: train loss: 0.026425309479236603, val loss: 0.09869300574064255\n",
      "Epoch 2961: train loss: 0.027855470776557922, val loss: 0.12541578710079193\n",
      "Epoch 2962: train loss: 0.031710557639598846, val loss: 0.09414302557706833\n",
      "Epoch 2963: train loss: 0.026606395840644836, val loss: 0.13526108860969543\n",
      "Epoch 2964: train loss: 0.029489560052752495, val loss: 0.05965852364897728\n",
      "Epoch 2965: train loss: 0.031000100076198578, val loss: 0.09882960468530655\n",
      "Epoch 2966: train loss: 0.030911674723029137, val loss: 0.11437996476888657\n",
      "Epoch 2967: train loss: 0.03363201394677162, val loss: 0.133795827627182\n",
      "Epoch 2968: train loss: 0.03602317348122597, val loss: 0.07708369940519333\n",
      "Epoch 2969: train loss: 0.03314729034900665, val loss: 0.0898120254278183\n",
      "Epoch 2970: train loss: 0.0262621883302927, val loss: 0.051223039627075195\n",
      "Epoch 2971: train loss: 0.02259702794253826, val loss: 0.11406346410512924\n",
      "Epoch 2972: train loss: 0.028095752000808716, val loss: 0.10598273575305939\n",
      "Epoch 2973: train loss: 0.02547309175133705, val loss: 0.09078744053840637\n",
      "Epoch 2974: train loss: 0.020897243171930313, val loss: 0.06737949699163437\n",
      "Epoch 2975: train loss: 0.03155704215168953, val loss: 0.10830643028020859\n",
      "Epoch 2976: train loss: 0.03433357551693916, val loss: 0.11268105357885361\n",
      "Epoch 2977: train loss: 0.02671368420124054, val loss: 0.11346302181482315\n",
      "Epoch 2978: train loss: 0.030545691028237343, val loss: 0.05690762400627136\n",
      "Epoch 2979: train loss: 0.02280919812619686, val loss: 0.09546171128749847\n",
      "Epoch 2980: train loss: 0.028553402051329613, val loss: 0.060544610023498535\n",
      "Epoch 2981: train loss: 0.02992936782538891, val loss: 0.09706823527812958\n",
      "Epoch 2982: train loss: 0.032373297959566116, val loss: 0.08248195052146912\n",
      "Epoch 2983: train loss: 0.02509204111993313, val loss: 0.07975375652313232\n",
      "Epoch 2984: train loss: 0.0288846455514431, val loss: 0.08021636307239532\n",
      "Epoch 2985: train loss: 0.036453623324632645, val loss: 0.07258899509906769\n",
      "Epoch 2986: train loss: 0.02417038381099701, val loss: 0.11646869033575058\n",
      "Epoch 2987: train loss: 0.02990630269050598, val loss: 0.13440871238708496\n",
      "Epoch 2988: train loss: 0.023515067994594574, val loss: 0.09625327587127686\n",
      "Epoch 2989: train loss: 0.03751790523529053, val loss: 0.09790313243865967\n",
      "Epoch 2990: train loss: 0.04003139212727547, val loss: 0.056703921407461166\n",
      "Epoch 2991: train loss: 0.02292712591588497, val loss: 0.06134435161948204\n",
      "Epoch 2992: train loss: 0.03173759952187538, val loss: 0.09451812505722046\n",
      "Epoch 2993: train loss: 0.03517112135887146, val loss: 0.07238411158323288\n",
      "Epoch 2994: train loss: 0.030950075015425682, val loss: 0.04800591990351677\n",
      "Epoch 2995: train loss: 0.022667011246085167, val loss: 0.04539017751812935\n",
      "Epoch 2996: train loss: 0.03234999254345894, val loss: 0.07912962883710861\n",
      "Epoch 2997: train loss: 0.03026832640171051, val loss: 0.05174741894006729\n",
      "Epoch 2998: train loss: 0.03052651695907116, val loss: 0.0648164376616478\n",
      "Epoch 2999: train loss: 0.03684331104159355, val loss: 0.0803530365228653\n",
      "Epoch 3000: train loss: 0.029579557478427887, val loss: 0.11562458425760269\n",
      "Epoch 3001: train loss: 0.02286350540816784, val loss: 0.04536570608615875\n",
      "Epoch 3002: train loss: 0.02343573607504368, val loss: 0.07395883649587631\n",
      "Epoch 3003: train loss: 0.027577441185712814, val loss: 0.10021628439426422\n",
      "Epoch 3004: train loss: 0.0245247483253479, val loss: 0.08241888135671616\n",
      "Epoch 3005: train loss: 0.024468207731842995, val loss: 0.044053006917238235\n",
      "Epoch 3006: train loss: 0.030758462846279144, val loss: 0.05880310758948326\n",
      "Epoch 3007: train loss: 0.03039807826280594, val loss: 0.09003394842147827\n",
      "Epoch 3008: train loss: 0.02145116776227951, val loss: 0.09931185096502304\n",
      "Epoch 3009: train loss: 0.027478154748678207, val loss: 0.07381115853786469\n",
      "Epoch 3010: train loss: 0.028148455545306206, val loss: 0.13454532623291016\n",
      "Epoch 3011: train loss: 0.032195206731557846, val loss: 0.08211579918861389\n",
      "Epoch 3012: train loss: 0.024221856147050858, val loss: 0.08779484033584595\n",
      "Epoch 3013: train loss: 0.03446488082408905, val loss: 0.08527767658233643\n",
      "Epoch 3014: train loss: 0.026843275874853134, val loss: 0.06016182526946068\n",
      "Epoch 3015: train loss: 0.02721213921904564, val loss: 0.09248953312635422\n",
      "Epoch 3016: train loss: 0.027368392795324326, val loss: 0.03351644054055214\n",
      "Epoch 3017: train loss: 0.03133860602974892, val loss: 0.06143781170248985\n",
      "Epoch 3018: train loss: 0.027005275711417198, val loss: 0.094550721347332\n",
      "Epoch 3019: train loss: 0.024391530081629753, val loss: 0.07348234206438065\n",
      "Epoch 3020: train loss: 0.024441521614789963, val loss: 0.0851675271987915\n",
      "Epoch 3021: train loss: 0.026594417169690132, val loss: 0.07198208570480347\n",
      "Epoch 3022: train loss: 0.028036072850227356, val loss: 0.09701384603977203\n",
      "Epoch 3023: train loss: 0.02366441860795021, val loss: 0.06348729133605957\n",
      "Epoch 3024: train loss: 0.036839161068201065, val loss: 0.08337608724832535\n",
      "Epoch 3025: train loss: 0.03316788747906685, val loss: 0.07094281166791916\n",
      "Epoch 3026: train loss: 0.026160622015595436, val loss: 0.118960440158844\n",
      "Epoch 3027: train loss: 0.026252681389451027, val loss: 0.06616868078708649\n",
      "Epoch 3028: train loss: 0.03639229014515877, val loss: 0.10723786801099777\n",
      "Epoch 3029: train loss: 0.02957296371459961, val loss: 0.10203925520181656\n",
      "Epoch 3030: train loss: 0.030715635046362877, val loss: 0.07904708385467529\n",
      "Epoch 3031: train loss: 0.02607000432908535, val loss: 0.08985406160354614\n",
      "Epoch 3032: train loss: 0.02933814376592636, val loss: 0.12476625293493271\n",
      "Epoch 3033: train loss: 0.02926548384130001, val loss: 0.0418858677148819\n",
      "Epoch 3034: train loss: 0.02719285897910595, val loss: 0.08890143781900406\n",
      "Epoch 3035: train loss: 0.0323716476559639, val loss: 0.09634383767843246\n",
      "Epoch 3036: train loss: 0.03283323720097542, val loss: 0.053424663841724396\n",
      "Epoch 3037: train loss: 0.028185341507196426, val loss: 0.07324455678462982\n",
      "Epoch 3038: train loss: 0.0255283173173666, val loss: 0.0639679953455925\n",
      "Epoch 3039: train loss: 0.02377276122570038, val loss: 0.10471143573522568\n",
      "Epoch 3040: train loss: 0.021356450393795967, val loss: 0.12337829917669296\n",
      "Epoch 3041: train loss: 0.030293652787804604, val loss: 0.08178535103797913\n",
      "Epoch 3042: train loss: 0.023911789059638977, val loss: 0.09538169950246811\n",
      "Epoch 3043: train loss: 0.025834502652287483, val loss: 0.06832059472799301\n",
      "Epoch 3044: train loss: 0.02628405950963497, val loss: 0.10868459194898605\n",
      "Epoch 3045: train loss: 0.02527458406984806, val loss: 0.07436507940292358\n",
      "Epoch 3046: train loss: 0.02484545297920704, val loss: 0.08579494804143906\n",
      "Epoch 3047: train loss: 0.02264498919248581, val loss: 0.061037611216306686\n",
      "Epoch 3048: train loss: 0.03135015815496445, val loss: 0.04758116230368614\n",
      "Epoch 3049: train loss: 0.02703133225440979, val loss: 0.07278227806091309\n",
      "Epoch 3050: train loss: 0.027918575331568718, val loss: 0.0933171957731247\n",
      "Epoch 3051: train loss: 0.02832423895597458, val loss: 0.08105570822954178\n",
      "Epoch 3052: train loss: 0.031106721609830856, val loss: 0.10128634423017502\n",
      "Epoch 3053: train loss: 0.030009625479578972, val loss: 0.0651935413479805\n",
      "Epoch 3054: train loss: 0.022012146189808846, val loss: 0.07825201749801636\n",
      "Epoch 3055: train loss: 0.02277335152029991, val loss: 0.10106463730335236\n",
      "Epoch 3056: train loss: 0.021141933277249336, val loss: 0.1078432947397232\n",
      "Epoch 3057: train loss: 0.024490850046277046, val loss: 0.11203570663928986\n",
      "Epoch 3058: train loss: 0.03296484425663948, val loss: 0.13828390836715698\n",
      "Epoch 3059: train loss: 0.025623952969908714, val loss: 0.06184697896242142\n",
      "Epoch 3060: train loss: 0.03199251368641853, val loss: 0.061891764402389526\n",
      "Epoch 3061: train loss: 0.02068975754082203, val loss: 0.0949801579117775\n",
      "Epoch 3062: train loss: 0.030019095167517662, val loss: 0.08056705445051193\n",
      "Epoch 3063: train loss: 0.033511675894260406, val loss: 0.04775330424308777\n",
      "Epoch 3064: train loss: 0.027598204091191292, val loss: 0.08573675900697708\n",
      "Epoch 3065: train loss: 0.029519405215978622, val loss: 0.05518907308578491\n",
      "Epoch 3066: train loss: 0.024168020114302635, val loss: 0.07486851513385773\n",
      "Epoch 3067: train loss: 0.03161783888936043, val loss: 0.08268700540065765\n",
      "Epoch 3068: train loss: 0.02863130159676075, val loss: 0.08277478069067001\n",
      "Epoch 3069: train loss: 0.030618971213698387, val loss: 0.06619659066200256\n",
      "Epoch 3070: train loss: 0.02761133387684822, val loss: 0.09187861531972885\n",
      "Epoch 3071: train loss: 0.02713565155863762, val loss: 0.09878265857696533\n",
      "Epoch 3072: train loss: 0.03280248865485191, val loss: 0.08044334501028061\n",
      "Epoch 3073: train loss: 0.03641464561223984, val loss: 0.1118069663643837\n",
      "Epoch 3074: train loss: 0.026712680235505104, val loss: 0.08912727236747742\n",
      "Epoch 3075: train loss: 0.030107077211141586, val loss: 0.09206973761320114\n",
      "Epoch 3076: train loss: 0.02850315347313881, val loss: 0.0895925983786583\n",
      "Epoch 3077: train loss: 0.032244715839624405, val loss: 0.04892033711075783\n",
      "Epoch 3078: train loss: 0.021064767614006996, val loss: 0.10468616336584091\n",
      "Epoch 3079: train loss: 0.042028121650218964, val loss: 0.07801305502653122\n",
      "Epoch 3080: train loss: 0.0288230050355196, val loss: 0.06615613400936127\n",
      "Epoch 3081: train loss: 0.029472192749381065, val loss: 0.09804530441761017\n",
      "Epoch 3082: train loss: 0.02093798667192459, val loss: 0.10664800554513931\n",
      "Epoch 3083: train loss: 0.029120467603206635, val loss: 0.08892985433340073\n",
      "Epoch 3084: train loss: 0.02833535522222519, val loss: 0.08626054972410202\n",
      "Epoch 3085: train loss: 0.027411457151174545, val loss: 0.11062103509902954\n",
      "Epoch 3086: train loss: 0.03949804604053497, val loss: 0.1026819497346878\n",
      "Epoch 3087: train loss: 0.02370510995388031, val loss: 0.0919833779335022\n",
      "Epoch 3088: train loss: 0.02525915578007698, val loss: 0.09600919485092163\n",
      "Epoch 3089: train loss: 0.03301994502544403, val loss: 0.12378527969121933\n",
      "Epoch 3090: train loss: 0.0334462970495224, val loss: 0.0792323499917984\n",
      "Epoch 3091: train loss: 0.026148246601223946, val loss: 0.0739472508430481\n",
      "Epoch 3092: train loss: 0.024381183087825775, val loss: 0.05895788595080376\n",
      "Epoch 3093: train loss: 0.030314592644572258, val loss: 0.10298054665327072\n",
      "Epoch 3094: train loss: 0.03945115581154823, val loss: 0.0671817883849144\n",
      "Epoch 3095: train loss: 0.025941917672753334, val loss: 0.05548201873898506\n",
      "Epoch 3096: train loss: 0.031470704823732376, val loss: 0.07738398760557175\n",
      "Epoch 3097: train loss: 0.030986087396740913, val loss: 0.08659229427576065\n",
      "Epoch 3098: train loss: 0.025441903620958328, val loss: 0.05046674236655235\n",
      "Epoch 3099: train loss: 0.031067945063114166, val loss: 0.05740618705749512\n",
      "Epoch 3100: train loss: 0.02347651869058609, val loss: 0.0566166453063488\n",
      "Epoch 3101: train loss: 0.023012977093458176, val loss: 0.0982414186000824\n",
      "Epoch 3102: train loss: 0.02594933658838272, val loss: 0.11261742562055588\n",
      "Epoch 3103: train loss: 0.03619346022605896, val loss: 0.05554648116230965\n",
      "Epoch 3104: train loss: 0.027771366760134697, val loss: 0.10790043324232101\n",
      "Epoch 3105: train loss: 0.03776416555047035, val loss: 0.09028040617704391\n",
      "Epoch 3106: train loss: 0.03482755646109581, val loss: 0.0943663939833641\n",
      "Epoch 3107: train loss: 0.03284483775496483, val loss: 0.11097411066293716\n",
      "Epoch 3108: train loss: 0.0362095907330513, val loss: 0.07021062821149826\n",
      "Epoch 3109: train loss: 0.026160381734371185, val loss: 0.13057225942611694\n",
      "Epoch 3110: train loss: 0.023367252200841904, val loss: 0.08317625522613525\n",
      "Epoch 3111: train loss: 0.03309641033411026, val loss: 0.10126333683729172\n",
      "Epoch 3112: train loss: 0.03042130544781685, val loss: 0.11024492233991623\n",
      "Epoch 3113: train loss: 0.029274877160787582, val loss: 0.05129954218864441\n",
      "Epoch 3114: train loss: 0.029719017446041107, val loss: 0.10352792590856552\n",
      "Epoch 3115: train loss: 0.02766568958759308, val loss: 0.11602641642093658\n",
      "Epoch 3116: train loss: 0.024582771584391594, val loss: 0.07136563211679459\n",
      "Epoch 3117: train loss: 0.022871199995279312, val loss: 0.08014024049043655\n",
      "Epoch 3118: train loss: 0.02222517319023609, val loss: 0.04341975599527359\n",
      "Epoch 3119: train loss: 0.0277925543487072, val loss: 0.1088578850030899\n",
      "Epoch 3120: train loss: 0.02958700992166996, val loss: 0.0974595844745636\n",
      "Epoch 3121: train loss: 0.03148432821035385, val loss: 0.10988395661115646\n",
      "Epoch 3122: train loss: 0.026120197027921677, val loss: 0.08820255100727081\n",
      "Epoch 3123: train loss: 0.027857817709445953, val loss: 0.10330909490585327\n",
      "Epoch 3124: train loss: 0.022524768486618996, val loss: 0.0771406888961792\n",
      "Epoch 3125: train loss: 0.023572249338030815, val loss: 0.10775714367628098\n",
      "Epoch 3126: train loss: 0.031460873782634735, val loss: 0.10133682936429977\n",
      "Epoch 3127: train loss: 0.02429565042257309, val loss: 0.061982523649930954\n",
      "Epoch 3128: train loss: 0.0278936680406332, val loss: 0.07803092896938324\n",
      "Epoch 3129: train loss: 0.02900661900639534, val loss: 0.07816623151302338\n",
      "Epoch 3130: train loss: 0.02952076680958271, val loss: 0.08822982758283615\n",
      "Epoch 3131: train loss: 0.03449907898902893, val loss: 0.10810661315917969\n",
      "Epoch 3132: train loss: 0.02571794204413891, val loss: 0.09047044813632965\n",
      "Epoch 3133: train loss: 0.027648495510220528, val loss: 0.09454421699047089\n",
      "Epoch 3134: train loss: 0.02912289835512638, val loss: 0.060072239488363266\n",
      "Epoch 3135: train loss: 0.030651070177555084, val loss: 0.08931528031826019\n",
      "Epoch 3136: train loss: 0.03119461052119732, val loss: 0.05816720798611641\n",
      "Epoch 3137: train loss: 0.023713361471891403, val loss: 0.11305370181798935\n",
      "Epoch 3138: train loss: 0.029985010623931885, val loss: 0.057360511273145676\n",
      "Epoch 3139: train loss: 0.03293042257428169, val loss: 0.09606839716434479\n",
      "Epoch 3140: train loss: 0.032017771154642105, val loss: 0.05224720761179924\n",
      "Epoch 3141: train loss: 0.03182055056095123, val loss: 0.06926069408655167\n",
      "Epoch 3142: train loss: 0.01732032559812069, val loss: 0.09243112057447433\n",
      "Epoch 3143: train loss: 0.030719825997948647, val loss: 0.09204792231321335\n",
      "Epoch 3144: train loss: 0.031865883618593216, val loss: 0.12828664481639862\n",
      "Epoch 3145: train loss: 0.025149207562208176, val loss: 0.12744544446468353\n",
      "Epoch 3146: train loss: 0.028334589675068855, val loss: 0.09123168140649796\n",
      "Epoch 3147: train loss: 0.032786931842565536, val loss: 0.06499212235212326\n",
      "Epoch 3148: train loss: 0.026403695344924927, val loss: 0.10727196931838989\n",
      "Epoch 3149: train loss: 0.030010640621185303, val loss: 0.13622280955314636\n",
      "Epoch 3150: train loss: 0.03211185336112976, val loss: 0.11083680391311646\n",
      "Epoch 3151: train loss: 0.038563698530197144, val loss: 0.10336863249540329\n",
      "Epoch 3152: train loss: 0.03133381903171539, val loss: 0.10328848659992218\n",
      "Epoch 3153: train loss: 0.029365040361881256, val loss: 0.07187824696302414\n",
      "Epoch 3154: train loss: 0.035091761499643326, val loss: 0.11570894718170166\n",
      "Epoch 3155: train loss: 0.035440318286418915, val loss: 0.10291223973035812\n",
      "Epoch 3156: train loss: 0.03265511244535446, val loss: 0.0815756544470787\n",
      "Epoch 3157: train loss: 0.023691125214099884, val loss: 0.07705142349004745\n",
      "Epoch 3158: train loss: 0.030096817761659622, val loss: 0.10236723721027374\n",
      "Epoch 3159: train loss: 0.029252678155899048, val loss: 0.09108632057905197\n",
      "Epoch 3160: train loss: 0.028771009296178818, val loss: 0.09863349050283432\n",
      "Epoch 3161: train loss: 0.024699682369828224, val loss: 0.07712372392416\n",
      "Epoch 3162: train loss: 0.04042397812008858, val loss: 0.16415338218212128\n",
      "Epoch 3163: train loss: 0.030705681070685387, val loss: 0.13883772492408752\n",
      "Epoch 3164: train loss: 0.030453655868768692, val loss: 0.05642537400126457\n",
      "Epoch 3165: train loss: 0.03214295953512192, val loss: 0.08430545777082443\n",
      "Epoch 3166: train loss: 0.031861644238233566, val loss: 0.10226942598819733\n",
      "Epoch 3167: train loss: 0.03690082207322121, val loss: 0.09197697043418884\n",
      "Epoch 3168: train loss: 0.020845605060458183, val loss: 0.06959725171327591\n",
      "Epoch 3169: train loss: 0.04293684661388397, val loss: 0.05834325775504112\n",
      "Epoch 3170: train loss: 0.029785556718707085, val loss: 0.08147294074296951\n",
      "Epoch 3171: train loss: 0.03610438480973244, val loss: 0.10052573680877686\n",
      "Epoch 3172: train loss: 0.025155993178486824, val loss: 0.09887158870697021\n",
      "Epoch 3173: train loss: 0.04341680184006691, val loss: 0.08108450472354889\n",
      "Epoch 3174: train loss: 0.03774155676364899, val loss: 0.07315468788146973\n",
      "Epoch 3175: train loss: 0.02508021704852581, val loss: 0.10004141181707382\n",
      "Epoch 3176: train loss: 0.02835218235850334, val loss: 0.10534524917602539\n",
      "Epoch 3177: train loss: 0.0325751006603241, val loss: 0.07830202579498291\n",
      "Epoch 3178: train loss: 0.03108609840273857, val loss: 0.06512264907360077\n",
      "Epoch 3179: train loss: 0.025569669902324677, val loss: 0.07856526225805283\n",
      "Epoch 3180: train loss: 0.03960268944501877, val loss: 0.08832240849733353\n",
      "Epoch 3181: train loss: 0.032746076583862305, val loss: 0.04965361952781677\n",
      "Epoch 3182: train loss: 0.038159728050231934, val loss: 0.05761469155550003\n",
      "Epoch 3183: train loss: 0.028316879644989967, val loss: 0.07371024787425995\n",
      "Epoch 3184: train loss: 0.03016025386750698, val loss: 0.07187497615814209\n",
      "Epoch 3185: train loss: 0.02565850131213665, val loss: 0.1158619150519371\n",
      "Epoch 3186: train loss: 0.037324581295251846, val loss: 0.07172798365354538\n",
      "Epoch 3187: train loss: 0.036383140832185745, val loss: 0.08712391555309296\n",
      "Epoch 3188: train loss: 0.026857813820242882, val loss: 0.030682766810059547\n",
      "Epoch 3189: train loss: 0.02573320083320141, val loss: 0.07682591676712036\n",
      "Epoch 3190: train loss: 0.028745751827955246, val loss: 0.12814536690711975\n",
      "Epoch 3191: train loss: 0.03522169589996338, val loss: 0.049694281071424484\n",
      "Epoch 3192: train loss: 0.029023729264736176, val loss: 0.0781749039888382\n",
      "Epoch 3193: train loss: 0.03385433554649353, val loss: 0.12498793751001358\n",
      "Epoch 3194: train loss: 0.029290534555912018, val loss: 0.13536177575588226\n",
      "Epoch 3195: train loss: 0.025106647983193398, val loss: 0.07961129397153854\n",
      "Epoch 3196: train loss: 0.0350516214966774, val loss: 0.1096089631319046\n",
      "Epoch 3197: train loss: 0.023032331839203835, val loss: 0.08097517490386963\n",
      "Epoch 3198: train loss: 0.028018491342663765, val loss: 0.060298461467027664\n",
      "Epoch 3199: train loss: 0.028842484578490257, val loss: 0.06206054612994194\n",
      "Epoch 3200: train loss: 0.02817201241850853, val loss: 0.08256366103887558\n",
      "Epoch 3201: train loss: 0.03370816633105278, val loss: 0.09321345388889313\n",
      "Epoch 3202: train loss: 0.031092828139662743, val loss: 0.12230651825666428\n",
      "Epoch 3203: train loss: 0.03600674122571945, val loss: 0.07021977752447128\n",
      "Epoch 3204: train loss: 0.03264617547392845, val loss: 0.09701307117938995\n",
      "Epoch 3205: train loss: 0.031784359365701675, val loss: 0.09093157202005386\n",
      "Epoch 3206: train loss: 0.025142937898635864, val loss: 0.11138444393873215\n",
      "Epoch 3207: train loss: 0.02617940492928028, val loss: 0.0802910253405571\n",
      "Epoch 3208: train loss: 0.027590440586209297, val loss: 0.07256289571523666\n",
      "Epoch 3209: train loss: 0.026656871661543846, val loss: 0.09286225587129593\n",
      "Epoch 3210: train loss: 0.038138195872306824, val loss: 0.07309745252132416\n",
      "Epoch 3211: train loss: 0.02468723990023136, val loss: 0.11925387382507324\n",
      "Epoch 3212: train loss: 0.036245960742235184, val loss: 0.0676189437508583\n",
      "Epoch 3213: train loss: 0.027701327577233315, val loss: 0.13544288277626038\n",
      "Epoch 3214: train loss: 0.03272121399641037, val loss: 0.062178611755371094\n",
      "Epoch 3215: train loss: 0.03388995677232742, val loss: 0.0853317379951477\n",
      "Epoch 3216: train loss: 0.024080915376544, val loss: 0.08557553589344025\n",
      "Epoch 3217: train loss: 0.02691754326224327, val loss: 0.0757891908288002\n",
      "Epoch 3218: train loss: 0.026153916493058205, val loss: 0.07732518762350082\n",
      "Epoch 3219: train loss: 0.029466427862644196, val loss: 0.06150027737021446\n",
      "Epoch 3220: train loss: 0.028231950476765633, val loss: 0.08024189621210098\n",
      "Epoch 3221: train loss: 0.03149227052927017, val loss: 0.08479896187782288\n",
      "Epoch 3222: train loss: 0.025103358551859856, val loss: 0.0793905183672905\n",
      "Epoch 3223: train loss: 0.025803789496421814, val loss: 0.04782965034246445\n",
      "Epoch 3224: train loss: 0.027353249490261078, val loss: 0.06416398286819458\n",
      "Epoch 3225: train loss: 0.029817640781402588, val loss: 0.06698142737150192\n",
      "Epoch 3226: train loss: 0.03650597110390663, val loss: 0.10052204132080078\n",
      "Epoch 3227: train loss: 0.022622020915150642, val loss: 0.09438734501600266\n",
      "Epoch 3228: train loss: 0.025663869455456734, val loss: 0.09242399781942368\n",
      "Epoch 3229: train loss: 0.03275257721543312, val loss: 0.07028205692768097\n",
      "Epoch 3230: train loss: 0.03177139163017273, val loss: 0.09010864794254303\n",
      "Epoch 3231: train loss: 0.026671821251511574, val loss: 0.12207558006048203\n",
      "Epoch 3232: train loss: 0.03831516578793526, val loss: 0.07417863607406616\n",
      "Epoch 3233: train loss: 0.03398071974515915, val loss: 0.09872628003358841\n",
      "Epoch 3234: train loss: 0.037298135459423065, val loss: 0.06750263273715973\n",
      "Epoch 3235: train loss: 0.03730707988142967, val loss: 0.10146141052246094\n",
      "Epoch 3236: train loss: 0.037297457456588745, val loss: 0.049988746643066406\n",
      "Epoch 3237: train loss: 0.023185350000858307, val loss: 0.07748841494321823\n",
      "Epoch 3238: train loss: 0.028120659291744232, val loss: 0.08714821189641953\n",
      "Epoch 3239: train loss: 0.031751666218042374, val loss: 0.07670953124761581\n",
      "Epoch 3240: train loss: 0.030586615204811096, val loss: 0.10360312461853027\n",
      "Epoch 3241: train loss: 0.04154643788933754, val loss: 0.09957858920097351\n",
      "Epoch 3242: train loss: 0.029414966702461243, val loss: 0.054833244532346725\n",
      "Epoch 3243: train loss: 0.03321685642004013, val loss: 0.08541148155927658\n",
      "Epoch 3244: train loss: 0.030702486634254456, val loss: 0.1065300926566124\n",
      "Epoch 3245: train loss: 0.03138427436351776, val loss: 0.07102277874946594\n",
      "Epoch 3246: train loss: 0.03246590495109558, val loss: 0.07231704145669937\n",
      "Epoch 3247: train loss: 0.03178689256310463, val loss: 0.07867089658975601\n",
      "Epoch 3248: train loss: 0.02757062390446663, val loss: 0.11280167102813721\n",
      "Epoch 3249: train loss: 0.03399674594402313, val loss: 0.07759027928113937\n",
      "Epoch 3250: train loss: 0.03407195955514908, val loss: 0.10461127012968063\n",
      "Epoch 3251: train loss: 0.02695326693356037, val loss: 0.08538562059402466\n",
      "Epoch 3252: train loss: 0.025689052417874336, val loss: 0.05737878754734993\n",
      "Epoch 3253: train loss: 0.026119941845536232, val loss: 0.07030393928289413\n",
      "Epoch 3254: train loss: 0.02962520532310009, val loss: 0.08378574997186661\n",
      "Epoch 3255: train loss: 0.03353491052985191, val loss: 0.0368049219250679\n",
      "Epoch 3256: train loss: 0.03381023555994034, val loss: 0.09162652492523193\n",
      "Epoch 3257: train loss: 0.028342854231595993, val loss: 0.11949725449085236\n",
      "Epoch 3258: train loss: 0.02797953598201275, val loss: 0.057279348373413086\n",
      "Epoch 3259: train loss: 0.026203813031315804, val loss: 0.1080617681145668\n",
      "Epoch 3260: train loss: 0.025750966742634773, val loss: 0.11428643763065338\n",
      "Epoch 3261: train loss: 0.028097370639443398, val loss: 0.13376514613628387\n",
      "Epoch 3262: train loss: 0.026909934356808662, val loss: 0.06894278526306152\n",
      "Epoch 3263: train loss: 0.036543089896440506, val loss: 0.08841542154550552\n",
      "Epoch 3264: train loss: 0.0362963005900383, val loss: 0.14885731041431427\n",
      "Epoch 3265: train loss: 0.02457239478826523, val loss: 0.07995346188545227\n",
      "Epoch 3266: train loss: 0.029502032324671745, val loss: 0.10263516008853912\n",
      "Epoch 3267: train loss: 0.02561122551560402, val loss: 0.1020587682723999\n",
      "Epoch 3268: train loss: 0.027074528858065605, val loss: 0.08076008409261703\n",
      "Epoch 3269: train loss: 0.02857757732272148, val loss: 0.10717324167490005\n",
      "Epoch 3270: train loss: 0.034843314439058304, val loss: 0.0534466989338398\n",
      "Epoch 3271: train loss: 0.027009449899196625, val loss: 0.07737099379301071\n",
      "Epoch 3272: train loss: 0.026819871738553047, val loss: 0.09529508650302887\n",
      "Epoch 3273: train loss: 0.0325736366212368, val loss: 0.08989798277616501\n",
      "Epoch 3274: train loss: 0.021831907331943512, val loss: 0.07406369596719742\n",
      "Epoch 3275: train loss: 0.03733367845416069, val loss: 0.09973108023405075\n",
      "Epoch 3276: train loss: 0.029898596927523613, val loss: 0.09060408920049667\n",
      "Epoch 3277: train loss: 0.03557811677455902, val loss: 0.07550054043531418\n",
      "Epoch 3278: train loss: 0.025843244045972824, val loss: 0.08189096301794052\n",
      "Epoch 3279: train loss: 0.03075985424220562, val loss: 0.042710937559604645\n",
      "Epoch 3280: train loss: 0.03211222216486931, val loss: 0.06617952883243561\n",
      "Epoch 3281: train loss: 0.03185191750526428, val loss: 0.10136514902114868\n",
      "Epoch 3282: train loss: 0.033832862973213196, val loss: 0.07238145917654037\n",
      "Epoch 3283: train loss: 0.03261452168226242, val loss: 0.0731835812330246\n",
      "Epoch 3284: train loss: 0.031982824206352234, val loss: 0.06205923482775688\n",
      "Epoch 3285: train loss: 0.016574136912822723, val loss: 0.06982600688934326\n",
      "Epoch 3286: train loss: 0.0283927284181118, val loss: 0.1183071956038475\n",
      "Epoch 3287: train loss: 0.030726078897714615, val loss: 0.0671980008482933\n",
      "Epoch 3288: train loss: 0.028971539810299873, val loss: 0.0532509870827198\n",
      "Epoch 3289: train loss: 0.03710586950182915, val loss: 0.11186635494232178\n",
      "Epoch 3290: train loss: 0.029876502230763435, val loss: 0.07488223165273666\n",
      "Epoch 3291: train loss: 0.029436154291033745, val loss: 0.06636624783277512\n",
      "Epoch 3292: train loss: 0.020625337958335876, val loss: 0.09679005295038223\n",
      "Epoch 3293: train loss: 0.029213903471827507, val loss: 0.0979054719209671\n",
      "Epoch 3294: train loss: 0.024434620514512062, val loss: 0.10732340812683105\n",
      "Epoch 3295: train loss: 0.025896403938531876, val loss: 0.09581362456083298\n",
      "Epoch 3296: train loss: 0.026827532798051834, val loss: 0.07295312732458115\n",
      "Epoch 3297: train loss: 0.02300688438117504, val loss: 0.05969133600592613\n",
      "Epoch 3298: train loss: 0.022689810022711754, val loss: 0.08882074803113937\n",
      "Epoch 3299: train loss: 0.029426686465740204, val loss: 0.09865600615739822\n",
      "Epoch 3300: train loss: 0.02791127748787403, val loss: 0.09939258545637131\n",
      "Epoch 3301: train loss: 0.03421863541007042, val loss: 0.08108889311552048\n",
      "Epoch 3302: train loss: 0.03454005718231201, val loss: 0.11966347694396973\n",
      "Epoch 3303: train loss: 0.03487943857908249, val loss: 0.10661256313323975\n",
      "Epoch 3304: train loss: 0.024245090782642365, val loss: 0.09247007220983505\n",
      "Epoch 3305: train loss: 0.019134728237986565, val loss: 0.1103290542960167\n",
      "Epoch 3306: train loss: 0.028301946818828583, val loss: 0.07537473738193512\n",
      "Epoch 3307: train loss: 0.03368118777871132, val loss: 0.11586391180753708\n",
      "Epoch 3308: train loss: 0.033031076192855835, val loss: 0.09470836073160172\n",
      "Epoch 3309: train loss: 0.03081122599542141, val loss: 0.0792856439948082\n",
      "Epoch 3310: train loss: 0.027438050135970116, val loss: 0.08588613569736481\n",
      "Epoch 3311: train loss: 0.0294675100594759, val loss: 0.11419141292572021\n",
      "Epoch 3312: train loss: 0.03286445885896683, val loss: 0.093930684030056\n",
      "Epoch 3313: train loss: 0.02927485667169094, val loss: 0.09537137299776077\n",
      "Epoch 3314: train loss: 0.045938894152641296, val loss: 0.0689876452088356\n",
      "Epoch 3315: train loss: 0.028259558603167534, val loss: 0.09015476703643799\n",
      "Epoch 3316: train loss: 0.03370882198214531, val loss: 0.09305062144994736\n",
      "Epoch 3317: train loss: 0.03036605380475521, val loss: 0.10295040905475616\n",
      "Epoch 3318: train loss: 0.02764974534511566, val loss: 0.1097516417503357\n",
      "Epoch 3319: train loss: 0.025267047807574272, val loss: 0.053066518157720566\n",
      "Epoch 3320: train loss: 0.020986024290323257, val loss: 0.10534689575433731\n",
      "Epoch 3321: train loss: 0.01884019374847412, val loss: 0.1036544218659401\n",
      "Epoch 3322: train loss: 0.030798934400081635, val loss: 0.09225866198539734\n",
      "Epoch 3323: train loss: 0.031459320336580276, val loss: 0.11375682801008224\n",
      "Epoch 3324: train loss: 0.028494762256741524, val loss: 0.05832833796739578\n",
      "Epoch 3325: train loss: 0.020423322916030884, val loss: 0.09547366946935654\n",
      "Epoch 3326: train loss: 0.0352335087954998, val loss: 0.04490208253264427\n",
      "Epoch 3327: train loss: 0.031454432755708694, val loss: 0.06155172735452652\n",
      "Epoch 3328: train loss: 0.026427024975419044, val loss: 0.08901816606521606\n",
      "Epoch 3329: train loss: 0.02351861633360386, val loss: 0.05887811258435249\n",
      "Epoch 3330: train loss: 0.031052090227603912, val loss: 0.08923014253377914\n",
      "Epoch 3331: train loss: 0.03722480684518814, val loss: 0.10668563842773438\n",
      "Epoch 3332: train loss: 0.02172493189573288, val loss: 0.12704305350780487\n",
      "Epoch 3333: train loss: 0.021754156798124313, val loss: 0.0852634534239769\n",
      "Epoch 3334: train loss: 0.0262113306671381, val loss: 0.03715408965945244\n",
      "Epoch 3335: train loss: 0.024441830813884735, val loss: 0.1031343936920166\n",
      "Epoch 3336: train loss: 0.030391007661819458, val loss: 0.07661136239767075\n",
      "Epoch 3337: train loss: 0.03011752851307392, val loss: 0.10607384890317917\n",
      "Epoch 3338: train loss: 0.03474372997879982, val loss: 0.05942327901721001\n",
      "Epoch 3339: train loss: 0.027186628431081772, val loss: 0.13379277288913727\n",
      "Epoch 3340: train loss: 0.025137970224022865, val loss: 0.12018781155347824\n",
      "Epoch 3341: train loss: 0.02974170632660389, val loss: 0.044096898287534714\n",
      "Epoch 3342: train loss: 0.048947252333164215, val loss: 0.07116137444972992\n",
      "Epoch 3343: train loss: 0.027335690334439278, val loss: 0.08141341805458069\n",
      "Epoch 3344: train loss: 0.02680901437997818, val loss: 0.07065275311470032\n",
      "Epoch 3345: train loss: 0.02281547710299492, val loss: 0.06507265567779541\n",
      "Epoch 3346: train loss: 0.020992998033761978, val loss: 0.12218492478132248\n",
      "Epoch 3347: train loss: 0.023291509598493576, val loss: 0.09699798375368118\n",
      "Epoch 3348: train loss: 0.022486234083771706, val loss: 0.13486342132091522\n",
      "Epoch 3349: train loss: 0.025240246206521988, val loss: 0.10725685209035873\n",
      "Epoch 3350: train loss: 0.028036950156092644, val loss: 0.10770954191684723\n",
      "Epoch 3351: train loss: 0.03392229229211807, val loss: 0.10106144100427628\n",
      "Epoch 3352: train loss: 0.03375443071126938, val loss: 0.1319819539785385\n",
      "Epoch 3353: train loss: 0.025664648041129112, val loss: 0.07069215923547745\n",
      "Epoch 3354: train loss: 0.02319968119263649, val loss: 0.08610441535711288\n",
      "Epoch 3355: train loss: 0.02572227083146572, val loss: 0.061167337000370026\n",
      "Epoch 3356: train loss: 0.03055240958929062, val loss: 0.1051424965262413\n",
      "Epoch 3357: train loss: 0.02420073375105858, val loss: 0.08359500765800476\n",
      "Epoch 3358: train loss: 0.02976793795824051, val loss: 0.11508778482675552\n",
      "Epoch 3359: train loss: 0.03248201683163643, val loss: 0.0875946432352066\n",
      "Epoch 3360: train loss: 0.03012707829475403, val loss: 0.06440376490354538\n",
      "Epoch 3361: train loss: 0.031480103731155396, val loss: 0.09052202850580215\n",
      "Epoch 3362: train loss: 0.03855321556329727, val loss: 0.049034152179956436\n",
      "Epoch 3363: train loss: 0.03297121822834015, val loss: 0.10934288799762726\n",
      "Epoch 3364: train loss: 0.030011018738150597, val loss: 0.08129456639289856\n",
      "Epoch 3365: train loss: 0.03237268328666687, val loss: 0.10910395532846451\n",
      "Epoch 3366: train loss: 0.035952452570199966, val loss: 0.07351479679346085\n",
      "Epoch 3367: train loss: 0.04237506166100502, val loss: 0.03578577935695648\n",
      "Epoch 3368: train loss: 0.041935212910175323, val loss: 0.10471790283918381\n",
      "Epoch 3369: train loss: 0.026440611109137535, val loss: 0.1212787851691246\n",
      "Epoch 3370: train loss: 0.03352072089910507, val loss: 0.09605816751718521\n",
      "Epoch 3371: train loss: 0.030856173485517502, val loss: 0.07329895347356796\n",
      "Epoch 3372: train loss: 0.035582732409238815, val loss: 0.05624544620513916\n",
      "Epoch 3373: train loss: 0.03212474659085274, val loss: 0.05322805792093277\n",
      "Epoch 3374: train loss: 0.02502443641424179, val loss: 0.08235486596822739\n",
      "Epoch 3375: train loss: 0.02662275731563568, val loss: 0.08870239555835724\n",
      "Epoch 3376: train loss: 0.029058318585157394, val loss: 0.06327684968709946\n",
      "Epoch 3377: train loss: 0.03415282443165779, val loss: 0.09079404920339584\n",
      "Epoch 3378: train loss: 0.03158242255449295, val loss: 0.08407404273748398\n",
      "Epoch 3379: train loss: 0.03385728597640991, val loss: 0.16485634446144104\n",
      "Epoch 3380: train loss: 0.035264335572719574, val loss: 0.07030770927667618\n",
      "Epoch 3381: train loss: 0.028763681650161743, val loss: 0.1029580608010292\n",
      "Epoch 3382: train loss: 0.026677962392568588, val loss: 0.10624581575393677\n",
      "Epoch 3383: train loss: 0.031410399824380875, val loss: 0.08605458587408066\n",
      "Epoch 3384: train loss: 0.028445614501833916, val loss: 0.10994364321231842\n",
      "Epoch 3385: train loss: 0.02894032932817936, val loss: 0.07625385373830795\n",
      "Epoch 3386: train loss: 0.029413452371954918, val loss: 0.10858664661645889\n",
      "Epoch 3387: train loss: 0.02800806425511837, val loss: 0.06641745567321777\n",
      "Epoch 3388: train loss: 0.029575973749160767, val loss: 0.07659917324781418\n",
      "Epoch 3389: train loss: 0.02412746287882328, val loss: 0.09273885935544968\n",
      "Epoch 3390: train loss: 0.028558144345879555, val loss: 0.08609601110219955\n",
      "Epoch 3391: train loss: 0.028700223192572594, val loss: 0.09591531753540039\n",
      "Epoch 3392: train loss: 0.025249706581234932, val loss: 0.05471162870526314\n",
      "Epoch 3393: train loss: 0.024372681975364685, val loss: 0.09017382562160492\n",
      "Epoch 3394: train loss: 0.021290743723511696, val loss: 0.11959981918334961\n",
      "Epoch 3395: train loss: 0.02101065404713154, val loss: 0.08283861726522446\n",
      "Epoch 3396: train loss: 0.02662821114063263, val loss: 0.09885633736848831\n",
      "Epoch 3397: train loss: 0.042030222713947296, val loss: 0.08885157853364944\n",
      "Epoch 3398: train loss: 0.03985363617539406, val loss: 0.06817584484815598\n",
      "Epoch 3399: train loss: 0.025609271600842476, val loss: 0.04967667534947395\n",
      "Epoch 3400: train loss: 0.03612999990582466, val loss: 0.04791538044810295\n",
      "Epoch 3401: train loss: 0.03202851116657257, val loss: 0.0767810270190239\n",
      "Epoch 3402: train loss: 0.028058364987373352, val loss: 0.09384438395500183\n",
      "Epoch 3403: train loss: 0.03260018676519394, val loss: 0.09066516160964966\n",
      "Epoch 3404: train loss: 0.023769833147525787, val loss: 0.11782504618167877\n",
      "Epoch 3405: train loss: 0.031984150409698486, val loss: 0.08171358704566956\n",
      "Epoch 3406: train loss: 0.021635746583342552, val loss: 0.05625656247138977\n",
      "Epoch 3407: train loss: 0.025080574676394463, val loss: 0.07408218830823898\n",
      "Epoch 3408: train loss: 0.031391166150569916, val loss: 0.10110988467931747\n",
      "Epoch 3409: train loss: 0.021137718111276627, val loss: 0.1075756773352623\n",
      "Epoch 3410: train loss: 0.03177634999155998, val loss: 0.08033011853694916\n",
      "Epoch 3411: train loss: 0.020137211307883263, val loss: 0.05037171393632889\n",
      "Epoch 3412: train loss: 0.02636002190411091, val loss: 0.07863626629114151\n",
      "Epoch 3413: train loss: 0.024981943890452385, val loss: 0.07760502398014069\n",
      "Epoch 3414: train loss: 0.030763773247599602, val loss: 0.07090277969837189\n",
      "Epoch 3415: train loss: 0.02574986405670643, val loss: 0.05609869584441185\n",
      "Epoch 3416: train loss: 0.03516635298728943, val loss: 0.07785878330469131\n",
      "Epoch 3417: train loss: 0.029832683503627777, val loss: 0.11814140528440475\n",
      "Epoch 3418: train loss: 0.025110982358455658, val loss: 0.08994866907596588\n",
      "Epoch 3419: train loss: 0.032620687037706375, val loss: 0.07597193866968155\n",
      "Epoch 3420: train loss: 0.030012046918272972, val loss: 0.049214523285627365\n",
      "Epoch 3421: train loss: 0.02729720249772072, val loss: 0.1014445349574089\n",
      "Epoch 3422: train loss: 0.025989850983023643, val loss: 0.10035033524036407\n",
      "Epoch 3423: train loss: 0.02266412042081356, val loss: 0.11376740783452988\n",
      "Epoch 3424: train loss: 0.0320238322019577, val loss: 0.07613154500722885\n",
      "Epoch 3425: train loss: 0.02772744558751583, val loss: 0.07037100940942764\n",
      "Epoch 3426: train loss: 0.029040686786174774, val loss: 0.09450476616621017\n",
      "Epoch 3427: train loss: 0.021855825558304787, val loss: 0.06229367479681969\n",
      "Epoch 3428: train loss: 0.02855335921049118, val loss: 0.061770517379045486\n",
      "Epoch 3429: train loss: 0.02845408394932747, val loss: 0.08837565034627914\n",
      "Epoch 3430: train loss: 0.028767654672265053, val loss: 0.05876678228378296\n",
      "Epoch 3431: train loss: 0.02460261434316635, val loss: 0.06412767618894577\n",
      "Epoch 3432: train loss: 0.022830545902252197, val loss: 0.08720842748880386\n",
      "Epoch 3433: train loss: 0.029179779812693596, val loss: 0.102124884724617\n",
      "Epoch 3434: train loss: 0.02974565327167511, val loss: 0.05759815126657486\n",
      "Epoch 3435: train loss: 0.02288992702960968, val loss: 0.08358724415302277\n",
      "Epoch 3436: train loss: 0.027851110324263573, val loss: 0.05687849596142769\n",
      "Epoch 3437: train loss: 0.036980655044317245, val loss: 0.08539829403162003\n",
      "Epoch 3438: train loss: 0.028027577325701714, val loss: 0.07759831100702286\n",
      "Epoch 3439: train loss: 0.021739844232797623, val loss: 0.05842586234211922\n",
      "Epoch 3440: train loss: 0.0357133187353611, val loss: 0.04888012632727623\n",
      "Epoch 3441: train loss: 0.033136654645204544, val loss: 0.07755625993013382\n",
      "Epoch 3442: train loss: 0.02925674244761467, val loss: 0.08692332357168198\n",
      "Epoch 3443: train loss: 0.024323223158717155, val loss: 0.0835379883646965\n",
      "Epoch 3444: train loss: 0.028144456446170807, val loss: 0.078945092856884\n",
      "Epoch 3445: train loss: 0.03962744027376175, val loss: 0.06281238049268723\n",
      "Epoch 3446: train loss: 0.03449087589979172, val loss: 0.13909849524497986\n",
      "Epoch 3447: train loss: 0.03637966513633728, val loss: 0.12306560575962067\n",
      "Epoch 3448: train loss: 0.038338907063007355, val loss: 0.0765378475189209\n",
      "Epoch 3449: train loss: 0.03172127902507782, val loss: 0.09112649410963058\n",
      "Epoch 3450: train loss: 0.02541307359933853, val loss: 0.08533524721860886\n",
      "Epoch 3451: train loss: 0.0267065167427063, val loss: 0.10439576953649521\n",
      "Epoch 3452: train loss: 0.026314735412597656, val loss: 0.08563738316297531\n",
      "Epoch 3453: train loss: 0.03472450375556946, val loss: 0.1038748249411583\n",
      "Epoch 3454: train loss: 0.03305577486753464, val loss: 0.043344270437955856\n",
      "Epoch 3455: train loss: 0.02917260117828846, val loss: 0.04792596399784088\n",
      "Epoch 3456: train loss: 0.03333264961838722, val loss: 0.07494135200977325\n",
      "Epoch 3457: train loss: 0.040479741990566254, val loss: 0.06320834159851074\n",
      "Epoch 3458: train loss: 0.030405081808567047, val loss: 0.12668131291866302\n",
      "Epoch 3459: train loss: 0.031799908727407455, val loss: 0.1150735542178154\n",
      "Epoch 3460: train loss: 0.027050945907831192, val loss: 0.03087550960481167\n",
      "Epoch 3461: train loss: 0.023449596017599106, val loss: 0.1005595251917839\n",
      "Epoch 3462: train loss: 0.02790984883904457, val loss: 0.0718231126666069\n",
      "Epoch 3463: train loss: 0.03481023386120796, val loss: 0.051936086267232895\n",
      "Epoch 3464: train loss: 0.03307373449206352, val loss: 0.1018245741724968\n",
      "Epoch 3465: train loss: 0.03259247913956642, val loss: 0.0750330314040184\n",
      "Epoch 3466: train loss: 0.028513766825199127, val loss: 0.05883343145251274\n",
      "Epoch 3467: train loss: 0.03175227716565132, val loss: 0.08468147367238998\n",
      "Epoch 3468: train loss: 0.02816416136920452, val loss: 0.0745108500123024\n",
      "Epoch 3469: train loss: 0.030535433441400528, val loss: 0.07450086623430252\n",
      "Epoch 3470: train loss: 0.021020447835326195, val loss: 0.08923589438199997\n",
      "Epoch 3471: train loss: 0.01922008767724037, val loss: 0.09786415100097656\n",
      "Epoch 3472: train loss: 0.03159405291080475, val loss: 0.10912968218326569\n",
      "Epoch 3473: train loss: 0.025423526763916016, val loss: 0.08115226030349731\n",
      "Epoch 3474: train loss: 0.029452530667185783, val loss: 0.10282337665557861\n",
      "Epoch 3475: train loss: 0.02310357615351677, val loss: 0.08742775768041611\n",
      "Epoch 3476: train loss: 0.026661120355129242, val loss: 0.048037003725767136\n",
      "Epoch 3477: train loss: 0.027862586081027985, val loss: 0.11334600299596786\n",
      "Epoch 3478: train loss: 0.023870734497904778, val loss: 0.06775730103254318\n",
      "Epoch 3479: train loss: 0.02732979506254196, val loss: 0.09414350986480713\n",
      "Epoch 3480: train loss: 0.03218759223818779, val loss: 0.17280299961566925\n",
      "Epoch 3481: train loss: 0.02647220343351364, val loss: 0.08204914629459381\n",
      "Epoch 3482: train loss: 0.027448877692222595, val loss: 0.07597438246011734\n",
      "Epoch 3483: train loss: 0.023296110332012177, val loss: 0.04720962420105934\n",
      "Epoch 3484: train loss: 0.023226197808980942, val loss: 0.10813456028699875\n",
      "Epoch 3485: train loss: 0.027973702177405357, val loss: 0.04929200932383537\n",
      "Epoch 3486: train loss: 0.03023586794734001, val loss: 0.08520732074975967\n",
      "Epoch 3487: train loss: 0.029247308149933815, val loss: 0.10669324547052383\n",
      "Epoch 3488: train loss: 0.0254947729408741, val loss: 0.09980888664722443\n",
      "Epoch 3489: train loss: 0.02211780659854412, val loss: 0.08550842851400375\n",
      "Epoch 3490: train loss: 0.020468952134251595, val loss: 0.08000149577856064\n",
      "Epoch 3491: train loss: 0.025654640048742294, val loss: 0.097391776740551\n",
      "Epoch 3492: train loss: 0.026350397616624832, val loss: 0.06017858535051346\n",
      "Epoch 3493: train loss: 0.04357019066810608, val loss: 0.0773555114865303\n",
      "Epoch 3494: train loss: 0.03741554170846939, val loss: 0.08004309237003326\n",
      "Epoch 3495: train loss: 0.03273354098200798, val loss: 0.07873320579528809\n",
      "Epoch 3496: train loss: 0.0375911183655262, val loss: 0.08012589067220688\n",
      "Epoch 3497: train loss: 0.031307172030210495, val loss: 0.08166705816984177\n",
      "Epoch 3498: train loss: 0.0358160026371479, val loss: 0.06527330726385117\n",
      "Epoch 3499: train loss: 0.037518009543418884, val loss: 0.08799929171800613\n",
      "Epoch 3500: train loss: 0.02695191279053688, val loss: 0.1414407342672348\n",
      "Epoch 3501: train loss: 0.03043472021818161, val loss: 0.09587056189775467\n",
      "Epoch 3502: train loss: 0.02889435365796089, val loss: 0.08104601502418518\n",
      "Epoch 3503: train loss: 0.024441732093691826, val loss: 0.1012258529663086\n",
      "Epoch 3504: train loss: 0.029613150283694267, val loss: 0.12958790361881256\n",
      "Epoch 3505: train loss: 0.02672879956662655, val loss: 0.08305730670690536\n",
      "Epoch 3506: train loss: 0.020463552325963974, val loss: 0.06425713747739792\n",
      "Epoch 3507: train loss: 0.024197552353143692, val loss: 0.09289109706878662\n",
      "Epoch 3508: train loss: 0.02254486456513405, val loss: 0.0714506059885025\n",
      "Epoch 3509: train loss: 0.023300502449274063, val loss: 0.09292089194059372\n",
      "Epoch 3510: train loss: 0.026474742218852043, val loss: 0.07512088865041733\n",
      "Epoch 3511: train loss: 0.027095641940832138, val loss: 0.05863797292113304\n",
      "Epoch 3512: train loss: 0.02624676562845707, val loss: 0.07201747596263885\n",
      "Epoch 3513: train loss: 0.029285522177815437, val loss: 0.07756006717681885\n",
      "Epoch 3514: train loss: 0.0310510266572237, val loss: 0.07181758433580399\n",
      "Epoch 3515: train loss: 0.033775150775909424, val loss: 0.10547192394733429\n",
      "Epoch 3516: train loss: 0.03134911507368088, val loss: 0.07287592440843582\n",
      "Epoch 3517: train loss: 0.03138861060142517, val loss: 0.11810500919818878\n",
      "Epoch 3518: train loss: 0.023312555626034737, val loss: 0.09861664474010468\n",
      "Epoch 3519: train loss: 0.024390019476413727, val loss: 0.12478476762771606\n",
      "Epoch 3520: train loss: 0.018538089469075203, val loss: 0.06966713815927505\n",
      "Epoch 3521: train loss: 0.02583642676472664, val loss: 0.061280250549316406\n",
      "Epoch 3522: train loss: 0.029419075697660446, val loss: 0.0743524581193924\n",
      "Epoch 3523: train loss: 0.023894373327493668, val loss: 0.09314564615488052\n",
      "Epoch 3524: train loss: 0.018390769138932228, val loss: 0.08558141440153122\n",
      "Epoch 3525: train loss: 0.02499443292617798, val loss: 0.140659362077713\n",
      "Epoch 3526: train loss: 0.029383178800344467, val loss: 0.07679828256368637\n",
      "Epoch 3527: train loss: 0.025454718619585037, val loss: 0.10103706270456314\n",
      "Epoch 3528: train loss: 0.03664432838559151, val loss: 0.10701628774404526\n",
      "Epoch 3529: train loss: 0.02518889121711254, val loss: 0.10355398803949356\n",
      "Epoch 3530: train loss: 0.03057742677628994, val loss: 0.13557936251163483\n",
      "Epoch 3531: train loss: 0.03294815868139267, val loss: 0.13361869752407074\n",
      "Epoch 3532: train loss: 0.023490015417337418, val loss: 0.09011180698871613\n",
      "Epoch 3533: train loss: 0.02578842081129551, val loss: 0.12937524914741516\n",
      "Epoch 3534: train loss: 0.024884801357984543, val loss: 0.07601466029882431\n",
      "Epoch 3535: train loss: 0.029206303879618645, val loss: 0.12153778225183487\n",
      "Epoch 3536: train loss: 0.021610461175441742, val loss: 0.06049686297774315\n",
      "Epoch 3537: train loss: 0.038382239639759064, val loss: 0.09751810133457184\n",
      "Epoch 3538: train loss: 0.03245716542005539, val loss: 0.06694481521844864\n",
      "Epoch 3539: train loss: 0.028066443279385567, val loss: 0.07532352209091187\n",
      "Epoch 3540: train loss: 0.02372015453875065, val loss: 0.08072049915790558\n",
      "Epoch 3541: train loss: 0.021909965202212334, val loss: 0.11726472526788712\n",
      "Epoch 3542: train loss: 0.02639106474816799, val loss: 0.09569448232650757\n",
      "Epoch 3543: train loss: 0.030029727146029472, val loss: 0.036971528083086014\n",
      "Epoch 3544: train loss: 0.026404008269309998, val loss: 0.04789149761199951\n",
      "Epoch 3545: train loss: 0.022747887298464775, val loss: 0.04524048790335655\n",
      "Epoch 3546: train loss: 0.018440308049321175, val loss: 0.08044599741697311\n",
      "Epoch 3547: train loss: 0.020549019798636436, val loss: 0.0831281766295433\n",
      "Epoch 3548: train loss: 0.02242652140557766, val loss: 0.08935833722352982\n",
      "Epoch 3549: train loss: 0.021417543292045593, val loss: 0.08960466831922531\n",
      "Epoch 3550: train loss: 0.02254398725926876, val loss: 0.0818236693739891\n",
      "Epoch 3551: train loss: 0.022908302024006844, val loss: 0.09752286970615387\n",
      "Epoch 3552: train loss: 0.024076849222183228, val loss: 0.11527729034423828\n",
      "Epoch 3553: train loss: 0.017547419294714928, val loss: 0.15478776395320892\n",
      "Epoch 3554: train loss: 0.0197678841650486, val loss: 0.08460060507059097\n",
      "Epoch 3555: train loss: 0.01580803468823433, val loss: 0.06940441578626633\n",
      "Epoch 3556: train loss: 0.02920784242451191, val loss: 0.10571157187223434\n",
      "Epoch 3557: train loss: 0.026444459334015846, val loss: 0.08683989197015762\n",
      "Epoch 3558: train loss: 0.02732980065047741, val loss: 0.07761204242706299\n",
      "Epoch 3559: train loss: 0.02478301338851452, val loss: 0.07322704046964645\n",
      "Epoch 3560: train loss: 0.022370820865035057, val loss: 0.08686039596796036\n",
      "Epoch 3561: train loss: 0.027045838534832, val loss: 0.10502982139587402\n",
      "Epoch 3562: train loss: 0.02493320032954216, val loss: 0.07063744962215424\n",
      "Epoch 3563: train loss: 0.025975188240408897, val loss: 0.0806698203086853\n",
      "Epoch 3564: train loss: 0.02205660194158554, val loss: 0.08623301237821579\n",
      "Epoch 3565: train loss: 0.025409052148461342, val loss: 0.0966571569442749\n",
      "Epoch 3566: train loss: 0.029270503669977188, val loss: 0.05847986415028572\n",
      "Epoch 3567: train loss: 0.021583741530776024, val loss: 0.09088069200515747\n",
      "Epoch 3568: train loss: 0.028162144124507904, val loss: 0.1020667552947998\n",
      "Epoch 3569: train loss: 0.025218630209565163, val loss: 0.09202035516500473\n",
      "Epoch 3570: train loss: 0.026299551129341125, val loss: 0.08590225130319595\n",
      "Epoch 3571: train loss: 0.032238464802503586, val loss: 0.1199221983551979\n",
      "Epoch 3572: train loss: 0.018801672384142876, val loss: 0.05691823363304138\n",
      "Epoch 3573: train loss: 0.022588666528463364, val loss: 0.0848187804222107\n",
      "Epoch 3574: train loss: 0.0309897530823946, val loss: 0.06167199835181236\n",
      "Epoch 3575: train loss: 0.022662071511149406, val loss: 0.07620945572853088\n",
      "Epoch 3576: train loss: 0.02580120600759983, val loss: 0.10615966469049454\n",
      "Epoch 3577: train loss: 0.026922011747956276, val loss: 0.07833719998598099\n",
      "Epoch 3578: train loss: 0.03150052949786186, val loss: 0.06786497682332993\n",
      "Epoch 3579: train loss: 0.021840224042534828, val loss: 0.0826173722743988\n",
      "Epoch 3580: train loss: 0.0218906719237566, val loss: 0.09469690173864365\n",
      "Epoch 3581: train loss: 0.027540555223822594, val loss: 0.08746457099914551\n",
      "Epoch 3582: train loss: 0.030427180230617523, val loss: 0.12438418716192245\n",
      "Epoch 3583: train loss: 0.026048408821225166, val loss: 0.08802156895399094\n",
      "Epoch 3584: train loss: 0.028174567967653275, val loss: 0.0713743343949318\n",
      "Epoch 3585: train loss: 0.02598383277654648, val loss: 0.0693012997508049\n",
      "Epoch 3586: train loss: 0.027521351352334023, val loss: 0.08232548087835312\n",
      "Epoch 3587: train loss: 0.027646193280816078, val loss: 0.08311615139245987\n",
      "Epoch 3588: train loss: 0.01929229125380516, val loss: 0.0939660295844078\n",
      "Epoch 3589: train loss: 0.020648088306188583, val loss: 0.09918226301670074\n",
      "Epoch 3590: train loss: 0.01931132562458515, val loss: 0.06226256489753723\n",
      "Epoch 3591: train loss: 0.021419856697320938, val loss: 0.10810422897338867\n",
      "Epoch 3592: train loss: 0.027954626828432083, val loss: 0.05557844042778015\n",
      "Epoch 3593: train loss: 0.0187530554831028, val loss: 0.10670147091150284\n",
      "Epoch 3594: train loss: 0.029179023578763008, val loss: 0.09706676006317139\n",
      "Epoch 3595: train loss: 0.02136022225022316, val loss: 0.09809336811304092\n",
      "Epoch 3596: train loss: 0.027604784816503525, val loss: 0.1163354441523552\n",
      "Epoch 3597: train loss: 0.02067331224679947, val loss: 0.08500651270151138\n",
      "Epoch 3598: train loss: 0.01649993658065796, val loss: 0.05703803524374962\n",
      "Epoch 3599: train loss: 0.02900828793644905, val loss: 0.07529164105653763\n",
      "Epoch 3600: train loss: 0.022624937817454338, val loss: 0.09884927421808243\n",
      "Epoch 3601: train loss: 0.017737481743097305, val loss: 0.08517592400312424\n",
      "Epoch 3602: train loss: 0.018839873373508453, val loss: 0.06463413685560226\n",
      "Epoch 3603: train loss: 0.02895224466919899, val loss: 0.052894193679094315\n",
      "Epoch 3604: train loss: 0.024444611743092537, val loss: 0.05147243291139603\n",
      "Epoch 3605: train loss: 0.027916522696614265, val loss: 0.07590799778699875\n",
      "Epoch 3606: train loss: 0.02634015493094921, val loss: 0.051878441125154495\n",
      "Epoch 3607: train loss: 0.030375365167856216, val loss: 0.07073000818490982\n",
      "Epoch 3608: train loss: 0.02256454899907112, val loss: 0.11599808931350708\n",
      "Epoch 3609: train loss: 0.017066558822989464, val loss: 0.0806039497256279\n",
      "Epoch 3610: train loss: 0.019807441160082817, val loss: 0.09714134782552719\n",
      "Epoch 3611: train loss: 0.028504233807325363, val loss: 0.07257591933012009\n",
      "Epoch 3612: train loss: 0.01712888479232788, val loss: 0.12113926559686661\n",
      "Epoch 3613: train loss: 0.028462441638112068, val loss: 0.09848447889089584\n",
      "Epoch 3614: train loss: 0.023216523230075836, val loss: 0.08493392914533615\n",
      "Epoch 3615: train loss: 0.031486958265304565, val loss: 0.07951626926660538\n",
      "Epoch 3616: train loss: 0.023541085422039032, val loss: 0.09372528642416\n",
      "Epoch 3617: train loss: 0.030582057312130928, val loss: 0.07291849702596664\n",
      "Epoch 3618: train loss: 0.030828868970274925, val loss: 0.07933418452739716\n",
      "Epoch 3619: train loss: 0.030963486060500145, val loss: 0.10095825046300888\n",
      "Epoch 3620: train loss: 0.03623759746551514, val loss: 0.08902575820684433\n",
      "Epoch 3621: train loss: 0.029682906344532967, val loss: 0.08055590093135834\n",
      "Epoch 3622: train loss: 0.03370054438710213, val loss: 0.10327921062707901\n",
      "Epoch 3623: train loss: 0.030150266364216805, val loss: 0.07752342522144318\n",
      "Epoch 3624: train loss: 0.03212229907512665, val loss: 0.11168650537729263\n",
      "Epoch 3625: train loss: 0.021725837141275406, val loss: 0.0918099656701088\n",
      "Epoch 3626: train loss: 0.021097801625728607, val loss: 0.0976187139749527\n",
      "Epoch 3627: train loss: 0.02710459567606449, val loss: 0.12567616999149323\n",
      "Epoch 3628: train loss: 0.02325810305774212, val loss: 0.07110434770584106\n",
      "Epoch 3629: train loss: 0.028111744672060013, val loss: 0.09509721398353577\n",
      "Epoch 3630: train loss: 0.030363144353032112, val loss: 0.12720251083374023\n",
      "Epoch 3631: train loss: 0.02571832574903965, val loss: 0.12025507539510727\n",
      "Epoch 3632: train loss: 0.02546859346330166, val loss: 0.11587150394916534\n",
      "Epoch 3633: train loss: 0.03658530116081238, val loss: 0.08325006067752838\n",
      "Epoch 3634: train loss: 0.028291989117860794, val loss: 0.07635580748319626\n",
      "Epoch 3635: train loss: 0.026352057233452797, val loss: 0.09361367672681808\n",
      "Epoch 3636: train loss: 0.03010583482682705, val loss: 0.12078356742858887\n",
      "Epoch 3637: train loss: 0.025356054306030273, val loss: 0.09574242681264877\n",
      "Epoch 3638: train loss: 0.026683013886213303, val loss: 0.05519036203622818\n",
      "Epoch 3639: train loss: 0.019435204565525055, val loss: 0.13919846713542938\n",
      "Epoch 3640: train loss: 0.024414870887994766, val loss: 0.1019739881157875\n",
      "Epoch 3641: train loss: 0.026186352595686913, val loss: 0.0941440612077713\n",
      "Epoch 3642: train loss: 0.024102523922920227, val loss: 0.10591702908277512\n",
      "Epoch 3643: train loss: 0.027809374034404755, val loss: 0.12156359106302261\n",
      "Epoch 3644: train loss: 0.02764257974922657, val loss: 0.11117470264434814\n",
      "Epoch 3645: train loss: 0.023529354482889175, val loss: 0.0932079553604126\n",
      "Epoch 3646: train loss: 0.0328042209148407, val loss: 0.05436283349990845\n",
      "Epoch 3647: train loss: 0.04180765897035599, val loss: 0.08403672277927399\n",
      "Epoch 3648: train loss: 0.025055544450879097, val loss: 0.08902666717767715\n",
      "Epoch 3649: train loss: 0.02634035237133503, val loss: 0.0955713540315628\n",
      "Epoch 3650: train loss: 0.029220305383205414, val loss: 0.08886680752038956\n",
      "Epoch 3651: train loss: 0.022451022639870644, val loss: 0.05033959820866585\n",
      "Epoch 3652: train loss: 0.02881578356027603, val loss: 0.05665438249707222\n",
      "Epoch 3653: train loss: 0.020918166264891624, val loss: 0.11075440794229507\n",
      "Epoch 3654: train loss: 0.022878430783748627, val loss: 0.06321946531534195\n",
      "Epoch 3655: train loss: 0.03036675602197647, val loss: 0.09497980028390884\n",
      "Epoch 3656: train loss: 0.03320751711726189, val loss: 0.0865296944975853\n",
      "Epoch 3657: train loss: 0.03398623690009117, val loss: 0.06995987892150879\n",
      "Epoch 3658: train loss: 0.027478449046611786, val loss: 0.08608908206224442\n",
      "Epoch 3659: train loss: 0.022786729037761688, val loss: 0.03508283942937851\n",
      "Epoch 3660: train loss: 0.024704083800315857, val loss: 0.06584399938583374\n",
      "Epoch 3661: train loss: 0.0248687956482172, val loss: 0.09782074391841888\n",
      "Epoch 3662: train loss: 0.021585695445537567, val loss: 0.0832672268152237\n",
      "Epoch 3663: train loss: 0.029599566012620926, val loss: 0.049080740660429\n",
      "Epoch 3664: train loss: 0.029114199802279472, val loss: 0.07348907738924026\n",
      "Epoch 3665: train loss: 0.026037275791168213, val loss: 0.06172124296426773\n",
      "Epoch 3666: train loss: 0.03211339935660362, val loss: 0.0756864994764328\n",
      "Epoch 3667: train loss: 0.023713134229183197, val loss: 0.07497673481702805\n",
      "Epoch 3668: train loss: 0.02869567461311817, val loss: 0.09783384948968887\n",
      "Epoch 3669: train loss: 0.028908606618642807, val loss: 0.057572316378355026\n",
      "Epoch 3670: train loss: 0.024189064279198647, val loss: 0.1347232609987259\n",
      "Epoch 3671: train loss: 0.02822875790297985, val loss: 0.0926850363612175\n",
      "Epoch 3672: train loss: 0.026576582342386246, val loss: 0.08967994898557663\n",
      "Epoch 3673: train loss: 0.02338993363082409, val loss: 0.07331246882677078\n",
      "Epoch 3674: train loss: 0.01753089763224125, val loss: 0.09216953814029694\n",
      "Epoch 3675: train loss: 0.024937571957707405, val loss: 0.07796957343816757\n",
      "Epoch 3676: train loss: 0.023506425321102142, val loss: 0.07561284303665161\n",
      "Epoch 3677: train loss: 0.02967674843966961, val loss: 0.12446528673171997\n",
      "Epoch 3678: train loss: 0.02596311829984188, val loss: 0.09846416115760803\n",
      "Epoch 3679: train loss: 0.02976847253739834, val loss: 0.10506434738636017\n",
      "Epoch 3680: train loss: 0.019047575071454048, val loss: 0.06633131206035614\n",
      "Epoch 3681: train loss: 0.01712406799197197, val loss: 0.07455926388502121\n",
      "Epoch 3682: train loss: 0.024460766464471817, val loss: 0.09042232483625412\n",
      "Epoch 3683: train loss: 0.023092476651072502, val loss: 0.08432122319936752\n",
      "Epoch 3684: train loss: 0.018145190551877022, val loss: 0.07246307283639908\n",
      "Epoch 3685: train loss: 0.023487022146582603, val loss: 0.07724695652723312\n",
      "Epoch 3686: train loss: 0.020260676741600037, val loss: 0.04953683540225029\n",
      "Epoch 3687: train loss: 0.02519197016954422, val loss: 0.07781407982110977\n",
      "Epoch 3688: train loss: 0.028595205396413803, val loss: 0.05349431559443474\n",
      "Epoch 3689: train loss: 0.024817200377583504, val loss: 0.07831517606973648\n",
      "Epoch 3690: train loss: 0.02516672946512699, val loss: 0.05336819961667061\n",
      "Epoch 3691: train loss: 0.01999780163168907, val loss: 0.04682237654924393\n",
      "Epoch 3692: train loss: 0.03040846809744835, val loss: 0.07129178196191788\n",
      "Epoch 3693: train loss: 0.02300836518406868, val loss: 0.07789640873670578\n",
      "Epoch 3694: train loss: 0.024921216070652008, val loss: 0.08495800197124481\n",
      "Epoch 3695: train loss: 0.02057594619691372, val loss: 0.09508416801691055\n",
      "Epoch 3696: train loss: 0.0280366949737072, val loss: 0.07336074858903885\n",
      "Epoch 3697: train loss: 0.02368667535483837, val loss: 0.057621777057647705\n",
      "Epoch 3698: train loss: 0.034481268376111984, val loss: 0.1122555136680603\n",
      "Epoch 3699: train loss: 0.026290303096175194, val loss: 0.09290393441915512\n",
      "Epoch 3700: train loss: 0.02821238711476326, val loss: 0.11062677204608917\n",
      "Epoch 3701: train loss: 0.02461239881813526, val loss: 0.06623005121946335\n",
      "Epoch 3702: train loss: 0.024624714627861977, val loss: 0.09555863589048386\n",
      "Epoch 3703: train loss: 0.02868356741964817, val loss: 0.08216910809278488\n",
      "Epoch 3704: train loss: 0.034728966653347015, val loss: 0.060624536126852036\n",
      "Epoch 3705: train loss: 0.024366304278373718, val loss: 0.09124983102083206\n",
      "Epoch 3706: train loss: 0.029712272807955742, val loss: 0.09190322458744049\n",
      "Epoch 3707: train loss: 0.020044533535838127, val loss: 0.05315977334976196\n",
      "Epoch 3708: train loss: 0.028918961063027382, val loss: 0.10062535852193832\n",
      "Epoch 3709: train loss: 0.030684800818562508, val loss: 0.17550566792488098\n",
      "Epoch 3710: train loss: 0.03274691104888916, val loss: 0.04840826615691185\n",
      "Epoch 3711: train loss: 0.02039179764688015, val loss: 0.09437965601682663\n",
      "Epoch 3712: train loss: 0.02473704144358635, val loss: 0.09414879232645035\n",
      "Epoch 3713: train loss: 0.04319798946380615, val loss: 0.060122013092041016\n",
      "Epoch 3714: train loss: 0.025772100314497948, val loss: 0.07438608258962631\n",
      "Epoch 3715: train loss: 0.024933820590376854, val loss: 0.08578405529260635\n",
      "Epoch 3716: train loss: 0.02833501808345318, val loss: 0.09271515160799026\n",
      "Epoch 3717: train loss: 0.02848176844418049, val loss: 0.08448950946331024\n",
      "Epoch 3718: train loss: 0.023328660055994987, val loss: 0.0717962384223938\n",
      "Epoch 3719: train loss: 0.025903888046741486, val loss: 0.054729998111724854\n",
      "Epoch 3720: train loss: 0.03010244481265545, val loss: 0.08879854530096054\n",
      "Epoch 3721: train loss: 0.026022914797067642, val loss: 0.11101778596639633\n",
      "Epoch 3722: train loss: 0.03010772168636322, val loss: 0.08963264524936676\n",
      "Epoch 3723: train loss: 0.02651956118643284, val loss: 0.1245659813284874\n",
      "Epoch 3724: train loss: 0.03136444091796875, val loss: 0.061004430055618286\n",
      "Epoch 3725: train loss: 0.024709399789571762, val loss: 0.07419988512992859\n",
      "Epoch 3726: train loss: 0.02713116630911827, val loss: 0.0935712605714798\n",
      "Epoch 3727: train loss: 0.026466773822903633, val loss: 0.09957372397184372\n",
      "Epoch 3728: train loss: 0.021194834262132645, val loss: 0.06792984157800674\n",
      "Epoch 3729: train loss: 0.02413136698305607, val loss: 0.06903975456953049\n",
      "Epoch 3730: train loss: 0.025862347334623337, val loss: 0.0904298648238182\n",
      "Epoch 3731: train loss: 0.019801065325737, val loss: 0.054291434586048126\n",
      "Epoch 3732: train loss: 0.01771809346973896, val loss: 0.07665761560201645\n",
      "Epoch 3733: train loss: 0.021054238080978394, val loss: 0.11097706854343414\n",
      "Epoch 3734: train loss: 0.025871414691209793, val loss: 0.0673394724726677\n",
      "Epoch 3735: train loss: 0.02026824653148651, val loss: 0.09378711134195328\n",
      "Epoch 3736: train loss: 0.02079136110842228, val loss: 0.05706096813082695\n",
      "Epoch 3737: train loss: 0.029664961621165276, val loss: 0.08059289306402206\n",
      "Epoch 3738: train loss: 0.029145199805498123, val loss: 0.061528969556093216\n",
      "Epoch 3739: train loss: 0.01808943785727024, val loss: 0.07800760120153427\n",
      "Epoch 3740: train loss: 0.018104100599884987, val loss: 0.09449763596057892\n",
      "Epoch 3741: train loss: 0.024670880287885666, val loss: 0.04123922809958458\n",
      "Epoch 3742: train loss: 0.03289136290550232, val loss: 0.06527344137430191\n",
      "Epoch 3743: train loss: 0.02084330841898918, val loss: 0.08576042950153351\n",
      "Epoch 3744: train loss: 0.026464728638529778, val loss: 0.06529954820871353\n",
      "Epoch 3745: train loss: 0.02579910308122635, val loss: 0.05596565082669258\n",
      "Epoch 3746: train loss: 0.027123674750328064, val loss: 0.05523150786757469\n",
      "Epoch 3747: train loss: 0.031135136261582375, val loss: 0.05723888799548149\n",
      "Epoch 3748: train loss: 0.02580619789659977, val loss: 0.04481425881385803\n",
      "Epoch 3749: train loss: 0.03448067232966423, val loss: 0.06611282378435135\n",
      "Epoch 3750: train loss: 0.022482888773083687, val loss: 0.0950808897614479\n",
      "Epoch 3751: train loss: 0.021851105615496635, val loss: 0.07334063202142715\n",
      "Epoch 3752: train loss: 0.021641308441758156, val loss: 0.13075527548789978\n",
      "Epoch 3753: train loss: 0.019789593294262886, val loss: 0.09286939352750778\n",
      "Epoch 3754: train loss: 0.02407795563340187, val loss: 0.036893971264362335\n",
      "Epoch 3755: train loss: 0.019252413883805275, val loss: 0.13636715710163116\n",
      "Epoch 3756: train loss: 0.02512337453663349, val loss: 0.07168764621019363\n",
      "Epoch 3757: train loss: 0.039921339601278305, val loss: 0.11080236732959747\n",
      "Epoch 3758: train loss: 0.019167138263583183, val loss: 0.09829951077699661\n",
      "Epoch 3759: train loss: 0.02509058080613613, val loss: 0.09598348289728165\n",
      "Epoch 3760: train loss: 0.029524894431233406, val loss: 0.09982875734567642\n",
      "Epoch 3761: train loss: 0.033210065215826035, val loss: 0.12287350744009018\n",
      "Epoch 3762: train loss: 0.033690351992845535, val loss: 0.041011471301317215\n",
      "Epoch 3763: train loss: 0.03210229054093361, val loss: 0.04007444903254509\n",
      "Epoch 3764: train loss: 0.02687215618789196, val loss: 0.10563907772302628\n",
      "Epoch 3765: train loss: 0.037927042692899704, val loss: 0.0579465888440609\n",
      "Epoch 3766: train loss: 0.03260994702577591, val loss: 0.09646300971508026\n",
      "Epoch 3767: train loss: 0.036067090928554535, val loss: 0.07364281266927719\n",
      "Epoch 3768: train loss: 0.026538213714957237, val loss: 0.056939419358968735\n",
      "Epoch 3769: train loss: 0.02021569386124611, val loss: 0.028437314555048943\n",
      "Epoch 3770: train loss: 0.028062911704182625, val loss: 0.09684077650308609\n",
      "Epoch 3771: train loss: 0.02053990587592125, val loss: 0.07173984497785568\n",
      "Epoch 3772: train loss: 0.031466804444789886, val loss: 0.048717837780714035\n",
      "Epoch 3773: train loss: 0.02257770486176014, val loss: 0.09899234026670456\n",
      "Epoch 3774: train loss: 0.023465849459171295, val loss: 0.1221546158194542\n",
      "Epoch 3775: train loss: 0.025875050574541092, val loss: 0.06282301992177963\n",
      "Epoch 3776: train loss: 0.029336119070649147, val loss: 0.06334562599658966\n",
      "Epoch 3777: train loss: 0.023558733984827995, val loss: 0.08413128554821014\n",
      "Epoch 3778: train loss: 0.028978582471609116, val loss: 0.10740502178668976\n",
      "Epoch 3779: train loss: 0.023061932995915413, val loss: 0.08519843965768814\n",
      "Epoch 3780: train loss: 0.02491692267358303, val loss: 0.10079105198383331\n",
      "Epoch 3781: train loss: 0.027575552463531494, val loss: 0.07496141642332077\n",
      "Epoch 3782: train loss: 0.033691294491291046, val loss: 0.06918960809707642\n",
      "Epoch 3783: train loss: 0.02521131932735443, val loss: 0.05515142157673836\n",
      "Epoch 3784: train loss: 0.0224833432585001, val loss: 0.03159584477543831\n",
      "Epoch 3785: train loss: 0.025172626599669456, val loss: 0.0822685956954956\n",
      "Epoch 3786: train loss: 0.0234629288315773, val loss: 0.05460035800933838\n",
      "Epoch 3787: train loss: 0.018071789294481277, val loss: 0.10223423689603806\n",
      "Epoch 3788: train loss: 0.022762220352888107, val loss: 0.08440696448087692\n",
      "Epoch 3789: train loss: 0.017549671232700348, val loss: 0.13291527330875397\n",
      "Epoch 3790: train loss: 0.025055138394236565, val loss: 0.06520634144544601\n",
      "Epoch 3791: train loss: 0.018576927483081818, val loss: 0.05917704105377197\n",
      "Epoch 3792: train loss: 0.029768245294690132, val loss: 0.08120305836200714\n",
      "Epoch 3793: train loss: 0.02834857627749443, val loss: 0.06568219512701035\n",
      "Epoch 3794: train loss: 0.02189221978187561, val loss: 0.06697734445333481\n",
      "Epoch 3795: train loss: 0.019925110042095184, val loss: 0.08124865591526031\n",
      "Epoch 3796: train loss: 0.028096942231059074, val loss: 0.052327703684568405\n",
      "Epoch 3797: train loss: 0.025700988247990608, val loss: 0.05517623573541641\n",
      "Epoch 3798: train loss: 0.02319580502808094, val loss: 0.0564061664044857\n",
      "Epoch 3799: train loss: 0.0252400990575552, val loss: 0.05710100010037422\n",
      "Epoch 3800: train loss: 0.023147977888584137, val loss: 0.06732872128486633\n",
      "Epoch 3801: train loss: 0.02482057921588421, val loss: 0.06713906675577164\n",
      "Epoch 3802: train loss: 0.02264491282403469, val loss: 0.06261178106069565\n",
      "Epoch 3803: train loss: 0.025227950885891914, val loss: 0.08674769848585129\n",
      "Epoch 3804: train loss: 0.03410954400897026, val loss: 0.08530845493078232\n",
      "Epoch 3805: train loss: 0.024985745549201965, val loss: 0.0794253796339035\n",
      "Epoch 3806: train loss: 0.029485004022717476, val loss: 0.0394098125398159\n",
      "Epoch 3807: train loss: 0.02299816906452179, val loss: 0.04879533872008324\n",
      "Epoch 3808: train loss: 0.02754998207092285, val loss: 0.05124092102050781\n",
      "Epoch 3809: train loss: 0.03497493267059326, val loss: 0.09695588797330856\n",
      "Epoch 3810: train loss: 0.023905156180262566, val loss: 0.08865738660097122\n",
      "Epoch 3811: train loss: 0.021328549832105637, val loss: 0.09468675404787064\n",
      "Epoch 3812: train loss: 0.0235417690128088, val loss: 0.11964128166437149\n",
      "Epoch 3813: train loss: 0.024963660165667534, val loss: 0.08066027611494064\n",
      "Epoch 3814: train loss: 0.020764373242855072, val loss: 0.08613716810941696\n",
      "Epoch 3815: train loss: 0.024092532694339752, val loss: 0.059660911560058594\n",
      "Epoch 3816: train loss: 0.03140225633978844, val loss: 0.0534074604511261\n",
      "Epoch 3817: train loss: 0.03194549307227135, val loss: 0.11345260590314865\n",
      "Epoch 3818: train loss: 0.0201715175062418, val loss: 0.12543198466300964\n",
      "Epoch 3819: train loss: 0.023973243311047554, val loss: 0.10074110329151154\n",
      "Epoch 3820: train loss: 0.023843219503760338, val loss: 0.06705597788095474\n",
      "Epoch 3821: train loss: 0.02669069543480873, val loss: 0.0364549420773983\n",
      "Epoch 3822: train loss: 0.026348957791924477, val loss: 0.05923708900809288\n",
      "Epoch 3823: train loss: 0.02489335834980011, val loss: 0.0922294408082962\n",
      "Epoch 3824: train loss: 0.028985528275370598, val loss: 0.09902860224246979\n",
      "Epoch 3825: train loss: 0.028765270486474037, val loss: 0.0858059898018837\n",
      "Epoch 3826: train loss: 0.029274335131049156, val loss: 0.05402233824133873\n",
      "Epoch 3827: train loss: 0.031211165711283684, val loss: 0.09297806024551392\n",
      "Epoch 3828: train loss: 0.024199025705456734, val loss: 0.09046819061040878\n",
      "Epoch 3829: train loss: 0.02049652859568596, val loss: 0.06998058408498764\n",
      "Epoch 3830: train loss: 0.02942223660647869, val loss: 0.062350478023290634\n",
      "Epoch 3831: train loss: 0.021758534014225006, val loss: 0.15202496945858002\n",
      "Epoch 3832: train loss: 0.025721170008182526, val loss: 0.10624853521585464\n",
      "Epoch 3833: train loss: 0.02845066227018833, val loss: 0.05917126685380936\n",
      "Epoch 3834: train loss: 0.034722454845905304, val loss: 0.09798218309879303\n",
      "Epoch 3835: train loss: 0.02897467091679573, val loss: 0.11824672669172287\n",
      "Epoch 3836: train loss: 0.018442563712596893, val loss: 0.0906413346529007\n",
      "Epoch 3837: train loss: 0.027064114809036255, val loss: 0.07518007606267929\n",
      "Epoch 3838: train loss: 0.03031080588698387, val loss: 0.10572190582752228\n",
      "Epoch 3839: train loss: 0.024942457675933838, val loss: 0.11003034561872482\n",
      "Epoch 3840: train loss: 0.030129168182611465, val loss: 0.11417367309331894\n",
      "Epoch 3841: train loss: 0.029771089553833008, val loss: 0.10743631422519684\n",
      "Epoch 3842: train loss: 0.024494966492056847, val loss: 0.10279905050992966\n",
      "Epoch 3843: train loss: 0.022565463557839394, val loss: 0.09818974882364273\n",
      "Epoch 3844: train loss: 0.02285275235772133, val loss: 0.09501268714666367\n",
      "Epoch 3845: train loss: 0.024411017075181007, val loss: 0.12278313934803009\n",
      "Epoch 3846: train loss: 0.021158888936042786, val loss: 0.11257848888635635\n",
      "Epoch 3847: train loss: 0.019230136647820473, val loss: 0.06953652203083038\n",
      "Epoch 3848: train loss: 0.02304733172059059, val loss: 0.09391099214553833\n",
      "Epoch 3849: train loss: 0.024299537762999535, val loss: 0.06739121675491333\n",
      "Epoch 3850: train loss: 0.02640283666551113, val loss: 0.05848867446184158\n",
      "Epoch 3851: train loss: 0.023025648668408394, val loss: 0.11399205029010773\n",
      "Epoch 3852: train loss: 0.028446413576602936, val loss: 0.09635891020298004\n",
      "Epoch 3853: train loss: 0.029557058587670326, val loss: 0.08931925892829895\n",
      "Epoch 3854: train loss: 0.0239455234259367, val loss: 0.08182308077812195\n",
      "Epoch 3855: train loss: 0.02774052694439888, val loss: 0.05129457265138626\n",
      "Epoch 3856: train loss: 0.022874901071190834, val loss: 0.08349620550870895\n",
      "Epoch 3857: train loss: 0.030290493741631508, val loss: 0.07759925723075867\n",
      "Epoch 3858: train loss: 0.03305363282561302, val loss: 0.06314815580844879\n",
      "Epoch 3859: train loss: 0.020578395575284958, val loss: 0.051003146916627884\n",
      "Epoch 3860: train loss: 0.02596100978553295, val loss: 0.0812152624130249\n",
      "Epoch 3861: train loss: 0.022630834951996803, val loss: 0.06937652826309204\n",
      "Epoch 3862: train loss: 0.02548294886946678, val loss: 0.05146656185388565\n",
      "Epoch 3863: train loss: 0.03720191493630409, val loss: 0.12405848503112793\n",
      "Epoch 3864: train loss: 0.023081332445144653, val loss: 0.08475269377231598\n",
      "Epoch 3865: train loss: 0.03457977995276451, val loss: 0.08870669454336166\n",
      "Epoch 3866: train loss: 0.025125175714492798, val loss: 0.05124510079622269\n",
      "Epoch 3867: train loss: 0.038759712129831314, val loss: 0.07115525007247925\n",
      "Epoch 3868: train loss: 0.022655995562672615, val loss: 0.09324765205383301\n",
      "Epoch 3869: train loss: 0.024987544864416122, val loss: 0.09725413471460342\n",
      "Epoch 3870: train loss: 0.017070159316062927, val loss: 0.07793412357568741\n",
      "Epoch 3871: train loss: 0.023974549025297165, val loss: 0.08035226166248322\n",
      "Epoch 3872: train loss: 0.019085466861724854, val loss: 0.08496139198541641\n",
      "Epoch 3873: train loss: 0.024645276367664337, val loss: 0.05266100913286209\n",
      "Epoch 3874: train loss: 0.025080863386392593, val loss: 0.06835168600082397\n",
      "Epoch 3875: train loss: 0.02082901820540428, val loss: 0.11248729377985\n",
      "Epoch 3876: train loss: 0.026326239109039307, val loss: 0.08986099809408188\n",
      "Epoch 3877: train loss: 0.020917708054184914, val loss: 0.11892759799957275\n",
      "Epoch 3878: train loss: 0.03432783856987953, val loss: 0.05616210028529167\n",
      "Epoch 3879: train loss: 0.03183777257800102, val loss: 0.05007261037826538\n",
      "Epoch 3880: train loss: 0.03087131306529045, val loss: 0.1106632724404335\n",
      "Epoch 3881: train loss: 0.020456114783883095, val loss: 0.0961427316069603\n",
      "Epoch 3882: train loss: 0.020604293793439865, val loss: 0.07174710929393768\n",
      "Epoch 3883: train loss: 0.024805255234241486, val loss: 0.06403187662363052\n",
      "Epoch 3884: train loss: 0.025676865130662918, val loss: 0.08324220031499863\n",
      "Epoch 3885: train loss: 0.020988715812563896, val loss: 0.06289099901914597\n",
      "Epoch 3886: train loss: 0.0199188943952322, val loss: 0.07605151832103729\n",
      "Epoch 3887: train loss: 0.018706632778048515, val loss: 0.11380815505981445\n",
      "Epoch 3888: train loss: 0.027207745239138603, val loss: 0.08440439403057098\n",
      "Epoch 3889: train loss: 0.03206243738532066, val loss: 0.08752848953008652\n",
      "Epoch 3890: train loss: 0.028569968417286873, val loss: 0.07820715010166168\n",
      "Epoch 3891: train loss: 0.02602957747876644, val loss: 0.10687541216611862\n",
      "Epoch 3892: train loss: 0.02665065787732601, val loss: 0.1120571419596672\n",
      "Epoch 3893: train loss: 0.031428005546331406, val loss: 0.06358825415372849\n",
      "Epoch 3894: train loss: 0.0298137404024601, val loss: 0.0649898499250412\n",
      "Epoch 3895: train loss: 0.02974763512611389, val loss: 0.06358128041028976\n",
      "Epoch 3896: train loss: 0.03598291799426079, val loss: 0.06508520245552063\n",
      "Epoch 3897: train loss: 0.027241354808211327, val loss: 0.08886115998029709\n",
      "Epoch 3898: train loss: 0.022834211587905884, val loss: 0.08033610135316849\n",
      "Epoch 3899: train loss: 0.0227280855178833, val loss: 0.10494138300418854\n",
      "Epoch 3900: train loss: 0.03238594904541969, val loss: 0.05562235042452812\n",
      "Epoch 3901: train loss: 0.03252370283007622, val loss: 0.09713005274534225\n",
      "Epoch 3902: train loss: 0.019285092130303383, val loss: 0.07956892251968384\n",
      "Epoch 3903: train loss: 0.026886243373155594, val loss: 0.09406988322734833\n",
      "Epoch 3904: train loss: 0.030164215713739395, val loss: 0.08474832773208618\n",
      "Epoch 3905: train loss: 0.027239244431257248, val loss: 0.08543127030134201\n",
      "Epoch 3906: train loss: 0.02707996964454651, val loss: 0.09273048490285873\n",
      "Epoch 3907: train loss: 0.02053135074675083, val loss: 0.04313826933503151\n",
      "Epoch 3908: train loss: 0.031426772475242615, val loss: 0.10260599106550217\n",
      "Epoch 3909: train loss: 0.02832890674471855, val loss: 0.06697313487529755\n",
      "Epoch 3910: train loss: 0.02611946128308773, val loss: 0.06698481738567352\n",
      "Epoch 3911: train loss: 0.023048557341098785, val loss: 0.07132846117019653\n",
      "Epoch 3912: train loss: 0.02507423423230648, val loss: 0.06979864090681076\n",
      "Epoch 3913: train loss: 0.03162914887070656, val loss: 0.11875619739294052\n",
      "Epoch 3914: train loss: 0.030820118263363838, val loss: 0.07262565940618515\n",
      "Epoch 3915: train loss: 0.02219715341925621, val loss: 0.09128487855195999\n",
      "Epoch 3916: train loss: 0.029002664610743523, val loss: 0.07895253598690033\n",
      "Epoch 3917: train loss: 0.023859813809394836, val loss: 0.12550592422485352\n",
      "Epoch 3918: train loss: 0.024101363494992256, val loss: 0.11003154516220093\n",
      "Epoch 3919: train loss: 0.02261369302868843, val loss: 0.11967829614877701\n",
      "Epoch 3920: train loss: 0.0321442075073719, val loss: 0.08610116690397263\n",
      "Epoch 3921: train loss: 0.027467457577586174, val loss: 0.06596773117780685\n",
      "Epoch 3922: train loss: 0.03680245578289032, val loss: 0.08224568516016006\n",
      "Epoch 3923: train loss: 0.02601560577750206, val loss: 0.0706091895699501\n",
      "Epoch 3924: train loss: 0.027358321473002434, val loss: 0.04846055060625076\n",
      "Epoch 3925: train loss: 0.031970225274562836, val loss: 0.08759951591491699\n",
      "Epoch 3926: train loss: 0.03179614618420601, val loss: 0.08032869547605515\n",
      "Epoch 3927: train loss: 0.035576220601797104, val loss: 0.12596449255943298\n",
      "Epoch 3928: train loss: 0.016890624538064003, val loss: 0.11332426220178604\n",
      "Epoch 3929: train loss: 0.024987641721963882, val loss: 0.07015236467123032\n",
      "Epoch 3930: train loss: 0.027304120361804962, val loss: 0.04873758181929588\n",
      "Epoch 3931: train loss: 0.024249576032161713, val loss: 0.09927840530872345\n",
      "Epoch 3932: train loss: 0.03541482239961624, val loss: 0.10584717988967896\n",
      "Epoch 3933: train loss: 0.02051607333123684, val loss: 0.06538327783346176\n",
      "Epoch 3934: train loss: 0.02576916478574276, val loss: 0.08352800458669662\n",
      "Epoch 3935: train loss: 0.030876485630869865, val loss: 0.062166184186935425\n",
      "Epoch 3936: train loss: 0.028160972520709038, val loss: 0.07826422154903412\n",
      "Epoch 3937: train loss: 0.03235895559191704, val loss: 0.0922098383307457\n",
      "Epoch 3938: train loss: 0.02633051946759224, val loss: 0.06223609670996666\n",
      "Epoch 3939: train loss: 0.028719302266836166, val loss: 0.04481101781129837\n",
      "Epoch 3940: train loss: 0.024644486606121063, val loss: 0.047034792602062225\n",
      "Epoch 3941: train loss: 0.021994465962052345, val loss: 0.0753512755036354\n",
      "Epoch 3942: train loss: 0.021375976502895355, val loss: 0.0703054890036583\n",
      "Epoch 3943: train loss: 0.019846374168992043, val loss: 0.07706044614315033\n",
      "Epoch 3944: train loss: 0.031562380492687225, val loss: 0.048546191304922104\n",
      "Epoch 3945: train loss: 0.02090667188167572, val loss: 0.06174067407846451\n",
      "Epoch 3946: train loss: 0.022505082190036774, val loss: 0.07344774156808853\n",
      "Epoch 3947: train loss: 0.03132889047265053, val loss: 0.10948538780212402\n",
      "Epoch 3948: train loss: 0.033004697412252426, val loss: 0.10757070779800415\n",
      "Epoch 3949: train loss: 0.030186738818883896, val loss: 0.08654742687940598\n",
      "Epoch 3950: train loss: 0.02756696380674839, val loss: 0.10369038581848145\n",
      "Epoch 3951: train loss: 0.02127450704574585, val loss: 0.07655029743909836\n",
      "Epoch 3952: train loss: 0.02620258368551731, val loss: 0.08381568640470505\n",
      "Epoch 3953: train loss: 0.025717729702591896, val loss: 0.09406136721372604\n",
      "Epoch 3954: train loss: 0.026447691023349762, val loss: 0.08124440163373947\n",
      "Epoch 3955: train loss: 0.02299230732023716, val loss: 0.10380998998880386\n",
      "Epoch 3956: train loss: 0.029976092278957367, val loss: 0.05419541522860527\n",
      "Epoch 3957: train loss: 0.020267857238650322, val loss: 0.08593324571847916\n",
      "Epoch 3958: train loss: 0.02729880064725876, val loss: 0.17671003937721252\n",
      "Epoch 3959: train loss: 0.02895846962928772, val loss: 0.0649193674325943\n",
      "Epoch 3960: train loss: 0.025739211589097977, val loss: 0.0854673758149147\n",
      "Epoch 3961: train loss: 0.031244751065969467, val loss: 0.17025168240070343\n",
      "Epoch 3962: train loss: 0.04398459568619728, val loss: 0.0586845763027668\n",
      "Epoch 3963: train loss: 0.030639538541436195, val loss: 0.0412374809384346\n",
      "Epoch 3964: train loss: 0.025788476690649986, val loss: 0.07783197611570358\n",
      "Epoch 3965: train loss: 0.024366557598114014, val loss: 0.05980510264635086\n",
      "Epoch 3966: train loss: 0.02171917073428631, val loss: 0.09448297321796417\n",
      "Epoch 3967: train loss: 0.026200173422694206, val loss: 0.06163065508008003\n",
      "Epoch 3968: train loss: 0.020210489630699158, val loss: 0.0861707553267479\n",
      "Epoch 3969: train loss: 0.03306378796696663, val loss: 0.10832561552524567\n",
      "Epoch 3970: train loss: 0.029224926605820656, val loss: 0.11583247035741806\n",
      "Epoch 3971: train loss: 0.02034665271639824, val loss: 0.12190821021795273\n",
      "Epoch 3972: train loss: 0.028982311487197876, val loss: 0.11481668800115585\n",
      "Epoch 3973: train loss: 0.019851302728056908, val loss: 0.052523549646139145\n",
      "Epoch 3974: train loss: 0.021000944077968597, val loss: 0.06249625235795975\n",
      "Epoch 3975: train loss: 0.021094078198075294, val loss: 0.05511987954378128\n",
      "Epoch 3976: train loss: 0.030648987740278244, val loss: 0.054633405059576035\n",
      "Epoch 3977: train loss: 0.030311282724142075, val loss: 0.08289553225040436\n",
      "Epoch 3978: train loss: 0.022634336724877357, val loss: 0.09537699073553085\n",
      "Epoch 3979: train loss: 0.03981088101863861, val loss: 0.08021502941846848\n",
      "Epoch 3980: train loss: 0.03531334549188614, val loss: 0.08426393568515778\n",
      "Epoch 3981: train loss: 0.024939987808465958, val loss: 0.08634904772043228\n",
      "Epoch 3982: train loss: 0.024462932720780373, val loss: 0.08332714438438416\n",
      "Epoch 3983: train loss: 0.022690678015351295, val loss: 0.12359123677015305\n",
      "Epoch 3984: train loss: 0.02030489593744278, val loss: 0.07037331163883209\n",
      "Epoch 3985: train loss: 0.02402305044233799, val loss: 0.0697440505027771\n",
      "Epoch 3986: train loss: 0.023462794721126556, val loss: 0.07432834059000015\n",
      "Epoch 3987: train loss: 0.018422923982143402, val loss: 0.10523317009210587\n",
      "Epoch 3988: train loss: 0.026068465784192085, val loss: 0.05571252107620239\n",
      "Epoch 3989: train loss: 0.03666527569293976, val loss: 0.1165200024843216\n",
      "Epoch 3990: train loss: 0.025306813418865204, val loss: 0.11022531241178513\n",
      "Epoch 3991: train loss: 0.028490623459219933, val loss: 0.08977330476045609\n",
      "Epoch 3992: train loss: 0.018491435796022415, val loss: 0.09843090921640396\n",
      "Epoch 3993: train loss: 0.02745748683810234, val loss: 0.12456943839788437\n",
      "Epoch 3994: train loss: 0.02118479646742344, val loss: 0.0844619944691658\n",
      "Epoch 3995: train loss: 0.02210286259651184, val loss: 0.06835796684026718\n",
      "Epoch 3996: train loss: 0.025201421231031418, val loss: 0.11320465058088303\n",
      "Epoch 3997: train loss: 0.01960374228656292, val loss: 0.11679498106241226\n",
      "Epoch 3998: train loss: 0.034587059170007706, val loss: 0.0966540202498436\n",
      "Epoch 3999: train loss: 0.021580712869763374, val loss: 0.05257062986493111\n",
      "Epoch 4000: train loss: 0.025103598833084106, val loss: 0.1144798994064331\n",
      "Epoch 4001: train loss: 0.029246147722005844, val loss: 0.09257855266332626\n",
      "Epoch 4002: train loss: 0.022271132096648216, val loss: 0.11026620864868164\n",
      "Epoch 4003: train loss: 0.02087227627635002, val loss: 0.08320838212966919\n",
      "Epoch 4004: train loss: 0.023113489151000977, val loss: 0.08027409017086029\n",
      "Epoch 4005: train loss: 0.02492447942495346, val loss: 0.13357888162136078\n",
      "Epoch 4006: train loss: 0.02778337337076664, val loss: 0.14328475296497345\n",
      "Epoch 4007: train loss: 0.020869173109531403, val loss: 0.056176770478487015\n",
      "Epoch 4008: train loss: 0.02927990071475506, val loss: 0.07862824201583862\n",
      "Epoch 4009: train loss: 0.02024885080754757, val loss: 0.10706155747175217\n",
      "Epoch 4010: train loss: 0.020389605313539505, val loss: 0.12675108015537262\n",
      "Epoch 4011: train loss: 0.027346685528755188, val loss: 0.08133912086486816\n",
      "Epoch 4012: train loss: 0.02708137407898903, val loss: 0.0687117949128151\n",
      "Epoch 4013: train loss: 0.030894774943590164, val loss: 0.11496593803167343\n",
      "Epoch 4014: train loss: 0.020284658297896385, val loss: 0.06171861290931702\n",
      "Epoch 4015: train loss: 0.028479155153036118, val loss: 0.0689242035150528\n",
      "Epoch 4016: train loss: 0.026051532477140427, val loss: 0.057912956923246384\n",
      "Epoch 4017: train loss: 0.024233723059296608, val loss: 0.09704318642616272\n",
      "Epoch 4018: train loss: 0.024066586047410965, val loss: 0.12859338521957397\n",
      "Epoch 4019: train loss: 0.03454343602061272, val loss: 0.08235309273004532\n",
      "Epoch 4020: train loss: 0.0349401980638504, val loss: 0.08253677934408188\n",
      "Epoch 4021: train loss: 0.0367145873606205, val loss: 0.09689294546842575\n",
      "Epoch 4022: train loss: 0.02929927222430706, val loss: 0.1479855328798294\n",
      "Epoch 4023: train loss: 0.03511182963848114, val loss: 0.12320982664823532\n",
      "Epoch 4024: train loss: 0.024983391165733337, val loss: 0.044785358011722565\n",
      "Epoch 4025: train loss: 0.025818565860390663, val loss: 0.10230886191129684\n",
      "Epoch 4026: train loss: 0.030374500900506973, val loss: 0.06186268478631973\n",
      "Epoch 4027: train loss: 0.03047601692378521, val loss: 0.08752035349607468\n",
      "Epoch 4028: train loss: 0.015527655370533466, val loss: 0.07259120047092438\n",
      "Epoch 4029: train loss: 0.02941006049513817, val loss: 0.10047459602355957\n",
      "Epoch 4030: train loss: 0.028083544224500656, val loss: 0.0873752310872078\n",
      "Epoch 4031: train loss: 0.021742410957813263, val loss: 0.09268759191036224\n",
      "Epoch 4032: train loss: 0.02728869952261448, val loss: 0.0978480726480484\n",
      "Epoch 4033: train loss: 0.02675548382103443, val loss: 0.11551300436258316\n",
      "Epoch 4034: train loss: 0.017475131899118423, val loss: 0.09881170839071274\n",
      "Epoch 4035: train loss: 0.023527368903160095, val loss: 0.09212320297956467\n",
      "Epoch 4036: train loss: 0.02527519129216671, val loss: 0.058518003672361374\n",
      "Epoch 4037: train loss: 0.026721036061644554, val loss: 0.13759897649288177\n",
      "Epoch 4038: train loss: 0.022093823179602623, val loss: 0.059036608785390854\n",
      "Epoch 4039: train loss: 0.029167648404836655, val loss: 0.07985633611679077\n",
      "Epoch 4040: train loss: 0.017967894673347473, val loss: 0.05306151509284973\n",
      "Epoch 4041: train loss: 0.019258571788668633, val loss: 0.12275823205709457\n",
      "Epoch 4042: train loss: 0.023904209956526756, val loss: 0.1327597200870514\n",
      "Epoch 4043: train loss: 0.027511116117239, val loss: 0.07233677059412003\n",
      "Epoch 4044: train loss: 0.024281945079565048, val loss: 0.052664052695035934\n",
      "Epoch 4045: train loss: 0.03105168417096138, val loss: 0.08112572133541107\n",
      "Epoch 4046: train loss: 0.025218240916728973, val loss: 0.11615657806396484\n",
      "Epoch 4047: train loss: 0.02009761892259121, val loss: 0.08015954494476318\n",
      "Epoch 4048: train loss: 0.023735182359814644, val loss: 0.09316500276327133\n",
      "Epoch 4049: train loss: 0.027348969131708145, val loss: 0.097592793405056\n",
      "Epoch 4050: train loss: 0.030753793194890022, val loss: 0.09679701179265976\n",
      "Epoch 4051: train loss: 0.027395082637667656, val loss: 0.07266142219305038\n",
      "Epoch 4052: train loss: 0.031506236642599106, val loss: 0.09807514399290085\n",
      "Epoch 4053: train loss: 0.025916730985045433, val loss: 0.051026977598667145\n",
      "Epoch 4054: train loss: 0.02660384774208069, val loss: 0.08661255985498428\n",
      "Epoch 4055: train loss: 0.02698589488863945, val loss: 0.09346532821655273\n",
      "Epoch 4056: train loss: 0.024898629635572433, val loss: 0.10616625845432281\n",
      "Epoch 4057: train loss: 0.015877269208431244, val loss: 0.061518628150224686\n",
      "Epoch 4058: train loss: 0.028531726449728012, val loss: 0.07958286255598068\n",
      "Epoch 4059: train loss: 0.03065335564315319, val loss: 0.075630322098732\n",
      "Epoch 4060: train loss: 0.026355648413300514, val loss: 0.10823024809360504\n",
      "Epoch 4061: train loss: 0.032912224531173706, val loss: 0.06930094957351685\n",
      "Epoch 4062: train loss: 0.0366177000105381, val loss: 0.04513749107718468\n",
      "Epoch 4063: train loss: 0.032766539603471756, val loss: 0.1157214418053627\n",
      "Epoch 4064: train loss: 0.0244864821434021, val loss: 0.10932501405477524\n",
      "Epoch 4065: train loss: 0.0223665963858366, val loss: 0.0662069171667099\n",
      "Epoch 4066: train loss: 0.023401310667395592, val loss: 0.07951473444700241\n",
      "Epoch 4067: train loss: 0.028671521693468094, val loss: 0.06430044025182724\n",
      "Epoch 4068: train loss: 0.02525976113975048, val loss: 0.10660343617200851\n",
      "Epoch 4069: train loss: 0.030706465244293213, val loss: 0.12915034592151642\n",
      "Epoch 4070: train loss: 0.024556918069720268, val loss: 0.09012281149625778\n",
      "Epoch 4071: train loss: 0.027031371369957924, val loss: 0.1087246760725975\n",
      "Epoch 4072: train loss: 0.0220501609146595, val loss: 0.0893448069691658\n",
      "Epoch 4073: train loss: 0.02490760199725628, val loss: 0.05681596323847771\n",
      "Epoch 4074: train loss: 0.018186073750257492, val loss: 0.05464879423379898\n",
      "Epoch 4075: train loss: 0.031072339043021202, val loss: 0.12612059712409973\n",
      "Epoch 4076: train loss: 0.03250255063176155, val loss: 0.10993576049804688\n",
      "Epoch 4077: train loss: 0.022065041586756706, val loss: 0.07809501141309738\n",
      "Epoch 4078: train loss: 0.02254154160618782, val loss: 0.09908941388130188\n",
      "Epoch 4079: train loss: 0.020321562886238098, val loss: 0.08752842992544174\n",
      "Epoch 4080: train loss: 0.030857529491186142, val loss: 0.04553328454494476\n",
      "Epoch 4081: train loss: 0.02796175889670849, val loss: 0.07008352130651474\n",
      "Epoch 4082: train loss: 0.02702431008219719, val loss: 0.09497463703155518\n",
      "Epoch 4083: train loss: 0.01776932366192341, val loss: 0.12411089986562729\n",
      "Epoch 4084: train loss: 0.020219286903738976, val loss: 0.08030903339385986\n",
      "Epoch 4085: train loss: 0.025148263201117516, val loss: 0.07411441951990128\n",
      "Epoch 4086: train loss: 0.026291536167263985, val loss: 0.11198899894952774\n",
      "Epoch 4087: train loss: 0.03431318327784538, val loss: 0.10034836828708649\n",
      "Epoch 4088: train loss: 0.040137890726327896, val loss: 0.05410286411643028\n",
      "Epoch 4089: train loss: 0.02159096673130989, val loss: 0.052779268473386765\n",
      "Epoch 4090: train loss: 0.023086702451109886, val loss: 0.054030466824769974\n",
      "Epoch 4091: train loss: 0.028319789096713066, val loss: 0.054679907858371735\n",
      "Epoch 4092: train loss: 0.023236630484461784, val loss: 0.08343642950057983\n",
      "Epoch 4093: train loss: 0.019524753093719482, val loss: 0.09888020902872086\n",
      "Epoch 4094: train loss: 0.025302860885858536, val loss: 0.09134522825479507\n",
      "Epoch 4095: train loss: 0.02466031163930893, val loss: 0.06652884185314178\n",
      "Epoch 4096: train loss: 0.02264769934117794, val loss: 0.09402921050786972\n",
      "Epoch 4097: train loss: 0.027722062543034554, val loss: 0.03842011094093323\n",
      "Epoch 4098: train loss: 0.02995392307639122, val loss: 0.13335011899471283\n",
      "Epoch 4099: train loss: 0.02733912505209446, val loss: 0.09284914284944534\n",
      "Epoch 4100: train loss: 0.026387760415673256, val loss: 0.07251366227865219\n",
      "Epoch 4101: train loss: 0.020497480407357216, val loss: 0.08342815935611725\n",
      "Epoch 4102: train loss: 0.02685081958770752, val loss: 0.06991832703351974\n",
      "Epoch 4103: train loss: 0.02627122774720192, val loss: 0.06617452204227448\n",
      "Epoch 4104: train loss: 0.023560527712106705, val loss: 0.04598371312022209\n",
      "Epoch 4105: train loss: 0.027881311252713203, val loss: 0.05353875085711479\n",
      "Epoch 4106: train loss: 0.021298013627529144, val loss: 0.10472694784402847\n",
      "Epoch 4107: train loss: 0.029840322211384773, val loss: 0.08994730561971664\n",
      "Epoch 4108: train loss: 0.0255813580006361, val loss: 0.12454777210950851\n",
      "Epoch 4109: train loss: 0.023568185046315193, val loss: 0.11669156700372696\n",
      "Epoch 4110: train loss: 0.03726327419281006, val loss: 0.06502913683652878\n",
      "Epoch 4111: train loss: 0.024870244786143303, val loss: 0.05738576874136925\n",
      "Epoch 4112: train loss: 0.021673019975423813, val loss: 0.059210408478975296\n",
      "Epoch 4113: train loss: 0.021358635276556015, val loss: 0.09078270941972733\n",
      "Epoch 4114: train loss: 0.02193968929350376, val loss: 0.06305529922246933\n",
      "Epoch 4115: train loss: 0.03133342042565346, val loss: 0.117472343146801\n",
      "Epoch 4116: train loss: 0.03409523144364357, val loss: 0.11557669937610626\n",
      "Epoch 4117: train loss: 0.026033787056803703, val loss: 0.0986107662320137\n",
      "Epoch 4118: train loss: 0.029513727873563766, val loss: 0.07033856958150864\n",
      "Epoch 4119: train loss: 0.02576437033712864, val loss: 0.09962781518697739\n",
      "Epoch 4120: train loss: 0.02880280837416649, val loss: 0.08306878805160522\n",
      "Epoch 4121: train loss: 0.02499670535326004, val loss: 0.05637871101498604\n",
      "Epoch 4122: train loss: 0.02711312659084797, val loss: 0.10275886207818985\n",
      "Epoch 4123: train loss: 0.020528553053736687, val loss: 0.06741251051425934\n",
      "Epoch 4124: train loss: 0.034364551305770874, val loss: 0.08037035912275314\n",
      "Epoch 4125: train loss: 0.025947099551558495, val loss: 0.09177243709564209\n",
      "Epoch 4126: train loss: 0.02661953866481781, val loss: 0.05484582111239433\n",
      "Epoch 4127: train loss: 0.02020099386572838, val loss: 0.07869463413953781\n",
      "Epoch 4128: train loss: 0.01993735134601593, val loss: 0.08572691679000854\n",
      "Epoch 4129: train loss: 0.026315441355109215, val loss: 0.08050511032342911\n",
      "Epoch 4130: train loss: 0.02766941487789154, val loss: 0.1424969583749771\n",
      "Epoch 4131: train loss: 0.024760795757174492, val loss: 0.10718929767608643\n",
      "Epoch 4132: train loss: 0.02027028240263462, val loss: 0.08888000249862671\n",
      "Epoch 4133: train loss: 0.025087745860219002, val loss: 0.09647766500711441\n",
      "Epoch 4134: train loss: 0.025852005928754807, val loss: 0.1047271117568016\n",
      "Epoch 4135: train loss: 0.027135666459798813, val loss: 0.08207222074270248\n",
      "Epoch 4136: train loss: 0.026177015155553818, val loss: 0.11086113750934601\n",
      "Epoch 4137: train loss: 0.02858012355864048, val loss: 0.05953913554549217\n",
      "Epoch 4138: train loss: 0.022699274122714996, val loss: 0.11734973639249802\n",
      "Epoch 4139: train loss: 0.04059932380914688, val loss: 0.08281259983778\n",
      "Epoch 4140: train loss: 0.026795249432325363, val loss: 0.0768410861492157\n",
      "Epoch 4141: train loss: 0.02820238284766674, val loss: 0.05830452963709831\n",
      "Epoch 4142: train loss: 0.02764587476849556, val loss: 0.09472383558750153\n",
      "Epoch 4143: train loss: 0.02703862264752388, val loss: 0.0660305842757225\n",
      "Epoch 4144: train loss: 0.024134673178195953, val loss: 0.08734557777643204\n",
      "Epoch 4145: train loss: 0.02638273313641548, val loss: 0.07186379283666611\n",
      "Epoch 4146: train loss: 0.02533997967839241, val loss: 0.10316045582294464\n",
      "Epoch 4147: train loss: 0.018146805465221405, val loss: 0.07931945472955704\n",
      "Epoch 4148: train loss: 0.020972587168216705, val loss: 0.11311361938714981\n",
      "Epoch 4149: train loss: 0.02741633541882038, val loss: 0.08472912758588791\n",
      "Epoch 4150: train loss: 0.02532842941582203, val loss: 0.07433094829320908\n",
      "Epoch 4151: train loss: 0.02644968591630459, val loss: 0.09215044230222702\n",
      "Epoch 4152: train loss: 0.03335146605968475, val loss: 0.10517263412475586\n",
      "Epoch 4153: train loss: 0.020438164472579956, val loss: 0.10777552425861359\n",
      "Epoch 4154: train loss: 0.027195829898118973, val loss: 0.0743865966796875\n",
      "Epoch 4155: train loss: 0.022536102682352066, val loss: 0.08689466118812561\n",
      "Epoch 4156: train loss: 0.027752015739679337, val loss: 0.0716792494058609\n",
      "Epoch 4157: train loss: 0.02594117633998394, val loss: 0.07928520441055298\n",
      "Epoch 4158: train loss: 0.019873816519975662, val loss: 0.06569766253232956\n",
      "Epoch 4159: train loss: 0.02113926224410534, val loss: 0.06773445755243301\n",
      "Epoch 4160: train loss: 0.022319220006465912, val loss: 0.07984233647584915\n",
      "Epoch 4161: train loss: 0.02365528605878353, val loss: 0.09922908991575241\n",
      "Epoch 4162: train loss: 0.01800142601132393, val loss: 0.11567485332489014\n",
      "Epoch 4163: train loss: 0.024316204711794853, val loss: 0.08195128291845322\n",
      "Epoch 4164: train loss: 0.02895229123532772, val loss: 0.0972377136349678\n",
      "Epoch 4165: train loss: 0.02369692176580429, val loss: 0.05590986832976341\n",
      "Epoch 4166: train loss: 0.02838672511279583, val loss: 0.07150787115097046\n",
      "Epoch 4167: train loss: 0.03032725676894188, val loss: 0.0773845836520195\n",
      "Epoch 4168: train loss: 0.03021451085805893, val loss: 0.07121419906616211\n",
      "Epoch 4169: train loss: 0.03555721417069435, val loss: 0.1072114035487175\n",
      "Epoch 4170: train loss: 0.02278837189078331, val loss: 0.03819051384925842\n",
      "Epoch 4171: train loss: 0.025222470983862877, val loss: 0.08801624923944473\n",
      "Epoch 4172: train loss: 0.0200753565877676, val loss: 0.07521384209394455\n",
      "Epoch 4173: train loss: 0.029208384454250336, val loss: 0.06817135214805603\n",
      "Epoch 4174: train loss: 0.02023198828101158, val loss: 0.1247897669672966\n",
      "Epoch 4175: train loss: 0.029854601249098778, val loss: 0.12220682948827744\n",
      "Epoch 4176: train loss: 0.029503172263503075, val loss: 0.10914278030395508\n",
      "Epoch 4177: train loss: 0.026681717485189438, val loss: 0.1017310619354248\n",
      "Epoch 4178: train loss: 0.02268085815012455, val loss: 0.08105335384607315\n",
      "Epoch 4179: train loss: 0.03371536731719971, val loss: 0.09967350959777832\n",
      "Epoch 4180: train loss: 0.024800708517432213, val loss: 0.07635479420423508\n",
      "Epoch 4181: train loss: 0.03211503103375435, val loss: 0.07576185464859009\n",
      "Epoch 4182: train loss: 0.017198961228132248, val loss: 0.08197588473558426\n",
      "Epoch 4183: train loss: 0.022470925003290176, val loss: 0.11674509197473526\n",
      "Epoch 4184: train loss: 0.02863161452114582, val loss: 0.07717627286911011\n",
      "Epoch 4185: train loss: 0.03343886509537697, val loss: 0.0920124351978302\n",
      "Epoch 4186: train loss: 0.020225727930665016, val loss: 0.10453350841999054\n",
      "Epoch 4187: train loss: 0.02341408282518387, val loss: 0.05616460368037224\n",
      "Epoch 4188: train loss: 0.021797992289066315, val loss: 0.07442709058523178\n",
      "Epoch 4189: train loss: 0.02753203734755516, val loss: 0.0327315479516983\n",
      "Epoch 4190: train loss: 0.027589835226535797, val loss: 0.09772864729166031\n",
      "Epoch 4191: train loss: 0.028371578082442284, val loss: 0.14077915251255035\n",
      "Epoch 4192: train loss: 0.030648989602923393, val loss: 0.08257059752941132\n",
      "Epoch 4193: train loss: 0.022242644801735878, val loss: 0.07419323921203613\n",
      "Epoch 4194: train loss: 0.020124156028032303, val loss: 0.0693042054772377\n",
      "Epoch 4195: train loss: 0.024069897830486298, val loss: 0.07087245583534241\n",
      "Epoch 4196: train loss: 0.028821345418691635, val loss: 0.06551983952522278\n",
      "Epoch 4197: train loss: 0.023884471505880356, val loss: 0.06538237631320953\n",
      "Epoch 4198: train loss: 0.030007973313331604, val loss: 0.0385381244122982\n",
      "Epoch 4199: train loss: 0.020001264289021492, val loss: 0.06287262588739395\n",
      "Epoch 4200: train loss: 0.024371806532144547, val loss: 0.11264242231845856\n",
      "Epoch 4201: train loss: 0.019040437415242195, val loss: 0.12806110084056854\n",
      "Epoch 4202: train loss: 0.02131776325404644, val loss: 0.06299632042646408\n",
      "Epoch 4203: train loss: 0.027775632217526436, val loss: 0.0734282061457634\n",
      "Epoch 4204: train loss: 0.02048674039542675, val loss: 0.07471241056919098\n",
      "Epoch 4205: train loss: 0.026249555870890617, val loss: 0.06375714391469955\n",
      "Epoch 4206: train loss: 0.02695854753255844, val loss: 0.07743435353040695\n",
      "Epoch 4207: train loss: 0.022318124771118164, val loss: 0.05128449946641922\n",
      "Epoch 4208: train loss: 0.029774650931358337, val loss: 0.07774458825588226\n",
      "Epoch 4209: train loss: 0.020361803472042084, val loss: 0.08821920305490494\n",
      "Epoch 4210: train loss: 0.018732624128460884, val loss: 0.08722072839736938\n",
      "Epoch 4211: train loss: 0.02436436153948307, val loss: 0.0799102708697319\n",
      "Epoch 4212: train loss: 0.01697867549955845, val loss: 0.0638333186507225\n",
      "Epoch 4213: train loss: 0.021741127595305443, val loss: 0.07801377773284912\n",
      "Epoch 4214: train loss: 0.029307782649993896, val loss: 0.04189480468630791\n",
      "Epoch 4215: train loss: 0.02387094311416149, val loss: 0.10092148929834366\n",
      "Epoch 4216: train loss: 0.02724555879831314, val loss: 0.061289962381124496\n",
      "Epoch 4217: train loss: 0.022183887660503387, val loss: 0.08984734863042831\n",
      "Epoch 4218: train loss: 0.01857270672917366, val loss: 0.06669237464666367\n",
      "Epoch 4219: train loss: 0.025131449103355408, val loss: 0.0776527151465416\n",
      "Epoch 4220: train loss: 0.025097951292991638, val loss: 0.08774616569280624\n",
      "Epoch 4221: train loss: 0.027684286236763, val loss: 0.06979922205209732\n",
      "Epoch 4222: train loss: 0.028519928455352783, val loss: 0.06148306280374527\n",
      "Epoch 4223: train loss: 0.024167051538825035, val loss: 0.04009702429175377\n",
      "Epoch 4224: train loss: 0.03144577518105507, val loss: 0.1029360294342041\n",
      "Epoch 4225: train loss: 0.02652711048722267, val loss: 0.07439276576042175\n",
      "Epoch 4226: train loss: 0.021526506170630455, val loss: 0.07370021194219589\n",
      "Epoch 4227: train loss: 0.028913695365190506, val loss: 0.06483163684606552\n",
      "Epoch 4228: train loss: 0.030516445636749268, val loss: 0.07756059616804123\n",
      "Epoch 4229: train loss: 0.02826356515288353, val loss: 0.07642992585897446\n",
      "Epoch 4230: train loss: 0.029074640944600105, val loss: 0.08050751686096191\n",
      "Epoch 4231: train loss: 0.03344661369919777, val loss: 0.08500733226537704\n",
      "Epoch 4232: train loss: 0.01974264532327652, val loss: 0.06210262328386307\n",
      "Epoch 4233: train loss: 0.023093562573194504, val loss: 0.11352745443582535\n",
      "Epoch 4234: train loss: 0.025343623012304306, val loss: 0.07246870547533035\n",
      "Epoch 4235: train loss: 0.018058309331536293, val loss: 0.052229732275009155\n",
      "Epoch 4236: train loss: 0.017970263957977295, val loss: 0.12146353721618652\n",
      "Epoch 4237: train loss: 0.018414560705423355, val loss: 0.05474315211176872\n",
      "Epoch 4238: train loss: 0.02256321720778942, val loss: 0.10057683289051056\n",
      "Epoch 4239: train loss: 0.02767709642648697, val loss: 0.08976307511329651\n",
      "Epoch 4240: train loss: 0.023484280332922935, val loss: 0.04682603105902672\n",
      "Epoch 4241: train loss: 0.02341855876147747, val loss: 0.07071477174758911\n",
      "Epoch 4242: train loss: 0.026508808135986328, val loss: 0.07489697635173798\n",
      "Epoch 4243: train loss: 0.02488357201218605, val loss: 0.08111216872930527\n",
      "Epoch 4244: train loss: 0.021684469655156136, val loss: 0.037046920508146286\n",
      "Epoch 4245: train loss: 0.027048900723457336, val loss: 0.12915180623531342\n",
      "Epoch 4246: train loss: 0.023079074919223785, val loss: 0.10490170866250992\n",
      "Epoch 4247: train loss: 0.031192293390631676, val loss: 0.05964697524905205\n",
      "Epoch 4248: train loss: 0.023737384006381035, val loss: 0.06583408266305923\n",
      "Epoch 4249: train loss: 0.022231563925743103, val loss: 0.06030509993433952\n",
      "Epoch 4250: train loss: 0.016000783070921898, val loss: 0.08970776945352554\n",
      "Epoch 4251: train loss: 0.03420376777648926, val loss: 0.08676896244287491\n",
      "Epoch 4252: train loss: 0.0216118972748518, val loss: 0.06021115928888321\n",
      "Epoch 4253: train loss: 0.022035522386431694, val loss: 0.07612311840057373\n",
      "Epoch 4254: train loss: 0.019167467951774597, val loss: 0.0798717513680458\n",
      "Epoch 4255: train loss: 0.026994159445166588, val loss: 0.0637793242931366\n",
      "Epoch 4256: train loss: 0.023230841383337975, val loss: 0.07906664907932281\n",
      "Epoch 4257: train loss: 0.024557888507843018, val loss: 0.06577237695455551\n",
      "Epoch 4258: train loss: 0.018397079780697823, val loss: 0.06543614715337753\n",
      "Epoch 4259: train loss: 0.024134445935487747, val loss: 0.06558430194854736\n",
      "Epoch 4260: train loss: 0.027494624257087708, val loss: 0.08846817165613174\n",
      "Epoch 4261: train loss: 0.020385434851050377, val loss: 0.0950002670288086\n",
      "Epoch 4262: train loss: 0.01942240260541439, val loss: 0.11875226348638535\n",
      "Epoch 4263: train loss: 0.02170405164361, val loss: 0.10664445161819458\n",
      "Epoch 4264: train loss: 0.025570912286639214, val loss: 0.07177650183439255\n",
      "Epoch 4265: train loss: 0.02753283828496933, val loss: 0.04898935556411743\n",
      "Epoch 4266: train loss: 0.01974746584892273, val loss: 0.0668359026312828\n",
      "Epoch 4267: train loss: 0.026060597971081734, val loss: 0.08624390512704849\n",
      "Epoch 4268: train loss: 0.014771761372685432, val loss: 0.10877863317728043\n",
      "Epoch 4269: train loss: 0.021060651168227196, val loss: 0.11160843819379807\n",
      "Epoch 4270: train loss: 0.027927648276090622, val loss: 0.06219374015927315\n",
      "Epoch 4271: train loss: 0.03296614810824394, val loss: 0.07777372002601624\n",
      "Epoch 4272: train loss: 0.017232662066817284, val loss: 0.09062420576810837\n",
      "Epoch 4273: train loss: 0.023767895996570587, val loss: 0.08649062365293503\n",
      "Epoch 4274: train loss: 0.02855607680976391, val loss: 0.09151726961135864\n",
      "Epoch 4275: train loss: 0.027653848752379417, val loss: 0.06705529987812042\n",
      "Epoch 4276: train loss: 0.022865047678351402, val loss: 0.057140566408634186\n",
      "Epoch 4277: train loss: 0.041047126054763794, val loss: 0.08384396880865097\n",
      "Epoch 4278: train loss: 0.02311883121728897, val loss: 0.08195184916257858\n",
      "Epoch 4279: train loss: 0.030316036194562912, val loss: 0.09038207679986954\n",
      "Epoch 4280: train loss: 0.028351610526442528, val loss: 0.061531245708465576\n",
      "Epoch 4281: train loss: 0.02051686681807041, val loss: 0.09838434308767319\n",
      "Epoch 4282: train loss: 0.025739744305610657, val loss: 0.07253533601760864\n",
      "Epoch 4283: train loss: 0.016866331920027733, val loss: 0.06200474128127098\n",
      "Epoch 4284: train loss: 0.01905875653028488, val loss: 0.09959596395492554\n",
      "Epoch 4285: train loss: 0.02218337543308735, val loss: 0.09568005055189133\n",
      "Epoch 4286: train loss: 0.021081365644931793, val loss: 0.0430266447365284\n",
      "Epoch 4287: train loss: 0.025905024260282516, val loss: 0.04568888619542122\n",
      "Epoch 4288: train loss: 0.023823818191885948, val loss: 0.06787294149398804\n",
      "Epoch 4289: train loss: 0.02061171643435955, val loss: 0.06268662959337234\n",
      "Epoch 4290: train loss: 0.015491541475057602, val loss: 0.09944405406713486\n",
      "Epoch 4291: train loss: 0.019264742732048035, val loss: 0.05843006446957588\n",
      "Epoch 4292: train loss: 0.020169507712125778, val loss: 0.06982692331075668\n",
      "Epoch 4293: train loss: 0.023544929921627045, val loss: 0.04725639522075653\n",
      "Epoch 4294: train loss: 0.023177623748779297, val loss: 0.08733867853879929\n",
      "Epoch 4295: train loss: 0.024706147611141205, val loss: 0.1251550167798996\n",
      "Epoch 4296: train loss: 0.02542036399245262, val loss: 0.12719464302062988\n",
      "Epoch 4297: train loss: 0.021171648055315018, val loss: 0.06630926579236984\n",
      "Epoch 4298: train loss: 0.020151125267148018, val loss: 0.05590313300490379\n",
      "Epoch 4299: train loss: 0.01925470307469368, val loss: 0.05411764979362488\n",
      "Epoch 4300: train loss: 0.018330302089452744, val loss: 0.05235208198428154\n",
      "Epoch 4301: train loss: 0.015140093863010406, val loss: 0.12376832962036133\n",
      "Epoch 4302: train loss: 0.022489482536911964, val loss: 0.12299678474664688\n",
      "Epoch 4303: train loss: 0.028802568092942238, val loss: 0.08184518665075302\n",
      "Epoch 4304: train loss: 0.022215090692043304, val loss: 0.06193787604570389\n",
      "Epoch 4305: train loss: 0.021214690059423447, val loss: 0.06997504830360413\n",
      "Epoch 4306: train loss: 0.017116637900471687, val loss: 0.07396921515464783\n",
      "Epoch 4307: train loss: 0.030327796936035156, val loss: 0.05550020933151245\n",
      "Epoch 4308: train loss: 0.023317743092775345, val loss: 0.09184407442808151\n",
      "Epoch 4309: train loss: 0.01766306906938553, val loss: 0.08914078772068024\n",
      "Epoch 4310: train loss: 0.01556228194385767, val loss: 0.09739159792661667\n",
      "Epoch 4311: train loss: 0.02308392897248268, val loss: 0.07126074284315109\n",
      "Epoch 4312: train loss: 0.01663755066692829, val loss: 0.10604309290647507\n",
      "Epoch 4313: train loss: 0.02467447519302368, val loss: 0.08622627705335617\n",
      "Epoch 4314: train loss: 0.027619097381830215, val loss: 0.07306274026632309\n",
      "Epoch 4315: train loss: 0.02218669280409813, val loss: 0.07094894349575043\n",
      "Epoch 4316: train loss: 0.032861799001693726, val loss: 0.048594653606414795\n",
      "Epoch 4317: train loss: 0.018932336941361427, val loss: 0.09860458225011826\n",
      "Epoch 4318: train loss: 0.0222647562623024, val loss: 0.10427006334066391\n",
      "Epoch 4319: train loss: 0.02086775004863739, val loss: 0.1110544204711914\n",
      "Epoch 4320: train loss: 0.022989491000771523, val loss: 0.10331644117832184\n",
      "Epoch 4321: train loss: 0.023667987436056137, val loss: 0.09968548268079758\n",
      "Epoch 4322: train loss: 0.027122408151626587, val loss: 0.11436472088098526\n",
      "Epoch 4323: train loss: 0.02955441363155842, val loss: 0.04988924786448479\n",
      "Epoch 4324: train loss: 0.018093721941113472, val loss: 0.07775528728961945\n",
      "Epoch 4325: train loss: 0.033873654901981354, val loss: 0.050067316740751266\n",
      "Epoch 4326: train loss: 0.019684594124555588, val loss: 0.06102041155099869\n",
      "Epoch 4327: train loss: 0.016352206468582153, val loss: 0.09190114587545395\n",
      "Epoch 4328: train loss: 0.02328740991652012, val loss: 0.07560396194458008\n",
      "Epoch 4329: train loss: 0.019126590341329575, val loss: 0.03948172554373741\n",
      "Epoch 4330: train loss: 0.025597261264920235, val loss: 0.09847500175237656\n",
      "Epoch 4331: train loss: 0.025944339111447334, val loss: 0.0878835991024971\n",
      "Epoch 4332: train loss: 0.019244832918047905, val loss: 0.05588141083717346\n",
      "Epoch 4333: train loss: 0.018526580184698105, val loss: 0.08671822398900986\n",
      "Epoch 4334: train loss: 0.025605225935578346, val loss: 0.05963922664523125\n",
      "Epoch 4335: train loss: 0.023266205564141273, val loss: 0.048418063670396805\n",
      "Epoch 4336: train loss: 0.02054017223417759, val loss: 0.12102261930704117\n",
      "Epoch 4337: train loss: 0.025388946756720543, val loss: 0.10096284002065659\n",
      "Epoch 4338: train loss: 0.022439781576395035, val loss: 0.13333569467067719\n",
      "Epoch 4339: train loss: 0.034129973500967026, val loss: 0.09468995779752731\n",
      "Epoch 4340: train loss: 0.024522390216588974, val loss: 0.048878204077482224\n",
      "Epoch 4341: train loss: 0.02128870040178299, val loss: 0.03775414451956749\n",
      "Epoch 4342: train loss: 0.034518130123615265, val loss: 0.0816727802157402\n",
      "Epoch 4343: train loss: 0.03805007413029671, val loss: 0.06918764114379883\n",
      "Epoch 4344: train loss: 0.02533683553338051, val loss: 0.049455005675554276\n",
      "Epoch 4345: train loss: 0.029245538637042046, val loss: 0.09383740276098251\n",
      "Epoch 4346: train loss: 0.02926749736070633, val loss: 0.08771520107984543\n",
      "Epoch 4347: train loss: 0.02923794463276863, val loss: 0.06305443495512009\n",
      "Epoch 4348: train loss: 0.01647428795695305, val loss: 0.07183273881673813\n",
      "Epoch 4349: train loss: 0.027060477063059807, val loss: 0.10985920578241348\n",
      "Epoch 4350: train loss: 0.0213762316852808, val loss: 0.10929926484823227\n",
      "Epoch 4351: train loss: 0.02935102768242359, val loss: 0.0779537782073021\n",
      "Epoch 4352: train loss: 0.029531346634030342, val loss: 0.09358658641576767\n",
      "Epoch 4353: train loss: 0.023444555699825287, val loss: 0.0986034944653511\n",
      "Epoch 4354: train loss: 0.018669109791517258, val loss: 0.0809754878282547\n",
      "Epoch 4355: train loss: 0.01681520976126194, val loss: 0.05391199514269829\n",
      "Epoch 4356: train loss: 0.022547217085957527, val loss: 0.053129713982343674\n",
      "Epoch 4357: train loss: 0.02758369781076908, val loss: 0.11618101596832275\n",
      "Epoch 4358: train loss: 0.019984010607004166, val loss: 0.06363116204738617\n",
      "Epoch 4359: train loss: 0.020362189039587975, val loss: 0.092038094997406\n",
      "Epoch 4360: train loss: 0.02464771829545498, val loss: 0.08982591331005096\n",
      "Epoch 4361: train loss: 0.026243533939123154, val loss: 0.06719910353422165\n",
      "Epoch 4362: train loss: 0.029222965240478516, val loss: 0.0802571102976799\n",
      "Epoch 4363: train loss: 0.020856434479355812, val loss: 0.06852772831916809\n",
      "Epoch 4364: train loss: 0.022314051166176796, val loss: 0.06380009651184082\n",
      "Epoch 4365: train loss: 0.027257194742560387, val loss: 0.08914532512426376\n",
      "Epoch 4366: train loss: 0.023488737642765045, val loss: 0.08008347451686859\n",
      "Epoch 4367: train loss: 0.025531291961669922, val loss: 0.08577456325292587\n",
      "Epoch 4368: train loss: 0.01803659461438656, val loss: 0.12215624004602432\n",
      "Epoch 4369: train loss: 0.021156419068574905, val loss: 0.02920757234096527\n",
      "Epoch 4370: train loss: 0.026741160079836845, val loss: 0.0637417808175087\n",
      "Epoch 4371: train loss: 0.025851793587207794, val loss: 0.07304636389017105\n",
      "Epoch 4372: train loss: 0.0315060131251812, val loss: 0.09016022086143494\n",
      "Epoch 4373: train loss: 0.02137898840010166, val loss: 0.12173169106245041\n",
      "Epoch 4374: train loss: 0.022476747632026672, val loss: 0.07681024819612503\n",
      "Epoch 4375: train loss: 0.020937666296958923, val loss: 0.060219503939151764\n",
      "Epoch 4376: train loss: 0.02969793789088726, val loss: 0.05867823585867882\n",
      "Epoch 4377: train loss: 0.024796446785330772, val loss: 0.12029349058866501\n",
      "Epoch 4378: train loss: 0.02765515446662903, val loss: 0.054527487605810165\n",
      "Epoch 4379: train loss: 0.02422645501792431, val loss: 0.07241761684417725\n",
      "Epoch 4380: train loss: 0.027467159554362297, val loss: 0.03623789921402931\n",
      "Epoch 4381: train loss: 0.029499288648366928, val loss: 0.07184165716171265\n",
      "Epoch 4382: train loss: 0.024464061483740807, val loss: 0.08323230594396591\n",
      "Epoch 4383: train loss: 0.023397568613290787, val loss: 0.07369942963123322\n",
      "Epoch 4384: train loss: 0.023641616106033325, val loss: 0.08324030041694641\n",
      "Epoch 4385: train loss: 0.02876298688352108, val loss: 0.1021982803940773\n",
      "Epoch 4386: train loss: 0.022612599655985832, val loss: 0.061571717262268066\n",
      "Epoch 4387: train loss: 0.023523623123764992, val loss: 0.09496940672397614\n",
      "Epoch 4388: train loss: 0.02022651955485344, val loss: 0.03002406656742096\n",
      "Epoch 4389: train loss: 0.022390268743038177, val loss: 0.04346824809908867\n",
      "Epoch 4390: train loss: 0.02216416597366333, val loss: 0.11413177102804184\n",
      "Epoch 4391: train loss: 0.01760280318558216, val loss: 0.06521152704954147\n",
      "Epoch 4392: train loss: 0.03046991117298603, val loss: 0.07319055497646332\n",
      "Epoch 4393: train loss: 0.02054857648909092, val loss: 0.07109633833169937\n",
      "Epoch 4394: train loss: 0.027446916326880455, val loss: 0.07661271095275879\n",
      "Epoch 4395: train loss: 0.023167753592133522, val loss: 0.08214899897575378\n",
      "Epoch 4396: train loss: 0.023374712094664574, val loss: 0.0386580191552639\n",
      "Epoch 4397: train loss: 0.02699732780456543, val loss: 0.1016606017947197\n",
      "Epoch 4398: train loss: 0.02448158524930477, val loss: 0.07140611857175827\n",
      "Epoch 4399: train loss: 0.01946510188281536, val loss: 0.043206531554460526\n",
      "Epoch 4400: train loss: 0.030766021460294724, val loss: 0.07972247153520584\n",
      "Epoch 4401: train loss: 0.027634525671601295, val loss: 0.10710334777832031\n",
      "Epoch 4402: train loss: 0.027005620300769806, val loss: 0.09309672564268112\n",
      "Epoch 4403: train loss: 0.024340447038412094, val loss: 0.08969194442033768\n",
      "Epoch 4404: train loss: 0.02275962382555008, val loss: 0.05497456341981888\n",
      "Epoch 4405: train loss: 0.019583698362112045, val loss: 0.06952018290758133\n",
      "Epoch 4406: train loss: 0.01850452646613121, val loss: 0.03839307650923729\n",
      "Epoch 4407: train loss: 0.019447775557637215, val loss: 0.07783148437738419\n",
      "Epoch 4408: train loss: 0.022059766575694084, val loss: 0.08057218044996262\n",
      "Epoch 4409: train loss: 0.01754952222108841, val loss: 0.047212209552526474\n",
      "Epoch 4410: train loss: 0.025295455008745193, val loss: 0.04590930789709091\n",
      "Epoch 4411: train loss: 0.015666792169213295, val loss: 0.0667930319905281\n",
      "Epoch 4412: train loss: 0.024314040318131447, val loss: 0.052678145468235016\n",
      "Epoch 4413: train loss: 0.019435260444879532, val loss: 0.072153739631176\n",
      "Epoch 4414: train loss: 0.025631621479988098, val loss: 0.06739728897809982\n",
      "Epoch 4415: train loss: 0.02918214350938797, val loss: 0.0934075340628624\n",
      "Epoch 4416: train loss: 0.02222452126443386, val loss: 0.07704418152570724\n",
      "Epoch 4417: train loss: 0.02253495715558529, val loss: 0.08282147347927094\n",
      "Epoch 4418: train loss: 0.02339743822813034, val loss: 0.09121198952198029\n",
      "Epoch 4419: train loss: 0.023143630474805832, val loss: 0.12175589054822922\n",
      "Epoch 4420: train loss: 0.022960498929023743, val loss: 0.07969756424427032\n",
      "Epoch 4421: train loss: 0.02614881843328476, val loss: 0.07518519461154938\n",
      "Epoch 4422: train loss: 0.022783566266298294, val loss: 0.07589919120073318\n",
      "Epoch 4423: train loss: 0.02398490719497204, val loss: 0.07684949785470963\n",
      "Epoch 4424: train loss: 0.024825138971209526, val loss: 0.089322030544281\n",
      "Epoch 4425: train loss: 0.02705780230462551, val loss: 0.09821251779794693\n",
      "Epoch 4426: train loss: 0.023545021191239357, val loss: 0.0626317486166954\n",
      "Epoch 4427: train loss: 0.01810775324702263, val loss: 0.07455054670572281\n",
      "Epoch 4428: train loss: 0.026241321116685867, val loss: 0.06992249190807343\n",
      "Epoch 4429: train loss: 0.02653619274497032, val loss: 0.1176251545548439\n",
      "Epoch 4430: train loss: 0.025968758389353752, val loss: 0.06132112815976143\n",
      "Epoch 4431: train loss: 0.022260401397943497, val loss: 0.07667361944913864\n",
      "Epoch 4432: train loss: 0.02856467291712761, val loss: 0.1053764820098877\n",
      "Epoch 4433: train loss: 0.025609644129872322, val loss: 0.0525272861123085\n",
      "Epoch 4434: train loss: 0.022764937952160835, val loss: 0.049036890268325806\n",
      "Epoch 4435: train loss: 0.0230589397251606, val loss: 0.13096700608730316\n",
      "Epoch 4436: train loss: 0.022175034508109093, val loss: 0.1408371776342392\n",
      "Epoch 4437: train loss: 0.016687516123056412, val loss: 0.10543198883533478\n",
      "Epoch 4438: train loss: 0.02489672228693962, val loss: 0.08536305278539658\n",
      "Epoch 4439: train loss: 0.030699005350470543, val loss: 0.062406182289123535\n",
      "Epoch 4440: train loss: 0.017178133130073547, val loss: 0.07318675518035889\n",
      "Epoch 4441: train loss: 0.021528111770749092, val loss: 0.11197109520435333\n",
      "Epoch 4442: train loss: 0.02606436423957348, val loss: 0.05372139438986778\n",
      "Epoch 4443: train loss: 0.025936327874660492, val loss: 0.0661671981215477\n",
      "Epoch 4444: train loss: 0.0341339148581028, val loss: 0.07354360818862915\n",
      "Epoch 4445: train loss: 0.021444546058773994, val loss: 0.10825580358505249\n",
      "Epoch 4446: train loss: 0.02175837568938732, val loss: 0.0937706008553505\n",
      "Epoch 4447: train loss: 0.02595386654138565, val loss: 0.0750226229429245\n",
      "Epoch 4448: train loss: 0.01861991360783577, val loss: 0.05493795871734619\n",
      "Epoch 4449: train loss: 0.024180205538868904, val loss: 0.0886959359049797\n",
      "Epoch 4450: train loss: 0.016977161169052124, val loss: 0.06843013316392899\n",
      "Epoch 4451: train loss: 0.023815324530005455, val loss: 0.08239026367664337\n",
      "Epoch 4452: train loss: 0.019901450723409653, val loss: 0.05394117161631584\n",
      "Epoch 4453: train loss: 0.021032582968473434, val loss: 0.06430930644273758\n",
      "Epoch 4454: train loss: 0.02332240529358387, val loss: 0.0522574782371521\n",
      "Epoch 4455: train loss: 0.018380213528871536, val loss: 0.058301474899053574\n",
      "Epoch 4456: train loss: 0.027983009815216064, val loss: 0.09627971053123474\n",
      "Epoch 4457: train loss: 0.0189172625541687, val loss: 0.07571446150541306\n",
      "Epoch 4458: train loss: 0.02700473554432392, val loss: 0.10555221885442734\n",
      "Epoch 4459: train loss: 0.019188106060028076, val loss: 0.10475771874189377\n",
      "Epoch 4460: train loss: 0.01757182739675045, val loss: 0.05671965703368187\n",
      "Epoch 4461: train loss: 0.02455424703657627, val loss: 0.06501124054193497\n",
      "Epoch 4462: train loss: 0.029416747391223907, val loss: 0.08520586788654327\n",
      "Epoch 4463: train loss: 0.01792135275900364, val loss: 0.079642154276371\n",
      "Epoch 4464: train loss: 0.016441870480775833, val loss: 0.04735928401350975\n",
      "Epoch 4465: train loss: 0.02248683013021946, val loss: 0.07770346850156784\n",
      "Epoch 4466: train loss: 0.02280014380812645, val loss: 0.04552143067121506\n",
      "Epoch 4467: train loss: 0.020382730290293694, val loss: 0.09633687883615494\n",
      "Epoch 4468: train loss: 0.022830398753285408, val loss: 0.1135798916220665\n",
      "Epoch 4469: train loss: 0.018323561176657677, val loss: 0.12274523079395294\n",
      "Epoch 4470: train loss: 0.015137220732867718, val loss: 0.0455225370824337\n",
      "Epoch 4471: train loss: 0.01751626282930374, val loss: 0.05770445987582207\n",
      "Epoch 4472: train loss: 0.026008665561676025, val loss: 0.09009958058595657\n",
      "Epoch 4473: train loss: 0.019747156649827957, val loss: 0.055596619844436646\n",
      "Epoch 4474: train loss: 0.03179016336798668, val loss: 0.08346221596002579\n",
      "Epoch 4475: train loss: 0.021538330242037773, val loss: 0.12641821801662445\n",
      "Epoch 4476: train loss: 0.028271447867155075, val loss: 0.09428435564041138\n",
      "Epoch 4477: train loss: 0.022763671353459358, val loss: 0.0855436846613884\n",
      "Epoch 4478: train loss: 0.018674436956644058, val loss: 0.08216091245412827\n",
      "Epoch 4479: train loss: 0.027410542592406273, val loss: 0.09133600443601608\n",
      "Epoch 4480: train loss: 0.025564230978488922, val loss: 0.07501854747533798\n",
      "Epoch 4481: train loss: 0.019616497680544853, val loss: 0.09950108826160431\n",
      "Epoch 4482: train loss: 0.016266491264104843, val loss: 0.10730176419019699\n",
      "Epoch 4483: train loss: 0.017188483849167824, val loss: 0.11874629557132721\n",
      "Epoch 4484: train loss: 0.02625359408557415, val loss: 0.10500001162290573\n",
      "Epoch 4485: train loss: 0.02157941274344921, val loss: 0.06258558481931686\n",
      "Epoch 4486: train loss: 0.018529441207647324, val loss: 0.08565904945135117\n",
      "Epoch 4487: train loss: 0.023499542847275734, val loss: 0.057932283729314804\n",
      "Epoch 4488: train loss: 0.027858581393957138, val loss: 0.08730174601078033\n",
      "Epoch 4489: train loss: 0.02368171513080597, val loss: 0.07219851016998291\n",
      "Epoch 4490: train loss: 0.023379366844892502, val loss: 0.08847715705633163\n",
      "Epoch 4491: train loss: 0.023332389071583748, val loss: 0.08031255751848221\n",
      "Epoch 4492: train loss: 0.024220025166869164, val loss: 0.049607183784246445\n",
      "Epoch 4493: train loss: 0.01623549684882164, val loss: 0.11774864047765732\n",
      "Epoch 4494: train loss: 0.01623254083096981, val loss: 0.1209627315402031\n",
      "Epoch 4495: train loss: 0.025191670283675194, val loss: 0.039703063666820526\n",
      "Epoch 4496: train loss: 0.019983747974038124, val loss: 0.060693200677633286\n",
      "Epoch 4497: train loss: 0.021021835505962372, val loss: 0.07369574159383774\n",
      "Epoch 4498: train loss: 0.015475193969905376, val loss: 0.08060021698474884\n",
      "Epoch 4499: train loss: 0.021885225549340248, val loss: 0.10187631100416183\n",
      "Epoch 4500: train loss: 0.020662959665060043, val loss: 0.06242919713258743\n",
      "Epoch 4501: train loss: 0.025011969730257988, val loss: 0.0704181045293808\n",
      "Epoch 4502: train loss: 0.0232316292822361, val loss: 0.06835262477397919\n",
      "Epoch 4503: train loss: 0.029601825401186943, val loss: 0.08773253113031387\n",
      "Epoch 4504: train loss: 0.026906872168183327, val loss: 0.046134915202856064\n",
      "Epoch 4505: train loss: 0.028314169496297836, val loss: 0.14191173017024994\n",
      "Epoch 4506: train loss: 0.02507520280778408, val loss: 0.07680229097604752\n",
      "Epoch 4507: train loss: 0.023423902690410614, val loss: 0.0743989571928978\n",
      "Epoch 4508: train loss: 0.024943003430962563, val loss: 0.08782487362623215\n",
      "Epoch 4509: train loss: 0.025647364556789398, val loss: 0.10949870198965073\n",
      "Epoch 4510: train loss: 0.02511957474052906, val loss: 0.0724954605102539\n",
      "Epoch 4511: train loss: 0.019701490178704262, val loss: 0.03425920009613037\n",
      "Epoch 4512: train loss: 0.022746140137314796, val loss: 0.1002335473895073\n",
      "Epoch 4513: train loss: 0.01933703012764454, val loss: 0.06243399903178215\n",
      "Epoch 4514: train loss: 0.01838265359401703, val loss: 0.05362565442919731\n",
      "Epoch 4515: train loss: 0.022173969075083733, val loss: 0.060563694685697556\n",
      "Epoch 4516: train loss: 0.021924151107668877, val loss: 0.08818461000919342\n",
      "Epoch 4517: train loss: 0.02298184111714363, val loss: 0.07171594351530075\n",
      "Epoch 4518: train loss: 0.02440585382282734, val loss: 0.0931326374411583\n",
      "Epoch 4519: train loss: 0.020714417099952698, val loss: 0.10182662308216095\n",
      "Epoch 4520: train loss: 0.030806276947259903, val loss: 0.05890246108174324\n",
      "Epoch 4521: train loss: 0.021487761288881302, val loss: 0.08461631834506989\n",
      "Epoch 4522: train loss: 0.028843948617577553, val loss: 0.09750986099243164\n",
      "Epoch 4523: train loss: 0.0213463231921196, val loss: 0.06847729533910751\n",
      "Epoch 4524: train loss: 0.02840862050652504, val loss: 0.1169365644454956\n",
      "Epoch 4525: train loss: 0.02771962620317936, val loss: 0.08670934289693832\n",
      "Epoch 4526: train loss: 0.02134121023118496, val loss: 0.0978972390294075\n",
      "Epoch 4527: train loss: 0.02521076798439026, val loss: 0.039799656718969345\n",
      "Epoch 4528: train loss: 0.02519512176513672, val loss: 0.061088670045137405\n",
      "Epoch 4529: train loss: 0.013975989073514938, val loss: 0.06196211650967598\n",
      "Epoch 4530: train loss: 0.023628665134310722, val loss: 0.10518026351928711\n",
      "Epoch 4531: train loss: 0.02070918120443821, val loss: 0.07264794409275055\n",
      "Epoch 4532: train loss: 0.02335062064230442, val loss: 0.07678482681512833\n",
      "Epoch 4533: train loss: 0.019103556871414185, val loss: 0.11700046062469482\n",
      "Epoch 4534: train loss: 0.021051594987511635, val loss: 0.09652306884527206\n",
      "Epoch 4535: train loss: 0.023123327642679214, val loss: 0.07311172038316727\n",
      "Epoch 4536: train loss: 0.03475300967693329, val loss: 0.013010111637413502\n",
      "Epoch 4537: train loss: 0.028921082615852356, val loss: 0.03245054930448532\n",
      "Epoch 4538: train loss: 0.02232830785214901, val loss: 0.044509317725896835\n",
      "Epoch 4539: train loss: 0.0321645550429821, val loss: 0.037316057831048965\n",
      "Epoch 4540: train loss: 0.025346308946609497, val loss: 0.060434937477111816\n",
      "Epoch 4541: train loss: 0.028514420613646507, val loss: 0.07934264093637466\n",
      "Epoch 4542: train loss: 0.029522938653826714, val loss: 0.07151227444410324\n",
      "Epoch 4543: train loss: 0.03222472965717316, val loss: 0.08473377674818039\n",
      "Epoch 4544: train loss: 0.023783499374985695, val loss: 0.049528103321790695\n",
      "Epoch 4545: train loss: 0.026406526565551758, val loss: 0.0874311551451683\n",
      "Epoch 4546: train loss: 0.03260486572980881, val loss: 0.07590605318546295\n",
      "Epoch 4547: train loss: 0.025265555828809738, val loss: 0.054314833134412766\n",
      "Epoch 4548: train loss: 0.029045412316918373, val loss: 0.11633942276239395\n",
      "Epoch 4549: train loss: 0.024454699829220772, val loss: 0.09287679195404053\n",
      "Epoch 4550: train loss: 0.03529423847794533, val loss: 0.0657060369849205\n",
      "Epoch 4551: train loss: 0.02994532696902752, val loss: 0.08114605396986008\n",
      "Epoch 4552: train loss: 0.028170006349682808, val loss: 0.07569536566734314\n",
      "Epoch 4553: train loss: 0.028034867718815804, val loss: 0.05488030984997749\n",
      "Epoch 4554: train loss: 0.029749033972620964, val loss: 0.041395921260118484\n",
      "Epoch 4555: train loss: 0.0259125716984272, val loss: 0.07497707009315491\n",
      "Epoch 4556: train loss: 0.02493995800614357, val loss: 0.03470047935843468\n",
      "Epoch 4557: train loss: 0.018661197274923325, val loss: 0.06050502136349678\n",
      "Epoch 4558: train loss: 0.02256951294839382, val loss: 0.08063864707946777\n",
      "Epoch 4559: train loss: 0.02453218400478363, val loss: 0.07540277391672134\n",
      "Epoch 4560: train loss: 0.03172720968723297, val loss: 0.03620181977748871\n",
      "Epoch 4561: train loss: 0.02434139885008335, val loss: 0.056257348507642746\n",
      "Epoch 4562: train loss: 0.028498530387878418, val loss: 0.05163437873125076\n",
      "Epoch 4563: train loss: 0.026079121977090836, val loss: 0.04769706353545189\n",
      "Epoch 4564: train loss: 0.018814461305737495, val loss: 0.05622406676411629\n",
      "Epoch 4565: train loss: 0.023528248071670532, val loss: 0.057419098913669586\n",
      "Epoch 4566: train loss: 0.024626372382044792, val loss: 0.08971166610717773\n",
      "Epoch 4567: train loss: 0.02000933326780796, val loss: 0.055616844445466995\n",
      "Epoch 4568: train loss: 0.030045779421925545, val loss: 0.07194914668798447\n",
      "Epoch 4569: train loss: 0.027849338948726654, val loss: 0.04233299940824509\n",
      "Epoch 4570: train loss: 0.02518925815820694, val loss: 0.0803532525897026\n",
      "Epoch 4571: train loss: 0.02031533606350422, val loss: 0.03139885142445564\n",
      "Epoch 4572: train loss: 0.021720770746469498, val loss: 0.051695872098207474\n",
      "Epoch 4573: train loss: 0.019634827971458435, val loss: 0.11639404296875\n",
      "Epoch 4574: train loss: 0.025240018963813782, val loss: 0.08888044208288193\n",
      "Epoch 4575: train loss: 0.01984689012169838, val loss: 0.04137575253844261\n",
      "Epoch 4576: train loss: 0.026416080072522163, val loss: 0.0766148716211319\n",
      "Epoch 4577: train loss: 0.018590906634926796, val loss: 0.07747303694486618\n",
      "Epoch 4578: train loss: 0.019093165174126625, val loss: 0.0891423374414444\n",
      "Epoch 4579: train loss: 0.023217011243104935, val loss: 0.0970168188214302\n",
      "Epoch 4580: train loss: 0.018220718950033188, val loss: 0.10479708015918732\n",
      "Epoch 4581: train loss: 0.023742247372865677, val loss: 0.08022766560316086\n",
      "Epoch 4582: train loss: 0.016333546489477158, val loss: 0.055294185876846313\n",
      "Epoch 4583: train loss: 0.029623037204146385, val loss: 0.0655321255326271\n",
      "Epoch 4584: train loss: 0.03419547155499458, val loss: 0.03689226135611534\n",
      "Epoch 4585: train loss: 0.032427217811346054, val loss: 0.09247235208749771\n",
      "Epoch 4586: train loss: 0.023728081956505775, val loss: 0.08856480568647385\n",
      "Epoch 4587: train loss: 0.02640358917415142, val loss: 0.06021776795387268\n",
      "Epoch 4588: train loss: 0.02146255224943161, val loss: 0.110298290848732\n",
      "Epoch 4589: train loss: 0.03391808643937111, val loss: 0.11718781292438507\n",
      "Epoch 4590: train loss: 0.022236673161387444, val loss: 0.04931115359067917\n",
      "Epoch 4591: train loss: 0.028950160369277, val loss: 0.09771251678466797\n",
      "Epoch 4592: train loss: 0.023314548656344414, val loss: 0.13064147531986237\n",
      "Epoch 4593: train loss: 0.02037281170487404, val loss: 0.11165284365415573\n",
      "Epoch 4594: train loss: 0.018814658746123314, val loss: 0.06975238025188446\n",
      "Epoch 4595: train loss: 0.027513880282640457, val loss: 0.08175288885831833\n",
      "Epoch 4596: train loss: 0.02579321153461933, val loss: 0.1039297804236412\n",
      "Epoch 4597: train loss: 0.03265354037284851, val loss: 0.13086417317390442\n",
      "Epoch 4598: train loss: 0.021734023466706276, val loss: 0.08210011571645737\n",
      "Epoch 4599: train loss: 0.022529106587171555, val loss: 0.11785469204187393\n",
      "Epoch 4600: train loss: 0.024249590933322906, val loss: 0.06586983799934387\n",
      "Epoch 4601: train loss: 0.02654937654733658, val loss: 0.08479153364896774\n",
      "Epoch 4602: train loss: 0.02625551074743271, val loss: 0.0664595291018486\n",
      "Epoch 4603: train loss: 0.02727399207651615, val loss: 0.08382309973239899\n",
      "Epoch 4604: train loss: 0.029015732929110527, val loss: 0.06639081239700317\n",
      "Epoch 4605: train loss: 0.023766592144966125, val loss: 0.06203350052237511\n",
      "Epoch 4606: train loss: 0.02186303399503231, val loss: 0.08207517117261887\n",
      "Epoch 4607: train loss: 0.021035706624388695, val loss: 0.07589364051818848\n",
      "Epoch 4608: train loss: 0.024373572319746017, val loss: 0.06552337855100632\n",
      "Epoch 4609: train loss: 0.015132077969610691, val loss: 0.08407865464687347\n",
      "Epoch 4610: train loss: 0.02343396097421646, val loss: 0.12824411690235138\n",
      "Epoch 4611: train loss: 0.02508338913321495, val loss: 0.058633774518966675\n",
      "Epoch 4612: train loss: 0.027240034192800522, val loss: 0.051957882940769196\n",
      "Epoch 4613: train loss: 0.029048163443803787, val loss: 0.10343187302350998\n",
      "Epoch 4614: train loss: 0.020926136523485184, val loss: 0.06172802671790123\n",
      "Epoch 4615: train loss: 0.025740407407283783, val loss: 0.06552473455667496\n",
      "Epoch 4616: train loss: 0.02022603712975979, val loss: 0.09912263602018356\n",
      "Epoch 4617: train loss: 0.022789176553487778, val loss: 0.0631783977150917\n",
      "Epoch 4618: train loss: 0.020506976172327995, val loss: 0.08725439757108688\n",
      "Epoch 4619: train loss: 0.016738152131438255, val loss: 0.09298540651798248\n",
      "Epoch 4620: train loss: 0.021679194644093513, val loss: 0.0811256691813469\n",
      "Epoch 4621: train loss: 0.021097682416439056, val loss: 0.07080240547657013\n",
      "Epoch 4622: train loss: 0.019861606881022453, val loss: 0.09799197316169739\n",
      "Epoch 4623: train loss: 0.026018435135483742, val loss: 0.10940384864807129\n",
      "Epoch 4624: train loss: 0.02852260321378708, val loss: 0.11353404819965363\n",
      "Epoch 4625: train loss: 0.02205723337829113, val loss: 0.14863310754299164\n",
      "Epoch 4626: train loss: 0.024545473977923393, val loss: 0.07221487909555435\n",
      "Epoch 4627: train loss: 0.02168414182960987, val loss: 0.10345800220966339\n",
      "Epoch 4628: train loss: 0.028194846585392952, val loss: 0.046563729643821716\n",
      "Epoch 4629: train loss: 0.022493775933980942, val loss: 0.09009461104869843\n",
      "Epoch 4630: train loss: 0.026935331523418427, val loss: 0.053865622729063034\n",
      "Epoch 4631: train loss: 0.025452380999922752, val loss: 0.0990799143910408\n",
      "Epoch 4632: train loss: 0.02919822372496128, val loss: 0.12236429750919342\n",
      "Epoch 4633: train loss: 0.02857150509953499, val loss: 0.055037349462509155\n",
      "Epoch 4634: train loss: 0.02310548722743988, val loss: 0.05302486568689346\n",
      "Epoch 4635: train loss: 0.022387998178601265, val loss: 0.06504415720701218\n",
      "Epoch 4636: train loss: 0.015737198293209076, val loss: 0.05985333397984505\n",
      "Epoch 4637: train loss: 0.03071175143122673, val loss: 0.08074783533811569\n",
      "Epoch 4638: train loss: 0.016626233235001564, val loss: 0.07898815721273422\n",
      "Epoch 4639: train loss: 0.026526372879743576, val loss: 0.06421830505132675\n",
      "Epoch 4640: train loss: 0.019889282062649727, val loss: 0.08231528103351593\n",
      "Epoch 4641: train loss: 0.023057561367750168, val loss: 0.0698409453034401\n",
      "Epoch 4642: train loss: 0.019631637260317802, val loss: 0.07381308078765869\n",
      "Epoch 4643: train loss: 0.019509077072143555, val loss: 0.07062359899282455\n",
      "Epoch 4644: train loss: 0.026420587673783302, val loss: 0.06552227586507797\n",
      "Epoch 4645: train loss: 0.022162942215800285, val loss: 0.06645530462265015\n",
      "Epoch 4646: train loss: 0.021052589640021324, val loss: 0.11575017124414444\n",
      "Epoch 4647: train loss: 0.021651646122336388, val loss: 0.06770090013742447\n",
      "Epoch 4648: train loss: 0.016645558178424835, val loss: 0.07045655697584152\n",
      "Epoch 4649: train loss: 0.024963200092315674, val loss: 0.04818881303071976\n",
      "Epoch 4650: train loss: 0.023920075967907906, val loss: 0.08366059511899948\n",
      "Epoch 4651: train loss: 0.02221098355948925, val loss: 0.04655518755316734\n",
      "Epoch 4652: train loss: 0.023857032880187035, val loss: 0.0751497820019722\n",
      "Epoch 4653: train loss: 0.024670535698533058, val loss: 0.0733351930975914\n",
      "Epoch 4654: train loss: 0.022796643897891045, val loss: 0.08304863423109055\n",
      "Epoch 4655: train loss: 0.025772185996174812, val loss: 0.10794506222009659\n",
      "Epoch 4656: train loss: 0.021200401708483696, val loss: 0.10098548978567123\n",
      "Epoch 4657: train loss: 0.016168242320418358, val loss: 0.07091693580150604\n",
      "Epoch 4658: train loss: 0.022848114371299744, val loss: 0.12037452310323715\n",
      "Epoch 4659: train loss: 0.01729549467563629, val loss: 0.06460034847259521\n",
      "Epoch 4660: train loss: 0.017989883199334145, val loss: 0.04661862552165985\n",
      "Epoch 4661: train loss: 0.015574758872389793, val loss: 0.06665142625570297\n",
      "Epoch 4662: train loss: 0.016119912266731262, val loss: 0.08534063398838043\n",
      "Epoch 4663: train loss: 0.017811238765716553, val loss: 0.07332183420658112\n",
      "Epoch 4664: train loss: 0.019813314080238342, val loss: 0.07183549553155899\n",
      "Epoch 4665: train loss: 0.021176856011152267, val loss: 0.07700104266405106\n",
      "Epoch 4666: train loss: 0.025899438187479973, val loss: 0.07289969176054001\n",
      "Epoch 4667: train loss: 0.022245630621910095, val loss: 0.07774355262517929\n",
      "Epoch 4668: train loss: 0.02141503058373928, val loss: 0.06829646974802017\n",
      "Epoch 4669: train loss: 0.0231663528829813, val loss: 0.053785692900419235\n",
      "Epoch 4670: train loss: 0.02023962326347828, val loss: 0.15246275067329407\n",
      "Epoch 4671: train loss: 0.024187175557017326, val loss: 0.09824217110872269\n",
      "Epoch 4672: train loss: 0.02407742664217949, val loss: 0.05806426331400871\n",
      "Epoch 4673: train loss: 0.02896582894027233, val loss: 0.10962869971990585\n",
      "Epoch 4674: train loss: 0.02268482930958271, val loss: 0.05970287322998047\n",
      "Epoch 4675: train loss: 0.02003840170800686, val loss: 0.10730385035276413\n",
      "Epoch 4676: train loss: 0.021471111103892326, val loss: 0.07530244439840317\n",
      "Epoch 4677: train loss: 0.01640866883099079, val loss: 0.06992547959089279\n",
      "Epoch 4678: train loss: 0.0209302119910717, val loss: 0.060151755809783936\n",
      "Epoch 4679: train loss: 0.024972420185804367, val loss: 0.06071963533759117\n",
      "Epoch 4680: train loss: 0.0257941335439682, val loss: 0.12196262180805206\n",
      "Epoch 4681: train loss: 0.025453025475144386, val loss: 0.04681446775794029\n",
      "Epoch 4682: train loss: 0.02706446684896946, val loss: 0.09559150785207748\n",
      "Epoch 4683: train loss: 0.020285500213503838, val loss: 0.060065578669309616\n",
      "Epoch 4684: train loss: 0.03008170798420906, val loss: 0.05617031455039978\n",
      "Epoch 4685: train loss: 0.02355089969933033, val loss: 0.06123972684144974\n",
      "Epoch 4686: train loss: 0.028367597609758377, val loss: 0.09728710353374481\n",
      "Epoch 4687: train loss: 0.028844717890024185, val loss: 0.092946857213974\n",
      "Epoch 4688: train loss: 0.0312763936817646, val loss: 0.10187677294015884\n",
      "Epoch 4689: train loss: 0.019242556765675545, val loss: 0.08131634443998337\n",
      "Epoch 4690: train loss: 0.01911989413201809, val loss: 0.07307088375091553\n",
      "Epoch 4691: train loss: 0.018419567495584488, val loss: 0.11549010127782822\n",
      "Epoch 4692: train loss: 0.02269163727760315, val loss: 0.08380597084760666\n",
      "Epoch 4693: train loss: 0.03147187456488609, val loss: 0.06745088845491409\n",
      "Epoch 4694: train loss: 0.025552131235599518, val loss: 0.08896364271640778\n",
      "Epoch 4695: train loss: 0.0207056924700737, val loss: 0.0785910114645958\n",
      "Epoch 4696: train loss: 0.025218328461050987, val loss: 0.09239771217107773\n",
      "Epoch 4697: train loss: 0.026143673807382584, val loss: 0.10320314019918442\n",
      "Epoch 4698: train loss: 0.020379217341542244, val loss: 0.09959251433610916\n",
      "Epoch 4699: train loss: 0.0266891960054636, val loss: 0.06947975605726242\n",
      "Epoch 4700: train loss: 0.03685780242085457, val loss: 0.12297797203063965\n",
      "Epoch 4701: train loss: 0.036121267825365067, val loss: 0.06969905644655228\n",
      "Epoch 4702: train loss: 0.029796240851283073, val loss: 0.08243675529956818\n",
      "Epoch 4703: train loss: 0.025465091690421104, val loss: 0.06012251600623131\n",
      "Epoch 4704: train loss: 0.02838926389813423, val loss: 0.061986882239580154\n",
      "Epoch 4705: train loss: 0.026612618938088417, val loss: 0.07537564635276794\n",
      "Epoch 4706: train loss: 0.03469344228506088, val loss: 0.048068150877952576\n",
      "Epoch 4707: train loss: 0.0265511441975832, val loss: 0.06456129997968674\n",
      "Epoch 4708: train loss: 0.022149214521050453, val loss: 0.09502532333135605\n",
      "Epoch 4709: train loss: 0.022788560017943382, val loss: 0.05823789909482002\n",
      "Epoch 4710: train loss: 0.026398733258247375, val loss: 0.06273608654737473\n",
      "Epoch 4711: train loss: 0.027509871870279312, val loss: 0.0757555142045021\n",
      "Epoch 4712: train loss: 0.02512468211352825, val loss: 0.10272743552923203\n",
      "Epoch 4713: train loss: 0.023993555456399918, val loss: 0.15063397586345673\n",
      "Epoch 4714: train loss: 0.02858562581241131, val loss: 0.04789907857775688\n",
      "Epoch 4715: train loss: 0.024046892300248146, val loss: 0.05906424671411514\n",
      "Epoch 4716: train loss: 0.02129255048930645, val loss: 0.05424216017127037\n",
      "Epoch 4717: train loss: 0.023052413016557693, val loss: 0.058349013328552246\n",
      "Epoch 4718: train loss: 0.028660086914896965, val loss: 0.0827704668045044\n",
      "Epoch 4719: train loss: 0.020654382184147835, val loss: 0.03605976328253746\n",
      "Epoch 4720: train loss: 0.025995759293437004, val loss: 0.05760371685028076\n",
      "Epoch 4721: train loss: 0.03299781680107117, val loss: 0.0869852676987648\n",
      "Epoch 4722: train loss: 0.02978474460542202, val loss: 0.08710818737745285\n",
      "Epoch 4723: train loss: 0.030364004895091057, val loss: 0.061599791049957275\n",
      "Epoch 4724: train loss: 0.026775464415550232, val loss: 0.09418697655200958\n",
      "Epoch 4725: train loss: 0.023036805912852287, val loss: 0.09672540426254272\n",
      "Epoch 4726: train loss: 0.02990894392132759, val loss: 0.049291688948869705\n",
      "Epoch 4727: train loss: 0.02863527461886406, val loss: 0.05223473906517029\n",
      "Epoch 4728: train loss: 0.023081839084625244, val loss: 0.06097736582159996\n",
      "Epoch 4729: train loss: 0.025074392557144165, val loss: 0.0758713036775589\n",
      "Epoch 4730: train loss: 0.02886330336332321, val loss: 0.05004603788256645\n",
      "Epoch 4731: train loss: 0.02004728838801384, val loss: 0.13437287509441376\n",
      "Epoch 4732: train loss: 0.022068170830607414, val loss: 0.06275065988302231\n",
      "Epoch 4733: train loss: 0.033431921154260635, val loss: 0.08784965425729752\n",
      "Epoch 4734: train loss: 0.02153218723833561, val loss: 0.08819620311260223\n",
      "Epoch 4735: train loss: 0.027834715321660042, val loss: 0.08226078003644943\n",
      "Epoch 4736: train loss: 0.026385970413684845, val loss: 0.10060552507638931\n",
      "Epoch 4737: train loss: 0.017820343375205994, val loss: 0.06759358942508698\n",
      "Epoch 4738: train loss: 0.02672324702143669, val loss: 0.11323583126068115\n",
      "Epoch 4739: train loss: 0.019458260387182236, val loss: 0.04786607623100281\n",
      "Epoch 4740: train loss: 0.025226321071386337, val loss: 0.06513684242963791\n",
      "Epoch 4741: train loss: 0.017775921151041985, val loss: 0.04580435901880264\n",
      "Epoch 4742: train loss: 0.021539799869060516, val loss: 0.07421618700027466\n",
      "Epoch 4743: train loss: 0.019146978855133057, val loss: 0.07937505096197128\n",
      "Epoch 4744: train loss: 0.03101843222975731, val loss: 0.0718318372964859\n",
      "Epoch 4745: train loss: 0.020486608147621155, val loss: 0.04698095843195915\n",
      "Epoch 4746: train loss: 0.01841788925230503, val loss: 0.08602326363325119\n",
      "Epoch 4747: train loss: 0.01792704500257969, val loss: 0.06457288563251495\n",
      "Epoch 4748: train loss: 0.026935486122965813, val loss: 0.10331632941961288\n",
      "Epoch 4749: train loss: 0.023165028542280197, val loss: 0.05433957651257515\n",
      "Epoch 4750: train loss: 0.02882070653140545, val loss: 0.056433308869600296\n",
      "Epoch 4751: train loss: 0.031322479248046875, val loss: 0.065971240401268\n",
      "Epoch 4752: train loss: 0.028854966163635254, val loss: 0.07069213688373566\n",
      "Epoch 4753: train loss: 0.028604969382286072, val loss: 0.07669984549283981\n",
      "Epoch 4754: train loss: 0.0239089448004961, val loss: 0.10447438061237335\n",
      "Epoch 4755: train loss: 0.028482189401984215, val loss: 0.0708598792552948\n",
      "Epoch 4756: train loss: 0.019857730716466904, val loss: 0.0735074132680893\n",
      "Epoch 4757: train loss: 0.02066548727452755, val loss: 0.1243884339928627\n",
      "Epoch 4758: train loss: 0.025953255593776703, val loss: 0.08324452489614487\n",
      "Epoch 4759: train loss: 0.02530473843216896, val loss: 0.09622976928949356\n",
      "Epoch 4760: train loss: 0.024364525452256203, val loss: 0.04867881163954735\n",
      "Epoch 4761: train loss: 0.022990556433796883, val loss: 0.04467862844467163\n",
      "Epoch 4762: train loss: 0.020298290997743607, val loss: 0.060038384050130844\n",
      "Epoch 4763: train loss: 0.01657048612833023, val loss: 0.08441981673240662\n",
      "Epoch 4764: train loss: 0.01640164852142334, val loss: 0.1118793860077858\n",
      "Epoch 4765: train loss: 0.01925894431769848, val loss: 0.10174050182104111\n",
      "Epoch 4766: train loss: 0.027183080092072487, val loss: 0.06798309087753296\n",
      "Epoch 4767: train loss: 0.021361369639635086, val loss: 0.04515690356492996\n",
      "Epoch 4768: train loss: 0.024246206507086754, val loss: 0.07984312623739243\n",
      "Epoch 4769: train loss: 0.026180963963270187, val loss: 0.06236676126718521\n",
      "Epoch 4770: train loss: 0.027273496612906456, val loss: 0.11367090046405792\n",
      "Epoch 4771: train loss: 0.02538091316819191, val loss: 0.09718113392591476\n",
      "Epoch 4772: train loss: 0.025795528665184975, val loss: 0.06945446878671646\n",
      "Epoch 4773: train loss: 0.02981460466980934, val loss: 0.12124227732419968\n",
      "Epoch 4774: train loss: 0.015467124991118908, val loss: 0.06751058250665665\n",
      "Epoch 4775: train loss: 0.018169647082686424, val loss: 0.10600109398365021\n",
      "Epoch 4776: train loss: 0.025333203375339508, val loss: 0.04104176163673401\n",
      "Epoch 4777: train loss: 0.0286994781345129, val loss: 0.0736723244190216\n",
      "Epoch 4778: train loss: 0.02273620292544365, val loss: 0.09069599211215973\n",
      "Epoch 4779: train loss: 0.02058989182114601, val loss: 0.11169653385877609\n",
      "Epoch 4780: train loss: 0.02214861288666725, val loss: 0.12689794600009918\n",
      "Epoch 4781: train loss: 0.02435394376516342, val loss: 0.05240765959024429\n",
      "Epoch 4782: train loss: 0.02458595298230648, val loss: 0.07339715957641602\n",
      "Epoch 4783: train loss: 0.023534178733825684, val loss: 0.12707875669002533\n",
      "Epoch 4784: train loss: 0.01589530147612095, val loss: 0.10327162593603134\n",
      "Epoch 4785: train loss: 0.029325582087039948, val loss: 0.05823801830410957\n",
      "Epoch 4786: train loss: 0.03223215416073799, val loss: 0.06115245446562767\n",
      "Epoch 4787: train loss: 0.024912310764193535, val loss: 0.09038387984037399\n",
      "Epoch 4788: train loss: 0.023885011672973633, val loss: 0.08742938935756683\n",
      "Epoch 4789: train loss: 0.022245895117521286, val loss: 0.08306790888309479\n",
      "Epoch 4790: train loss: 0.02247985079884529, val loss: 0.0844736322760582\n",
      "Epoch 4791: train loss: 0.024559974670410156, val loss: 0.06809644401073456\n",
      "Epoch 4792: train loss: 0.018879210576415062, val loss: 0.06533246487379074\n",
      "Epoch 4793: train loss: 0.02248922735452652, val loss: 0.04905888810753822\n",
      "Epoch 4794: train loss: 0.03604830056428909, val loss: 0.12439854443073273\n",
      "Epoch 4795: train loss: 0.025842871516942978, val loss: 0.10841646045446396\n",
      "Epoch 4796: train loss: 0.022472895681858063, val loss: 0.0703359842300415\n",
      "Epoch 4797: train loss: 0.025551537051796913, val loss: 0.07444246858358383\n",
      "Epoch 4798: train loss: 0.02854151837527752, val loss: 0.06263034790754318\n",
      "Epoch 4799: train loss: 0.03281285986304283, val loss: 0.09971961379051208\n",
      "Epoch 4800: train loss: 0.01747581921517849, val loss: 0.07597526907920837\n",
      "Epoch 4801: train loss: 0.027110768482089043, val loss: 0.04253838583827019\n",
      "Epoch 4802: train loss: 0.025223057717084885, val loss: 0.11864783614873886\n",
      "Epoch 4803: train loss: 0.03558109328150749, val loss: 0.04164442792534828\n",
      "Epoch 4804: train loss: 0.022915581241250038, val loss: 0.08681406825780869\n",
      "Epoch 4805: train loss: 0.02052854746580124, val loss: 0.11959376186132431\n",
      "Epoch 4806: train loss: 0.01702437922358513, val loss: 0.06346070021390915\n",
      "Epoch 4807: train loss: 0.025523710995912552, val loss: 0.10629399865865707\n",
      "Epoch 4808: train loss: 0.020281601697206497, val loss: 0.0948149785399437\n",
      "Epoch 4809: train loss: 0.017603900283575058, val loss: 0.12044944614171982\n",
      "Epoch 4810: train loss: 0.019796893000602722, val loss: 0.05435902625322342\n",
      "Epoch 4811: train loss: 0.01952318474650383, val loss: 0.05351982265710831\n",
      "Epoch 4812: train loss: 0.020558301359415054, val loss: 0.09589622914791107\n",
      "Epoch 4813: train loss: 0.027278339490294456, val loss: 0.09862355142831802\n",
      "Epoch 4814: train loss: 0.023821158334612846, val loss: 0.08282693475484848\n",
      "Epoch 4815: train loss: 0.020349035039544106, val loss: 0.09573104232549667\n",
      "Epoch 4816: train loss: 0.014485153369605541, val loss: 0.0884208232164383\n",
      "Epoch 4817: train loss: 0.01771346852183342, val loss: 0.09258174151182175\n",
      "Epoch 4818: train loss: 0.015871109440922737, val loss: 0.06625102460384369\n",
      "Epoch 4819: train loss: 0.026563309133052826, val loss: 0.04407205060124397\n",
      "Epoch 4820: train loss: 0.02520490251481533, val loss: 0.04756201058626175\n",
      "Epoch 4821: train loss: 0.015384587459266186, val loss: 0.06622874736785889\n",
      "Epoch 4822: train loss: 0.025033697485923767, val loss: 0.09378411620855331\n",
      "Epoch 4823: train loss: 0.019993428140878677, val loss: 0.07608793675899506\n",
      "Epoch 4824: train loss: 0.019500035792589188, val loss: 0.09988414496183395\n",
      "Epoch 4825: train loss: 0.02801360934972763, val loss: 0.09949196875095367\n",
      "Epoch 4826: train loss: 0.02319261059165001, val loss: 0.06822823733091354\n",
      "Epoch 4827: train loss: 0.021533580496907234, val loss: 0.05925711989402771\n",
      "Epoch 4828: train loss: 0.020738450810313225, val loss: 0.04198711737990379\n",
      "Epoch 4829: train loss: 0.020715175196528435, val loss: 0.09012065082788467\n",
      "Epoch 4830: train loss: 0.026183439418673515, val loss: 0.04804573953151703\n",
      "Epoch 4831: train loss: 0.027392461895942688, val loss: 0.08319653570652008\n",
      "Epoch 4832: train loss: 0.02093377336859703, val loss: 0.07029218971729279\n",
      "Epoch 4833: train loss: 0.025216802954673767, val loss: 0.06903049349784851\n",
      "Epoch 4834: train loss: 0.019641151651740074, val loss: 0.07257730513811111\n",
      "Epoch 4835: train loss: 0.030584441497921944, val loss: 0.03484394773840904\n",
      "Epoch 4836: train loss: 0.02334846928715706, val loss: 0.06369099766016006\n",
      "Epoch 4837: train loss: 0.02411080338060856, val loss: 0.06120813637971878\n",
      "Epoch 4838: train loss: 0.023210378363728523, val loss: 0.07399411499500275\n",
      "Epoch 4839: train loss: 0.02511797472834587, val loss: 0.07791658490896225\n",
      "Epoch 4840: train loss: 0.022746017202734947, val loss: 0.06868521869182587\n",
      "Epoch 4841: train loss: 0.016932440921664238, val loss: 0.10340694338083267\n",
      "Epoch 4842: train loss: 0.021570120006799698, val loss: 0.06274712085723877\n",
      "Epoch 4843: train loss: 0.028682444244623184, val loss: 0.07836201786994934\n",
      "Epoch 4844: train loss: 0.02529222145676613, val loss: 0.040293704718351364\n",
      "Epoch 4845: train loss: 0.02509719878435135, val loss: 0.08633031696081161\n",
      "Epoch 4846: train loss: 0.023629523813724518, val loss: 0.0659174844622612\n",
      "Epoch 4847: train loss: 0.02019723877310753, val loss: 0.09989732503890991\n",
      "Epoch 4848: train loss: 0.02212189882993698, val loss: 0.11131105571985245\n",
      "Epoch 4849: train loss: 0.024431811645627022, val loss: 0.07636532932519913\n",
      "Epoch 4850: train loss: 0.02339063212275505, val loss: 0.03908872231841087\n",
      "Epoch 4851: train loss: 0.02414526976644993, val loss: 0.08361589908599854\n",
      "Epoch 4852: train loss: 0.02684667892754078, val loss: 0.07682689279317856\n",
      "Epoch 4853: train loss: 0.022586068138480186, val loss: 0.07510511577129364\n",
      "Epoch 4854: train loss: 0.02108602784574032, val loss: 0.09670265018939972\n",
      "Epoch 4855: train loss: 0.01995854079723358, val loss: 0.08387170732021332\n",
      "Epoch 4856: train loss: 0.027176735922694206, val loss: 0.05913889408111572\n",
      "Epoch 4857: train loss: 0.02240835316479206, val loss: 0.060583461076021194\n",
      "Epoch 4858: train loss: 0.027046093717217445, val loss: 0.12700298428535461\n",
      "Epoch 4859: train loss: 0.026645351201295853, val loss: 0.04107607901096344\n",
      "Epoch 4860: train loss: 0.023562267422676086, val loss: 0.05911445617675781\n",
      "Epoch 4861: train loss: 0.026989847421646118, val loss: 0.09623197466135025\n",
      "Epoch 4862: train loss: 0.017289603129029274, val loss: 0.1397293508052826\n",
      "Epoch 4863: train loss: 0.02914462983608246, val loss: 0.08461347222328186\n",
      "Epoch 4864: train loss: 0.018337097018957138, val loss: 0.09867457300424576\n",
      "Epoch 4865: train loss: 0.02621406503021717, val loss: 0.09785286337137222\n",
      "Epoch 4866: train loss: 0.022341527044773102, val loss: 0.06735414266586304\n",
      "Epoch 4867: train loss: 0.02548014186322689, val loss: 0.051676299422979355\n",
      "Epoch 4868: train loss: 0.025358015671372414, val loss: 0.05949027091264725\n",
      "Epoch 4869: train loss: 0.027666496112942696, val loss: 0.08984943479299545\n",
      "Epoch 4870: train loss: 0.019268745556473732, val loss: 0.07339780032634735\n",
      "Epoch 4871: train loss: 0.016767336055636406, val loss: 0.053618110716342926\n",
      "Epoch 4872: train loss: 0.02346240170300007, val loss: 0.0898100957274437\n",
      "Epoch 4873: train loss: 0.020736606791615486, val loss: 0.07988335937261581\n",
      "Epoch 4874: train loss: 0.017816811800003052, val loss: 0.08363506197929382\n",
      "Epoch 4875: train loss: 0.029185188934206963, val loss: 0.08681998401880264\n",
      "Epoch 4876: train loss: 0.021197041496634483, val loss: 0.08953427523374557\n",
      "Epoch 4877: train loss: 0.016853000968694687, val loss: 0.08150343596935272\n",
      "Epoch 4878: train loss: 0.025973118841648102, val loss: 0.099051333963871\n",
      "Epoch 4879: train loss: 0.029037054628133774, val loss: 0.08130943775177002\n",
      "Epoch 4880: train loss: 0.0184277705848217, val loss: 0.08980416506528854\n",
      "Epoch 4881: train loss: 0.01894606277346611, val loss: 0.0793701633810997\n",
      "Epoch 4882: train loss: 0.023629970848560333, val loss: 0.06509728729724884\n",
      "Epoch 4883: train loss: 0.029795389622449875, val loss: 0.05582718923687935\n",
      "Epoch 4884: train loss: 0.03763240948319435, val loss: 0.07188984006643295\n",
      "Epoch 4885: train loss: 0.023547999560832977, val loss: 0.09535040706396103\n",
      "Epoch 4886: train loss: 0.01974569261074066, val loss: 0.062409866601228714\n",
      "Epoch 4887: train loss: 0.024337811395525932, val loss: 0.09151613712310791\n",
      "Epoch 4888: train loss: 0.02499217540025711, val loss: 0.06388244777917862\n",
      "Epoch 4889: train loss: 0.021013639867305756, val loss: 0.11151578277349472\n",
      "Epoch 4890: train loss: 0.019569572061300278, val loss: 0.09869106113910675\n",
      "Epoch 4891: train loss: 0.023134857416152954, val loss: 0.08829446136951447\n",
      "Epoch 4892: train loss: 0.01728624850511551, val loss: 0.05714533478021622\n",
      "Epoch 4893: train loss: 0.020651552826166153, val loss: 0.07170241326093674\n",
      "Epoch 4894: train loss: 0.021018516272306442, val loss: 0.06631480902433395\n",
      "Epoch 4895: train loss: 0.024736866354942322, val loss: 0.08641377091407776\n",
      "Epoch 4896: train loss: 0.021623793989419937, val loss: 0.05715595558285713\n",
      "Epoch 4897: train loss: 0.02442753314971924, val loss: 0.05533714219927788\n",
      "Epoch 4898: train loss: 0.02458324283361435, val loss: 0.04764246568083763\n",
      "Epoch 4899: train loss: 0.02893586829304695, val loss: 0.08223555237054825\n",
      "Epoch 4900: train loss: 0.026058457791805267, val loss: 0.05723202973604202\n",
      "Epoch 4901: train loss: 0.018536213785409927, val loss: 0.08031254261732101\n",
      "Epoch 4902: train loss: 0.023099534213542938, val loss: 0.06821205466985703\n",
      "Epoch 4903: train loss: 0.02517060749232769, val loss: 0.07680636644363403\n",
      "Epoch 4904: train loss: 0.01927279122173786, val loss: 0.0983547791838646\n",
      "Epoch 4905: train loss: 0.021175017580389977, val loss: 0.08795849978923798\n",
      "Epoch 4906: train loss: 0.02818497270345688, val loss: 0.08095147460699081\n",
      "Epoch 4907: train loss: 0.016573555767536163, val loss: 0.07552088797092438\n",
      "Epoch 4908: train loss: 0.02185060828924179, val loss: 0.05222778394818306\n",
      "Epoch 4909: train loss: 0.02120739221572876, val loss: 0.09001445025205612\n",
      "Epoch 4910: train loss: 0.01785907708108425, val loss: 0.07659503072500229\n",
      "Epoch 4911: train loss: 0.02426038682460785, val loss: 0.06651084870100021\n",
      "Epoch 4912: train loss: 0.022982802242040634, val loss: 0.06223450228571892\n",
      "Epoch 4913: train loss: 0.023083187639713287, val loss: 0.06274645775556564\n",
      "Epoch 4914: train loss: 0.021413009613752365, val loss: 0.09995440393686295\n",
      "Epoch 4915: train loss: 0.020501889288425446, val loss: 0.08970146626234055\n",
      "Epoch 4916: train loss: 0.02092786878347397, val loss: 0.07492014020681381\n",
      "Epoch 4917: train loss: 0.020484836772084236, val loss: 0.05315142869949341\n",
      "Epoch 4918: train loss: 0.021649066358804703, val loss: 0.04283418878912926\n",
      "Epoch 4919: train loss: 0.02004762925207615, val loss: 0.08405714482069016\n",
      "Epoch 4920: train loss: 0.022784916684031487, val loss: 0.12225072830915451\n",
      "Epoch 4921: train loss: 0.023112120106816292, val loss: 0.05045957490801811\n",
      "Epoch 4922: train loss: 0.015896912664175034, val loss: 0.09286817163228989\n",
      "Epoch 4923: train loss: 0.017367493361234665, val loss: 0.08285196870565414\n",
      "Epoch 4924: train loss: 0.025798682123422623, val loss: 0.05129413679242134\n",
      "Epoch 4925: train loss: 0.028613615781068802, val loss: 0.06042354181408882\n",
      "Epoch 4926: train loss: 0.02786167524755001, val loss: 0.051763683557510376\n",
      "Epoch 4927: train loss: 0.029811060056090355, val loss: 0.05943943187594414\n",
      "Epoch 4928: train loss: 0.023324577137827873, val loss: 0.08771833032369614\n",
      "Epoch 4929: train loss: 0.018638595938682556, val loss: 0.10090724378824234\n",
      "Epoch 4930: train loss: 0.01915496215224266, val loss: 0.08124182373285294\n",
      "Epoch 4931: train loss: 0.024427086114883423, val loss: 0.1067439466714859\n",
      "Epoch 4932: train loss: 0.017572810873389244, val loss: 0.08271018415689468\n",
      "Epoch 4933: train loss: 0.028057385236024857, val loss: 0.07309480756521225\n",
      "Epoch 4934: train loss: 0.02501598186790943, val loss: 0.0686877965927124\n",
      "Epoch 4935: train loss: 0.036735404282808304, val loss: 0.08813902735710144\n",
      "Epoch 4936: train loss: 0.020704906433820724, val loss: 0.08138853311538696\n",
      "Epoch 4937: train loss: 0.027636714279651642, val loss: 0.11063706874847412\n",
      "Epoch 4938: train loss: 0.01691644825041294, val loss: 0.08529464900493622\n",
      "Epoch 4939: train loss: 0.029626181349158287, val loss: 0.093765489757061\n",
      "Epoch 4940: train loss: 0.02463013306260109, val loss: 0.0839301347732544\n",
      "Epoch 4941: train loss: 0.024349160492420197, val loss: 0.09666787832975388\n",
      "Epoch 4942: train loss: 0.022664541378617287, val loss: 0.05766136199235916\n",
      "Epoch 4943: train loss: 0.018621457740664482, val loss: 0.07772832363843918\n",
      "Epoch 4944: train loss: 0.026165755465626717, val loss: 0.05993267521262169\n",
      "Epoch 4945: train loss: 0.02350873127579689, val loss: 0.08332961797714233\n",
      "Epoch 4946: train loss: 0.017883656546473503, val loss: 0.11410655081272125\n",
      "Epoch 4947: train loss: 0.030143151059746742, val loss: 0.04598826915025711\n",
      "Epoch 4948: train loss: 0.01887127384543419, val loss: 0.08074379712343216\n",
      "Epoch 4949: train loss: 0.02949357032775879, val loss: 0.06041518971323967\n",
      "Epoch 4950: train loss: 0.03006078489124775, val loss: 0.04358562454581261\n",
      "Epoch 4951: train loss: 0.024003803730010986, val loss: 0.08623119443655014\n",
      "Epoch 4952: train loss: 0.02654455229640007, val loss: 0.08152580261230469\n",
      "Epoch 4953: train loss: 0.036395296454429626, val loss: 0.03383490443229675\n",
      "Epoch 4954: train loss: 0.02316165342926979, val loss: 0.0694989487528801\n",
      "Epoch 4955: train loss: 0.01919146440923214, val loss: 0.06655043363571167\n",
      "Epoch 4956: train loss: 0.030623771250247955, val loss: 0.10729801654815674\n",
      "Epoch 4957: train loss: 0.024550342932343483, val loss: 0.06425067037343979\n",
      "Epoch 4958: train loss: 0.01913624256849289, val loss: 0.07782017439603806\n",
      "Epoch 4959: train loss: 0.02507646754384041, val loss: 0.07654263079166412\n",
      "Epoch 4960: train loss: 0.022177288308739662, val loss: 0.09949902445077896\n",
      "Epoch 4961: train loss: 0.023256225511431694, val loss: 0.11711492389440536\n",
      "Epoch 4962: train loss: 0.01486236322671175, val loss: 0.056171584874391556\n",
      "Epoch 4963: train loss: 0.029677143320441246, val loss: 0.05243559554219246\n",
      "Epoch 4964: train loss: 0.02123694121837616, val loss: 0.07633230090141296\n",
      "Epoch 4965: train loss: 0.018442342057824135, val loss: 0.09051306545734406\n",
      "Epoch 4966: train loss: 0.019499003887176514, val loss: 0.09393098950386047\n",
      "Epoch 4967: train loss: 0.018359871581196785, val loss: 0.0868953987956047\n",
      "Epoch 4968: train loss: 0.015926092863082886, val loss: 0.10819965600967407\n",
      "Epoch 4969: train loss: 0.018529068678617477, val loss: 0.09391225874423981\n",
      "Epoch 4970: train loss: 0.02906709909439087, val loss: 0.11424332112073898\n",
      "Epoch 4971: train loss: 0.017496781423687935, val loss: 0.05527948960661888\n",
      "Epoch 4972: train loss: 0.016100158914923668, val loss: 0.08479030430316925\n",
      "Epoch 4973: train loss: 0.019633665680885315, val loss: 0.0939883217215538\n",
      "Epoch 4974: train loss: 0.026114506646990776, val loss: 0.04630574584007263\n",
      "Epoch 4975: train loss: 0.026867590844631195, val loss: 0.055088914930820465\n",
      "Epoch 4976: train loss: 0.029728004708886147, val loss: 0.06801611185073853\n",
      "Epoch 4977: train loss: 0.020459694787859917, val loss: 0.05352923274040222\n",
      "Epoch 4978: train loss: 0.02067836932837963, val loss: 0.08688826113939285\n",
      "Epoch 4979: train loss: 0.020498747006058693, val loss: 0.07077687233686447\n",
      "Epoch 4980: train loss: 0.01962485909461975, val loss: 0.08041644096374512\n",
      "Epoch 4981: train loss: 0.021153170615434647, val loss: 0.07632656395435333\n",
      "Epoch 4982: train loss: 0.030120208859443665, val loss: 0.043123919516801834\n",
      "Epoch 4983: train loss: 0.022428322583436966, val loss: 0.09505854547023773\n",
      "Epoch 4984: train loss: 0.021366016939282417, val loss: 0.08758972585201263\n",
      "Epoch 4985: train loss: 0.014560811221599579, val loss: 0.08480994403362274\n",
      "Epoch 4986: train loss: 0.019453182816505432, val loss: 0.08571875095367432\n",
      "Epoch 4987: train loss: 0.02410963922739029, val loss: 0.08301547914743423\n",
      "Epoch 4988: train loss: 0.020151233300566673, val loss: 0.1271219551563263\n",
      "Epoch 4989: train loss: 0.023587381467223167, val loss: 0.09226292371749878\n",
      "Epoch 4990: train loss: 0.019853485748171806, val loss: 0.07291527837514877\n",
      "Epoch 4991: train loss: 0.025242000818252563, val loss: 0.06622316688299179\n",
      "Epoch 4992: train loss: 0.022914068773388863, val loss: 0.08927716314792633\n",
      "Epoch 4993: train loss: 0.028007980436086655, val loss: 0.060948800295591354\n",
      "Epoch 4994: train loss: 0.022742707282304764, val loss: 0.07152307033538818\n",
      "Epoch 4995: train loss: 0.02142399549484253, val loss: 0.09139783680438995\n",
      "Epoch 4996: train loss: 0.025329584255814552, val loss: 0.05562698468565941\n",
      "Epoch 4997: train loss: 0.024772649630904198, val loss: 0.07377832382917404\n",
      "Epoch 4998: train loss: 0.04364607483148575, val loss: 0.05858294293284416\n",
      "Epoch 4999: train loss: 0.023995082825422287, val loss: 0.06096463277935982\n",
      "Epoch 5000: train loss: 0.022741610184311867, val loss: 0.06750913709402084\n",
      "Epoch 5001: train loss: 0.021626431494951248, val loss: 0.07122602313756943\n",
      "Epoch 5002: train loss: 0.02403847500681877, val loss: 0.09824953973293304\n",
      "Epoch 5003: train loss: 0.019130999222397804, val loss: 0.06507772207260132\n",
      "Epoch 5004: train loss: 0.02825123816728592, val loss: 0.08249925822019577\n",
      "Epoch 5005: train loss: 0.02310207299888134, val loss: 0.047401104122400284\n",
      "Epoch 5006: train loss: 0.020931949838995934, val loss: 0.08036667108535767\n",
      "Epoch 5007: train loss: 0.025054605677723885, val loss: 0.0606636181473732\n",
      "Epoch 5008: train loss: 0.021959427744150162, val loss: 0.038751836866140366\n",
      "Epoch 5009: train loss: 0.017641250044107437, val loss: 0.08472002297639847\n",
      "Epoch 5010: train loss: 0.021715303882956505, val loss: 0.05123040825128555\n",
      "Epoch 5011: train loss: 0.028547341004014015, val loss: 0.11362558603286743\n",
      "Epoch 5012: train loss: 0.026380732655525208, val loss: 0.08411487191915512\n",
      "Epoch 5013: train loss: 0.020514648407697678, val loss: 0.0862506851553917\n",
      "Epoch 5014: train loss: 0.023128537461161613, val loss: 0.04663524031639099\n",
      "Epoch 5015: train loss: 0.01790802553296089, val loss: 0.08600341528654099\n",
      "Epoch 5016: train loss: 0.020595436915755272, val loss: 0.09932810068130493\n",
      "Epoch 5017: train loss: 0.019975995644927025, val loss: 0.054177094250917435\n",
      "Epoch 5018: train loss: 0.023809311911463737, val loss: 0.0853428915143013\n",
      "Epoch 5019: train loss: 0.021554000675678253, val loss: 0.06826131790876389\n",
      "Epoch 5020: train loss: 0.025902606546878815, val loss: 0.08523573726415634\n",
      "Epoch 5021: train loss: 0.02009139023721218, val loss: 0.09860909730195999\n",
      "Epoch 5022: train loss: 0.023023923859000206, val loss: 0.08568663150072098\n",
      "Epoch 5023: train loss: 0.0281972736120224, val loss: 0.11118452996015549\n",
      "Epoch 5024: train loss: 0.028046581894159317, val loss: 0.06069253757596016\n",
      "Epoch 5025: train loss: 0.02037370577454567, val loss: 0.0681593045592308\n",
      "Epoch 5026: train loss: 0.03413143754005432, val loss: 0.10007122904062271\n",
      "Epoch 5027: train loss: 0.02423892915248871, val loss: 0.0788552388548851\n",
      "Epoch 5028: train loss: 0.026312118396162987, val loss: 0.07883558422327042\n",
      "Epoch 5029: train loss: 0.023624075576663017, val loss: 0.07230453193187714\n",
      "Epoch 5030: train loss: 0.018339233472943306, val loss: 0.07645311951637268\n",
      "Epoch 5031: train loss: 0.02775563672184944, val loss: 0.08755333721637726\n",
      "Epoch 5032: train loss: 0.022949758917093277, val loss: 0.05292186886072159\n",
      "Epoch 5033: train loss: 0.02229166403412819, val loss: 0.07367902249097824\n",
      "Epoch 5034: train loss: 0.021389853209257126, val loss: 0.08137145638465881\n",
      "Epoch 5035: train loss: 0.028979264199733734, val loss: 0.092070572078228\n",
      "Epoch 5036: train loss: 0.03681933879852295, val loss: 0.06337188929319382\n",
      "Epoch 5037: train loss: 0.02348758652806282, val loss: 0.09820783138275146\n",
      "Epoch 5038: train loss: 0.026913844048976898, val loss: 0.03423510864377022\n",
      "Epoch 5039: train loss: 0.026774058118462563, val loss: 0.08061408996582031\n",
      "Epoch 5040: train loss: 0.022406578063964844, val loss: 0.0936012789607048\n",
      "Epoch 5041: train loss: 0.03830988332629204, val loss: 0.07616382837295532\n",
      "Epoch 5042: train loss: 0.020226679742336273, val loss: 0.07432825863361359\n",
      "Epoch 5043: train loss: 0.023587338626384735, val loss: 0.08441932499408722\n",
      "Epoch 5044: train loss: 0.028370831161737442, val loss: 0.08380432426929474\n",
      "Epoch 5045: train loss: 0.02874702401459217, val loss: 0.07778407633304596\n",
      "Epoch 5046: train loss: 0.028019418939948082, val loss: 0.06328999251127243\n",
      "Epoch 5047: train loss: 0.022636577486991882, val loss: 0.08099567145109177\n",
      "Epoch 5048: train loss: 0.028289461508393288, val loss: 0.06118383631110191\n",
      "Epoch 5049: train loss: 0.02747165784239769, val loss: 0.10972604900598526\n",
      "Epoch 5050: train loss: 0.02242043800652027, val loss: 0.07733776420354843\n",
      "Epoch 5051: train loss: 0.02739371731877327, val loss: 0.04478157311677933\n",
      "Epoch 5052: train loss: 0.03503836691379547, val loss: 0.07637625932693481\n",
      "Epoch 5053: train loss: 0.0202250424772501, val loss: 0.0410674512386322\n",
      "Epoch 5054: train loss: 0.021849965676665306, val loss: 0.06908447295427322\n",
      "Epoch 5055: train loss: 0.023607680574059486, val loss: 0.15237374603748322\n",
      "Epoch 5056: train loss: 0.02292053960263729, val loss: 0.07394690811634064\n",
      "Epoch 5057: train loss: 0.021219797432422638, val loss: 0.0609159953892231\n",
      "Epoch 5058: train loss: 0.026252444833517075, val loss: 0.09013320505619049\n",
      "Epoch 5059: train loss: 0.03616313636302948, val loss: 0.06763575226068497\n",
      "Epoch 5060: train loss: 0.020508889108896255, val loss: 0.05603843927383423\n",
      "Epoch 5061: train loss: 0.026843609288334846, val loss: 0.04648889973759651\n",
      "Epoch 5062: train loss: 0.01959037408232689, val loss: 0.05667649582028389\n",
      "Epoch 5063: train loss: 0.024693988263607025, val loss: 0.0704546868801117\n",
      "Epoch 5064: train loss: 0.02087586745619774, val loss: 0.058897603303194046\n",
      "Epoch 5065: train loss: 0.023509588092565536, val loss: 0.09989836066961288\n",
      "Epoch 5066: train loss: 0.02161679044365883, val loss: 0.06872003525495529\n",
      "Epoch 5067: train loss: 0.029342154040932655, val loss: 0.06936096400022507\n",
      "Epoch 5068: train loss: 0.024093832820653915, val loss: 0.06714450567960739\n",
      "Epoch 5069: train loss: 0.025728661566972733, val loss: 0.03699170798063278\n",
      "Epoch 5070: train loss: 0.0298351738601923, val loss: 0.10552570968866348\n",
      "Epoch 5071: train loss: 0.02828013151884079, val loss: 0.08023962378501892\n",
      "Epoch 5072: train loss: 0.022261906415224075, val loss: 0.055036794394254684\n",
      "Epoch 5073: train loss: 0.028305960819125175, val loss: 0.08387082815170288\n",
      "Epoch 5074: train loss: 0.024525495246052742, val loss: 0.08548137545585632\n",
      "Epoch 5075: train loss: 0.019021322950720787, val loss: 0.12695251405239105\n",
      "Epoch 5076: train loss: 0.01729094609618187, val loss: 0.08305978029966354\n",
      "Epoch 5077: train loss: 0.021731389686465263, val loss: 0.06579488515853882\n",
      "Epoch 5078: train loss: 0.022180331870913506, val loss: 0.10300806909799576\n",
      "Epoch 5079: train loss: 0.023119650781154633, val loss: 0.05207590013742447\n",
      "Epoch 5080: train loss: 0.027708565816283226, val loss: 0.06728457659482956\n",
      "Epoch 5081: train loss: 0.03528963774442673, val loss: 0.06709495931863785\n",
      "Epoch 5082: train loss: 0.021488267928361893, val loss: 0.05516895279288292\n",
      "Epoch 5083: train loss: 0.028262827545404434, val loss: 0.04687165468931198\n",
      "Epoch 5084: train loss: 0.023531164973974228, val loss: 0.060523103922605515\n",
      "Epoch 5085: train loss: 0.03289409726858139, val loss: 0.09951956570148468\n",
      "Epoch 5086: train loss: 0.024798067286610603, val loss: 0.12994757294654846\n",
      "Epoch 5087: train loss: 0.02779887057840824, val loss: 0.10020139068365097\n",
      "Epoch 5088: train loss: 0.02022571489214897, val loss: 0.0673418715596199\n",
      "Epoch 5089: train loss: 0.01621970348060131, val loss: 0.06761042028665543\n",
      "Epoch 5090: train loss: 0.023512104526162148, val loss: 0.054739952087402344\n",
      "Epoch 5091: train loss: 0.019472414627671242, val loss: 0.13347060978412628\n",
      "Epoch 5092: train loss: 0.020801428705453873, val loss: 0.04594452679157257\n",
      "Epoch 5093: train loss: 0.023885738104581833, val loss: 0.05917873606085777\n",
      "Epoch 5094: train loss: 0.021120991557836533, val loss: 0.10409776121377945\n",
      "Epoch 5095: train loss: 0.022623367607593536, val loss: 0.0800410807132721\n",
      "Epoch 5096: train loss: 0.021349286660552025, val loss: 0.045180413872003555\n",
      "Epoch 5097: train loss: 0.01696942374110222, val loss: 0.07643811404705048\n",
      "Epoch 5098: train loss: 0.024872468784451485, val loss: 0.090138740837574\n",
      "Epoch 5099: train loss: 0.025935642421245575, val loss: 0.11300694197416306\n",
      "Epoch 5100: train loss: 0.021590949967503548, val loss: 0.0980149582028389\n",
      "Epoch 5101: train loss: 0.02351846732199192, val loss: 0.08682756870985031\n",
      "Epoch 5102: train loss: 0.026775117963552475, val loss: 0.0706816017627716\n",
      "Epoch 5103: train loss: 0.0221417173743248, val loss: 0.07238057255744934\n",
      "Epoch 5104: train loss: 0.0209562536329031, val loss: 0.07386129349470139\n",
      "Epoch 5105: train loss: 0.021895304322242737, val loss: 0.061702217906713486\n",
      "Epoch 5106: train loss: 0.020077591761946678, val loss: 0.06776627153158188\n",
      "Epoch 5107: train loss: 0.024327125400304794, val loss: 0.09610377997159958\n",
      "Epoch 5108: train loss: 0.02234279364347458, val loss: 0.06480314582586288\n",
      "Epoch 5109: train loss: 0.015375048853456974, val loss: 0.08556416630744934\n",
      "Epoch 5110: train loss: 0.015377990901470184, val loss: 0.09372641891241074\n",
      "Epoch 5111: train loss: 0.030252860859036446, val loss: 0.08308931440114975\n",
      "Epoch 5112: train loss: 0.021472187712788582, val loss: 0.09024869650602341\n",
      "Epoch 5113: train loss: 0.031072992831468582, val loss: 0.06718270480632782\n",
      "Epoch 5114: train loss: 0.02233889140188694, val loss: 0.08514945954084396\n",
      "Epoch 5115: train loss: 0.02544979564845562, val loss: 0.06655773520469666\n",
      "Epoch 5116: train loss: 0.017863433808088303, val loss: 0.07070046663284302\n",
      "Epoch 5117: train loss: 0.018857503309845924, val loss: 0.10630078613758087\n",
      "Epoch 5118: train loss: 0.026525162160396576, val loss: 0.07290852814912796\n",
      "Epoch 5119: train loss: 0.024362219497561455, val loss: 0.05784289166331291\n",
      "Epoch 5120: train loss: 0.016822386533021927, val loss: 0.06600531935691833\n",
      "Epoch 5121: train loss: 0.0202140212059021, val loss: 0.09971152245998383\n",
      "Epoch 5122: train loss: 0.025469541549682617, val loss: 0.13334670662879944\n",
      "Epoch 5123: train loss: 0.016585299745202065, val loss: 0.10577920824289322\n",
      "Epoch 5124: train loss: 0.029891453683376312, val loss: 0.07706551998853683\n",
      "Epoch 5125: train loss: 0.02556120604276657, val loss: 0.05726826936006546\n",
      "Epoch 5126: train loss: 0.017282355576753616, val loss: 0.09294028580188751\n",
      "Epoch 5127: train loss: 0.019877230748534203, val loss: 0.11041988432407379\n",
      "Epoch 5128: train loss: 0.026351260021328926, val loss: 0.06559433043003082\n",
      "Epoch 5129: train loss: 0.02330390363931656, val loss: 0.06943366676568985\n",
      "Epoch 5130: train loss: 0.019730782136321068, val loss: 0.0972491130232811\n",
      "Epoch 5131: train loss: 0.020602915436029434, val loss: 0.056223005056381226\n",
      "Epoch 5132: train loss: 0.0249948613345623, val loss: 0.059113580733537674\n",
      "Epoch 5133: train loss: 0.026301758363842964, val loss: 0.08827412128448486\n",
      "Epoch 5134: train loss: 0.028058364987373352, val loss: 0.10731629282236099\n",
      "Epoch 5135: train loss: 0.028813252225518227, val loss: 0.06508175283670425\n",
      "Epoch 5136: train loss: 0.018164554610848427, val loss: 0.07701165974140167\n",
      "Epoch 5137: train loss: 0.023941364139318466, val loss: 0.08372924476861954\n",
      "Epoch 5138: train loss: 0.022030264139175415, val loss: 0.14002545177936554\n",
      "Epoch 5139: train loss: 0.020529525354504585, val loss: 0.05179433897137642\n",
      "Epoch 5140: train loss: 0.03164873644709587, val loss: 0.05566379427909851\n",
      "Epoch 5141: train loss: 0.0207490436732769, val loss: 0.056114811450242996\n",
      "Epoch 5142: train loss: 0.01601622998714447, val loss: 0.06320717185735703\n",
      "Epoch 5143: train loss: 0.02379758097231388, val loss: 0.062472905963659286\n",
      "Epoch 5144: train loss: 0.032441310584545135, val loss: 0.043242331594228745\n",
      "Epoch 5145: train loss: 0.02290625125169754, val loss: 0.1021404042840004\n",
      "Epoch 5146: train loss: 0.02116873301565647, val loss: 0.07357187569141388\n",
      "Epoch 5147: train loss: 0.023138539865612984, val loss: 0.08593270927667618\n",
      "Epoch 5148: train loss: 0.02017246186733246, val loss: 0.07033683359622955\n",
      "Epoch 5149: train loss: 0.024026699364185333, val loss: 0.0460205003619194\n",
      "Epoch 5150: train loss: 0.022461211308836937, val loss: 0.07311930507421494\n",
      "Epoch 5151: train loss: 0.022366244345903397, val loss: 0.06850253790616989\n",
      "Epoch 5152: train loss: 0.017862802371382713, val loss: 0.061233218759298325\n",
      "Epoch 5153: train loss: 0.019717147573828697, val loss: 0.056450556963682175\n",
      "Epoch 5154: train loss: 0.021256202831864357, val loss: 0.07911904156208038\n",
      "Epoch 5155: train loss: 0.017516085878014565, val loss: 0.07073592394590378\n",
      "Epoch 5156: train loss: 0.01462086383253336, val loss: 0.06815711408853531\n",
      "Epoch 5157: train loss: 0.01395051646977663, val loss: 0.040709640830755234\n",
      "Epoch 5158: train loss: 0.020436739549040794, val loss: 0.06859659403562546\n",
      "Epoch 5159: train loss: 0.023591985926032066, val loss: 0.12249336391687393\n",
      "Epoch 5160: train loss: 0.017607247456908226, val loss: 0.059910714626312256\n",
      "Epoch 5161: train loss: 0.02208317071199417, val loss: 0.06302448362112045\n",
      "Epoch 5162: train loss: 0.023045241832733154, val loss: 0.11637995392084122\n",
      "Epoch 5163: train loss: 0.024042706936597824, val loss: 0.08532398194074631\n",
      "Epoch 5164: train loss: 0.02328982762992382, val loss: 0.04147401079535484\n",
      "Epoch 5165: train loss: 0.018889348953962326, val loss: 0.04529976844787598\n",
      "Epoch 5166: train loss: 0.01792891137301922, val loss: 0.10434108972549438\n",
      "Epoch 5167: train loss: 0.030272075906395912, val loss: 0.08492261171340942\n",
      "Epoch 5168: train loss: 0.028151540085673332, val loss: 0.07797457277774811\n",
      "Epoch 5169: train loss: 0.028722980991005898, val loss: 0.05492371320724487\n",
      "Epoch 5170: train loss: 0.028310440480709076, val loss: 0.06769952923059464\n",
      "Epoch 5171: train loss: 0.025115426629781723, val loss: 0.059226103127002716\n",
      "Epoch 5172: train loss: 0.02388165332376957, val loss: 0.06285174936056137\n",
      "Epoch 5173: train loss: 0.023750560358166695, val loss: 0.05750558525323868\n",
      "Epoch 5174: train loss: 0.016680588945746422, val loss: 0.07492487877607346\n",
      "Epoch 5175: train loss: 0.022062264382839203, val loss: 0.11673319339752197\n",
      "Epoch 5176: train loss: 0.01617615297436714, val loss: 0.09197302907705307\n",
      "Epoch 5177: train loss: 0.02066081017255783, val loss: 0.064609594643116\n",
      "Epoch 5178: train loss: 0.029530340805649757, val loss: 0.08335011452436447\n",
      "Epoch 5179: train loss: 0.020783836022019386, val loss: 0.07094325870275497\n",
      "Epoch 5180: train loss: 0.023780766874551773, val loss: 0.07375581562519073\n",
      "Epoch 5181: train loss: 0.017815954983234406, val loss: 0.09598521143198013\n",
      "Epoch 5182: train loss: 0.01860240288078785, val loss: 0.06962865591049194\n",
      "Epoch 5183: train loss: 0.02982828952372074, val loss: 0.0790986716747284\n",
      "Epoch 5184: train loss: 0.024635815992951393, val loss: 0.07886874675750732\n",
      "Epoch 5185: train loss: 0.02676502987742424, val loss: 0.11909456551074982\n",
      "Epoch 5186: train loss: 0.022876903414726257, val loss: 0.10648398846387863\n",
      "Epoch 5187: train loss: 0.020222850143909454, val loss: 0.07344561070203781\n",
      "Epoch 5188: train loss: 0.021549349650740623, val loss: 0.09011360257863998\n",
      "Epoch 5189: train loss: 0.0258362777531147, val loss: 0.0813836082816124\n",
      "Epoch 5190: train loss: 0.02664698101580143, val loss: 0.053441036492586136\n",
      "Epoch 5191: train loss: 0.01968380995094776, val loss: 0.08700753003358841\n",
      "Epoch 5192: train loss: 0.01582866720855236, val loss: 0.06919004768133163\n",
      "Epoch 5193: train loss: 0.021366801112890244, val loss: 0.0860551968216896\n",
      "Epoch 5194: train loss: 0.015951327979564667, val loss: 0.08593602478504181\n",
      "Epoch 5195: train loss: 0.014483045786619186, val loss: 0.060226988047361374\n",
      "Epoch 5196: train loss: 0.021057849749922752, val loss: 0.10291117429733276\n",
      "Epoch 5197: train loss: 0.012842517346143723, val loss: 0.08299615979194641\n",
      "Epoch 5198: train loss: 0.01909179985523224, val loss: 0.10346164554357529\n",
      "Epoch 5199: train loss: 0.016555074602365494, val loss: 0.0844436064362526\n",
      "Epoch 5200: train loss: 0.01793566532433033, val loss: 0.12160602957010269\n",
      "Epoch 5201: train loss: 0.020269479602575302, val loss: 0.04351460933685303\n",
      "Epoch 5202: train loss: 0.030041247606277466, val loss: 0.06338875740766525\n",
      "Epoch 5203: train loss: 0.015860557556152344, val loss: 0.09567014873027802\n",
      "Epoch 5204: train loss: 0.02390124276280403, val loss: 0.09102415293455124\n",
      "Epoch 5205: train loss: 0.02568851038813591, val loss: 0.10518129169940948\n",
      "Epoch 5206: train loss: 0.025696616619825363, val loss: 0.08118116855621338\n",
      "Epoch 5207: train loss: 0.01828564889729023, val loss: 0.09090632945299149\n",
      "Epoch 5208: train loss: 0.019102176651358604, val loss: 0.0479329489171505\n",
      "Epoch 5209: train loss: 0.021810878068208694, val loss: 0.05815885215997696\n",
      "Epoch 5210: train loss: 0.021255144849419594, val loss: 0.0744849219918251\n",
      "Epoch 5211: train loss: 0.025120971724390984, val loss: 0.08589266985654831\n",
      "Epoch 5212: train loss: 0.022265277802944183, val loss: 0.05520636960864067\n",
      "Epoch 5213: train loss: 0.019272327423095703, val loss: 0.07588598877191544\n",
      "Epoch 5214: train loss: 0.018608901649713516, val loss: 0.0845780298113823\n",
      "Epoch 5215: train loss: 0.021954426541924477, val loss: 0.08708585053682327\n",
      "Epoch 5216: train loss: 0.01928321085870266, val loss: 0.12599045038223267\n",
      "Epoch 5217: train loss: 0.01582915335893631, val loss: 0.07176779955625534\n",
      "Epoch 5218: train loss: 0.019061509519815445, val loss: 0.09531114995479584\n",
      "Epoch 5219: train loss: 0.015806101262569427, val loss: 0.08213221281766891\n",
      "Epoch 5220: train loss: 0.018733469769358635, val loss: 0.08340715616941452\n",
      "Epoch 5221: train loss: 0.02201315201818943, val loss: 0.08533238619565964\n",
      "Epoch 5222: train loss: 0.020295020192861557, val loss: 0.08897237479686737\n",
      "Epoch 5223: train loss: 0.022415651008486748, val loss: 0.07044797390699387\n",
      "Epoch 5224: train loss: 0.022570006549358368, val loss: 0.058891117572784424\n",
      "Epoch 5225: train loss: 0.025646865367889404, val loss: 0.03997162729501724\n",
      "Epoch 5226: train loss: 0.01950117014348507, val loss: 0.06661857664585114\n",
      "Epoch 5227: train loss: 0.02064702659845352, val loss: 0.04319421201944351\n",
      "Epoch 5228: train loss: 0.015570354647934437, val loss: 0.049732744693756104\n",
      "Epoch 5229: train loss: 0.020954104140400887, val loss: 0.12385113537311554\n",
      "Epoch 5230: train loss: 0.02370324544608593, val loss: 0.07706230878829956\n",
      "Epoch 5231: train loss: 0.016257667914032936, val loss: 0.09915412962436676\n",
      "Epoch 5232: train loss: 0.015812473371624947, val loss: 0.047658491879701614\n",
      "Epoch 5233: train loss: 0.02165454998612404, val loss: 0.09444012492895126\n",
      "Epoch 5234: train loss: 0.023596348240971565, val loss: 0.07822912186384201\n",
      "Epoch 5235: train loss: 0.015568563714623451, val loss: 0.06523895263671875\n",
      "Epoch 5236: train loss: 0.022418223321437836, val loss: 0.09122162312269211\n",
      "Epoch 5237: train loss: 0.028906358405947685, val loss: 0.08724561333656311\n",
      "Epoch 5238: train loss: 0.02058231271803379, val loss: 0.07362925261259079\n",
      "Epoch 5239: train loss: 0.022303467616438866, val loss: 0.08466235548257828\n",
      "Epoch 5240: train loss: 0.015372881665825844, val loss: 0.09317509084939957\n",
      "Epoch 5241: train loss: 0.02661532536149025, val loss: 0.07463878393173218\n",
      "Epoch 5242: train loss: 0.019960327073931694, val loss: 0.10135543346405029\n",
      "Epoch 5243: train loss: 0.01975858397781849, val loss: 0.06807577610015869\n",
      "Epoch 5244: train loss: 0.019779035821557045, val loss: 0.0712370052933693\n",
      "Epoch 5245: train loss: 0.022556114941835403, val loss: 0.044064413756132126\n",
      "Epoch 5246: train loss: 0.020646225661039352, val loss: 0.06924120336771011\n",
      "Epoch 5247: train loss: 0.021841924637556076, val loss: 0.10971540212631226\n",
      "Epoch 5248: train loss: 0.015651142224669456, val loss: 0.10147880762815475\n",
      "Epoch 5249: train loss: 0.02188933826982975, val loss: 0.07287770509719849\n",
      "Epoch 5250: train loss: 0.022588683292269707, val loss: 0.06782924383878708\n",
      "Epoch 5251: train loss: 0.01859886199235916, val loss: 0.07734925299882889\n",
      "Epoch 5252: train loss: 0.021024079993367195, val loss: 0.043646037578582764\n",
      "Epoch 5253: train loss: 0.01961897499859333, val loss: 0.10593798011541367\n",
      "Epoch 5254: train loss: 0.02187643013894558, val loss: 0.04699734225869179\n",
      "Epoch 5255: train loss: 0.016940511763095856, val loss: 0.08530087769031525\n",
      "Epoch 5256: train loss: 0.022439585998654366, val loss: 0.08597874641418457\n",
      "Epoch 5257: train loss: 0.023383591324090958, val loss: 0.137198805809021\n",
      "Epoch 5258: train loss: 0.02329092100262642, val loss: 0.060694675892591476\n",
      "Epoch 5259: train loss: 0.018720952793955803, val loss: 0.0724470391869545\n",
      "Epoch 5260: train loss: 0.01839950680732727, val loss: 0.053062554448843\n",
      "Epoch 5261: train loss: 0.015567795373499393, val loss: 0.08630739152431488\n",
      "Epoch 5262: train loss: 0.018341463059186935, val loss: 0.09548675268888474\n",
      "Epoch 5263: train loss: 0.01961061917245388, val loss: 0.08781442791223526\n",
      "Epoch 5264: train loss: 0.024059349671006203, val loss: 0.0651107206940651\n",
      "Epoch 5265: train loss: 0.020031245425343513, val loss: 0.10404049605131149\n",
      "Epoch 5266: train loss: 0.02070784382522106, val loss: 0.08259966224431992\n",
      "Epoch 5267: train loss: 0.023066887632012367, val loss: 0.04061882570385933\n",
      "Epoch 5268: train loss: 0.02462056279182434, val loss: 0.08821850270032883\n",
      "Epoch 5269: train loss: 0.01966470293700695, val loss: 0.10499631613492966\n",
      "Epoch 5270: train loss: 0.027500132098793983, val loss: 0.07701592147350311\n",
      "Epoch 5271: train loss: 0.02593275159597397, val loss: 0.04719656705856323\n",
      "Epoch 5272: train loss: 0.029317723587155342, val loss: 0.05583702400326729\n",
      "Epoch 5273: train loss: 0.018777310848236084, val loss: 0.12597812712192535\n",
      "Epoch 5274: train loss: 0.0180611964315176, val loss: 0.08440342545509338\n",
      "Epoch 5275: train loss: 0.017904767766594887, val loss: 0.059309959411621094\n",
      "Epoch 5276: train loss: 0.020445432513952255, val loss: 0.09118364751338959\n",
      "Epoch 5277: train loss: 0.018892021849751472, val loss: 0.04714112728834152\n",
      "Epoch 5278: train loss: 0.01890682429075241, val loss: 0.09428270161151886\n",
      "Epoch 5279: train loss: 0.027981115505099297, val loss: 0.08423691242933273\n",
      "Epoch 5280: train loss: 0.01866736263036728, val loss: 0.09145492315292358\n",
      "Epoch 5281: train loss: 0.020861497148871422, val loss: 0.08059412986040115\n",
      "Epoch 5282: train loss: 0.021969597786664963, val loss: 0.06237082555890083\n",
      "Epoch 5283: train loss: 0.026420095935463905, val loss: 0.09077145904302597\n",
      "Epoch 5284: train loss: 0.025858132168650627, val loss: 0.05445876345038414\n",
      "Epoch 5285: train loss: 0.01744496077299118, val loss: 0.059141624718904495\n",
      "Epoch 5286: train loss: 0.01997862197458744, val loss: 0.1269381195306778\n",
      "Epoch 5287: train loss: 0.02159479260444641, val loss: 0.06120876595377922\n",
      "Epoch 5288: train loss: 0.02494148164987564, val loss: 0.05173293501138687\n",
      "Epoch 5289: train loss: 0.016321858391165733, val loss: 0.07270439714193344\n",
      "Epoch 5290: train loss: 0.020867766812443733, val loss: 0.09974256157875061\n",
      "Epoch 5291: train loss: 0.023267392069101334, val loss: 0.07073061913251877\n",
      "Epoch 5292: train loss: 0.023036766797304153, val loss: 0.06428796052932739\n",
      "Epoch 5293: train loss: 0.018462684005498886, val loss: 0.09299467504024506\n",
      "Epoch 5294: train loss: 0.01629520021378994, val loss: 0.04345022141933441\n",
      "Epoch 5295: train loss: 0.026426218450069427, val loss: 0.03129402920603752\n",
      "Epoch 5296: train loss: 0.0323667973279953, val loss: 0.05457815155386925\n",
      "Epoch 5297: train loss: 0.02849825844168663, val loss: 0.055084358900785446\n",
      "Epoch 5298: train loss: 0.021633313968777657, val loss: 0.09480629116296768\n",
      "Epoch 5299: train loss: 0.02317298762500286, val loss: 0.06444471329450607\n",
      "Epoch 5300: train loss: 0.021530473604798317, val loss: 0.12482207268476486\n",
      "Epoch 5301: train loss: 0.023759206756949425, val loss: 0.08949293941259384\n",
      "Epoch 5302: train loss: 0.031241009011864662, val loss: 0.09643819183111191\n",
      "Epoch 5303: train loss: 0.022831976413726807, val loss: 0.07028607279062271\n",
      "Epoch 5304: train loss: 0.022601237520575523, val loss: 0.07017765194177628\n",
      "Epoch 5305: train loss: 0.02882346138358116, val loss: 0.07828842848539352\n",
      "Epoch 5306: train loss: 0.026053080335259438, val loss: 0.05436713621020317\n",
      "Epoch 5307: train loss: 0.01897123083472252, val loss: 0.09616030007600784\n",
      "Epoch 5308: train loss: 0.01988809183239937, val loss: 0.0686773732304573\n",
      "Epoch 5309: train loss: 0.026335570961236954, val loss: 0.07250282168388367\n",
      "Epoch 5310: train loss: 0.015772538259625435, val loss: 0.05862581357359886\n",
      "Epoch 5311: train loss: 0.026289770379662514, val loss: 0.09089485555887222\n",
      "Epoch 5312: train loss: 0.019347120076417923, val loss: 0.10945427417755127\n",
      "Epoch 5313: train loss: 0.022424010559916496, val loss: 0.04974173381924629\n",
      "Epoch 5314: train loss: 0.02114194817841053, val loss: 0.06450659781694412\n",
      "Epoch 5315: train loss: 0.01748829521238804, val loss: 0.09829003363847733\n",
      "Epoch 5316: train loss: 0.020375212654471397, val loss: 0.08134680241346359\n",
      "Epoch 5317: train loss: 0.02588711306452751, val loss: 0.06555401533842087\n",
      "Epoch 5318: train loss: 0.02079964056611061, val loss: 0.07150495797395706\n",
      "Epoch 5319: train loss: 0.022018656134605408, val loss: 0.03800089284777641\n",
      "Epoch 5320: train loss: 0.02033630758523941, val loss: 0.07572304457426071\n",
      "Epoch 5321: train loss: 0.01927119493484497, val loss: 0.06213514879345894\n",
      "Epoch 5322: train loss: 0.023468291386961937, val loss: 0.08104115724563599\n",
      "Epoch 5323: train loss: 0.01824290305376053, val loss: 0.053253915160894394\n",
      "Epoch 5324: train loss: 0.024314234033226967, val loss: 0.047150593250989914\n",
      "Epoch 5325: train loss: 0.02515062876045704, val loss: 0.07972439378499985\n",
      "Epoch 5326: train loss: 0.021311983466148376, val loss: 0.06139184162020683\n",
      "Epoch 5327: train loss: 0.018217215314507484, val loss: 0.11954398453235626\n",
      "Epoch 5328: train loss: 0.01656441017985344, val loss: 0.0867329016327858\n",
      "Epoch 5329: train loss: 0.02214270457625389, val loss: 0.08445168286561966\n",
      "Epoch 5330: train loss: 0.016812337562441826, val loss: 0.05325967073440552\n",
      "Epoch 5331: train loss: 0.024037543684244156, val loss: 0.07224364578723907\n",
      "Epoch 5332: train loss: 0.02587234228849411, val loss: 0.07764293253421783\n",
      "Epoch 5333: train loss: 0.01818784698843956, val loss: 0.06452658027410507\n",
      "Epoch 5334: train loss: 0.020611710846424103, val loss: 0.06884220242500305\n",
      "Epoch 5335: train loss: 0.023705122992396355, val loss: 0.048706840723752975\n",
      "Epoch 5336: train loss: 0.02304627187550068, val loss: 0.08587703853845596\n",
      "Epoch 5337: train loss: 0.017668602988123894, val loss: 0.11763717234134674\n",
      "Epoch 5338: train loss: 0.019518310204148293, val loss: 0.08486925810575485\n",
      "Epoch 5339: train loss: 0.0179701317101717, val loss: 0.08606239408254623\n",
      "Epoch 5340: train loss: 0.020528091117739677, val loss: 0.037402763962745667\n",
      "Epoch 5341: train loss: 0.02078859880566597, val loss: 0.08446382731199265\n",
      "Epoch 5342: train loss: 0.01995491050183773, val loss: 0.05615311861038208\n",
      "Epoch 5343: train loss: 0.021955296397209167, val loss: 0.09006872028112411\n",
      "Epoch 5344: train loss: 0.022633984684944153, val loss: 0.050658244639635086\n",
      "Epoch 5345: train loss: 0.02142912894487381, val loss: 0.0954042449593544\n",
      "Epoch 5346: train loss: 0.02599279396235943, val loss: 0.09292643517255783\n",
      "Epoch 5347: train loss: 0.024453015998005867, val loss: 0.09021653234958649\n",
      "Epoch 5348: train loss: 0.017155464738607407, val loss: 0.07001631706953049\n",
      "Epoch 5349: train loss: 0.018268171697854996, val loss: 0.09082527458667755\n",
      "Epoch 5350: train loss: 0.0252073984593153, val loss: 0.05191757157444954\n",
      "Epoch 5351: train loss: 0.025690259411931038, val loss: 0.09266570210456848\n",
      "Epoch 5352: train loss: 0.018458383157849312, val loss: 0.06087822839617729\n",
      "Epoch 5353: train loss: 0.02213347889482975, val loss: 0.07040820270776749\n",
      "Epoch 5354: train loss: 0.020975681021809578, val loss: 0.07563270628452301\n",
      "Epoch 5355: train loss: 0.024101711809635162, val loss: 0.07507959008216858\n",
      "Epoch 5356: train loss: 0.021679675206542015, val loss: 0.05979926511645317\n",
      "Epoch 5357: train loss: 0.027485260739922523, val loss: 0.05745350942015648\n",
      "Epoch 5358: train loss: 0.02505558729171753, val loss: 0.07398112863302231\n",
      "Epoch 5359: train loss: 0.018708841875195503, val loss: 0.08002673834562302\n",
      "Epoch 5360: train loss: 0.024520602077245712, val loss: 0.09201022237539291\n",
      "Epoch 5361: train loss: 0.022534243762493134, val loss: 0.08392039686441422\n",
      "Epoch 5362: train loss: 0.020002782344818115, val loss: 0.054878950119018555\n",
      "Epoch 5363: train loss: 0.019848832860589027, val loss: 0.055493175983428955\n",
      "Epoch 5364: train loss: 0.018080076202750206, val loss: 0.06973890215158463\n",
      "Epoch 5365: train loss: 0.01921628974378109, val loss: 0.09971683472394943\n",
      "Epoch 5366: train loss: 0.021577825769782066, val loss: 0.08437178283929825\n",
      "Epoch 5367: train loss: 0.025639217346906662, val loss: 0.08486159890890121\n",
      "Epoch 5368: train loss: 0.01888125203549862, val loss: 0.08919121325016022\n",
      "Epoch 5369: train loss: 0.018504155799746513, val loss: 0.04892313852906227\n",
      "Epoch 5370: train loss: 0.013817040249705315, val loss: 0.06411529332399368\n",
      "Epoch 5371: train loss: 0.017931295558810234, val loss: 0.09318847954273224\n",
      "Epoch 5372: train loss: 0.016253922134637833, val loss: 0.08471404761075974\n",
      "Epoch 5373: train loss: 0.01830023154616356, val loss: 0.08687818795442581\n",
      "Epoch 5374: train loss: 0.021082105115056038, val loss: 0.0751902386546135\n",
      "Epoch 5375: train loss: 0.020184272900223732, val loss: 0.1224854364991188\n",
      "Epoch 5376: train loss: 0.019876789301633835, val loss: 0.1262224316596985\n",
      "Epoch 5377: train loss: 0.019879300147294998, val loss: 0.06077248975634575\n",
      "Epoch 5378: train loss: 0.015379436314105988, val loss: 0.09702646732330322\n",
      "Epoch 5379: train loss: 0.01746109127998352, val loss: 0.0766596868634224\n",
      "Epoch 5380: train loss: 0.01592053845524788, val loss: 0.08911701291799545\n",
      "Epoch 5381: train loss: 0.024260060861706734, val loss: 0.07274637371301651\n",
      "Epoch 5382: train loss: 0.02616034634411335, val loss: 0.07269207388162613\n",
      "Epoch 5383: train loss: 0.020473796874284744, val loss: 0.061213720589876175\n",
      "Epoch 5384: train loss: 0.015730205923318863, val loss: 0.06270868331193924\n",
      "Epoch 5385: train loss: 0.018750401213765144, val loss: 0.05278656631708145\n",
      "Epoch 5386: train loss: 0.013484964147210121, val loss: 0.07946315407752991\n",
      "Epoch 5387: train loss: 0.017199629917740822, val loss: 0.08726167678833008\n",
      "Epoch 5388: train loss: 0.013889518566429615, val loss: 0.05649835988879204\n",
      "Epoch 5389: train loss: 0.014254193753004074, val loss: 0.08747600018978119\n",
      "Epoch 5390: train loss: 0.02684680186212063, val loss: 0.08173462003469467\n",
      "Epoch 5391: train loss: 0.024364976212382317, val loss: 0.06479309499263763\n",
      "Epoch 5392: train loss: 0.024644607678055763, val loss: 0.04025419056415558\n",
      "Epoch 5393: train loss: 0.02208312228322029, val loss: 0.0740261897444725\n",
      "Epoch 5394: train loss: 0.016148267313838005, val loss: 0.06494133174419403\n",
      "Epoch 5395: train loss: 0.022836796939373016, val loss: 0.0858062356710434\n",
      "Epoch 5396: train loss: 0.021135669201612473, val loss: 0.07675659656524658\n",
      "Epoch 5397: train loss: 0.02266746200621128, val loss: 0.08018642663955688\n",
      "Epoch 5398: train loss: 0.022983413189649582, val loss: 0.0635957345366478\n",
      "Epoch 5399: train loss: 0.01906968094408512, val loss: 0.06263487786054611\n",
      "Epoch 5400: train loss: 0.019114896655082703, val loss: 0.10100751370191574\n",
      "Epoch 5401: train loss: 0.013460255227982998, val loss: 0.07490217685699463\n",
      "Epoch 5402: train loss: 0.022429397329688072, val loss: 0.07410118728876114\n",
      "Epoch 5403: train loss: 0.02427096478641033, val loss: 0.09014303237199783\n",
      "Epoch 5404: train loss: 0.021026039496064186, val loss: 0.1031481996178627\n",
      "Epoch 5405: train loss: 0.019935287535190582, val loss: 0.07123517990112305\n",
      "Epoch 5406: train loss: 0.015220532193779945, val loss: 0.08519434928894043\n",
      "Epoch 5407: train loss: 0.018050266429781914, val loss: 0.08698340505361557\n",
      "Epoch 5408: train loss: 0.017182596027851105, val loss: 0.10194269567728043\n",
      "Epoch 5409: train loss: 0.0175555981695652, val loss: 0.07103773206472397\n",
      "Epoch 5410: train loss: 0.023599430918693542, val loss: 0.061569154262542725\n",
      "Epoch 5411: train loss: 0.02345721796154976, val loss: 0.026034018024802208\n",
      "Epoch 5412: train loss: 0.020007116720080376, val loss: 0.07867595553398132\n",
      "Epoch 5413: train loss: 0.014818096533417702, val loss: 0.05101877450942993\n",
      "Epoch 5414: train loss: 0.023665200918912888, val loss: 0.08289166539907455\n",
      "Epoch 5415: train loss: 0.014912368729710579, val loss: 0.06157384067773819\n",
      "Epoch 5416: train loss: 0.014975917525589466, val loss: 0.04656437039375305\n",
      "Epoch 5417: train loss: 0.01888551004230976, val loss: 0.07287484407424927\n",
      "Epoch 5418: train loss: 0.022901777178049088, val loss: 0.06814533472061157\n",
      "Epoch 5419: train loss: 0.018448250368237495, val loss: 0.0678887888789177\n",
      "Epoch 5420: train loss: 0.015088334679603577, val loss: 0.09526516497135162\n",
      "Epoch 5421: train loss: 0.022399751469492912, val loss: 0.041123297065496445\n",
      "Epoch 5422: train loss: 0.017935901880264282, val loss: 0.05771174654364586\n",
      "Epoch 5423: train loss: 0.02429930493235588, val loss: 0.059390075504779816\n",
      "Epoch 5424: train loss: 0.018691258504986763, val loss: 0.10795557498931885\n",
      "Epoch 5425: train loss: 0.02024098113179207, val loss: 0.0741121917963028\n",
      "Epoch 5426: train loss: 0.02592048980295658, val loss: 0.07906161993741989\n",
      "Epoch 5427: train loss: 0.014659539796411991, val loss: 0.060328781604766846\n",
      "Epoch 5428: train loss: 0.027528297156095505, val loss: 0.0615069642663002\n",
      "Epoch 5429: train loss: 0.021701550111174583, val loss: 0.06887564808130264\n",
      "Epoch 5430: train loss: 0.017676251009106636, val loss: 0.04746074602007866\n",
      "Epoch 5431: train loss: 0.018089938908815384, val loss: 0.09091012924909592\n",
      "Epoch 5432: train loss: 0.018718920648097992, val loss: 0.1047222837805748\n",
      "Epoch 5433: train loss: 0.020619530230760574, val loss: 0.06740702688694\n",
      "Epoch 5434: train loss: 0.017508236691355705, val loss: 0.025347506627440453\n",
      "Epoch 5435: train loss: 0.022645780816674232, val loss: 0.10398104041814804\n",
      "Epoch 5436: train loss: 0.018380703404545784, val loss: 0.0954127088189125\n",
      "Epoch 5437: train loss: 0.016984347254037857, val loss: 0.09842316806316376\n",
      "Epoch 5438: train loss: 0.016346348449587822, val loss: 0.07214656472206116\n",
      "Epoch 5439: train loss: 0.018458247184753418, val loss: 0.09335830062627792\n",
      "Epoch 5440: train loss: 0.022618921473622322, val loss: 0.09637296944856644\n",
      "Epoch 5441: train loss: 0.012748023495078087, val loss: 0.07822471112012863\n",
      "Epoch 5442: train loss: 0.02837163396179676, val loss: 0.07519584894180298\n",
      "Epoch 5443: train loss: 0.01366222184151411, val loss: 0.07345361262559891\n",
      "Epoch 5444: train loss: 0.013400002382695675, val loss: 0.11976519972085953\n",
      "Epoch 5445: train loss: 0.022666383534669876, val loss: 0.08174227923154831\n",
      "Epoch 5446: train loss: 0.015699654817581177, val loss: 0.08356912434101105\n",
      "Epoch 5447: train loss: 0.022319963201880455, val loss: 0.07460971176624298\n",
      "Epoch 5448: train loss: 0.021656429395079613, val loss: 0.06880491226911545\n",
      "Epoch 5449: train loss: 0.015627220273017883, val loss: 0.07052502036094666\n",
      "Epoch 5450: train loss: 0.014822234399616718, val loss: 0.084959015250206\n",
      "Epoch 5451: train loss: 0.013955417089164257, val loss: 0.12097125500440598\n",
      "Epoch 5452: train loss: 0.02153736725449562, val loss: 0.11063625663518906\n",
      "Epoch 5453: train loss: 0.018814271315932274, val loss: 0.07862897962331772\n",
      "Epoch 5454: train loss: 0.017123334109783173, val loss: 0.11451595276594162\n",
      "Epoch 5455: train loss: 0.01919778250157833, val loss: 0.06304030865430832\n",
      "Epoch 5456: train loss: 0.020678970962762833, val loss: 0.11483579128980637\n",
      "Epoch 5457: train loss: 0.01733557879924774, val loss: 0.051378317177295685\n",
      "Epoch 5458: train loss: 0.01520551834255457, val loss: 0.08479243516921997\n",
      "Epoch 5459: train loss: 0.024127066135406494, val loss: 0.07279794663190842\n",
      "Epoch 5460: train loss: 0.009677889756858349, val loss: 0.08650340884923935\n",
      "Epoch 5461: train loss: 0.019857145845890045, val loss: 0.11667364090681076\n",
      "Epoch 5462: train loss: 0.0166418869048357, val loss: 0.09197162836790085\n",
      "Epoch 5463: train loss: 0.01609186828136444, val loss: 0.09270186722278595\n",
      "Epoch 5464: train loss: 0.019491443410515785, val loss: 0.065040722489357\n",
      "Epoch 5465: train loss: 0.020904982462525368, val loss: 0.07221880555152893\n",
      "Epoch 5466: train loss: 0.022265546023845673, val loss: 0.09526259452104568\n",
      "Epoch 5467: train loss: 0.020182274281978607, val loss: 0.0713617280125618\n",
      "Epoch 5468: train loss: 0.014718477614223957, val loss: 0.056088220328092575\n",
      "Epoch 5469: train loss: 0.016742506995797157, val loss: 0.07693717628717422\n",
      "Epoch 5470: train loss: 0.020676830783486366, val loss: 0.07869851589202881\n",
      "Epoch 5471: train loss: 0.013501374050974846, val loss: 0.09551437944173813\n",
      "Epoch 5472: train loss: 0.01739630475640297, val loss: 0.11499116569757462\n",
      "Epoch 5473: train loss: 0.018000299111008644, val loss: 0.04926908388733864\n",
      "Epoch 5474: train loss: 0.013604875653982162, val loss: 0.06756371259689331\n",
      "Epoch 5475: train loss: 0.019159182906150818, val loss: 0.07463384419679642\n",
      "Epoch 5476: train loss: 0.018344929441809654, val loss: 0.06321948021650314\n",
      "Epoch 5477: train loss: 0.02199130691587925, val loss: 0.08518089354038239\n",
      "Epoch 5478: train loss: 0.022251347079873085, val loss: 0.08209949731826782\n",
      "Epoch 5479: train loss: 0.020239219069480896, val loss: 0.07373564690351486\n",
      "Epoch 5480: train loss: 0.017404353246092796, val loss: 0.06826797872781754\n",
      "Epoch 5481: train loss: 0.018221113830804825, val loss: 0.0573713593184948\n",
      "Epoch 5482: train loss: 0.02027469500899315, val loss: 0.057286765426397324\n",
      "Epoch 5483: train loss: 0.0188777856528759, val loss: 0.07277947664260864\n",
      "Epoch 5484: train loss: 0.02102629281580448, val loss: 0.07502241432666779\n",
      "Epoch 5485: train loss: 0.026005711406469345, val loss: 0.036804016679525375\n",
      "Epoch 5486: train loss: 0.018794922158122063, val loss: 0.09238849580287933\n",
      "Epoch 5487: train loss: 0.01929418370127678, val loss: 0.16141803562641144\n",
      "Epoch 5488: train loss: 0.020098451524972916, val loss: 0.09018149226903915\n",
      "Epoch 5489: train loss: 0.018411576747894287, val loss: 0.08783122152090073\n",
      "Epoch 5490: train loss: 0.01597418636083603, val loss: 0.06181515380740166\n",
      "Epoch 5491: train loss: 0.018828345462679863, val loss: 0.04902457818388939\n",
      "Epoch 5492: train loss: 0.030632009729743004, val loss: 0.10328202694654465\n",
      "Epoch 5493: train loss: 0.017751015722751617, val loss: 0.11176484078168869\n",
      "Epoch 5494: train loss: 0.020588411018252373, val loss: 0.09642747789621353\n",
      "Epoch 5495: train loss: 0.018914468586444855, val loss: 0.048038993030786514\n",
      "Epoch 5496: train loss: 0.015620993450284004, val loss: 0.07273638993501663\n",
      "Epoch 5497: train loss: 0.017681628465652466, val loss: 0.09612282365560532\n",
      "Epoch 5498: train loss: 0.020280715078115463, val loss: 0.1090998649597168\n",
      "Epoch 5499: train loss: 0.01867527700960636, val loss: 0.061560727655887604\n",
      "Epoch 5500: train loss: 0.018042366951704025, val loss: 0.06515252590179443\n",
      "Epoch 5501: train loss: 0.023797044530510902, val loss: 0.10505195707082748\n",
      "Epoch 5502: train loss: 0.022011268883943558, val loss: 0.05993089824914932\n",
      "Epoch 5503: train loss: 0.01759193278849125, val loss: 0.07766132056713104\n",
      "Epoch 5504: train loss: 0.022954480722546577, val loss: 0.03592905402183533\n",
      "Epoch 5505: train loss: 0.0215169545263052, val loss: 0.054431892931461334\n",
      "Epoch 5506: train loss: 0.019758036360144615, val loss: 0.08728934079408646\n",
      "Epoch 5507: train loss: 0.021445022895932198, val loss: 0.048251207917928696\n",
      "Epoch 5508: train loss: 0.019427629187703133, val loss: 0.061051707714796066\n",
      "Epoch 5509: train loss: 0.01885479874908924, val loss: 0.0384298712015152\n",
      "Epoch 5510: train loss: 0.028041718527674675, val loss: 0.06251990795135498\n",
      "Epoch 5511: train loss: 0.030301783233880997, val loss: 0.04957812651991844\n",
      "Epoch 5512: train loss: 0.013939771801233292, val loss: 0.0759437307715416\n",
      "Epoch 5513: train loss: 0.018795151263475418, val loss: 0.06329499930143356\n",
      "Epoch 5514: train loss: 0.02070709690451622, val loss: 0.07517217844724655\n",
      "Epoch 5515: train loss: 0.011458389461040497, val loss: 0.10459043085575104\n",
      "Epoch 5516: train loss: 0.02396887168288231, val loss: 0.07513850927352905\n",
      "Epoch 5517: train loss: 0.025350015610456467, val loss: 0.06336744874715805\n",
      "Epoch 5518: train loss: 0.0232642013579607, val loss: 0.07729370146989822\n",
      "Epoch 5519: train loss: 0.026992665603756905, val loss: 0.07704528421163559\n",
      "Epoch 5520: train loss: 0.025219663977622986, val loss: 0.06981982290744781\n",
      "Epoch 5521: train loss: 0.017572391778230667, val loss: 0.05604953691363335\n",
      "Epoch 5522: train loss: 0.021209711208939552, val loss: 0.08321280032396317\n",
      "Epoch 5523: train loss: 0.021808389574289322, val loss: 0.08306143432855606\n",
      "Epoch 5524: train loss: 0.01653444580733776, val loss: 0.045530494302511215\n",
      "Epoch 5525: train loss: 0.01651759445667267, val loss: 0.12826628983020782\n",
      "Epoch 5526: train loss: 0.02166561409831047, val loss: 0.08790234476327896\n",
      "Epoch 5527: train loss: 0.024462854489684105, val loss: 0.060167666524648666\n",
      "Epoch 5528: train loss: 0.02140302211046219, val loss: 0.06545001268386841\n",
      "Epoch 5529: train loss: 0.020548461005091667, val loss: 0.06518515199422836\n",
      "Epoch 5530: train loss: 0.025794781744480133, val loss: 0.07615378499031067\n",
      "Epoch 5531: train loss: 0.021020378917455673, val loss: 0.07419291138648987\n",
      "Epoch 5532: train loss: 0.02127811498939991, val loss: 0.04413433000445366\n",
      "Epoch 5533: train loss: 0.019558535888791084, val loss: 0.07182427495718002\n",
      "Epoch 5534: train loss: 0.02207467146217823, val loss: 0.10422410815954208\n",
      "Epoch 5535: train loss: 0.025870157405734062, val loss: 0.07051129639148712\n",
      "Epoch 5536: train loss: 0.018742434680461884, val loss: 0.07473864406347275\n",
      "Epoch 5537: train loss: 0.014089571312069893, val loss: 0.06640633195638657\n",
      "Epoch 5538: train loss: 0.01539465133100748, val loss: 0.07592951506376266\n",
      "Epoch 5539: train loss: 0.01849420927464962, val loss: 0.05065440759062767\n",
      "Epoch 5540: train loss: 0.019070342183113098, val loss: 0.0666273906826973\n",
      "Epoch 5541: train loss: 0.014928462915122509, val loss: 0.08024974912405014\n",
      "Epoch 5542: train loss: 0.023154810070991516, val loss: 0.0769519954919815\n",
      "Epoch 5543: train loss: 0.023992221802473068, val loss: 0.051209039986133575\n",
      "Epoch 5544: train loss: 0.01681661605834961, val loss: 0.05019889399409294\n",
      "Epoch 5545: train loss: 0.024077745154500008, val loss: 0.07619865983724594\n",
      "Epoch 5546: train loss: 0.016374913975596428, val loss: 0.08888484537601471\n",
      "Epoch 5547: train loss: 0.019796986132860184, val loss: 0.05674923211336136\n",
      "Epoch 5548: train loss: 0.023211520165205002, val loss: 0.05417921766638756\n",
      "Epoch 5549: train loss: 0.019486572593450546, val loss: 0.06124049425125122\n",
      "Epoch 5550: train loss: 0.022895565256476402, val loss: 0.05826438590884209\n",
      "Epoch 5551: train loss: 0.023998016491532326, val loss: 0.09752444177865982\n",
      "Epoch 5552: train loss: 0.02482498623430729, val loss: 0.07790172100067139\n",
      "Epoch 5553: train loss: 0.01734398677945137, val loss: 0.08607833087444305\n",
      "Epoch 5554: train loss: 0.021799499168992043, val loss: 0.09083851426839828\n",
      "Epoch 5555: train loss: 0.014789135195314884, val loss: 0.08556000143289566\n",
      "Epoch 5556: train loss: 0.01935640536248684, val loss: 0.06159413978457451\n",
      "Epoch 5557: train loss: 0.01763392798602581, val loss: 0.06430001556873322\n",
      "Epoch 5558: train loss: 0.021692894399166107, val loss: 0.05535304173827171\n",
      "Epoch 5559: train loss: 0.01743033528327942, val loss: 0.046386297792196274\n",
      "Epoch 5560: train loss: 0.017130296677350998, val loss: 0.04633414000272751\n",
      "Epoch 5561: train loss: 0.01961423270404339, val loss: 0.039630234241485596\n",
      "Epoch 5562: train loss: 0.018149640411138535, val loss: 0.09626821428537369\n",
      "Epoch 5563: train loss: 0.014800256118178368, val loss: 0.08016970008611679\n",
      "Epoch 5564: train loss: 0.025783846154808998, val loss: 0.06604234129190445\n",
      "Epoch 5565: train loss: 0.01674698479473591, val loss: 0.057898759841918945\n",
      "Epoch 5566: train loss: 0.027810558676719666, val loss: 0.08614782243967056\n",
      "Epoch 5567: train loss: 0.01848999224603176, val loss: 0.08700552582740784\n",
      "Epoch 5568: train loss: 0.026868360117077827, val loss: 0.09581746906042099\n",
      "Epoch 5569: train loss: 0.029726527631282806, val loss: 0.07255053520202637\n",
      "Epoch 5570: train loss: 0.021110279485583305, val loss: 0.08840232342481613\n",
      "Epoch 5571: train loss: 0.02844623476266861, val loss: 0.0804009884595871\n",
      "Epoch 5572: train loss: 0.018179859966039658, val loss: 0.08454755693674088\n",
      "Epoch 5573: train loss: 0.035651493817567825, val loss: 0.10723650455474854\n",
      "Epoch 5574: train loss: 0.02034946158528328, val loss: 0.10842036455869675\n",
      "Epoch 5575: train loss: 0.02069135382771492, val loss: 0.07143320143222809\n",
      "Epoch 5576: train loss: 0.02255694568157196, val loss: 0.08062928915023804\n",
      "Epoch 5577: train loss: 0.02873385325074196, val loss: 0.04897474870085716\n",
      "Epoch 5578: train loss: 0.017346903681755066, val loss: 0.07279254496097565\n",
      "Epoch 5579: train loss: 0.023135898634791374, val loss: 0.07653120905160904\n",
      "Epoch 5580: train loss: 0.023981953039765358, val loss: 0.10200915485620499\n",
      "Epoch 5581: train loss: 0.021311845630407333, val loss: 0.08759863674640656\n",
      "Epoch 5582: train loss: 0.025073526427149773, val loss: 0.12469136714935303\n",
      "Epoch 5583: train loss: 0.01685461774468422, val loss: 0.07927315682172775\n",
      "Epoch 5584: train loss: 0.021438250318169594, val loss: 0.1117912158370018\n",
      "Epoch 5585: train loss: 0.01962060108780861, val loss: 0.06817925721406937\n",
      "Epoch 5586: train loss: 0.02004878781735897, val loss: 0.07259195297956467\n",
      "Epoch 5587: train loss: 0.022690048441290855, val loss: 0.14479513466358185\n",
      "Epoch 5588: train loss: 0.023932723328471184, val loss: 0.09030298888683319\n",
      "Epoch 5589: train loss: 0.028446441516280174, val loss: 0.0904257670044899\n",
      "Epoch 5590: train loss: 0.021825652569532394, val loss: 0.06027253344655037\n",
      "Epoch 5591: train loss: 0.020445099100470543, val loss: 0.061876267194747925\n",
      "Epoch 5592: train loss: 0.03065871261060238, val loss: 0.12918733060359955\n",
      "Epoch 5593: train loss: 0.023038335144519806, val loss: 0.0786309614777565\n",
      "Epoch 5594: train loss: 0.01450097095221281, val loss: 0.060602784156799316\n",
      "Epoch 5595: train loss: 0.0201917365193367, val loss: 0.0890619233250618\n",
      "Epoch 5596: train loss: 0.020953258499503136, val loss: 0.06373824924230576\n",
      "Epoch 5597: train loss: 0.016785332933068275, val loss: 0.06927032023668289\n",
      "Epoch 5598: train loss: 0.021375266835093498, val loss: 0.06999175995588303\n",
      "Epoch 5599: train loss: 0.023308202624320984, val loss: 0.11763546615839005\n",
      "Epoch 5600: train loss: 0.01954558491706848, val loss: 0.06773025542497635\n",
      "Epoch 5601: train loss: 0.020991303026676178, val loss: 0.10690426081418991\n",
      "Epoch 5602: train loss: 0.01786036603152752, val loss: 0.06956803053617477\n",
      "Epoch 5603: train loss: 0.022845618426799774, val loss: 0.04022825509309769\n",
      "Epoch 5604: train loss: 0.020546704530715942, val loss: 0.1259872168302536\n",
      "Epoch 5605: train loss: 0.020172161981463432, val loss: 0.08950785547494888\n",
      "Epoch 5606: train loss: 0.030220242217183113, val loss: 0.048651307821273804\n",
      "Epoch 5607: train loss: 0.0250647384673357, val loss: 0.03284713253378868\n",
      "Epoch 5608: train loss: 0.01772891916334629, val loss: 0.07014559209346771\n",
      "Epoch 5609: train loss: 0.01516918744891882, val loss: 0.07803358882665634\n",
      "Epoch 5610: train loss: 0.01996798627078533, val loss: 0.08258426189422607\n",
      "Epoch 5611: train loss: 0.016489766538143158, val loss: 0.059611499309539795\n",
      "Epoch 5612: train loss: 0.024823063984513283, val loss: 0.08053538203239441\n",
      "Epoch 5613: train loss: 0.021554991602897644, val loss: 0.06368165463209152\n",
      "Epoch 5614: train loss: 0.024127857759594917, val loss: 0.06556294858455658\n",
      "Epoch 5615: train loss: 0.014899174682796001, val loss: 0.060213178396224976\n",
      "Epoch 5616: train loss: 0.015893589705228806, val loss: 0.07751475274562836\n",
      "Epoch 5617: train loss: 0.020369742065668106, val loss: 0.046898044645786285\n",
      "Epoch 5618: train loss: 0.016204815357923508, val loss: 0.1028670072555542\n",
      "Epoch 5619: train loss: 0.021753020584583282, val loss: 0.06088500842452049\n",
      "Epoch 5620: train loss: 0.024917179718613625, val loss: 0.07882040739059448\n",
      "Epoch 5621: train loss: 0.02091783843934536, val loss: 0.058077406138181686\n",
      "Epoch 5622: train loss: 0.02198234759271145, val loss: 0.04723311960697174\n",
      "Epoch 5623: train loss: 0.020586449652910233, val loss: 0.0663125142455101\n",
      "Epoch 5624: train loss: 0.023260042071342468, val loss: 0.052480507642030716\n",
      "Epoch 5625: train loss: 0.02206421084702015, val loss: 0.05315235257148743\n",
      "Epoch 5626: train loss: 0.025707807391881943, val loss: 0.07868189364671707\n",
      "Epoch 5627: train loss: 0.017436062917113304, val loss: 0.08054816722869873\n",
      "Epoch 5628: train loss: 0.017542893067002296, val loss: 0.08063413947820663\n",
      "Epoch 5629: train loss: 0.01657264493405819, val loss: 0.04989979788661003\n",
      "Epoch 5630: train loss: 0.02157689444720745, val loss: 0.15480606257915497\n",
      "Epoch 5631: train loss: 0.01686018891632557, val loss: 0.04767884686589241\n",
      "Epoch 5632: train loss: 0.017057077959179878, val loss: 0.08157595247030258\n",
      "Epoch 5633: train loss: 0.01781420223414898, val loss: 0.08040131628513336\n",
      "Epoch 5634: train loss: 0.024916797876358032, val loss: 0.06738052517175674\n",
      "Epoch 5635: train loss: 0.01816200651228428, val loss: 0.08473499864339828\n",
      "Epoch 5636: train loss: 0.01764727383852005, val loss: 0.10424653440713882\n",
      "Epoch 5637: train loss: 0.016111506149172783, val loss: 0.05284692719578743\n",
      "Epoch 5638: train loss: 0.021490950137376785, val loss: 0.07077546417713165\n",
      "Epoch 5639: train loss: 0.018948610872030258, val loss: 0.0756402313709259\n",
      "Epoch 5640: train loss: 0.02687664143741131, val loss: 0.07497941702604294\n",
      "Epoch 5641: train loss: 0.021321164444088936, val loss: 0.0956776961684227\n",
      "Epoch 5642: train loss: 0.01759466901421547, val loss: 0.08869149535894394\n",
      "Epoch 5643: train loss: 0.026768941432237625, val loss: 0.060518670827150345\n",
      "Epoch 5644: train loss: 0.030844174325466156, val loss: 0.11521416157484055\n",
      "Epoch 5645: train loss: 0.019968688488006592, val loss: 0.08273763954639435\n",
      "Epoch 5646: train loss: 0.02092316932976246, val loss: 0.07615742087364197\n",
      "Epoch 5647: train loss: 0.021902326494455338, val loss: 0.06509660929441452\n",
      "Epoch 5648: train loss: 0.026767272502183914, val loss: 0.10566402971744537\n",
      "Epoch 5649: train loss: 0.027368875220417976, val loss: 0.09639137238264084\n",
      "Epoch 5650: train loss: 0.023845277726650238, val loss: 0.06946586072444916\n",
      "Epoch 5651: train loss: 0.02892586961388588, val loss: 0.08121214807033539\n",
      "Epoch 5652: train loss: 0.024251334369182587, val loss: 0.08392041176557541\n",
      "Epoch 5653: train loss: 0.03009885735809803, val loss: 0.06668074429035187\n",
      "Epoch 5654: train loss: 0.02221306972205639, val loss: 0.067592553794384\n",
      "Epoch 5655: train loss: 0.023118725046515465, val loss: 0.10205342620611191\n",
      "Epoch 5656: train loss: 0.032179057598114014, val loss: 0.10992338508367538\n",
      "Epoch 5657: train loss: 0.024147432297468185, val loss: 0.0678192526102066\n",
      "Epoch 5658: train loss: 0.025123922154307365, val loss: 0.0985628142952919\n",
      "Epoch 5659: train loss: 0.01716948300600052, val loss: 0.044884901493787766\n",
      "Epoch 5660: train loss: 0.021608509123325348, val loss: 0.0330958254635334\n",
      "Epoch 5661: train loss: 0.026861265301704407, val loss: 0.03889378532767296\n",
      "Epoch 5662: train loss: 0.02214636094868183, val loss: 0.09636131674051285\n",
      "Epoch 5663: train loss: 0.02216959185898304, val loss: 0.03589750453829765\n",
      "Epoch 5664: train loss: 0.024037351831793785, val loss: 0.06292971223592758\n",
      "Epoch 5665: train loss: 0.017778735607862473, val loss: 0.11011157184839249\n",
      "Epoch 5666: train loss: 0.021007128059864044, val loss: 0.0927518755197525\n",
      "Epoch 5667: train loss: 0.01577799580991268, val loss: 0.07373040169477463\n",
      "Epoch 5668: train loss: 0.03072839416563511, val loss: 0.06781556457281113\n",
      "Epoch 5669: train loss: 0.020885787904262543, val loss: 0.0789036974310875\n",
      "Epoch 5670: train loss: 0.018792958930134773, val loss: 0.0789499580860138\n",
      "Epoch 5671: train loss: 0.0226069875061512, val loss: 0.09401491284370422\n",
      "Epoch 5672: train loss: 0.02513098157942295, val loss: 0.061394330114126205\n",
      "Epoch 5673: train loss: 0.01715332269668579, val loss: 0.055035848170518875\n",
      "Epoch 5674: train loss: 0.018055709078907967, val loss: 0.09154614061117172\n",
      "Epoch 5675: train loss: 0.01930585317313671, val loss: 0.07316748052835464\n",
      "Epoch 5676: train loss: 0.020469211041927338, val loss: 0.1254076212644577\n",
      "Epoch 5677: train loss: 0.029831547290086746, val loss: 0.07804688066244125\n",
      "Epoch 5678: train loss: 0.0241412166506052, val loss: 0.09065701067447662\n",
      "Epoch 5679: train loss: 0.024471838027238846, val loss: 0.04024685546755791\n",
      "Epoch 5680: train loss: 0.028010092675685883, val loss: 0.08282812684774399\n",
      "Epoch 5681: train loss: 0.02112891711294651, val loss: 0.1014522835612297\n",
      "Epoch 5682: train loss: 0.020053042098879814, val loss: 0.0971263125538826\n",
      "Epoch 5683: train loss: 0.01778390258550644, val loss: 0.07890425622463226\n",
      "Epoch 5684: train loss: 0.024975448846817017, val loss: 0.045893486589193344\n",
      "Epoch 5685: train loss: 0.022534964606165886, val loss: 0.10640281438827515\n",
      "Epoch 5686: train loss: 0.02500586025416851, val loss: 0.10017921030521393\n",
      "Epoch 5687: train loss: 0.019727174192667007, val loss: 0.11068155616521835\n",
      "Epoch 5688: train loss: 0.022690946236252785, val loss: 0.08550891280174255\n",
      "Epoch 5689: train loss: 0.01984846591949463, val loss: 0.10073485225439072\n",
      "Epoch 5690: train loss: 0.016315128654241562, val loss: 0.09423583745956421\n",
      "Epoch 5691: train loss: 0.019255513325333595, val loss: 0.08731460571289062\n",
      "Epoch 5692: train loss: 0.018035337328910828, val loss: 0.06795012205839157\n",
      "Epoch 5693: train loss: 0.02061462588608265, val loss: 0.11858035624027252\n",
      "Epoch 5694: train loss: 0.02217906527221203, val loss: 0.0703214555978775\n",
      "Epoch 5695: train loss: 0.020716024562716484, val loss: 0.13927637040615082\n",
      "Epoch 5696: train loss: 0.019228586927056313, val loss: 0.06569453328847885\n",
      "Epoch 5697: train loss: 0.02109360508620739, val loss: 0.0516197569668293\n",
      "Epoch 5698: train loss: 0.02173253335058689, val loss: 0.1039457693696022\n",
      "Epoch 5699: train loss: 0.024535587057471275, val loss: 0.10599595308303833\n",
      "Epoch 5700: train loss: 0.019707370549440384, val loss: 0.09894436597824097\n",
      "Epoch 5701: train loss: 0.0209210142493248, val loss: 0.09960615634918213\n",
      "Epoch 5702: train loss: 0.024489371106028557, val loss: 0.07436702400445938\n",
      "Epoch 5703: train loss: 0.02334146574139595, val loss: 0.07172578573226929\n",
      "Epoch 5704: train loss: 0.015197668224573135, val loss: 0.14216874539852142\n",
      "Epoch 5705: train loss: 0.02338266931474209, val loss: 0.09095878899097443\n",
      "Epoch 5706: train loss: 0.029124891385436058, val loss: 0.06755831092596054\n",
      "Epoch 5707: train loss: 0.027532178908586502, val loss: 0.09028972685337067\n",
      "Epoch 5708: train loss: 0.022718409076333046, val loss: 0.08997422456741333\n",
      "Epoch 5709: train loss: 0.024267422035336494, val loss: 0.09318758547306061\n",
      "Epoch 5710: train loss: 0.02884027175605297, val loss: 0.0862463191151619\n",
      "Epoch 5711: train loss: 0.017959842458367348, val loss: 0.07752659171819687\n",
      "Epoch 5712: train loss: 0.022751424461603165, val loss: 0.0803699716925621\n",
      "Epoch 5713: train loss: 0.023804593831300735, val loss: 0.09050912410020828\n",
      "Epoch 5714: train loss: 0.021539008244872093, val loss: 0.08396400511264801\n",
      "Epoch 5715: train loss: 0.03148125484585762, val loss: 0.05646130070090294\n",
      "Epoch 5716: train loss: 0.01959213800728321, val loss: 0.10328949987888336\n",
      "Epoch 5717: train loss: 0.01995036192238331, val loss: 0.0822349414229393\n",
      "Epoch 5718: train loss: 0.028402747586369514, val loss: 0.0840446874499321\n",
      "Epoch 5719: train loss: 0.024527624249458313, val loss: 0.06375241279602051\n",
      "Epoch 5720: train loss: 0.02817458100616932, val loss: 0.06867935508489609\n",
      "Epoch 5721: train loss: 0.023715006187558174, val loss: 0.052775751799345016\n",
      "Epoch 5722: train loss: 0.020533207803964615, val loss: 0.1156211867928505\n",
      "Epoch 5723: train loss: 0.025017278268933296, val loss: 0.0713394284248352\n",
      "Epoch 5724: train loss: 0.01924833282828331, val loss: 0.0821375623345375\n",
      "Epoch 5725: train loss: 0.027675215154886246, val loss: 0.08043617755174637\n",
      "Epoch 5726: train loss: 0.024259038269519806, val loss: 0.0812564566731453\n",
      "Epoch 5727: train loss: 0.025126248598098755, val loss: 0.08041971176862717\n",
      "Epoch 5728: train loss: 0.03123421035706997, val loss: 0.1074141412973404\n",
      "Epoch 5729: train loss: 0.02914651297032833, val loss: 0.1367950290441513\n",
      "Epoch 5730: train loss: 0.034854210913181305, val loss: 0.0953042134642601\n",
      "Epoch 5731: train loss: 0.01843830570578575, val loss: 0.15337376296520233\n",
      "Epoch 5732: train loss: 0.02213245816528797, val loss: 0.06403807550668716\n",
      "Epoch 5733: train loss: 0.023296669125556946, val loss: 0.08551608771085739\n",
      "Epoch 5734: train loss: 0.019913263618946075, val loss: 0.07037656754255295\n",
      "Epoch 5735: train loss: 0.023488111793994904, val loss: 0.06635443866252899\n",
      "Epoch 5736: train loss: 0.02718125283718109, val loss: 0.05135289952158928\n",
      "Epoch 5737: train loss: 0.028807194903492928, val loss: 0.06317508965730667\n",
      "Epoch 5738: train loss: 0.02831697277724743, val loss: 0.07383225858211517\n",
      "Epoch 5739: train loss: 0.028181025758385658, val loss: 0.06623118370771408\n",
      "Epoch 5740: train loss: 0.024912215769290924, val loss: 0.08303990215063095\n",
      "Epoch 5741: train loss: 0.031564030796289444, val loss: 0.06008542701601982\n",
      "Epoch 5742: train loss: 0.022004202008247375, val loss: 0.05348172411322594\n",
      "Epoch 5743: train loss: 0.019177356734871864, val loss: 0.057735562324523926\n",
      "Epoch 5744: train loss: 0.01980377547442913, val loss: 0.10712851583957672\n",
      "Epoch 5745: train loss: 0.03064947947859764, val loss: 0.13032761216163635\n",
      "Epoch 5746: train loss: 0.01850033365190029, val loss: 0.0488215871155262\n",
      "Epoch 5747: train loss: 0.026418576017022133, val loss: 0.047223348170518875\n",
      "Epoch 5748: train loss: 0.02538703940808773, val loss: 0.1106918603181839\n",
      "Epoch 5749: train loss: 0.02310473471879959, val loss: 0.09709300845861435\n",
      "Epoch 5750: train loss: 0.019181858748197556, val loss: 0.08543073385953903\n",
      "Epoch 5751: train loss: 0.022435426712036133, val loss: 0.07706709951162338\n",
      "Epoch 5752: train loss: 0.02530759759247303, val loss: 0.061715930700302124\n",
      "Epoch 5753: train loss: 0.027841653674840927, val loss: 0.06421729177236557\n",
      "Epoch 5754: train loss: 0.020410917699337006, val loss: 0.086698018014431\n",
      "Epoch 5755: train loss: 0.029144367203116417, val loss: 0.08818195760250092\n",
      "Epoch 5756: train loss: 0.026318185031414032, val loss: 0.1105339527130127\n",
      "Epoch 5757: train loss: 0.03323325514793396, val loss: 0.09883391857147217\n",
      "Epoch 5758: train loss: 0.02065468765795231, val loss: 0.12014880031347275\n",
      "Epoch 5759: train loss: 0.023571865633130074, val loss: 0.06954091042280197\n",
      "Epoch 5760: train loss: 0.01719864457845688, val loss: 0.06847136467695236\n",
      "Epoch 5761: train loss: 0.02307482436299324, val loss: 0.036363404244184494\n",
      "Epoch 5762: train loss: 0.022145850583910942, val loss: 0.05980546027421951\n",
      "Epoch 5763: train loss: 0.026048462837934494, val loss: 0.050663482397794724\n",
      "Epoch 5764: train loss: 0.02167190983891487, val loss: 0.07576066255569458\n",
      "Epoch 5765: train loss: 0.01891268603503704, val loss: 0.12939241528511047\n",
      "Epoch 5766: train loss: 0.019587382674217224, val loss: 0.09505206346511841\n",
      "Epoch 5767: train loss: 0.02203328162431717, val loss: 0.09584250301122665\n",
      "Epoch 5768: train loss: 0.023002272471785545, val loss: 0.06848438084125519\n",
      "Epoch 5769: train loss: 0.019882436841726303, val loss: 0.07597538083791733\n",
      "Epoch 5770: train loss: 0.02693234570324421, val loss: 0.06663902848958969\n",
      "Epoch 5771: train loss: 0.02266840636730194, val loss: 0.1056414470076561\n",
      "Epoch 5772: train loss: 0.02472635917365551, val loss: 0.07438186556100845\n",
      "Epoch 5773: train loss: 0.024254390969872475, val loss: 0.08052517473697662\n",
      "Epoch 5774: train loss: 0.020006785169243813, val loss: 0.09085530042648315\n",
      "Epoch 5775: train loss: 0.02174675278365612, val loss: 0.05565772205591202\n",
      "Epoch 5776: train loss: 0.022967776283621788, val loss: 0.07952132821083069\n",
      "Epoch 5777: train loss: 0.020701814442873, val loss: 0.06444155424833298\n",
      "Epoch 5778: train loss: 0.022733386605978012, val loss: 0.07998631149530411\n",
      "Epoch 5779: train loss: 0.02446787618100643, val loss: 0.11912703514099121\n",
      "Epoch 5780: train loss: 0.020882800221443176, val loss: 0.05473281070590019\n",
      "Epoch 5781: train loss: 0.01650329679250717, val loss: 0.060589563101530075\n",
      "Epoch 5782: train loss: 0.02285018563270569, val loss: 0.0872851237654686\n",
      "Epoch 5783: train loss: 0.02648640237748623, val loss: 0.07321222126483917\n",
      "Epoch 5784: train loss: 0.024006560444831848, val loss: 0.1131158098578453\n",
      "Epoch 5785: train loss: 0.021384024992585182, val loss: 0.055439747869968414\n",
      "Epoch 5786: train loss: 0.023359669372439384, val loss: 0.08449523895978928\n",
      "Epoch 5787: train loss: 0.015678154304623604, val loss: 0.05849098786711693\n",
      "Epoch 5788: train loss: 0.016911594197154045, val loss: 0.1092263013124466\n",
      "Epoch 5789: train loss: 0.021897241473197937, val loss: 0.08202087879180908\n",
      "Epoch 5790: train loss: 0.02546561323106289, val loss: 0.06357678025960922\n",
      "Epoch 5791: train loss: 0.023388953879475594, val loss: 0.06910809129476547\n",
      "Epoch 5792: train loss: 0.022886235266923904, val loss: 0.0755648985505104\n",
      "Epoch 5793: train loss: 0.024777114391326904, val loss: 0.053952015936374664\n",
      "Epoch 5794: train loss: 0.025968531146645546, val loss: 0.06384521722793579\n",
      "Epoch 5795: train loss: 0.032352034002542496, val loss: 0.07209461182355881\n",
      "Epoch 5796: train loss: 0.020824987441301346, val loss: 0.07060282677412033\n",
      "Epoch 5797: train loss: 0.020730439573526382, val loss: 0.08235064893960953\n",
      "Epoch 5798: train loss: 0.02434202842414379, val loss: 0.07987391203641891\n",
      "Epoch 5799: train loss: 0.02473798207938671, val loss: 0.075367271900177\n",
      "Epoch 5800: train loss: 0.022321049124002457, val loss: 0.08239107578992844\n",
      "Epoch 5801: train loss: 0.021045295521616936, val loss: 0.04666310176253319\n",
      "Epoch 5802: train loss: 0.02054792270064354, val loss: 0.07044270634651184\n",
      "Epoch 5803: train loss: 0.020526157692074776, val loss: 0.0401369109749794\n",
      "Epoch 5804: train loss: 0.02769753523170948, val loss: 0.09713683277368546\n",
      "Epoch 5805: train loss: 0.02477036602795124, val loss: 0.09430141746997833\n",
      "Epoch 5806: train loss: 0.020369157195091248, val loss: 0.06220075115561485\n",
      "Epoch 5807: train loss: 0.03623353689908981, val loss: 0.08156605809926987\n",
      "Epoch 5808: train loss: 0.023758867755532265, val loss: 0.15223775804042816\n",
      "Epoch 5809: train loss: 0.023303501307964325, val loss: 0.1353306621313095\n",
      "Epoch 5810: train loss: 0.02083994261920452, val loss: 0.08544965088367462\n",
      "Epoch 5811: train loss: 0.023634659126400948, val loss: 0.051736533641815186\n",
      "Epoch 5812: train loss: 0.023970527574419975, val loss: 0.10597385466098785\n",
      "Epoch 5813: train loss: 0.02008175291121006, val loss: 0.09392306953668594\n",
      "Epoch 5814: train loss: 0.02232225053012371, val loss: 0.06512608379125595\n",
      "Epoch 5815: train loss: 0.03261899575591087, val loss: 0.09821801632642746\n",
      "Epoch 5816: train loss: 0.01905129663646221, val loss: 0.07748862355947495\n",
      "Epoch 5817: train loss: 0.02344038523733616, val loss: 0.11468949168920517\n",
      "Epoch 5818: train loss: 0.018430618569254875, val loss: 0.11158116161823273\n",
      "Epoch 5819: train loss: 0.019451502710580826, val loss: 0.08591973781585693\n",
      "Epoch 5820: train loss: 0.022700991481542587, val loss: 0.07772175967693329\n",
      "Epoch 5821: train loss: 0.022266624495387077, val loss: 0.09910108894109726\n",
      "Epoch 5822: train loss: 0.020732268691062927, val loss: 0.09158351272344589\n",
      "Epoch 5823: train loss: 0.020543890073895454, val loss: 0.08579492568969727\n",
      "Epoch 5824: train loss: 0.01722043938934803, val loss: 0.08498658239841461\n",
      "Epoch 5825: train loss: 0.02385762520134449, val loss: 0.056536074727773666\n",
      "Epoch 5826: train loss: 0.018574465066194534, val loss: 0.09009496122598648\n",
      "Epoch 5827: train loss: 0.012734656222164631, val loss: 0.061415113508701324\n",
      "Epoch 5828: train loss: 0.014465336687862873, val loss: 0.08831273019313812\n",
      "Epoch 5829: train loss: 0.01941823959350586, val loss: 0.0644230842590332\n",
      "Epoch 5830: train loss: 0.017958689481019974, val loss: 0.0838397741317749\n",
      "Epoch 5831: train loss: 0.020038427785038948, val loss: 0.050193000584840775\n",
      "Epoch 5832: train loss: 0.021551720798015594, val loss: 0.048704471439123154\n",
      "Epoch 5833: train loss: 0.023751582950353622, val loss: 0.07481954991817474\n",
      "Epoch 5834: train loss: 0.014678712002933025, val loss: 0.03810092434287071\n",
      "Epoch 5835: train loss: 0.016925780102610588, val loss: 0.07460403442382812\n",
      "Epoch 5836: train loss: 0.017775529995560646, val loss: 0.07433830946683884\n",
      "Epoch 5837: train loss: 0.014777936041355133, val loss: 0.04812796786427498\n",
      "Epoch 5838: train loss: 0.01889754645526409, val loss: 0.07978863269090652\n",
      "Epoch 5839: train loss: 0.019631879404187202, val loss: 0.06396985054016113\n",
      "Epoch 5840: train loss: 0.017515644431114197, val loss: 0.06387288868427277\n",
      "Epoch 5841: train loss: 0.022182490676641464, val loss: 0.06696365028619766\n",
      "Epoch 5842: train loss: 0.021762894466519356, val loss: 0.08430976420640945\n",
      "Epoch 5843: train loss: 0.014739503152668476, val loss: 0.09580842405557632\n",
      "Epoch 5844: train loss: 0.02044249139726162, val loss: 0.07328327000141144\n",
      "Epoch 5845: train loss: 0.023402627557516098, val loss: 0.08883306384086609\n",
      "Epoch 5846: train loss: 0.01950879395008087, val loss: 0.09066349267959595\n",
      "Epoch 5847: train loss: 0.01938033476471901, val loss: 0.08308740705251694\n",
      "Epoch 5848: train loss: 0.020691856741905212, val loss: 0.08836988359689713\n",
      "Epoch 5849: train loss: 0.01650482974946499, val loss: 0.07265259325504303\n",
      "Epoch 5850: train loss: 0.01825324445962906, val loss: 0.10412106662988663\n",
      "Epoch 5851: train loss: 0.015796566382050514, val loss: 0.09331551194190979\n",
      "Epoch 5852: train loss: 0.021318454295396805, val loss: 0.04585394635796547\n",
      "Epoch 5853: train loss: 0.018142372369766235, val loss: 0.07789433002471924\n",
      "Epoch 5854: train loss: 0.02675769105553627, val loss: 0.08566268533468246\n",
      "Epoch 5855: train loss: 0.01792713813483715, val loss: 0.07956694811582565\n",
      "Epoch 5856: train loss: 0.02073724940419197, val loss: 0.070023313164711\n",
      "Epoch 5857: train loss: 0.0179018322378397, val loss: 0.06460388749837875\n",
      "Epoch 5858: train loss: 0.017534354701638222, val loss: 0.049682870507240295\n",
      "Epoch 5859: train loss: 0.015954839065670967, val loss: 0.08399727195501328\n",
      "Epoch 5860: train loss: 0.01820003055036068, val loss: 0.07057680189609528\n",
      "Epoch 5861: train loss: 0.024004513397812843, val loss: 0.07585938274860382\n",
      "Epoch 5862: train loss: 0.017587097361683846, val loss: 0.08016681671142578\n",
      "Epoch 5863: train loss: 0.02110304869711399, val loss: 0.14500272274017334\n",
      "Epoch 5864: train loss: 0.022959306836128235, val loss: 0.11225626617670059\n",
      "Epoch 5865: train loss: 0.023460883647203445, val loss: 0.11382532119750977\n",
      "Epoch 5866: train loss: 0.028073741123080254, val loss: 0.09006424248218536\n",
      "Epoch 5867: train loss: 0.024280354380607605, val loss: 0.05923013761639595\n",
      "Epoch 5868: train loss: 0.019950149580836296, val loss: 0.07482202351093292\n",
      "Epoch 5869: train loss: 0.018311632797122, val loss: 0.07193384319543839\n",
      "Epoch 5870: train loss: 0.023885736241936684, val loss: 0.11123447865247726\n",
      "Epoch 5871: train loss: 0.01891990192234516, val loss: 0.07583530992269516\n",
      "Epoch 5872: train loss: 0.02047131583094597, val loss: 0.07068028301000595\n",
      "Epoch 5873: train loss: 0.013761846348643303, val loss: 0.06973227858543396\n",
      "Epoch 5874: train loss: 0.02328181080520153, val loss: 0.06847446411848068\n",
      "Epoch 5875: train loss: 0.02196684293448925, val loss: 0.11403615772724152\n",
      "Epoch 5876: train loss: 0.018509170040488243, val loss: 0.05250517651438713\n",
      "Epoch 5877: train loss: 0.024006610736250877, val loss: 0.031186381354928017\n",
      "Epoch 5878: train loss: 0.013424323871731758, val loss: 0.04337294399738312\n",
      "Epoch 5879: train loss: 0.023692786693572998, val loss: 0.046916525810956955\n",
      "Epoch 5880: train loss: 0.0228346586227417, val loss: 0.04394032806158066\n",
      "Epoch 5881: train loss: 0.021341366693377495, val loss: 0.05900934338569641\n",
      "Epoch 5882: train loss: 0.02375851757824421, val loss: 0.05047459527850151\n",
      "Epoch 5883: train loss: 0.020194297656416893, val loss: 0.060273345559835434\n",
      "Epoch 5884: train loss: 0.018666964024305344, val loss: 0.08648239076137543\n",
      "Epoch 5885: train loss: 0.021427683532238007, val loss: 0.08253906667232513\n",
      "Epoch 5886: train loss: 0.013265382498502731, val loss: 0.08822504431009293\n",
      "Epoch 5887: train loss: 0.024417666718363762, val loss: 0.0735497921705246\n",
      "Epoch 5888: train loss: 0.017729509621858597, val loss: 0.08938883990049362\n",
      "Epoch 5889: train loss: 0.022064749151468277, val loss: 0.05994134768843651\n",
      "Epoch 5890: train loss: 0.02008596621453762, val loss: 0.08904979377985\n",
      "Epoch 5891: train loss: 0.020796999335289, val loss: 0.0887279063463211\n",
      "Epoch 5892: train loss: 0.018639856949448586, val loss: 0.11041948944330215\n",
      "Epoch 5893: train loss: 0.019522400572896004, val loss: 0.08297485113143921\n",
      "Epoch 5894: train loss: 0.015960725024342537, val loss: 0.08691203594207764\n",
      "Epoch 5895: train loss: 0.024352531880140305, val loss: 0.05222424492239952\n",
      "Epoch 5896: train loss: 0.02336730808019638, val loss: 0.08099379390478134\n",
      "Epoch 5897: train loss: 0.02188458852469921, val loss: 0.10066479444503784\n",
      "Epoch 5898: train loss: 0.019485890865325928, val loss: 0.07061775773763657\n",
      "Epoch 5899: train loss: 0.019613010808825493, val loss: 0.08568514138460159\n",
      "Epoch 5900: train loss: 0.022806629538536072, val loss: 0.07402186095714569\n",
      "Epoch 5901: train loss: 0.018978532403707504, val loss: 0.09421984851360321\n",
      "Epoch 5902: train loss: 0.018697068095207214, val loss: 0.0647314190864563\n",
      "Epoch 5903: train loss: 0.018993349745869637, val loss: 0.07253685593605042\n",
      "Epoch 5904: train loss: 0.02205287106335163, val loss: 0.046020835638046265\n",
      "Epoch 5905: train loss: 0.015151592902839184, val loss: 0.0795406848192215\n",
      "Epoch 5906: train loss: 0.01524696871638298, val loss: 0.05815587565302849\n",
      "Epoch 5907: train loss: 0.02099047228693962, val loss: 0.07835653424263\n",
      "Epoch 5908: train loss: 0.021655317395925522, val loss: 0.08703038096427917\n",
      "Epoch 5909: train loss: 0.017630355432629585, val loss: 0.07219218462705612\n",
      "Epoch 5910: train loss: 0.01985751837491989, val loss: 0.09907283633947372\n",
      "Epoch 5911: train loss: 0.017025774344801903, val loss: 0.05860896781086922\n",
      "Epoch 5912: train loss: 0.018488699570298195, val loss: 0.055316727608442307\n",
      "Epoch 5913: train loss: 0.014062169007956982, val loss: 0.08516953140497208\n",
      "Epoch 5914: train loss: 0.013765273615717888, val loss: 0.13716167211532593\n",
      "Epoch 5915: train loss: 0.02133955806493759, val loss: 0.0918954536318779\n",
      "Epoch 5916: train loss: 0.017371775582432747, val loss: 0.07374637573957443\n",
      "Epoch 5917: train loss: 0.016961077228188515, val loss: 0.05261640623211861\n",
      "Epoch 5918: train loss: 0.021221531555056572, val loss: 0.05563196539878845\n",
      "Epoch 5919: train loss: 0.021186338737607002, val loss: 0.07968355715274811\n",
      "Epoch 5920: train loss: 0.019367435947060585, val loss: 0.1007562056183815\n",
      "Epoch 5921: train loss: 0.01795054040849209, val loss: 0.09497687220573425\n",
      "Epoch 5922: train loss: 0.01867278665304184, val loss: 0.07944393157958984\n",
      "Epoch 5923: train loss: 0.011844092048704624, val loss: 0.09296997636556625\n",
      "Epoch 5924: train loss: 0.01748625561594963, val loss: 0.05492815375328064\n",
      "Epoch 5925: train loss: 0.01913360506296158, val loss: 0.06509073823690414\n",
      "Epoch 5926: train loss: 0.015871889889240265, val loss: 0.057725705206394196\n",
      "Epoch 5927: train loss: 0.017916738986968994, val loss: 0.06600552052259445\n",
      "Epoch 5928: train loss: 0.02711082063615322, val loss: 0.08960580080747604\n",
      "Epoch 5929: train loss: 0.018715526908636093, val loss: 0.06118929386138916\n",
      "Epoch 5930: train loss: 0.01869148015975952, val loss: 0.07471194863319397\n",
      "Epoch 5931: train loss: 0.025616172701120377, val loss: 0.11110527813434601\n",
      "Epoch 5932: train loss: 0.012920906767249107, val loss: 0.0850672647356987\n",
      "Epoch 5933: train loss: 0.019235435873270035, val loss: 0.03575463965535164\n",
      "Epoch 5934: train loss: 0.01702268235385418, val loss: 0.05204085633158684\n",
      "Epoch 5935: train loss: 0.01924065500497818, val loss: 0.0810144767165184\n",
      "Epoch 5936: train loss: 0.022991036996245384, val loss: 0.0922885611653328\n",
      "Epoch 5937: train loss: 0.019290663301944733, val loss: 0.0747508555650711\n",
      "Epoch 5938: train loss: 0.017972221598029137, val loss: 0.10124941915273666\n",
      "Epoch 5939: train loss: 0.019105738028883934, val loss: 0.09787260740995407\n",
      "Epoch 5940: train loss: 0.018369050696492195, val loss: 0.08095208555459976\n",
      "Epoch 5941: train loss: 0.020151913166046143, val loss: 0.09413527697324753\n",
      "Epoch 5942: train loss: 0.016804112121462822, val loss: 0.11344067007303238\n",
      "Epoch 5943: train loss: 0.01773023046553135, val loss: 0.07250542938709259\n",
      "Epoch 5944: train loss: 0.014444231986999512, val loss: 0.06354456394910812\n",
      "Epoch 5945: train loss: 0.01705925725400448, val loss: 0.03752618655562401\n",
      "Epoch 5946: train loss: 0.01774914562702179, val loss: 0.05382655933499336\n",
      "Epoch 5947: train loss: 0.0194157175719738, val loss: 0.05908763036131859\n",
      "Epoch 5948: train loss: 0.016813896596431732, val loss: 0.08016616106033325\n",
      "Epoch 5949: train loss: 0.020282818004488945, val loss: 0.06761830300092697\n",
      "Epoch 5950: train loss: 0.015076486393809319, val loss: 0.08499950170516968\n",
      "Epoch 5951: train loss: 0.018224969506263733, val loss: 0.08120135217905045\n",
      "Epoch 5952: train loss: 0.020963679999113083, val loss: 0.039817918092012405\n",
      "Epoch 5953: train loss: 0.01803867518901825, val loss: 0.0638800784945488\n",
      "Epoch 5954: train loss: 0.019116055220365524, val loss: 0.08170010894536972\n",
      "Epoch 5955: train loss: 0.020545843988656998, val loss: 0.1031305342912674\n",
      "Epoch 5956: train loss: 0.022615833207964897, val loss: 0.06576410681009293\n",
      "Epoch 5957: train loss: 0.014942052774131298, val loss: 0.06548228114843369\n",
      "Epoch 5958: train loss: 0.018734179437160492, val loss: 0.05511071905493736\n",
      "Epoch 5959: train loss: 0.020721299573779106, val loss: 0.038429029285907745\n",
      "Epoch 5960: train loss: 0.023720959201455116, val loss: 0.08679509162902832\n",
      "Epoch 5961: train loss: 0.020211009308695793, val loss: 0.05543818697333336\n",
      "Epoch 5962: train loss: 0.02162269502878189, val loss: 0.06507601588964462\n",
      "Epoch 5963: train loss: 0.020025040954351425, val loss: 0.029735863208770752\n",
      "Epoch 5964: train loss: 0.025545189157128334, val loss: 0.03970092162489891\n",
      "Epoch 5965: train loss: 0.01623699627816677, val loss: 0.053933531045913696\n",
      "Epoch 5966: train loss: 0.016967719420790672, val loss: 0.0659254938364029\n",
      "Epoch 5967: train loss: 0.021053973585367203, val loss: 0.08619166910648346\n",
      "Epoch 5968: train loss: 0.014364900067448616, val loss: 0.08244509994983673\n",
      "Epoch 5969: train loss: 0.018518680706620216, val loss: 0.11127334088087082\n",
      "Epoch 5970: train loss: 0.015339719131588936, val loss: 0.06454108655452728\n",
      "Epoch 5971: train loss: 0.025011448189616203, val loss: 0.08454623073339462\n",
      "Epoch 5972: train loss: 0.022673359140753746, val loss: 0.09666986018419266\n",
      "Epoch 5973: train loss: 0.029207881540060043, val loss: 0.04606080800294876\n",
      "Epoch 5974: train loss: 0.01942467875778675, val loss: 0.0672060176730156\n",
      "Epoch 5975: train loss: 0.018759144470095634, val loss: 0.04928722605109215\n",
      "Epoch 5976: train loss: 0.017973288893699646, val loss: 0.06365036219358444\n",
      "Epoch 5977: train loss: 0.01893075555562973, val loss: 0.0962875634431839\n",
      "Epoch 5978: train loss: 0.020017007365822792, val loss: 0.07580622285604477\n",
      "Epoch 5979: train loss: 0.014604736119508743, val loss: 0.08052326738834381\n",
      "Epoch 5980: train loss: 0.017684105783700943, val loss: 0.06911567598581314\n",
      "Epoch 5981: train loss: 0.018436048179864883, val loss: 0.07489591836929321\n",
      "Epoch 5982: train loss: 0.018626298755407333, val loss: 0.057402707636356354\n",
      "Epoch 5983: train loss: 0.02378310076892376, val loss: 0.04922924563288689\n",
      "Epoch 5984: train loss: 0.021864119917154312, val loss: 0.06835749000310898\n",
      "Epoch 5985: train loss: 0.026864826679229736, val loss: 0.08215624839067459\n",
      "Epoch 5986: train loss: 0.017901182174682617, val loss: 0.05537010356783867\n",
      "Epoch 5987: train loss: 0.020597413182258606, val loss: 0.060162197798490524\n",
      "Epoch 5988: train loss: 0.017888011410832405, val loss: 0.07452747225761414\n",
      "Epoch 5989: train loss: 0.016161736100912094, val loss: 0.09936773777008057\n",
      "Epoch 5990: train loss: 0.020946169272065163, val loss: 0.07889172434806824\n",
      "Epoch 5991: train loss: 0.02705763839185238, val loss: 0.09343882650136948\n",
      "Epoch 5992: train loss: 0.01639452576637268, val loss: 0.09044421464204788\n",
      "Epoch 5993: train loss: 0.025104589760303497, val loss: 0.06559790670871735\n",
      "Epoch 5994: train loss: 0.01643112115561962, val loss: 0.1250988095998764\n",
      "Epoch 5995: train loss: 0.019437741488218307, val loss: 0.059164028614759445\n",
      "Epoch 5996: train loss: 0.018524276092648506, val loss: 0.05190864950418472\n",
      "Epoch 5997: train loss: 0.018338706344366074, val loss: 0.03905659168958664\n",
      "Epoch 5998: train loss: 0.020752273499965668, val loss: 0.0584985725581646\n",
      "Epoch 5999: train loss: 0.016874849796295166, val loss: 0.065433070063591\n",
      "Epoch 6000: train loss: 0.019160417839884758, val loss: 0.06798791140317917\n",
      "Epoch 6001: train loss: 0.020518485456705093, val loss: 0.07502032816410065\n",
      "Epoch 6002: train loss: 0.021947067230939865, val loss: 0.07841068506240845\n",
      "Epoch 6003: train loss: 0.0168028362095356, val loss: 0.04533739015460014\n",
      "Epoch 6004: train loss: 0.01685061864554882, val loss: 0.08334086090326309\n",
      "Epoch 6005: train loss: 0.02010466903448105, val loss: 0.07782872766256332\n",
      "Epoch 6006: train loss: 0.01698451116681099, val loss: 0.11543788760900497\n",
      "Epoch 6007: train loss: 0.025536170229315758, val loss: 0.05960199981927872\n",
      "Epoch 6008: train loss: 0.02221492864191532, val loss: 0.058701515197753906\n",
      "Epoch 6009: train loss: 0.01395309530198574, val loss: 0.08472015708684921\n",
      "Epoch 6010: train loss: 0.013834799639880657, val loss: 0.09309618920087814\n",
      "Epoch 6011: train loss: 0.013916164636611938, val loss: 0.07404153048992157\n",
      "Epoch 6012: train loss: 0.019254576414823532, val loss: 0.09493309259414673\n",
      "Epoch 6013: train loss: 0.0189736969769001, val loss: 0.09374155104160309\n",
      "Epoch 6014: train loss: 0.024590006098151207, val loss: 0.05364488437771797\n",
      "Epoch 6015: train loss: 0.01702428236603737, val loss: 0.029235688969492912\n",
      "Epoch 6016: train loss: 0.0202644020318985, val loss: 0.06652994453907013\n",
      "Epoch 6017: train loss: 0.017319636419415474, val loss: 0.06865012645721436\n",
      "Epoch 6018: train loss: 0.016552187502384186, val loss: 0.10385455936193466\n",
      "Epoch 6019: train loss: 0.017487671226263046, val loss: 0.06808973103761673\n",
      "Epoch 6020: train loss: 0.02116711437702179, val loss: 0.07129450887441635\n",
      "Epoch 6021: train loss: 0.019054632633924484, val loss: 0.09669337421655655\n",
      "Epoch 6022: train loss: 0.020469482988119125, val loss: 0.06494555622339249\n",
      "Epoch 6023: train loss: 0.018318427726626396, val loss: 0.0877990648150444\n",
      "Epoch 6024: train loss: 0.017330240458250046, val loss: 0.07742124795913696\n",
      "Epoch 6025: train loss: 0.016430450603365898, val loss: 0.0734410211443901\n",
      "Epoch 6026: train loss: 0.018983274698257446, val loss: 0.08333214372396469\n",
      "Epoch 6027: train loss: 0.019746476784348488, val loss: 0.11361373960971832\n",
      "Epoch 6028: train loss: 0.02569921873509884, val loss: 0.05135437473654747\n",
      "Epoch 6029: train loss: 0.01563507691025734, val loss: 0.09064745903015137\n",
      "Epoch 6030: train loss: 0.019201427698135376, val loss: 0.08874418586492538\n",
      "Epoch 6031: train loss: 0.015734832733869553, val loss: 0.07683535665273666\n",
      "Epoch 6032: train loss: 0.017136327922344208, val loss: 0.07480131834745407\n",
      "Epoch 6033: train loss: 0.019628873094916344, val loss: 0.08752920478582382\n",
      "Epoch 6034: train loss: 0.0254795104265213, val loss: 0.07679244130849838\n",
      "Epoch 6035: train loss: 0.019870877265930176, val loss: 0.07445170730352402\n",
      "Epoch 6036: train loss: 0.031115267425775528, val loss: 0.07100769877433777\n",
      "Epoch 6037: train loss: 0.021550176665186882, val loss: 0.06516436487436295\n",
      "Epoch 6038: train loss: 0.02720465697348118, val loss: 0.07168073952198029\n",
      "Epoch 6039: train loss: 0.02303881198167801, val loss: 0.061534274369478226\n",
      "Epoch 6040: train loss: 0.014796802774071693, val loss: 0.10676773637533188\n",
      "Epoch 6041: train loss: 0.019492289051413536, val loss: 0.10957898199558258\n",
      "Epoch 6042: train loss: 0.01561949122697115, val loss: 0.06148775294423103\n",
      "Epoch 6043: train loss: 0.016226911917328835, val loss: 0.07651287317276001\n",
      "Epoch 6044: train loss: 0.013798123225569725, val loss: 0.10449015349149704\n",
      "Epoch 6045: train loss: 0.02146151103079319, val loss: 0.05995795875787735\n",
      "Epoch 6046: train loss: 0.01457522064447403, val loss: 0.05829433351755142\n",
      "Epoch 6047: train loss: 0.014577606692910194, val loss: 0.08321734517812729\n",
      "Epoch 6048: train loss: 0.023472007364034653, val loss: 0.055734287947416306\n",
      "Epoch 6049: train loss: 0.014452760107815266, val loss: 0.05870659276843071\n",
      "Epoch 6050: train loss: 0.013841910287737846, val loss: 0.11445146054029465\n",
      "Epoch 6051: train loss: 0.019025329500436783, val loss: 0.09531253576278687\n",
      "Epoch 6052: train loss: 0.026411056518554688, val loss: 0.07722999900579453\n",
      "Epoch 6053: train loss: 0.013711786828935146, val loss: 0.05073267221450806\n",
      "Epoch 6054: train loss: 0.019320370629429817, val loss: 0.09824448078870773\n",
      "Epoch 6055: train loss: 0.019639229401946068, val loss: 0.09634849429130554\n",
      "Epoch 6056: train loss: 0.027180243283510208, val loss: 0.06274528801441193\n",
      "Epoch 6057: train loss: 0.022371690720319748, val loss: 0.11019632965326309\n",
      "Epoch 6058: train loss: 0.021967653185129166, val loss: 0.081466443836689\n",
      "Epoch 6059: train loss: 0.01714264415204525, val loss: 0.06246837601065636\n",
      "Epoch 6060: train loss: 0.016774429008364677, val loss: 0.05301114544272423\n",
      "Epoch 6061: train loss: 0.02143082208931446, val loss: 0.10244371742010117\n",
      "Epoch 6062: train loss: 0.02472890354692936, val loss: 0.1186983585357666\n",
      "Epoch 6063: train loss: 0.017489418387413025, val loss: 0.06483059376478195\n",
      "Epoch 6064: train loss: 0.022170528769493103, val loss: 0.04489997774362564\n",
      "Epoch 6065: train loss: 0.0218204353004694, val loss: 0.0636296197772026\n",
      "Epoch 6066: train loss: 0.020274603739380836, val loss: 0.0894380509853363\n",
      "Epoch 6067: train loss: 0.022897863760590553, val loss: 0.05445743352174759\n",
      "Epoch 6068: train loss: 0.01932099089026451, val loss: 0.08331848680973053\n",
      "Epoch 6069: train loss: 0.020736515522003174, val loss: 0.08920164406299591\n",
      "Epoch 6070: train loss: 0.018000008538365364, val loss: 0.05468861013650894\n",
      "Epoch 6071: train loss: 0.02777799777686596, val loss: 0.06079965829849243\n",
      "Epoch 6072: train loss: 0.021965764462947845, val loss: 0.06841200590133667\n",
      "Epoch 6073: train loss: 0.02037111297249794, val loss: 0.08212997019290924\n",
      "Epoch 6074: train loss: 0.034509945660829544, val loss: 0.08253193646669388\n",
      "Epoch 6075: train loss: 0.019354265183210373, val loss: 0.06947393715381622\n",
      "Epoch 6076: train loss: 0.02108568511903286, val loss: 0.06047677621245384\n",
      "Epoch 6077: train loss: 0.015888221561908722, val loss: 0.10357666015625\n",
      "Epoch 6078: train loss: 0.02427707239985466, val loss: 0.0778811052441597\n",
      "Epoch 6079: train loss: 0.019684869796037674, val loss: 0.06932105869054794\n",
      "Epoch 6080: train loss: 0.022104598581790924, val loss: 0.11048032343387604\n",
      "Epoch 6081: train loss: 0.025896862149238586, val loss: 0.10093560069799423\n",
      "Epoch 6082: train loss: 0.025243772193789482, val loss: 0.06975680589675903\n",
      "Epoch 6083: train loss: 0.018593767657876015, val loss: 0.05200963094830513\n",
      "Epoch 6084: train loss: 0.02790997549891472, val loss: 0.090570367872715\n",
      "Epoch 6085: train loss: 0.017069917172193527, val loss: 0.08277108520269394\n",
      "Epoch 6086: train loss: 0.02234024554491043, val loss: 0.055942352861166\n",
      "Epoch 6087: train loss: 0.020719697698950768, val loss: 0.07493819296360016\n",
      "Epoch 6088: train loss: 0.014347529970109463, val loss: 0.04886862635612488\n",
      "Epoch 6089: train loss: 0.015473296865820885, val loss: 0.08244051784276962\n",
      "Epoch 6090: train loss: 0.02211402915418148, val loss: 0.07611289620399475\n",
      "Epoch 6091: train loss: 0.014697651378810406, val loss: 0.08286513388156891\n",
      "Epoch 6092: train loss: 0.019911348819732666, val loss: 0.09171473979949951\n",
      "Epoch 6093: train loss: 0.021637415513396263, val loss: 0.05427419766783714\n",
      "Epoch 6094: train loss: 0.020398827269673347, val loss: 0.07993662357330322\n",
      "Epoch 6095: train loss: 0.0158755574375391, val loss: 0.09772047400474548\n",
      "Epoch 6096: train loss: 0.015839731320738792, val loss: 0.07721880823373795\n",
      "Epoch 6097: train loss: 0.023763343691825867, val loss: 0.09621074050664902\n",
      "Epoch 6098: train loss: 0.020756592974066734, val loss: 0.08253731578588486\n",
      "Epoch 6099: train loss: 0.019915051758289337, val loss: 0.09766104072332382\n",
      "Epoch 6100: train loss: 0.02028573676943779, val loss: 0.05873733386397362\n",
      "Epoch 6101: train loss: 0.01540820486843586, val loss: 0.04420008137822151\n",
      "Epoch 6102: train loss: 0.019670631736516953, val loss: 0.03056919574737549\n",
      "Epoch 6103: train loss: 0.020445549860596657, val loss: 0.046286653727293015\n",
      "Epoch 6104: train loss: 0.022876674309372902, val loss: 0.03417053818702698\n",
      "Epoch 6105: train loss: 0.023661380633711815, val loss: 0.05207926034927368\n",
      "Epoch 6106: train loss: 0.01995054818689823, val loss: 0.08261942118406296\n",
      "Epoch 6107: train loss: 0.01954377442598343, val loss: 0.06959740817546844\n",
      "Epoch 6108: train loss: 0.019074365496635437, val loss: 0.10817515850067139\n",
      "Epoch 6109: train loss: 0.01611592248082161, val loss: 0.06851174682378769\n",
      "Epoch 6110: train loss: 0.02425125241279602, val loss: 0.062013424932956696\n",
      "Epoch 6111: train loss: 0.01693323813378811, val loss: 0.06869817525148392\n",
      "Epoch 6112: train loss: 0.020905299112200737, val loss: 0.08802091330289841\n",
      "Epoch 6113: train loss: 0.020181434229016304, val loss: 0.05689368396997452\n",
      "Epoch 6114: train loss: 0.025647329166531563, val loss: 0.07715039700269699\n",
      "Epoch 6115: train loss: 0.019787797704339027, val loss: 0.0661601796746254\n",
      "Epoch 6116: train loss: 0.020523620769381523, val loss: 0.10810358822345734\n",
      "Epoch 6117: train loss: 0.01785053126513958, val loss: 0.09955554455518723\n",
      "Epoch 6118: train loss: 0.01956995017826557, val loss: 0.04169951751828194\n",
      "Epoch 6119: train loss: 0.02073299139738083, val loss: 0.058525122702121735\n",
      "Epoch 6120: train loss: 0.01894979737699032, val loss: 0.06765138357877731\n",
      "Epoch 6121: train loss: 0.015391935594379902, val loss: 0.1130373477935791\n",
      "Epoch 6122: train loss: 0.01974247209727764, val loss: 0.04071779549121857\n",
      "Epoch 6123: train loss: 0.017701493576169014, val loss: 0.06508269160985947\n",
      "Epoch 6124: train loss: 0.01875504106283188, val loss: 0.05921727418899536\n",
      "Epoch 6125: train loss: 0.017529454082250595, val loss: 0.07151462882757187\n",
      "Epoch 6126: train loss: 0.018135635182261467, val loss: 0.09872227907180786\n",
      "Epoch 6127: train loss: 0.02596384473145008, val loss: 0.08150693029165268\n",
      "Epoch 6128: train loss: 0.016795098781585693, val loss: 0.06627344340085983\n",
      "Epoch 6129: train loss: 0.014653447084128857, val loss: 0.08503737300634384\n",
      "Epoch 6130: train loss: 0.016641391441226006, val loss: 0.08306876569986343\n",
      "Epoch 6131: train loss: 0.015847859904170036, val loss: 0.05726581811904907\n",
      "Epoch 6132: train loss: 0.01490913424640894, val loss: 0.047764409333467484\n",
      "Epoch 6133: train loss: 0.021025294438004494, val loss: 0.07194250077009201\n",
      "Epoch 6134: train loss: 0.018462589010596275, val loss: 0.07592131197452545\n",
      "Epoch 6135: train loss: 0.015660429373383522, val loss: 0.0752032995223999\n",
      "Epoch 6136: train loss: 0.011573090218007565, val loss: 0.08903049677610397\n",
      "Epoch 6137: train loss: 0.014319343492388725, val loss: 0.07229550927877426\n",
      "Epoch 6138: train loss: 0.02422449365258217, val loss: 0.07536057382822037\n",
      "Epoch 6139: train loss: 0.018786536529660225, val loss: 0.053286101669073105\n",
      "Epoch 6140: train loss: 0.01906251348555088, val loss: 0.10233991593122482\n",
      "Epoch 6141: train loss: 0.019713198766112328, val loss: 0.036984335631132126\n",
      "Epoch 6142: train loss: 0.018555663526058197, val loss: 0.05402189493179321\n",
      "Epoch 6143: train loss: 0.019014637917280197, val loss: 0.0689937099814415\n",
      "Epoch 6144: train loss: 0.025243395939469337, val loss: 0.0915040448307991\n",
      "Epoch 6145: train loss: 0.018866954371333122, val loss: 0.062482137233018875\n",
      "Epoch 6146: train loss: 0.013420693576335907, val loss: 0.04862160608172417\n",
      "Epoch 6147: train loss: 0.013702094554901123, val loss: 0.060313593596220016\n",
      "Epoch 6148: train loss: 0.017225567251443863, val loss: 0.10155178606510162\n",
      "Epoch 6149: train loss: 0.017969032749533653, val loss: 0.04969049617648125\n",
      "Epoch 6150: train loss: 0.016553858295083046, val loss: 0.09723284095525742\n",
      "Epoch 6151: train loss: 0.02294863574206829, val loss: 0.09243263304233551\n",
      "Epoch 6152: train loss: 0.020977819338440895, val loss: 0.09475892037153244\n",
      "Epoch 6153: train loss: 0.022033948451280594, val loss: 0.097148597240448\n",
      "Epoch 6154: train loss: 0.02315106987953186, val loss: 0.07768122106790543\n",
      "Epoch 6155: train loss: 0.018293114379048347, val loss: 0.07053622603416443\n",
      "Epoch 6156: train loss: 0.020580314099788666, val loss: 0.07826697826385498\n",
      "Epoch 6157: train loss: 0.023426946252584457, val loss: 0.09248659759759903\n",
      "Epoch 6158: train loss: 0.016123374924063683, val loss: 0.06669556349515915\n",
      "Epoch 6159: train loss: 0.01487453281879425, val loss: 0.08594174683094025\n",
      "Epoch 6160: train loss: 0.023446885868906975, val loss: 0.09582215547561646\n",
      "Epoch 6161: train loss: 0.023417064920067787, val loss: 0.07660140842199326\n",
      "Epoch 6162: train loss: 0.014155848883092403, val loss: 0.037744347006082535\n",
      "Epoch 6163: train loss: 0.01502755656838417, val loss: 0.08234107494354248\n",
      "Epoch 6164: train loss: 0.020234806463122368, val loss: 0.050627827644348145\n",
      "Epoch 6165: train loss: 0.019593045115470886, val loss: 0.06931135803461075\n",
      "Epoch 6166: train loss: 0.023141145706176758, val loss: 0.10521384328603745\n",
      "Epoch 6167: train loss: 0.021109689027071, val loss: 0.06115229055285454\n",
      "Epoch 6168: train loss: 0.018158715218305588, val loss: 0.050290971994400024\n",
      "Epoch 6169: train loss: 0.0182022862136364, val loss: 0.13738542795181274\n",
      "Epoch 6170: train loss: 0.019132954999804497, val loss: 0.06116226315498352\n",
      "Epoch 6171: train loss: 0.022373290732502937, val loss: 0.08899378776550293\n",
      "Epoch 6172: train loss: 0.02305685169994831, val loss: 0.06386204063892365\n",
      "Epoch 6173: train loss: 0.020622219890356064, val loss: 0.07421507686376572\n",
      "Epoch 6174: train loss: 0.016810182482004166, val loss: 0.09556698054075241\n",
      "Epoch 6175: train loss: 0.02537146396934986, val loss: 0.10072095692157745\n",
      "Epoch 6176: train loss: 0.019476788118481636, val loss: 0.08932127803564072\n",
      "Epoch 6177: train loss: 0.022459756582975388, val loss: 0.08625345677137375\n",
      "Epoch 6178: train loss: 0.019260918721556664, val loss: 0.11543234437704086\n",
      "Epoch 6179: train loss: 0.02850501798093319, val loss: 0.04071766883134842\n",
      "Epoch 6180: train loss: 0.026151198893785477, val loss: 0.07666730880737305\n",
      "Epoch 6181: train loss: 0.02565598487854004, val loss: 0.10750959068536758\n",
      "Epoch 6182: train loss: 0.022643735632300377, val loss: 0.08645700663328171\n",
      "Epoch 6183: train loss: 0.019901389256119728, val loss: 0.0789012610912323\n",
      "Epoch 6184: train loss: 0.01911414973437786, val loss: 0.06822926551103592\n",
      "Epoch 6185: train loss: 0.015408892184495926, val loss: 0.10601548105478287\n",
      "Epoch 6186: train loss: 0.024320757016539574, val loss: 0.09644778817892075\n",
      "Epoch 6187: train loss: 0.0207587331533432, val loss: 0.09343712776899338\n",
      "Epoch 6188: train loss: 0.023333679884672165, val loss: 0.032129328697919846\n",
      "Epoch 6189: train loss: 0.015784230083227158, val loss: 0.11829882115125656\n",
      "Epoch 6190: train loss: 0.01746487058699131, val loss: 0.10360293835401535\n",
      "Epoch 6191: train loss: 0.02086261473596096, val loss: 0.07262983173131943\n",
      "Epoch 6192: train loss: 0.020108360797166824, val loss: 0.08434686809778214\n",
      "Epoch 6193: train loss: 0.021846754476428032, val loss: 0.07078563421964645\n",
      "Epoch 6194: train loss: 0.018575141206383705, val loss: 0.06679406017065048\n",
      "Epoch 6195: train loss: 0.023167364299297333, val loss: 0.061843376606702805\n",
      "Epoch 6196: train loss: 0.015553036704659462, val loss: 0.06490907073020935\n",
      "Epoch 6197: train loss: 0.01702646166086197, val loss: 0.07152708619832993\n",
      "Epoch 6198: train loss: 0.016273997724056244, val loss: 0.08025506138801575\n",
      "Epoch 6199: train loss: 0.023105714470148087, val loss: 0.03529645502567291\n",
      "Epoch 6200: train loss: 0.014097683131694794, val loss: 0.049502454698085785\n",
      "Epoch 6201: train loss: 0.017645608633756638, val loss: 0.03866150602698326\n",
      "Epoch 6202: train loss: 0.018472688272595406, val loss: 0.06882735341787338\n",
      "Epoch 6203: train loss: 0.028184715658426285, val loss: 0.0654282495379448\n",
      "Epoch 6204: train loss: 0.019309470430016518, val loss: 0.07766709476709366\n",
      "Epoch 6205: train loss: 0.012169752269983292, val loss: 0.055918868631124496\n",
      "Epoch 6206: train loss: 0.017195437103509903, val loss: 0.07154399156570435\n",
      "Epoch 6207: train loss: 0.01731916517019272, val loss: 0.04817349091172218\n",
      "Epoch 6208: train loss: 0.0269057247787714, val loss: 0.06244217976927757\n",
      "Epoch 6209: train loss: 0.013942927122116089, val loss: 0.11066346615552902\n",
      "Epoch 6210: train loss: 0.01677354983985424, val loss: 0.03848825767636299\n",
      "Epoch 6211: train loss: 0.01706772670149803, val loss: 0.1032484918832779\n",
      "Epoch 6212: train loss: 0.016653509810566902, val loss: 0.07690522819757462\n",
      "Epoch 6213: train loss: 0.01695925183594227, val loss: 0.06287788599729538\n",
      "Epoch 6214: train loss: 0.01800541952252388, val loss: 0.06580346077680588\n",
      "Epoch 6215: train loss: 0.014405983500182629, val loss: 0.04734841734170914\n",
      "Epoch 6216: train loss: 0.021963436156511307, val loss: 0.06902052462100983\n",
      "Epoch 6217: train loss: 0.017181679606437683, val loss: 0.0515199676156044\n",
      "Epoch 6218: train loss: 0.017822306603193283, val loss: 0.03777186945080757\n",
      "Epoch 6219: train loss: 0.016398640349507332, val loss: 0.08385666459798813\n",
      "Epoch 6220: train loss: 0.017219223082065582, val loss: 0.07329943031072617\n",
      "Epoch 6221: train loss: 0.01782936230301857, val loss: 0.0824805200099945\n",
      "Epoch 6222: train loss: 0.01714738830924034, val loss: 0.06896897405385971\n",
      "Epoch 6223: train loss: 0.015294983983039856, val loss: 0.06534899771213531\n",
      "Epoch 6224: train loss: 0.01880268007516861, val loss: 0.07495389878749847\n",
      "Epoch 6225: train loss: 0.018957145512104034, val loss: 0.09655796736478806\n",
      "Epoch 6226: train loss: 0.01789560168981552, val loss: 0.06005467101931572\n",
      "Epoch 6227: train loss: 0.01922014355659485, val loss: 0.07908796519041061\n",
      "Epoch 6228: train loss: 0.017421506345272064, val loss: 0.06509511917829514\n",
      "Epoch 6229: train loss: 0.023258471861481667, val loss: 0.05902419239282608\n",
      "Epoch 6230: train loss: 0.017844749614596367, val loss: 0.061647843569517136\n",
      "Epoch 6231: train loss: 0.016935449093580246, val loss: 0.08530934900045395\n",
      "Epoch 6232: train loss: 0.016108546406030655, val loss: 0.09808314591646194\n",
      "Epoch 6233: train loss: 0.0264809038490057, val loss: 0.06785565614700317\n",
      "Epoch 6234: train loss: 0.01757214032113552, val loss: 0.05849284678697586\n",
      "Epoch 6235: train loss: 0.01993950456380844, val loss: 0.10793646425008774\n",
      "Epoch 6236: train loss: 0.01849823258817196, val loss: 0.05401073023676872\n",
      "Epoch 6237: train loss: 0.020451705902814865, val loss: 0.06670407205820084\n",
      "Epoch 6238: train loss: 0.02185973711311817, val loss: 0.04902365058660507\n",
      "Epoch 6239: train loss: 0.02015523985028267, val loss: 0.07918695360422134\n",
      "Epoch 6240: train loss: 0.025694500654935837, val loss: 0.09526180475950241\n",
      "Epoch 6241: train loss: 0.02300998941063881, val loss: 0.056558649986982346\n",
      "Epoch 6242: train loss: 0.02271188609302044, val loss: 0.06371697038412094\n",
      "Epoch 6243: train loss: 0.02123756892979145, val loss: 0.059375155717134476\n",
      "Epoch 6244: train loss: 0.020890045911073685, val loss: 0.06022932752966881\n",
      "Epoch 6245: train loss: 0.014265782199800014, val loss: 0.08489800989627838\n",
      "Epoch 6246: train loss: 0.0173257477581501, val loss: 0.07483020424842834\n",
      "Epoch 6247: train loss: 0.01740427128970623, val loss: 0.0473967008292675\n",
      "Epoch 6248: train loss: 0.02176351100206375, val loss: 0.10316842049360275\n",
      "Epoch 6249: train loss: 0.02434832789003849, val loss: 0.0889742448925972\n",
      "Epoch 6250: train loss: 0.020983511582016945, val loss: 0.06605494767427444\n",
      "Epoch 6251: train loss: 0.018224649131298065, val loss: 0.06242387369275093\n",
      "Epoch 6252: train loss: 0.019608262926340103, val loss: 0.08341573923826218\n",
      "Epoch 6253: train loss: 0.01630317233502865, val loss: 0.08451788872480392\n",
      "Epoch 6254: train loss: 0.0222029872238636, val loss: 0.06948410719633102\n",
      "Epoch 6255: train loss: 0.015259822830557823, val loss: 0.06609144061803818\n",
      "Epoch 6256: train loss: 0.017663436010479927, val loss: 0.1074753925204277\n",
      "Epoch 6257: train loss: 0.012768330052495003, val loss: 0.0530150905251503\n",
      "Epoch 6258: train loss: 0.018126724287867546, val loss: 0.09034717082977295\n",
      "Epoch 6259: train loss: 0.013275072909891605, val loss: 0.09494545310735703\n",
      "Epoch 6260: train loss: 0.01407533511519432, val loss: 0.0945967584848404\n",
      "Epoch 6261: train loss: 0.021221691742539406, val loss: 0.09642130881547928\n",
      "Epoch 6262: train loss: 0.017100319266319275, val loss: 0.0769452452659607\n",
      "Epoch 6263: train loss: 0.014869851991534233, val loss: 0.07737617939710617\n",
      "Epoch 6264: train loss: 0.013201926834881306, val loss: 0.0737106129527092\n",
      "Epoch 6265: train loss: 0.022365480661392212, val loss: 0.04990818724036217\n",
      "Epoch 6266: train loss: 0.020321011543273926, val loss: 0.11329338699579239\n",
      "Epoch 6267: train loss: 0.016828341409564018, val loss: 0.1010494977235794\n",
      "Epoch 6268: train loss: 0.02059999853372574, val loss: 0.11435820907354355\n",
      "Epoch 6269: train loss: 0.023096811026334763, val loss: 0.07245144993066788\n",
      "Epoch 6270: train loss: 0.01652020774781704, val loss: 0.06067973002791405\n",
      "Epoch 6271: train loss: 0.016405140981078148, val loss: 0.06758645921945572\n",
      "Epoch 6272: train loss: 0.014785430394113064, val loss: 0.07680463790893555\n",
      "Epoch 6273: train loss: 0.019653374329209328, val loss: 0.05958820506930351\n",
      "Epoch 6274: train loss: 0.01669933646917343, val loss: 0.08530769497156143\n",
      "Epoch 6275: train loss: 0.015707628801465034, val loss: 0.1162264347076416\n",
      "Epoch 6276: train loss: 0.023788658902049065, val loss: 0.06242871284484863\n",
      "Epoch 6277: train loss: 0.02029726468026638, val loss: 0.05755946785211563\n",
      "Epoch 6278: train loss: 0.015944773331284523, val loss: 0.05559951812028885\n",
      "Epoch 6279: train loss: 0.017096059396862984, val loss: 0.11403512954711914\n",
      "Epoch 6280: train loss: 0.02103026770055294, val loss: 0.09014404565095901\n",
      "Epoch 6281: train loss: 0.01642039231956005, val loss: 0.06129689887166023\n",
      "Epoch 6282: train loss: 0.015241199173033237, val loss: 0.10537254810333252\n",
      "Epoch 6283: train loss: 0.01734069548547268, val loss: 0.08003456145524979\n",
      "Epoch 6284: train loss: 0.021200254559516907, val loss: 0.08470696210861206\n",
      "Epoch 6285: train loss: 0.021176764741539955, val loss: 0.10288859903812408\n",
      "Epoch 6286: train loss: 0.02278161607682705, val loss: 0.07710597664117813\n",
      "Epoch 6287: train loss: 0.02375141903758049, val loss: 0.038314446806907654\n",
      "Epoch 6288: train loss: 0.019235609099268913, val loss: 0.09746695309877396\n",
      "Epoch 6289: train loss: 0.01704089716076851, val loss: 0.10096030682325363\n",
      "Epoch 6290: train loss: 0.015338071621954441, val loss: 0.0650457963347435\n",
      "Epoch 6291: train loss: 0.012338798493146896, val loss: 0.08819228410720825\n",
      "Epoch 6292: train loss: 0.013009515590965748, val loss: 0.08832775801420212\n",
      "Epoch 6293: train loss: 0.019734449684619904, val loss: 0.05075858160853386\n",
      "Epoch 6294: train loss: 0.014689331874251366, val loss: 0.08995752781629562\n",
      "Epoch 6295: train loss: 0.019854547455906868, val loss: 0.0633154958486557\n",
      "Epoch 6296: train loss: 0.02383774146437645, val loss: 0.10471471399068832\n",
      "Epoch 6297: train loss: 0.03569924831390381, val loss: 0.08732486516237259\n",
      "Epoch 6298: train loss: 0.020361151546239853, val loss: 0.08389561623334885\n",
      "Epoch 6299: train loss: 0.01876172237098217, val loss: 0.07371457666158676\n",
      "Epoch 6300: train loss: 0.014237184077501297, val loss: 0.08997531980276108\n",
      "Epoch 6301: train loss: 0.02214408665895462, val loss: 0.059441544115543365\n",
      "Epoch 6302: train loss: 0.01774916984140873, val loss: 0.07143072038888931\n",
      "Epoch 6303: train loss: 0.02207174338400364, val loss: 0.13340146839618683\n",
      "Epoch 6304: train loss: 0.019601693376898766, val loss: 0.14902572333812714\n",
      "Epoch 6305: train loss: 0.0163714736700058, val loss: 0.10843024402856827\n",
      "Epoch 6306: train loss: 0.01608923077583313, val loss: 0.07706618309020996\n",
      "Epoch 6307: train loss: 0.019085250794887543, val loss: 0.08436458557844162\n",
      "Epoch 6308: train loss: 0.016096321865916252, val loss: 0.11738082021474838\n",
      "Epoch 6309: train loss: 0.02534572407603264, val loss: 0.061972614377737045\n",
      "Epoch 6310: train loss: 0.020640941336750984, val loss: 0.09949616342782974\n",
      "Epoch 6311: train loss: 0.01417301781475544, val loss: 0.0789312869310379\n",
      "Epoch 6312: train loss: 0.017220810055732727, val loss: 0.07354988157749176\n",
      "Epoch 6313: train loss: 0.013807286508381367, val loss: 0.04855481907725334\n",
      "Epoch 6314: train loss: 0.017660226672887802, val loss: 0.07071851938962936\n",
      "Epoch 6315: train loss: 0.013183061964809895, val loss: 0.10924579948186874\n",
      "Epoch 6316: train loss: 0.023428941145539284, val loss: 0.05222126469016075\n",
      "Epoch 6317: train loss: 0.011639706790447235, val loss: 0.10729984194040298\n",
      "Epoch 6318: train loss: 0.023048777133226395, val loss: 0.09418432414531708\n",
      "Epoch 6319: train loss: 0.01584109291434288, val loss: 0.08492817729711533\n",
      "Epoch 6320: train loss: 0.019589340314269066, val loss: 0.1278655081987381\n",
      "Epoch 6321: train loss: 0.01860562525689602, val loss: 0.0652896985411644\n",
      "Epoch 6322: train loss: 0.02479260228574276, val loss: 0.05912617966532707\n",
      "Epoch 6323: train loss: 0.018146732822060585, val loss: 0.08306069672107697\n",
      "Epoch 6324: train loss: 0.020471055060625076, val loss: 0.09849443286657333\n",
      "Epoch 6325: train loss: 0.01698012836277485, val loss: 0.05881485342979431\n",
      "Epoch 6326: train loss: 0.018356826156377792, val loss: 0.07435499131679535\n",
      "Epoch 6327: train loss: 0.019287968054413795, val loss: 0.11117042601108551\n",
      "Epoch 6328: train loss: 0.018751168623566628, val loss: 0.06339498609304428\n",
      "Epoch 6329: train loss: 0.023180915042757988, val loss: 0.056076861917972565\n",
      "Epoch 6330: train loss: 0.022247685119509697, val loss: 0.09105218946933746\n",
      "Epoch 6331: train loss: 0.018597101792693138, val loss: 0.07919273525476456\n",
      "Epoch 6332: train loss: 0.019105732440948486, val loss: 0.11779534071683884\n",
      "Epoch 6333: train loss: 0.01680445671081543, val loss: 0.08129607141017914\n",
      "Epoch 6334: train loss: 0.018637722358107567, val loss: 0.09800631552934647\n",
      "Epoch 6335: train loss: 0.019322456791996956, val loss: 0.05964469909667969\n",
      "Epoch 6336: train loss: 0.015250856988132, val loss: 0.0712120532989502\n",
      "Epoch 6337: train loss: 0.013884817250072956, val loss: 0.08452253043651581\n",
      "Epoch 6338: train loss: 0.012223823927342892, val loss: 0.08247818797826767\n",
      "Epoch 6339: train loss: 0.023717371746897697, val loss: 0.12979678809642792\n",
      "Epoch 6340: train loss: 0.016228629276156425, val loss: 0.09956475347280502\n",
      "Epoch 6341: train loss: 0.02032696269452572, val loss: 0.0783529207110405\n",
      "Epoch 6342: train loss: 0.013087058439850807, val loss: 0.09279956668615341\n",
      "Epoch 6343: train loss: 0.010806143283843994, val loss: 0.06628065556287766\n",
      "Epoch 6344: train loss: 0.019998343661427498, val loss: 0.08434677869081497\n",
      "Epoch 6345: train loss: 0.018375543877482414, val loss: 0.08056008815765381\n",
      "Epoch 6346: train loss: 0.016386697068810463, val loss: 0.094975046813488\n",
      "Epoch 6347: train loss: 0.017361827194690704, val loss: 0.10417450964450836\n",
      "Epoch 6348: train loss: 0.013615441508591175, val loss: 0.07583072036504745\n",
      "Epoch 6349: train loss: 0.022275347262620926, val loss: 0.04642169550061226\n",
      "Epoch 6350: train loss: 0.017949694767594337, val loss: 0.07498794049024582\n",
      "Epoch 6351: train loss: 0.01485828310251236, val loss: 0.08175937831401825\n",
      "Epoch 6352: train loss: 0.016819367185235023, val loss: 0.08239228278398514\n",
      "Epoch 6353: train loss: 0.016734207049012184, val loss: 0.07928993552923203\n",
      "Epoch 6354: train loss: 0.020730216056108475, val loss: 0.07123572379350662\n",
      "Epoch 6355: train loss: 0.014118831604719162, val loss: 0.062201131135225296\n",
      "Epoch 6356: train loss: 0.012286954559385777, val loss: 0.08483653515577316\n",
      "Epoch 6357: train loss: 0.016089022159576416, val loss: 0.06670255959033966\n",
      "Epoch 6358: train loss: 0.015152977779507637, val loss: 0.09202902764081955\n",
      "Epoch 6359: train loss: 0.017245955765247345, val loss: 0.06761851161718369\n",
      "Epoch 6360: train loss: 0.0193968303501606, val loss: 0.06233428791165352\n",
      "Epoch 6361: train loss: 0.017102735117077827, val loss: 0.059410374611616135\n",
      "Epoch 6362: train loss: 0.01324544008821249, val loss: 0.07730600237846375\n",
      "Epoch 6363: train loss: 0.023420503363013268, val loss: 0.08206630498170853\n",
      "Epoch 6364: train loss: 0.024233251810073853, val loss: 0.06416285037994385\n",
      "Epoch 6365: train loss: 0.026020443066954613, val loss: 0.05542100593447685\n",
      "Epoch 6366: train loss: 0.018709003925323486, val loss: 0.11942460387945175\n",
      "Epoch 6367: train loss: 0.01838555932044983, val loss: 0.050442684441804886\n",
      "Epoch 6368: train loss: 0.018322348594665527, val loss: 0.05582503229379654\n",
      "Epoch 6369: train loss: 0.02319403551518917, val loss: 0.10154664516448975\n",
      "Epoch 6370: train loss: 0.02044595219194889, val loss: 0.061640407890081406\n",
      "Epoch 6371: train loss: 0.018448714166879654, val loss: 0.07000727206468582\n",
      "Epoch 6372: train loss: 0.02021535485982895, val loss: 0.06330786645412445\n",
      "Epoch 6373: train loss: 0.019268840551376343, val loss: 0.06339371204376221\n",
      "Epoch 6374: train loss: 0.013879358768463135, val loss: 0.06292802095413208\n",
      "Epoch 6375: train loss: 0.02493397146463394, val loss: 0.09034492820501328\n",
      "Epoch 6376: train loss: 0.02108002081513405, val loss: 0.07460612058639526\n",
      "Epoch 6377: train loss: 0.018892774358391762, val loss: 0.08047493547201157\n",
      "Epoch 6378: train loss: 0.017656246200203896, val loss: 0.07532389461994171\n",
      "Epoch 6379: train loss: 0.01822551339864731, val loss: 0.09836115688085556\n",
      "Epoch 6380: train loss: 0.015388791449368, val loss: 0.06295639276504517\n",
      "Epoch 6381: train loss: 0.019866807386279106, val loss: 0.059781547635793686\n",
      "Epoch 6382: train loss: 0.023785168305039406, val loss: 0.09727617353200912\n",
      "Epoch 6383: train loss: 0.01904892921447754, val loss: 0.0649016723036766\n",
      "Epoch 6384: train loss: 0.022901365533471107, val loss: 0.057385534048080444\n",
      "Epoch 6385: train loss: 0.023927981033921242, val loss: 0.05951082706451416\n",
      "Epoch 6386: train loss: 0.015118644572794437, val loss: 0.0778336301445961\n",
      "Epoch 6387: train loss: 0.01995674893260002, val loss: 0.0713072419166565\n",
      "Epoch 6388: train loss: 0.01746509224176407, val loss: 0.06846188753843307\n",
      "Epoch 6389: train loss: 0.021206574514508247, val loss: 0.08485006541013718\n",
      "Epoch 6390: train loss: 0.023752707988023758, val loss: 0.1062685027718544\n",
      "Epoch 6391: train loss: 0.01775491237640381, val loss: 0.07057733833789825\n",
      "Epoch 6392: train loss: 0.020922746509313583, val loss: 0.06423919647932053\n",
      "Epoch 6393: train loss: 0.030404768884181976, val loss: 0.043450530618429184\n",
      "Epoch 6394: train loss: 0.020220208913087845, val loss: 0.07490330189466476\n",
      "Epoch 6395: train loss: 0.02053125947713852, val loss: 0.08503089845180511\n",
      "Epoch 6396: train loss: 0.015355534851551056, val loss: 0.06493452191352844\n",
      "Epoch 6397: train loss: 0.027513552457094193, val loss: 0.05003088712692261\n",
      "Epoch 6398: train loss: 0.014171367511153221, val loss: 0.04342302307486534\n",
      "Epoch 6399: train loss: 0.02077566087245941, val loss: 0.06237800791859627\n",
      "Epoch 6400: train loss: 0.017042063176631927, val loss: 0.06690151244401932\n",
      "Epoch 6401: train loss: 0.01364499144256115, val loss: 0.08934231847524643\n",
      "Epoch 6402: train loss: 0.015221727080643177, val loss: 0.05930540710687637\n",
      "Epoch 6403: train loss: 0.020501120015978813, val loss: 0.07844387739896774\n",
      "Epoch 6404: train loss: 0.017000725492835045, val loss: 0.08723266422748566\n",
      "Epoch 6405: train loss: 0.029722781851887703, val loss: 0.07171698659658432\n",
      "Epoch 6406: train loss: 0.022696368396282196, val loss: 0.07097870111465454\n",
      "Epoch 6407: train loss: 0.023297935724258423, val loss: 0.06333794444799423\n",
      "Epoch 6408: train loss: 0.01947588473558426, val loss: 0.09172216802835464\n",
      "Epoch 6409: train loss: 0.02202717587351799, val loss: 0.06213745102286339\n",
      "Epoch 6410: train loss: 0.018753739073872566, val loss: 0.09305217862129211\n",
      "Epoch 6411: train loss: 0.02117517776787281, val loss: 0.10498817265033722\n",
      "Epoch 6412: train loss: 0.015859141945838928, val loss: 0.04904088377952576\n",
      "Epoch 6413: train loss: 0.017153935506939888, val loss: 0.07637401670217514\n",
      "Epoch 6414: train loss: 0.02359931170940399, val loss: 0.13605885207653046\n",
      "Epoch 6415: train loss: 0.01743013970553875, val loss: 0.1064295545220375\n",
      "Epoch 6416: train loss: 0.01676444709300995, val loss: 0.06933093070983887\n",
      "Epoch 6417: train loss: 0.01570453681051731, val loss: 0.07904262840747833\n",
      "Epoch 6418: train loss: 0.019508270546793938, val loss: 0.04802748188376427\n",
      "Epoch 6419: train loss: 0.016257742419838905, val loss: 0.0885876789689064\n",
      "Epoch 6420: train loss: 0.018695179373025894, val loss: 0.07007455080747604\n",
      "Epoch 6421: train loss: 0.013339010998606682, val loss: 0.09825169295072556\n",
      "Epoch 6422: train loss: 0.024601640179753304, val loss: 0.047961171716451645\n",
      "Epoch 6423: train loss: 0.014582928270101547, val loss: 0.0696505680680275\n",
      "Epoch 6424: train loss: 0.019647488370537758, val loss: 0.07688155770301819\n",
      "Epoch 6425: train loss: 0.01610351912677288, val loss: 0.0662587583065033\n",
      "Epoch 6426: train loss: 0.028133660554885864, val loss: 0.1028762087225914\n",
      "Epoch 6427: train loss: 0.017718752846121788, val loss: 0.05882234498858452\n",
      "Epoch 6428: train loss: 0.02208702266216278, val loss: 0.08012723177671432\n",
      "Epoch 6429: train loss: 0.01723952777683735, val loss: 0.08611299097537994\n",
      "Epoch 6430: train loss: 0.01895652897655964, val loss: 0.061860401183366776\n",
      "Epoch 6431: train loss: 0.019721707329154015, val loss: 0.05648580193519592\n",
      "Epoch 6432: train loss: 0.01979450136423111, val loss: 0.05310657620429993\n",
      "Epoch 6433: train loss: 0.019392244517803192, val loss: 0.09822556376457214\n",
      "Epoch 6434: train loss: 0.017295433208346367, val loss: 0.09456997364759445\n",
      "Epoch 6435: train loss: 0.014889039099216461, val loss: 0.061693765223026276\n",
      "Epoch 6436: train loss: 0.02332313358783722, val loss: 0.07933048903942108\n",
      "Epoch 6437: train loss: 0.018102679401636124, val loss: 0.06085348129272461\n",
      "Epoch 6438: train loss: 0.015777187421917915, val loss: 0.05618154630064964\n",
      "Epoch 6439: train loss: 0.0225699320435524, val loss: 0.07361521571874619\n",
      "Epoch 6440: train loss: 0.016384972259402275, val loss: 0.07687804102897644\n",
      "Epoch 6441: train loss: 0.012210119515657425, val loss: 0.05545644089579582\n",
      "Epoch 6442: train loss: 0.027352720499038696, val loss: 0.09239473193883896\n",
      "Epoch 6443: train loss: 0.020174654200673103, val loss: 0.08566734939813614\n",
      "Epoch 6444: train loss: 0.021825963631272316, val loss: 0.07401808351278305\n",
      "Epoch 6445: train loss: 0.017783787101507187, val loss: 0.10140841454267502\n",
      "Epoch 6446: train loss: 0.012383300811052322, val loss: 0.07078152894973755\n",
      "Epoch 6447: train loss: 0.016933513805270195, val loss: 0.07330739498138428\n",
      "Epoch 6448: train loss: 0.02178175188601017, val loss: 0.06799683719873428\n",
      "Epoch 6449: train loss: 0.03183344751596451, val loss: 0.056064557284116745\n",
      "Epoch 6450: train loss: 0.018215546384453773, val loss: 0.06626143306493759\n",
      "Epoch 6451: train loss: 0.015099364332854748, val loss: 0.07537215948104858\n",
      "Epoch 6452: train loss: 0.03439638018608093, val loss: 0.05687401816248894\n",
      "Epoch 6453: train loss: 0.016987139359116554, val loss: 0.08413374423980713\n",
      "Epoch 6454: train loss: 0.028206415474414825, val loss: 0.06375525146722794\n",
      "Epoch 6455: train loss: 0.019010430201888084, val loss: 0.06100925803184509\n",
      "Epoch 6456: train loss: 0.02511545829474926, val loss: 0.09498649090528488\n",
      "Epoch 6457: train loss: 0.020077379420399666, val loss: 0.0650639683008194\n",
      "Epoch 6458: train loss: 0.02340303547680378, val loss: 0.09058203548192978\n",
      "Epoch 6459: train loss: 0.02732868306338787, val loss: 0.07976861298084259\n",
      "Epoch 6460: train loss: 0.02494458667933941, val loss: 0.06999944150447845\n",
      "Epoch 6461: train loss: 0.021539287641644478, val loss: 0.07165049761533737\n",
      "Epoch 6462: train loss: 0.023400843143463135, val loss: 0.059862252324819565\n",
      "Epoch 6463: train loss: 0.026353830471634865, val loss: 0.06962057203054428\n",
      "Epoch 6464: train loss: 0.023893199861049652, val loss: 0.08293438702821732\n",
      "Epoch 6465: train loss: 0.015940958634018898, val loss: 0.06480184942483902\n",
      "Epoch 6466: train loss: 0.024541357532143593, val loss: 0.0586782805621624\n",
      "Epoch 6467: train loss: 0.01702379249036312, val loss: 0.05362780764698982\n",
      "Epoch 6468: train loss: 0.014548125676810741, val loss: 0.12717212736606598\n",
      "Epoch 6469: train loss: 0.015313380397856236, val loss: 0.0798126682639122\n",
      "Epoch 6470: train loss: 0.027300601825118065, val loss: 0.07010824978351593\n",
      "Epoch 6471: train loss: 0.02598027139902115, val loss: 0.04315289855003357\n",
      "Epoch 6472: train loss: 0.020945660769939423, val loss: 0.07426759600639343\n",
      "Epoch 6473: train loss: 0.024732809513807297, val loss: 0.11120782047510147\n",
      "Epoch 6474: train loss: 0.022545067593455315, val loss: 0.0598653219640255\n",
      "Epoch 6475: train loss: 0.01976945251226425, val loss: 0.08492773771286011\n",
      "Epoch 6476: train loss: 0.017080482095479965, val loss: 0.06878562271595001\n",
      "Epoch 6477: train loss: 0.021499112248420715, val loss: 0.07450303435325623\n",
      "Epoch 6478: train loss: 0.02420877479016781, val loss: 0.09619758278131485\n",
      "Epoch 6479: train loss: 0.01852484419941902, val loss: 0.05762491747736931\n",
      "Epoch 6480: train loss: 0.019975248724222183, val loss: 0.05286095663905144\n",
      "Epoch 6481: train loss: 0.02258935011923313, val loss: 0.06896372884511948\n",
      "Epoch 6482: train loss: 0.018587598577141762, val loss: 0.05872553586959839\n",
      "Epoch 6483: train loss: 0.024435976520180702, val loss: 0.06754585355520248\n",
      "Epoch 6484: train loss: 0.0240729171782732, val loss: 0.054836224764585495\n",
      "Epoch 6485: train loss: 0.02251500077545643, val loss: 0.0670580193400383\n",
      "Epoch 6486: train loss: 0.01990635134279728, val loss: 0.11780617386102676\n",
      "Epoch 6487: train loss: 0.015618936158716679, val loss: 0.0597093291580677\n",
      "Epoch 6488: train loss: 0.016969848424196243, val loss: 0.06686781346797943\n",
      "Epoch 6489: train loss: 0.019868280738592148, val loss: 0.08787556737661362\n",
      "Epoch 6490: train loss: 0.01978359743952751, val loss: 0.04496265947818756\n",
      "Epoch 6491: train loss: 0.022482559084892273, val loss: 0.08873824030160904\n",
      "Epoch 6492: train loss: 0.022310085594654083, val loss: 0.07316049188375473\n",
      "Epoch 6493: train loss: 0.01664862409234047, val loss: 0.07976971566677094\n",
      "Epoch 6494: train loss: 0.022025061771273613, val loss: 0.029683584347367287\n",
      "Epoch 6495: train loss: 0.021193256601691246, val loss: 0.08255968242883682\n",
      "Epoch 6496: train loss: 0.016859309747815132, val loss: 0.09430977702140808\n",
      "Epoch 6497: train loss: 0.014456738717854023, val loss: 0.10107006132602692\n",
      "Epoch 6498: train loss: 0.022338734939694405, val loss: 0.0638367086648941\n",
      "Epoch 6499: train loss: 0.023988502100110054, val loss: 0.12801127135753632\n",
      "Epoch 6500: train loss: 0.025266626849770546, val loss: 0.0510333776473999\n",
      "Epoch 6501: train loss: 0.024328360334038734, val loss: 0.10609608143568039\n",
      "Epoch 6502: train loss: 0.026912692934274673, val loss: 0.05919882282614708\n",
      "Epoch 6503: train loss: 0.016118107363581657, val loss: 0.07330571860074997\n",
      "Epoch 6504: train loss: 0.022376762703061104, val loss: 0.09953134506940842\n",
      "Epoch 6505: train loss: 0.0181187205016613, val loss: 0.05893874913454056\n",
      "Epoch 6506: train loss: 0.013372941873967648, val loss: 0.14294865727424622\n",
      "Epoch 6507: train loss: 0.014716203324496746, val loss: 0.08800681680440903\n",
      "Epoch 6508: train loss: 0.017375344410538673, val loss: 0.06467768549919128\n",
      "Epoch 6509: train loss: 0.023286039009690285, val loss: 0.09146507829427719\n",
      "Epoch 6510: train loss: 0.02023417502641678, val loss: 0.06556554138660431\n",
      "Epoch 6511: train loss: 0.018399517983198166, val loss: 0.08493738621473312\n",
      "Epoch 6512: train loss: 0.016448434442281723, val loss: 0.0747646763920784\n",
      "Epoch 6513: train loss: 0.020254800096154213, val loss: 0.11239814013242722\n",
      "Epoch 6514: train loss: 0.016113853082060814, val loss: 0.09028521925210953\n",
      "Epoch 6515: train loss: 0.021624470129609108, val loss: 0.09245114773511887\n",
      "Epoch 6516: train loss: 0.017925789579749107, val loss: 0.09128087013959885\n",
      "Epoch 6517: train loss: 0.01656177267432213, val loss: 0.06196592003107071\n",
      "Epoch 6518: train loss: 0.01720046065747738, val loss: 0.04406546801328659\n",
      "Epoch 6519: train loss: 0.029886607080698013, val loss: 0.09060128778219223\n",
      "Epoch 6520: train loss: 0.01472255028784275, val loss: 0.07191084325313568\n",
      "Epoch 6521: train loss: 0.014336439780890942, val loss: 0.04628558084368706\n",
      "Epoch 6522: train loss: 0.01996566727757454, val loss: 0.07865222543478012\n",
      "Epoch 6523: train loss: 0.016415132209658623, val loss: 0.07152921706438065\n",
      "Epoch 6524: train loss: 0.022360293194651604, val loss: 0.13545720279216766\n",
      "Epoch 6525: train loss: 0.01809779554605484, val loss: 0.09294290840625763\n",
      "Epoch 6526: train loss: 0.018550211563706398, val loss: 0.09230287373065948\n",
      "Epoch 6527: train loss: 0.01605203188955784, val loss: 0.06593623012304306\n",
      "Epoch 6528: train loss: 0.023026002570986748, val loss: 0.05069239065051079\n",
      "Epoch 6529: train loss: 0.02364218235015869, val loss: 0.08219760656356812\n",
      "Epoch 6530: train loss: 0.015276794321835041, val loss: 0.06967610120773315\n",
      "Epoch 6531: train loss: 0.02222483418881893, val loss: 0.07390403747558594\n",
      "Epoch 6532: train loss: 0.02743196301162243, val loss: 0.04255503788590431\n",
      "Epoch 6533: train loss: 0.012143864296376705, val loss: 0.0640006735920906\n",
      "Epoch 6534: train loss: 0.014718767255544662, val loss: 0.08511870354413986\n",
      "Epoch 6535: train loss: 0.02006213180720806, val loss: 0.059561338275671005\n",
      "Epoch 6536: train loss: 0.02335488051176071, val loss: 0.08439882844686508\n",
      "Epoch 6537: train loss: 0.020983943715691566, val loss: 0.10983920097351074\n",
      "Epoch 6538: train loss: 0.02071337029337883, val loss: 0.08273061364889145\n",
      "Epoch 6539: train loss: 0.02452883869409561, val loss: 0.03718683868646622\n",
      "Epoch 6540: train loss: 0.02245410345494747, val loss: 0.06882359087467194\n",
      "Epoch 6541: train loss: 0.020330319181084633, val loss: 0.07394776493310928\n",
      "Epoch 6542: train loss: 0.019807837903499603, val loss: 0.0784597173333168\n",
      "Epoch 6543: train loss: 0.016826650127768517, val loss: 0.037786126136779785\n",
      "Epoch 6544: train loss: 0.01846231333911419, val loss: 0.07982852309942245\n",
      "Epoch 6545: train loss: 0.014830059371888638, val loss: 0.061982136219739914\n",
      "Epoch 6546: train loss: 0.02380821481347084, val loss: 0.08902847766876221\n",
      "Epoch 6547: train loss: 0.014681808650493622, val loss: 0.0687100887298584\n",
      "Epoch 6548: train loss: 0.017251882702112198, val loss: 0.05869770795106888\n",
      "Epoch 6549: train loss: 0.024632452055811882, val loss: 0.0964507982134819\n",
      "Epoch 6550: train loss: 0.015599489212036133, val loss: 0.06378652155399323\n",
      "Epoch 6551: train loss: 0.02597171999514103, val loss: 0.08534350246191025\n",
      "Epoch 6552: train loss: 0.016233336180448532, val loss: 0.03710118681192398\n",
      "Epoch 6553: train loss: 0.020206715911626816, val loss: 0.07013465464115143\n",
      "Epoch 6554: train loss: 0.018365388736128807, val loss: 0.0794910341501236\n",
      "Epoch 6555: train loss: 0.023937422782182693, val loss: 0.03347172960639\n",
      "Epoch 6556: train loss: 0.02391120046377182, val loss: 0.057220395654439926\n",
      "Epoch 6557: train loss: 0.02384689263999462, val loss: 0.05473257228732109\n",
      "Epoch 6558: train loss: 0.02284926176071167, val loss: 0.05108194425702095\n",
      "Epoch 6559: train loss: 0.019614344462752342, val loss: 0.07447882741689682\n",
      "Epoch 6560: train loss: 0.021134335547685623, val loss: 0.05703982338309288\n",
      "Epoch 6561: train loss: 0.01989995315670967, val loss: 0.08653158694505692\n",
      "Epoch 6562: train loss: 0.021542411297559738, val loss: 0.08061148971319199\n",
      "Epoch 6563: train loss: 0.02367003820836544, val loss: 0.09944877028465271\n",
      "Epoch 6564: train loss: 0.021499289199709892, val loss: 0.06961985677480698\n",
      "Epoch 6565: train loss: 0.018048932775855064, val loss: 0.10120918601751328\n",
      "Epoch 6566: train loss: 0.02014402113854885, val loss: 0.03494549170136452\n",
      "Epoch 6567: train loss: 0.01913219690322876, val loss: 0.04710787534713745\n",
      "Epoch 6568: train loss: 0.014659429900348186, val loss: 0.08034259080886841\n",
      "Epoch 6569: train loss: 0.02069179341197014, val loss: 0.053368378430604935\n",
      "Epoch 6570: train loss: 0.017916031181812286, val loss: 0.10777026414871216\n",
      "Epoch 6571: train loss: 0.022205853834748268, val loss: 0.06760319322347641\n",
      "Epoch 6572: train loss: 0.019271813333034515, val loss: 0.0750398263335228\n",
      "Epoch 6573: train loss: 0.015419880859553814, val loss: 0.08769626915454865\n",
      "Epoch 6574: train loss: 0.02884402871131897, val loss: 0.128905788064003\n",
      "Epoch 6575: train loss: 0.01673530973494053, val loss: 0.0737532526254654\n",
      "Epoch 6576: train loss: 0.0261443629860878, val loss: 0.06882500648498535\n",
      "Epoch 6577: train loss: 0.017423629760742188, val loss: 0.08156819641590118\n",
      "Epoch 6578: train loss: 0.015737252309918404, val loss: 0.07121672481298447\n",
      "Epoch 6579: train loss: 0.017323080450296402, val loss: 0.055585719645023346\n",
      "Epoch 6580: train loss: 0.01748361997306347, val loss: 0.07781101763248444\n",
      "Epoch 6581: train loss: 0.0233804602175951, val loss: 0.05650705099105835\n",
      "Epoch 6582: train loss: 0.018187202513217926, val loss: 0.09603410214185715\n",
      "Epoch 6583: train loss: 0.017991075292229652, val loss: 0.09031961858272552\n",
      "Epoch 6584: train loss: 0.02273469604551792, val loss: 0.06747087091207504\n",
      "Epoch 6585: train loss: 0.02101801708340645, val loss: 0.0717201828956604\n",
      "Epoch 6586: train loss: 0.01880551502108574, val loss: 0.05757293850183487\n",
      "Epoch 6587: train loss: 0.016631845384836197, val loss: 0.103181853890419\n",
      "Epoch 6588: train loss: 0.02149084210395813, val loss: 0.05636465549468994\n",
      "Epoch 6589: train loss: 0.01627805456519127, val loss: 0.10473469644784927\n",
      "Epoch 6590: train loss: 0.021214893087744713, val loss: 0.0893302783370018\n",
      "Epoch 6591: train loss: 0.01845056749880314, val loss: 0.11752207577228546\n",
      "Epoch 6592: train loss: 0.017802292481064796, val loss: 0.1222412958741188\n",
      "Epoch 6593: train loss: 0.019208746030926704, val loss: 0.052943795919418335\n",
      "Epoch 6594: train loss: 0.01847108267247677, val loss: 0.06308555603027344\n",
      "Epoch 6595: train loss: 0.016589196398854256, val loss: 0.06714081764221191\n",
      "Epoch 6596: train loss: 0.019044678658246994, val loss: 0.05924931913614273\n",
      "Epoch 6597: train loss: 0.019687050953507423, val loss: 0.08172961324453354\n",
      "Epoch 6598: train loss: 0.02499469369649887, val loss: 0.09301769733428955\n",
      "Epoch 6599: train loss: 0.020840780809521675, val loss: 0.06666441261768341\n",
      "Epoch 6600: train loss: 0.019561797380447388, val loss: 0.06386012583971024\n",
      "Epoch 6601: train loss: 0.016525141894817352, val loss: 0.06672510504722595\n",
      "Epoch 6602: train loss: 0.022210802882909775, val loss: 0.07449599355459213\n",
      "Epoch 6603: train loss: 0.023997142910957336, val loss: 0.0844213142991066\n",
      "Epoch 6604: train loss: 0.018210621550679207, val loss: 0.053691744804382324\n",
      "Epoch 6605: train loss: 0.022198934108018875, val loss: 0.051175978034734726\n",
      "Epoch 6606: train loss: 0.016995897516608238, val loss: 0.059148527681827545\n",
      "Epoch 6607: train loss: 0.021550003439188004, val loss: 0.058050353080034256\n",
      "Epoch 6608: train loss: 0.018361791968345642, val loss: 0.06920164078474045\n",
      "Epoch 6609: train loss: 0.01791711151599884, val loss: 0.04110737890005112\n",
      "Epoch 6610: train loss: 0.024088794365525246, val loss: 0.09105946868658066\n",
      "Epoch 6611: train loss: 0.012581530027091503, val loss: 0.07972358912229538\n",
      "Epoch 6612: train loss: 0.0159288439899683, val loss: 0.06463220715522766\n",
      "Epoch 6613: train loss: 0.018209567293524742, val loss: 0.05905146151781082\n",
      "Epoch 6614: train loss: 0.018200842663645744, val loss: 0.04938899725675583\n",
      "Epoch 6615: train loss: 0.017544781789183617, val loss: 0.038443565368652344\n",
      "Epoch 6616: train loss: 0.017535310238599777, val loss: 0.08317858725786209\n",
      "Epoch 6617: train loss: 0.017803192138671875, val loss: 0.10298850387334824\n",
      "Epoch 6618: train loss: 0.024894021451473236, val loss: 0.06111622601747513\n",
      "Epoch 6619: train loss: 0.01693640649318695, val loss: 0.07826092094182968\n",
      "Epoch 6620: train loss: 0.01598036102950573, val loss: 0.06911484897136688\n",
      "Epoch 6621: train loss: 0.017428135499358177, val loss: 0.06749604642391205\n",
      "Epoch 6622: train loss: 0.018573632463812828, val loss: 0.06214647367596626\n",
      "Epoch 6623: train loss: 0.02029999904334545, val loss: 0.07314351201057434\n",
      "Epoch 6624: train loss: 0.018118679523468018, val loss: 0.06590371578931808\n",
      "Epoch 6625: train loss: 0.021565742790699005, val loss: 0.06912299245595932\n",
      "Epoch 6626: train loss: 0.02510569989681244, val loss: 0.06370572745800018\n",
      "Epoch 6627: train loss: 0.026088889688253403, val loss: 0.12405171245336533\n",
      "Epoch 6628: train loss: 0.01718619093298912, val loss: 0.05158321186900139\n",
      "Epoch 6629: train loss: 0.021876921877264977, val loss: 0.04760773107409477\n",
      "Epoch 6630: train loss: 0.02429964952170849, val loss: 0.07208471745252609\n",
      "Epoch 6631: train loss: 0.019679425284266472, val loss: 0.07476971298456192\n",
      "Epoch 6632: train loss: 0.020525703206658363, val loss: 0.10277757793664932\n",
      "Epoch 6633: train loss: 0.023328861221671104, val loss: 0.08501686155796051\n",
      "Epoch 6634: train loss: 0.017865683883428574, val loss: 0.05223016068339348\n",
      "Epoch 6635: train loss: 0.02513252943754196, val loss: 0.08075235038995743\n",
      "Epoch 6636: train loss: 0.022105496376752853, val loss: 0.10831065475940704\n",
      "Epoch 6637: train loss: 0.019559942185878754, val loss: 0.06543892621994019\n",
      "Epoch 6638: train loss: 0.018624575808644295, val loss: 0.08518418669700623\n",
      "Epoch 6639: train loss: 0.020469555631279945, val loss: 0.05213078483939171\n",
      "Epoch 6640: train loss: 0.020291423425078392, val loss: 0.06559702008962631\n",
      "Epoch 6641: train loss: 0.02578521892428398, val loss: 0.04548785462975502\n",
      "Epoch 6642: train loss: 0.02682090923190117, val loss: 0.01965474523603916\n",
      "Epoch 6643: train loss: 0.0288921520113945, val loss: 0.04361235722899437\n",
      "Epoch 6644: train loss: 0.023338085040450096, val loss: 0.09253276884555817\n",
      "Epoch 6645: train loss: 0.023535994812846184, val loss: 0.056740857660770416\n",
      "Epoch 6646: train loss: 0.02025003544986248, val loss: 0.08890574425458908\n",
      "Epoch 6647: train loss: 0.032516710460186005, val loss: 0.09304054081439972\n",
      "Epoch 6648: train loss: 0.022752614691853523, val loss: 0.06456059217453003\n",
      "Epoch 6649: train loss: 0.014820622280240059, val loss: 0.10247372835874557\n",
      "Epoch 6650: train loss: 0.024885358288884163, val loss: 0.1102403998374939\n",
      "Epoch 6651: train loss: 0.02121611312031746, val loss: 0.09427224844694138\n",
      "Epoch 6652: train loss: 0.01834162138402462, val loss: 0.05308462306857109\n",
      "Epoch 6653: train loss: 0.021262669935822487, val loss: 0.06892051547765732\n",
      "Epoch 6654: train loss: 0.019779104739427567, val loss: 0.0645776316523552\n",
      "Epoch 6655: train loss: 0.023424804210662842, val loss: 0.13797084987163544\n",
      "Epoch 6656: train loss: 0.015703685581684113, val loss: 0.08374026417732239\n",
      "Epoch 6657: train loss: 0.0211983323097229, val loss: 0.04169736057519913\n",
      "Epoch 6658: train loss: 0.01427833829075098, val loss: 0.07608657330274582\n",
      "Epoch 6659: train loss: 0.023841824382543564, val loss: 0.09535166621208191\n",
      "Epoch 6660: train loss: 0.019384965300559998, val loss: 0.06122681125998497\n",
      "Epoch 6661: train loss: 0.021256491541862488, val loss: 0.0484900027513504\n",
      "Epoch 6662: train loss: 0.023356692865490913, val loss: 0.08462991565465927\n",
      "Epoch 6663: train loss: 0.021128853783011436, val loss: 0.08364652842283249\n",
      "Epoch 6664: train loss: 0.022167576476931572, val loss: 0.06521632522344589\n",
      "Epoch 6665: train loss: 0.017860880121588707, val loss: 0.05080818757414818\n",
      "Epoch 6666: train loss: 0.024113729596138, val loss: 0.12725511193275452\n",
      "Epoch 6667: train loss: 0.026410045102238655, val loss: 0.05531958490610123\n",
      "Epoch 6668: train loss: 0.020871365442872047, val loss: 0.04453488439321518\n",
      "Epoch 6669: train loss: 0.02034599892795086, val loss: 0.12255263328552246\n",
      "Epoch 6670: train loss: 0.019725704565644264, val loss: 0.1078762337565422\n",
      "Epoch 6671: train loss: 0.015989109873771667, val loss: 0.10730896145105362\n",
      "Epoch 6672: train loss: 0.023428885266184807, val loss: 0.05003265291452408\n",
      "Epoch 6673: train loss: 0.019319508224725723, val loss: 0.10389944165945053\n",
      "Epoch 6674: train loss: 0.019859224557876587, val loss: 0.06010397896170616\n",
      "Epoch 6675: train loss: 0.023836888372898102, val loss: 0.07959728688001633\n",
      "Epoch 6676: train loss: 0.013234708458185196, val loss: 0.08969807624816895\n",
      "Epoch 6677: train loss: 0.019635319709777832, val loss: 0.059258054941892624\n",
      "Epoch 6678: train loss: 0.022029047831892967, val loss: 0.0889546126127243\n",
      "Epoch 6679: train loss: 0.017893100157380104, val loss: 0.10490753501653671\n",
      "Epoch 6680: train loss: 0.021883761510252953, val loss: 0.08119143545627594\n",
      "Epoch 6681: train loss: 0.020623106509447098, val loss: 0.06172890588641167\n",
      "Epoch 6682: train loss: 0.01555759459733963, val loss: 0.08839458972215652\n",
      "Epoch 6683: train loss: 0.030831634998321533, val loss: 0.08447007089853287\n",
      "Epoch 6684: train loss: 0.01970175839960575, val loss: 0.07399066537618637\n",
      "Epoch 6685: train loss: 0.022412538528442383, val loss: 0.07800881564617157\n",
      "Epoch 6686: train loss: 0.02247074991464615, val loss: 0.08558573573827744\n",
      "Epoch 6687: train loss: 0.016452664509415627, val loss: 0.10306098312139511\n",
      "Epoch 6688: train loss: 0.01575654186308384, val loss: 0.07588803023099899\n",
      "Epoch 6689: train loss: 0.022550096735358238, val loss: 0.10527276992797852\n",
      "Epoch 6690: train loss: 0.024588240310549736, val loss: 0.09518913179636002\n",
      "Epoch 6691: train loss: 0.022193552926182747, val loss: 0.09437213838100433\n",
      "Epoch 6692: train loss: 0.019011827185750008, val loss: 0.08160512894392014\n",
      "Epoch 6693: train loss: 0.02395218051970005, val loss: 0.08987877517938614\n",
      "Epoch 6694: train loss: 0.020100511610507965, val loss: 0.1235058531165123\n",
      "Epoch 6695: train loss: 0.017814502120018005, val loss: 0.11276207119226456\n",
      "Epoch 6696: train loss: 0.019932821393013, val loss: 0.12832091748714447\n",
      "Epoch 6697: train loss: 0.018702173605561256, val loss: 0.08123129606246948\n",
      "Epoch 6698: train loss: 0.023947376757860184, val loss: 0.10508276522159576\n",
      "Epoch 6699: train loss: 0.015174765139818192, val loss: 0.08512493222951889\n",
      "Epoch 6700: train loss: 0.02220509573817253, val loss: 0.09496624767780304\n",
      "Epoch 6701: train loss: 0.01314466167241335, val loss: 0.062141504138708115\n",
      "Epoch 6702: train loss: 0.022223955020308495, val loss: 0.05868967995047569\n",
      "Epoch 6703: train loss: 0.029036911204457283, val loss: 0.0423336997628212\n",
      "Epoch 6704: train loss: 0.028201034292578697, val loss: 0.09838660806417465\n",
      "Epoch 6705: train loss: 0.023136261850595474, val loss: 0.060772862285375595\n",
      "Epoch 6706: train loss: 0.026740046218037605, val loss: 0.10182561725378036\n",
      "Epoch 6707: train loss: 0.023749129846692085, val loss: 0.03962178900837898\n",
      "Epoch 6708: train loss: 0.020914774388074875, val loss: 0.07979889959096909\n",
      "Epoch 6709: train loss: 0.020351119339466095, val loss: 0.10032776743173599\n",
      "Epoch 6710: train loss: 0.02400747500360012, val loss: 0.08141565322875977\n",
      "Epoch 6711: train loss: 0.023508507758378983, val loss: 0.11458396911621094\n",
      "Epoch 6712: train loss: 0.019099455326795578, val loss: 0.10052535682916641\n",
      "Epoch 6713: train loss: 0.014422438107430935, val loss: 0.06349878758192062\n",
      "Epoch 6714: train loss: 0.020457293838262558, val loss: 0.0988660678267479\n",
      "Epoch 6715: train loss: 0.028503479436039925, val loss: 0.08739256858825684\n",
      "Epoch 6716: train loss: 0.020292440429329872, val loss: 0.036452215164899826\n",
      "Epoch 6717: train loss: 0.018351178616285324, val loss: 0.08612724393606186\n",
      "Epoch 6718: train loss: 0.035469651222229004, val loss: 0.05044139176607132\n",
      "Epoch 6719: train loss: 0.019986402243375778, val loss: 0.05408892780542374\n",
      "Epoch 6720: train loss: 0.020117126405239105, val loss: 0.09174755215644836\n",
      "Epoch 6721: train loss: 0.02515271306037903, val loss: 0.04992474988102913\n",
      "Epoch 6722: train loss: 0.022767266258597374, val loss: 0.03724848851561546\n",
      "Epoch 6723: train loss: 0.019077295437455177, val loss: 0.07179949432611465\n",
      "Epoch 6724: train loss: 0.02479838766157627, val loss: 0.05989878252148628\n",
      "Epoch 6725: train loss: 0.019400453194975853, val loss: 0.08360686898231506\n",
      "Epoch 6726: train loss: 0.02554692141711712, val loss: 0.07739067077636719\n",
      "Epoch 6727: train loss: 0.016761505976319313, val loss: 0.05688946321606636\n",
      "Epoch 6728: train loss: 0.01783665269613266, val loss: 0.07852324098348618\n",
      "Epoch 6729: train loss: 0.022868404164910316, val loss: 0.06062200665473938\n",
      "Epoch 6730: train loss: 0.022201525047421455, val loss: 0.09491019695997238\n",
      "Epoch 6731: train loss: 0.023011399433016777, val loss: 0.06630300730466843\n",
      "Epoch 6732: train loss: 0.021193912252783775, val loss: 0.09125974029302597\n",
      "Epoch 6733: train loss: 0.01986938714981079, val loss: 0.08524704724550247\n",
      "Epoch 6734: train loss: 0.023929743096232414, val loss: 0.07766865938901901\n",
      "Epoch 6735: train loss: 0.018990550190210342, val loss: 0.08421500027179718\n",
      "Epoch 6736: train loss: 0.015495414845645428, val loss: 0.0585060715675354\n",
      "Epoch 6737: train loss: 0.02087215706706047, val loss: 0.07703951746225357\n",
      "Epoch 6738: train loss: 0.020512279123067856, val loss: 0.08093626797199249\n",
      "Epoch 6739: train loss: 0.020323317497968674, val loss: 0.08346929401159286\n",
      "Epoch 6740: train loss: 0.026751931756734848, val loss: 0.06332725286483765\n",
      "Epoch 6741: train loss: 0.017464425414800644, val loss: 0.10184849798679352\n",
      "Epoch 6742: train loss: 0.01936347596347332, val loss: 0.0511813759803772\n",
      "Epoch 6743: train loss: 0.023192215710878372, val loss: 0.08280486613512039\n",
      "Epoch 6744: train loss: 0.024701301008462906, val loss: 0.06337811797857285\n",
      "Epoch 6745: train loss: 0.020072007551789284, val loss: 0.0523444302380085\n",
      "Epoch 6746: train loss: 0.01923169381916523, val loss: 0.06315996497869492\n",
      "Epoch 6747: train loss: 0.018696095794439316, val loss: 0.08110537379980087\n",
      "Epoch 6748: train loss: 0.015218120068311691, val loss: 0.06446506828069687\n",
      "Epoch 6749: train loss: 0.017638636752963066, val loss: 0.07491689175367355\n",
      "Epoch 6750: train loss: 0.021294061094522476, val loss: 0.059462081640958786\n",
      "Epoch 6751: train loss: 0.022281859070062637, val loss: 0.053949594497680664\n",
      "Epoch 6752: train loss: 0.02039465494453907, val loss: 0.082671158015728\n",
      "Epoch 6753: train loss: 0.024058207869529724, val loss: 0.07168037444353104\n",
      "Epoch 6754: train loss: 0.01778436079621315, val loss: 0.07646294683218002\n",
      "Epoch 6755: train loss: 0.023836437612771988, val loss: 0.08574779331684113\n",
      "Epoch 6756: train loss: 0.019091404974460602, val loss: 0.037241131067276\n",
      "Epoch 6757: train loss: 0.017218053340911865, val loss: 0.07943566143512726\n",
      "Epoch 6758: train loss: 0.018503790721297264, val loss: 0.06030004099011421\n",
      "Epoch 6759: train loss: 0.022645076736807823, val loss: 0.03835513815283775\n",
      "Epoch 6760: train loss: 0.021816551685333252, val loss: 0.04190850257873535\n",
      "Epoch 6761: train loss: 0.01778191700577736, val loss: 0.06915800273418427\n",
      "Epoch 6762: train loss: 0.01653924398124218, val loss: 0.07932605594396591\n",
      "Epoch 6763: train loss: 0.01874779723584652, val loss: 0.07313665002584457\n",
      "Epoch 6764: train loss: 0.018928678706288338, val loss: 0.05434001237154007\n",
      "Epoch 6765: train loss: 0.025953184813261032, val loss: 0.07738985121250153\n",
      "Epoch 6766: train loss: 0.01771668717265129, val loss: 0.04383000731468201\n",
      "Epoch 6767: train loss: 0.016745420172810555, val loss: 0.07900822162628174\n",
      "Epoch 6768: train loss: 0.02383580058813095, val loss: 0.11511901766061783\n",
      "Epoch 6769: train loss: 0.02205422706902027, val loss: 0.12041106075048447\n",
      "Epoch 6770: train loss: 0.027499111369252205, val loss: 0.08273198455572128\n",
      "Epoch 6771: train loss: 0.02057223953306675, val loss: 0.04898852854967117\n",
      "Epoch 6772: train loss: 0.023777682334184647, val loss: 0.09545767307281494\n",
      "Epoch 6773: train loss: 0.013495971448719501, val loss: 0.06985863298177719\n",
      "Epoch 6774: train loss: 0.01946905441582203, val loss: 0.12167932838201523\n",
      "Epoch 6775: train loss: 0.020816564559936523, val loss: 0.05666666105389595\n",
      "Epoch 6776: train loss: 0.019920993596315384, val loss: 0.06907695531845093\n",
      "Epoch 6777: train loss: 0.02189510688185692, val loss: 0.05534868314862251\n",
      "Epoch 6778: train loss: 0.01774429902434349, val loss: 0.09860815107822418\n",
      "Epoch 6779: train loss: 0.01896202377974987, val loss: 0.07436654716730118\n",
      "Epoch 6780: train loss: 0.020892394706606865, val loss: 0.1426638811826706\n",
      "Epoch 6781: train loss: 0.018563546240329742, val loss: 0.07358842343091965\n",
      "Epoch 6782: train loss: 0.024297306314110756, val loss: 0.04865128919482231\n",
      "Epoch 6783: train loss: 0.02031530626118183, val loss: 0.06381808966398239\n",
      "Epoch 6784: train loss: 0.028137825429439545, val loss: 0.08466017246246338\n",
      "Epoch 6785: train loss: 0.017044631764292717, val loss: 0.1372990906238556\n",
      "Epoch 6786: train loss: 0.01986689306795597, val loss: 0.07935457676649094\n",
      "Epoch 6787: train loss: 0.017298711463809013, val loss: 0.047613970935344696\n",
      "Epoch 6788: train loss: 0.022633293643593788, val loss: 0.0724436417222023\n",
      "Epoch 6789: train loss: 0.01653219945728779, val loss: 0.08505361527204514\n",
      "Epoch 6790: train loss: 0.01826821267604828, val loss: 0.09414254128932953\n",
      "Epoch 6791: train loss: 0.017434049397706985, val loss: 0.06719795614480972\n",
      "Epoch 6792: train loss: 0.017155271023511887, val loss: 0.05358967185020447\n",
      "Epoch 6793: train loss: 0.013655251823365688, val loss: 0.059363748878240585\n",
      "Epoch 6794: train loss: 0.023720938712358475, val loss: 0.07838045805692673\n",
      "Epoch 6795: train loss: 0.017999503761529922, val loss: 0.09207414835691452\n",
      "Epoch 6796: train loss: 0.025532370433211327, val loss: 0.03993261978030205\n",
      "Epoch 6797: train loss: 0.01772969216108322, val loss: 0.0591030977666378\n",
      "Epoch 6798: train loss: 0.02669226936995983, val loss: 0.08000034093856812\n",
      "Epoch 6799: train loss: 0.016088848933577538, val loss: 0.0669858530163765\n",
      "Epoch 6800: train loss: 0.02394363470375538, val loss: 0.09107163548469543\n",
      "Epoch 6801: train loss: 0.01361801102757454, val loss: 0.0990409180521965\n",
      "Epoch 6802: train loss: 0.01829301379621029, val loss: 0.05353058502078056\n",
      "Epoch 6803: train loss: 0.016794782131910324, val loss: 0.07744207233190536\n",
      "Epoch 6804: train loss: 0.019677871838212013, val loss: 0.07302866876125336\n",
      "Epoch 6805: train loss: 0.017233379185199738, val loss: 0.08673274517059326\n",
      "Epoch 6806: train loss: 0.017189959064126015, val loss: 0.08395951986312866\n",
      "Epoch 6807: train loss: 0.014740923419594765, val loss: 0.09277334064245224\n",
      "Epoch 6808: train loss: 0.02050120383501053, val loss: 0.06622680276632309\n",
      "Epoch 6809: train loss: 0.018381163477897644, val loss: 0.10686814785003662\n",
      "Epoch 6810: train loss: 0.01525872852653265, val loss: 0.09858854115009308\n",
      "Epoch 6811: train loss: 0.01472565345466137, val loss: 0.0733105018734932\n",
      "Epoch 6812: train loss: 0.019382929429411888, val loss: 0.09938535839319229\n",
      "Epoch 6813: train loss: 0.014236858114600182, val loss: 0.11354904621839523\n",
      "Epoch 6814: train loss: 0.018804704770445824, val loss: 0.09009978920221329\n",
      "Epoch 6815: train loss: 0.028002940118312836, val loss: 0.07448388636112213\n",
      "Epoch 6816: train loss: 0.01645566336810589, val loss: 0.11316191405057907\n",
      "Epoch 6817: train loss: 0.017924346029758453, val loss: 0.057964373379945755\n",
      "Epoch 6818: train loss: 0.01921140030026436, val loss: 0.06786437332630157\n",
      "Epoch 6819: train loss: 0.019498489797115326, val loss: 0.06666605919599533\n",
      "Epoch 6820: train loss: 0.01744231767952442, val loss: 0.10577761381864548\n",
      "Epoch 6821: train loss: 0.018801862373948097, val loss: 0.07579006254673004\n",
      "Epoch 6822: train loss: 0.020965969190001488, val loss: 0.06000768765807152\n",
      "Epoch 6823: train loss: 0.022654226049780846, val loss: 0.09888392686843872\n",
      "Epoch 6824: train loss: 0.018965013325214386, val loss: 0.10666697472333908\n",
      "Epoch 6825: train loss: 0.020389234647154808, val loss: 0.09563103318214417\n",
      "Epoch 6826: train loss: 0.021100444719195366, val loss: 0.10458312183618546\n",
      "Epoch 6827: train loss: 0.01634899340569973, val loss: 0.0706859827041626\n",
      "Epoch 6828: train loss: 0.019676892086863518, val loss: 0.09966769069433212\n",
      "Epoch 6829: train loss: 0.017635414376854897, val loss: 0.03777817264199257\n",
      "Epoch 6830: train loss: 0.02306935004889965, val loss: 0.08560505509376526\n",
      "Epoch 6831: train loss: 0.02374374121427536, val loss: 0.11568112671375275\n",
      "Epoch 6832: train loss: 0.018229983747005463, val loss: 0.0698275938630104\n",
      "Epoch 6833: train loss: 0.015635782852768898, val loss: 0.07924093306064606\n",
      "Epoch 6834: train loss: 0.020044168457388878, val loss: 0.06259875744581223\n",
      "Epoch 6835: train loss: 0.01735542342066765, val loss: 0.06974367797374725\n",
      "Epoch 6836: train loss: 0.019577112048864365, val loss: 0.0965661033987999\n",
      "Epoch 6837: train loss: 0.012493823654949665, val loss: 0.12010041624307632\n",
      "Epoch 6838: train loss: 0.022733399644494057, val loss: 0.05335510894656181\n",
      "Epoch 6839: train loss: 0.021054726094007492, val loss: 0.0744878277182579\n",
      "Epoch 6840: train loss: 0.018653592094779015, val loss: 0.09577854722738266\n",
      "Epoch 6841: train loss: 0.021732836961746216, val loss: 0.05727676302194595\n",
      "Epoch 6842: train loss: 0.014335311949253082, val loss: 0.1026846170425415\n",
      "Epoch 6843: train loss: 0.021188799291849136, val loss: 0.08179932832717896\n",
      "Epoch 6844: train loss: 0.012524996884167194, val loss: 0.08125145733356476\n",
      "Epoch 6845: train loss: 0.02146892435848713, val loss: 0.06483159959316254\n",
      "Epoch 6846: train loss: 0.01983090676367283, val loss: 0.07316821068525314\n",
      "Epoch 6847: train loss: 0.026568986475467682, val loss: 0.06799667328596115\n",
      "Epoch 6848: train loss: 0.023058244958519936, val loss: 0.06576446443796158\n",
      "Epoch 6849: train loss: 0.018587330356240273, val loss: 0.06366173177957535\n",
      "Epoch 6850: train loss: 0.01909622922539711, val loss: 0.06006132438778877\n",
      "Epoch 6851: train loss: 0.022180676460266113, val loss: 0.11219930648803711\n",
      "Epoch 6852: train loss: 0.019118547439575195, val loss: 0.10020889341831207\n",
      "Epoch 6853: train loss: 0.01563713327050209, val loss: 0.12798349559307098\n",
      "Epoch 6854: train loss: 0.029667478054761887, val loss: 0.08917751163244247\n",
      "Epoch 6855: train loss: 0.02024049311876297, val loss: 0.0861402079463005\n",
      "Epoch 6856: train loss: 0.021122628822922707, val loss: 0.06855513900518417\n",
      "Epoch 6857: train loss: 0.01976100727915764, val loss: 0.07869403064250946\n",
      "Epoch 6858: train loss: 0.023345738649368286, val loss: 0.029058337211608887\n",
      "Epoch 6859: train loss: 0.020336702466011047, val loss: 0.10457111895084381\n",
      "Epoch 6860: train loss: 0.027965625748038292, val loss: 0.07688931375741959\n",
      "Epoch 6861: train loss: 0.01753889210522175, val loss: 0.046991314738988876\n",
      "Epoch 6862: train loss: 0.02006223052740097, val loss: 0.06248548626899719\n",
      "Epoch 6863: train loss: 0.02498689293861389, val loss: 0.1339046210050583\n",
      "Epoch 6864: train loss: 0.025665882974863052, val loss: 0.13147428631782532\n",
      "Epoch 6865: train loss: 0.030235735699534416, val loss: 0.1043030247092247\n",
      "Epoch 6866: train loss: 0.020486313849687576, val loss: 0.04130862280726433\n",
      "Epoch 6867: train loss: 0.020024936646223068, val loss: 0.13131217658519745\n",
      "Epoch 6868: train loss: 0.018124988302588463, val loss: 0.07176323235034943\n",
      "Epoch 6869: train loss: 0.022304249927401543, val loss: 0.06166354939341545\n",
      "Epoch 6870: train loss: 0.018379028886556625, val loss: 0.059208907186985016\n",
      "Epoch 6871: train loss: 0.024621684104204178, val loss: 0.09217654913663864\n",
      "Epoch 6872: train loss: 0.01931759901344776, val loss: 0.09408048540353775\n",
      "Epoch 6873: train loss: 0.017501138150691986, val loss: 0.08961015194654465\n",
      "Epoch 6874: train loss: 0.0174757968634367, val loss: 0.05439004302024841\n",
      "Epoch 6875: train loss: 0.01598695106804371, val loss: 0.08378147333860397\n",
      "Epoch 6876: train loss: 0.022493720054626465, val loss: 0.060780275613069534\n",
      "Epoch 6877: train loss: 0.016341736540198326, val loss: 0.046275507658720016\n",
      "Epoch 6878: train loss: 0.015269401483237743, val loss: 0.07193083316087723\n",
      "Epoch 6879: train loss: 0.012675255537033081, val loss: 0.09865313768386841\n",
      "Epoch 6880: train loss: 0.017057254910469055, val loss: 0.07433798164129257\n",
      "Epoch 6881: train loss: 0.01781501993536949, val loss: 0.07199589908123016\n",
      "Epoch 6882: train loss: 0.016007207334041595, val loss: 0.07652276009321213\n",
      "Epoch 6883: train loss: 0.015103367157280445, val loss: 0.06917677074670792\n",
      "Epoch 6884: train loss: 0.020462241023778915, val loss: 0.07497117668390274\n",
      "Epoch 6885: train loss: 0.017098112031817436, val loss: 0.06531620025634766\n",
      "Epoch 6886: train loss: 0.023948749527335167, val loss: 0.08301350474357605\n",
      "Epoch 6887: train loss: 0.019933436065912247, val loss: 0.05432242900133133\n",
      "Epoch 6888: train loss: 0.021526122465729713, val loss: 0.06655800342559814\n",
      "Epoch 6889: train loss: 0.014928656630218029, val loss: 0.06807448714971542\n",
      "Epoch 6890: train loss: 0.015992747619748116, val loss: 0.059476256370544434\n",
      "Epoch 6891: train loss: 0.018576886504888535, val loss: 0.08150782436132431\n",
      "Epoch 6892: train loss: 0.016522623598575592, val loss: 0.04595169052481651\n",
      "Epoch 6893: train loss: 0.016749193891882896, val loss: 0.07812213897705078\n",
      "Epoch 6894: train loss: 0.019677741453051567, val loss: 0.09772727638483047\n",
      "Epoch 6895: train loss: 0.015924137085676193, val loss: 0.07154911011457443\n",
      "Epoch 6896: train loss: 0.01640104316174984, val loss: 0.07051490992307663\n",
      "Epoch 6897: train loss: 0.019098054617643356, val loss: 0.06288813054561615\n",
      "Epoch 6898: train loss: 0.021415526047348976, val loss: 0.06397324800491333\n",
      "Epoch 6899: train loss: 0.017933370545506477, val loss: 0.05805729702115059\n",
      "Epoch 6900: train loss: 0.02065356634557247, val loss: 0.02781878411769867\n",
      "Epoch 6901: train loss: 0.021350018680095673, val loss: 0.09923775494098663\n",
      "Epoch 6902: train loss: 0.023672228679060936, val loss: 0.1006123423576355\n",
      "Epoch 6903: train loss: 0.021279627457261086, val loss: 0.05405169352889061\n",
      "Epoch 6904: train loss: 0.01915104128420353, val loss: 0.12060403823852539\n",
      "Epoch 6905: train loss: 0.015249006450176239, val loss: 0.09814762324094772\n",
      "Epoch 6906: train loss: 0.02080724947154522, val loss: 0.0939670205116272\n",
      "Epoch 6907: train loss: 0.019760042428970337, val loss: 0.04298577830195427\n",
      "Epoch 6908: train loss: 0.01183784008026123, val loss: 0.07298523932695389\n",
      "Epoch 6909: train loss: 0.014544516801834106, val loss: 0.07414913177490234\n",
      "Epoch 6910: train loss: 0.027031276375055313, val loss: 0.07432617247104645\n",
      "Epoch 6911: train loss: 0.019859816879034042, val loss: 0.07468920946121216\n",
      "Epoch 6912: train loss: 0.01607697084546089, val loss: 0.07426924258470535\n",
      "Epoch 6913: train loss: 0.015169565565884113, val loss: 0.055393852293491364\n",
      "Epoch 6914: train loss: 0.029509814456105232, val loss: 0.04578068107366562\n",
      "Epoch 6915: train loss: 0.0174307432025671, val loss: 0.050811029970645905\n",
      "Epoch 6916: train loss: 0.014592952094972134, val loss: 0.07656189799308777\n",
      "Epoch 6917: train loss: 0.01893037185072899, val loss: 0.08051284402608871\n",
      "Epoch 6918: train loss: 0.0159304179251194, val loss: 0.05738145112991333\n",
      "Epoch 6919: train loss: 0.019368408247828484, val loss: 0.14153139293193817\n",
      "Epoch 6920: train loss: 0.017668239772319794, val loss: 0.06466416269540787\n",
      "Epoch 6921: train loss: 0.018972719088196754, val loss: 0.04353853687644005\n",
      "Epoch 6922: train loss: 0.019441897049546242, val loss: 0.07054052501916885\n",
      "Epoch 6923: train loss: 0.01984838955104351, val loss: 0.09847890585660934\n",
      "Epoch 6924: train loss: 0.01371300034224987, val loss: 0.050201982259750366\n",
      "Epoch 6925: train loss: 0.018437348306179047, val loss: 0.06509645283222198\n",
      "Epoch 6926: train loss: 0.015056844800710678, val loss: 0.0702686458826065\n",
      "Epoch 6927: train loss: 0.014857500791549683, val loss: 0.07383934408426285\n",
      "Epoch 6928: train loss: 0.02827312797307968, val loss: 0.07834800332784653\n",
      "Epoch 6929: train loss: 0.014793849550187588, val loss: 0.07348746806383133\n",
      "Epoch 6930: train loss: 0.02100304514169693, val loss: 0.06896232813596725\n",
      "Epoch 6931: train loss: 0.022692304104566574, val loss: 0.09157711267471313\n",
      "Epoch 6932: train loss: 0.01840183325111866, val loss: 0.047207217663526535\n",
      "Epoch 6933: train loss: 0.022175518795847893, val loss: 0.05479198694229126\n",
      "Epoch 6934: train loss: 0.023667005822062492, val loss: 0.06574772298336029\n",
      "Epoch 6935: train loss: 0.018977852538228035, val loss: 0.07020284235477448\n",
      "Epoch 6936: train loss: 0.015757683664560318, val loss: 0.07805740088224411\n",
      "Epoch 6937: train loss: 0.013729396276175976, val loss: 0.04314489662647247\n",
      "Epoch 6938: train loss: 0.020204469561576843, val loss: 0.0706075057387352\n",
      "Epoch 6939: train loss: 0.015008354559540749, val loss: 0.12980525195598602\n",
      "Epoch 6940: train loss: 0.017888428643345833, val loss: 0.0462380014359951\n",
      "Epoch 6941: train loss: 0.015350021421909332, val loss: 0.07959521561861038\n",
      "Epoch 6942: train loss: 0.017315443605184555, val loss: 0.07659312337636948\n",
      "Epoch 6943: train loss: 0.017245300114154816, val loss: 0.08146393299102783\n",
      "Epoch 6944: train loss: 0.022265726700425148, val loss: 0.06576146930456161\n",
      "Epoch 6945: train loss: 0.014130959287285805, val loss: 0.06805052608251572\n",
      "Epoch 6946: train loss: 0.020384635776281357, val loss: 0.07763876765966415\n",
      "Epoch 6947: train loss: 0.022924385964870453, val loss: 0.08424114435911179\n",
      "Epoch 6948: train loss: 0.02547476626932621, val loss: 0.13387218117713928\n",
      "Epoch 6949: train loss: 0.019969619810581207, val loss: 0.09539962559938431\n",
      "Epoch 6950: train loss: 0.019861334934830666, val loss: 0.044700395315885544\n",
      "Epoch 6951: train loss: 0.018358387053012848, val loss: 0.07892721146345139\n",
      "Epoch 6952: train loss: 0.025849448516964912, val loss: 0.0506855733692646\n",
      "Epoch 6953: train loss: 0.017930885776877403, val loss: 0.09409888833761215\n",
      "Epoch 6954: train loss: 0.0234653539955616, val loss: 0.046199943870306015\n",
      "Epoch 6955: train loss: 0.021324211731553078, val loss: 0.0391693152487278\n",
      "Epoch 6956: train loss: 0.014325044117867947, val loss: 0.07909611612558365\n",
      "Epoch 6957: train loss: 0.031359340995550156, val loss: 0.04826967790722847\n",
      "Epoch 6958: train loss: 0.012307320721447468, val loss: 0.060785312205553055\n",
      "Epoch 6959: train loss: 0.015072943642735481, val loss: 0.078436478972435\n",
      "Epoch 6960: train loss: 0.011838804930448532, val loss: 0.030835241079330444\n",
      "Epoch 6961: train loss: 0.015558987855911255, val loss: 0.06091419607400894\n",
      "Epoch 6962: train loss: 0.024036690592765808, val loss: 0.057742323726415634\n",
      "Epoch 6963: train loss: 0.02115132287144661, val loss: 0.06645383685827255\n",
      "Epoch 6964: train loss: 0.019649382680654526, val loss: 0.05433765798807144\n",
      "Epoch 6965: train loss: 0.013427438214421272, val loss: 0.07409997284412384\n",
      "Epoch 6966: train loss: 0.013810410164296627, val loss: 0.08454301953315735\n",
      "Epoch 6967: train loss: 0.017314167693257332, val loss: 0.06396755576133728\n",
      "Epoch 6968: train loss: 0.019867440685629845, val loss: 0.04550489783287048\n",
      "Epoch 6969: train loss: 0.014478970319032669, val loss: 0.0799628272652626\n",
      "Epoch 6970: train loss: 0.012765688821673393, val loss: 0.06628356128931046\n",
      "Epoch 6971: train loss: 0.01878127083182335, val loss: 0.06015917286276817\n",
      "Epoch 6972: train loss: 0.028022948652505875, val loss: 0.092373788356781\n",
      "Epoch 6973: train loss: 0.017998432740569115, val loss: 0.05175280198454857\n",
      "Epoch 6974: train loss: 0.02188459038734436, val loss: 0.04203678295016289\n",
      "Epoch 6975: train loss: 0.02145380526781082, val loss: 0.08097859472036362\n",
      "Epoch 6976: train loss: 0.023945173248648643, val loss: 0.047113753855228424\n",
      "Epoch 6977: train loss: 0.02236146666109562, val loss: 0.08855171501636505\n",
      "Epoch 6978: train loss: 0.02147517167031765, val loss: 0.06607513874769211\n",
      "Epoch 6979: train loss: 0.01476513035595417, val loss: 0.07490947097539902\n",
      "Epoch 6980: train loss: 0.010882895439863205, val loss: 0.1046205535531044\n",
      "Epoch 6981: train loss: 0.01533906627446413, val loss: 0.04164500907063484\n",
      "Epoch 6982: train loss: 0.019366107881069183, val loss: 0.04026758670806885\n",
      "Epoch 6983: train loss: 0.01555723138153553, val loss: 0.055349767208099365\n",
      "Epoch 6984: train loss: 0.01850040629506111, val loss: 0.06160853058099747\n",
      "Epoch 6985: train loss: 0.02085525542497635, val loss: 0.06275128573179245\n",
      "Epoch 6986: train loss: 0.02101636864244938, val loss: 0.09323755651712418\n",
      "Epoch 6987: train loss: 0.01529735792428255, val loss: 0.035716794431209564\n",
      "Epoch 6988: train loss: 0.025250397622585297, val loss: 0.06788138300180435\n",
      "Epoch 6989: train loss: 0.013979628682136536, val loss: 0.10133552551269531\n",
      "Epoch 6990: train loss: 0.013610546477138996, val loss: 0.07707180082798004\n",
      "Epoch 6991: train loss: 0.022066228091716766, val loss: 0.06951606273651123\n",
      "Epoch 6992: train loss: 0.016121840104460716, val loss: 0.07051394134759903\n",
      "Epoch 6993: train loss: 0.01960070990025997, val loss: 0.06698259711265564\n",
      "Epoch 6994: train loss: 0.022805387154221535, val loss: 0.07768720388412476\n",
      "Epoch 6995: train loss: 0.015491223894059658, val loss: 0.07407037168741226\n",
      "Epoch 6996: train loss: 0.018850138410925865, val loss: 0.04795053228735924\n",
      "Epoch 6997: train loss: 0.01745731383562088, val loss: 0.04152977094054222\n",
      "Epoch 6998: train loss: 0.023931020870804787, val loss: 0.07611695677042007\n",
      "Epoch 6999: train loss: 0.02430727891623974, val loss: 0.057689912617206573\n",
      "Epoch 7000: train loss: 0.02062833309173584, val loss: 0.09219401329755783\n",
      "Epoch 7001: train loss: 0.016960470005869865, val loss: 0.05108078196644783\n",
      "Epoch 7002: train loss: 0.01962658017873764, val loss: 0.06737023591995239\n",
      "Epoch 7003: train loss: 0.01957280933856964, val loss: 0.06191499903798103\n",
      "Epoch 7004: train loss: 0.020183434709906578, val loss: 0.061670463532209396\n",
      "Epoch 7005: train loss: 0.01593780890107155, val loss: 0.03267635032534599\n",
      "Epoch 7006: train loss: 0.020229758694767952, val loss: 0.05641934275627136\n",
      "Epoch 7007: train loss: 0.01944422721862793, val loss: 0.04470372572541237\n",
      "Epoch 7008: train loss: 0.01949252374470234, val loss: 0.07478596270084381\n",
      "Epoch 7009: train loss: 0.013851507566869259, val loss: 0.10379817336797714\n",
      "Epoch 7010: train loss: 0.017272617667913437, val loss: 0.12992870807647705\n",
      "Epoch 7011: train loss: 0.01500751730054617, val loss: 0.08244576305150986\n",
      "Epoch 7012: train loss: 0.01578199863433838, val loss: 0.07498010247945786\n",
      "Epoch 7013: train loss: 0.020476745441555977, val loss: 0.08041667938232422\n",
      "Epoch 7014: train loss: 0.015437696129083633, val loss: 0.10737166553735733\n",
      "Epoch 7015: train loss: 0.016335858032107353, val loss: 0.06260158121585846\n",
      "Epoch 7016: train loss: 0.019337082281708717, val loss: 0.05133695527911186\n",
      "Epoch 7017: train loss: 0.017602700740098953, val loss: 0.061399322003126144\n",
      "Epoch 7018: train loss: 0.020383769646286964, val loss: 0.07387971132993698\n",
      "Epoch 7019: train loss: 0.01861700229346752, val loss: 0.06575115770101547\n",
      "Epoch 7020: train loss: 0.01628711447119713, val loss: 0.04792419821023941\n",
      "Epoch 7021: train loss: 0.02918635867536068, val loss: 0.04616091027855873\n",
      "Epoch 7022: train loss: 0.01864505186676979, val loss: 0.060064077377319336\n",
      "Epoch 7023: train loss: 0.02279265597462654, val loss: 0.0604236014187336\n",
      "Epoch 7024: train loss: 0.0183930154889822, val loss: 0.05432397127151489\n",
      "Epoch 7025: train loss: 0.02365221455693245, val loss: 0.09779677540063858\n",
      "Epoch 7026: train loss: 0.012621374800801277, val loss: 0.06376363337039948\n",
      "Epoch 7027: train loss: 0.025168128311634064, val loss: 0.037767063826322556\n",
      "Epoch 7028: train loss: 0.017348023131489754, val loss: 0.0636209025979042\n",
      "Epoch 7029: train loss: 0.01583142578601837, val loss: 0.0889551192522049\n",
      "Epoch 7030: train loss: 0.014972067438066006, val loss: 0.04951280355453491\n",
      "Epoch 7031: train loss: 0.02207080088555813, val loss: 0.06683925539255142\n",
      "Epoch 7032: train loss: 0.015175886452198029, val loss: 0.12580808997154236\n",
      "Epoch 7033: train loss: 0.01744026318192482, val loss: 0.0662284716963768\n",
      "Epoch 7034: train loss: 0.014873170293867588, val loss: 0.06385160982608795\n",
      "Epoch 7035: train loss: 0.015615980140864849, val loss: 0.09780507534742355\n",
      "Epoch 7036: train loss: 0.01724517159163952, val loss: 0.05879030376672745\n",
      "Epoch 7037: train loss: 0.01927381008863449, val loss: 0.08732591569423676\n",
      "Epoch 7038: train loss: 0.02107096277177334, val loss: 0.06538669764995575\n",
      "Epoch 7039: train loss: 0.01908465474843979, val loss: 0.08983156085014343\n",
      "Epoch 7040: train loss: 0.017310092225670815, val loss: 0.059576958417892456\n",
      "Epoch 7041: train loss: 0.014041999354958534, val loss: 0.05894448235630989\n",
      "Epoch 7042: train loss: 0.017373964190483093, val loss: 0.05627427250146866\n",
      "Epoch 7043: train loss: 0.017285024747252464, val loss: 0.1005900651216507\n",
      "Epoch 7044: train loss: 0.01882091537117958, val loss: 0.09231100231409073\n",
      "Epoch 7045: train loss: 0.019764818251132965, val loss: 0.06018955633044243\n",
      "Epoch 7046: train loss: 0.01967763341963291, val loss: 0.07317434996366501\n",
      "Epoch 7047: train loss: 0.014155070297420025, val loss: 0.06706051528453827\n",
      "Epoch 7048: train loss: 0.019441893324255943, val loss: 0.11319688707590103\n",
      "Epoch 7049: train loss: 0.016138151288032532, val loss: 0.06440369039773941\n",
      "Epoch 7050: train loss: 0.01656850427389145, val loss: 0.03461566939949989\n",
      "Epoch 7051: train loss: 0.023286951705813408, val loss: 0.09781332314014435\n",
      "Epoch 7052: train loss: 0.015529852360486984, val loss: 0.043810196220874786\n",
      "Epoch 7053: train loss: 0.020215241238474846, val loss: 0.07564698159694672\n",
      "Epoch 7054: train loss: 0.02196955494582653, val loss: 0.05972365662455559\n",
      "Epoch 7055: train loss: 0.016221491619944572, val loss: 0.09452848881483078\n",
      "Epoch 7056: train loss: 0.02424808219075203, val loss: 0.056198712438344955\n",
      "Epoch 7057: train loss: 0.021820267662405968, val loss: 0.07429423928260803\n",
      "Epoch 7058: train loss: 0.01715388149023056, val loss: 0.07806175947189331\n",
      "Epoch 7059: train loss: 0.016138603910803795, val loss: 0.05736642703413963\n",
      "Epoch 7060: train loss: 0.018424874171614647, val loss: 0.08106466382741928\n",
      "Epoch 7061: train loss: 0.021325960755348206, val loss: 0.06185128167271614\n",
      "Epoch 7062: train loss: 0.01637229695916176, val loss: 0.055142756551504135\n",
      "Epoch 7063: train loss: 0.0170639269053936, val loss: 0.062073517590761185\n",
      "Epoch 7064: train loss: 0.01794378086924553, val loss: 0.07241927832365036\n",
      "Epoch 7065: train loss: 0.024267520755529404, val loss: 0.09565430879592896\n",
      "Epoch 7066: train loss: 0.016403567045927048, val loss: 0.08397234231233597\n",
      "Epoch 7067: train loss: 0.020534249022603035, val loss: 0.05781280994415283\n",
      "Epoch 7068: train loss: 0.01329870242625475, val loss: 0.05003178119659424\n",
      "Epoch 7069: train loss: 0.02170514315366745, val loss: 0.04796045646071434\n",
      "Epoch 7070: train loss: 0.01625780016183853, val loss: 0.09788630157709122\n",
      "Epoch 7071: train loss: 0.015556331723928452, val loss: 0.0640793889760971\n",
      "Epoch 7072: train loss: 0.018181148916482925, val loss: 0.07006970047950745\n",
      "Epoch 7073: train loss: 0.01605198346078396, val loss: 0.05921721085906029\n",
      "Epoch 7074: train loss: 0.014904974028468132, val loss: 0.06631719321012497\n",
      "Epoch 7075: train loss: 0.012755201198160648, val loss: 0.050445735454559326\n",
      "Epoch 7076: train loss: 0.017734864726662636, val loss: 0.04061278700828552\n",
      "Epoch 7077: train loss: 0.018770406022667885, val loss: 0.06821965426206589\n",
      "Epoch 7078: train loss: 0.017455965280532837, val loss: 0.08949729055166245\n",
      "Epoch 7079: train loss: 0.017196878790855408, val loss: 0.060181111097335815\n",
      "Epoch 7080: train loss: 0.015760378912091255, val loss: 0.06906165182590485\n",
      "Epoch 7081: train loss: 0.023598959669470787, val loss: 0.07469311356544495\n",
      "Epoch 7082: train loss: 0.01658874750137329, val loss: 0.06469693034887314\n",
      "Epoch 7083: train loss: 0.022111017256975174, val loss: 0.04120315983891487\n",
      "Epoch 7084: train loss: 0.013254604302346706, val loss: 0.11029603332281113\n",
      "Epoch 7085: train loss: 0.013731935992836952, val loss: 0.07039749622344971\n",
      "Epoch 7086: train loss: 0.019026609137654305, val loss: 0.07471416890621185\n",
      "Epoch 7087: train loss: 0.017803015187382698, val loss: 0.06081807613372803\n",
      "Epoch 7088: train loss: 0.016454564407467842, val loss: 0.04834996163845062\n",
      "Epoch 7089: train loss: 0.024386253207921982, val loss: 0.08402132242918015\n",
      "Epoch 7090: train loss: 0.020258404314517975, val loss: 0.04824245721101761\n",
      "Epoch 7091: train loss: 0.0237637497484684, val loss: 0.0842503011226654\n",
      "Epoch 7092: train loss: 0.017997682094573975, val loss: 0.06697484105825424\n",
      "Epoch 7093: train loss: 0.014842349104583263, val loss: 0.11770675331354141\n",
      "Epoch 7094: train loss: 0.014532928355038166, val loss: 0.046510349959135056\n",
      "Epoch 7095: train loss: 0.016085097566246986, val loss: 0.055676113814115524\n",
      "Epoch 7096: train loss: 0.0175301656126976, val loss: 0.0701538473367691\n",
      "Epoch 7097: train loss: 0.017905184999108315, val loss: 0.08245386928319931\n",
      "Epoch 7098: train loss: 0.016582319512963295, val loss: 0.07544896751642227\n",
      "Epoch 7099: train loss: 0.01289300061762333, val loss: 0.08130627125501633\n",
      "Epoch 7100: train loss: 0.014784523285925388, val loss: 0.06314247101545334\n",
      "Epoch 7101: train loss: 0.014771021902561188, val loss: 0.04892582446336746\n",
      "Epoch 7102: train loss: 0.015710966661572456, val loss: 0.05060979351401329\n",
      "Epoch 7103: train loss: 0.02073323354125023, val loss: 0.0685938149690628\n",
      "Epoch 7104: train loss: 0.018117718398571014, val loss: 0.059620607644319534\n",
      "Epoch 7105: train loss: 0.012430541217327118, val loss: 0.06017594411969185\n",
      "Epoch 7106: train loss: 0.01645021326839924, val loss: 0.0554383285343647\n",
      "Epoch 7107: train loss: 0.012765532359480858, val loss: 0.05721745640039444\n",
      "Epoch 7108: train loss: 0.02052714116871357, val loss: 0.10441785305738449\n",
      "Epoch 7109: train loss: 0.01795804128050804, val loss: 0.06720790266990662\n",
      "Epoch 7110: train loss: 0.015669910237193108, val loss: 0.057809796184301376\n",
      "Epoch 7111: train loss: 0.01599929854273796, val loss: 0.05184520035982132\n",
      "Epoch 7112: train loss: 0.02090410888195038, val loss: 0.11321456730365753\n",
      "Epoch 7113: train loss: 0.013672984205186367, val loss: 0.05944858863949776\n",
      "Epoch 7114: train loss: 0.01835351623594761, val loss: 0.04326644167304039\n",
      "Epoch 7115: train loss: 0.016609149053692818, val loss: 0.06222491338849068\n",
      "Epoch 7116: train loss: 0.013575033284723759, val loss: 0.10715857893228531\n",
      "Epoch 7117: train loss: 0.011422308161854744, val loss: 0.0528617799282074\n",
      "Epoch 7118: train loss: 0.016463838517665863, val loss: 0.07206390053033829\n",
      "Epoch 7119: train loss: 0.01312336977571249, val loss: 0.06363695859909058\n",
      "Epoch 7120: train loss: 0.017708992585539818, val loss: 0.07010286301374435\n",
      "Epoch 7121: train loss: 0.011389050632715225, val loss: 0.06105539947748184\n",
      "Epoch 7122: train loss: 0.024017954245209694, val loss: 0.12585459649562836\n",
      "Epoch 7123: train loss: 0.020000096410512924, val loss: 0.07116572558879852\n",
      "Epoch 7124: train loss: 0.012737812474370003, val loss: 0.07752194255590439\n",
      "Epoch 7125: train loss: 0.027742533013224602, val loss: 0.08570463955402374\n",
      "Epoch 7126: train loss: 0.020836425945162773, val loss: 0.052978355437517166\n",
      "Epoch 7127: train loss: 0.012326020747423172, val loss: 0.11811621487140656\n",
      "Epoch 7128: train loss: 0.0235299002379179, val loss: 0.08492252975702286\n",
      "Epoch 7129: train loss: 0.014685209840536118, val loss: 0.02264513075351715\n",
      "Epoch 7130: train loss: 0.014871238730847836, val loss: 0.08484859764575958\n",
      "Epoch 7131: train loss: 0.018928933888673782, val loss: 0.05171699449419975\n",
      "Epoch 7132: train loss: 0.01498335786163807, val loss: 0.050861794501543045\n",
      "Epoch 7133: train loss: 0.020353101193904877, val loss: 0.07030903548002243\n",
      "Epoch 7134: train loss: 0.019155656918883324, val loss: 0.07647790759801865\n",
      "Epoch 7135: train loss: 0.021786289289593697, val loss: 0.06750840693712234\n",
      "Epoch 7136: train loss: 0.017249075695872307, val loss: 0.05768553167581558\n",
      "Epoch 7137: train loss: 0.016341935843229294, val loss: 0.05627460032701492\n",
      "Epoch 7138: train loss: 0.019088052213191986, val loss: 0.0434434674680233\n",
      "Epoch 7139: train loss: 0.01459300983697176, val loss: 0.09120555222034454\n",
      "Epoch 7140: train loss: 0.021748164668679237, val loss: 0.050815679132938385\n",
      "Epoch 7141: train loss: 0.015804698690772057, val loss: 0.04942290857434273\n",
      "Epoch 7142: train loss: 0.01940041221678257, val loss: 0.06240719556808472\n",
      "Epoch 7143: train loss: 0.018783992156386375, val loss: 0.05779634788632393\n",
      "Epoch 7144: train loss: 0.019809802994132042, val loss: 0.062434252351522446\n",
      "Epoch 7145: train loss: 0.014445623382925987, val loss: 0.05703408271074295\n",
      "Epoch 7146: train loss: 0.014690187759697437, val loss: 0.09139946103096008\n",
      "Epoch 7147: train loss: 0.022877903655171394, val loss: 0.04603501409292221\n",
      "Epoch 7148: train loss: 0.01598198153078556, val loss: 0.07495022565126419\n",
      "Epoch 7149: train loss: 0.018472978845238686, val loss: 0.06333000212907791\n",
      "Epoch 7150: train loss: 0.010424574837088585, val loss: 0.059230413287878036\n",
      "Epoch 7151: train loss: 0.015800468623638153, val loss: 0.08241262286901474\n",
      "Epoch 7152: train loss: 0.020994359627366066, val loss: 0.06736321747303009\n",
      "Epoch 7153: train loss: 0.014070462435483932, val loss: 0.08526615798473358\n",
      "Epoch 7154: train loss: 0.017883870750665665, val loss: 0.07289773225784302\n",
      "Epoch 7155: train loss: 0.01825558952987194, val loss: 0.08977218717336655\n",
      "Epoch 7156: train loss: 0.016226056963205338, val loss: 0.0620281882584095\n",
      "Epoch 7157: train loss: 0.01850401610136032, val loss: 0.07381691783666611\n",
      "Epoch 7158: train loss: 0.018364010378718376, val loss: 0.09834232181310654\n",
      "Epoch 7159: train loss: 0.01613934151828289, val loss: 0.08891381323337555\n",
      "Epoch 7160: train loss: 0.015993524342775345, val loss: 0.07545023411512375\n",
      "Epoch 7161: train loss: 0.021595146507024765, val loss: 0.0864957794547081\n",
      "Epoch 7162: train loss: 0.01672055572271347, val loss: 0.11146175116300583\n",
      "Epoch 7163: train loss: 0.031328100711107254, val loss: 0.08419259637594223\n",
      "Epoch 7164: train loss: 0.019542261958122253, val loss: 0.10045812278985977\n",
      "Epoch 7165: train loss: 0.01535232923924923, val loss: 0.08537714928388596\n",
      "Epoch 7166: train loss: 0.01584845595061779, val loss: 0.04675152525305748\n",
      "Epoch 7167: train loss: 0.018181422725319862, val loss: 0.08355984091758728\n",
      "Epoch 7168: train loss: 0.019474348053336143, val loss: 0.08110293000936508\n",
      "Epoch 7169: train loss: 0.023375488817691803, val loss: 0.07579165697097778\n",
      "Epoch 7170: train loss: 0.01806815154850483, val loss: 0.06552783399820328\n",
      "Epoch 7171: train loss: 0.01627938263118267, val loss: 0.07398565858602524\n",
      "Epoch 7172: train loss: 0.022615304216742516, val loss: 0.08469763398170471\n",
      "Epoch 7173: train loss: 0.01358628086745739, val loss: 0.053367432206869125\n",
      "Epoch 7174: train loss: 0.025366224348545074, val loss: 0.08192210644483566\n",
      "Epoch 7175: train loss: 0.016201769933104515, val loss: 0.06872325390577316\n",
      "Epoch 7176: train loss: 0.014315467327833176, val loss: 0.0633232519030571\n",
      "Epoch 7177: train loss: 0.02363542839884758, val loss: 0.08465217798948288\n",
      "Epoch 7178: train loss: 0.012976829893887043, val loss: 0.05918285250663757\n",
      "Epoch 7179: train loss: 0.023929256945848465, val loss: 0.04417863488197327\n",
      "Epoch 7180: train loss: 0.019674524664878845, val loss: 0.06119733676314354\n",
      "Epoch 7181: train loss: 0.017522117123007774, val loss: 0.06792561709880829\n",
      "Epoch 7182: train loss: 0.014611088670790195, val loss: 0.08281264454126358\n",
      "Epoch 7183: train loss: 0.023715512827038765, val loss: 0.09095126390457153\n",
      "Epoch 7184: train loss: 0.01686253771185875, val loss: 0.07426580041646957\n",
      "Epoch 7185: train loss: 0.01928563602268696, val loss: 0.07595938444137573\n",
      "Epoch 7186: train loss: 0.019981075078248978, val loss: 0.07802228629589081\n",
      "Epoch 7187: train loss: 0.01838863268494606, val loss: 0.07386891543865204\n",
      "Epoch 7188: train loss: 0.017847247421741486, val loss: 0.08888755738735199\n",
      "Epoch 7189: train loss: 0.014793905429542065, val loss: 0.07628183811903\n",
      "Epoch 7190: train loss: 0.02050696313381195, val loss: 0.07703407853841782\n",
      "Epoch 7191: train loss: 0.020060276612639427, val loss: 0.10364928096532822\n",
      "Epoch 7192: train loss: 0.017380475997924805, val loss: 0.04981398209929466\n",
      "Epoch 7193: train loss: 0.024640033021569252, val loss: 0.06309959292411804\n",
      "Epoch 7194: train loss: 0.024003364145755768, val loss: 0.05022958666086197\n",
      "Epoch 7195: train loss: 0.015271085314452648, val loss: 0.07963540405035019\n",
      "Epoch 7196: train loss: 0.011976104229688644, val loss: 0.05739438533782959\n",
      "Epoch 7197: train loss: 0.020137175917625427, val loss: 0.0805453285574913\n",
      "Epoch 7198: train loss: 0.015172857791185379, val loss: 0.10348384827375412\n",
      "Epoch 7199: train loss: 0.021541452035307884, val loss: 0.11323274672031403\n",
      "Epoch 7200: train loss: 0.013351351022720337, val loss: 0.038472700864076614\n",
      "Epoch 7201: train loss: 0.02127971313893795, val loss: 0.07039519399404526\n",
      "Epoch 7202: train loss: 0.01374770700931549, val loss: 0.07093163579702377\n",
      "Epoch 7203: train loss: 0.014742637053132057, val loss: 0.06514675915241241\n",
      "Epoch 7204: train loss: 0.013928873464465141, val loss: 0.042806513607501984\n",
      "Epoch 7205: train loss: 0.019501978531479836, val loss: 0.06060683727264404\n",
      "Epoch 7206: train loss: 0.015451882034540176, val loss: 0.07568804174661636\n",
      "Epoch 7207: train loss: 0.012558831833302975, val loss: 0.0442761592566967\n",
      "Epoch 7208: train loss: 0.02276991866528988, val loss: 0.0923427864909172\n",
      "Epoch 7209: train loss: 0.015265446156263351, val loss: 0.15147697925567627\n",
      "Epoch 7210: train loss: 0.019300639629364014, val loss: 0.03335067629814148\n",
      "Epoch 7211: train loss: 0.015638407319784164, val loss: 0.05327046662569046\n",
      "Epoch 7212: train loss: 0.019654085859656334, val loss: 0.07703015953302383\n",
      "Epoch 7213: train loss: 0.012963046319782734, val loss: 0.09702993929386139\n",
      "Epoch 7214: train loss: 0.012189044617116451, val loss: 0.10417816787958145\n",
      "Epoch 7215: train loss: 0.019946923479437828, val loss: 0.038280732929706573\n",
      "Epoch 7216: train loss: 0.014784304425120354, val loss: 0.0711485743522644\n",
      "Epoch 7217: train loss: 0.02490394376218319, val loss: 0.0689234510064125\n",
      "Epoch 7218: train loss: 0.01823296584188938, val loss: 0.04790530353784561\n",
      "Epoch 7219: train loss: 0.0194708239287138, val loss: 0.09409639984369278\n",
      "Epoch 7220: train loss: 0.018722645938396454, val loss: 0.06173232942819595\n",
      "Epoch 7221: train loss: 0.015882177278399467, val loss: 0.07981289178133011\n",
      "Epoch 7222: train loss: 0.01656574383378029, val loss: 0.06890582293272018\n",
      "Epoch 7223: train loss: 0.015791557729244232, val loss: 0.056197572499513626\n",
      "Epoch 7224: train loss: 0.015636060386896133, val loss: 0.05676751211285591\n",
      "Epoch 7225: train loss: 0.014274503104388714, val loss: 0.10932181030511856\n",
      "Epoch 7226: train loss: 0.014016822911798954, val loss: 0.12725520133972168\n",
      "Epoch 7227: train loss: 0.02142472006380558, val loss: 0.05655582621693611\n",
      "Epoch 7228: train loss: 0.016094904392957687, val loss: 0.07104562968015671\n",
      "Epoch 7229: train loss: 0.020518597215414047, val loss: 0.05574795603752136\n",
      "Epoch 7230: train loss: 0.015503101982176304, val loss: 0.03139206022024155\n",
      "Epoch 7231: train loss: 0.017790209501981735, val loss: 0.06628479808568954\n",
      "Epoch 7232: train loss: 0.012870325706899166, val loss: 0.06668543070554733\n",
      "Epoch 7233: train loss: 0.014844516292214394, val loss: 0.0621127150952816\n",
      "Epoch 7234: train loss: 0.025218792259693146, val loss: 0.09029417484998703\n",
      "Epoch 7235: train loss: 0.02222392149269581, val loss: 0.07753598690032959\n",
      "Epoch 7236: train loss: 0.013952630572021008, val loss: 0.03251113370060921\n",
      "Epoch 7237: train loss: 0.017188701778650284, val loss: 0.0893242135643959\n",
      "Epoch 7238: train loss: 0.018637165427207947, val loss: 0.05580923706293106\n",
      "Epoch 7239: train loss: 0.01813150756061077, val loss: 0.061854004859924316\n",
      "Epoch 7240: train loss: 0.015805263072252274, val loss: 0.042305897921323776\n",
      "Epoch 7241: train loss: 0.014891299419105053, val loss: 0.07523428648710251\n",
      "Epoch 7242: train loss: 0.017595119774341583, val loss: 0.05312776565551758\n",
      "Epoch 7243: train loss: 0.011710384860634804, val loss: 0.09852690994739532\n",
      "Epoch 7244: train loss: 0.015936342999339104, val loss: 0.07290863245725632\n",
      "Epoch 7245: train loss: 0.01702154241502285, val loss: 0.0835837721824646\n",
      "Epoch 7246: train loss: 0.015284967608749866, val loss: 0.0545584075152874\n",
      "Epoch 7247: train loss: 0.013600495643913746, val loss: 0.06366672366857529\n",
      "Epoch 7248: train loss: 0.011327415704727173, val loss: 0.03969022259116173\n",
      "Epoch 7249: train loss: 0.017864609137177467, val loss: 0.1016557589173317\n",
      "Epoch 7250: train loss: 0.013177067041397095, val loss: 0.07797360420227051\n",
      "Epoch 7251: train loss: 0.023878661915659904, val loss: 0.04790845885872841\n",
      "Epoch 7252: train loss: 0.013432069681584835, val loss: 0.03937225788831711\n",
      "Epoch 7253: train loss: 0.015786074101924896, val loss: 0.06018416956067085\n",
      "Epoch 7254: train loss: 0.018267057836055756, val loss: 0.0457887127995491\n",
      "Epoch 7255: train loss: 0.02191653847694397, val loss: 0.095072902739048\n",
      "Epoch 7256: train loss: 0.022445764392614365, val loss: 0.04758954048156738\n",
      "Epoch 7257: train loss: 0.01600942760705948, val loss: 0.054861392825841904\n",
      "Epoch 7258: train loss: 0.015469908714294434, val loss: 0.11928174644708633\n",
      "Epoch 7259: train loss: 0.01655440591275692, val loss: 0.11223701387643814\n",
      "Epoch 7260: train loss: 0.0158704686909914, val loss: 0.0806274265050888\n",
      "Epoch 7261: train loss: 0.01441764086484909, val loss: 0.06005798652768135\n",
      "Epoch 7262: train loss: 0.011582406237721443, val loss: 0.0850338563323021\n",
      "Epoch 7263: train loss: 0.010994845069944859, val loss: 0.10204961150884628\n",
      "Epoch 7264: train loss: 0.021502893418073654, val loss: 0.03796577826142311\n",
      "Epoch 7265: train loss: 0.015854783356189728, val loss: 0.06892930716276169\n",
      "Epoch 7266: train loss: 0.023569904267787933, val loss: 0.08384060859680176\n",
      "Epoch 7267: train loss: 0.020201506093144417, val loss: 0.06810710579156876\n",
      "Epoch 7268: train loss: 0.026622377336025238, val loss: 0.06717914342880249\n",
      "Epoch 7269: train loss: 0.020389877259731293, val loss: 0.1164390966296196\n",
      "Epoch 7270: train loss: 0.022480836138129234, val loss: 0.08127082139253616\n",
      "Epoch 7271: train loss: 0.020923426374793053, val loss: 0.06611358374357224\n",
      "Epoch 7272: train loss: 0.016739286482334137, val loss: 0.06799580901861191\n",
      "Epoch 7273: train loss: 0.01762978732585907, val loss: 0.045475419610738754\n",
      "Epoch 7274: train loss: 0.021145915612578392, val loss: 0.08495020121335983\n",
      "Epoch 7275: train loss: 0.01262222696095705, val loss: 0.07761692255735397\n",
      "Epoch 7276: train loss: 0.016832057386636734, val loss: 0.10022647678852081\n",
      "Epoch 7277: train loss: 0.01653287373483181, val loss: 0.047007326036691666\n",
      "Epoch 7278: train loss: 0.016577547416090965, val loss: 0.05719074234366417\n",
      "Epoch 7279: train loss: 0.019885025918483734, val loss: 0.10839670896530151\n",
      "Epoch 7280: train loss: 0.01408133003860712, val loss: 0.10174081474542618\n",
      "Epoch 7281: train loss: 0.019606662914156914, val loss: 0.09162476658821106\n",
      "Epoch 7282: train loss: 0.01934085786342621, val loss: 0.083046555519104\n",
      "Epoch 7283: train loss: 0.015492752194404602, val loss: 0.08578305691480637\n",
      "Epoch 7284: train loss: 0.012583693489432335, val loss: 0.06231250241398811\n",
      "Epoch 7285: train loss: 0.015336712822318077, val loss: 0.06886401027441025\n",
      "Epoch 7286: train loss: 0.017715804278850555, val loss: 0.046020425856113434\n",
      "Epoch 7287: train loss: 0.016226917505264282, val loss: 0.035645730793476105\n",
      "Epoch 7288: train loss: 0.018608488142490387, val loss: 0.08388169854879379\n",
      "Epoch 7289: train loss: 0.014991238713264465, val loss: 0.09107376635074615\n",
      "Epoch 7290: train loss: 0.02187473326921463, val loss: 0.06130721792578697\n",
      "Epoch 7291: train loss: 0.01643133908510208, val loss: 0.07248883694410324\n",
      "Epoch 7292: train loss: 0.021027687937021255, val loss: 0.06918783485889435\n",
      "Epoch 7293: train loss: 0.018207155168056488, val loss: 0.08718646317720413\n",
      "Epoch 7294: train loss: 0.019849970936775208, val loss: 0.07950601726770401\n",
      "Epoch 7295: train loss: 0.018959075212478638, val loss: 0.0715007334947586\n",
      "Epoch 7296: train loss: 0.020203405991196632, val loss: 0.08246954530477524\n",
      "Epoch 7297: train loss: 0.01647789590060711, val loss: 0.056280940771102905\n",
      "Epoch 7298: train loss: 0.014964827336370945, val loss: 0.08178840577602386\n",
      "Epoch 7299: train loss: 0.019962286576628685, val loss: 0.0309632308781147\n",
      "Epoch 7300: train loss: 0.015088193118572235, val loss: 0.13266602158546448\n",
      "Epoch 7301: train loss: 0.018988875672221184, val loss: 0.09414122253656387\n",
      "Epoch 7302: train loss: 0.024395307525992393, val loss: 0.06808707863092422\n",
      "Epoch 7303: train loss: 0.020982634276151657, val loss: 0.08319730311632156\n",
      "Epoch 7304: train loss: 0.021362606436014175, val loss: 0.08326591551303864\n",
      "Epoch 7305: train loss: 0.017925957217812538, val loss: 0.08394252508878708\n",
      "Epoch 7306: train loss: 0.017632195726037025, val loss: 0.03423164039850235\n",
      "Epoch 7307: train loss: 0.01346114743500948, val loss: 0.11106681078672409\n",
      "Epoch 7308: train loss: 0.018165403977036476, val loss: 0.0741189643740654\n",
      "Epoch 7309: train loss: 0.018174009397625923, val loss: 0.048136916011571884\n",
      "Epoch 7310: train loss: 0.03149313107132912, val loss: 0.05598921328783035\n",
      "Epoch 7311: train loss: 0.013694369234144688, val loss: 0.0707009807229042\n",
      "Epoch 7312: train loss: 0.020361298695206642, val loss: 0.0883200541138649\n",
      "Epoch 7313: train loss: 0.01924561709165573, val loss: 0.05968053266406059\n",
      "Epoch 7314: train loss: 0.018144456669688225, val loss: 0.05122755840420723\n",
      "Epoch 7315: train loss: 0.020527925342321396, val loss: 0.08724770694971085\n",
      "Epoch 7316: train loss: 0.0158856101334095, val loss: 0.0803150162100792\n",
      "Epoch 7317: train loss: 0.017936091870069504, val loss: 0.061396028846502304\n",
      "Epoch 7318: train loss: 0.021171648055315018, val loss: 0.10092371702194214\n",
      "Epoch 7319: train loss: 0.020758679136633873, val loss: 0.040185265243053436\n",
      "Epoch 7320: train loss: 0.01946701854467392, val loss: 0.08038171380758286\n",
      "Epoch 7321: train loss: 0.017580609768629074, val loss: 0.07825183868408203\n",
      "Epoch 7322: train loss: 0.020719129592180252, val loss: 0.06476982682943344\n",
      "Epoch 7323: train loss: 0.016147488728165627, val loss: 0.0418672040104866\n",
      "Epoch 7324: train loss: 0.01613985188305378, val loss: 0.062357474118471146\n",
      "Epoch 7325: train loss: 0.013805702328681946, val loss: 0.0568145215511322\n",
      "Epoch 7326: train loss: 0.019069712609052658, val loss: 0.07437678426504135\n",
      "Epoch 7327: train loss: 0.013543018139898777, val loss: 0.03704673796892166\n",
      "Epoch 7328: train loss: 0.01358980592340231, val loss: 0.0939711183309555\n",
      "Epoch 7329: train loss: 0.016336513683199883, val loss: 0.04557190090417862\n",
      "Epoch 7330: train loss: 0.019180884584784508, val loss: 0.11547129601240158\n",
      "Epoch 7331: train loss: 0.014338335022330284, val loss: 0.056705743074417114\n",
      "Epoch 7332: train loss: 0.013673549517989159, val loss: 0.05999200418591499\n",
      "Epoch 7333: train loss: 0.01630578003823757, val loss: 0.04451406002044678\n",
      "Epoch 7334: train loss: 0.018058275803923607, val loss: 0.0951484814286232\n",
      "Epoch 7335: train loss: 0.016940973699092865, val loss: 0.07313326746225357\n",
      "Epoch 7336: train loss: 0.021146239712834358, val loss: 0.08937598764896393\n",
      "Epoch 7337: train loss: 0.017315534874796867, val loss: 0.06580068171024323\n",
      "Epoch 7338: train loss: 0.014243937097489834, val loss: 0.06084644794464111\n",
      "Epoch 7339: train loss: 0.019916102290153503, val loss: 0.11457961052656174\n",
      "Epoch 7340: train loss: 0.018140383064746857, val loss: 0.08170073479413986\n",
      "Epoch 7341: train loss: 0.022742871195077896, val loss: 0.08757413178682327\n",
      "Epoch 7342: train loss: 0.01605670154094696, val loss: 0.07524975389242172\n",
      "Epoch 7343: train loss: 0.013148748315870762, val loss: 0.051314473152160645\n",
      "Epoch 7344: train loss: 0.020384151488542557, val loss: 0.05015242099761963\n",
      "Epoch 7345: train loss: 0.014963041990995407, val loss: 0.05140010267496109\n",
      "Epoch 7346: train loss: 0.01560121402144432, val loss: 0.03565424308180809\n",
      "Epoch 7347: train loss: 0.019380535930395126, val loss: 0.0899314358830452\n",
      "Epoch 7348: train loss: 0.015047788619995117, val loss: 0.06858044117689133\n",
      "Epoch 7349: train loss: 0.014457790181040764, val loss: 0.13001306354999542\n",
      "Epoch 7350: train loss: 0.020934250205755234, val loss: 0.026631755754351616\n",
      "Epoch 7351: train loss: 0.018976496532559395, val loss: 0.06782776117324829\n",
      "Epoch 7352: train loss: 0.020532840862870216, val loss: 0.02955612912774086\n",
      "Epoch 7353: train loss: 0.016837740316987038, val loss: 0.05728745460510254\n",
      "Epoch 7354: train loss: 0.014903049916028976, val loss: 0.06470797210931778\n",
      "Epoch 7355: train loss: 0.01723211631178856, val loss: 0.041648976504802704\n",
      "Epoch 7356: train loss: 0.018428020179271698, val loss: 0.05030534788966179\n",
      "Epoch 7357: train loss: 0.01579168625175953, val loss: 0.07322283834218979\n",
      "Epoch 7358: train loss: 0.0196873489767313, val loss: 0.05092707276344299\n",
      "Epoch 7359: train loss: 0.011937388218939304, val loss: 0.058648861944675446\n",
      "Epoch 7360: train loss: 0.015573224052786827, val loss: 0.10125499218702316\n",
      "Epoch 7361: train loss: 0.0140509819611907, val loss: 0.04948790743947029\n",
      "Epoch 7362: train loss: 0.011990920640528202, val loss: 0.06396112591028214\n",
      "Epoch 7363: train loss: 0.017183853313326836, val loss: 0.07316837459802628\n",
      "Epoch 7364: train loss: 0.017332594841718674, val loss: 0.07334047555923462\n",
      "Epoch 7365: train loss: 0.021122785285115242, val loss: 0.07232826948165894\n",
      "Epoch 7366: train loss: 0.01897077076137066, val loss: 0.08996132761240005\n",
      "Epoch 7367: train loss: 0.018622955307364464, val loss: 0.0844956561923027\n",
      "Epoch 7368: train loss: 0.019907932728528976, val loss: 0.04774898290634155\n",
      "Epoch 7369: train loss: 0.015258977189660072, val loss: 0.08502741158008575\n",
      "Epoch 7370: train loss: 0.021581575274467468, val loss: 0.07665561884641647\n",
      "Epoch 7371: train loss: 0.018111974000930786, val loss: 0.10230656713247299\n",
      "Epoch 7372: train loss: 0.016104551032185555, val loss: 0.10037548840045929\n",
      "Epoch 7373: train loss: 0.02231215499341488, val loss: 0.057539816945791245\n",
      "Epoch 7374: train loss: 0.017429951578378677, val loss: 0.07625705003738403\n",
      "Epoch 7375: train loss: 0.020287636667490005, val loss: 0.05933239683508873\n",
      "Epoch 7376: train loss: 0.01458210963755846, val loss: 0.06354091316461563\n",
      "Epoch 7377: train loss: 0.01680922321975231, val loss: 0.11366039514541626\n",
      "Epoch 7378: train loss: 0.015156814828515053, val loss: 0.07166007161140442\n",
      "Epoch 7379: train loss: 0.01609988324344158, val loss: 0.046444714069366455\n",
      "Epoch 7380: train loss: 0.02413053624331951, val loss: 0.059593211859464645\n",
      "Epoch 7381: train loss: 0.015589145012199879, val loss: 0.07943826168775558\n",
      "Epoch 7382: train loss: 0.021841440349817276, val loss: 0.058558281511068344\n",
      "Epoch 7383: train loss: 0.02172933518886566, val loss: 0.07303392142057419\n",
      "Epoch 7384: train loss: 0.016087746247649193, val loss: 0.05275978520512581\n",
      "Epoch 7385: train loss: 0.023131530731916428, val loss: 0.05313315987586975\n",
      "Epoch 7386: train loss: 0.017385924234986305, val loss: 0.09041287750005722\n",
      "Epoch 7387: train loss: 0.016133004799485207, val loss: 0.05778178200125694\n",
      "Epoch 7388: train loss: 0.018194379284977913, val loss: 0.10125412046909332\n",
      "Epoch 7389: train loss: 0.02757308818399906, val loss: 0.05326850339770317\n",
      "Epoch 7390: train loss: 0.020654330030083656, val loss: 0.07144670933485031\n",
      "Epoch 7391: train loss: 0.017825601622462273, val loss: 0.07663240283727646\n",
      "Epoch 7392: train loss: 0.018068257719278336, val loss: 0.0471939817070961\n",
      "Epoch 7393: train loss: 0.01592949964106083, val loss: 0.053425382822752\n",
      "Epoch 7394: train loss: 0.015711897984147072, val loss: 0.030816426500678062\n",
      "Epoch 7395: train loss: 0.016094068065285683, val loss: 0.057638298720121384\n",
      "Epoch 7396: train loss: 0.015615889802575111, val loss: 0.058264583349227905\n",
      "Epoch 7397: train loss: 0.017355432733893394, val loss: 0.08136709034442902\n",
      "Epoch 7398: train loss: 0.014229614287614822, val loss: 0.08299993723630905\n",
      "Epoch 7399: train loss: 0.019128000363707542, val loss: 0.10224789381027222\n",
      "Epoch 7400: train loss: 0.019458714872598648, val loss: 0.07672841101884842\n",
      "Epoch 7401: train loss: 0.017162442207336426, val loss: 0.058115411549806595\n",
      "Epoch 7402: train loss: 0.019246401265263557, val loss: 0.06855525076389313\n",
      "Epoch 7403: train loss: 0.02525818906724453, val loss: 0.07931061834096909\n",
      "Epoch 7404: train loss: 0.02326572872698307, val loss: 0.10074756294488907\n",
      "Epoch 7405: train loss: 0.015418783761560917, val loss: 0.09282958507537842\n",
      "Epoch 7406: train loss: 0.021604780107736588, val loss: 0.07344967126846313\n",
      "Epoch 7407: train loss: 0.017753131687641144, val loss: 0.11635291576385498\n",
      "Epoch 7408: train loss: 0.02015128917992115, val loss: 0.06651618331670761\n",
      "Epoch 7409: train loss: 0.012529379688203335, val loss: 0.09457875788211823\n",
      "Epoch 7410: train loss: 0.013268833979964256, val loss: 0.07071691751480103\n",
      "Epoch 7411: train loss: 0.015507452189922333, val loss: 0.09096655994653702\n",
      "Epoch 7412: train loss: 0.016913097351789474, val loss: 0.0785176157951355\n",
      "Epoch 7413: train loss: 0.015648847445845604, val loss: 0.06048866733908653\n",
      "Epoch 7414: train loss: 0.012828879989683628, val loss: 0.06098004803061485\n",
      "Epoch 7415: train loss: 0.017258988693356514, val loss: 0.08379256725311279\n",
      "Epoch 7416: train loss: 0.014481396414339542, val loss: 0.05532866716384888\n",
      "Epoch 7417: train loss: 0.01817750744521618, val loss: 0.08079048991203308\n",
      "Epoch 7418: train loss: 0.017607273533940315, val loss: 0.09032919257879257\n",
      "Epoch 7419: train loss: 0.023504532873630524, val loss: 0.10260827839374542\n",
      "Epoch 7420: train loss: 0.01607053168118, val loss: 0.0737508162856102\n",
      "Epoch 7421: train loss: 0.018761634826660156, val loss: 0.06789455562829971\n",
      "Epoch 7422: train loss: 0.018392236903309822, val loss: 0.08458966016769409\n",
      "Epoch 7423: train loss: 0.018610207363963127, val loss: 0.05513835698366165\n",
      "Epoch 7424: train loss: 0.0191488116979599, val loss: 0.04868839681148529\n",
      "Epoch 7425: train loss: 0.0182521790266037, val loss: 0.055957090109586716\n",
      "Epoch 7426: train loss: 0.014567597769200802, val loss: 0.10447551310062408\n",
      "Epoch 7427: train loss: 0.017610494047403336, val loss: 0.09202531725168228\n",
      "Epoch 7428: train loss: 0.02120891772210598, val loss: 0.05225379392504692\n",
      "Epoch 7429: train loss: 0.019557392224669456, val loss: 0.05120226740837097\n",
      "Epoch 7430: train loss: 0.014066614210605621, val loss: 0.09738500416278839\n",
      "Epoch 7431: train loss: 0.014924932271242142, val loss: 0.08630272001028061\n",
      "Epoch 7432: train loss: 0.026568705216050148, val loss: 0.08088855445384979\n",
      "Epoch 7433: train loss: 0.02085057459771633, val loss: 0.05111388489603996\n",
      "Epoch 7434: train loss: 0.013084692880511284, val loss: 0.06947080045938492\n",
      "Epoch 7435: train loss: 0.014531604945659637, val loss: 0.08192073553800583\n",
      "Epoch 7436: train loss: 0.020343264564871788, val loss: 0.04824058711528778\n",
      "Epoch 7437: train loss: 0.02149197645485401, val loss: 0.07756665349006653\n",
      "Epoch 7438: train loss: 0.01749243587255478, val loss: 0.07356130331754684\n",
      "Epoch 7439: train loss: 0.014424700289964676, val loss: 0.06673576682806015\n",
      "Epoch 7440: train loss: 0.02052275463938713, val loss: 0.1107698455452919\n",
      "Epoch 7441: train loss: 0.014768960885703564, val loss: 0.06675583124160767\n",
      "Epoch 7442: train loss: 0.020139478147029877, val loss: 0.11789431422948837\n",
      "Epoch 7443: train loss: 0.022646445780992508, val loss: 0.053575750440359116\n",
      "Epoch 7444: train loss: 0.017636660486459732, val loss: 0.1072252169251442\n",
      "Epoch 7445: train loss: 0.0161314457654953, val loss: 0.03351631015539169\n",
      "Epoch 7446: train loss: 0.016407225281000137, val loss: 0.09111005812883377\n",
      "Epoch 7447: train loss: 0.017827099189162254, val loss: 0.08144044876098633\n",
      "Epoch 7448: train loss: 0.011618105694651604, val loss: 0.0960320457816124\n",
      "Epoch 7449: train loss: 0.015296319499611855, val loss: 0.10977502167224884\n",
      "Epoch 7450: train loss: 0.015494185499846935, val loss: 0.06982260197401047\n",
      "Epoch 7451: train loss: 0.02356940507888794, val loss: 0.06492540240287781\n",
      "Epoch 7452: train loss: 0.016562586650252342, val loss: 0.05848217010498047\n",
      "Epoch 7453: train loss: 0.017980940639972687, val loss: 0.041376836597919464\n",
      "Epoch 7454: train loss: 0.02253444865345955, val loss: 0.0718136578798294\n",
      "Epoch 7455: train loss: 0.013734759762883186, val loss: 0.07993676513433456\n",
      "Epoch 7456: train loss: 0.019738351926207542, val loss: 0.09420717507600784\n",
      "Epoch 7457: train loss: 0.010199697688221931, val loss: 0.047732945531606674\n",
      "Epoch 7458: train loss: 0.01528923399746418, val loss: 0.0722896158695221\n",
      "Epoch 7459: train loss: 0.0105790626257658, val loss: 0.05978339537978172\n",
      "Epoch 7460: train loss: 0.014897866174578667, val loss: 0.10308468341827393\n",
      "Epoch 7461: train loss: 0.014552472159266472, val loss: 0.046975791454315186\n",
      "Epoch 7462: train loss: 0.0146775608882308, val loss: 0.06843005865812302\n",
      "Epoch 7463: train loss: 0.011457690969109535, val loss: 0.06273862719535828\n",
      "Epoch 7464: train loss: 0.013975966721773148, val loss: 0.08303117007017136\n",
      "Epoch 7465: train loss: 0.019699349999427795, val loss: 0.07366345077753067\n",
      "Epoch 7466: train loss: 0.01736736111342907, val loss: 0.05984126403927803\n",
      "Epoch 7467: train loss: 0.014982528984546661, val loss: 0.10588441044092178\n",
      "Epoch 7468: train loss: 0.017854614183306694, val loss: 0.04386555403470993\n",
      "Epoch 7469: train loss: 0.029514456167817116, val loss: 0.040483154356479645\n",
      "Epoch 7470: train loss: 0.023219933733344078, val loss: 0.07673944532871246\n",
      "Epoch 7471: train loss: 0.016685301437973976, val loss: 0.08705601841211319\n",
      "Epoch 7472: train loss: 0.015588520094752312, val loss: 0.07053593546152115\n",
      "Epoch 7473: train loss: 0.021491413936018944, val loss: 0.053347405046224594\n",
      "Epoch 7474: train loss: 0.01432037353515625, val loss: 0.051677901297807693\n",
      "Epoch 7475: train loss: 0.03249921277165413, val loss: 0.07936690002679825\n",
      "Epoch 7476: train loss: 0.026967473328113556, val loss: 0.07175945490598679\n",
      "Epoch 7477: train loss: 0.01707880198955536, val loss: 0.05926121398806572\n",
      "Epoch 7478: train loss: 0.015111489221453667, val loss: 0.05324885994195938\n",
      "Epoch 7479: train loss: 0.015880730003118515, val loss: 0.07617149502038956\n",
      "Epoch 7480: train loss: 0.012543398886919022, val loss: 0.08869310468435287\n",
      "Epoch 7481: train loss: 0.015552878379821777, val loss: 0.06085919961333275\n",
      "Epoch 7482: train loss: 0.01383492723107338, val loss: 0.05103934556245804\n",
      "Epoch 7483: train loss: 0.024412842467427254, val loss: 0.06140825152397156\n",
      "Epoch 7484: train loss: 0.012093763798475266, val loss: 0.0714065209031105\n",
      "Epoch 7485: train loss: 0.01894821971654892, val loss: 0.08002463728189468\n",
      "Epoch 7486: train loss: 0.020627137273550034, val loss: 0.0732363611459732\n",
      "Epoch 7487: train loss: 0.01893119513988495, val loss: 0.03986019641160965\n",
      "Epoch 7488: train loss: 0.018338285386562347, val loss: 0.08673720806837082\n",
      "Epoch 7489: train loss: 0.017059464007616043, val loss: 0.06824006140232086\n",
      "Epoch 7490: train loss: 0.023842573165893555, val loss: 0.05670067295432091\n",
      "Epoch 7491: train loss: 0.026885000988841057, val loss: 0.07857995480298996\n",
      "Epoch 7492: train loss: 0.017229607328772545, val loss: 0.050276290625333786\n",
      "Epoch 7493: train loss: 0.022934241220355034, val loss: 0.07596602290868759\n",
      "Epoch 7494: train loss: 0.013036925345659256, val loss: 0.06444334983825684\n",
      "Epoch 7495: train loss: 0.013361077755689621, val loss: 0.06894558668136597\n",
      "Epoch 7496: train loss: 0.02285359427332878, val loss: 0.09331544488668442\n",
      "Epoch 7497: train loss: 0.016461437568068504, val loss: 0.057290077209472656\n",
      "Epoch 7498: train loss: 0.023007160052657127, val loss: 0.07555489242076874\n",
      "Epoch 7499: train loss: 0.014660137705504894, val loss: 0.06688375025987625\n",
      "Epoch 7500: train loss: 0.016887562349438667, val loss: 0.05279223248362541\n",
      "Epoch 7501: train loss: 0.020194442942738533, val loss: 0.05214741826057434\n",
      "Epoch 7502: train loss: 0.012212717905640602, val loss: 0.053162284195423126\n",
      "Epoch 7503: train loss: 0.017785772681236267, val loss: 0.07260171324014664\n",
      "Epoch 7504: train loss: 0.018414698541164398, val loss: 0.05673886463046074\n",
      "Epoch 7505: train loss: 0.020219380035996437, val loss: 0.06949596852064133\n",
      "Epoch 7506: train loss: 0.01432081125676632, val loss: 0.05467788502573967\n",
      "Epoch 7507: train loss: 0.014022535644471645, val loss: 0.0736963078379631\n",
      "Epoch 7508: train loss: 0.019533174112439156, val loss: 0.08097078651189804\n",
      "Epoch 7509: train loss: 0.01585831493139267, val loss: 0.10656442493200302\n",
      "Epoch 7510: train loss: 0.017514178529381752, val loss: 0.06340641528367996\n",
      "Epoch 7511: train loss: 0.02946244366466999, val loss: 0.07900767773389816\n",
      "Epoch 7512: train loss: 0.03242211043834686, val loss: 0.08680025488138199\n",
      "Epoch 7513: train loss: 0.020075304433703423, val loss: 0.10226764529943466\n",
      "Epoch 7514: train loss: 0.019976213574409485, val loss: 0.03861240670084953\n",
      "Epoch 7515: train loss: 0.023254601284861565, val loss: 0.09632706642150879\n",
      "Epoch 7516: train loss: 0.018419988453388214, val loss: 0.08631026744842529\n",
      "Epoch 7517: train loss: 0.015004467219114304, val loss: 0.06287248432636261\n",
      "Epoch 7518: train loss: 0.016240626573562622, val loss: 0.07867210358381271\n",
      "Epoch 7519: train loss: 0.017006177455186844, val loss: 0.04875990375876427\n",
      "Epoch 7520: train loss: 0.016211435198783875, val loss: 0.06981243193149567\n",
      "Epoch 7521: train loss: 0.016975965350866318, val loss: 0.058924492448568344\n",
      "Epoch 7522: train loss: 0.020488156005740166, val loss: 0.0627264454960823\n",
      "Epoch 7523: train loss: 0.022641947492957115, val loss: 0.0579778328537941\n",
      "Epoch 7524: train loss: 0.01763826794922352, val loss: 0.06619181483983994\n",
      "Epoch 7525: train loss: 0.013250632211565971, val loss: 0.07918714731931686\n",
      "Epoch 7526: train loss: 0.021207552403211594, val loss: 0.05098738893866539\n",
      "Epoch 7527: train loss: 0.018249593675136566, val loss: 0.06027504429221153\n",
      "Epoch 7528: train loss: 0.02195299044251442, val loss: 0.07987967878580093\n",
      "Epoch 7529: train loss: 0.017345355823636055, val loss: 0.06809162348508835\n",
      "Epoch 7530: train loss: 0.0199327003210783, val loss: 0.0961286649107933\n",
      "Epoch 7531: train loss: 0.014497929252684116, val loss: 0.04428096115589142\n",
      "Epoch 7532: train loss: 0.022615093737840652, val loss: 0.06397827714681625\n",
      "Epoch 7533: train loss: 0.014820939861238003, val loss: 0.07434989511966705\n",
      "Epoch 7534: train loss: 0.022419288754463196, val loss: 0.07346396893262863\n",
      "Epoch 7535: train loss: 0.019393526017665863, val loss: 0.06682294607162476\n",
      "Epoch 7536: train loss: 0.01860291138291359, val loss: 0.08173181116580963\n",
      "Epoch 7537: train loss: 0.01928841508924961, val loss: 0.09817677736282349\n",
      "Epoch 7538: train loss: 0.01915029250085354, val loss: 0.06636466085910797\n",
      "Epoch 7539: train loss: 0.016092024743556976, val loss: 0.06131929159164429\n",
      "Epoch 7540: train loss: 0.01325185876339674, val loss: 0.07102087885141373\n",
      "Epoch 7541: train loss: 0.02029186487197876, val loss: 0.14539365470409393\n",
      "Epoch 7542: train loss: 0.015766551718115807, val loss: 0.11573994159698486\n",
      "Epoch 7543: train loss: 0.015823474153876305, val loss: 0.08844564855098724\n",
      "Epoch 7544: train loss: 0.015449927188456059, val loss: 0.05647484213113785\n",
      "Epoch 7545: train loss: 0.021614698693156242, val loss: 0.1011754646897316\n",
      "Epoch 7546: train loss: 0.02266664057970047, val loss: 0.10098572075366974\n",
      "Epoch 7547: train loss: 0.017915427684783936, val loss: 0.05638265609741211\n",
      "Epoch 7548: train loss: 0.017548315227031708, val loss: 0.05853980407118797\n",
      "Epoch 7549: train loss: 0.01799335703253746, val loss: 0.0530952513217926\n",
      "Epoch 7550: train loss: 0.01716516725718975, val loss: 0.07761933654546738\n",
      "Epoch 7551: train loss: 0.015925031155347824, val loss: 0.06670200079679489\n",
      "Epoch 7552: train loss: 0.016602620482444763, val loss: 0.05548888444900513\n",
      "Epoch 7553: train loss: 0.018921319395303726, val loss: 0.05960213765501976\n",
      "Epoch 7554: train loss: 0.01759120635688305, val loss: 0.07962556928396225\n",
      "Epoch 7555: train loss: 0.015473988838493824, val loss: 0.07409252226352692\n",
      "Epoch 7556: train loss: 0.015743354335427284, val loss: 0.07526649534702301\n",
      "Epoch 7557: train loss: 0.016143351793289185, val loss: 0.04991008713841438\n",
      "Epoch 7558: train loss: 0.02376641146838665, val loss: 0.07810944318771362\n",
      "Epoch 7559: train loss: 0.014484774321317673, val loss: 0.05348653346300125\n",
      "Epoch 7560: train loss: 0.01951540820300579, val loss: 0.07842065393924713\n",
      "Epoch 7561: train loss: 0.013769534416496754, val loss: 0.09098561853170395\n",
      "Epoch 7562: train loss: 0.016860999166965485, val loss: 0.02142566628754139\n",
      "Epoch 7563: train loss: 0.015978870913386345, val loss: 0.03903883323073387\n",
      "Epoch 7564: train loss: 0.021948903799057007, val loss: 0.04371948167681694\n",
      "Epoch 7565: train loss: 0.0177132710814476, val loss: 0.08335458487272263\n",
      "Epoch 7566: train loss: 0.020231090486049652, val loss: 0.12035498768091202\n",
      "Epoch 7567: train loss: 0.016950109973549843, val loss: 0.06272762268781662\n",
      "Epoch 7568: train loss: 0.016657019034028053, val loss: 0.07529782503843307\n",
      "Epoch 7569: train loss: 0.016692595556378365, val loss: 0.09910181909799576\n",
      "Epoch 7570: train loss: 0.018027491867542267, val loss: 0.12118060886859894\n",
      "Epoch 7571: train loss: 0.015516171231865883, val loss: 0.0730879083275795\n",
      "Epoch 7572: train loss: 0.016457702964544296, val loss: 0.05649368092417717\n",
      "Epoch 7573: train loss: 0.01887836679816246, val loss: 0.05939365550875664\n",
      "Epoch 7574: train loss: 0.02623194083571434, val loss: 0.06060323119163513\n",
      "Epoch 7575: train loss: 0.01909811608493328, val loss: 0.09237122535705566\n",
      "Epoch 7576: train loss: 0.01760052889585495, val loss: 0.07385572791099548\n",
      "Epoch 7577: train loss: 0.014337131753563881, val loss: 0.04399743303656578\n",
      "Epoch 7578: train loss: 0.017041953280568123, val loss: 0.05874922499060631\n",
      "Epoch 7579: train loss: 0.021247120574116707, val loss: 0.03040301240980625\n",
      "Epoch 7580: train loss: 0.015082117170095444, val loss: 0.06653343886137009\n",
      "Epoch 7581: train loss: 0.020556719973683357, val loss: 0.06044347956776619\n",
      "Epoch 7582: train loss: 0.0246335007250309, val loss: 0.06990716606378555\n",
      "Epoch 7583: train loss: 0.016793208196759224, val loss: 0.08299309760332108\n",
      "Epoch 7584: train loss: 0.011863388121128082, val loss: 0.07078253477811813\n",
      "Epoch 7585: train loss: 0.0198824405670166, val loss: 0.06312692910432816\n",
      "Epoch 7586: train loss: 0.01633450947701931, val loss: 0.0563795231282711\n",
      "Epoch 7587: train loss: 0.020382609218358994, val loss: 0.05574609711766243\n",
      "Epoch 7588: train loss: 0.014931835234165192, val loss: 0.05774335190653801\n",
      "Epoch 7589: train loss: 0.024537796154618263, val loss: 0.03961272910237312\n",
      "Epoch 7590: train loss: 0.019211819395422935, val loss: 0.1330583542585373\n",
      "Epoch 7591: train loss: 0.014473885297775269, val loss: 0.08716017007827759\n",
      "Epoch 7592: train loss: 0.019336121156811714, val loss: 0.088795967400074\n",
      "Epoch 7593: train loss: 0.01849212870001793, val loss: 0.07623570412397385\n",
      "Epoch 7594: train loss: 0.01956145465373993, val loss: 0.05405990406870842\n",
      "Epoch 7595: train loss: 0.015457283705472946, val loss: 0.0491667315363884\n",
      "Epoch 7596: train loss: 0.01934846118092537, val loss: 0.07565668970346451\n",
      "Epoch 7597: train loss: 0.02224728651344776, val loss: 0.05492781475186348\n",
      "Epoch 7598: train loss: 0.02151733823120594, val loss: 0.05338090658187866\n",
      "Epoch 7599: train loss: 0.017746882513165474, val loss: 0.06695139408111572\n",
      "Epoch 7600: train loss: 0.013203498907387257, val loss: 0.0936742052435875\n",
      "Epoch 7601: train loss: 0.022721244022250175, val loss: 0.0906296968460083\n",
      "Epoch 7602: train loss: 0.013123583979904652, val loss: 0.07338353991508484\n",
      "Epoch 7603: train loss: 0.015662357211112976, val loss: 0.053854215890169144\n",
      "Epoch 7604: train loss: 0.01844281330704689, val loss: 0.05220765620470047\n",
      "Epoch 7605: train loss: 0.01946466788649559, val loss: 0.10502060502767563\n",
      "Epoch 7606: train loss: 0.01696242205798626, val loss: 0.04237613081932068\n",
      "Epoch 7607: train loss: 0.018000295385718346, val loss: 0.07183457911014557\n",
      "Epoch 7608: train loss: 0.017011605203151703, val loss: 0.06526850908994675\n",
      "Epoch 7609: train loss: 0.021464794874191284, val loss: 0.06341379880905151\n",
      "Epoch 7610: train loss: 0.018863702192902565, val loss: 0.06437265127897263\n",
      "Epoch 7611: train loss: 0.02911267802119255, val loss: 0.04265442118048668\n",
      "Epoch 7612: train loss: 0.017072224989533424, val loss: 0.05949478968977928\n",
      "Epoch 7613: train loss: 0.021760063245892525, val loss: 0.06120467931032181\n",
      "Epoch 7614: train loss: 0.013503101654350758, val loss: 0.04015982896089554\n",
      "Epoch 7615: train loss: 0.013403007760643959, val loss: 0.09275650233030319\n",
      "Epoch 7616: train loss: 0.013287337496876717, val loss: 0.11192210018634796\n",
      "Epoch 7617: train loss: 0.015231100842356682, val loss: 0.07761146128177643\n",
      "Epoch 7618: train loss: 0.01927877403795719, val loss: 0.08096498996019363\n",
      "Epoch 7619: train loss: 0.01709405705332756, val loss: 0.07806327193975449\n",
      "Epoch 7620: train loss: 0.011342895217239857, val loss: 0.0973493680357933\n",
      "Epoch 7621: train loss: 0.016364142298698425, val loss: 0.05193963646888733\n",
      "Epoch 7622: train loss: 0.015443357639014721, val loss: 0.0935850441455841\n",
      "Epoch 7623: train loss: 0.015241528861224651, val loss: 0.06291475892066956\n",
      "Epoch 7624: train loss: 0.014968443661928177, val loss: 0.0456828735768795\n",
      "Epoch 7625: train loss: 0.013866840861737728, val loss: 0.05013878270983696\n",
      "Epoch 7626: train loss: 0.018709590658545494, val loss: 0.07319335639476776\n",
      "Epoch 7627: train loss: 0.015516648069024086, val loss: 0.098366878926754\n",
      "Epoch 7628: train loss: 0.01534963771700859, val loss: 0.09003088623285294\n",
      "Epoch 7629: train loss: 0.02167024090886116, val loss: 0.10607884079217911\n",
      "Epoch 7630: train loss: 0.01743313856422901, val loss: 0.06232299283146858\n",
      "Epoch 7631: train loss: 0.01782807894051075, val loss: 0.059714991599321365\n",
      "Epoch 7632: train loss: 0.014768616296350956, val loss: 0.05555334314703941\n",
      "Epoch 7633: train loss: 0.014772423543035984, val loss: 0.06971987336874008\n",
      "Epoch 7634: train loss: 0.01653657853603363, val loss: 0.07471803575754166\n",
      "Epoch 7635: train loss: 0.01847369410097599, val loss: 0.05394719913601875\n",
      "Epoch 7636: train loss: 0.011802902445197105, val loss: 0.07313839346170425\n",
      "Epoch 7637: train loss: 0.013300183229148388, val loss: 0.07351745665073395\n",
      "Epoch 7638: train loss: 0.016877641901373863, val loss: 0.04837004095315933\n",
      "Epoch 7639: train loss: 0.01601843349635601, val loss: 0.07403235882520676\n",
      "Epoch 7640: train loss: 0.011203535832464695, val loss: 0.09032413363456726\n",
      "Epoch 7641: train loss: 0.01932455226778984, val loss: 0.07880308479070663\n",
      "Epoch 7642: train loss: 0.018217789009213448, val loss: 0.0796978548169136\n",
      "Epoch 7643: train loss: 0.016313211992383003, val loss: 0.040273409336805344\n",
      "Epoch 7644: train loss: 0.014797894284129143, val loss: 0.09199892729520798\n",
      "Epoch 7645: train loss: 0.021251585334539413, val loss: 0.11074650287628174\n",
      "Epoch 7646: train loss: 0.01594175398349762, val loss: 0.05725189670920372\n",
      "Epoch 7647: train loss: 0.024442287161946297, val loss: 0.049277469515800476\n",
      "Epoch 7648: train loss: 0.022603843361139297, val loss: 0.0688837543129921\n",
      "Epoch 7649: train loss: 0.0185781829059124, val loss: 0.06506528705358505\n",
      "Epoch 7650: train loss: 0.010773052461445332, val loss: 0.05483657866716385\n",
      "Epoch 7651: train loss: 0.020564867183566093, val loss: 0.06330136209726334\n",
      "Epoch 7652: train loss: 0.020268166437745094, val loss: 0.09295965731143951\n",
      "Epoch 7653: train loss: 0.02826809510588646, val loss: 0.1104590892791748\n",
      "Epoch 7654: train loss: 0.02106551080942154, val loss: 0.06274698674678802\n",
      "Epoch 7655: train loss: 0.019042743369936943, val loss: 0.08773728460073471\n",
      "Epoch 7656: train loss: 0.018496686592698097, val loss: 0.0823516845703125\n",
      "Epoch 7657: train loss: 0.024736765772104263, val loss: 0.06714118272066116\n",
      "Epoch 7658: train loss: 0.014274222776293755, val loss: 0.0748618021607399\n",
      "Epoch 7659: train loss: 0.015367554500699043, val loss: 0.11060690134763718\n",
      "Epoch 7660: train loss: 0.01797076314687729, val loss: 0.07945029437541962\n",
      "Epoch 7661: train loss: 0.012369365431368351, val loss: 0.06684885919094086\n",
      "Epoch 7662: train loss: 0.018561169505119324, val loss: 0.08989785611629486\n",
      "Epoch 7663: train loss: 0.021472491323947906, val loss: 0.08241518586874008\n",
      "Epoch 7664: train loss: 0.01878819428384304, val loss: 0.07855391502380371\n",
      "Epoch 7665: train loss: 0.01651894487440586, val loss: 0.06571052223443985\n",
      "Epoch 7666: train loss: 0.021146226674318314, val loss: 0.07974866032600403\n",
      "Epoch 7667: train loss: 0.01945122890174389, val loss: 0.054607536643743515\n",
      "Epoch 7668: train loss: 0.01749534159898758, val loss: 0.0894765630364418\n",
      "Epoch 7669: train loss: 0.020535718649625778, val loss: 0.06880371272563934\n",
      "Epoch 7670: train loss: 0.019334763288497925, val loss: 0.08128704875707626\n",
      "Epoch 7671: train loss: 0.011129209771752357, val loss: 0.0736919492483139\n",
      "Epoch 7672: train loss: 0.015891503542661667, val loss: 0.05242648348212242\n",
      "Epoch 7673: train loss: 0.018674207851290703, val loss: 0.07097786664962769\n",
      "Epoch 7674: train loss: 0.015850767493247986, val loss: 0.058354686945676804\n",
      "Epoch 7675: train loss: 0.013523848727345467, val loss: 0.07306899130344391\n",
      "Epoch 7676: train loss: 0.02499346248805523, val loss: 0.07855316996574402\n",
      "Epoch 7677: train loss: 0.015141824260354042, val loss: 0.06640604883432388\n",
      "Epoch 7678: train loss: 0.019139720126986504, val loss: 0.09972485154867172\n",
      "Epoch 7679: train loss: 0.018394136801362038, val loss: 0.07564038038253784\n",
      "Epoch 7680: train loss: 0.011887314729392529, val loss: 0.08604396134614944\n",
      "Epoch 7681: train loss: 0.020875638350844383, val loss: 0.045633167028427124\n",
      "Epoch 7682: train loss: 0.0129685178399086, val loss: 0.05992020294070244\n",
      "Epoch 7683: train loss: 0.014181524515151978, val loss: 0.03429505601525307\n",
      "Epoch 7684: train loss: 0.019554339349269867, val loss: 0.095575250685215\n",
      "Epoch 7685: train loss: 0.021156394854187965, val loss: 0.05924847349524498\n",
      "Epoch 7686: train loss: 0.019262349233031273, val loss: 0.08598462492227554\n",
      "Epoch 7687: train loss: 0.01473360788077116, val loss: 0.06448721885681152\n",
      "Epoch 7688: train loss: 0.01752755604684353, val loss: 0.09597144275903702\n",
      "Epoch 7689: train loss: 0.017351383343338966, val loss: 0.0626680999994278\n",
      "Epoch 7690: train loss: 0.013950887136161327, val loss: 0.04287971183657646\n",
      "Epoch 7691: train loss: 0.026982354000210762, val loss: 0.028376897796988487\n",
      "Epoch 7692: train loss: 0.018763341009616852, val loss: 0.10608407109975815\n",
      "Epoch 7693: train loss: 0.01925395429134369, val loss: 0.07106800377368927\n",
      "Epoch 7694: train loss: 0.014330490492284298, val loss: 0.08184987306594849\n",
      "Epoch 7695: train loss: 0.026912566274404526, val loss: 0.07023202627897263\n",
      "Epoch 7696: train loss: 0.016959352418780327, val loss: 0.07416920363903046\n",
      "Epoch 7697: train loss: 0.020517729222774506, val loss: 0.05010930821299553\n",
      "Epoch 7698: train loss: 0.017236188054084778, val loss: 0.09473295509815216\n",
      "Epoch 7699: train loss: 0.015767741948366165, val loss: 0.09910493344068527\n",
      "Epoch 7700: train loss: 0.017007581889629364, val loss: 0.09857335686683655\n",
      "Epoch 7701: train loss: 0.020271923393011093, val loss: 0.08175446838140488\n",
      "Epoch 7702: train loss: 0.017187511548399925, val loss: 0.09823252260684967\n",
      "Epoch 7703: train loss: 0.013518601655960083, val loss: 0.09010731428861618\n",
      "Epoch 7704: train loss: 0.01615774631500244, val loss: 0.08671721071004868\n",
      "Epoch 7705: train loss: 0.016985474154353142, val loss: 0.0927005186676979\n",
      "Epoch 7706: train loss: 0.017468923702836037, val loss: 0.11691873520612717\n",
      "Epoch 7707: train loss: 0.025554800406098366, val loss: 0.07515045255422592\n",
      "Epoch 7708: train loss: 0.018678687512874603, val loss: 0.08182785660028458\n",
      "Epoch 7709: train loss: 0.017134230583906174, val loss: 0.0640813410282135\n",
      "Epoch 7710: train loss: 0.018467871472239494, val loss: 0.09901610761880875\n",
      "Epoch 7711: train loss: 0.013426677323877811, val loss: 0.07190098613500595\n",
      "Epoch 7712: train loss: 0.01436674129217863, val loss: 0.056939758360385895\n",
      "Epoch 7713: train loss: 0.024296345189213753, val loss: 0.0543438084423542\n",
      "Epoch 7714: train loss: 0.018472354859113693, val loss: 0.0619795024394989\n",
      "Epoch 7715: train loss: 0.025714801624417305, val loss: 0.058084167540073395\n",
      "Epoch 7716: train loss: 0.012922353111207485, val loss: 0.08730372041463852\n",
      "Epoch 7717: train loss: 0.02217169478535652, val loss: 0.06814348697662354\n",
      "Epoch 7718: train loss: 0.017008710652589798, val loss: 0.07926996797323227\n",
      "Epoch 7719: train loss: 0.020224634557962418, val loss: 0.1262463629245758\n",
      "Epoch 7720: train loss: 0.02273988164961338, val loss: 0.046595118939876556\n",
      "Epoch 7721: train loss: 0.01586294174194336, val loss: 0.07854648679494858\n",
      "Epoch 7722: train loss: 0.018153753131628036, val loss: 0.08179635554552078\n",
      "Epoch 7723: train loss: 0.020633060485124588, val loss: 0.11857610940933228\n",
      "Epoch 7724: train loss: 0.017861850559711456, val loss: 0.11251386255025864\n",
      "Epoch 7725: train loss: 0.01897408813238144, val loss: 0.058922018855810165\n",
      "Epoch 7726: train loss: 0.02348235435783863, val loss: 0.0612962506711483\n",
      "Epoch 7727: train loss: 0.023530401289463043, val loss: 0.07827261835336685\n",
      "Epoch 7728: train loss: 0.015473832376301289, val loss: 0.06482880562543869\n",
      "Epoch 7729: train loss: 0.015259462408721447, val loss: 0.08897902071475983\n",
      "Epoch 7730: train loss: 0.01696365885436535, val loss: 0.0942758321762085\n",
      "Epoch 7731: train loss: 0.01671462319791317, val loss: 0.07481835782527924\n",
      "Epoch 7732: train loss: 0.018526792526245117, val loss: 0.05864457041025162\n",
      "Epoch 7733: train loss: 0.016960512846708298, val loss: 0.08588413149118423\n",
      "Epoch 7734: train loss: 0.01964493840932846, val loss: 0.06533163785934448\n",
      "Epoch 7735: train loss: 0.01511954516172409, val loss: 0.07550422102212906\n",
      "Epoch 7736: train loss: 0.01870386302471161, val loss: 0.035751406103372574\n",
      "Epoch 7737: train loss: 0.019321627914905548, val loss: 0.08156799525022507\n",
      "Epoch 7738: train loss: 0.020423727110028267, val loss: 0.07910481095314026\n",
      "Epoch 7739: train loss: 0.0155020821839571, val loss: 0.057943910360336304\n",
      "Epoch 7740: train loss: 0.01425323635339737, val loss: 0.08051686733961105\n",
      "Epoch 7741: train loss: 0.016336312517523766, val loss: 0.05954165384173393\n",
      "Epoch 7742: train loss: 0.0171984750777483, val loss: 0.036563318222761154\n",
      "Epoch 7743: train loss: 0.01849398948252201, val loss: 0.08189497143030167\n",
      "Epoch 7744: train loss: 0.01799706369638443, val loss: 0.07125818729400635\n",
      "Epoch 7745: train loss: 0.02118057943880558, val loss: 0.0322108268737793\n",
      "Epoch 7746: train loss: 0.013229611329734325, val loss: 0.04849214479327202\n",
      "Epoch 7747: train loss: 0.014623153954744339, val loss: 0.06097511574625969\n",
      "Epoch 7748: train loss: 0.016172008588910103, val loss: 0.06170142441987991\n",
      "Epoch 7749: train loss: 0.02073822170495987, val loss: 0.07314054667949677\n",
      "Epoch 7750: train loss: 0.018956467509269714, val loss: 0.05247677117586136\n",
      "Epoch 7751: train loss: 0.02431054227054119, val loss: 0.04773040488362312\n",
      "Epoch 7752: train loss: 0.014014964923262596, val loss: 0.07238129526376724\n",
      "Epoch 7753: train loss: 0.020827969536185265, val loss: 0.053478728979825974\n",
      "Epoch 7754: train loss: 0.019899597391486168, val loss: 0.08282013982534409\n",
      "Epoch 7755: train loss: 0.02005789242684841, val loss: 0.017710229381918907\n",
      "Epoch 7756: train loss: 0.01791943423449993, val loss: 0.06492158025503159\n",
      "Epoch 7757: train loss: 0.020530911162495613, val loss: 0.0818755254149437\n",
      "Epoch 7758: train loss: 0.018184619024395943, val loss: 0.0690375342965126\n",
      "Epoch 7759: train loss: 0.02002568729221821, val loss: 0.05516767501831055\n",
      "Epoch 7760: train loss: 0.014817983843386173, val loss: 0.07585425674915314\n",
      "Epoch 7761: train loss: 0.019868094474077225, val loss: 0.11164157837629318\n",
      "Epoch 7762: train loss: 0.018241869285702705, val loss: 0.06582530587911606\n",
      "Epoch 7763: train loss: 0.02000308968126774, val loss: 0.061214644461870193\n",
      "Epoch 7764: train loss: 0.019059009850025177, val loss: 0.09399445354938507\n",
      "Epoch 7765: train loss: 0.015153501182794571, val loss: 0.08460063487291336\n",
      "Epoch 7766: train loss: 0.018997473642230034, val loss: 0.09114056080579758\n",
      "Epoch 7767: train loss: 0.017462454736232758, val loss: 0.05339081212878227\n",
      "Epoch 7768: train loss: 0.019215207546949387, val loss: 0.07959815114736557\n",
      "Epoch 7769: train loss: 0.021158745512366295, val loss: 0.04249159246683121\n",
      "Epoch 7770: train loss: 0.016209082677960396, val loss: 0.0716913715004921\n",
      "Epoch 7771: train loss: 0.013078107498586178, val loss: 0.08883917331695557\n",
      "Epoch 7772: train loss: 0.01405270118266344, val loss: 0.07645393908023834\n",
      "Epoch 7773: train loss: 0.016029929742217064, val loss: 0.05667947605252266\n",
      "Epoch 7774: train loss: 0.0137515002861619, val loss: 0.06957941502332687\n",
      "Epoch 7775: train loss: 0.01639523170888424, val loss: 0.07150902599096298\n",
      "Epoch 7776: train loss: 0.01676628179848194, val loss: 0.07218452543020248\n",
      "Epoch 7777: train loss: 0.014802551828324795, val loss: 0.11448609083890915\n",
      "Epoch 7778: train loss: 0.022060999646782875, val loss: 0.052784211933612823\n",
      "Epoch 7779: train loss: 0.016271531581878662, val loss: 0.05980018526315689\n",
      "Epoch 7780: train loss: 0.01693156734108925, val loss: 0.10505711287260056\n",
      "Epoch 7781: train loss: 0.01698971353471279, val loss: 0.051598817110061646\n",
      "Epoch 7782: train loss: 0.013936246745288372, val loss: 0.06002548336982727\n",
      "Epoch 7783: train loss: 0.014686604961752892, val loss: 0.06343302130699158\n",
      "Epoch 7784: train loss: 0.016636384651064873, val loss: 0.06904218345880508\n",
      "Epoch 7785: train loss: 0.01918000355362892, val loss: 0.06252221018075943\n",
      "Epoch 7786: train loss: 0.013857891783118248, val loss: 0.07255574315786362\n",
      "Epoch 7787: train loss: 0.017735326662659645, val loss: 0.08779419213533401\n",
      "Epoch 7788: train loss: 0.013600938953459263, val loss: 0.06325974315404892\n",
      "Epoch 7789: train loss: 0.00981307215988636, val loss: 0.06982462853193283\n",
      "Epoch 7790: train loss: 0.02693462371826172, val loss: 0.05395117029547691\n",
      "Epoch 7791: train loss: 0.01940849795937538, val loss: 0.10577511787414551\n",
      "Epoch 7792: train loss: 0.015006090514361858, val loss: 0.09211615473031998\n",
      "Epoch 7793: train loss: 0.01779112033545971, val loss: 0.09245511144399643\n",
      "Epoch 7794: train loss: 0.02141401544213295, val loss: 0.06758817285299301\n",
      "Epoch 7795: train loss: 0.014484046027064323, val loss: 0.09029883146286011\n",
      "Epoch 7796: train loss: 0.01711379550397396, val loss: 0.04804998263716698\n",
      "Epoch 7797: train loss: 0.019402271136641502, val loss: 0.09623339027166367\n",
      "Epoch 7798: train loss: 0.021246520802378654, val loss: 0.11136557906866074\n",
      "Epoch 7799: train loss: 0.021995553746819496, val loss: 0.07733575254678726\n",
      "Epoch 7800: train loss: 0.015173123218119144, val loss: 0.059337545186281204\n",
      "Epoch 7801: train loss: 0.016149254515767097, val loss: 0.052357207983732224\n",
      "Epoch 7802: train loss: 0.019416656345129013, val loss: 0.07253672182559967\n",
      "Epoch 7803: train loss: 0.021564863622188568, val loss: 0.05257723852992058\n",
      "Epoch 7804: train loss: 0.01692609116435051, val loss: 0.0555824413895607\n",
      "Epoch 7805: train loss: 0.019003696739673615, val loss: 0.1383756697177887\n",
      "Epoch 7806: train loss: 0.02328711934387684, val loss: 0.06632868945598602\n",
      "Epoch 7807: train loss: 0.01469323318451643, val loss: 0.09249380975961685\n",
      "Epoch 7808: train loss: 0.01624004915356636, val loss: 0.0634130984544754\n",
      "Epoch 7809: train loss: 0.017802158370614052, val loss: 0.07932108640670776\n",
      "Epoch 7810: train loss: 0.017235636711120605, val loss: 0.09320037811994553\n",
      "Epoch 7811: train loss: 0.016570918262004852, val loss: 0.09642771631479263\n",
      "Epoch 7812: train loss: 0.019475610926747322, val loss: 0.09275263547897339\n",
      "Epoch 7813: train loss: 0.018130742013454437, val loss: 0.11086898297071457\n",
      "Epoch 7814: train loss: 0.017877278849482536, val loss: 0.09776430577039719\n",
      "Epoch 7815: train loss: 0.017894895747303963, val loss: 0.09511952102184296\n",
      "Epoch 7816: train loss: 0.0188737865537405, val loss: 0.10935844480991364\n",
      "Epoch 7817: train loss: 0.02207317389547825, val loss: 0.052312612533569336\n",
      "Epoch 7818: train loss: 0.021489618346095085, val loss: 0.05456060916185379\n",
      "Epoch 7819: train loss: 0.016813158988952637, val loss: 0.08364000171422958\n",
      "Epoch 7820: train loss: 0.024188615381717682, val loss: 0.05188831314444542\n",
      "Epoch 7821: train loss: 0.02107010968029499, val loss: 0.08057331293821335\n",
      "Epoch 7822: train loss: 0.017478933557868004, val loss: 0.06903364509344101\n",
      "Epoch 7823: train loss: 0.022713767364621162, val loss: 0.08576589077711105\n",
      "Epoch 7824: train loss: 0.014071989804506302, val loss: 0.08161750435829163\n",
      "Epoch 7825: train loss: 0.016165563836693764, val loss: 0.08724625408649445\n",
      "Epoch 7826: train loss: 0.015714731067419052, val loss: 0.079952172935009\n",
      "Epoch 7827: train loss: 0.01839117705821991, val loss: 0.08418496698141098\n",
      "Epoch 7828: train loss: 0.019123278558254242, val loss: 0.07977008819580078\n",
      "Epoch 7829: train loss: 0.022023113444447517, val loss: 0.09039004892110825\n",
      "Epoch 7830: train loss: 0.014351194724440575, val loss: 0.05114401504397392\n",
      "Epoch 7831: train loss: 0.026089811697602272, val loss: 0.0813295841217041\n",
      "Epoch 7832: train loss: 0.016383741050958633, val loss: 0.08679916709661484\n",
      "Epoch 7833: train loss: 0.017481589689850807, val loss: 0.07788854092359543\n",
      "Epoch 7834: train loss: 0.0256000105291605, val loss: 0.09427934139966965\n",
      "Epoch 7835: train loss: 0.019597789272665977, val loss: 0.10129591077566147\n",
      "Epoch 7836: train loss: 0.018000921234488487, val loss: 0.06060395389795303\n",
      "Epoch 7837: train loss: 0.018438613042235374, val loss: 0.05069885775446892\n",
      "Epoch 7838: train loss: 0.022852150723338127, val loss: 0.06425521522760391\n",
      "Epoch 7839: train loss: 0.01802246645092964, val loss: 0.09336607903242111\n",
      "Epoch 7840: train loss: 0.018074918538331985, val loss: 0.08984825760126114\n",
      "Epoch 7841: train loss: 0.020768113434314728, val loss: 0.07526271790266037\n",
      "Epoch 7842: train loss: 0.016195794567465782, val loss: 0.07328668981790543\n",
      "Epoch 7843: train loss: 0.014883510768413544, val loss: 0.10709483921527863\n",
      "Epoch 7844: train loss: 0.02111419290304184, val loss: 0.0852055698633194\n",
      "Epoch 7845: train loss: 0.014566072262823582, val loss: 0.09752310812473297\n",
      "Epoch 7846: train loss: 0.01542642991989851, val loss: 0.07341613620519638\n",
      "Epoch 7847: train loss: 0.01554360706359148, val loss: 0.12067123502492905\n",
      "Epoch 7848: train loss: 0.020741913467645645, val loss: 0.06915046274662018\n",
      "Epoch 7849: train loss: 0.01720469631254673, val loss: 0.0629769042134285\n",
      "Epoch 7850: train loss: 0.01913199946284294, val loss: 0.06245933845639229\n",
      "Epoch 7851: train loss: 0.020409423857927322, val loss: 0.05292540788650513\n",
      "Epoch 7852: train loss: 0.01588071882724762, val loss: 0.07331899553537369\n",
      "Epoch 7853: train loss: 0.021375684067606926, val loss: 0.07417172193527222\n",
      "Epoch 7854: train loss: 0.022484196349978447, val loss: 0.05824264511466026\n",
      "Epoch 7855: train loss: 0.01664569601416588, val loss: 0.07067728042602539\n",
      "Epoch 7856: train loss: 0.03059125505387783, val loss: 0.07751766592264175\n",
      "Epoch 7857: train loss: 0.01612241566181183, val loss: 0.11242357641458511\n",
      "Epoch 7858: train loss: 0.01564534194767475, val loss: 0.10818948596715927\n",
      "Epoch 7859: train loss: 0.016923798248171806, val loss: 0.08370961248874664\n",
      "Epoch 7860: train loss: 0.021701013669371605, val loss: 0.07793497294187546\n",
      "Epoch 7861: train loss: 0.026328856125473976, val loss: 0.07292582839727402\n",
      "Epoch 7862: train loss: 0.017878500744700432, val loss: 0.12432612478733063\n",
      "Epoch 7863: train loss: 0.01612868160009384, val loss: 0.09066789597272873\n",
      "Epoch 7864: train loss: 0.022336896508932114, val loss: 0.06305750459432602\n",
      "Epoch 7865: train loss: 0.012371081858873367, val loss: 0.08320052176713943\n",
      "Epoch 7866: train loss: 0.01784292422235012, val loss: 0.08597973734140396\n",
      "Epoch 7867: train loss: 0.0157337486743927, val loss: 0.07055167853832245\n",
      "Epoch 7868: train loss: 0.024942446500062943, val loss: 0.08414515852928162\n",
      "Epoch 7869: train loss: 0.015023870393633842, val loss: 0.10222883522510529\n",
      "Epoch 7870: train loss: 0.016561590135097504, val loss: 0.037760671228170395\n",
      "Epoch 7871: train loss: 0.01985756866633892, val loss: 0.07401599735021591\n",
      "Epoch 7872: train loss: 0.01716463267803192, val loss: 0.12162437289953232\n",
      "Epoch 7873: train loss: 0.015479350462555885, val loss: 0.09499263763427734\n",
      "Epoch 7874: train loss: 0.01800905354321003, val loss: 0.08581917732954025\n",
      "Epoch 7875: train loss: 0.013831131160259247, val loss: 0.07657434046268463\n",
      "Epoch 7876: train loss: 0.017685526981949806, val loss: 0.09828347712755203\n",
      "Epoch 7877: train loss: 0.017970064654946327, val loss: 0.09346795827150345\n",
      "Epoch 7878: train loss: 0.01672620326280594, val loss: 0.08863747119903564\n",
      "Epoch 7879: train loss: 0.01742399111390114, val loss: 0.06662368029356003\n",
      "Epoch 7880: train loss: 0.013730358332395554, val loss: 0.10560603439807892\n",
      "Epoch 7881: train loss: 0.022999992594122887, val loss: 0.06140735372900963\n",
      "Epoch 7882: train loss: 0.013930694200098515, val loss: 0.09562237560749054\n",
      "Epoch 7883: train loss: 0.013987994752824306, val loss: 0.08807668089866638\n",
      "Epoch 7884: train loss: 0.02379433810710907, val loss: 0.08019457757472992\n",
      "Epoch 7885: train loss: 0.020096544176340103, val loss: 0.07906387001276016\n",
      "Epoch 7886: train loss: 0.025314681231975555, val loss: 0.09447409957647324\n",
      "Epoch 7887: train loss: 0.014580504037439823, val loss: 0.07175520807504654\n",
      "Epoch 7888: train loss: 0.022461194545030594, val loss: 0.07794439792633057\n",
      "Epoch 7889: train loss: 0.016067568212747574, val loss: 0.07750888913869858\n",
      "Epoch 7890: train loss: 0.01692529022693634, val loss: 0.08008655160665512\n",
      "Epoch 7891: train loss: 0.016049547120928764, val loss: 0.06741634756326675\n",
      "Epoch 7892: train loss: 0.017482880502939224, val loss: 0.08423458784818649\n",
      "Epoch 7893: train loss: 0.022262753918766975, val loss: 0.0712152048945427\n",
      "Epoch 7894: train loss: 0.017056908458471298, val loss: 0.07851485162973404\n",
      "Epoch 7895: train loss: 0.013749836944043636, val loss: 0.07976378500461578\n",
      "Epoch 7896: train loss: 0.02485525980591774, val loss: 0.04981307312846184\n",
      "Epoch 7897: train loss: 0.013171207159757614, val loss: 0.07128290086984634\n",
      "Epoch 7898: train loss: 0.019786302000284195, val loss: 0.10138751566410065\n",
      "Epoch 7899: train loss: 0.013189107179641724, val loss: 0.06764908134937286\n",
      "Epoch 7900: train loss: 0.018294036388397217, val loss: 0.08197300881147385\n",
      "Epoch 7901: train loss: 0.014169936999678612, val loss: 0.05715623125433922\n",
      "Epoch 7902: train loss: 0.015663977712392807, val loss: 0.08867984265089035\n",
      "Epoch 7903: train loss: 0.01509439293295145, val loss: 0.053415145725011826\n",
      "Epoch 7904: train loss: 0.019491903483867645, val loss: 0.04808778688311577\n",
      "Epoch 7905: train loss: 0.019827499985694885, val loss: 0.05527161434292793\n",
      "Epoch 7906: train loss: 0.015160380862653255, val loss: 0.09669238328933716\n",
      "Epoch 7907: train loss: 0.018330764025449753, val loss: 0.0660543218255043\n",
      "Epoch 7908: train loss: 0.01651478186249733, val loss: 0.050760384649038315\n",
      "Epoch 7909: train loss: 0.018274106085300446, val loss: 0.08056467026472092\n",
      "Epoch 7910: train loss: 0.022516688331961632, val loss: 0.08379525691270828\n",
      "Epoch 7911: train loss: 0.01702452078461647, val loss: 0.09887079149484634\n",
      "Epoch 7912: train loss: 0.01528959535062313, val loss: 0.07694107294082642\n",
      "Epoch 7913: train loss: 0.016672277823090553, val loss: 0.09043390303850174\n",
      "Epoch 7914: train loss: 0.019377326592803, val loss: 0.07610565423965454\n",
      "Epoch 7915: train loss: 0.019253114238381386, val loss: 0.06791903078556061\n",
      "Epoch 7916: train loss: 0.020480358973145485, val loss: 0.06717224419116974\n",
      "Epoch 7917: train loss: 0.010661480948328972, val loss: 0.04117171838879585\n",
      "Epoch 7918: train loss: 0.016245462000370026, val loss: 0.1002117395401001\n",
      "Epoch 7919: train loss: 0.02028709277510643, val loss: 0.06916791945695877\n",
      "Epoch 7920: train loss: 0.017457595095038414, val loss: 0.06942109763622284\n",
      "Epoch 7921: train loss: 0.0167543962597847, val loss: 0.046306025236845016\n",
      "Epoch 7922: train loss: 0.01678255759179592, val loss: 0.06306258589029312\n",
      "Epoch 7923: train loss: 0.017183398827910423, val loss: 0.12989817559719086\n",
      "Epoch 7924: train loss: 0.018981534987688065, val loss: 0.08231991529464722\n",
      "Epoch 7925: train loss: 0.01321405079215765, val loss: 0.05177069827914238\n",
      "Epoch 7926: train loss: 0.01399347372353077, val loss: 0.052143462002277374\n",
      "Epoch 7927: train loss: 0.011325306259095669, val loss: 0.07440490275621414\n",
      "Epoch 7928: train loss: 0.014850812964141369, val loss: 0.06877744197845459\n",
      "Epoch 7929: train loss: 0.016656704246997833, val loss: 0.08665359765291214\n",
      "Epoch 7930: train loss: 0.01851418800652027, val loss: 0.06695689260959625\n",
      "Epoch 7931: train loss: 0.014260338619351387, val loss: 0.07319188863039017\n",
      "Epoch 7932: train loss: 0.017493143677711487, val loss: 0.0774105042219162\n",
      "Epoch 7933: train loss: 0.017527423799037933, val loss: 0.07178418338298798\n",
      "Epoch 7934: train loss: 0.019963327795267105, val loss: 0.048355039209127426\n",
      "Epoch 7935: train loss: 0.020102035254240036, val loss: 0.08197261393070221\n",
      "Epoch 7936: train loss: 0.010752083733677864, val loss: 0.07220609486103058\n",
      "Epoch 7937: train loss: 0.014452396892011166, val loss: 0.06497885286808014\n",
      "Epoch 7938: train loss: 0.01915346272289753, val loss: 0.049734532833099365\n",
      "Epoch 7939: train loss: 0.018473803997039795, val loss: 0.08857335150241852\n",
      "Epoch 7940: train loss: 0.01567528024315834, val loss: 0.0789981260895729\n",
      "Epoch 7941: train loss: 0.010842268355190754, val loss: 0.04388664290308952\n",
      "Epoch 7942: train loss: 0.016381757333874702, val loss: 0.07824552059173584\n",
      "Epoch 7943: train loss: 0.025932567194104195, val loss: 0.05624957010149956\n",
      "Epoch 7944: train loss: 0.017815710976719856, val loss: 0.0621783621609211\n",
      "Epoch 7945: train loss: 0.018366215750575066, val loss: 0.08942902833223343\n",
      "Epoch 7946: train loss: 0.015828708186745644, val loss: 0.06235307455062866\n",
      "Epoch 7947: train loss: 0.016092555597424507, val loss: 0.04626896604895592\n",
      "Epoch 7948: train loss: 0.016628222540020943, val loss: 0.05523490533232689\n",
      "Epoch 7949: train loss: 0.015100504271686077, val loss: 0.07563171535730362\n",
      "Epoch 7950: train loss: 0.015424948185682297, val loss: 0.08044265955686569\n",
      "Epoch 7951: train loss: 0.015515417791903019, val loss: 0.08307033777236938\n",
      "Epoch 7952: train loss: 0.012256505899131298, val loss: 0.04831346496939659\n",
      "Epoch 7953: train loss: 0.014206412248313427, val loss: 0.0786321684718132\n",
      "Epoch 7954: train loss: 0.016782348975539207, val loss: 0.06732143461704254\n",
      "Epoch 7955: train loss: 0.02197360061109066, val loss: 0.05043845996260643\n",
      "Epoch 7956: train loss: 0.013586708344519138, val loss: 0.07639004290103912\n",
      "Epoch 7957: train loss: 0.018798505887389183, val loss: 0.07059381157159805\n",
      "Epoch 7958: train loss: 0.018041929230093956, val loss: 0.06614553183317184\n",
      "Epoch 7959: train loss: 0.01744106225669384, val loss: 0.07631485164165497\n",
      "Epoch 7960: train loss: 0.01789652556180954, val loss: 0.033549126237630844\n",
      "Epoch 7961: train loss: 0.016471929848194122, val loss: 0.04595664516091347\n",
      "Epoch 7962: train loss: 0.010777652263641357, val loss: 0.07821326702833176\n",
      "Epoch 7963: train loss: 0.013276438228785992, val loss: 0.11257531493902206\n",
      "Epoch 7964: train loss: 0.019981512799859047, val loss: 0.08843015879392624\n",
      "Epoch 7965: train loss: 0.016266822814941406, val loss: 0.09215444326400757\n",
      "Epoch 7966: train loss: 0.0185836348682642, val loss: 0.07543619722127914\n",
      "Epoch 7967: train loss: 0.014164739288389683, val loss: 0.06955372542142868\n",
      "Epoch 7968: train loss: 0.017994100227952003, val loss: 0.08991368114948273\n",
      "Epoch 7969: train loss: 0.013760484755039215, val loss: 0.07305299490690231\n",
      "Epoch 7970: train loss: 0.0151825575158, val loss: 0.0813080444931984\n",
      "Epoch 7971: train loss: 0.019417209550738335, val loss: 0.08233743160963058\n",
      "Epoch 7972: train loss: 0.015519351698458195, val loss: 0.03948696330189705\n",
      "Epoch 7973: train loss: 0.013844002038240433, val loss: 0.07703358680009842\n",
      "Epoch 7974: train loss: 0.01751362532377243, val loss: 0.10838260501623154\n",
      "Epoch 7975: train loss: 0.01802743226289749, val loss: 0.06404632329940796\n",
      "Epoch 7976: train loss: 0.013528723269701004, val loss: 0.07421814650297165\n",
      "Epoch 7977: train loss: 0.017372513189911842, val loss: 0.039379917085170746\n",
      "Epoch 7978: train loss: 0.019548015668988228, val loss: 0.09826387465000153\n",
      "Epoch 7979: train loss: 0.017068661749362946, val loss: 0.08926484733819962\n",
      "Epoch 7980: train loss: 0.02117186225950718, val loss: 0.07815859466791153\n",
      "Epoch 7981: train loss: 0.018078789114952087, val loss: 0.09035369008779526\n",
      "Epoch 7982: train loss: 0.01831226982176304, val loss: 0.08809243887662888\n",
      "Epoch 7983: train loss: 0.014527122490108013, val loss: 0.07508078962564468\n",
      "Epoch 7984: train loss: 0.013764907605946064, val loss: 0.05169019103050232\n",
      "Epoch 7985: train loss: 0.01562614180147648, val loss: 0.0921778604388237\n",
      "Epoch 7986: train loss: 0.01289924792945385, val loss: 0.087407685816288\n",
      "Epoch 7987: train loss: 0.023307302966713905, val loss: 0.0911564901471138\n",
      "Epoch 7988: train loss: 0.014891200698912144, val loss: 0.06438957899808884\n",
      "Epoch 7989: train loss: 0.016410144045948982, val loss: 0.0842379480600357\n",
      "Epoch 7990: train loss: 0.016262546181678772, val loss: 0.07841438055038452\n",
      "Epoch 7991: train loss: 0.016059286892414093, val loss: 0.10008752346038818\n",
      "Epoch 7992: train loss: 0.0223686620593071, val loss: 0.04939894378185272\n",
      "Epoch 7993: train loss: 0.019494926556944847, val loss: 0.0944402739405632\n",
      "Epoch 7994: train loss: 0.0174042209982872, val loss: 0.0455012284219265\n",
      "Epoch 7995: train loss: 0.014818244613707066, val loss: 0.07247240096330643\n",
      "Epoch 7996: train loss: 0.018707364797592163, val loss: 0.08028575032949448\n",
      "Epoch 7997: train loss: 0.01802038960158825, val loss: 0.04563179239630699\n",
      "Epoch 7998: train loss: 0.02054857276380062, val loss: 0.06015815958380699\n",
      "Epoch 7999: train loss: 0.016738001257181168, val loss: 0.09080103784799576\n",
      "Epoch 8000: train loss: 0.017256980761885643, val loss: 0.039825696498155594\n",
      "Epoch 8001: train loss: 0.020442664623260498, val loss: 0.12295794486999512\n",
      "Epoch 8002: train loss: 0.01993827149271965, val loss: 0.07906246185302734\n",
      "Epoch 8003: train loss: 0.01650811918079853, val loss: 0.0786222442984581\n",
      "Epoch 8004: train loss: 0.015215791761875153, val loss: 0.1063854917883873\n",
      "Epoch 8005: train loss: 0.025663824751973152, val loss: 0.059159208089113235\n",
      "Epoch 8006: train loss: 0.025846820324659348, val loss: 0.11065059900283813\n",
      "Epoch 8007: train loss: 0.021489610895514488, val loss: 0.07325141876935959\n",
      "Epoch 8008: train loss: 0.016369914636015892, val loss: 0.05509500578045845\n",
      "Epoch 8009: train loss: 0.012755284085869789, val loss: 0.06253426522016525\n",
      "Epoch 8010: train loss: 0.01425899937748909, val loss: 0.06931351125240326\n",
      "Epoch 8011: train loss: 0.015084391459822655, val loss: 0.07429319620132446\n",
      "Epoch 8012: train loss: 0.017508933320641518, val loss: 0.06569857150316238\n",
      "Epoch 8013: train loss: 0.012283318676054478, val loss: 0.04494703188538551\n",
      "Epoch 8014: train loss: 0.016940923407673836, val loss: 0.06942006945610046\n",
      "Epoch 8015: train loss: 0.013736598193645477, val loss: 0.11373282968997955\n",
      "Epoch 8016: train loss: 0.014373386278748512, val loss: 0.10373573750257492\n",
      "Epoch 8017: train loss: 0.017986204475164413, val loss: 0.03250681981444359\n",
      "Epoch 8018: train loss: 0.014901791699230671, val loss: 0.03611643612384796\n",
      "Epoch 8019: train loss: 0.015198628418147564, val loss: 0.08341088145971298\n",
      "Epoch 8020: train loss: 0.017455853521823883, val loss: 0.06637287139892578\n",
      "Epoch 8021: train loss: 0.02260770834982395, val loss: 0.0465272031724453\n",
      "Epoch 8022: train loss: 0.013914386741816998, val loss: 0.08510024100542068\n",
      "Epoch 8023: train loss: 0.01903735101222992, val loss: 0.08152041584253311\n",
      "Epoch 8024: train loss: 0.01862243004143238, val loss: 0.048924386501312256\n",
      "Epoch 8025: train loss: 0.01339307427406311, val loss: 0.07911204546689987\n",
      "Epoch 8026: train loss: 0.01637119986116886, val loss: 0.08789105713367462\n",
      "Epoch 8027: train loss: 0.01953890360891819, val loss: 0.05496744439005852\n",
      "Epoch 8028: train loss: 0.011451121419668198, val loss: 0.07861775159835815\n",
      "Epoch 8029: train loss: 0.014631903730332851, val loss: 0.05281908065080643\n",
      "Epoch 8030: train loss: 0.013296008110046387, val loss: 0.07273878902196884\n",
      "Epoch 8031: train loss: 0.017320241779088974, val loss: 0.06107109785079956\n",
      "Epoch 8032: train loss: 0.013958843424916267, val loss: 0.03967464342713356\n",
      "Epoch 8033: train loss: 0.016014786437153816, val loss: 0.07876115292310715\n",
      "Epoch 8034: train loss: 0.014973076991736889, val loss: 0.08162470906972885\n",
      "Epoch 8035: train loss: 0.0174105241894722, val loss: 0.09307610243558884\n",
      "Epoch 8036: train loss: 0.026640325784683228, val loss: 0.06259949505329132\n",
      "Epoch 8037: train loss: 0.024098597466945648, val loss: 0.05796504020690918\n",
      "Epoch 8038: train loss: 0.01539912261068821, val loss: 0.02806684374809265\n",
      "Epoch 8039: train loss: 0.01848672330379486, val loss: 0.06084976717829704\n",
      "Epoch 8040: train loss: 0.015871800482273102, val loss: 0.07235594838857651\n",
      "Epoch 8041: train loss: 0.010582889430224895, val loss: 0.08213686943054199\n",
      "Epoch 8042: train loss: 0.016164081171154976, val loss: 0.06830310821533203\n",
      "Epoch 8043: train loss: 0.01699826680123806, val loss: 0.08312060683965683\n",
      "Epoch 8044: train loss: 0.017677724361419678, val loss: 0.10397342592477798\n",
      "Epoch 8045: train loss: 0.01631391979753971, val loss: 0.07232129573822021\n",
      "Epoch 8046: train loss: 0.014060254208743572, val loss: 0.07115545123815536\n",
      "Epoch 8047: train loss: 0.01553426869213581, val loss: 0.08433759957551956\n",
      "Epoch 8048: train loss: 0.01571113057434559, val loss: 0.05578271299600601\n",
      "Epoch 8049: train loss: 0.020048843696713448, val loss: 0.04983602091670036\n",
      "Epoch 8050: train loss: 0.01671792007982731, val loss: 0.07409892976284027\n",
      "Epoch 8051: train loss: 0.01577753573656082, val loss: 0.11762890964746475\n",
      "Epoch 8052: train loss: 0.01663665659725666, val loss: 0.10710272938013077\n",
      "Epoch 8053: train loss: 0.016553392633795738, val loss: 0.06200063228607178\n",
      "Epoch 8054: train loss: 0.011797785758972168, val loss: 0.09434995800256729\n",
      "Epoch 8055: train loss: 0.020034046843647957, val loss: 0.08384612202644348\n",
      "Epoch 8056: train loss: 0.015346190892159939, val loss: 0.07514939457178116\n",
      "Epoch 8057: train loss: 0.015389061532914639, val loss: 0.07254719734191895\n",
      "Epoch 8058: train loss: 0.014821484684944153, val loss: 0.08215297758579254\n",
      "Epoch 8059: train loss: 0.017353413626551628, val loss: 0.09318911284208298\n",
      "Epoch 8060: train loss: 0.014188099652528763, val loss: 0.06098804622888565\n",
      "Epoch 8061: train loss: 0.016119178384542465, val loss: 0.0451531745493412\n",
      "Epoch 8062: train loss: 0.01805846206843853, val loss: 0.05853624269366264\n",
      "Epoch 8063: train loss: 0.01691761612892151, val loss: 0.04959329962730408\n",
      "Epoch 8064: train loss: 0.013095076195895672, val loss: 0.07897044718265533\n",
      "Epoch 8065: train loss: 0.015950463712215424, val loss: 0.040758948773145676\n",
      "Epoch 8066: train loss: 0.017688559368252754, val loss: 0.06196073442697525\n",
      "Epoch 8067: train loss: 0.016500327736139297, val loss: 0.058288730680942535\n",
      "Epoch 8068: train loss: 0.012333168648183346, val loss: 0.0909239873290062\n",
      "Epoch 8069: train loss: 0.019130004569888115, val loss: 0.07923721522092819\n",
      "Epoch 8070: train loss: 0.014445561915636063, val loss: 0.04952197149395943\n",
      "Epoch 8071: train loss: 0.01995254121720791, val loss: 0.056326974183321\n",
      "Epoch 8072: train loss: 0.013824136927723885, val loss: 0.10897435247898102\n",
      "Epoch 8073: train loss: 0.016686713322997093, val loss: 0.12075439840555191\n",
      "Epoch 8074: train loss: 0.016140997409820557, val loss: 0.058120813220739365\n",
      "Epoch 8075: train loss: 0.015445428900420666, val loss: 0.07144634425640106\n",
      "Epoch 8076: train loss: 0.01874469220638275, val loss: 0.08105164021253586\n",
      "Epoch 8077: train loss: 0.011046911589801311, val loss: 0.07966114580631256\n",
      "Epoch 8078: train loss: 0.019269222393631935, val loss: 0.06710758060216904\n",
      "Epoch 8079: train loss: 0.01910274103283882, val loss: 0.06961607933044434\n",
      "Epoch 8080: train loss: 0.01422768086194992, val loss: 0.05990612506866455\n",
      "Epoch 8081: train loss: 0.01729082316160202, val loss: 0.04572754725813866\n",
      "Epoch 8082: train loss: 0.01916424185037613, val loss: 0.07720919698476791\n",
      "Epoch 8083: train loss: 0.014387547969818115, val loss: 0.10494985431432724\n",
      "Epoch 8084: train loss: 0.016600653529167175, val loss: 0.048901546746492386\n",
      "Epoch 8085: train loss: 0.017034579068422318, val loss: 0.09986326843500137\n",
      "Epoch 8086: train loss: 0.014674725010991096, val loss: 0.07287712395191193\n",
      "Epoch 8087: train loss: 0.01143274363130331, val loss: 0.12844951450824738\n",
      "Epoch 8088: train loss: 0.016862737014889717, val loss: 0.11157609522342682\n",
      "Epoch 8089: train loss: 0.016979185864329338, val loss: 0.03597315773367882\n",
      "Epoch 8090: train loss: 0.014688669703900814, val loss: 0.11996269226074219\n",
      "Epoch 8091: train loss: 0.018994610756635666, val loss: 0.07802364975214005\n",
      "Epoch 8092: train loss: 0.01786774769425392, val loss: 0.08077319711446762\n",
      "Epoch 8093: train loss: 0.017110031098127365, val loss: 0.08583390712738037\n",
      "Epoch 8094: train loss: 0.016313135623931885, val loss: 0.07962683588266373\n",
      "Epoch 8095: train loss: 0.016225561499595642, val loss: 0.06711611896753311\n",
      "Epoch 8096: train loss: 0.017627958208322525, val loss: 0.07128723710775375\n",
      "Epoch 8097: train loss: 0.01842327229678631, val loss: 0.070653036236763\n",
      "Epoch 8098: train loss: 0.022540776059031487, val loss: 0.08853965252637863\n",
      "Epoch 8099: train loss: 0.012162836268544197, val loss: 0.06424091011285782\n",
      "Epoch 8100: train loss: 0.013548984192311764, val loss: 0.0538606233894825\n",
      "Epoch 8101: train loss: 0.016196003183722496, val loss: 0.0892244204878807\n",
      "Epoch 8102: train loss: 0.021347593516111374, val loss: 0.06243816018104553\n",
      "Epoch 8103: train loss: 0.01297531183809042, val loss: 0.09277571737766266\n",
      "Epoch 8104: train loss: 0.01578494720160961, val loss: 0.098426952958107\n",
      "Epoch 8105: train loss: 0.012348623014986515, val loss: 0.0786750465631485\n",
      "Epoch 8106: train loss: 0.013227547518908978, val loss: 0.05382256582379341\n",
      "Epoch 8107: train loss: 0.011793367564678192, val loss: 0.09819089621305466\n",
      "Epoch 8108: train loss: 0.013793552294373512, val loss: 0.06847422569990158\n",
      "Epoch 8109: train loss: 0.013378591276705265, val loss: 0.08193295449018478\n",
      "Epoch 8110: train loss: 0.017826762050390244, val loss: 0.08839554339647293\n",
      "Epoch 8111: train loss: 0.016536109149456024, val loss: 0.13365276157855988\n",
      "Epoch 8112: train loss: 0.014903375878930092, val loss: 0.09025394916534424\n",
      "Epoch 8113: train loss: 0.016588324680924416, val loss: 0.042244669049978256\n",
      "Epoch 8114: train loss: 0.018163837492465973, val loss: 0.06221333146095276\n",
      "Epoch 8115: train loss: 0.020206978544592857, val loss: 0.058991629630327225\n",
      "Epoch 8116: train loss: 0.019053222611546516, val loss: 0.07595278322696686\n",
      "Epoch 8117: train loss: 0.01738591492176056, val loss: 0.07073652744293213\n",
      "Epoch 8118: train loss: 0.0200433861464262, val loss: 0.03295587748289108\n",
      "Epoch 8119: train loss: 0.01443330105394125, val loss: 0.11504142731428146\n",
      "Epoch 8120: train loss: 0.01327858678996563, val loss: 0.08057280629873276\n",
      "Epoch 8121: train loss: 0.015581777319312096, val loss: 0.12038660049438477\n",
      "Epoch 8122: train loss: 0.01538421306759119, val loss: 0.05206206068396568\n",
      "Epoch 8123: train loss: 0.012834708206355572, val loss: 0.03782258555293083\n",
      "Epoch 8124: train loss: 0.015024666674435139, val loss: 0.07148294895887375\n",
      "Epoch 8125: train loss: 0.019682005047798157, val loss: 0.050899799913167953\n",
      "Epoch 8126: train loss: 0.01756877824664116, val loss: 0.046940285712480545\n",
      "Epoch 8127: train loss: 0.01690060645341873, val loss: 0.056109536439180374\n",
      "Epoch 8128: train loss: 0.010639932937920094, val loss: 0.06799051910638809\n",
      "Epoch 8129: train loss: 0.01581549644470215, val loss: 0.09266450256109238\n",
      "Epoch 8130: train loss: 0.018077321350574493, val loss: 0.033398013561964035\n",
      "Epoch 8131: train loss: 0.016528528183698654, val loss: 0.05737863853573799\n",
      "Epoch 8132: train loss: 0.01725897006690502, val loss: 0.09411513060331345\n",
      "Epoch 8133: train loss: 0.01889164187014103, val loss: 0.04647520184516907\n",
      "Epoch 8134: train loss: 0.019932573661208153, val loss: 0.09998541325330734\n",
      "Epoch 8135: train loss: 0.016520554199814796, val loss: 0.08925541490316391\n",
      "Epoch 8136: train loss: 0.017985517159104347, val loss: 0.0886930376291275\n",
      "Epoch 8137: train loss: 0.013836188241839409, val loss: 0.04972678795456886\n",
      "Epoch 8138: train loss: 0.018041478469967842, val loss: 0.05143505334854126\n",
      "Epoch 8139: train loss: 0.018659228459000587, val loss: 0.0864783450961113\n",
      "Epoch 8140: train loss: 0.009497805498540401, val loss: 0.05490986630320549\n",
      "Epoch 8141: train loss: 0.010817066766321659, val loss: 0.09712138772010803\n",
      "Epoch 8142: train loss: 0.016925806179642677, val loss: 0.06507685780525208\n",
      "Epoch 8143: train loss: 0.01836756058037281, val loss: 0.061726976186037064\n",
      "Epoch 8144: train loss: 0.01290968433022499, val loss: 0.05678025633096695\n",
      "Epoch 8145: train loss: 0.020225537940859795, val loss: 0.06758131831884384\n",
      "Epoch 8146: train loss: 0.013949444517493248, val loss: 0.06519593298435211\n",
      "Epoch 8147: train loss: 0.011358545161783695, val loss: 0.07296108454465866\n",
      "Epoch 8148: train loss: 0.020315656438469887, val loss: 0.07951098680496216\n",
      "Epoch 8149: train loss: 0.020131239667534828, val loss: 0.09536558389663696\n",
      "Epoch 8150: train loss: 0.011603533290326595, val loss: 0.06863311678171158\n",
      "Epoch 8151: train loss: 0.021359717473387718, val loss: 0.08832409232854843\n",
      "Epoch 8152: train loss: 0.012653165496885777, val loss: 0.08656061440706253\n",
      "Epoch 8153: train loss: 0.013674073852598667, val loss: 0.06173845753073692\n",
      "Epoch 8154: train loss: 0.01662745326757431, val loss: 0.06433480232954025\n",
      "Epoch 8155: train loss: 0.012615320272743702, val loss: 0.06381439417600632\n",
      "Epoch 8156: train loss: 0.01890449970960617, val loss: 0.08735095709562302\n",
      "Epoch 8157: train loss: 0.012518394738435745, val loss: 0.051518917083740234\n",
      "Epoch 8158: train loss: 0.01839161105453968, val loss: 0.07809726148843765\n",
      "Epoch 8159: train loss: 0.015739453956484795, val loss: 0.08581387996673584\n",
      "Epoch 8160: train loss: 0.011424422264099121, val loss: 0.04835633561015129\n",
      "Epoch 8161: train loss: 0.012796749360859394, val loss: 0.05418390780687332\n",
      "Epoch 8162: train loss: 0.014938045293092728, val loss: 0.03377525135874748\n",
      "Epoch 8163: train loss: 0.014780929312109947, val loss: 0.12839582562446594\n",
      "Epoch 8164: train loss: 0.01971135474741459, val loss: 0.06019863486289978\n",
      "Epoch 8165: train loss: 0.02013479731976986, val loss: 0.10454974323511124\n",
      "Epoch 8166: train loss: 0.016719020903110504, val loss: 0.08096250146627426\n",
      "Epoch 8167: train loss: 0.018535947427153587, val loss: 0.04695207625627518\n",
      "Epoch 8168: train loss: 0.011873343959450722, val loss: 0.05552782490849495\n",
      "Epoch 8169: train loss: 0.019682232290506363, val loss: 0.06810985505580902\n",
      "Epoch 8170: train loss: 0.01566324569284916, val loss: 0.04910975694656372\n",
      "Epoch 8171: train loss: 0.021492836996912956, val loss: 0.04570942744612694\n",
      "Epoch 8172: train loss: 0.01439558994024992, val loss: 0.07388357818126678\n",
      "Epoch 8173: train loss: 0.02100088819861412, val loss: 0.05499647930264473\n",
      "Epoch 8174: train loss: 0.018111303448677063, val loss: 0.03601163253188133\n",
      "Epoch 8175: train loss: 0.021377580240368843, val loss: 0.04973876476287842\n",
      "Epoch 8176: train loss: 0.014961009845137596, val loss: 0.07735001295804977\n",
      "Epoch 8177: train loss: 0.012303895317018032, val loss: 0.07700037956237793\n",
      "Epoch 8178: train loss: 0.015217657200992107, val loss: 0.0503883920609951\n",
      "Epoch 8179: train loss: 0.012243583798408508, val loss: 0.06234721094369888\n",
      "Epoch 8180: train loss: 0.015067614614963531, val loss: 0.06467213481664658\n",
      "Epoch 8181: train loss: 0.015771878883242607, val loss: 0.08607615530490875\n",
      "Epoch 8182: train loss: 0.013736805878579617, val loss: 0.0690448060631752\n",
      "Epoch 8183: train loss: 0.026474980637431145, val loss: 0.07076067477464676\n",
      "Epoch 8184: train loss: 0.013022220693528652, val loss: 0.058527447283267975\n",
      "Epoch 8185: train loss: 0.016061797738075256, val loss: 0.056712355464696884\n",
      "Epoch 8186: train loss: 0.01280552614480257, val loss: 0.06094619259238243\n",
      "Epoch 8187: train loss: 0.013652033172547817, val loss: 0.0958280935883522\n",
      "Epoch 8188: train loss: 0.011487212963402271, val loss: 0.07014868408441544\n",
      "Epoch 8189: train loss: 0.023153794929385185, val loss: 0.0414106622338295\n",
      "Epoch 8190: train loss: 0.02543063461780548, val loss: 0.045284297317266464\n",
      "Epoch 8191: train loss: 0.015225095674395561, val loss: 0.06238824129104614\n",
      "Epoch 8192: train loss: 0.016625164076685905, val loss: 0.07396616041660309\n",
      "Epoch 8193: train loss: 0.017622672021389008, val loss: 0.0531759150326252\n",
      "Epoch 8194: train loss: 0.016678042709827423, val loss: 0.10489871352910995\n",
      "Epoch 8195: train loss: 0.016280455514788628, val loss: 0.07731210440397263\n",
      "Epoch 8196: train loss: 0.016117900609970093, val loss: 0.057857125997543335\n",
      "Epoch 8197: train loss: 0.014140714891254902, val loss: 0.08071116358041763\n",
      "Epoch 8198: train loss: 0.012944988906383514, val loss: 0.08523644506931305\n",
      "Epoch 8199: train loss: 0.014738314785063267, val loss: 0.10172846168279648\n",
      "Epoch 8200: train loss: 0.012043600901961327, val loss: 0.051835980266332626\n",
      "Epoch 8201: train loss: 0.017504598945379257, val loss: 0.08814483135938644\n",
      "Epoch 8202: train loss: 0.016854731366038322, val loss: 0.049997493624687195\n",
      "Epoch 8203: train loss: 0.020361004397273064, val loss: 0.059314336627721786\n",
      "Epoch 8204: train loss: 0.012859017588198185, val loss: 0.06294753402471542\n",
      "Epoch 8205: train loss: 0.015848098322749138, val loss: 0.0878232941031456\n",
      "Epoch 8206: train loss: 0.012992742471396923, val loss: 0.10656286776065826\n",
      "Epoch 8207: train loss: 0.017456937581300735, val loss: 0.07032247632741928\n",
      "Epoch 8208: train loss: 0.01257462240755558, val loss: 0.05855916813015938\n",
      "Epoch 8209: train loss: 0.014394502155482769, val loss: 0.11252863705158234\n",
      "Epoch 8210: train loss: 0.020209884271025658, val loss: 0.10508344322443008\n",
      "Epoch 8211: train loss: 0.02199995331466198, val loss: 0.061747800558805466\n",
      "Epoch 8212: train loss: 0.016618600115180016, val loss: 0.08983065187931061\n",
      "Epoch 8213: train loss: 0.015014434233307838, val loss: 0.06773889809846878\n",
      "Epoch 8214: train loss: 0.011855187825858593, val loss: 0.09246119111776352\n",
      "Epoch 8215: train loss: 0.015713095664978027, val loss: 0.062428224831819534\n",
      "Epoch 8216: train loss: 0.017852501943707466, val loss: 0.05062985420227051\n",
      "Epoch 8217: train loss: 0.010607503354549408, val loss: 0.05945746973156929\n",
      "Epoch 8218: train loss: 0.01624472439289093, val loss: 0.11839675158262253\n",
      "Epoch 8219: train loss: 0.018400171771645546, val loss: 0.10014249384403229\n",
      "Epoch 8220: train loss: 0.01329209003597498, val loss: 0.054075878113508224\n",
      "Epoch 8221: train loss: 0.019256526604294777, val loss: 0.07333355396986008\n",
      "Epoch 8222: train loss: 0.009333065710961819, val loss: 0.05852421745657921\n",
      "Epoch 8223: train loss: 0.01993929222226143, val loss: 0.08129531890153885\n",
      "Epoch 8224: train loss: 0.016602521762251854, val loss: 0.040185168385505676\n",
      "Epoch 8225: train loss: 0.011897996068000793, val loss: 0.0567607544362545\n",
      "Epoch 8226: train loss: 0.013768530450761318, val loss: 0.046045538038015366\n",
      "Epoch 8227: train loss: 0.014403759501874447, val loss: 0.06534332036972046\n",
      "Epoch 8228: train loss: 0.015824945643544197, val loss: 0.06425511091947556\n",
      "Epoch 8229: train loss: 0.01489512249827385, val loss: 0.07356826961040497\n",
      "Epoch 8230: train loss: 0.014966647140681744, val loss: 0.08703426271677017\n",
      "Epoch 8231: train loss: 0.015279057435691357, val loss: 0.10686378926038742\n",
      "Epoch 8232: train loss: 0.020338961854577065, val loss: 0.06420034170150757\n",
      "Epoch 8233: train loss: 0.013311684131622314, val loss: 0.09891953319311142\n",
      "Epoch 8234: train loss: 0.020136160776019096, val loss: 0.04029716178774834\n",
      "Epoch 8235: train loss: 0.014385675080120564, val loss: 0.07539929449558258\n",
      "Epoch 8236: train loss: 0.013784187845885754, val loss: 0.08821389824151993\n",
      "Epoch 8237: train loss: 0.023495711386203766, val loss: 0.04647982865571976\n",
      "Epoch 8238: train loss: 0.013137610629200935, val loss: 0.06878537684679031\n",
      "Epoch 8239: train loss: 0.019374828785657883, val loss: 0.07282120734453201\n",
      "Epoch 8240: train loss: 0.012678353115916252, val loss: 0.09609261900186539\n",
      "Epoch 8241: train loss: 0.017268149182200432, val loss: 0.08294720947742462\n",
      "Epoch 8242: train loss: 0.01835360750555992, val loss: 0.0642806887626648\n",
      "Epoch 8243: train loss: 0.01714087463915348, val loss: 0.047117944806814194\n",
      "Epoch 8244: train loss: 0.02031496912240982, val loss: 0.05138624459505081\n",
      "Epoch 8245: train loss: 0.014589952304959297, val loss: 0.0924864187836647\n",
      "Epoch 8246: train loss: 0.019374897703528404, val loss: 0.05606430396437645\n",
      "Epoch 8247: train loss: 0.02048603817820549, val loss: 0.0841321274638176\n",
      "Epoch 8248: train loss: 0.018781142309308052, val loss: 0.07632150501012802\n",
      "Epoch 8249: train loss: 0.01728614792227745, val loss: 0.06419440358877182\n",
      "Epoch 8250: train loss: 0.017651990056037903, val loss: 0.03407256305217743\n",
      "Epoch 8251: train loss: 0.015577603131532669, val loss: 0.0559784360229969\n",
      "Epoch 8252: train loss: 0.018741421401500702, val loss: 0.07537468522787094\n",
      "Epoch 8253: train loss: 0.016005277633666992, val loss: 0.10179837048053741\n",
      "Epoch 8254: train loss: 0.017994966357946396, val loss: 0.09213479608297348\n",
      "Epoch 8255: train loss: 0.015525714494287968, val loss: 0.11052055656909943\n",
      "Epoch 8256: train loss: 0.018221329897642136, val loss: 0.07493629306554794\n",
      "Epoch 8257: train loss: 0.014286053366959095, val loss: 0.06787203997373581\n",
      "Epoch 8258: train loss: 0.013217571191489697, val loss: 0.12217270582914352\n",
      "Epoch 8259: train loss: 0.015770377591252327, val loss: 0.08306693285703659\n",
      "Epoch 8260: train loss: 0.011973298154771328, val loss: 0.05843555927276611\n",
      "Epoch 8261: train loss: 0.013950805179774761, val loss: 0.054319482296705246\n",
      "Epoch 8262: train loss: 0.014365315437316895, val loss: 0.10135193169116974\n",
      "Epoch 8263: train loss: 0.017521120607852936, val loss: 0.09651335328817368\n",
      "Epoch 8264: train loss: 0.012089446187019348, val loss: 0.056608058512210846\n",
      "Epoch 8265: train loss: 0.01528081949800253, val loss: 0.0542457215487957\n",
      "Epoch 8266: train loss: 0.013530909083783627, val loss: 0.06872592121362686\n",
      "Epoch 8267: train loss: 0.01055658794939518, val loss: 0.0858638808131218\n",
      "Epoch 8268: train loss: 0.021389223635196686, val loss: 0.09163028001785278\n",
      "Epoch 8269: train loss: 0.01692221127450466, val loss: 0.05999166518449783\n",
      "Epoch 8270: train loss: 0.01420015748590231, val loss: 0.0940568596124649\n",
      "Epoch 8271: train loss: 0.014395847916603088, val loss: 0.07879288494586945\n",
      "Epoch 8272: train loss: 0.016404006630182266, val loss: 0.0672006830573082\n",
      "Epoch 8273: train loss: 0.01944149099290371, val loss: 0.06599333137273788\n",
      "Epoch 8274: train loss: 0.01524239033460617, val loss: 0.07333412021398544\n",
      "Epoch 8275: train loss: 0.01674262247979641, val loss: 0.060966551303863525\n",
      "Epoch 8276: train loss: 0.019298406317830086, val loss: 0.06310214102268219\n",
      "Epoch 8277: train loss: 0.012782610952854156, val loss: 0.08136721700429916\n",
      "Epoch 8278: train loss: 0.020068824291229248, val loss: 0.07237281650304794\n",
      "Epoch 8279: train loss: 0.011835901066660881, val loss: 0.06733018904924393\n",
      "Epoch 8280: train loss: 0.01512004341930151, val loss: 0.0949820727109909\n",
      "Epoch 8281: train loss: 0.01387310866266489, val loss: 0.07392359524965286\n",
      "Epoch 8282: train loss: 0.015613033436238766, val loss: 0.08254425972700119\n",
      "Epoch 8283: train loss: 0.01267289649695158, val loss: 0.09291233867406845\n",
      "Epoch 8284: train loss: 0.014486148953437805, val loss: 0.07398547977209091\n",
      "Epoch 8285: train loss: 0.017100509256124496, val loss: 0.07887493818998337\n",
      "Epoch 8286: train loss: 0.016444405540823936, val loss: 0.06168859079480171\n",
      "Epoch 8287: train loss: 0.011681787669658661, val loss: 0.09505770355463028\n",
      "Epoch 8288: train loss: 0.01842258870601654, val loss: 0.05239842087030411\n",
      "Epoch 8289: train loss: 0.013406462036073208, val loss: 0.05534352734684944\n",
      "Epoch 8290: train loss: 0.014828518033027649, val loss: 0.07201191037893295\n",
      "Epoch 8291: train loss: 0.015393055975437164, val loss: 0.055261559784412384\n",
      "Epoch 8292: train loss: 0.020042704418301582, val loss: 0.1033039465546608\n",
      "Epoch 8293: train loss: 0.020540473982691765, val loss: 0.07546960562467575\n",
      "Epoch 8294: train loss: 0.016998514533042908, val loss: 0.07670341432094574\n",
      "Epoch 8295: train loss: 0.01765584945678711, val loss: 0.06309040635824203\n",
      "Epoch 8296: train loss: 0.0157003216445446, val loss: 0.07942033559083939\n",
      "Epoch 8297: train loss: 0.017792262136936188, val loss: 0.051327433437108994\n",
      "Epoch 8298: train loss: 0.017492642626166344, val loss: 0.06768973916769028\n",
      "Epoch 8299: train loss: 0.01461469940841198, val loss: 0.07347714900970459\n",
      "Epoch 8300: train loss: 0.014028745703399181, val loss: 0.07576049864292145\n",
      "Epoch 8301: train loss: 0.017612280324101448, val loss: 0.06814247369766235\n",
      "Epoch 8302: train loss: 0.017897436395287514, val loss: 0.050757717341184616\n",
      "Epoch 8303: train loss: 0.019114244729280472, val loss: 0.061449360102415085\n",
      "Epoch 8304: train loss: 0.016529206186532974, val loss: 0.07401620596647263\n",
      "Epoch 8305: train loss: 0.01900568976998329, val loss: 0.05992719158530235\n",
      "Epoch 8306: train loss: 0.01630449667572975, val loss: 0.09169455617666245\n",
      "Epoch 8307: train loss: 0.011815516278147697, val loss: 0.08624783903360367\n",
      "Epoch 8308: train loss: 0.01515360176563263, val loss: 0.06130237504839897\n",
      "Epoch 8309: train loss: 0.019589440897107124, val loss: 0.04832126200199127\n",
      "Epoch 8310: train loss: 0.02116709016263485, val loss: 0.08085998147726059\n",
      "Epoch 8311: train loss: 0.013523788191378117, val loss: 0.06461293250322342\n",
      "Epoch 8312: train loss: 0.013416016474366188, val loss: 0.06059317663311958\n",
      "Epoch 8313: train loss: 0.021725837141275406, val loss: 0.11271519958972931\n",
      "Epoch 8314: train loss: 0.021020926535129547, val loss: 0.06352442502975464\n",
      "Epoch 8315: train loss: 0.025000223889946938, val loss: 0.10035409778356552\n",
      "Epoch 8316: train loss: 0.01748955249786377, val loss: 0.07905180752277374\n",
      "Epoch 8317: train loss: 0.014999485574662685, val loss: 0.06488674134016037\n",
      "Epoch 8318: train loss: 0.012101272121071815, val loss: 0.09119509905576706\n",
      "Epoch 8319: train loss: 0.018666202202439308, val loss: 0.07579278200864792\n",
      "Epoch 8320: train loss: 0.011254715733230114, val loss: 0.06209911033511162\n",
      "Epoch 8321: train loss: 0.015377691015601158, val loss: 0.07048308104276657\n",
      "Epoch 8322: train loss: 0.014576994813978672, val loss: 0.0578685887157917\n",
      "Epoch 8323: train loss: 0.012780866585671902, val loss: 0.06037762388586998\n",
      "Epoch 8324: train loss: 0.01339612528681755, val loss: 0.0504242442548275\n",
      "Epoch 8325: train loss: 0.011733196675777435, val loss: 0.06807666271924973\n",
      "Epoch 8326: train loss: 0.01023178268224001, val loss: 0.0745217427611351\n",
      "Epoch 8327: train loss: 0.015545107424259186, val loss: 0.07361715286970139\n",
      "Epoch 8328: train loss: 0.011681037954986095, val loss: 0.03954998031258583\n",
      "Epoch 8329: train loss: 0.016893751919269562, val loss: 0.060425080358982086\n",
      "Epoch 8330: train loss: 0.015237762592732906, val loss: 0.07423355430364609\n",
      "Epoch 8331: train loss: 0.015255637466907501, val loss: 0.09430940449237823\n",
      "Epoch 8332: train loss: 0.018863119184970856, val loss: 0.04799553379416466\n",
      "Epoch 8333: train loss: 0.01110019814223051, val loss: 0.04450708627700806\n",
      "Epoch 8334: train loss: 0.022970125079154968, val loss: 0.08652245253324509\n",
      "Epoch 8335: train loss: 0.01468248013406992, val loss: 0.06099941208958626\n",
      "Epoch 8336: train loss: 0.013820251449942589, val loss: 0.04227898642420769\n",
      "Epoch 8337: train loss: 0.017263196408748627, val loss: 0.09041669964790344\n",
      "Epoch 8338: train loss: 0.015318282879889011, val loss: 0.059286929666996\n",
      "Epoch 8339: train loss: 0.011616344563663006, val loss: 0.05116526037454605\n",
      "Epoch 8340: train loss: 0.018963651731610298, val loss: 0.06682653725147247\n",
      "Epoch 8341: train loss: 0.011490174569189548, val loss: 0.06472364813089371\n",
      "Epoch 8342: train loss: 0.012942589819431305, val loss: 0.07257623225450516\n",
      "Epoch 8343: train loss: 0.013624811545014381, val loss: 0.05490591749548912\n",
      "Epoch 8344: train loss: 0.017499152570962906, val loss: 0.06498941034078598\n",
      "Epoch 8345: train loss: 0.014233923517167568, val loss: 0.042083512991666794\n",
      "Epoch 8346: train loss: 0.01869591511785984, val loss: 0.06443127244710922\n",
      "Epoch 8347: train loss: 0.013892047107219696, val loss: 0.052214957773685455\n",
      "Epoch 8348: train loss: 0.016142956912517548, val loss: 0.08456803113222122\n",
      "Epoch 8349: train loss: 0.017533617094159126, val loss: 0.09599762409925461\n",
      "Epoch 8350: train loss: 0.017236048355698586, val loss: 0.10154111683368683\n",
      "Epoch 8351: train loss: 0.011223946698009968, val loss: 0.05604670196771622\n",
      "Epoch 8352: train loss: 0.015211659483611584, val loss: 0.08040675520896912\n",
      "Epoch 8353: train loss: 0.01944461092352867, val loss: 0.08336597681045532\n",
      "Epoch 8354: train loss: 0.017902588471770287, val loss: 0.1154986172914505\n",
      "Epoch 8355: train loss: 0.014159750193357468, val loss: 0.10137925297021866\n",
      "Epoch 8356: train loss: 0.02149338461458683, val loss: 0.05467139557003975\n",
      "Epoch 8357: train loss: 0.01650962419807911, val loss: 0.06042036414146423\n",
      "Epoch 8358: train loss: 0.016280479729175568, val loss: 0.08898135274648666\n",
      "Epoch 8359: train loss: 0.01949102059006691, val loss: 0.07451758533716202\n",
      "Epoch 8360: train loss: 0.015167655423283577, val loss: 0.09622155874967575\n",
      "Epoch 8361: train loss: 0.01431704219430685, val loss: 0.07986129075288773\n",
      "Epoch 8362: train loss: 0.016247577965259552, val loss: 0.07015027850866318\n",
      "Epoch 8363: train loss: 0.012109852395951748, val loss: 0.07313820719718933\n",
      "Epoch 8364: train loss: 0.018206292763352394, val loss: 0.04963541403412819\n",
      "Epoch 8365: train loss: 0.013818670064210892, val loss: 0.06869993358850479\n",
      "Epoch 8366: train loss: 0.015144352801144123, val loss: 0.10900934040546417\n",
      "Epoch 8367: train loss: 0.01307130977511406, val loss: 0.07565565407276154\n",
      "Epoch 8368: train loss: 0.015572248958051205, val loss: 0.10385794937610626\n",
      "Epoch 8369: train loss: 0.013219205662608147, val loss: 0.07291781157255173\n",
      "Epoch 8370: train loss: 0.01657991111278534, val loss: 0.04237716272473335\n",
      "Epoch 8371: train loss: 0.020936938002705574, val loss: 0.08592508733272552\n",
      "Epoch 8372: train loss: 0.012327670119702816, val loss: 0.0907537117600441\n",
      "Epoch 8373: train loss: 0.01670064777135849, val loss: 0.07273515313863754\n",
      "Epoch 8374: train loss: 0.016667954623699188, val loss: 0.042428355664014816\n",
      "Epoch 8375: train loss: 0.014049993827939034, val loss: 0.09131493419408798\n",
      "Epoch 8376: train loss: 0.01655288226902485, val loss: 0.09304530918598175\n",
      "Epoch 8377: train loss: 0.013692294247448444, val loss: 0.05462925508618355\n",
      "Epoch 8378: train loss: 0.017365867272019386, val loss: 0.08090794831514359\n",
      "Epoch 8379: train loss: 0.013747917488217354, val loss: 0.044339168816804886\n",
      "Epoch 8380: train loss: 0.015289678238332272, val loss: 0.12317228317260742\n",
      "Epoch 8381: train loss: 0.021799832582473755, val loss: 0.09103795140981674\n",
      "Epoch 8382: train loss: 0.014289716258645058, val loss: 0.11362434923648834\n",
      "Epoch 8383: train loss: 0.016179857775568962, val loss: 0.09216709434986115\n",
      "Epoch 8384: train loss: 0.0177624449133873, val loss: 0.07632940262556076\n",
      "Epoch 8385: train loss: 0.035903461277484894, val loss: 0.07844573259353638\n",
      "Epoch 8386: train loss: 0.023147815838456154, val loss: 0.06343910098075867\n",
      "Epoch 8387: train loss: 0.02022228203713894, val loss: 0.054854001849889755\n",
      "Epoch 8388: train loss: 0.019812270998954773, val loss: 0.09632053226232529\n",
      "Epoch 8389: train loss: 0.015935489907860756, val loss: 0.1258169710636139\n",
      "Epoch 8390: train loss: 0.021438276395201683, val loss: 0.08503907173871994\n",
      "Epoch 8391: train loss: 0.020975757390260696, val loss: 0.06549384444952011\n",
      "Epoch 8392: train loss: 0.01622091233730316, val loss: 0.08871031552553177\n",
      "Epoch 8393: train loss: 0.014137022197246552, val loss: 0.08078720420598984\n",
      "Epoch 8394: train loss: 0.014291642233729362, val loss: 0.09095936268568039\n",
      "Epoch 8395: train loss: 0.01904790848493576, val loss: 0.07398619502782822\n",
      "Epoch 8396: train loss: 0.013716340065002441, val loss: 0.07485724985599518\n",
      "Epoch 8397: train loss: 0.01590271107852459, val loss: 0.052595674991607666\n",
      "Epoch 8398: train loss: 0.016701655462384224, val loss: 0.05895961448550224\n",
      "Epoch 8399: train loss: 0.01802235282957554, val loss: 0.07643226534128189\n",
      "Epoch 8400: train loss: 0.018541458994150162, val loss: 0.07625969499349594\n",
      "Epoch 8401: train loss: 0.019783126190304756, val loss: 0.08627793937921524\n",
      "Epoch 8402: train loss: 0.017619267106056213, val loss: 0.0541318915784359\n",
      "Epoch 8403: train loss: 0.012572542764246464, val loss: 0.08591414988040924\n",
      "Epoch 8404: train loss: 0.022069409489631653, val loss: 0.10196387767791748\n",
      "Epoch 8405: train loss: 0.012785816565155983, val loss: 0.05977094918489456\n",
      "Epoch 8406: train loss: 0.021634716540575027, val loss: 0.06418808549642563\n",
      "Epoch 8407: train loss: 0.017174912616610527, val loss: 0.06652247905731201\n",
      "Epoch 8408: train loss: 0.018812844529747963, val loss: 0.07260574400424957\n",
      "Epoch 8409: train loss: 0.01966034807264805, val loss: 0.05393895134329796\n",
      "Epoch 8410: train loss: 0.015142390504479408, val loss: 0.06016726419329643\n",
      "Epoch 8411: train loss: 0.012943580746650696, val loss: 0.06989436596632004\n",
      "Epoch 8412: train loss: 0.016369929537177086, val loss: 0.06978537887334824\n",
      "Epoch 8413: train loss: 0.014503786340355873, val loss: 0.07187395542860031\n",
      "Epoch 8414: train loss: 0.012845238670706749, val loss: 0.10586302727460861\n",
      "Epoch 8415: train loss: 0.01871420070528984, val loss: 0.07189007848501205\n",
      "Epoch 8416: train loss: 0.016753733158111572, val loss: 0.10670532286167145\n",
      "Epoch 8417: train loss: 0.013785537332296371, val loss: 0.05512326955795288\n",
      "Epoch 8418: train loss: 0.015915123745799065, val loss: 0.05053069815039635\n",
      "Epoch 8419: train loss: 0.01712820492684841, val loss: 0.08972818404436111\n",
      "Epoch 8420: train loss: 0.024906044825911522, val loss: 0.05583390220999718\n",
      "Epoch 8421: train loss: 0.01251932792365551, val loss: 0.08865723758935928\n",
      "Epoch 8422: train loss: 0.013598283752799034, val loss: 0.1224241629242897\n",
      "Epoch 8423: train loss: 0.013328621163964272, val loss: 0.08527347445487976\n",
      "Epoch 8424: train loss: 0.014458341524004936, val loss: 0.06033929064869881\n",
      "Epoch 8425: train loss: 0.016405783593654633, val loss: 0.06598866730928421\n",
      "Epoch 8426: train loss: 0.02131073735654354, val loss: 0.09137845039367676\n",
      "Epoch 8427: train loss: 0.020824674516916275, val loss: 0.05045223981142044\n",
      "Epoch 8428: train loss: 0.02075597457587719, val loss: 0.06876877695322037\n",
      "Epoch 8429: train loss: 0.018231399357318878, val loss: 0.06393007189035416\n",
      "Epoch 8430: train loss: 0.022279661148786545, val loss: 0.0690247043967247\n",
      "Epoch 8431: train loss: 0.02634047158062458, val loss: 0.07972278445959091\n",
      "Epoch 8432: train loss: 0.016442565247416496, val loss: 0.0601612888276577\n",
      "Epoch 8433: train loss: 0.02101190946996212, val loss: 0.08580668270587921\n",
      "Epoch 8434: train loss: 0.017935022711753845, val loss: 0.05394670367240906\n",
      "Epoch 8435: train loss: 0.02301386557519436, val loss: 0.11444304138422012\n",
      "Epoch 8436: train loss: 0.020915081724524498, val loss: 0.07779812067747116\n",
      "Epoch 8437: train loss: 0.015151063911616802, val loss: 0.07029374688863754\n",
      "Epoch 8438: train loss: 0.016518089920282364, val loss: 0.07783036679029465\n",
      "Epoch 8439: train loss: 0.01509274635463953, val loss: 0.05494309216737747\n",
      "Epoch 8440: train loss: 0.026706356555223465, val loss: 0.10405931621789932\n",
      "Epoch 8441: train loss: 0.016485584899783134, val loss: 0.04798951372504234\n",
      "Epoch 8442: train loss: 0.01647210121154785, val loss: 0.11749953031539917\n",
      "Epoch 8443: train loss: 0.018649527803063393, val loss: 0.10033810138702393\n",
      "Epoch 8444: train loss: 0.020860489457845688, val loss: 0.0877377986907959\n",
      "Epoch 8445: train loss: 0.013219906948506832, val loss: 0.06997530162334442\n",
      "Epoch 8446: train loss: 0.015309787355363369, val loss: 0.058114346116781235\n",
      "Epoch 8447: train loss: 0.019093666225671768, val loss: 0.0567704513669014\n",
      "Epoch 8448: train loss: 0.01831028051674366, val loss: 0.0625668540596962\n",
      "Epoch 8449: train loss: 0.016399946063756943, val loss: 0.06515210121870041\n",
      "Epoch 8450: train loss: 0.019785616546869278, val loss: 0.039944808930158615\n",
      "Epoch 8451: train loss: 0.016523566097021103, val loss: 0.07998286932706833\n",
      "Epoch 8452: train loss: 0.015592756681144238, val loss: 0.08605401962995529\n",
      "Epoch 8453: train loss: 0.017062468454241753, val loss: 0.1250530630350113\n",
      "Epoch 8454: train loss: 0.01815924607217312, val loss: 0.07014245539903641\n",
      "Epoch 8455: train loss: 0.017283907160162926, val loss: 0.10401406139135361\n",
      "Epoch 8456: train loss: 0.018055304884910583, val loss: 0.09958716481924057\n",
      "Epoch 8457: train loss: 0.018118880689144135, val loss: 0.06755337119102478\n",
      "Epoch 8458: train loss: 0.016485150903463364, val loss: 0.07624497264623642\n",
      "Epoch 8459: train loss: 0.015904782339930534, val loss: 0.08278673887252808\n",
      "Epoch 8460: train loss: 0.017129600048065186, val loss: 0.06945619732141495\n",
      "Epoch 8461: train loss: 0.017146773636341095, val loss: 0.0812816172838211\n",
      "Epoch 8462: train loss: 0.014337152242660522, val loss: 0.059014614671468735\n",
      "Epoch 8463: train loss: 0.019428275525569916, val loss: 0.08911356329917908\n",
      "Epoch 8464: train loss: 0.018553132191300392, val loss: 0.10983774811029434\n",
      "Epoch 8465: train loss: 0.01918010227382183, val loss: 0.10160105675458908\n",
      "Epoch 8466: train loss: 0.016299089416861534, val loss: 0.07847198843955994\n",
      "Epoch 8467: train loss: 0.019206443801522255, val loss: 0.08503945916891098\n",
      "Epoch 8468: train loss: 0.017027661204338074, val loss: 0.06989376991987228\n",
      "Epoch 8469: train loss: 0.02002318575978279, val loss: 0.07372032850980759\n",
      "Epoch 8470: train loss: 0.014639473520219326, val loss: 0.11400287598371506\n",
      "Epoch 8471: train loss: 0.0209958553314209, val loss: 0.07423772662878036\n",
      "Epoch 8472: train loss: 0.019193511456251144, val loss: 0.06046785041689873\n",
      "Epoch 8473: train loss: 0.017951801419258118, val loss: 0.11743054538965225\n",
      "Epoch 8474: train loss: 0.017962858080863953, val loss: 0.07601264119148254\n",
      "Epoch 8475: train loss: 0.014611608348786831, val loss: 0.09836592525243759\n",
      "Epoch 8476: train loss: 0.014156845398247242, val loss: 0.06659646332263947\n",
      "Epoch 8477: train loss: 0.01329575665295124, val loss: 0.06341774761676788\n",
      "Epoch 8478: train loss: 0.019028333947062492, val loss: 0.08198446780443192\n",
      "Epoch 8479: train loss: 0.01707100123167038, val loss: 0.08071555197238922\n",
      "Epoch 8480: train loss: 0.010624459013342857, val loss: 0.07435383647680283\n",
      "Epoch 8481: train loss: 0.013968905434012413, val loss: 0.06521370261907578\n",
      "Epoch 8482: train loss: 0.014064346440136433, val loss: 0.08293930441141129\n",
      "Epoch 8483: train loss: 0.02327999845147133, val loss: 0.10265462845563889\n",
      "Epoch 8484: train loss: 0.013848919421434402, val loss: 0.07813484966754913\n",
      "Epoch 8485: train loss: 0.01953410543501377, val loss: 0.06781252473592758\n",
      "Epoch 8486: train loss: 0.012436916120350361, val loss: 0.13501732051372528\n",
      "Epoch 8487: train loss: 0.021480899304151535, val loss: 0.06745949387550354\n",
      "Epoch 8488: train loss: 0.02018718421459198, val loss: 0.11901023238897324\n",
      "Epoch 8489: train loss: 0.014478662982583046, val loss: 0.06903865188360214\n",
      "Epoch 8490: train loss: 0.018911881372332573, val loss: 0.07570802420377731\n",
      "Epoch 8491: train loss: 0.01379782147705555, val loss: 0.047935690730810165\n",
      "Epoch 8492: train loss: 0.018728068098425865, val loss: 0.0823834165930748\n",
      "Epoch 8493: train loss: 0.01330011710524559, val loss: 0.0484251007437706\n",
      "Epoch 8494: train loss: 0.01558280922472477, val loss: 0.060860633850097656\n",
      "Epoch 8495: train loss: 0.01506005972623825, val loss: 0.045759014785289764\n",
      "Epoch 8496: train loss: 0.012541146948933601, val loss: 0.062381695955991745\n",
      "Epoch 8497: train loss: 0.013876667246222496, val loss: 0.07346552610397339\n",
      "Epoch 8498: train loss: 0.01431652344763279, val loss: 0.08580794185400009\n",
      "Epoch 8499: train loss: 0.016720818355679512, val loss: 0.07049346715211868\n",
      "Epoch 8500: train loss: 0.02100161649286747, val loss: 0.07241766899824142\n",
      "Epoch 8501: train loss: 0.014165312051773071, val loss: 0.06554795056581497\n",
      "Epoch 8502: train loss: 0.018023045733571053, val loss: 0.06576025485992432\n",
      "Epoch 8503: train loss: 0.014249657280743122, val loss: 0.05241822078824043\n",
      "Epoch 8504: train loss: 0.010489123873412609, val loss: 0.0813274160027504\n",
      "Epoch 8505: train loss: 0.023943917825818062, val loss: 0.08433369547128677\n",
      "Epoch 8506: train loss: 0.018411865457892418, val loss: 0.07443464547395706\n",
      "Epoch 8507: train loss: 0.015614273957908154, val loss: 0.07260894775390625\n",
      "Epoch 8508: train loss: 0.016968049108982086, val loss: 0.0410376638174057\n",
      "Epoch 8509: train loss: 0.018345648422837257, val loss: 0.08312543481588364\n",
      "Epoch 8510: train loss: 0.01358709391206503, val loss: 0.09072107821702957\n",
      "Epoch 8511: train loss: 0.012150459922850132, val loss: 0.08162202686071396\n",
      "Epoch 8512: train loss: 0.014148401096463203, val loss: 0.04820430651307106\n",
      "Epoch 8513: train loss: 0.019585255533456802, val loss: 0.05876707658171654\n",
      "Epoch 8514: train loss: 0.011723599396646023, val loss: 0.05966056138277054\n",
      "Epoch 8515: train loss: 0.018655160441994667, val loss: 0.12571251392364502\n",
      "Epoch 8516: train loss: 0.020431632176041603, val loss: 0.10811632126569748\n",
      "Epoch 8517: train loss: 0.023954972624778748, val loss: 0.0754082053899765\n",
      "Epoch 8518: train loss: 0.01826523244380951, val loss: 0.07157528400421143\n",
      "Epoch 8519: train loss: 0.016085321083664894, val loss: 0.03546414524316788\n",
      "Epoch 8520: train loss: 0.02111322619020939, val loss: 0.06877995282411575\n",
      "Epoch 8521: train loss: 0.02024267055094242, val loss: 0.07533033192157745\n",
      "Epoch 8522: train loss: 0.017675045877695084, val loss: 0.07251980155706406\n",
      "Epoch 8523: train loss: 0.012317480519413948, val loss: 0.04266471788287163\n",
      "Epoch 8524: train loss: 0.015207932330667973, val loss: 0.11079366505146027\n",
      "Epoch 8525: train loss: 0.01871757209300995, val loss: 0.08678779751062393\n",
      "Epoch 8526: train loss: 0.019660288468003273, val loss: 0.06484358757734299\n",
      "Epoch 8527: train loss: 0.01808847486972809, val loss: 0.038299620151519775\n",
      "Epoch 8528: train loss: 0.02204160764813423, val loss: 0.05909527465701103\n",
      "Epoch 8529: train loss: 0.01935569941997528, val loss: 0.051101088523864746\n",
      "Epoch 8530: train loss: 0.015276453457772732, val loss: 0.06984208524227142\n",
      "Epoch 8531: train loss: 0.016852732747793198, val loss: 0.06600135564804077\n",
      "Epoch 8532: train loss: 0.012072822079062462, val loss: 0.047568000853061676\n",
      "Epoch 8533: train loss: 0.0249332282692194, val loss: 0.08293293416500092\n",
      "Epoch 8534: train loss: 0.01603081077337265, val loss: 0.0742107629776001\n",
      "Epoch 8535: train loss: 0.015974927693605423, val loss: 0.07904251664876938\n",
      "Epoch 8536: train loss: 0.013834024779498577, val loss: 0.05833134055137634\n",
      "Epoch 8537: train loss: 0.013912378810346127, val loss: 0.06756439059972763\n",
      "Epoch 8538: train loss: 0.015552354045212269, val loss: 0.07957921177148819\n",
      "Epoch 8539: train loss: 0.009748740121722221, val loss: 0.12975895404815674\n",
      "Epoch 8540: train loss: 0.015709077939391136, val loss: 0.0632098838686943\n",
      "Epoch 8541: train loss: 0.014028063043951988, val loss: 0.05654723569750786\n",
      "Epoch 8542: train loss: 0.02491164207458496, val loss: 0.09384621679782867\n",
      "Epoch 8543: train loss: 0.01677463762462139, val loss: 0.0542021282017231\n",
      "Epoch 8544: train loss: 0.019671186804771423, val loss: 0.106177918612957\n",
      "Epoch 8545: train loss: 0.017060158774256706, val loss: 0.0631754919886589\n",
      "Epoch 8546: train loss: 0.019251471385359764, val loss: 0.11377940326929092\n",
      "Epoch 8547: train loss: 0.021286064758896828, val loss: 0.02867080643773079\n",
      "Epoch 8548: train loss: 0.014967052266001701, val loss: 0.05416589975357056\n",
      "Epoch 8549: train loss: 0.02246486209332943, val loss: 0.05644975230097771\n",
      "Epoch 8550: train loss: 0.013416489586234093, val loss: 0.08971021324396133\n",
      "Epoch 8551: train loss: 0.0158909410238266, val loss: 0.06309907883405685\n",
      "Epoch 8552: train loss: 0.017441973090171814, val loss: 0.09571914374828339\n",
      "Epoch 8553: train loss: 0.0170236025005579, val loss: 0.06011345982551575\n",
      "Epoch 8554: train loss: 0.015306232497096062, val loss: 0.051823437213897705\n",
      "Epoch 8555: train loss: 0.01365487091243267, val loss: 0.04052422568202019\n",
      "Epoch 8556: train loss: 0.01699601113796234, val loss: 0.06312999129295349\n",
      "Epoch 8557: train loss: 0.01485795434564352, val loss: 0.09832463413476944\n",
      "Epoch 8558: train loss: 0.013888725079596043, val loss: 0.0718618854880333\n",
      "Epoch 8559: train loss: 0.016147078946232796, val loss: 0.058278024196624756\n",
      "Epoch 8560: train loss: 0.01641940325498581, val loss: 0.07862547785043716\n",
      "Epoch 8561: train loss: 0.014226105995476246, val loss: 0.054531436413526535\n",
      "Epoch 8562: train loss: 0.013533597812056541, val loss: 0.08976878225803375\n",
      "Epoch 8563: train loss: 0.013334478251636028, val loss: 0.064167819917202\n",
      "Epoch 8564: train loss: 0.01899467036128044, val loss: 0.07331156730651855\n",
      "Epoch 8565: train loss: 0.01554632093757391, val loss: 0.06186866760253906\n",
      "Epoch 8566: train loss: 0.01726849190890789, val loss: 0.0740695595741272\n",
      "Epoch 8567: train loss: 0.01807108148932457, val loss: 0.09132007509469986\n",
      "Epoch 8568: train loss: 0.013982655480504036, val loss: 0.06950398534536362\n",
      "Epoch 8569: train loss: 0.01549509447067976, val loss: 0.10398154705762863\n",
      "Epoch 8570: train loss: 0.013530008494853973, val loss: 0.06466980278491974\n",
      "Epoch 8571: train loss: 0.018374036997556686, val loss: 0.07382702827453613\n",
      "Epoch 8572: train loss: 0.017833393067121506, val loss: 0.11736621707677841\n",
      "Epoch 8573: train loss: 0.018086930736899376, val loss: 0.0800129771232605\n",
      "Epoch 8574: train loss: 0.02526502124965191, val loss: 0.08974198251962662\n",
      "Epoch 8575: train loss: 0.016635611653327942, val loss: 0.058711644262075424\n",
      "Epoch 8576: train loss: 0.012948514893651009, val loss: 0.11762745678424835\n",
      "Epoch 8577: train loss: 0.017173761501908302, val loss: 0.08068963885307312\n",
      "Epoch 8578: train loss: 0.016855377703905106, val loss: 0.05148480087518692\n",
      "Epoch 8579: train loss: 0.019174309447407722, val loss: 0.07465273141860962\n",
      "Epoch 8580: train loss: 0.01336714718490839, val loss: 0.07903935015201569\n",
      "Epoch 8581: train loss: 0.024654706940054893, val loss: 0.056772131472826004\n",
      "Epoch 8582: train loss: 0.015127445571124554, val loss: 0.0919247418642044\n",
      "Epoch 8583: train loss: 0.019485728815197945, val loss: 0.11532270908355713\n",
      "Epoch 8584: train loss: 0.012746980413794518, val loss: 0.10448294132947922\n",
      "Epoch 8585: train loss: 0.01710093580186367, val loss: 0.08911111950874329\n",
      "Epoch 8586: train loss: 0.017678260803222656, val loss: 0.06082365661859512\n",
      "Epoch 8587: train loss: 0.017479155212640762, val loss: 0.05576680973172188\n",
      "Epoch 8588: train loss: 0.01269280444830656, val loss: 0.10865452140569687\n",
      "Epoch 8589: train loss: 0.01920691318809986, val loss: 0.07553712278604507\n",
      "Epoch 8590: train loss: 0.01698884926736355, val loss: 0.05728145316243172\n",
      "Epoch 8591: train loss: 0.012456674128770828, val loss: 0.07990540564060211\n",
      "Epoch 8592: train loss: 0.015580590814352036, val loss: 0.06339111924171448\n",
      "Epoch 8593: train loss: 0.012779423967003822, val loss: 0.05176524072885513\n",
      "Epoch 8594: train loss: 0.014265546575188637, val loss: 0.12314391136169434\n",
      "Epoch 8595: train loss: 0.014254970476031303, val loss: 0.09156065434217453\n",
      "Epoch 8596: train loss: 0.01578601822257042, val loss: 0.08680659532546997\n",
      "Epoch 8597: train loss: 0.01259648334234953, val loss: 0.05679149180650711\n",
      "Epoch 8598: train loss: 0.021621987223625183, val loss: 0.08870933204889297\n",
      "Epoch 8599: train loss: 0.019615843892097473, val loss: 0.08277159929275513\n",
      "Epoch 8600: train loss: 0.016705861315131187, val loss: 0.09309834241867065\n",
      "Epoch 8601: train loss: 0.01589825004339218, val loss: 0.08254014700651169\n",
      "Epoch 8602: train loss: 0.018815195187926292, val loss: 0.07364841550588608\n",
      "Epoch 8603: train loss: 0.012802770361304283, val loss: 0.09538263082504272\n",
      "Epoch 8604: train loss: 0.019102931022644043, val loss: 0.07074394822120667\n",
      "Epoch 8605: train loss: 0.013495988212525845, val loss: 0.0545559823513031\n",
      "Epoch 8606: train loss: 0.01665961556136608, val loss: 0.08933556079864502\n",
      "Epoch 8607: train loss: 0.01956600323319435, val loss: 0.08851300925016403\n",
      "Epoch 8608: train loss: 0.014471668750047684, val loss: 0.06744547933340073\n",
      "Epoch 8609: train loss: 0.014277996495366096, val loss: 0.07420426607131958\n",
      "Epoch 8610: train loss: 0.011555181816220284, val loss: 0.06098521873354912\n",
      "Epoch 8611: train loss: 0.018118228763341904, val loss: 0.070284903049469\n",
      "Epoch 8612: train loss: 0.014692007564008236, val loss: 0.07334879785776138\n",
      "Epoch 8613: train loss: 0.012493935413658619, val loss: 0.0649971142411232\n",
      "Epoch 8614: train loss: 0.020358875393867493, val loss: 0.08640890568494797\n",
      "Epoch 8615: train loss: 0.015021693892776966, val loss: 0.09293491393327713\n",
      "Epoch 8616: train loss: 0.012362250126898289, val loss: 0.06242351606488228\n",
      "Epoch 8617: train loss: 0.01382102444767952, val loss: 0.0669177994132042\n",
      "Epoch 8618: train loss: 0.021713312715291977, val loss: 0.09537787735462189\n",
      "Epoch 8619: train loss: 0.02046685479581356, val loss: 0.07348451018333435\n",
      "Epoch 8620: train loss: 0.017027730122208595, val loss: 0.10752588510513306\n",
      "Epoch 8621: train loss: 0.018933894112706184, val loss: 0.07139720767736435\n",
      "Epoch 8622: train loss: 0.01284682285040617, val loss: 0.0896105170249939\n",
      "Epoch 8623: train loss: 0.01539822481572628, val loss: 0.021094894036650658\n",
      "Epoch 8624: train loss: 0.015141687355935574, val loss: 0.03613823279738426\n",
      "Epoch 8625: train loss: 0.013091218657791615, val loss: 0.08980291336774826\n",
      "Epoch 8626: train loss: 0.013557791709899902, val loss: 0.05684014782309532\n",
      "Epoch 8627: train loss: 0.020186878740787506, val loss: 0.07098530977964401\n",
      "Epoch 8628: train loss: 0.016169792041182518, val loss: 0.06574685871601105\n",
      "Epoch 8629: train loss: 0.02340596541762352, val loss: 0.10187238454818726\n",
      "Epoch 8630: train loss: 0.021970689296722412, val loss: 0.06759319454431534\n",
      "Epoch 8631: train loss: 0.022115129977464676, val loss: 0.053538646548986435\n",
      "Epoch 8632: train loss: 0.017662551254034042, val loss: 0.05945051833987236\n",
      "Epoch 8633: train loss: 0.017365530133247375, val loss: 0.0460943765938282\n",
      "Epoch 8634: train loss: 0.012838457711040974, val loss: 0.0859997496008873\n",
      "Epoch 8635: train loss: 0.01755089871585369, val loss: 0.08549805730581284\n",
      "Epoch 8636: train loss: 0.014506584033370018, val loss: 0.09996256977319717\n",
      "Epoch 8637: train loss: 0.024361880496144295, val loss: 0.09217534214258194\n",
      "Epoch 8638: train loss: 0.018467329442501068, val loss: 0.06717291474342346\n",
      "Epoch 8639: train loss: 0.01694333367049694, val loss: 0.059632834047079086\n",
      "Epoch 8640: train loss: 0.01979200169444084, val loss: 0.06269114464521408\n",
      "Epoch 8641: train loss: 0.014721964485943317, val loss: 0.13053983449935913\n",
      "Epoch 8642: train loss: 0.01431241910904646, val loss: 0.0768694207072258\n",
      "Epoch 8643: train loss: 0.017140261828899384, val loss: 0.05532270297408104\n",
      "Epoch 8644: train loss: 0.01584305241703987, val loss: 0.040562309324741364\n",
      "Epoch 8645: train loss: 0.013262311927974224, val loss: 0.06966328620910645\n",
      "Epoch 8646: train loss: 0.013753834180533886, val loss: 0.06931215524673462\n",
      "Epoch 8647: train loss: 0.013150697574019432, val loss: 0.09667426347732544\n",
      "Epoch 8648: train loss: 0.014801536686718464, val loss: 0.09322773665189743\n",
      "Epoch 8649: train loss: 0.018655436113476753, val loss: 0.07840485870838165\n",
      "Epoch 8650: train loss: 0.017738640308380127, val loss: 0.07376793771982193\n",
      "Epoch 8651: train loss: 0.016419822350144386, val loss: 0.06069358065724373\n",
      "Epoch 8652: train loss: 0.0144412312656641, val loss: 0.050550319254398346\n",
      "Epoch 8653: train loss: 0.01641540229320526, val loss: 0.05500847101211548\n",
      "Epoch 8654: train loss: 0.017374441027641296, val loss: 0.08515754342079163\n",
      "Epoch 8655: train loss: 0.01919514872133732, val loss: 0.06441719084978104\n",
      "Epoch 8656: train loss: 0.015136086381971836, val loss: 0.10122119635343552\n",
      "Epoch 8657: train loss: 0.017277095466852188, val loss: 0.07859855890274048\n",
      "Epoch 8658: train loss: 0.01815187558531761, val loss: 0.02916383184492588\n",
      "Epoch 8659: train loss: 0.016686318442225456, val loss: 0.08338092267513275\n",
      "Epoch 8660: train loss: 0.01589922234416008, val loss: 0.07445108145475388\n",
      "Epoch 8661: train loss: 0.021674230694770813, val loss: 0.08209457248449326\n",
      "Epoch 8662: train loss: 0.014207961969077587, val loss: 0.09186597913503647\n",
      "Epoch 8663: train loss: 0.021140674129128456, val loss: 0.053438104689121246\n",
      "Epoch 8664: train loss: 0.019623026251792908, val loss: 0.08845114707946777\n",
      "Epoch 8665: train loss: 0.022410912439227104, val loss: 0.05063730478286743\n",
      "Epoch 8666: train loss: 0.015793772414326668, val loss: 0.08110981434583664\n",
      "Epoch 8667: train loss: 0.015702569857239723, val loss: 0.08316202461719513\n",
      "Epoch 8668: train loss: 0.015520673245191574, val loss: 0.05581745132803917\n",
      "Epoch 8669: train loss: 0.017631864175200462, val loss: 0.0762193575501442\n",
      "Epoch 8670: train loss: 0.015332406386733055, val loss: 0.08296401053667068\n",
      "Epoch 8671: train loss: 0.01979677937924862, val loss: 0.0905134305357933\n",
      "Epoch 8672: train loss: 0.017596865072846413, val loss: 0.09315183013677597\n",
      "Epoch 8673: train loss: 0.018114272505044937, val loss: 0.07633011788129807\n",
      "Epoch 8674: train loss: 0.020494546741247177, val loss: 0.08493592590093613\n",
      "Epoch 8675: train loss: 0.016576146706938744, val loss: 0.07652212679386139\n",
      "Epoch 8676: train loss: 0.01621486432850361, val loss: 0.07053076475858688\n",
      "Epoch 8677: train loss: 0.020215479657053947, val loss: 0.07602817565202713\n",
      "Epoch 8678: train loss: 0.010260778479278088, val loss: 0.09750004857778549\n",
      "Epoch 8679: train loss: 0.021830683574080467, val loss: 0.0786566287279129\n",
      "Epoch 8680: train loss: 0.013010650873184204, val loss: 0.05896307900547981\n",
      "Epoch 8681: train loss: 0.01439464557915926, val loss: 0.05811845138669014\n",
      "Epoch 8682: train loss: 0.016824841499328613, val loss: 0.0633392259478569\n",
      "Epoch 8683: train loss: 0.016062220558524132, val loss: 0.09374238550662994\n",
      "Epoch 8684: train loss: 0.01208656094968319, val loss: 0.08622296154499054\n",
      "Epoch 8685: train loss: 0.016121868044137955, val loss: 0.047925230115652084\n",
      "Epoch 8686: train loss: 0.019267212599515915, val loss: 0.05880779027938843\n",
      "Epoch 8687: train loss: 0.018865106627345085, val loss: 0.09375514835119247\n",
      "Epoch 8688: train loss: 0.012180744670331478, val loss: 0.06968700140714645\n",
      "Epoch 8689: train loss: 0.01993907243013382, val loss: 0.05051959306001663\n",
      "Epoch 8690: train loss: 0.01421719416975975, val loss: 0.06181817129254341\n",
      "Epoch 8691: train loss: 0.01646294817328453, val loss: 0.05730916187167168\n",
      "Epoch 8692: train loss: 0.021445607766509056, val loss: 0.0763951763510704\n",
      "Epoch 8693: train loss: 0.015689667314291, val loss: 0.10748511552810669\n",
      "Epoch 8694: train loss: 0.016236690804362297, val loss: 0.0897233784198761\n",
      "Epoch 8695: train loss: 0.01682622730731964, val loss: 0.04422030225396156\n",
      "Epoch 8696: train loss: 0.013710899278521538, val loss: 0.0596776008605957\n",
      "Epoch 8697: train loss: 0.022392621263861656, val loss: 0.059117045253515244\n",
      "Epoch 8698: train loss: 0.02048967219889164, val loss: 0.05548495054244995\n",
      "Epoch 8699: train loss: 0.012300156056880951, val loss: 0.09070060402154922\n",
      "Epoch 8700: train loss: 0.0190888624638319, val loss: 0.04857820272445679\n",
      "Epoch 8701: train loss: 0.011318570002913475, val loss: 0.07654302567243576\n",
      "Epoch 8702: train loss: 0.01647564396262169, val loss: 0.07567761093378067\n",
      "Epoch 8703: train loss: 0.022656399756669998, val loss: 0.0741138681769371\n",
      "Epoch 8704: train loss: 0.021863771602511406, val loss: 0.12335287779569626\n",
      "Epoch 8705: train loss: 0.01095301192253828, val loss: 0.05453348159790039\n",
      "Epoch 8706: train loss: 0.026309948414564133, val loss: 0.030576540157198906\n",
      "Epoch 8707: train loss: 0.02239067666232586, val loss: 0.08322802186012268\n",
      "Epoch 8708: train loss: 0.01621915027499199, val loss: 0.06837549805641174\n",
      "Epoch 8709: train loss: 0.015421324409544468, val loss: 0.08171107620000839\n",
      "Epoch 8710: train loss: 0.0163886621594429, val loss: 0.11677594482898712\n",
      "Epoch 8711: train loss: 0.019402261823415756, val loss: 0.07804832607507706\n",
      "Epoch 8712: train loss: 0.01596023514866829, val loss: 0.06350668519735336\n",
      "Epoch 8713: train loss: 0.020920393988490105, val loss: 0.10184865444898605\n",
      "Epoch 8714: train loss: 0.016893547028303146, val loss: 0.058028824627399445\n",
      "Epoch 8715: train loss: 0.019466154277324677, val loss: 0.06526114791631699\n",
      "Epoch 8716: train loss: 0.014842362143099308, val loss: 0.107677161693573\n",
      "Epoch 8717: train loss: 0.023676449432969093, val loss: 0.08783935755491257\n",
      "Epoch 8718: train loss: 0.016261745244264603, val loss: 0.0728486105799675\n",
      "Epoch 8719: train loss: 0.023254526779055595, val loss: 0.05392531678080559\n",
      "Epoch 8720: train loss: 0.017271218821406364, val loss: 0.08206924796104431\n",
      "Epoch 8721: train loss: 0.01769711636006832, val loss: 0.09835269302129745\n",
      "Epoch 8722: train loss: 0.01446896605193615, val loss: 0.07429569214582443\n",
      "Epoch 8723: train loss: 0.015174688771367073, val loss: 0.11260072141885757\n",
      "Epoch 8724: train loss: 0.016152625903487206, val loss: 0.09645290672779083\n",
      "Epoch 8725: train loss: 0.02267410047352314, val loss: 0.076004259288311\n",
      "Epoch 8726: train loss: 0.01295107789337635, val loss: 0.06707457453012466\n",
      "Epoch 8727: train loss: 0.020616859197616577, val loss: 0.06564955413341522\n",
      "Epoch 8728: train loss: 0.016501611098647118, val loss: 0.08519922196865082\n",
      "Epoch 8729: train loss: 0.012191410176455975, val loss: 0.0840192437171936\n",
      "Epoch 8730: train loss: 0.012416713871061802, val loss: 0.10708804428577423\n",
      "Epoch 8731: train loss: 0.024141952395439148, val loss: 0.1461888998746872\n",
      "Epoch 8732: train loss: 0.020736295729875565, val loss: 0.047783661633729935\n",
      "Epoch 8733: train loss: 0.01809530146420002, val loss: 0.05722874402999878\n",
      "Epoch 8734: train loss: 0.022262727841734886, val loss: 0.11025857925415039\n",
      "Epoch 8735: train loss: 0.016361936926841736, val loss: 0.06665926426649094\n",
      "Epoch 8736: train loss: 0.015445342287421227, val loss: 0.07109586894512177\n",
      "Epoch 8737: train loss: 0.012344669550657272, val loss: 0.059273626655340195\n",
      "Epoch 8738: train loss: 0.015757407993078232, val loss: 0.06889732927083969\n",
      "Epoch 8739: train loss: 0.014161497354507446, val loss: 0.05688267946243286\n",
      "Epoch 8740: train loss: 0.015048151835799217, val loss: 0.08161252737045288\n",
      "Epoch 8741: train loss: 0.018231065943837166, val loss: 0.044476307928562164\n",
      "Epoch 8742: train loss: 0.02107892744243145, val loss: 0.04475432634353638\n",
      "Epoch 8743: train loss: 0.024876873940229416, val loss: 0.03826520964503288\n",
      "Epoch 8744: train loss: 0.015219672583043575, val loss: 0.04773075133562088\n",
      "Epoch 8745: train loss: 0.012992510572075844, val loss: 0.08147932589054108\n",
      "Epoch 8746: train loss: 0.0214717797935009, val loss: 0.07615172863006592\n",
      "Epoch 8747: train loss: 0.01574454829096794, val loss: 0.059075839817523956\n",
      "Epoch 8748: train loss: 0.017145317047834396, val loss: 0.11262047290802002\n",
      "Epoch 8749: train loss: 0.0175430066883564, val loss: 0.08880455791950226\n",
      "Epoch 8750: train loss: 0.014862018637359142, val loss: 0.07936889678239822\n",
      "Epoch 8751: train loss: 0.01232161559164524, val loss: 0.07308582216501236\n",
      "Epoch 8752: train loss: 0.01608858071267605, val loss: 0.04275834187865257\n",
      "Epoch 8753: train loss: 0.019554724916815758, val loss: 0.04470602795481682\n",
      "Epoch 8754: train loss: 0.011281333863735199, val loss: 0.06015564128756523\n",
      "Epoch 8755: train loss: 0.021005135029554367, val loss: 0.04564371332526207\n",
      "Epoch 8756: train loss: 0.013921797275543213, val loss: 0.13058070838451385\n",
      "Epoch 8757: train loss: 0.01545388251543045, val loss: 0.10355295985937119\n",
      "Epoch 8758: train loss: 0.015588954091072083, val loss: 0.092436283826828\n",
      "Epoch 8759: train loss: 0.016237271949648857, val loss: 0.03665556386113167\n",
      "Epoch 8760: train loss: 0.016440024599432945, val loss: 0.09015075117349625\n",
      "Epoch 8761: train loss: 0.02079235389828682, val loss: 0.05157836154103279\n",
      "Epoch 8762: train loss: 0.022451331838965416, val loss: 0.07439465075731277\n",
      "Epoch 8763: train loss: 0.018372761085629463, val loss: 0.07748226076364517\n",
      "Epoch 8764: train loss: 0.02400045469403267, val loss: 0.07487870752811432\n",
      "Epoch 8765: train loss: 0.016270851716399193, val loss: 0.049459487199783325\n",
      "Epoch 8766: train loss: 0.010985537432134151, val loss: 0.04565220698714256\n",
      "Epoch 8767: train loss: 0.013884813524782658, val loss: 0.07330501079559326\n",
      "Epoch 8768: train loss: 0.013385873287916183, val loss: 0.10740838199853897\n",
      "Epoch 8769: train loss: 0.019895296543836594, val loss: 0.057357460260391235\n",
      "Epoch 8770: train loss: 0.02549375779926777, val loss: 0.0574047826230526\n",
      "Epoch 8771: train loss: 0.015783555805683136, val loss: 0.06071652099490166\n",
      "Epoch 8772: train loss: 0.02139287069439888, val loss: 0.09289616346359253\n",
      "Epoch 8773: train loss: 0.020735828205943108, val loss: 0.08139174431562424\n",
      "Epoch 8774: train loss: 0.016131989657878876, val loss: 0.04841092973947525\n",
      "Epoch 8775: train loss: 0.012123079970479012, val loss: 0.0684850886464119\n",
      "Epoch 8776: train loss: 0.017212193459272385, val loss: 0.062047798186540604\n",
      "Epoch 8777: train loss: 0.012955984100699425, val loss: 0.07672020047903061\n",
      "Epoch 8778: train loss: 0.015482389368116856, val loss: 0.04344230517745018\n",
      "Epoch 8779: train loss: 0.014850066974759102, val loss: 0.06362111866474152\n",
      "Epoch 8780: train loss: 0.012348070740699768, val loss: 0.09029683470726013\n",
      "Epoch 8781: train loss: 0.014138096012175083, val loss: 0.06851942837238312\n",
      "Epoch 8782: train loss: 0.01509697362780571, val loss: 0.0375358872115612\n",
      "Epoch 8783: train loss: 0.024115590378642082, val loss: 0.07046276330947876\n",
      "Epoch 8784: train loss: 0.01949906162917614, val loss: 0.11563491076231003\n",
      "Epoch 8785: train loss: 0.011445807293057442, val loss: 0.05995115265250206\n",
      "Epoch 8786: train loss: 0.01150857750326395, val loss: 0.08890397846698761\n",
      "Epoch 8787: train loss: 0.01779823936522007, val loss: 0.07223682850599289\n",
      "Epoch 8788: train loss: 0.017386585474014282, val loss: 0.10042925179004669\n",
      "Epoch 8789: train loss: 0.018261760473251343, val loss: 0.08593010902404785\n",
      "Epoch 8790: train loss: 0.015508368611335754, val loss: 0.07723817974328995\n",
      "Epoch 8791: train loss: 0.021918125450611115, val loss: 0.053326744586229324\n",
      "Epoch 8792: train loss: 0.019644856452941895, val loss: 0.09096421301364899\n",
      "Epoch 8793: train loss: 0.016764307394623756, val loss: 0.0734853520989418\n",
      "Epoch 8794: train loss: 0.01519034057855606, val loss: 0.0650593638420105\n",
      "Epoch 8795: train loss: 0.019291972741484642, val loss: 0.03906889632344246\n",
      "Epoch 8796: train loss: 0.013488716445863247, val loss: 0.03827483952045441\n",
      "Epoch 8797: train loss: 0.0191245935857296, val loss: 0.08128265291452408\n",
      "Epoch 8798: train loss: 0.017448581755161285, val loss: 0.06201500818133354\n",
      "Epoch 8799: train loss: 0.013502494432032108, val loss: 0.07546498626470566\n",
      "Epoch 8800: train loss: 0.018389811739325523, val loss: 0.09112071245908737\n",
      "Epoch 8801: train loss: 0.014629359357059002, val loss: 0.09320878982543945\n",
      "Epoch 8802: train loss: 0.012945265509188175, val loss: 0.05814547464251518\n",
      "Epoch 8803: train loss: 0.012907476164400578, val loss: 0.07341813296079636\n",
      "Epoch 8804: train loss: 0.013641956262290478, val loss: 0.06316152960062027\n",
      "Epoch 8805: train loss: 0.018498167395591736, val loss: 0.07363124936819077\n",
      "Epoch 8806: train loss: 0.013805096037685871, val loss: 0.06918416172266006\n",
      "Epoch 8807: train loss: 0.01650938205420971, val loss: 0.12905672192573547\n",
      "Epoch 8808: train loss: 0.016992513090372086, val loss: 0.09261026233434677\n",
      "Epoch 8809: train loss: 0.01409934926778078, val loss: 0.03410125523805618\n",
      "Epoch 8810: train loss: 0.013253305107355118, val loss: 0.05623713880777359\n",
      "Epoch 8811: train loss: 0.016368841752409935, val loss: 0.07882856577634811\n",
      "Epoch 8812: train loss: 0.02015724778175354, val loss: 0.0588175430893898\n",
      "Epoch 8813: train loss: 0.01685834676027298, val loss: 0.07978623360395432\n",
      "Epoch 8814: train loss: 0.018105078488588333, val loss: 0.07426237314939499\n",
      "Epoch 8815: train loss: 0.016791386529803276, val loss: 0.06276808679103851\n",
      "Epoch 8816: train loss: 0.011722037568688393, val loss: 0.0605405755341053\n",
      "Epoch 8817: train loss: 0.014524492435157299, val loss: 0.05768057331442833\n",
      "Epoch 8818: train loss: 0.012173284776508808, val loss: 0.06873166561126709\n",
      "Epoch 8819: train loss: 0.013348044827580452, val loss: 0.08971377462148666\n",
      "Epoch 8820: train loss: 0.015368419699370861, val loss: 0.10743504762649536\n",
      "Epoch 8821: train loss: 0.022068435326218605, val loss: 0.07883205264806747\n",
      "Epoch 8822: train loss: 0.016510022804141045, val loss: 0.0724770650267601\n",
      "Epoch 8823: train loss: 0.014815900474786758, val loss: 0.0688706561923027\n",
      "Epoch 8824: train loss: 0.016958680003881454, val loss: 0.09801526367664337\n",
      "Epoch 8825: train loss: 0.02002435177564621, val loss: 0.10036883503198624\n",
      "Epoch 8826: train loss: 0.017582383006811142, val loss: 0.048300087451934814\n",
      "Epoch 8827: train loss: 0.017837360501289368, val loss: 0.07238207012414932\n",
      "Epoch 8828: train loss: 0.020411456003785133, val loss: 0.0651792511343956\n",
      "Epoch 8829: train loss: 0.01798482984304428, val loss: 0.04592715576291084\n",
      "Epoch 8830: train loss: 0.023803595453500748, val loss: 0.08844500035047531\n",
      "Epoch 8831: train loss: 0.014038153924047947, val loss: 0.09793245047330856\n",
      "Epoch 8832: train loss: 0.022715462371706963, val loss: 0.05147933587431908\n",
      "Epoch 8833: train loss: 0.01089779008179903, val loss: 0.08867569267749786\n",
      "Epoch 8834: train loss: 0.018037699162960052, val loss: 0.05334492400288582\n",
      "Epoch 8835: train loss: 0.018793130293488503, val loss: 0.05785483121871948\n",
      "Epoch 8836: train loss: 0.01987200602889061, val loss: 0.06372244656085968\n",
      "Epoch 8837: train loss: 0.015213879756629467, val loss: 0.08990079164505005\n",
      "Epoch 8838: train loss: 0.0242107342928648, val loss: 0.08323715627193451\n",
      "Epoch 8839: train loss: 0.01889524981379509, val loss: 0.073466956615448\n",
      "Epoch 8840: train loss: 0.019933709874749184, val loss: 0.06608061492443085\n",
      "Epoch 8841: train loss: 0.024861734360456467, val loss: 0.07142698764801025\n",
      "Epoch 8842: train loss: 0.01954326219856739, val loss: 0.08815739303827286\n",
      "Epoch 8843: train loss: 0.01771840639412403, val loss: 0.0797639861702919\n",
      "Epoch 8844: train loss: 0.02005760185420513, val loss: 0.08677095174789429\n",
      "Epoch 8845: train loss: 0.01901755854487419, val loss: 0.07584031671285629\n",
      "Epoch 8846: train loss: 0.015298024751245975, val loss: 0.12148081511259079\n",
      "Epoch 8847: train loss: 0.023045890033245087, val loss: 0.10002366453409195\n",
      "Epoch 8848: train loss: 0.01562831923365593, val loss: 0.09736179560422897\n",
      "Epoch 8849: train loss: 0.016383007168769836, val loss: 0.07366704195737839\n",
      "Epoch 8850: train loss: 0.02168523520231247, val loss: 0.07875170558691025\n",
      "Epoch 8851: train loss: 0.020591383799910545, val loss: 0.0767207145690918\n",
      "Epoch 8852: train loss: 0.017977401614189148, val loss: 0.05321161821484566\n",
      "Epoch 8853: train loss: 0.021065564826130867, val loss: 0.0889345109462738\n",
      "Epoch 8854: train loss: 0.012727971188724041, val loss: 0.06632224470376968\n",
      "Epoch 8855: train loss: 0.02517607808113098, val loss: 0.06522069871425629\n",
      "Epoch 8856: train loss: 0.01379349548369646, val loss: 0.11089437454938889\n",
      "Epoch 8857: train loss: 0.014565208926796913, val loss: 0.07513895630836487\n",
      "Epoch 8858: train loss: 0.01585492305457592, val loss: 0.06970132887363434\n",
      "Epoch 8859: train loss: 0.018840668722987175, val loss: 0.06627754122018814\n",
      "Epoch 8860: train loss: 0.01874963939189911, val loss: 0.049132563173770905\n",
      "Epoch 8861: train loss: 0.01637260988354683, val loss: 0.07203768938779831\n",
      "Epoch 8862: train loss: 0.019690725952386856, val loss: 0.07450418174266815\n",
      "Epoch 8863: train loss: 0.015955328941345215, val loss: 0.07272565364837646\n",
      "Epoch 8864: train loss: 0.01405912172049284, val loss: 0.13765446841716766\n",
      "Epoch 8865: train loss: 0.013522540219128132, val loss: 0.0766620934009552\n",
      "Epoch 8866: train loss: 0.01926235854625702, val loss: 0.05268440395593643\n",
      "Epoch 8867: train loss: 0.017821725457906723, val loss: 0.10925694555044174\n",
      "Epoch 8868: train loss: 0.01686633564531803, val loss: 0.033287595957517624\n",
      "Epoch 8869: train loss: 0.018842773512005806, val loss: 0.05609450489282608\n",
      "Epoch 8870: train loss: 0.01655428670346737, val loss: 0.06577575206756592\n",
      "Epoch 8871: train loss: 0.022791121155023575, val loss: 0.04773738980293274\n",
      "Epoch 8872: train loss: 0.02241441421210766, val loss: 0.07068037986755371\n",
      "Epoch 8873: train loss: 0.016576383262872696, val loss: 0.11610080301761627\n",
      "Epoch 8874: train loss: 0.020809398964047432, val loss: 0.07659092545509338\n",
      "Epoch 8875: train loss: 0.02102009765803814, val loss: 0.07776039093732834\n",
      "Epoch 8876: train loss: 0.012269891798496246, val loss: 0.07517935335636139\n",
      "Epoch 8877: train loss: 0.013866104185581207, val loss: 0.08077025413513184\n",
      "Epoch 8878: train loss: 0.02340175397694111, val loss: 0.07049072533845901\n",
      "Epoch 8879: train loss: 0.0165342278778553, val loss: 0.07717441767454147\n",
      "Epoch 8880: train loss: 0.018671663478016853, val loss: 0.06493156403303146\n",
      "Epoch 8881: train loss: 0.01568007282912731, val loss: 0.05616959556937218\n",
      "Epoch 8882: train loss: 0.012069707736372948, val loss: 0.048322033137083054\n",
      "Epoch 8883: train loss: 0.014482498168945312, val loss: 0.04431177303195\n",
      "Epoch 8884: train loss: 0.020182892680168152, val loss: 0.06955504417419434\n",
      "Epoch 8885: train loss: 0.018681902438402176, val loss: 0.07741276174783707\n",
      "Epoch 8886: train loss: 0.016814863309264183, val loss: 0.07625823467969894\n",
      "Epoch 8887: train loss: 0.013158568181097507, val loss: 0.08678507804870605\n",
      "Epoch 8888: train loss: 0.018140623345971107, val loss: 0.10178282111883163\n",
      "Epoch 8889: train loss: 0.01809714548289776, val loss: 0.06244206055998802\n",
      "Epoch 8890: train loss: 0.018049024045467377, val loss: 0.0872337743639946\n",
      "Epoch 8891: train loss: 0.01751401647925377, val loss: 0.06315922737121582\n",
      "Epoch 8892: train loss: 0.017222918570041656, val loss: 0.07007259875535965\n",
      "Epoch 8893: train loss: 0.018069131299853325, val loss: 0.06002159044146538\n",
      "Epoch 8894: train loss: 0.015337446704506874, val loss: 0.1376960277557373\n",
      "Epoch 8895: train loss: 0.014419078826904297, val loss: 0.07419172674417496\n",
      "Epoch 8896: train loss: 0.020185498520731926, val loss: 0.06687559187412262\n",
      "Epoch 8897: train loss: 0.012960202060639858, val loss: 0.11289858818054199\n",
      "Epoch 8898: train loss: 0.012807901948690414, val loss: 0.061653561890125275\n",
      "Epoch 8899: train loss: 0.016285831108689308, val loss: 0.074252188205719\n",
      "Epoch 8900: train loss: 0.019856002181768417, val loss: 0.10415960848331451\n",
      "Epoch 8901: train loss: 0.02213011123239994, val loss: 0.1277255266904831\n",
      "Epoch 8902: train loss: 0.023561948910355568, val loss: 0.07087326049804688\n",
      "Epoch 8903: train loss: 0.020771313458681107, val loss: 0.06728927046060562\n",
      "Epoch 8904: train loss: 0.0186153594404459, val loss: 0.07412602752447128\n",
      "Epoch 8905: train loss: 0.01874702237546444, val loss: 0.08609267324209213\n",
      "Epoch 8906: train loss: 0.020678764209151268, val loss: 0.05849626660346985\n",
      "Epoch 8907: train loss: 0.019160911440849304, val loss: 0.08281292021274567\n",
      "Epoch 8908: train loss: 0.021156253293156624, val loss: 0.05087745189666748\n",
      "Epoch 8909: train loss: 0.019274985417723656, val loss: 0.06953268498182297\n",
      "Epoch 8910: train loss: 0.017901388928294182, val loss: 0.039783742278814316\n",
      "Epoch 8911: train loss: 0.019368259236216545, val loss: 0.06307140737771988\n",
      "Epoch 8912: train loss: 0.020248087123036385, val loss: 0.09851446002721786\n",
      "Epoch 8913: train loss: 0.01681143045425415, val loss: 0.06322488933801651\n",
      "Epoch 8914: train loss: 0.014902607537806034, val loss: 0.061470936983823776\n",
      "Epoch 8915: train loss: 0.019976571202278137, val loss: 0.07899955660104752\n",
      "Epoch 8916: train loss: 0.02056157775223255, val loss: 0.08013267070055008\n",
      "Epoch 8917: train loss: 0.016834918409585953, val loss: 0.06513070315122604\n",
      "Epoch 8918: train loss: 0.02001551166176796, val loss: 0.061761803925037384\n",
      "Epoch 8919: train loss: 0.013371921144425869, val loss: 0.07757300138473511\n",
      "Epoch 8920: train loss: 0.016174137592315674, val loss: 0.060932010412216187\n",
      "Epoch 8921: train loss: 0.015008433721959591, val loss: 0.04834514483809471\n",
      "Epoch 8922: train loss: 0.017328284680843353, val loss: 0.05668753385543823\n",
      "Epoch 8923: train loss: 0.021948235109448433, val loss: 0.0633007362484932\n",
      "Epoch 8924: train loss: 0.015048795379698277, val loss: 0.06419380754232407\n",
      "Epoch 8925: train loss: 0.02367420867085457, val loss: 0.05167609453201294\n",
      "Epoch 8926: train loss: 0.017539018765091896, val loss: 0.06645753979682922\n",
      "Epoch 8927: train loss: 0.019732095301151276, val loss: 0.05062751844525337\n",
      "Epoch 8928: train loss: 0.019192539155483246, val loss: 0.09873887151479721\n",
      "Epoch 8929: train loss: 0.011196674779057503, val loss: 0.1225796490907669\n",
      "Epoch 8930: train loss: 0.019534144550561905, val loss: 0.08453879505395889\n",
      "Epoch 8931: train loss: 0.011681241914629936, val loss: 0.06322769075632095\n",
      "Epoch 8932: train loss: 0.01708250679075718, val loss: 0.09180622547864914\n",
      "Epoch 8933: train loss: 0.01609623245894909, val loss: 0.0809364914894104\n",
      "Epoch 8934: train loss: 0.018183890730142593, val loss: 0.10663094371557236\n",
      "Epoch 8935: train loss: 0.015303779393434525, val loss: 0.0754077211022377\n",
      "Epoch 8936: train loss: 0.022070743143558502, val loss: 0.09240197390317917\n",
      "Epoch 8937: train loss: 0.01481574960052967, val loss: 0.07647264003753662\n",
      "Epoch 8938: train loss: 0.013837997801601887, val loss: 0.05523666739463806\n",
      "Epoch 8939: train loss: 0.017787456512451172, val loss: 0.08279430121183395\n",
      "Epoch 8940: train loss: 0.014781310223042965, val loss: 0.08123254030942917\n",
      "Epoch 8941: train loss: 0.01895896904170513, val loss: 0.09567085653543472\n",
      "Epoch 8942: train loss: 0.014675237238407135, val loss: 0.06905144453048706\n",
      "Epoch 8943: train loss: 0.01224526297301054, val loss: 0.10981016606092453\n",
      "Epoch 8944: train loss: 0.01379705872386694, val loss: 0.10991568863391876\n",
      "Epoch 8945: train loss: 0.018071185797452927, val loss: 0.09321150183677673\n",
      "Epoch 8946: train loss: 0.019529324024915695, val loss: 0.08734793215990067\n",
      "Epoch 8947: train loss: 0.014641468413174152, val loss: 0.09043274074792862\n",
      "Epoch 8948: train loss: 0.011976725421845913, val loss: 0.04708150774240494\n",
      "Epoch 8949: train loss: 0.015540639869868755, val loss: 0.06094030290842056\n",
      "Epoch 8950: train loss: 0.016793621703982353, val loss: 0.06938191503286362\n",
      "Epoch 8951: train loss: 0.022854646667838097, val loss: 0.07525988668203354\n",
      "Epoch 8952: train loss: 0.01606891117990017, val loss: 0.07768130302429199\n",
      "Epoch 8953: train loss: 0.016365164890885353, val loss: 0.10819373279809952\n",
      "Epoch 8954: train loss: 0.017331311479210854, val loss: 0.07270976155996323\n",
      "Epoch 8955: train loss: 0.017714811488986015, val loss: 0.06923913210630417\n",
      "Epoch 8956: train loss: 0.022714681923389435, val loss: 0.059504300355911255\n",
      "Epoch 8957: train loss: 0.015013741329312325, val loss: 0.07397057116031647\n",
      "Epoch 8958: train loss: 0.013329767622053623, val loss: 0.11394789069890976\n",
      "Epoch 8959: train loss: 0.016415797173976898, val loss: 0.04869425296783447\n",
      "Epoch 8960: train loss: 0.011004849337041378, val loss: 0.07335766404867172\n",
      "Epoch 8961: train loss: 0.012106952257454395, val loss: 0.08299104869365692\n",
      "Epoch 8962: train loss: 0.012469183653593063, val loss: 0.10192640125751495\n",
      "Epoch 8963: train loss: 0.015414048917591572, val loss: 0.08066777884960175\n",
      "Epoch 8964: train loss: 0.018263958394527435, val loss: 0.07312043756246567\n",
      "Epoch 8965: train loss: 0.01730644516646862, val loss: 0.06369286775588989\n",
      "Epoch 8966: train loss: 0.019521113485097885, val loss: 0.07534167915582657\n",
      "Epoch 8967: train loss: 0.01260716188699007, val loss: 0.06204381212592125\n",
      "Epoch 8968: train loss: 0.014085022732615471, val loss: 0.07868006080389023\n",
      "Epoch 8969: train loss: 0.017940528690814972, val loss: 0.04706525057554245\n",
      "Epoch 8970: train loss: 0.013680307194590569, val loss: 0.0710713192820549\n",
      "Epoch 8971: train loss: 0.01723766326904297, val loss: 0.04792013764381409\n",
      "Epoch 8972: train loss: 0.015104694291949272, val loss: 0.08010218292474747\n",
      "Epoch 8973: train loss: 0.015789227560162544, val loss: 0.07291971147060394\n",
      "Epoch 8974: train loss: 0.014426522888243198, val loss: 0.05571776255965233\n",
      "Epoch 8975: train loss: 0.01340785063803196, val loss: 0.05591008812189102\n",
      "Epoch 8976: train loss: 0.017656216397881508, val loss: 0.09777390211820602\n",
      "Epoch 8977: train loss: 0.01897394098341465, val loss: 0.06647930294275284\n",
      "Epoch 8978: train loss: 0.016183795407414436, val loss: 0.08103891462087631\n",
      "Epoch 8979: train loss: 0.019715366885066032, val loss: 0.08512488007545471\n",
      "Epoch 8980: train loss: 0.013632685877382755, val loss: 0.10951085388660431\n",
      "Epoch 8981: train loss: 0.018841754645109177, val loss: 0.07646550983190536\n",
      "Epoch 8982: train loss: 0.02082683891057968, val loss: 0.0863746777176857\n",
      "Epoch 8983: train loss: 0.014076866209506989, val loss: 0.08498939126729965\n",
      "Epoch 8984: train loss: 0.015494748018682003, val loss: 0.07935026288032532\n",
      "Epoch 8985: train loss: 0.014784262515604496, val loss: 0.06619459390640259\n",
      "Epoch 8986: train loss: 0.01245639193803072, val loss: 0.09230992943048477\n",
      "Epoch 8987: train loss: 0.01812685653567314, val loss: 0.05036715418100357\n",
      "Epoch 8988: train loss: 0.01162467710673809, val loss: 0.05852462723851204\n",
      "Epoch 8989: train loss: 0.01492361817508936, val loss: 0.08012083917856216\n",
      "Epoch 8990: train loss: 0.014737400226294994, val loss: 0.09434974938631058\n",
      "Epoch 8991: train loss: 0.015461096540093422, val loss: 0.10287468880414963\n",
      "Epoch 8992: train loss: 0.01297234371304512, val loss: 0.07576413452625275\n",
      "Epoch 8993: train loss: 0.014506117440760136, val loss: 0.07744304835796356\n",
      "Epoch 8994: train loss: 0.013725346885621548, val loss: 0.07361804693937302\n",
      "Epoch 8995: train loss: 0.01469436101615429, val loss: 0.06948092579841614\n",
      "Epoch 8996: train loss: 0.015143842436373234, val loss: 0.08386792987585068\n",
      "Epoch 8997: train loss: 0.015516141429543495, val loss: 0.1264437884092331\n",
      "Epoch 8998: train loss: 0.014195302501320839, val loss: 0.056829214096069336\n",
      "Epoch 8999: train loss: 0.01970268227159977, val loss: 0.06321891397237778\n",
      "Epoch 9000: train loss: 0.020566297695040703, val loss: 0.0915575921535492\n",
      "Epoch 9001: train loss: 0.016082381829619408, val loss: 0.0348895862698555\n",
      "Epoch 9002: train loss: 0.01709875278174877, val loss: 0.04339324310421944\n",
      "Epoch 9003: train loss: 0.013867901638150215, val loss: 0.10615210980176926\n",
      "Epoch 9004: train loss: 0.015389262698590755, val loss: 0.08139605075120926\n",
      "Epoch 9005: train loss: 0.014703483320772648, val loss: 0.06129303574562073\n",
      "Epoch 9006: train loss: 0.017750397324562073, val loss: 0.0860307514667511\n",
      "Epoch 9007: train loss: 0.013419163413345814, val loss: 0.0743660032749176\n",
      "Epoch 9008: train loss: 0.009473741985857487, val loss: 0.07945761829614639\n",
      "Epoch 9009: train loss: 0.015414683148264885, val loss: 0.08665957301855087\n",
      "Epoch 9010: train loss: 0.020573362708091736, val loss: 0.06438006460666656\n",
      "Epoch 9011: train loss: 0.013878480531275272, val loss: 0.10268162935972214\n",
      "Epoch 9012: train loss: 0.013315842486917973, val loss: 0.06932276487350464\n",
      "Epoch 9013: train loss: 0.011432716622948647, val loss: 0.044329751282930374\n",
      "Epoch 9014: train loss: 0.012285585515201092, val loss: 0.06768367439508438\n",
      "Epoch 9015: train loss: 0.018061023205518723, val loss: 0.05192684009671211\n",
      "Epoch 9016: train loss: 0.012899747118353844, val loss: 0.09018983691930771\n",
      "Epoch 9017: train loss: 0.023027023300528526, val loss: 0.07142530381679535\n",
      "Epoch 9018: train loss: 0.011702725663781166, val loss: 0.08588535338640213\n",
      "Epoch 9019: train loss: 0.010621377266943455, val loss: 0.09374941885471344\n",
      "Epoch 9020: train loss: 0.013372273184359074, val loss: 0.04053037241101265\n",
      "Epoch 9021: train loss: 0.015335261821746826, val loss: 0.06466244906187057\n",
      "Epoch 9022: train loss: 0.015218442305922508, val loss: 0.06417873501777649\n",
      "Epoch 9023: train loss: 0.013244465924799442, val loss: 0.08226696401834488\n",
      "Epoch 9024: train loss: 0.012288854457437992, val loss: 0.08260404318571091\n",
      "Epoch 9025: train loss: 0.020797234028577805, val loss: 0.11076205223798752\n",
      "Epoch 9026: train loss: 0.014155364595353603, val loss: 0.0831555724143982\n",
      "Epoch 9027: train loss: 0.01239316537976265, val loss: 0.10645077377557755\n",
      "Epoch 9028: train loss: 0.02115708589553833, val loss: 0.06816806644201279\n",
      "Epoch 9029: train loss: 0.013620996847748756, val loss: 0.10440752655267715\n",
      "Epoch 9030: train loss: 0.014185270294547081, val loss: 0.10321978479623795\n",
      "Epoch 9031: train loss: 0.020112385973334312, val loss: 0.057786788791418076\n",
      "Epoch 9032: train loss: 0.013294382020831108, val loss: 0.0849861279129982\n",
      "Epoch 9033: train loss: 0.017157915979623795, val loss: 0.06573474407196045\n",
      "Epoch 9034: train loss: 0.019139666110277176, val loss: 0.051364488899707794\n",
      "Epoch 9035: train loss: 0.01705934666097164, val loss: 0.08941458910703659\n",
      "Epoch 9036: train loss: 0.010351818054914474, val loss: 0.07552896440029144\n",
      "Epoch 9037: train loss: 0.013818861916661263, val loss: 0.07166557759046555\n",
      "Epoch 9038: train loss: 0.01953829638659954, val loss: 0.05399603769183159\n",
      "Epoch 9039: train loss: 0.016889728605747223, val loss: 0.05668346956372261\n",
      "Epoch 9040: train loss: 0.011858816258609295, val loss: 0.07332640886306763\n",
      "Epoch 9041: train loss: 0.013329626992344856, val loss: 0.04089922457933426\n",
      "Epoch 9042: train loss: 0.01780628226697445, val loss: 0.08332574367523193\n",
      "Epoch 9043: train loss: 0.010985340923070908, val loss: 0.053327299654483795\n",
      "Epoch 9044: train loss: 0.01789015531539917, val loss: 0.06022964045405388\n",
      "Epoch 9045: train loss: 0.023292049765586853, val loss: 0.06697552651166916\n",
      "Epoch 9046: train loss: 0.012925244867801666, val loss: 0.04976532608270645\n",
      "Epoch 9047: train loss: 0.02047259919345379, val loss: 0.09926672279834747\n",
      "Epoch 9048: train loss: 0.01669463701546192, val loss: 0.08887755125761032\n",
      "Epoch 9049: train loss: 0.0164145790040493, val loss: 0.07970302551984787\n",
      "Epoch 9050: train loss: 0.01321861520409584, val loss: 0.0719737857580185\n",
      "Epoch 9051: train loss: 0.016684239730238914, val loss: 0.060384273529052734\n",
      "Epoch 9052: train loss: 0.01622028462588787, val loss: 0.0956098735332489\n",
      "Epoch 9053: train loss: 0.014915402047336102, val loss: 0.038328371942043304\n",
      "Epoch 9054: train loss: 0.017295904457569122, val loss: 0.07688096910715103\n",
      "Epoch 9055: train loss: 0.01492649968713522, val loss: 0.10270165652036667\n",
      "Epoch 9056: train loss: 0.02019345946609974, val loss: 0.08357897400856018\n",
      "Epoch 9057: train loss: 0.022040914744138718, val loss: 0.10856696218252182\n",
      "Epoch 9058: train loss: 0.021409129723906517, val loss: 0.06085623428225517\n",
      "Epoch 9059: train loss: 0.015890035778284073, val loss: 0.09038057178258896\n",
      "Epoch 9060: train loss: 0.01511098351329565, val loss: 0.09358815103769302\n",
      "Epoch 9061: train loss: 0.019431274384260178, val loss: 0.07175477594137192\n",
      "Epoch 9062: train loss: 0.01769269071519375, val loss: 0.07326498627662659\n",
      "Epoch 9063: train loss: 0.017626160755753517, val loss: 0.051234591752290726\n",
      "Epoch 9064: train loss: 0.013795650564134121, val loss: 0.07668258994817734\n",
      "Epoch 9065: train loss: 0.013357572257518768, val loss: 0.0458064079284668\n",
      "Epoch 9066: train loss: 0.011653225868940353, val loss: 0.06022884324193001\n",
      "Epoch 9067: train loss: 0.012390455231070518, val loss: 0.09885019063949585\n",
      "Epoch 9068: train loss: 0.019101381301879883, val loss: 0.04800930991768837\n",
      "Epoch 9069: train loss: 0.017069056630134583, val loss: 0.049352310597896576\n",
      "Epoch 9070: train loss: 0.020398057997226715, val loss: 0.06369643658399582\n",
      "Epoch 9071: train loss: 0.018971983343362808, val loss: 0.06823035329580307\n",
      "Epoch 9072: train loss: 0.015502951107919216, val loss: 0.06180768087506294\n",
      "Epoch 9073: train loss: 0.012687359936535358, val loss: 0.06710613518953323\n",
      "Epoch 9074: train loss: 0.013993721455335617, val loss: 0.06977164000272751\n",
      "Epoch 9075: train loss: 0.013220001943409443, val loss: 0.06427527219057083\n",
      "Epoch 9076: train loss: 0.015871785581111908, val loss: 0.056986670941114426\n",
      "Epoch 9077: train loss: 0.013956527225673199, val loss: 0.07922225445508957\n",
      "Epoch 9078: train loss: 0.018670704215765, val loss: 0.08259113878011703\n",
      "Epoch 9079: train loss: 0.016977228224277496, val loss: 0.09068717807531357\n",
      "Epoch 9080: train loss: 0.015510047785937786, val loss: 0.06689896434545517\n",
      "Epoch 9081: train loss: 0.022802384570240974, val loss: 0.11123076826334\n",
      "Epoch 9082: train loss: 0.017455944791436195, val loss: 0.044432610273361206\n",
      "Epoch 9083: train loss: 0.016967395320534706, val loss: 0.10452919453382492\n",
      "Epoch 9084: train loss: 0.01855325512588024, val loss: 0.08928302675485611\n",
      "Epoch 9085: train loss: 0.0177488774061203, val loss: 0.050982970744371414\n",
      "Epoch 9086: train loss: 0.018220650032162666, val loss: 0.06137656792998314\n",
      "Epoch 9087: train loss: 0.015063843689858913, val loss: 0.057502519339323044\n",
      "Epoch 9088: train loss: 0.013198930770158768, val loss: 0.05782036855816841\n",
      "Epoch 9089: train loss: 0.019896533340215683, val loss: 0.07804634422063828\n",
      "Epoch 9090: train loss: 0.019188741222023964, val loss: 0.07891516387462616\n",
      "Epoch 9091: train loss: 0.013567214831709862, val loss: 0.058538276702165604\n",
      "Epoch 9092: train loss: 0.01268712431192398, val loss: 0.053272511810064316\n",
      "Epoch 9093: train loss: 0.01836235634982586, val loss: 0.04301878809928894\n",
      "Epoch 9094: train loss: 0.014455597847700119, val loss: 0.05005446821451187\n",
      "Epoch 9095: train loss: 0.014964611269533634, val loss: 0.10140203684568405\n",
      "Epoch 9096: train loss: 0.01713750883936882, val loss: 0.05816338211297989\n",
      "Epoch 9097: train loss: 0.01232923660427332, val loss: 0.06987384706735611\n",
      "Epoch 9098: train loss: 0.016265610232949257, val loss: 0.07759547233581543\n",
      "Epoch 9099: train loss: 0.015089953318238258, val loss: 0.09830784797668457\n",
      "Epoch 9100: train loss: 0.021031979471445084, val loss: 0.09556844085454941\n",
      "Epoch 9101: train loss: 0.017086924985051155, val loss: 0.09133471548557281\n",
      "Epoch 9102: train loss: 0.02090313285589218, val loss: 0.056954432278871536\n",
      "Epoch 9103: train loss: 0.016641968861222267, val loss: 0.09051573276519775\n",
      "Epoch 9104: train loss: 0.014362440444529057, val loss: 0.04861893877387047\n",
      "Epoch 9105: train loss: 0.01141337864100933, val loss: 0.04873649403452873\n",
      "Epoch 9106: train loss: 0.015929540619254112, val loss: 0.06744255870580673\n",
      "Epoch 9107: train loss: 0.015583510510623455, val loss: 0.10200033336877823\n",
      "Epoch 9108: train loss: 0.011496308259665966, val loss: 0.06975574791431427\n",
      "Epoch 9109: train loss: 0.014402972534298897, val loss: 0.07261237502098083\n",
      "Epoch 9110: train loss: 0.015379685908555984, val loss: 0.06053083762526512\n",
      "Epoch 9111: train loss: 0.012797599658370018, val loss: 0.06994054466485977\n",
      "Epoch 9112: train loss: 0.015140348114073277, val loss: 0.06136121228337288\n",
      "Epoch 9113: train loss: 0.021058369427919388, val loss: 0.053670670837163925\n",
      "Epoch 9114: train loss: 0.017210694029927254, val loss: 0.06975005567073822\n",
      "Epoch 9115: train loss: 0.015757717192173004, val loss: 0.07795330137014389\n",
      "Epoch 9116: train loss: 0.015984201803803444, val loss: 0.03547714278101921\n",
      "Epoch 9117: train loss: 0.022649044170975685, val loss: 0.07477430254220963\n",
      "Epoch 9118: train loss: 0.019383501261472702, val loss: 0.1259143203496933\n",
      "Epoch 9119: train loss: 0.021263480186462402, val loss: 0.047031328082084656\n",
      "Epoch 9120: train loss: 0.01844978705048561, val loss: 0.10648436844348907\n",
      "Epoch 9121: train loss: 0.01723325066268444, val loss: 0.06294447183609009\n",
      "Epoch 9122: train loss: 0.01647598296403885, val loss: 0.05414275452494621\n",
      "Epoch 9123: train loss: 0.016192205250263214, val loss: 0.1171136423945427\n",
      "Epoch 9124: train loss: 0.015091336332261562, val loss: 0.09340120851993561\n",
      "Epoch 9125: train loss: 0.013927952386438847, val loss: 0.052018146961927414\n",
      "Epoch 9126: train loss: 0.01734462007880211, val loss: 0.04139550402760506\n",
      "Epoch 9127: train loss: 0.01205017976462841, val loss: 0.07669614255428314\n",
      "Epoch 9128: train loss: 0.01432226411998272, val loss: 0.05447004362940788\n",
      "Epoch 9129: train loss: 0.014670812524855137, val loss: 0.09839162975549698\n",
      "Epoch 9130: train loss: 0.01135814469307661, val loss: 0.06075255200266838\n",
      "Epoch 9131: train loss: 0.014356245286762714, val loss: 0.10718512535095215\n",
      "Epoch 9132: train loss: 0.012611058540642262, val loss: 0.09844554960727692\n",
      "Epoch 9133: train loss: 0.01746281608939171, val loss: 0.09469874948263168\n",
      "Epoch 9134: train loss: 0.018566681072115898, val loss: 0.09164690226316452\n",
      "Epoch 9135: train loss: 0.014075658284127712, val loss: 0.04654116556048393\n",
      "Epoch 9136: train loss: 0.012975080870091915, val loss: 0.09213898330926895\n",
      "Epoch 9137: train loss: 0.01532263495028019, val loss: 0.0662635937333107\n",
      "Epoch 9138: train loss: 0.01566377840936184, val loss: 0.06651430577039719\n",
      "Epoch 9139: train loss: 0.018906787037849426, val loss: 0.07182805985212326\n",
      "Epoch 9140: train loss: 0.012719505466520786, val loss: 0.04622403532266617\n",
      "Epoch 9141: train loss: 0.013927104882895947, val loss: 0.04176001250743866\n",
      "Epoch 9142: train loss: 0.02266637049615383, val loss: 0.03509452939033508\n",
      "Epoch 9143: train loss: 0.02207253687083721, val loss: 0.06141453981399536\n",
      "Epoch 9144: train loss: 0.015589065849781036, val loss: 0.09956558793783188\n",
      "Epoch 9145: train loss: 0.018664956092834473, val loss: 0.09697612375020981\n",
      "Epoch 9146: train loss: 0.013789445161819458, val loss: 0.06760367751121521\n",
      "Epoch 9147: train loss: 0.013668594881892204, val loss: 0.03422725945711136\n",
      "Epoch 9148: train loss: 0.017305104061961174, val loss: 0.09249573200941086\n",
      "Epoch 9149: train loss: 0.021195756271481514, val loss: 0.05214213952422142\n",
      "Epoch 9150: train loss: 0.011905232444405556, val loss: 0.06257198005914688\n",
      "Epoch 9151: train loss: 0.021650005131959915, val loss: 0.05378425121307373\n",
      "Epoch 9152: train loss: 0.014827386476099491, val loss: 0.08139883726835251\n",
      "Epoch 9153: train loss: 0.011380329728126526, val loss: 0.08815816789865494\n",
      "Epoch 9154: train loss: 0.01619766280055046, val loss: 0.055274318903684616\n",
      "Epoch 9155: train loss: 0.015381100587546825, val loss: 0.04985139146447182\n",
      "Epoch 9156: train loss: 0.019721567630767822, val loss: 0.06030458211898804\n",
      "Epoch 9157: train loss: 0.016725242137908936, val loss: 0.06873270124197006\n",
      "Epoch 9158: train loss: 0.020818766206502914, val loss: 0.054996490478515625\n",
      "Epoch 9159: train loss: 0.021614689379930496, val loss: 0.10970355570316315\n",
      "Epoch 9160: train loss: 0.018359532579779625, val loss: 0.09472743421792984\n",
      "Epoch 9161: train loss: 0.015673020854592323, val loss: 0.06049156188964844\n",
      "Epoch 9162: train loss: 0.01380701083689928, val loss: 0.07059144228696823\n",
      "Epoch 9163: train loss: 0.017436649650335312, val loss: 0.10292508453130722\n",
      "Epoch 9164: train loss: 0.015129116363823414, val loss: 0.054483067244291306\n",
      "Epoch 9165: train loss: 0.016007540747523308, val loss: 0.06494424492120743\n",
      "Epoch 9166: train loss: 0.018753742799162865, val loss: 0.06046150252223015\n",
      "Epoch 9167: train loss: 0.01259452011436224, val loss: 0.0640128031373024\n",
      "Epoch 9168: train loss: 0.01834673061966896, val loss: 0.09914983063936234\n",
      "Epoch 9169: train loss: 0.02042381465435028, val loss: 0.08604518324136734\n",
      "Epoch 9170: train loss: 0.017243269830942154, val loss: 0.08252529054880142\n",
      "Epoch 9171: train loss: 0.012101282365620136, val loss: 0.048658400774002075\n",
      "Epoch 9172: train loss: 0.014930996112525463, val loss: 0.04386143758893013\n",
      "Epoch 9173: train loss: 0.018022924661636353, val loss: 0.05992140993475914\n",
      "Epoch 9174: train loss: 0.01435334887355566, val loss: 0.043416477739810944\n",
      "Epoch 9175: train loss: 0.021380014717578888, val loss: 0.08744236081838608\n",
      "Epoch 9176: train loss: 0.012825743295252323, val loss: 0.05748738721013069\n",
      "Epoch 9177: train loss: 0.017643269151449203, val loss: 0.04730350524187088\n",
      "Epoch 9178: train loss: 0.020109781995415688, val loss: 0.04753124713897705\n",
      "Epoch 9179: train loss: 0.01858696900308132, val loss: 0.06824257224798203\n",
      "Epoch 9180: train loss: 0.013696111738681793, val loss: 0.06228482723236084\n",
      "Epoch 9181: train loss: 0.01880098506808281, val loss: 0.08828257769346237\n",
      "Epoch 9182: train loss: 0.020959844812750816, val loss: 0.03240163251757622\n",
      "Epoch 9183: train loss: 0.02854100987315178, val loss: 0.07133716344833374\n",
      "Epoch 9184: train loss: 0.012524968013167381, val loss: 0.04611217603087425\n",
      "Epoch 9185: train loss: 0.015154690481722355, val loss: 0.0646088719367981\n",
      "Epoch 9186: train loss: 0.016924237832427025, val loss: 0.09149851649999619\n",
      "Epoch 9187: train loss: 0.014021631330251694, val loss: 0.09542831033468246\n",
      "Epoch 9188: train loss: 0.02076726406812668, val loss: 0.05293714627623558\n",
      "Epoch 9189: train loss: 0.015558123588562012, val loss: 0.09119289368391037\n",
      "Epoch 9190: train loss: 0.013107525184750557, val loss: 0.06869997084140778\n",
      "Epoch 9191: train loss: 0.016469623893499374, val loss: 0.07342690974473953\n",
      "Epoch 9192: train loss: 0.015402335673570633, val loss: 0.05482408031821251\n",
      "Epoch 9193: train loss: 0.016964461654424667, val loss: 0.06250544637441635\n",
      "Epoch 9194: train loss: 0.013652229681611061, val loss: 0.07544086128473282\n",
      "Epoch 9195: train loss: 0.015481981448829174, val loss: 0.06945991516113281\n",
      "Epoch 9196: train loss: 0.017184050753712654, val loss: 0.04814456030726433\n",
      "Epoch 9197: train loss: 0.017023131251335144, val loss: 0.07547729462385178\n",
      "Epoch 9198: train loss: 0.019927289336919785, val loss: 0.06798523664474487\n",
      "Epoch 9199: train loss: 0.017593253403902054, val loss: 0.0663241297006607\n",
      "Epoch 9200: train loss: 0.016808174550533295, val loss: 0.05976904556155205\n",
      "Epoch 9201: train loss: 0.017659004777669907, val loss: 0.04943640157580376\n",
      "Epoch 9202: train loss: 0.014066215604543686, val loss: 0.049134839326143265\n",
      "Epoch 9203: train loss: 0.012112380936741829, val loss: 0.10713261365890503\n",
      "Epoch 9204: train loss: 0.01677772030234337, val loss: 0.11599339544773102\n",
      "Epoch 9205: train loss: 0.015490149147808552, val loss: 0.0638614222407341\n",
      "Epoch 9206: train loss: 0.01505572535097599, val loss: 0.0701708197593689\n",
      "Epoch 9207: train loss: 0.013092969544231892, val loss: 0.06257259845733643\n",
      "Epoch 9208: train loss: 0.020903799682855606, val loss: 0.05991103872656822\n",
      "Epoch 9209: train loss: 0.018392184749245644, val loss: 0.05265606567263603\n",
      "Epoch 9210: train loss: 0.01838248036801815, val loss: 0.10183975845575333\n",
      "Epoch 9211: train loss: 0.012977576814591885, val loss: 0.041235506534576416\n",
      "Epoch 9212: train loss: 0.02060304954648018, val loss: 0.05320988968014717\n",
      "Epoch 9213: train loss: 0.015992162749171257, val loss: 0.1094994768500328\n",
      "Epoch 9214: train loss: 0.017040036618709564, val loss: 0.08158786594867706\n",
      "Epoch 9215: train loss: 0.014580183662474155, val loss: 0.06751993298530579\n",
      "Epoch 9216: train loss: 0.011209176853299141, val loss: 0.047138381749391556\n",
      "Epoch 9217: train loss: 0.016643395647406578, val loss: 0.05089060217142105\n",
      "Epoch 9218: train loss: 0.016149450093507767, val loss: 0.07830452919006348\n",
      "Epoch 9219: train loss: 0.02065332792699337, val loss: 0.04022599384188652\n",
      "Epoch 9220: train loss: 0.024203414097428322, val loss: 0.05667132884263992\n",
      "Epoch 9221: train loss: 0.021499065682291985, val loss: 0.05694364383816719\n",
      "Epoch 9222: train loss: 0.024766353890299797, val loss: 0.10569252818822861\n",
      "Epoch 9223: train loss: 0.020697250962257385, val loss: 0.08191349357366562\n",
      "Epoch 9224: train loss: 0.01735328696668148, val loss: 0.05613771826028824\n",
      "Epoch 9225: train loss: 0.017298690974712372, val loss: 0.08909327536821365\n",
      "Epoch 9226: train loss: 0.015361092053353786, val loss: 0.045710671693086624\n",
      "Epoch 9227: train loss: 0.010761156678199768, val loss: 0.05261895805597305\n",
      "Epoch 9228: train loss: 0.014355400577187538, val loss: 0.05723857879638672\n",
      "Epoch 9229: train loss: 0.017976609990000725, val loss: 0.07550644874572754\n",
      "Epoch 9230: train loss: 0.014689273200929165, val loss: 0.075685515999794\n",
      "Epoch 9231: train loss: 0.012176845222711563, val loss: 0.07391566038131714\n",
      "Epoch 9232: train loss: 0.01390432845801115, val loss: 0.09901835024356842\n",
      "Epoch 9233: train loss: 0.014455609954893589, val loss: 0.0888160988688469\n",
      "Epoch 9234: train loss: 0.017455819994211197, val loss: 0.11958125978708267\n",
      "Epoch 9235: train loss: 0.01756008341908455, val loss: 0.04957646504044533\n",
      "Epoch 9236: train loss: 0.016235914081335068, val loss: 0.07711552083492279\n",
      "Epoch 9237: train loss: 0.018832677975296974, val loss: 0.08253814280033112\n",
      "Epoch 9238: train loss: 0.013345567509531975, val loss: 0.08159982413053513\n",
      "Epoch 9239: train loss: 0.016824331134557724, val loss: 0.07895894348621368\n",
      "Epoch 9240: train loss: 0.01544915046542883, val loss: 0.07398321479558945\n",
      "Epoch 9241: train loss: 0.016611414030194283, val loss: 0.08102797716856003\n",
      "Epoch 9242: train loss: 0.01706426590681076, val loss: 0.09478619694709778\n",
      "Epoch 9243: train loss: 0.011183266527950764, val loss: 0.0919746607542038\n",
      "Epoch 9244: train loss: 0.015274347737431526, val loss: 0.059407204389572144\n",
      "Epoch 9245: train loss: 0.017141299322247505, val loss: 0.08803122490644455\n",
      "Epoch 9246: train loss: 0.01041446439921856, val loss: 0.07138781994581223\n",
      "Epoch 9247: train loss: 0.011739124543964863, val loss: 0.07901361584663391\n",
      "Epoch 9248: train loss: 0.01683424599468708, val loss: 0.06991787254810333\n",
      "Epoch 9249: train loss: 0.01370195858180523, val loss: 0.0883973240852356\n",
      "Epoch 9250: train loss: 0.017255118116736412, val loss: 0.08364802598953247\n",
      "Epoch 9251: train loss: 0.01896725594997406, val loss: 0.08558391034603119\n",
      "Epoch 9252: train loss: 0.014712899923324585, val loss: 0.05401730164885521\n",
      "Epoch 9253: train loss: 0.01803169772028923, val loss: 0.07313349097967148\n",
      "Epoch 9254: train loss: 0.016441011801362038, val loss: 0.07294990122318268\n",
      "Epoch 9255: train loss: 0.010877609252929688, val loss: 0.0713239461183548\n",
      "Epoch 9256: train loss: 0.020666327327489853, val loss: 0.09654393047094345\n",
      "Epoch 9257: train loss: 0.01679988019168377, val loss: 0.05264255404472351\n",
      "Epoch 9258: train loss: 0.017718985676765442, val loss: 0.07816078513860703\n",
      "Epoch 9259: train loss: 0.014002087526023388, val loss: 0.07466043531894684\n",
      "Epoch 9260: train loss: 0.01074939500540495, val loss: 0.039289623498916626\n",
      "Epoch 9261: train loss: 0.0178479366004467, val loss: 0.0537116639316082\n",
      "Epoch 9262: train loss: 0.01910274289548397, val loss: 0.07679527252912521\n",
      "Epoch 9263: train loss: 0.019501369446516037, val loss: 0.04965022951364517\n",
      "Epoch 9264: train loss: 0.01447688601911068, val loss: 0.04429998621344566\n",
      "Epoch 9265: train loss: 0.013100378215312958, val loss: 0.0999409481883049\n",
      "Epoch 9266: train loss: 0.01667020097374916, val loss: 0.1000937968492508\n",
      "Epoch 9267: train loss: 0.012338315136730671, val loss: 0.08266562223434448\n",
      "Epoch 9268: train loss: 0.01569688320159912, val loss: 0.0683363527059555\n",
      "Epoch 9269: train loss: 0.012256642803549767, val loss: 0.059211622923612595\n",
      "Epoch 9270: train loss: 0.019883902743458748, val loss: 0.07709335535764694\n",
      "Epoch 9271: train loss: 0.013257339596748352, val loss: 0.09334409236907959\n",
      "Epoch 9272: train loss: 0.016958149150013924, val loss: 0.05029667541384697\n",
      "Epoch 9273: train loss: 0.014796559698879719, val loss: 0.055176425725221634\n",
      "Epoch 9274: train loss: 0.01324155367910862, val loss: 0.07564661651849747\n",
      "Epoch 9275: train loss: 0.011445355601608753, val loss: 0.0805678516626358\n",
      "Epoch 9276: train loss: 0.014469255693256855, val loss: 0.045382678508758545\n",
      "Epoch 9277: train loss: 0.015674440190196037, val loss: 0.05678216367959976\n",
      "Epoch 9278: train loss: 0.020252974703907967, val loss: 0.08587928861379623\n",
      "Epoch 9279: train loss: 0.014753460884094238, val loss: 0.062467630952596664\n",
      "Epoch 9280: train loss: 0.016605647280812263, val loss: 0.08018089830875397\n",
      "Epoch 9281: train loss: 0.015935143455863, val loss: 0.062435001134872437\n",
      "Epoch 9282: train loss: 0.0235738605260849, val loss: 0.06504865735769272\n",
      "Epoch 9283: train loss: 0.016396140679717064, val loss: 0.0675019919872284\n",
      "Epoch 9284: train loss: 0.014677878469228745, val loss: 0.06268142908811569\n",
      "Epoch 9285: train loss: 0.014315454289317131, val loss: 0.07564309984445572\n",
      "Epoch 9286: train loss: 0.013940800912678242, val loss: 0.0796809196472168\n",
      "Epoch 9287: train loss: 0.01684456691145897, val loss: 0.06234383583068848\n",
      "Epoch 9288: train loss: 0.015582561492919922, val loss: 0.04386143013834953\n",
      "Epoch 9289: train loss: 0.011558695696294308, val loss: 0.054207693785429\n",
      "Epoch 9290: train loss: 0.013467889279127121, val loss: 0.05552191659808159\n",
      "Epoch 9291: train loss: 0.01312070433050394, val loss: 0.08373942971229553\n",
      "Epoch 9292: train loss: 0.01993250474333763, val loss: 0.0981091633439064\n",
      "Epoch 9293: train loss: 0.017324000597000122, val loss: 0.07788866758346558\n",
      "Epoch 9294: train loss: 0.015119488351047039, val loss: 0.08805042505264282\n",
      "Epoch 9295: train loss: 0.01682516746222973, val loss: 0.07111256569623947\n",
      "Epoch 9296: train loss: 0.01402418501675129, val loss: 0.055938299745321274\n",
      "Epoch 9297: train loss: 0.017953762784600258, val loss: 0.05401542782783508\n",
      "Epoch 9298: train loss: 0.01548848208039999, val loss: 0.06427960097789764\n",
      "Epoch 9299: train loss: 0.021747777238488197, val loss: 0.10903766006231308\n",
      "Epoch 9300: train loss: 0.010940021835267544, val loss: 0.03466456010937691\n",
      "Epoch 9301: train loss: 0.018925633281469345, val loss: 0.07791861146688461\n",
      "Epoch 9302: train loss: 0.021007966250181198, val loss: 0.062322139739990234\n",
      "Epoch 9303: train loss: 0.013263233937323093, val loss: 0.08454027771949768\n",
      "Epoch 9304: train loss: 0.013100804761052132, val loss: 0.09215709567070007\n",
      "Epoch 9305: train loss: 0.01767813414335251, val loss: 0.05548527464270592\n",
      "Epoch 9306: train loss: 0.014588706195354462, val loss: 0.08164137601852417\n",
      "Epoch 9307: train loss: 0.014350149780511856, val loss: 0.0644678995013237\n",
      "Epoch 9308: train loss: 0.015734754502773285, val loss: 0.11130034923553467\n",
      "Epoch 9309: train loss: 0.022688833996653557, val loss: 0.044087495654821396\n",
      "Epoch 9310: train loss: 0.014856058172881603, val loss: 0.05540139600634575\n",
      "Epoch 9311: train loss: 0.015199549496173859, val loss: 0.05659285932779312\n",
      "Epoch 9312: train loss: 0.0214371420443058, val loss: 0.05824657902121544\n",
      "Epoch 9313: train loss: 0.012460283935070038, val loss: 0.057688791304826736\n",
      "Epoch 9314: train loss: 0.01504004281014204, val loss: 0.0686599463224411\n",
      "Epoch 9315: train loss: 0.01871618814766407, val loss: 0.06454354524612427\n",
      "Epoch 9316: train loss: 0.017506955191493034, val loss: 0.08317556977272034\n",
      "Epoch 9317: train loss: 0.016892602667212486, val loss: 0.045956339687108994\n",
      "Epoch 9318: train loss: 0.02135583944618702, val loss: 0.09157291054725647\n",
      "Epoch 9319: train loss: 0.02186466194689274, val loss: 0.07232866436243057\n",
      "Epoch 9320: train loss: 0.01842181757092476, val loss: 0.0633871927857399\n",
      "Epoch 9321: train loss: 0.012263908050954342, val loss: 0.08268478512763977\n",
      "Epoch 9322: train loss: 0.020188605412840843, val loss: 0.11496194452047348\n",
      "Epoch 9323: train loss: 0.02054913528263569, val loss: 0.07677433639764786\n",
      "Epoch 9324: train loss: 0.01668604277074337, val loss: 0.09571166336536407\n",
      "Epoch 9325: train loss: 0.020970996469259262, val loss: 0.053365688771009445\n",
      "Epoch 9326: train loss: 0.016361290588974953, val loss: 0.049233388155698776\n",
      "Epoch 9327: train loss: 0.014063392765820026, val loss: 0.08082222938537598\n",
      "Epoch 9328: train loss: 0.016054611653089523, val loss: 0.09309026598930359\n",
      "Epoch 9329: train loss: 0.014964275993406773, val loss: 0.0807347521185875\n",
      "Epoch 9330: train loss: 0.013342966325581074, val loss: 0.06000638008117676\n",
      "Epoch 9331: train loss: 0.018667032942175865, val loss: 0.06510063260793686\n",
      "Epoch 9332: train loss: 0.02027967758476734, val loss: 0.07045113295316696\n",
      "Epoch 9333: train loss: 0.010960821993649006, val loss: 0.09800344705581665\n",
      "Epoch 9334: train loss: 0.013943742960691452, val loss: 0.06833178550004959\n",
      "Epoch 9335: train loss: 0.013437198475003242, val loss: 0.03741500899195671\n",
      "Epoch 9336: train loss: 0.01399407908320427, val loss: 0.053813863545656204\n",
      "Epoch 9337: train loss: 0.016561107710003853, val loss: 0.04273631051182747\n",
      "Epoch 9338: train loss: 0.015833092853426933, val loss: 0.08376383781433105\n",
      "Epoch 9339: train loss: 0.012493735179305077, val loss: 0.09905228763818741\n",
      "Epoch 9340: train loss: 0.02202485129237175, val loss: 0.10762812942266464\n",
      "Epoch 9341: train loss: 0.019190145656466484, val loss: 0.09532458335161209\n",
      "Epoch 9342: train loss: 0.014910216443240643, val loss: 0.06130911782383919\n",
      "Epoch 9343: train loss: 0.013699794188141823, val loss: 0.06627887487411499\n",
      "Epoch 9344: train loss: 0.014492416754364967, val loss: 0.0802556648850441\n",
      "Epoch 9345: train loss: 0.015592677518725395, val loss: 0.05511634424328804\n",
      "Epoch 9346: train loss: 0.01379360631108284, val loss: 0.09341812878847122\n",
      "Epoch 9347: train loss: 0.017511053010821342, val loss: 0.0613219253718853\n",
      "Epoch 9348: train loss: 0.01287171896547079, val loss: 0.062431324273347855\n",
      "Epoch 9349: train loss: 0.015607262030243874, val loss: 0.08458507061004639\n",
      "Epoch 9350: train loss: 0.015167965553700924, val loss: 0.1061023399233818\n",
      "Epoch 9351: train loss: 0.018636001273989677, val loss: 0.07750170677900314\n",
      "Epoch 9352: train loss: 0.009631611406803131, val loss: 0.06074387580156326\n",
      "Epoch 9353: train loss: 0.011174892075359821, val loss: 0.05859629064798355\n",
      "Epoch 9354: train loss: 0.020495891571044922, val loss: 0.0764383003115654\n",
      "Epoch 9355: train loss: 0.014854599721729755, val loss: 0.057536132633686066\n",
      "Epoch 9356: train loss: 0.015932736918330193, val loss: 0.06688930094242096\n",
      "Epoch 9357: train loss: 0.011500938795506954, val loss: 0.06647761166095734\n",
      "Epoch 9358: train loss: 0.015255311504006386, val loss: 0.08819521963596344\n",
      "Epoch 9359: train loss: 0.014448068104684353, val loss: 0.04283974692225456\n",
      "Epoch 9360: train loss: 0.020556768402457237, val loss: 0.030653392896056175\n",
      "Epoch 9361: train loss: 0.013552399352192879, val loss: 0.10293864458799362\n",
      "Epoch 9362: train loss: 0.016323450952768326, val loss: 0.07957858592271805\n",
      "Epoch 9363: train loss: 0.014858433976769447, val loss: 0.12061303108930588\n",
      "Epoch 9364: train loss: 0.013211658224463463, val loss: 0.07513370364904404\n",
      "Epoch 9365: train loss: 0.024141548201441765, val loss: 0.04170261695981026\n",
      "Epoch 9366: train loss: 0.01475862693041563, val loss: 0.04303554818034172\n",
      "Epoch 9367: train loss: 0.017391566187143326, val loss: 0.10103420168161392\n",
      "Epoch 9368: train loss: 0.016168538480997086, val loss: 0.08663620799779892\n",
      "Epoch 9369: train loss: 0.013768564909696579, val loss: 0.05670766904950142\n",
      "Epoch 9370: train loss: 0.013616406358778477, val loss: 0.047075528651475906\n",
      "Epoch 9371: train loss: 0.01244396809488535, val loss: 0.05794795975089073\n",
      "Epoch 9372: train loss: 0.014713132753968239, val loss: 0.07166173309087753\n",
      "Epoch 9373: train loss: 0.01625254563987255, val loss: 0.047602444887161255\n",
      "Epoch 9374: train loss: 0.01382843405008316, val loss: 0.0867871642112732\n",
      "Epoch 9375: train loss: 0.011320063844323158, val loss: 0.09417854994535446\n",
      "Epoch 9376: train loss: 0.020131785422563553, val loss: 0.08340408653020859\n",
      "Epoch 9377: train loss: 0.014885286800563335, val loss: 0.049449749290943146\n",
      "Epoch 9378: train loss: 0.01463691983371973, val loss: 0.09043890237808228\n",
      "Epoch 9379: train loss: 0.011516992934048176, val loss: 0.06502675265073776\n",
      "Epoch 9380: train loss: 0.02196894958615303, val loss: 0.06403473764657974\n",
      "Epoch 9381: train loss: 0.015386698767542839, val loss: 0.07841654121875763\n",
      "Epoch 9382: train loss: 0.01374881062656641, val loss: 0.061417292803525925\n",
      "Epoch 9383: train loss: 0.0159460362046957, val loss: 0.08574943989515305\n",
      "Epoch 9384: train loss: 0.017199691385030746, val loss: 0.054181791841983795\n",
      "Epoch 9385: train loss: 0.015180794522166252, val loss: 0.08234766870737076\n",
      "Epoch 9386: train loss: 0.014516537077724934, val loss: 0.07305087149143219\n",
      "Epoch 9387: train loss: 0.013861291110515594, val loss: 0.08893708139657974\n",
      "Epoch 9388: train loss: 0.01557488739490509, val loss: 0.056736528873443604\n",
      "Epoch 9389: train loss: 0.009166382253170013, val loss: 0.07921835035085678\n",
      "Epoch 9390: train loss: 0.01637358032166958, val loss: 0.0783962830901146\n",
      "Epoch 9391: train loss: 0.017523206770420074, val loss: 0.05180841311812401\n",
      "Epoch 9392: train loss: 0.01480093039572239, val loss: 0.10535091161727905\n",
      "Epoch 9393: train loss: 0.014807301573455334, val loss: 0.0803743377327919\n",
      "Epoch 9394: train loss: 0.015601548366248608, val loss: 0.07268517464399338\n",
      "Epoch 9395: train loss: 0.01998467557132244, val loss: 0.09304600208997726\n",
      "Epoch 9396: train loss: 0.010828658938407898, val loss: 0.06115741655230522\n",
      "Epoch 9397: train loss: 0.018203848972916603, val loss: 0.11117193847894669\n",
      "Epoch 9398: train loss: 0.01317683793604374, val loss: 0.049438878893852234\n",
      "Epoch 9399: train loss: 0.01893547922372818, val loss: 0.0799301490187645\n",
      "Epoch 9400: train loss: 0.012523178942501545, val loss: 0.04395263269543648\n",
      "Epoch 9401: train loss: 0.014646698720753193, val loss: 0.06507369130849838\n",
      "Epoch 9402: train loss: 0.014939154498279095, val loss: 0.06797513365745544\n",
      "Epoch 9403: train loss: 0.018751928582787514, val loss: 0.04502272605895996\n",
      "Epoch 9404: train loss: 0.014585962519049644, val loss: 0.06204775720834732\n",
      "Epoch 9405: train loss: 0.012596875429153442, val loss: 0.03567446768283844\n",
      "Epoch 9406: train loss: 0.015938233584165573, val loss: 0.0857667326927185\n",
      "Epoch 9407: train loss: 0.019710615277290344, val loss: 0.05662723258137703\n",
      "Epoch 9408: train loss: 0.012841337360441685, val loss: 0.0525343082845211\n",
      "Epoch 9409: train loss: 0.016222869977355003, val loss: 0.0773954913020134\n",
      "Epoch 9410: train loss: 0.015392356552183628, val loss: 0.04535132646560669\n",
      "Epoch 9411: train loss: 0.016071245074272156, val loss: 0.1275034248828888\n",
      "Epoch 9412: train loss: 0.01717306300997734, val loss: 0.0978270024061203\n",
      "Epoch 9413: train loss: 0.015267947688698769, val loss: 0.11722498387098312\n",
      "Epoch 9414: train loss: 0.012576782144606113, val loss: 0.077395960688591\n",
      "Epoch 9415: train loss: 0.01577525958418846, val loss: 0.06478764861822128\n",
      "Epoch 9416: train loss: 0.009599760174751282, val loss: 0.08435966819524765\n",
      "Epoch 9417: train loss: 0.015587547793984413, val loss: 0.08968707174062729\n",
      "Epoch 9418: train loss: 0.012342019006609917, val loss: 0.05719106271862984\n",
      "Epoch 9419: train loss: 0.01177638303488493, val loss: 0.09887971729040146\n",
      "Epoch 9420: train loss: 0.012647527270019054, val loss: 0.06241123005747795\n",
      "Epoch 9421: train loss: 0.01166928093880415, val loss: 0.056361179798841476\n",
      "Epoch 9422: train loss: 0.01799999363720417, val loss: 0.05476970225572586\n",
      "Epoch 9423: train loss: 0.011851062998175621, val loss: 0.06952790170907974\n",
      "Epoch 9424: train loss: 0.02108212746679783, val loss: 0.046194881200790405\n",
      "Epoch 9425: train loss: 0.013334155082702637, val loss: 0.059389013797044754\n",
      "Epoch 9426: train loss: 0.016169171780347824, val loss: 0.07112197577953339\n",
      "Epoch 9427: train loss: 0.015349452383816242, val loss: 0.05755209922790527\n",
      "Epoch 9428: train loss: 0.01828424632549286, val loss: 0.08620882779359818\n",
      "Epoch 9429: train loss: 0.011143583804368973, val loss: 0.05636317655444145\n",
      "Epoch 9430: train loss: 0.012059158645570278, val loss: 0.032922375947237015\n",
      "Epoch 9431: train loss: 0.012592756189405918, val loss: 0.056316982954740524\n",
      "Epoch 9432: train loss: 0.017209716141223907, val loss: 0.08062354475259781\n",
      "Epoch 9433: train loss: 0.013347472064197063, val loss: 0.040510501712560654\n",
      "Epoch 9434: train loss: 0.015023198910057545, val loss: 0.10022139549255371\n",
      "Epoch 9435: train loss: 0.016096485778689384, val loss: 0.05553168058395386\n",
      "Epoch 9436: train loss: 0.008000405505299568, val loss: 0.07220645993947983\n",
      "Epoch 9437: train loss: 0.012063007801771164, val loss: 0.07052294164896011\n",
      "Epoch 9438: train loss: 0.01852707378566265, val loss: 0.047430992126464844\n",
      "Epoch 9439: train loss: 0.011296635493636131, val loss: 0.07229550927877426\n",
      "Epoch 9440: train loss: 0.018430108204483986, val loss: 0.054248493164777756\n",
      "Epoch 9441: train loss: 0.018392428755760193, val loss: 0.06610070914030075\n",
      "Epoch 9442: train loss: 0.014144206419587135, val loss: 0.04914216324687004\n",
      "Epoch 9443: train loss: 0.018678421154618263, val loss: 0.0569303035736084\n",
      "Epoch 9444: train loss: 0.016764113679528236, val loss: 0.059437524527311325\n",
      "Epoch 9445: train loss: 0.02462596446275711, val loss: 0.05751579999923706\n",
      "Epoch 9446: train loss: 0.017199590802192688, val loss: 0.08885934203863144\n",
      "Epoch 9447: train loss: 0.015446046367287636, val loss: 0.08292168378829956\n",
      "Epoch 9448: train loss: 0.014159553684294224, val loss: 0.08750159293413162\n",
      "Epoch 9449: train loss: 0.015383114106953144, val loss: 0.09299726784229279\n",
      "Epoch 9450: train loss: 0.012712854892015457, val loss: 0.07407291978597641\n",
      "Epoch 9451: train loss: 0.017143147066235542, val loss: 0.07596881687641144\n",
      "Epoch 9452: train loss: 0.020787958055734634, val loss: 0.04231121018528938\n",
      "Epoch 9453: train loss: 0.019887683913111687, val loss: 0.086863674223423\n",
      "Epoch 9454: train loss: 0.019417939707636833, val loss: 0.06916268169879913\n",
      "Epoch 9455: train loss: 0.009312929585576057, val loss: 0.0667920857667923\n",
      "Epoch 9456: train loss: 0.020385324954986572, val loss: 0.13914556801319122\n",
      "Epoch 9457: train loss: 0.013443353585898876, val loss: 0.10237089544534683\n",
      "Epoch 9458: train loss: 0.01896742545068264, val loss: 0.08435419946908951\n",
      "Epoch 9459: train loss: 0.015096227638423443, val loss: 0.0640982910990715\n",
      "Epoch 9460: train loss: 0.012280651368200779, val loss: 0.09409544616937637\n",
      "Epoch 9461: train loss: 0.019075017422437668, val loss: 0.06368524581193924\n",
      "Epoch 9462: train loss: 0.014942977577447891, val loss: 0.07636787742376328\n",
      "Epoch 9463: train loss: 0.012819040566682816, val loss: 0.09612198919057846\n",
      "Epoch 9464: train loss: 0.016309192404150963, val loss: 0.06317555159330368\n",
      "Epoch 9465: train loss: 0.02026115171611309, val loss: 0.08298417925834656\n",
      "Epoch 9466: train loss: 0.01681179367005825, val loss: 0.06618431210517883\n",
      "Epoch 9467: train loss: 0.015835575759410858, val loss: 0.06208663061261177\n",
      "Epoch 9468: train loss: 0.01542594749480486, val loss: 0.07567168027162552\n",
      "Epoch 9469: train loss: 0.018908509984612465, val loss: 0.0985252857208252\n",
      "Epoch 9470: train loss: 0.015692463144659996, val loss: 0.052168894559144974\n",
      "Epoch 9471: train loss: 0.018211698159575462, val loss: 0.07831821590662003\n",
      "Epoch 9472: train loss: 0.018686091527342796, val loss: 0.06629341095685959\n",
      "Epoch 9473: train loss: 0.017864670604467392, val loss: 0.048222847282886505\n",
      "Epoch 9474: train loss: 0.017937077209353447, val loss: 0.08725350350141525\n",
      "Epoch 9475: train loss: 0.021094949916005135, val loss: 0.08496540039777756\n",
      "Epoch 9476: train loss: 0.014413261786103249, val loss: 0.07312875241041183\n",
      "Epoch 9477: train loss: 0.022982224822044373, val loss: 0.037285901606082916\n",
      "Epoch 9478: train loss: 0.016037261113524437, val loss: 0.10509264469146729\n",
      "Epoch 9479: train loss: 0.015081575140357018, val loss: 0.08832899481058121\n",
      "Epoch 9480: train loss: 0.01578875631093979, val loss: 0.1245153471827507\n",
      "Epoch 9481: train loss: 0.01761292666196823, val loss: 0.06948816031217575\n",
      "Epoch 9482: train loss: 0.015846427530050278, val loss: 0.04550444334745407\n",
      "Epoch 9483: train loss: 0.01978643611073494, val loss: 0.07345853000879288\n",
      "Epoch 9484: train loss: 0.02666015364229679, val loss: 0.06980115175247192\n",
      "Epoch 9485: train loss: 0.017652617767453194, val loss: 0.08007585257291794\n",
      "Epoch 9486: train loss: 0.013449315913021564, val loss: 0.08157173544168472\n",
      "Epoch 9487: train loss: 0.014480935409665108, val loss: 0.060546696186065674\n",
      "Epoch 9488: train loss: 0.012688802555203438, val loss: 0.09432662278413773\n",
      "Epoch 9489: train loss: 0.016439642757177353, val loss: 0.07018422335386276\n",
      "Epoch 9490: train loss: 0.012996719218790531, val loss: 0.10034222900867462\n",
      "Epoch 9491: train loss: 0.012255125679075718, val loss: 0.06357164680957794\n",
      "Epoch 9492: train loss: 0.01743479073047638, val loss: 0.05008072406053543\n",
      "Epoch 9493: train loss: 0.015698634088039398, val loss: 0.09155797958374023\n",
      "Epoch 9494: train loss: 0.015931887552142143, val loss: 0.055218230932950974\n",
      "Epoch 9495: train loss: 0.014478085562586784, val loss: 0.0753583237528801\n",
      "Epoch 9496: train loss: 0.015900665894150734, val loss: 0.06506513804197311\n",
      "Epoch 9497: train loss: 0.019960111007094383, val loss: 0.05585675314068794\n",
      "Epoch 9498: train loss: 0.01597909815609455, val loss: 0.08116357773542404\n",
      "Epoch 9499: train loss: 0.01527115236967802, val loss: 0.09534060209989548\n",
      "Epoch 9500: train loss: 0.014632183127105236, val loss: 0.04991186782717705\n",
      "Epoch 9501: train loss: 0.015975262969732285, val loss: 0.1288914978504181\n",
      "Epoch 9502: train loss: 0.01061498187482357, val loss: 0.0858418270945549\n",
      "Epoch 9503: train loss: 0.01511993445456028, val loss: 0.07017651945352554\n",
      "Epoch 9504: train loss: 0.01633976586163044, val loss: 0.07447542250156403\n",
      "Epoch 9505: train loss: 0.017667613923549652, val loss: 0.059196826070547104\n",
      "Epoch 9506: train loss: 0.01630512624979019, val loss: 0.09465707838535309\n",
      "Epoch 9507: train loss: 0.01759554259479046, val loss: 0.06974142044782639\n",
      "Epoch 9508: train loss: 0.018231907859444618, val loss: 0.05783660337328911\n",
      "Epoch 9509: train loss: 0.01816990226507187, val loss: 0.08085977286100388\n",
      "Epoch 9510: train loss: 0.015699151903390884, val loss: 0.05998636409640312\n",
      "Epoch 9511: train loss: 0.016660427674651146, val loss: 0.08309940993785858\n",
      "Epoch 9512: train loss: 0.017764532938599586, val loss: 0.0545995719730854\n",
      "Epoch 9513: train loss: 0.017234886065125465, val loss: 0.03561807796359062\n",
      "Epoch 9514: train loss: 0.01611626148223877, val loss: 0.08019130676984787\n",
      "Epoch 9515: train loss: 0.02311415784060955, val loss: 0.05694195255637169\n",
      "Epoch 9516: train loss: 0.019120201468467712, val loss: 0.09125149250030518\n",
      "Epoch 9517: train loss: 0.017206668853759766, val loss: 0.0616527795791626\n",
      "Epoch 9518: train loss: 0.019516505300998688, val loss: 0.07373714447021484\n",
      "Epoch 9519: train loss: 0.013381035067141056, val loss: 0.06572719663381577\n",
      "Epoch 9520: train loss: 0.01277936715632677, val loss: 0.09831178933382034\n",
      "Epoch 9521: train loss: 0.01685321517288685, val loss: 0.04707592353224754\n",
      "Epoch 9522: train loss: 0.01042708195745945, val loss: 0.05215181037783623\n",
      "Epoch 9523: train loss: 0.015767715871334076, val loss: 0.0773714929819107\n",
      "Epoch 9524: train loss: 0.020181618630886078, val loss: 0.06842496246099472\n",
      "Epoch 9525: train loss: 0.017328321933746338, val loss: 0.11355729401111603\n",
      "Epoch 9526: train loss: 0.0160165224224329, val loss: 0.11062657833099365\n",
      "Epoch 9527: train loss: 0.021969221532344818, val loss: 0.09576334804296494\n",
      "Epoch 9528: train loss: 0.012269348837435246, val loss: 0.07017389684915543\n",
      "Epoch 9529: train loss: 0.01220164354890585, val loss: 0.06778408586978912\n",
      "Epoch 9530: train loss: 0.018884997814893723, val loss: 0.06674559414386749\n",
      "Epoch 9531: train loss: 0.013754921965301037, val loss: 0.07695695757865906\n",
      "Epoch 9532: train loss: 0.009593112394213676, val loss: 0.0753064975142479\n",
      "Epoch 9533: train loss: 0.015573267824947834, val loss: 0.06870170682668686\n",
      "Epoch 9534: train loss: 0.012259266339242458, val loss: 0.05854726955294609\n",
      "Epoch 9535: train loss: 0.020020941272377968, val loss: 0.04207020625472069\n",
      "Epoch 9536: train loss: 0.020366670563817024, val loss: 0.09746495634317398\n",
      "Epoch 9537: train loss: 0.023203855380415916, val loss: 0.07609193027019501\n",
      "Epoch 9538: train loss: 0.015330151654779911, val loss: 0.13652698695659637\n",
      "Epoch 9539: train loss: 0.014614826068282127, val loss: 0.03988593816757202\n",
      "Epoch 9540: train loss: 0.010869995690882206, val loss: 0.06258448213338852\n",
      "Epoch 9541: train loss: 0.015379577875137329, val loss: 0.0530443899333477\n",
      "Epoch 9542: train loss: 0.02155335806310177, val loss: 0.06981345266103745\n",
      "Epoch 9543: train loss: 0.022781791165471077, val loss: 0.09468575567007065\n",
      "Epoch 9544: train loss: 0.020683323964476585, val loss: 0.07098599523305893\n",
      "Epoch 9545: train loss: 0.016301525756716728, val loss: 0.05791666731238365\n",
      "Epoch 9546: train loss: 0.01584985852241516, val loss: 0.07366754859685898\n",
      "Epoch 9547: train loss: 0.017854122444987297, val loss: 0.1024637445807457\n",
      "Epoch 9548: train loss: 0.019534381106495857, val loss: 0.08602877706289291\n",
      "Epoch 9549: train loss: 0.017911748960614204, val loss: 0.06999969482421875\n",
      "Epoch 9550: train loss: 0.013320200145244598, val loss: 0.09814433753490448\n",
      "Epoch 9551: train loss: 0.019391236826777458, val loss: 0.068453848361969\n",
      "Epoch 9552: train loss: 0.020744163542985916, val loss: 0.057232584804296494\n",
      "Epoch 9553: train loss: 0.02115100622177124, val loss: 0.061426401138305664\n",
      "Epoch 9554: train loss: 0.01851658523082733, val loss: 0.08559222519397736\n",
      "Epoch 9555: train loss: 0.01788436621427536, val loss: 0.08657640218734741\n",
      "Epoch 9556: train loss: 0.017620088532567024, val loss: 0.0592738501727581\n",
      "Epoch 9557: train loss: 0.02503708004951477, val loss: 0.056548427790403366\n",
      "Epoch 9558: train loss: 0.021719807758927345, val loss: 0.0454317070543766\n",
      "Epoch 9559: train loss: 0.02013177052140236, val loss: 0.09138453751802444\n",
      "Epoch 9560: train loss: 0.014714457094669342, val loss: 0.06486726552248001\n",
      "Epoch 9561: train loss: 0.016305025666952133, val loss: 0.14152632653713226\n",
      "Epoch 9562: train loss: 0.023085076361894608, val loss: 0.07605466991662979\n",
      "Epoch 9563: train loss: 0.013141412287950516, val loss: 0.0691671147942543\n",
      "Epoch 9564: train loss: 0.011797470971941948, val loss: 0.07997946441173553\n",
      "Epoch 9565: train loss: 0.014335505664348602, val loss: 0.1100708618760109\n",
      "Epoch 9566: train loss: 0.01825263723731041, val loss: 0.12136667221784592\n",
      "Epoch 9567: train loss: 0.013016822747886181, val loss: 0.07064860314130783\n",
      "Epoch 9568: train loss: 0.01942511647939682, val loss: 0.08482243120670319\n",
      "Epoch 9569: train loss: 0.015273919329047203, val loss: 0.0571226142346859\n",
      "Epoch 9570: train loss: 0.019521888345479965, val loss: 0.07424815744161606\n",
      "Epoch 9571: train loss: 0.018695734441280365, val loss: 0.07757952064275742\n",
      "Epoch 9572: train loss: 0.014467762783169746, val loss: 0.07715384662151337\n",
      "Epoch 9573: train loss: 0.016449615359306335, val loss: 0.1255679577589035\n",
      "Epoch 9574: train loss: 0.016597187146544456, val loss: 0.06472887098789215\n",
      "Epoch 9575: train loss: 0.01779169775545597, val loss: 0.0693342313170433\n",
      "Epoch 9576: train loss: 0.015372199937701225, val loss: 0.08318664878606796\n",
      "Epoch 9577: train loss: 0.015630237758159637, val loss: 0.05007050186395645\n",
      "Epoch 9578: train loss: 0.017399394884705544, val loss: 0.0850234180688858\n",
      "Epoch 9579: train loss: 0.016604674980044365, val loss: 0.10488829761743546\n",
      "Epoch 9580: train loss: 0.013747223652899265, val loss: 0.07849013060331345\n",
      "Epoch 9581: train loss: 0.016022855415940285, val loss: 0.13048328459262848\n",
      "Epoch 9582: train loss: 0.022569188848137856, val loss: 0.04601183161139488\n",
      "Epoch 9583: train loss: 0.018939588218927383, val loss: 0.09281253069639206\n",
      "Epoch 9584: train loss: 0.01755939982831478, val loss: 0.10733523219823837\n",
      "Epoch 9585: train loss: 0.016252756118774414, val loss: 0.0481235608458519\n",
      "Epoch 9586: train loss: 0.015350478701293468, val loss: 0.0882939025759697\n",
      "Epoch 9587: train loss: 0.0137688759714365, val loss: 0.11778004467487335\n",
      "Epoch 9588: train loss: 0.013898080214858055, val loss: 0.09511768817901611\n",
      "Epoch 9589: train loss: 0.015432363376021385, val loss: 0.10600193589925766\n",
      "Epoch 9590: train loss: 0.013840116560459137, val loss: 0.09287489205598831\n",
      "Epoch 9591: train loss: 0.018301833420991898, val loss: 0.07684536278247833\n",
      "Epoch 9592: train loss: 0.01646767370402813, val loss: 0.06807171553373337\n",
      "Epoch 9593: train loss: 0.02091217041015625, val loss: 0.05333976820111275\n",
      "Epoch 9594: train loss: 0.016239678487181664, val loss: 0.09219618141651154\n",
      "Epoch 9595: train loss: 0.015004836954176426, val loss: 0.07919847965240479\n",
      "Epoch 9596: train loss: 0.014996261335909367, val loss: 0.0980847105383873\n",
      "Epoch 9597: train loss: 0.01701442524790764, val loss: 0.12446721643209457\n",
      "Epoch 9598: train loss: 0.019077038392424583, val loss: 0.0875568613409996\n",
      "Epoch 9599: train loss: 0.024279970675706863, val loss: 0.0721350833773613\n",
      "Epoch 9600: train loss: 0.015470851212739944, val loss: 0.06573358923196793\n",
      "Epoch 9601: train loss: 0.023490089923143387, val loss: 0.09655789285898209\n",
      "Epoch 9602: train loss: 0.023495059460401535, val loss: 0.10069998353719711\n",
      "Epoch 9603: train loss: 0.013321712613105774, val loss: 0.11177219450473785\n",
      "Epoch 9604: train loss: 0.020134583115577698, val loss: 0.09069880843162537\n",
      "Epoch 9605: train loss: 0.015388107858598232, val loss: 0.061757106333971024\n",
      "Epoch 9606: train loss: 0.013543902896344662, val loss: 0.08272331953048706\n",
      "Epoch 9607: train loss: 0.019396353513002396, val loss: 0.10674410313367844\n",
      "Epoch 9608: train loss: 0.013704976998269558, val loss: 0.047535840421915054\n",
      "Epoch 9609: train loss: 0.016006261110305786, val loss: 0.07987040281295776\n",
      "Epoch 9610: train loss: 0.02142835222184658, val loss: 0.082534059882164\n",
      "Epoch 9611: train loss: 0.016056889668107033, val loss: 0.06901521980762482\n",
      "Epoch 9612: train loss: 0.016290901228785515, val loss: 0.07872693985700607\n",
      "Epoch 9613: train loss: 0.01895497739315033, val loss: 0.07548560947179794\n",
      "Epoch 9614: train loss: 0.014445657841861248, val loss: 0.06005529686808586\n",
      "Epoch 9615: train loss: 0.01940753683447838, val loss: 0.06463678926229477\n",
      "Epoch 9616: train loss: 0.014945334754884243, val loss: 0.08984104543924332\n",
      "Epoch 9617: train loss: 0.015790242701768875, val loss: 0.09789397567510605\n",
      "Epoch 9618: train loss: 0.01753140799701214, val loss: 0.043844133615493774\n",
      "Epoch 9619: train loss: 0.02073455974459648, val loss: 0.09007403999567032\n",
      "Epoch 9620: train loss: 0.019698353484272957, val loss: 0.0785662829875946\n",
      "Epoch 9621: train loss: 0.017963947728276253, val loss: 0.09582358598709106\n",
      "Epoch 9622: train loss: 0.018919281661510468, val loss: 0.08748217672109604\n",
      "Epoch 9623: train loss: 0.017739973962306976, val loss: 0.06170293316245079\n",
      "Epoch 9624: train loss: 0.018039068207144737, val loss: 0.12487965822219849\n",
      "Epoch 9625: train loss: 0.018036620691418648, val loss: 0.08772873133420944\n",
      "Epoch 9626: train loss: 0.01289873942732811, val loss: 0.05269460752606392\n",
      "Epoch 9627: train loss: 0.013766036368906498, val loss: 0.08833027631044388\n",
      "Epoch 9628: train loss: 0.013584781438112259, val loss: 0.09965647757053375\n",
      "Epoch 9629: train loss: 0.017636872828006744, val loss: 0.13627329468727112\n",
      "Epoch 9630: train loss: 0.020572450011968613, val loss: 0.07182259112596512\n",
      "Epoch 9631: train loss: 0.02343897894024849, val loss: 0.04613831639289856\n",
      "Epoch 9632: train loss: 0.01680125668644905, val loss: 0.10554766654968262\n",
      "Epoch 9633: train loss: 0.01468933280557394, val loss: 0.08061428368091583\n",
      "Epoch 9634: train loss: 0.019830496981739998, val loss: 0.07293808460235596\n",
      "Epoch 9635: train loss: 0.01215881947427988, val loss: 0.05102619528770447\n",
      "Epoch 9636: train loss: 0.017365388572216034, val loss: 0.06804006546735764\n",
      "Epoch 9637: train loss: 0.016007516533136368, val loss: 0.09443914145231247\n",
      "Epoch 9638: train loss: 0.01856926828622818, val loss: 0.05614085868000984\n",
      "Epoch 9639: train loss: 0.010974615812301636, val loss: 0.08824872225522995\n",
      "Epoch 9640: train loss: 0.016766652464866638, val loss: 0.0931428074836731\n",
      "Epoch 9641: train loss: 0.013743813149631023, val loss: 0.06748079508543015\n",
      "Epoch 9642: train loss: 0.01628151722252369, val loss: 0.056045036762952805\n",
      "Epoch 9643: train loss: 0.01978098787367344, val loss: 0.06264551728963852\n",
      "Epoch 9644: train loss: 0.017761262133717537, val loss: 0.053257085382938385\n",
      "Epoch 9645: train loss: 0.012266578152775764, val loss: 0.1292233020067215\n",
      "Epoch 9646: train loss: 0.013835025019943714, val loss: 0.0728294774889946\n",
      "Epoch 9647: train loss: 0.014718186110258102, val loss: 0.07932896912097931\n",
      "Epoch 9648: train loss: 0.02301120199263096, val loss: 0.06574561446905136\n",
      "Epoch 9649: train loss: 0.019333476200699806, val loss: 0.0690731555223465\n",
      "Epoch 9650: train loss: 0.014737195335328579, val loss: 0.058229733258485794\n",
      "Epoch 9651: train loss: 0.0168430358171463, val loss: 0.08461597561836243\n",
      "Epoch 9652: train loss: 0.027223503217101097, val loss: 0.06131593510508537\n",
      "Epoch 9653: train loss: 0.019500328227877617, val loss: 0.07967852801084518\n",
      "Epoch 9654: train loss: 0.019473081454634666, val loss: 0.09131365269422531\n",
      "Epoch 9655: train loss: 0.024503052234649658, val loss: 0.06435935944318771\n",
      "Epoch 9656: train loss: 0.015921957790851593, val loss: 0.09027047455310822\n",
      "Epoch 9657: train loss: 0.017440907657146454, val loss: 0.09117567539215088\n",
      "Epoch 9658: train loss: 0.020862074568867683, val loss: 0.07093264907598495\n",
      "Epoch 9659: train loss: 0.012670989148318768, val loss: 0.11513397842645645\n",
      "Epoch 9660: train loss: 0.01478103268891573, val loss: 0.11470363289117813\n",
      "Epoch 9661: train loss: 0.018041595816612244, val loss: 0.09019996225833893\n",
      "Epoch 9662: train loss: 0.016034070402383804, val loss: 0.07393568754196167\n",
      "Epoch 9663: train loss: 0.01260337233543396, val loss: 0.10282754898071289\n",
      "Epoch 9664: train loss: 0.020989058539271355, val loss: 0.06386764347553253\n",
      "Epoch 9665: train loss: 0.012454459443688393, val loss: 0.090605728328228\n",
      "Epoch 9666: train loss: 0.015266739763319492, val loss: 0.06143365427851677\n",
      "Epoch 9667: train loss: 0.01907631941139698, val loss: 0.10175428539514542\n",
      "Epoch 9668: train loss: 0.01742406375706196, val loss: 0.09126289933919907\n",
      "Epoch 9669: train loss: 0.01189118530601263, val loss: 0.09429390728473663\n",
      "Epoch 9670: train loss: 0.01233165804296732, val loss: 0.05744713544845581\n",
      "Epoch 9671: train loss: 0.01996016502380371, val loss: 0.07212326675653458\n",
      "Epoch 9672: train loss: 0.01747564598917961, val loss: 0.09118539839982986\n",
      "Epoch 9673: train loss: 0.012177721597254276, val loss: 0.07952950149774551\n",
      "Epoch 9674: train loss: 0.013932236470282078, val loss: 0.07316045463085175\n",
      "Epoch 9675: train loss: 0.01538707036525011, val loss: 0.12019868195056915\n",
      "Epoch 9676: train loss: 0.013790475204586983, val loss: 0.07544384151697159\n",
      "Epoch 9677: train loss: 0.01673516258597374, val loss: 0.0745454728603363\n",
      "Epoch 9678: train loss: 0.018174301832914352, val loss: 0.07603207975625992\n",
      "Epoch 9679: train loss: 0.014881267212331295, val loss: 0.05211890488862991\n",
      "Epoch 9680: train loss: 0.015230058692395687, val loss: 0.06411126255989075\n",
      "Epoch 9681: train loss: 0.012630799785256386, val loss: 0.08091052621603012\n",
      "Epoch 9682: train loss: 0.016017865389585495, val loss: 0.0724843293428421\n",
      "Epoch 9683: train loss: 0.01630481891334057, val loss: 0.07940101623535156\n",
      "Epoch 9684: train loss: 0.011307589709758759, val loss: 0.05154929310083389\n",
      "Epoch 9685: train loss: 0.017722636461257935, val loss: 0.09741640836000443\n",
      "Epoch 9686: train loss: 0.017993394285440445, val loss: 0.052811313420534134\n",
      "Epoch 9687: train loss: 0.013917069882154465, val loss: 0.03872668370604515\n",
      "Epoch 9688: train loss: 0.015861844643950462, val loss: 0.07839839905500412\n",
      "Epoch 9689: train loss: 0.018289979547262192, val loss: 0.04494312033057213\n",
      "Epoch 9690: train loss: 0.01769472099840641, val loss: 0.07824200391769409\n",
      "Epoch 9691: train loss: 0.020369328558444977, val loss: 0.08438652753829956\n",
      "Epoch 9692: train loss: 0.017798198387026787, val loss: 0.07566314190626144\n",
      "Epoch 9693: train loss: 0.015430519357323647, val loss: 0.09385670721530914\n",
      "Epoch 9694: train loss: 0.018795525655150414, val loss: 0.07799743860960007\n",
      "Epoch 9695: train loss: 0.0185107234865427, val loss: 0.05774425342679024\n",
      "Epoch 9696: train loss: 0.022142015397548676, val loss: 0.051297176629304886\n",
      "Epoch 9697: train loss: 0.015447909943759441, val loss: 0.05968508869409561\n",
      "Epoch 9698: train loss: 0.01411210186779499, val loss: 0.1024622917175293\n",
      "Epoch 9699: train loss: 0.012191928923130035, val loss: 0.11992201954126358\n",
      "Epoch 9700: train loss: 0.016106242313981056, val loss: 0.0797448605298996\n",
      "Epoch 9701: train loss: 0.01661108434200287, val loss: 0.07905065268278122\n",
      "Epoch 9702: train loss: 0.015978381037712097, val loss: 0.07950877398252487\n",
      "Epoch 9703: train loss: 0.019776444882154465, val loss: 0.06645667552947998\n",
      "Epoch 9704: train loss: 0.01196029782295227, val loss: 0.08391191810369492\n",
      "Epoch 9705: train loss: 0.016256138682365417, val loss: 0.0741071030497551\n",
      "Epoch 9706: train loss: 0.020588064566254616, val loss: 0.0987691730260849\n",
      "Epoch 9707: train loss: 0.01830396056175232, val loss: 0.07720585912466049\n",
      "Epoch 9708: train loss: 0.016172409057617188, val loss: 0.10941696166992188\n",
      "Epoch 9709: train loss: 0.021742654964327812, val loss: 0.04436168074607849\n",
      "Epoch 9710: train loss: 0.020835699513554573, val loss: 0.06357310712337494\n",
      "Epoch 9711: train loss: 0.016863038763403893, val loss: 0.093013696372509\n",
      "Epoch 9712: train loss: 0.017508938908576965, val loss: 0.06562985479831696\n",
      "Epoch 9713: train loss: 0.01850469969213009, val loss: 0.1303422898054123\n",
      "Epoch 9714: train loss: 0.021815016865730286, val loss: 0.08434420078992844\n",
      "Epoch 9715: train loss: 0.01383539754897356, val loss: 0.09104454517364502\n",
      "Epoch 9716: train loss: 0.014798546209931374, val loss: 0.08150782436132431\n",
      "Epoch 9717: train loss: 0.02681247517466545, val loss: 0.06987681239843369\n",
      "Epoch 9718: train loss: 0.01706729643046856, val loss: 0.07234998047351837\n",
      "Epoch 9719: train loss: 0.012116067111492157, val loss: 0.055800676345825195\n",
      "Epoch 9720: train loss: 0.014896949753165245, val loss: 0.05755869299173355\n",
      "Epoch 9721: train loss: 0.024329079315066338, val loss: 0.11785902827978134\n",
      "Epoch 9722: train loss: 0.015113791450858116, val loss: 0.08041967451572418\n",
      "Epoch 9723: train loss: 0.014305232092738152, val loss: 0.06674877554178238\n",
      "Epoch 9724: train loss: 0.014511958695948124, val loss: 0.09202029556035995\n",
      "Epoch 9725: train loss: 0.01807381398975849, val loss: 0.05902343615889549\n",
      "Epoch 9726: train loss: 0.024777386337518692, val loss: 0.11962413787841797\n",
      "Epoch 9727: train loss: 0.020683806389570236, val loss: 0.05300847813487053\n",
      "Epoch 9728: train loss: 0.019926665350794792, val loss: 0.12259473651647568\n",
      "Epoch 9729: train loss: 0.018767360597848892, val loss: 0.0969369187951088\n",
      "Epoch 9730: train loss: 0.026172442361712456, val loss: 0.07574702054262161\n",
      "Epoch 9731: train loss: 0.014183039776980877, val loss: 0.05619560554623604\n",
      "Epoch 9732: train loss: 0.01602068543434143, val loss: 0.05241115018725395\n",
      "Epoch 9733: train loss: 0.013573814183473587, val loss: 0.09237927198410034\n",
      "Epoch 9734: train loss: 0.020655229687690735, val loss: 0.08541673421859741\n",
      "Epoch 9735: train loss: 0.01295553334057331, val loss: 0.08879069238901138\n",
      "Epoch 9736: train loss: 0.011894682422280312, val loss: 0.08499332517385483\n",
      "Epoch 9737: train loss: 0.015922430902719498, val loss: 0.07057473808526993\n",
      "Epoch 9738: train loss: 0.017294783145189285, val loss: 0.054041147232055664\n",
      "Epoch 9739: train loss: 0.014482683502137661, val loss: 0.06532038748264313\n",
      "Epoch 9740: train loss: 0.014808233827352524, val loss: 0.09965243935585022\n",
      "Epoch 9741: train loss: 0.018039019778370857, val loss: 0.0558197982609272\n",
      "Epoch 9742: train loss: 0.01091350894421339, val loss: 0.046124834567308426\n",
      "Epoch 9743: train loss: 0.022017531096935272, val loss: 0.09305661916732788\n",
      "Epoch 9744: train loss: 0.021268518641591072, val loss: 0.05511654540896416\n",
      "Epoch 9745: train loss: 0.018810445442795753, val loss: 0.06081375479698181\n",
      "Epoch 9746: train loss: 0.01756662130355835, val loss: 0.05616239830851555\n",
      "Epoch 9747: train loss: 0.02250596694648266, val loss: 0.07223277539014816\n",
      "Epoch 9748: train loss: 0.01736760139465332, val loss: 0.04545437917113304\n",
      "Epoch 9749: train loss: 0.01503995805978775, val loss: 0.08335739374160767\n",
      "Epoch 9750: train loss: 0.015341708436608315, val loss: 0.09436299651861191\n",
      "Epoch 9751: train loss: 0.016882136464118958, val loss: 0.08151786029338837\n",
      "Epoch 9752: train loss: 0.01172344759106636, val loss: 0.0388578325510025\n",
      "Epoch 9753: train loss: 0.01598672941327095, val loss: 0.0664782002568245\n",
      "Epoch 9754: train loss: 0.01949647068977356, val loss: 0.0715441107749939\n",
      "Epoch 9755: train loss: 0.013766513206064701, val loss: 0.056291352957487106\n",
      "Epoch 9756: train loss: 0.015581769868731499, val loss: 0.12154573202133179\n",
      "Epoch 9757: train loss: 0.01777416653931141, val loss: 0.06462842971086502\n",
      "Epoch 9758: train loss: 0.024846462532877922, val loss: 0.062467582523822784\n",
      "Epoch 9759: train loss: 0.02064419724047184, val loss: 0.11662500351667404\n",
      "Epoch 9760: train loss: 0.01450264360755682, val loss: 0.0648302510380745\n",
      "Epoch 9761: train loss: 0.017106488347053528, val loss: 0.1004415899515152\n",
      "Epoch 9762: train loss: 0.016354240477085114, val loss: 0.06001542881131172\n",
      "Epoch 9763: train loss: 0.022588344290852547, val loss: 0.07063867896795273\n",
      "Epoch 9764: train loss: 0.019776638597249985, val loss: 0.12372245639562607\n",
      "Epoch 9765: train loss: 0.01226808037608862, val loss: 0.08210957050323486\n",
      "Epoch 9766: train loss: 0.014353839680552483, val loss: 0.05146702006459236\n",
      "Epoch 9767: train loss: 0.019486146047711372, val loss: 0.08334747701883316\n",
      "Epoch 9768: train loss: 0.011292538605630398, val loss: 0.06458131223917007\n",
      "Epoch 9769: train loss: 0.010181162506341934, val loss: 0.059654127806425095\n",
      "Epoch 9770: train loss: 0.012976481579244137, val loss: 0.08106866478919983\n",
      "Epoch 9771: train loss: 0.015073929913341999, val loss: 0.07163222879171371\n",
      "Epoch 9772: train loss: 0.021143818274140358, val loss: 0.05586642026901245\n",
      "Epoch 9773: train loss: 0.011927451007068157, val loss: 0.07665648311376572\n",
      "Epoch 9774: train loss: 0.01869037002325058, val loss: 0.06244852766394615\n",
      "Epoch 9775: train loss: 0.01500529982149601, val loss: 0.06604256480932236\n",
      "Epoch 9776: train loss: 0.015015186741948128, val loss: 0.07093469053506851\n",
      "Epoch 9777: train loss: 0.017120633274316788, val loss: 0.0771685242652893\n",
      "Epoch 9778: train loss: 0.012648504227399826, val loss: 0.06510332971811295\n",
      "Epoch 9779: train loss: 0.01806398667395115, val loss: 0.05769602209329605\n",
      "Epoch 9780: train loss: 0.017284885048866272, val loss: 0.06994077563285828\n",
      "Epoch 9781: train loss: 0.020264921709895134, val loss: 0.10018952190876007\n",
      "Epoch 9782: train loss: 0.012798042967915535, val loss: 0.07277936488389969\n",
      "Epoch 9783: train loss: 0.023101653903722763, val loss: 0.1299145221710205\n",
      "Epoch 9784: train loss: 0.017736278474330902, val loss: 0.10067560523748398\n",
      "Epoch 9785: train loss: 0.016316991299390793, val loss: 0.07456731051206589\n",
      "Epoch 9786: train loss: 0.015865443274378777, val loss: 0.05293840914964676\n",
      "Epoch 9787: train loss: 0.01793019287288189, val loss: 0.08473750948905945\n",
      "Epoch 9788: train loss: 0.0190216526389122, val loss: 0.0728631392121315\n",
      "Epoch 9789: train loss: 0.015762902796268463, val loss: 0.05470609664916992\n",
      "Epoch 9790: train loss: 0.017106330022215843, val loss: 0.07740994542837143\n",
      "Epoch 9791: train loss: 0.01919522136449814, val loss: 0.09321626275777817\n",
      "Epoch 9792: train loss: 0.013804852031171322, val loss: 0.07014735788106918\n",
      "Epoch 9793: train loss: 0.015110482461750507, val loss: 0.07892348617315292\n",
      "Epoch 9794: train loss: 0.017579736188054085, val loss: 0.05922621488571167\n",
      "Epoch 9795: train loss: 0.011991584673523903, val loss: 0.07483063638210297\n",
      "Epoch 9796: train loss: 0.014785784296691418, val loss: 0.06651201844215393\n",
      "Epoch 9797: train loss: 0.01725824363529682, val loss: 0.060439687222242355\n",
      "Epoch 9798: train loss: 0.011915224604308605, val loss: 0.1073877140879631\n",
      "Epoch 9799: train loss: 0.016028497368097305, val loss: 0.0587504617869854\n",
      "Epoch 9800: train loss: 0.013619299046695232, val loss: 0.0667683407664299\n",
      "Epoch 9801: train loss: 0.015447148121893406, val loss: 0.05859679728746414\n",
      "Epoch 9802: train loss: 0.0141105055809021, val loss: 0.05021475628018379\n",
      "Epoch 9803: train loss: 0.015723856166005135, val loss: 0.10233556479215622\n",
      "Epoch 9804: train loss: 0.013428328558802605, val loss: 0.04538460075855255\n",
      "Epoch 9805: train loss: 0.017076000571250916, val loss: 0.08077072352170944\n",
      "Epoch 9806: train loss: 0.019274545833468437, val loss: 0.1065070629119873\n",
      "Epoch 9807: train loss: 0.017574919387698174, val loss: 0.07861735671758652\n",
      "Epoch 9808: train loss: 0.016780482605099678, val loss: 0.05523232743144035\n",
      "Epoch 9809: train loss: 0.022928794845938683, val loss: 0.07346243411302567\n",
      "Epoch 9810: train loss: 0.013485468924045563, val loss: 0.07342499494552612\n",
      "Epoch 9811: train loss: 0.014882167801260948, val loss: 0.07392790168523788\n",
      "Epoch 9812: train loss: 0.015756966546177864, val loss: 0.04916144162416458\n",
      "Epoch 9813: train loss: 0.017507418990135193, val loss: 0.04984500631690025\n",
      "Epoch 9814: train loss: 0.011744046583771706, val loss: 0.10125666856765747\n",
      "Epoch 9815: train loss: 0.015428506769239902, val loss: 0.11382400244474411\n",
      "Epoch 9816: train loss: 0.01591596193611622, val loss: 0.11109752953052521\n",
      "Epoch 9817: train loss: 0.015186409465968609, val loss: 0.08442852646112442\n",
      "Epoch 9818: train loss: 0.01722659543156624, val loss: 0.06387344002723694\n",
      "Epoch 9819: train loss: 0.017820460721850395, val loss: 0.061135511845350266\n",
      "Epoch 9820: train loss: 0.014528580941259861, val loss: 0.063442163169384\n",
      "Epoch 9821: train loss: 0.014852222055196762, val loss: 0.0738685354590416\n",
      "Epoch 9822: train loss: 0.01128525473177433, val loss: 0.09670235961675644\n",
      "Epoch 9823: train loss: 0.01730945333838463, val loss: 0.06105886772274971\n",
      "Epoch 9824: train loss: 0.0095630818977952, val loss: 0.08917799592018127\n",
      "Epoch 9825: train loss: 0.01593140885233879, val loss: 0.092952661216259\n",
      "Epoch 9826: train loss: 0.020719796419143677, val loss: 0.09283965080976486\n",
      "Epoch 9827: train loss: 0.02666427381336689, val loss: 0.06082966551184654\n",
      "Epoch 9828: train loss: 0.014334521256387234, val loss: 0.07781221717596054\n",
      "Epoch 9829: train loss: 0.020662205293774605, val loss: 0.06893599033355713\n",
      "Epoch 9830: train loss: 0.016585825011134148, val loss: 0.0775173157453537\n",
      "Epoch 9831: train loss: 0.02150503359735012, val loss: 0.05421619489789009\n",
      "Epoch 9832: train loss: 0.01649302989244461, val loss: 0.07362838834524155\n",
      "Epoch 9833: train loss: 0.01648106426000595, val loss: 0.06661070138216019\n",
      "Epoch 9834: train loss: 0.013479383662343025, val loss: 0.10820966213941574\n",
      "Epoch 9835: train loss: 0.016710178926587105, val loss: 0.08123656362295151\n",
      "Epoch 9836: train loss: 0.018506444990634918, val loss: 0.05922368913888931\n",
      "Epoch 9837: train loss: 0.01982150599360466, val loss: 0.07249262183904648\n",
      "Epoch 9838: train loss: 0.013423124328255653, val loss: 0.0696972906589508\n",
      "Epoch 9839: train loss: 0.01425368245691061, val loss: 0.07793617993593216\n",
      "Epoch 9840: train loss: 0.012934695929288864, val loss: 0.061948973685503006\n",
      "Epoch 9841: train loss: 0.016542727127671242, val loss: 0.03678998351097107\n",
      "Epoch 9842: train loss: 0.017801159992814064, val loss: 0.07309408485889435\n",
      "Epoch 9843: train loss: 0.0223331768065691, val loss: 0.10569975525140762\n",
      "Epoch 9844: train loss: 0.018506985157728195, val loss: 0.07303974777460098\n",
      "Epoch 9845: train loss: 0.017557717859745026, val loss: 0.07880260795354843\n",
      "Epoch 9846: train loss: 0.016677923500537872, val loss: 0.06785372644662857\n",
      "Epoch 9847: train loss: 0.018417704850435257, val loss: 0.05933290719985962\n",
      "Epoch 9848: train loss: 0.015257812105119228, val loss: 0.06850985437631607\n",
      "Epoch 9849: train loss: 0.01765977032482624, val loss: 0.10702428966760635\n",
      "Epoch 9850: train loss: 0.017276646569371223, val loss: 0.10124921798706055\n",
      "Epoch 9851: train loss: 0.011908452957868576, val loss: 0.0960034728050232\n",
      "Epoch 9852: train loss: 0.017525218427181244, val loss: 0.050042856484651566\n",
      "Epoch 9853: train loss: 0.013522367924451828, val loss: 0.09801675379276276\n",
      "Epoch 9854: train loss: 0.019178641960024834, val loss: 0.051211606711149216\n",
      "Epoch 9855: train loss: 0.015258913859724998, val loss: 0.09071388840675354\n",
      "Epoch 9856: train loss: 0.01994859240949154, val loss: 0.0867985412478447\n",
      "Epoch 9857: train loss: 0.02041776292026043, val loss: 0.08060882985591888\n",
      "Epoch 9858: train loss: 0.015270303934812546, val loss: 0.07263244688510895\n",
      "Epoch 9859: train loss: 0.013306714594364166, val loss: 0.06261195987462997\n",
      "Epoch 9860: train loss: 0.02019009366631508, val loss: 0.05782833322882652\n",
      "Epoch 9861: train loss: 0.021694647148251534, val loss: 0.11081399023532867\n",
      "Epoch 9862: train loss: 0.015870433300733566, val loss: 0.08464749157428741\n",
      "Epoch 9863: train loss: 0.023342175409197807, val loss: 0.07846689224243164\n",
      "Epoch 9864: train loss: 0.016713397577404976, val loss: 0.05621085315942764\n",
      "Epoch 9865: train loss: 0.019638733938336372, val loss: 0.04898368567228317\n",
      "Epoch 9866: train loss: 0.011104344390332699, val loss: 0.08142133802175522\n",
      "Epoch 9867: train loss: 0.013886605389416218, val loss: 0.044370006769895554\n",
      "Epoch 9868: train loss: 0.013768029399216175, val loss: 0.11125009506940842\n",
      "Epoch 9869: train loss: 0.011010045185685158, val loss: 0.07475128024816513\n",
      "Epoch 9870: train loss: 0.021564722061157227, val loss: 0.046935226768255234\n",
      "Epoch 9871: train loss: 0.014142188243567944, val loss: 0.09772690385580063\n",
      "Epoch 9872: train loss: 0.0182067658752203, val loss: 0.10013293474912643\n",
      "Epoch 9873: train loss: 0.011266960762441158, val loss: 0.1047186404466629\n",
      "Epoch 9874: train loss: 0.013220607303082943, val loss: 0.09306123107671738\n",
      "Epoch 9875: train loss: 0.010963046923279762, val loss: 0.07281095534563065\n",
      "Epoch 9876: train loss: 0.01628650166094303, val loss: 0.06374221295118332\n",
      "Epoch 9877: train loss: 0.016076140105724335, val loss: 0.051308512687683105\n",
      "Epoch 9878: train loss: 0.02622787468135357, val loss: 0.05412386730313301\n",
      "Epoch 9879: train loss: 0.01514350064098835, val loss: 0.08680716902017593\n",
      "Epoch 9880: train loss: 0.013314587995409966, val loss: 0.11198700964450836\n",
      "Epoch 9881: train loss: 0.012600619345903397, val loss: 0.08909767866134644\n",
      "Epoch 9882: train loss: 0.018825696781277657, val loss: 0.07273557037115097\n",
      "Epoch 9883: train loss: 0.014717254787683487, val loss: 0.05750897154211998\n",
      "Epoch 9884: train loss: 0.014687612652778625, val loss: 0.040766116231679916\n",
      "Epoch 9885: train loss: 0.016508927568793297, val loss: 0.04774469882249832\n",
      "Epoch 9886: train loss: 0.01176857203245163, val loss: 0.06437812000513077\n",
      "Epoch 9887: train loss: 0.011134064756333828, val loss: 0.08552373200654984\n",
      "Epoch 9888: train loss: 0.015344972722232342, val loss: 0.055188652127981186\n",
      "Epoch 9889: train loss: 0.01019688043743372, val loss: 0.06881692260503769\n",
      "Epoch 9890: train loss: 0.015346638858318329, val loss: 0.07156580686569214\n",
      "Epoch 9891: train loss: 0.018680516630411148, val loss: 0.0935610830783844\n",
      "Epoch 9892: train loss: 0.014790606684982777, val loss: 0.07846754044294357\n",
      "Epoch 9893: train loss: 0.016794173046946526, val loss: 0.06309982389211655\n",
      "Epoch 9894: train loss: 0.014226309955120087, val loss: 0.05924845486879349\n",
      "Epoch 9895: train loss: 0.018315093591809273, val loss: 0.07010481506586075\n",
      "Epoch 9896: train loss: 0.013672090135514736, val loss: 0.0647045448422432\n",
      "Epoch 9897: train loss: 0.011668860912322998, val loss: 0.07792554050683975\n",
      "Epoch 9898: train loss: 0.018636532127857208, val loss: 0.10144062340259552\n",
      "Epoch 9899: train loss: 0.01803920790553093, val loss: 0.09222656488418579\n",
      "Epoch 9900: train loss: 0.016601964831352234, val loss: 0.05153103545308113\n",
      "Epoch 9901: train loss: 0.02661677449941635, val loss: 0.10176339000463486\n",
      "Epoch 9902: train loss: 0.014875265769660473, val loss: 0.06742159277200699\n",
      "Epoch 9903: train loss: 0.015778344124555588, val loss: 0.07853373140096664\n",
      "Epoch 9904: train loss: 0.013007222674787045, val loss: 0.08088020980358124\n",
      "Epoch 9905: train loss: 0.01334757823497057, val loss: 0.04732755944132805\n",
      "Epoch 9906: train loss: 0.020193638280034065, val loss: 0.07124616205692291\n",
      "Epoch 9907: train loss: 0.017335891723632812, val loss: 0.10848045349121094\n",
      "Epoch 9908: train loss: 0.016769399866461754, val loss: 0.056464482098817825\n",
      "Epoch 9909: train loss: 0.015375804156064987, val loss: 0.07348180562257767\n",
      "Epoch 9910: train loss: 0.017321951687335968, val loss: 0.09997396171092987\n",
      "Epoch 9911: train loss: 0.015401792712509632, val loss: 0.08324990421533585\n",
      "Epoch 9912: train loss: 0.015991637483239174, val loss: 0.07580398768186569\n",
      "Epoch 9913: train loss: 0.013112286105751991, val loss: 0.1070004254579544\n",
      "Epoch 9914: train loss: 0.015760522335767746, val loss: 0.08675016462802887\n",
      "Epoch 9915: train loss: 0.014134794473648071, val loss: 0.09110689163208008\n",
      "Epoch 9916: train loss: 0.015504167415201664, val loss: 0.09227582067251205\n",
      "Epoch 9917: train loss: 0.017999112606048584, val loss: 0.051623668521642685\n",
      "Epoch 9918: train loss: 0.0134919174015522, val loss: 0.0438697375357151\n",
      "Epoch 9919: train loss: 0.019542111083865166, val loss: 0.09087196737527847\n",
      "Epoch 9920: train loss: 0.018372470512986183, val loss: 0.05449000746011734\n",
      "Epoch 9921: train loss: 0.015450599603354931, val loss: 0.06328878551721573\n",
      "Epoch 9922: train loss: 0.015654180198907852, val loss: 0.06881431490182877\n",
      "Epoch 9923: train loss: 0.017898036167025566, val loss: 0.08750955015420914\n",
      "Epoch 9924: train loss: 0.026820320636034012, val loss: 0.08611946552991867\n",
      "Epoch 9925: train loss: 0.018805209547281265, val loss: 0.06585278362035751\n",
      "Epoch 9926: train loss: 0.013800723478198051, val loss: 0.08482347428798676\n",
      "Epoch 9927: train loss: 0.015614597126841545, val loss: 0.08209063857793808\n",
      "Epoch 9928: train loss: 0.014386376366019249, val loss: 0.08683500438928604\n",
      "Epoch 9929: train loss: 0.016107438132166862, val loss: 0.0865134447813034\n",
      "Epoch 9930: train loss: 0.015257557854056358, val loss: 0.11517366021871567\n",
      "Epoch 9931: train loss: 0.011452640406787395, val loss: 0.07034209370613098\n",
      "Epoch 9932: train loss: 0.02106207236647606, val loss: 0.0962877944111824\n",
      "Epoch 9933: train loss: 0.01587928831577301, val loss: 0.05025447532534599\n",
      "Epoch 9934: train loss: 0.014106959104537964, val loss: 0.09929555654525757\n",
      "Epoch 9935: train loss: 0.017817730084061623, val loss: 0.07900715619325638\n",
      "Epoch 9936: train loss: 0.012805875390768051, val loss: 0.04501483961939812\n",
      "Epoch 9937: train loss: 0.016369653865695, val loss: 0.07698521763086319\n",
      "Epoch 9938: train loss: 0.014709781855344772, val loss: 0.052443403750658035\n",
      "Epoch 9939: train loss: 0.014530873857438564, val loss: 0.06796184927225113\n",
      "Epoch 9940: train loss: 0.010271764360368252, val loss: 0.06058957800269127\n",
      "Epoch 9941: train loss: 0.025252116844058037, val loss: 0.08338066190481186\n",
      "Epoch 9942: train loss: 0.01515644695609808, val loss: 0.09310745447874069\n",
      "Epoch 9943: train loss: 0.019605999812483788, val loss: 0.06393823772668839\n",
      "Epoch 9944: train loss: 0.01780632883310318, val loss: 0.11688927561044693\n",
      "Epoch 9945: train loss: 0.013524427078664303, val loss: 0.11041241884231567\n",
      "Epoch 9946: train loss: 0.014824001118540764, val loss: 0.047827575355768204\n",
      "Epoch 9947: train loss: 0.021566340699791908, val loss: 0.057612646371126175\n",
      "Epoch 9948: train loss: 0.015497599728405476, val loss: 0.07681053876876831\n",
      "Epoch 9949: train loss: 0.019787002354860306, val loss: 0.08116310834884644\n",
      "Epoch 9950: train loss: 0.012121008709073067, val loss: 0.055006157606840134\n",
      "Epoch 9951: train loss: 0.03399671986699104, val loss: 0.05669828876852989\n",
      "Epoch 9952: train loss: 0.01200919970870018, val loss: 0.10723967850208282\n",
      "Epoch 9953: train loss: 0.015692617744207382, val loss: 0.07269507646560669\n",
      "Epoch 9954: train loss: 0.012957271188497543, val loss: 0.06409110128879547\n",
      "Epoch 9955: train loss: 0.013553128577768803, val loss: 0.06280063092708588\n",
      "Epoch 9956: train loss: 0.0169572364538908, val loss: 0.0770931988954544\n",
      "Epoch 9957: train loss: 0.01878860779106617, val loss: 0.09574170410633087\n",
      "Epoch 9958: train loss: 0.012869263999164104, val loss: 0.06277593225240707\n",
      "Epoch 9959: train loss: 0.015650514513254166, val loss: 0.04030955955386162\n",
      "Epoch 9960: train loss: 0.01878168061375618, val loss: 0.09422316402196884\n",
      "Epoch 9961: train loss: 0.01647379994392395, val loss: 0.04779375344514847\n",
      "Epoch 9962: train loss: 0.01603078283369541, val loss: 0.08734087646007538\n",
      "Epoch 9963: train loss: 0.01419520378112793, val loss: 0.08380318433046341\n",
      "Epoch 9964: train loss: 0.014204738661646843, val loss: 0.0398390106856823\n",
      "Epoch 9965: train loss: 0.02164696902036667, val loss: 0.08918505907058716\n",
      "Epoch 9966: train loss: 0.014724965207278728, val loss: 0.10198657959699631\n",
      "Epoch 9967: train loss: 0.018951736390590668, val loss: 0.07320583611726761\n",
      "Epoch 9968: train loss: 0.018283801153302193, val loss: 0.0911235436797142\n",
      "Epoch 9969: train loss: 0.013441423885524273, val loss: 0.09641509503126144\n",
      "Epoch 9970: train loss: 0.012504968792200089, val loss: 0.08051280677318573\n",
      "Epoch 9971: train loss: 0.01311136782169342, val loss: 0.057689983397722244\n",
      "Epoch 9972: train loss: 0.015298701822757721, val loss: 0.07418154180049896\n",
      "Epoch 9973: train loss: 0.0194076020270586, val loss: 0.05027594789862633\n",
      "Epoch 9974: train loss: 0.0175523292273283, val loss: 0.09373798221349716\n",
      "Epoch 9975: train loss: 0.012735271826386452, val loss: 0.08850421756505966\n",
      "Epoch 9976: train loss: 0.023827897384762764, val loss: 0.11087153106927872\n",
      "Epoch 9977: train loss: 0.021795259788632393, val loss: 0.0879039391875267\n",
      "Epoch 9978: train loss: 0.02185715176165104, val loss: 0.05010459944605827\n",
      "Epoch 9979: train loss: 0.022276122123003006, val loss: 0.04441094025969505\n",
      "Epoch 9980: train loss: 0.01976081356406212, val loss: 0.06515531986951828\n",
      "Epoch 9981: train loss: 0.01914237067103386, val loss: 0.06225445494055748\n",
      "Epoch 9982: train loss: 0.012045549228787422, val loss: 0.07053949683904648\n",
      "Epoch 9983: train loss: 0.013850494287908077, val loss: 0.05019461736083031\n",
      "Epoch 9984: train loss: 0.02156445011496544, val loss: 0.06788235902786255\n",
      "Epoch 9985: train loss: 0.02606699801981449, val loss: 0.05578210577368736\n",
      "Epoch 9986: train loss: 0.022936999797821045, val loss: 0.05370834097266197\n",
      "Epoch 9987: train loss: 0.018116721883416176, val loss: 0.0657716765999794\n",
      "Epoch 9988: train loss: 0.015389231964945793, val loss: 0.07916722446680069\n",
      "Epoch 9989: train loss: 0.013124634511768818, val loss: 0.064397431910038\n",
      "Epoch 9990: train loss: 0.016502471640706062, val loss: 0.061851777136325836\n",
      "Epoch 9991: train loss: 0.016287781298160553, val loss: 0.062234487384557724\n",
      "Epoch 9992: train loss: 0.016288917511701584, val loss: 0.060853224247694016\n",
      "Epoch 9993: train loss: 0.022761112079024315, val loss: 0.06425929069519043\n",
      "Epoch 9994: train loss: 0.020335784181952477, val loss: 0.06544859707355499\n",
      "Epoch 9995: train loss: 0.01795678399503231, val loss: 0.07482252269983292\n",
      "Epoch 9996: train loss: 0.02228911779820919, val loss: 0.06564755737781525\n",
      "Epoch 9997: train loss: 0.01619209349155426, val loss: 0.10459695011377335\n",
      "Epoch 9998: train loss: 0.01524650864303112, val loss: 0.08777632564306259\n",
      "Epoch 9999: train loss: 0.014023058116436005, val loss: 0.07581748068332672\n",
      "Epoch 10000: train loss: 0.01600291021168232, val loss: 0.06320804357528687\n",
      "Predictions saved to results-upgrade.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "x_train_featured = pretrainedfeatures.transform(x_train).detach().cpu().numpy()\n",
    "# scaler = StandardScaler()\n",
    "# x_train_featured = scaler.fit_transform(x_train_featured)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())#.detach().cpu().numpy()\n",
    "# x_test_featured = scaler.transform(x_test_featured)\n",
    "# x_test_featured = torch.tensor(x_test_featured, dtype=torch.float).to(device)\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
