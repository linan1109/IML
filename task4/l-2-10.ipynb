{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-2-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e8df090c914b2283e106e51335a346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.5954902377785468, val loss: 0.23665323829650878\n",
      "Epoch 2: train loss: 0.19617454752143548, val loss: 0.19263132965564728\n",
      "Epoch 3: train loss: 0.16552482597438656, val loss: 0.15475986802577973\n",
      "Epoch 4: train loss: 0.14926622399748588, val loss: 0.14627709543704986\n",
      "Epoch 5: train loss: 0.13702247945751464, val loss: 0.12092979067564011\n",
      "Epoch 6: train loss: 0.12035149745916833, val loss: 0.11390004742145539\n",
      "Epoch 7: train loss: 0.11203349179029465, val loss: 0.1199230152964592\n",
      "Epoch 8: train loss: 0.10277247510759198, val loss: 0.08955838787555695\n",
      "Epoch 9: train loss: 0.09251854254639878, val loss: 0.09124751389026642\n",
      "Epoch 10: train loss: 0.0843619908435004, val loss: 0.07894353473186493\n",
      "Epoch 11: train loss: 0.07618013058389936, val loss: 0.07278787469863891\n",
      "Epoch 12: train loss: 0.06838532296857056, val loss: 0.06438391077518463\n",
      "Epoch 13: train loss: 0.06148890853293088, val loss: 0.07076709592342377\n",
      "Epoch 14: train loss: 0.05417832299215453, val loss: 0.06483159446716309\n",
      "Epoch 15: train loss: 0.04936509453216378, val loss: 0.04532818195223808\n",
      "Epoch 16: train loss: 0.04133602129744024, val loss: 0.03675540181994438\n",
      "Epoch 17: train loss: 0.03546800123550454, val loss: 0.03095203472673893\n",
      "Epoch 18: train loss: 0.030487719610029336, val loss: 0.0387946560382843\n",
      "Epoch 19: train loss: 0.026691113919932015, val loss: 0.02409565483033657\n",
      "Epoch 20: train loss: 0.022045912044693014, val loss: 0.023275944530963898\n",
      "Epoch 21: train loss: 0.018900516133831473, val loss: 0.018265634432435035\n",
      "Epoch 22: train loss: 0.016796335540863933, val loss: 0.015848154366016387\n",
      "Epoch 23: train loss: 0.014188168370297977, val loss: 0.013413684405386449\n",
      "Epoch 24: train loss: 0.012412107737240743, val loss: 0.012905364021658898\n",
      "Epoch 25: train loss: 0.01092030558948006, val loss: 0.010475209154188633\n",
      "Epoch 26: train loss: 0.009683438043965369, val loss: 0.009688789144158364\n",
      "Epoch 27: train loss: 0.008813393480011394, val loss: 0.008825302578508854\n",
      "Epoch 28: train loss: 0.00806178466413094, val loss: 0.008058583185076714\n",
      "Epoch 29: train loss: 0.00743904960467195, val loss: 0.00676725596934557\n",
      "Epoch 30: train loss: 0.007190356571759497, val loss: 0.007739739567041397\n",
      "Epoch 31: train loss: 0.00684667188187643, val loss: 0.0067864318788051605\n",
      "Epoch 32: train loss: 0.006490639345986503, val loss: 0.006644611954689026\n",
      "Epoch 33: train loss: 0.006408347402756312, val loss: 0.0074486019499599935\n",
      "Epoch 34: train loss: 0.0061723386002894565, val loss: 0.006542756039649248\n",
      "Epoch 35: train loss: 0.006331649417399752, val loss: 0.006962070949375629\n",
      "Epoch 36: train loss: 0.006123802745828823, val loss: 0.006258592758327722\n",
      "Epoch 37: train loss: 0.006199259226039356, val loss: 0.006239505633711815\n",
      "Epoch 38: train loss: 0.006114274701673765, val loss: 0.006966232940554619\n",
      "Epoch 39: train loss: 0.005972587134156908, val loss: 0.006378606915473938\n",
      "Epoch 40: train loss: 0.0059220325979499186, val loss: 0.007013367541134357\n",
      "Epoch 41: train loss: 0.006107534591184587, val loss: 0.0067209223583340645\n",
      "Epoch 42: train loss: 0.005912681209949815, val loss: 0.005892935153096915\n",
      "Epoch 43: train loss: 0.00595021332807991, val loss: 0.006132037945091724\n",
      "Epoch 44: train loss: 0.005870171646606557, val loss: 0.006123016364872455\n",
      "Epoch 45: train loss: 0.005615814136455254, val loss: 0.006522762440145016\n",
      "Epoch 46: train loss: 0.0057117285245687375, val loss: 0.005574065987020731\n",
      "Epoch 47: train loss: 0.005632287832167075, val loss: 0.007046211399137974\n",
      "Epoch 48: train loss: 0.005676545628023391, val loss: 0.005920186597853899\n",
      "Epoch 49: train loss: 0.005777350174225106, val loss: 0.006250154092907905\n",
      "Epoch 50: train loss: 0.005563759460056923, val loss: 0.00573181001842022\n",
      "Epoch 51: train loss: 0.005580734036102587, val loss: 0.005972554169595241\n",
      "Epoch 00052: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 52: train loss: 0.005545413037039796, val loss: 0.006564129449427128\n",
      "Epoch 53: train loss: 0.003974918287893643, val loss: 0.003857811711728573\n",
      "Epoch 54: train loss: 0.003203963913783735, val loss: 0.004678818002343178\n",
      "Epoch 55: train loss: 0.0031391803842342023, val loss: 0.003892980081960559\n",
      "Epoch 56: train loss: 0.003076664910898829, val loss: 0.004120422471314669\n",
      "Epoch 57: train loss: 0.003088866767644578, val loss: 0.0042047406304627655\n",
      "Epoch 58: train loss: 0.0030982718408031733, val loss: 0.003819725424051285\n",
      "Epoch 59: train loss: 0.0031424653502371237, val loss: 0.004166921544820071\n",
      "Epoch 60: train loss: 0.0030719790434730904, val loss: 0.0038519554827362297\n",
      "Epoch 61: train loss: 0.0031467545592815294, val loss: 0.003953453850001096\n",
      "Epoch 62: train loss: 0.0030267825827230603, val loss: 0.0043076581172645095\n",
      "Epoch 63: train loss: 0.002969114773203524, val loss: 0.00391323759034276\n",
      "Epoch 00064: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 64: train loss: 0.0031016523643415802, val loss: 0.0041734210159629585\n",
      "Epoch 65: train loss: 0.002528511868789792, val loss: 0.0034710712432861326\n",
      "Epoch 66: train loss: 0.0021984571148546375, val loss: 0.0033871901594102385\n",
      "Epoch 67: train loss: 0.0020989476792058165, val loss: 0.0032613706421107053\n",
      "Epoch 68: train loss: 0.0020443748601959373, val loss: 0.003401987439021468\n",
      "Epoch 69: train loss: 0.0020456584658169624, val loss: 0.003374688394367695\n",
      "Epoch 70: train loss: 0.0020157654202562205, val loss: 0.0035349044799804687\n",
      "Epoch 71: train loss: 0.002028841802736326, val loss: 0.0033815059270709755\n",
      "Epoch 72: train loss: 0.0020186772081924945, val loss: 0.0035172724295407533\n",
      "Epoch 00073: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 73: train loss: 0.0019722304648852776, val loss: 0.0033211820162832736\n",
      "Epoch 74: train loss: 0.0018264333978485392, val loss: 0.003205137848854065\n",
      "Epoch 75: train loss: 0.0017204148117832992, val loss: 0.0032543717585504055\n",
      "Epoch 76: train loss: 0.0017228300746302216, val loss: 0.0031502177324146032\n",
      "Epoch 77: train loss: 0.0016977917504477865, val loss: 0.0030661357156932354\n",
      "Epoch 78: train loss: 0.0016955633385639106, val loss: 0.0033713266383856534\n",
      "Epoch 79: train loss: 0.0016754854089219352, val loss: 0.0032693024948239327\n",
      "Epoch 80: train loss: 0.0016370604145435654, val loss: 0.003015246270224452\n",
      "Epoch 81: train loss: 0.0016535550561942616, val loss: 0.0028716601822525263\n",
      "Epoch 82: train loss: 0.0016229102344879386, val loss: 0.002982614077627659\n",
      "Epoch 83: train loss: 0.0016050928092504642, val loss: 0.00310527902841568\n",
      "Epoch 84: train loss: 0.0016108891667456042, val loss: 0.003274869253858924\n",
      "Epoch 85: train loss: 0.001599404487701855, val loss: 0.0032249400950968266\n",
      "Epoch 86: train loss: 0.0015965266836883158, val loss: 0.0029350366685539486\n",
      "Epoch 00087: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 87: train loss: 0.0015769326116851703, val loss: 0.003040372209623456\n",
      "Epoch 88: train loss: 0.0015458105828004831, val loss: 0.0030358479786664247\n",
      "Epoch 89: train loss: 0.001518880111887595, val loss: 0.0030867014322429895\n",
      "Epoch 90: train loss: 0.0015088959165601705, val loss: 0.002978595241904259\n",
      "Epoch 91: train loss: 0.0014877658365104272, val loss: 0.003049727814272046\n",
      "Epoch 92: train loss: 0.001500372751406869, val loss: 0.0030490791629999878\n",
      "Epoch 00093: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 93: train loss: 0.0014659242667827983, val loss: 0.0030291615799069404\n",
      "Epoch 94: train loss: 0.001469337314519347, val loss: 0.003162774920463562\n",
      "Epoch 95: train loss: 0.0014532190946177864, val loss: 0.0028873785361647605\n",
      "Epoch 96: train loss: 0.0014354304301419428, val loss: 0.0031702445484697818\n",
      "Epoch 97: train loss: 0.0014479424701243336, val loss: 0.0029732467718422413\n",
      "Epoch 98: train loss: 0.001474804702062844, val loss: 0.0029142607413232325\n",
      "Epoch 00099: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 99: train loss: 0.0014443536247411857, val loss: 0.002990253396332264\n",
      "Early stop at epoch 99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9134785e6f94430ba910b72a87d9ec52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 10.248974800109863\n",
      "Epoch 20: train loss: 9.8132963180542\n",
      "Epoch 30: train loss: 9.389999389648438\n",
      "Epoch 40: train loss: 8.9797945022583\n",
      "Epoch 50: train loss: 8.582974433898926\n",
      "Epoch 60: train loss: 8.199551582336426\n",
      "Epoch 70: train loss: 7.829376220703125\n",
      "Epoch 80: train loss: 7.472225189208984\n",
      "Epoch 90: train loss: 7.127838611602783\n",
      "Epoch 100: train loss: 6.7959418296813965\n",
      "Epoch 110: train loss: 6.476253986358643\n",
      "Epoch 120: train loss: 6.168496131896973\n",
      "Epoch 130: train loss: 5.872382640838623\n",
      "Epoch 140: train loss: 5.5876336097717285\n",
      "Epoch 150: train loss: 5.313969135284424\n",
      "Epoch 160: train loss: 5.051109313964844\n",
      "Epoch 170: train loss: 4.798775672912598\n",
      "Epoch 180: train loss: 4.556690692901611\n",
      "Epoch 190: train loss: 4.324579238891602\n",
      "Epoch 200: train loss: 4.102169036865234\n",
      "Epoch 210: train loss: 3.8891875743865967\n",
      "Epoch 220: train loss: 3.6853649616241455\n",
      "Epoch 230: train loss: 3.4904332160949707\n",
      "Epoch 240: train loss: 3.304126501083374\n",
      "Epoch 250: train loss: 3.126182794570923\n",
      "Epoch 260: train loss: 2.9563403129577637\n",
      "Epoch 270: train loss: 2.794342517852783\n",
      "Epoch 280: train loss: 2.6399343013763428\n",
      "Epoch 290: train loss: 2.492863178253174\n",
      "Epoch 300: train loss: 2.3528811931610107\n",
      "Epoch 310: train loss: 2.2197422981262207\n",
      "Epoch 320: train loss: 2.093205213546753\n",
      "Epoch 330: train loss: 1.9730312824249268\n",
      "Epoch 340: train loss: 1.8589870929718018\n",
      "Epoch 350: train loss: 1.7508413791656494\n",
      "Epoch 360: train loss: 1.6483677625656128\n",
      "Epoch 370: train loss: 1.5513445138931274\n",
      "Epoch 380: train loss: 1.459553837776184\n",
      "Epoch 390: train loss: 1.372781753540039\n",
      "Epoch 400: train loss: 1.2908196449279785\n",
      "Epoch 410: train loss: 1.213463544845581\n",
      "Epoch 420: train loss: 1.1405136585235596\n",
      "Epoch 430: train loss: 1.0717754364013672\n",
      "Epoch 440: train loss: 1.0070593357086182\n",
      "Epoch 450: train loss: 0.9461809396743774\n",
      "Epoch 460: train loss: 0.8889602422714233\n",
      "Epoch 470: train loss: 0.835222601890564\n",
      "Epoch 480: train loss: 0.7847992777824402\n",
      "Epoch 490: train loss: 0.7375257611274719\n",
      "Epoch 500: train loss: 0.6932430863380432\n",
      "Epoch 510: train loss: 0.6517974734306335\n",
      "Epoch 520: train loss: 0.6130402088165283\n",
      "Epoch 530: train loss: 0.5768284797668457\n",
      "Epoch 540: train loss: 0.5430236458778381\n",
      "Epoch 550: train loss: 0.5114930868148804\n",
      "Epoch 560: train loss: 0.48210883140563965\n",
      "Epoch 570: train loss: 0.4547482132911682\n",
      "Epoch 580: train loss: 0.4292934834957123\n",
      "Epoch 590: train loss: 0.40563178062438965\n",
      "Epoch 600: train loss: 0.38365522027015686\n",
      "Epoch 610: train loss: 0.3632607161998749\n",
      "Epoch 620: train loss: 0.34434974193573\n",
      "Epoch 630: train loss: 0.32682865858078003\n",
      "Epoch 640: train loss: 0.3106081783771515\n",
      "Epoch 650: train loss: 0.29560357332229614\n",
      "Epoch 660: train loss: 0.2817339599132538\n",
      "Epoch 670: train loss: 0.2689230144023895\n",
      "Epoch 680: train loss: 0.25709840655326843\n",
      "Epoch 690: train loss: 0.2461918592453003\n",
      "Epoch 700: train loss: 0.23613841831684113\n",
      "Epoch 710: train loss: 0.22687728703022003\n",
      "Epoch 720: train loss: 0.2183511108160019\n",
      "Epoch 730: train loss: 0.21050578355789185\n",
      "Epoch 740: train loss: 0.2032904028892517\n",
      "Epoch 750: train loss: 0.19665738940238953\n",
      "Epoch 760: train loss: 0.19056206941604614\n",
      "Epoch 770: train loss: 0.1849626898765564\n",
      "Epoch 780: train loss: 0.17982006072998047\n",
      "Epoch 790: train loss: 0.17509770393371582\n",
      "Epoch 800: train loss: 0.1707618236541748\n",
      "Epoch 810: train loss: 0.166780486702919\n",
      "Epoch 820: train loss: 0.16312438249588013\n",
      "Epoch 830: train loss: 0.15976624190807343\n",
      "Epoch 840: train loss: 0.15668059885501862\n",
      "Epoch 850: train loss: 0.15384401381015778\n",
      "Epoch 860: train loss: 0.15123483538627625\n",
      "Epoch 870: train loss: 0.14883290231227875\n",
      "Epoch 880: train loss: 0.1466197967529297\n",
      "Epoch 890: train loss: 0.14457844197750092\n",
      "Epoch 900: train loss: 0.1426931470632553\n",
      "Epoch 910: train loss: 0.1409495621919632\n",
      "Epoch 920: train loss: 0.13933442533016205\n",
      "Epoch 930: train loss: 0.1378355622291565\n",
      "Epoch 940: train loss: 0.13644194602966309\n",
      "Epoch 950: train loss: 0.13514344394207\n",
      "Epoch 960: train loss: 0.1339307725429535\n",
      "Epoch 970: train loss: 0.13279549777507782\n",
      "Epoch 980: train loss: 0.13172994554042816\n",
      "Epoch 990: train loss: 0.130727156996727\n",
      "Epoch 1000: train loss: 0.12978075444698334\n",
      "Epoch 1010: train loss: 0.12888503074645996\n",
      "Epoch 1020: train loss: 0.12803475558757782\n",
      "Epoch 1030: train loss: 0.12722522020339966\n",
      "Epoch 1040: train loss: 0.12645220756530762\n",
      "Epoch 1050: train loss: 0.12571187317371368\n",
      "Epoch 1060: train loss: 0.1250007152557373\n",
      "Epoch 1070: train loss: 0.12431571632623672\n",
      "Epoch 1080: train loss: 0.12365402281284332\n",
      "Epoch 1090: train loss: 0.12301316857337952\n",
      "Epoch 1100: train loss: 0.12239089608192444\n",
      "Epoch 1110: train loss: 0.1217852532863617\n",
      "Epoch 1120: train loss: 0.12119443714618683\n",
      "Epoch 1130: train loss: 0.12061682343482971\n",
      "Epoch 1140: train loss: 0.12005101144313812\n",
      "Epoch 1150: train loss: 0.11949574947357178\n",
      "Epoch 1160: train loss: 0.11894994229078293\n",
      "Epoch 1170: train loss: 0.11841260641813278\n",
      "Epoch 1180: train loss: 0.11788283288478851\n",
      "Epoch 1190: train loss: 0.11735989153385162\n",
      "Epoch 1200: train loss: 0.1168430894613266\n",
      "Epoch 1210: train loss: 0.1163318008184433\n",
      "Epoch 1220: train loss: 0.11582557111978531\n",
      "Epoch 1230: train loss: 0.11532391607761383\n",
      "Epoch 1240: train loss: 0.11482644081115723\n",
      "Epoch 1250: train loss: 0.11433281004428864\n",
      "Epoch 1260: train loss: 0.11384270340204239\n",
      "Epoch 1270: train loss: 0.11335588991641998\n",
      "Epoch 1280: train loss: 0.11287213116884232\n",
      "Epoch 1290: train loss: 0.11239124089479446\n",
      "Epoch 1300: train loss: 0.1119130551815033\n",
      "Epoch 1310: train loss: 0.11143749207258224\n",
      "Epoch 1320: train loss: 0.1109643429517746\n",
      "Epoch 1330: train loss: 0.11049357801675797\n",
      "Epoch 1340: train loss: 0.1100250855088234\n",
      "Epoch 1350: train loss: 0.10955880582332611\n",
      "Epoch 1360: train loss: 0.10909473150968552\n",
      "Epoch 1370: train loss: 0.10863277316093445\n",
      "Epoch 1380: train loss: 0.10817290842533112\n",
      "Epoch 1390: train loss: 0.10771510750055313\n",
      "Epoch 1400: train loss: 0.10725937783718109\n",
      "Epoch 1410: train loss: 0.1068057119846344\n",
      "Epoch 1420: train loss: 0.10635408014059067\n",
      "Epoch 1430: train loss: 0.10590451955795288\n",
      "Epoch 1440: train loss: 0.10545697808265686\n",
      "Epoch 1450: train loss: 0.10501150041818619\n",
      "Epoch 1460: train loss: 0.10456809401512146\n",
      "Epoch 1470: train loss: 0.10412676632404327\n",
      "Epoch 1480: train loss: 0.10368756204843521\n",
      "Epoch 1490: train loss: 0.10325044393539429\n",
      "Epoch 1500: train loss: 0.10281546413898468\n",
      "Epoch 1510: train loss: 0.10238263010978699\n",
      "Epoch 1520: train loss: 0.1019519791007042\n",
      "Epoch 1530: train loss: 0.1015235036611557\n",
      "Epoch 1540: train loss: 0.1010972410440445\n",
      "Epoch 1550: train loss: 0.10067319869995117\n",
      "Epoch 1560: train loss: 0.10025140643119812\n",
      "Epoch 1570: train loss: 0.09983188658952713\n",
      "Epoch 1580: train loss: 0.0994146540760994\n",
      "Epoch 1590: train loss: 0.09899972379207611\n",
      "Epoch 1600: train loss: 0.09858713299036026\n",
      "Epoch 1610: train loss: 0.09817689657211304\n",
      "Epoch 1620: train loss: 0.09776900708675385\n",
      "Epoch 1630: train loss: 0.09736352413892746\n",
      "Epoch 1640: train loss: 0.09696044772863388\n",
      "Epoch 1650: train loss: 0.0965597927570343\n",
      "Epoch 1660: train loss: 0.09616158902645111\n",
      "Epoch 1670: train loss: 0.09576583653688431\n",
      "Epoch 1680: train loss: 0.09537257999181747\n",
      "Epoch 1690: train loss: 0.09498181194067001\n",
      "Epoch 1700: train loss: 0.09459356963634491\n",
      "Epoch 1710: train loss: 0.09420786052942276\n",
      "Epoch 1720: train loss: 0.09382466226816177\n",
      "Epoch 1730: train loss: 0.09344405680894852\n",
      "Epoch 1740: train loss: 0.0930660218000412\n",
      "Epoch 1750: train loss: 0.09269056469202042\n",
      "Epoch 1760: train loss: 0.09231771528720856\n",
      "Epoch 1770: train loss: 0.09194747358560562\n",
      "Epoch 1780: train loss: 0.0915798619389534\n",
      "Epoch 1790: train loss: 0.09121488034725189\n",
      "Epoch 1800: train loss: 0.09085257351398468\n",
      "Epoch 1810: train loss: 0.09049289673566818\n",
      "Epoch 1820: train loss: 0.09013592451810837\n",
      "Epoch 1830: train loss: 0.08978162705898285\n",
      "Epoch 1840: train loss: 0.08943004161119461\n",
      "Epoch 1850: train loss: 0.08908115327358246\n",
      "Epoch 1860: train loss: 0.08873496949672699\n",
      "Epoch 1870: train loss: 0.08839148283004761\n",
      "Epoch 1880: train loss: 0.0880507081747055\n",
      "Epoch 1890: train loss: 0.08771270513534546\n",
      "Epoch 1900: train loss: 0.08737741410732269\n",
      "Epoch 1910: train loss: 0.087044857442379\n",
      "Epoch 1920: train loss: 0.08671507984399796\n",
      "Epoch 1930: train loss: 0.08638803660869598\n",
      "Epoch 1940: train loss: 0.08606376498937607\n",
      "Epoch 1950: train loss: 0.08574224263429642\n",
      "Epoch 1960: train loss: 0.08542348444461823\n",
      "Epoch 1970: train loss: 0.08510749787092209\n",
      "Epoch 1980: train loss: 0.08479426801204681\n",
      "Epoch 1990: train loss: 0.08448384702205658\n",
      "Epoch 2000: train loss: 0.08417616784572601\n",
      "Epoch 2010: train loss: 0.08387129008769989\n",
      "Epoch 2020: train loss: 0.08356918394565582\n",
      "Epoch 2030: train loss: 0.08326983451843262\n",
      "Epoch 2040: train loss: 0.08297329396009445\n",
      "Epoch 2050: train loss: 0.08267949521541595\n",
      "Epoch 2060: train loss: 0.0823884978890419\n",
      "Epoch 2070: train loss: 0.08210025727748871\n",
      "Epoch 2080: train loss: 0.08181478083133698\n",
      "Epoch 2090: train loss: 0.08153209835290909\n",
      "Epoch 2100: train loss: 0.08125215023756027\n",
      "Epoch 2110: train loss: 0.08097495883703232\n",
      "Epoch 2120: train loss: 0.08070056140422821\n",
      "Epoch 2130: train loss: 0.08042888343334198\n",
      "Epoch 2140: train loss: 0.08015996962785721\n",
      "Epoch 2150: train loss: 0.0798937976360321\n",
      "Epoch 2160: train loss: 0.07963036745786667\n",
      "Epoch 2170: train loss: 0.07936966419219971\n",
      "Epoch 2180: train loss: 0.07911168038845062\n",
      "Epoch 2190: train loss: 0.07885642349720001\n",
      "Epoch 2200: train loss: 0.07860387861728668\n",
      "Epoch 2210: train loss: 0.07835403829813004\n",
      "Epoch 2220: train loss: 0.07810690999031067\n",
      "Epoch 2230: train loss: 0.077862448990345\n",
      "Epoch 2240: train loss: 0.07762067764997482\n",
      "Epoch 2250: train loss: 0.07738158851861954\n",
      "Epoch 2260: train loss: 0.07714514434337616\n",
      "Epoch 2270: train loss: 0.07691136002540588\n",
      "Epoch 2280: train loss: 0.07668022066354752\n",
      "Epoch 2290: train loss: 0.07645171135663986\n",
      "Epoch 2300: train loss: 0.07622582465410233\n",
      "Epoch 2310: train loss: 0.07600254565477371\n",
      "Epoch 2320: train loss: 0.07578188925981522\n",
      "Epoch 2330: train loss: 0.07556378841400146\n",
      "Epoch 2340: train loss: 0.07534828037023544\n",
      "Epoch 2350: train loss: 0.07513534277677536\n",
      "Epoch 2360: train loss: 0.07492493093013763\n",
      "Epoch 2370: train loss: 0.07471708208322525\n",
      "Epoch 2380: train loss: 0.07451175153255463\n",
      "Epoch 2390: train loss: 0.07430892437696457\n",
      "Epoch 2400: train loss: 0.07410861551761627\n",
      "Epoch 2410: train loss: 0.07391078025102615\n",
      "Epoch 2420: train loss: 0.07371542602777481\n",
      "Epoch 2430: train loss: 0.07352251559495926\n",
      "Epoch 2440: train loss: 0.0733320340514183\n",
      "Epoch 2450: train loss: 0.07314399629831314\n",
      "Epoch 2460: train loss: 0.07295836508274078\n",
      "Epoch 2470: train loss: 0.07277513295412064\n",
      "Epoch 2480: train loss: 0.07259427756071091\n",
      "Epoch 2490: train loss: 0.07241576910018921\n",
      "Epoch 2500: train loss: 0.07223964482545853\n",
      "Epoch 2510: train loss: 0.0720658227801323\n",
      "Epoch 2520: train loss: 0.07189431041479111\n",
      "Epoch 2530: train loss: 0.07172512263059616\n",
      "Epoch 2540: train loss: 0.07155818492174149\n",
      "Epoch 2550: train loss: 0.07139353454113007\n",
      "Epoch 2560: train loss: 0.07123111933469772\n",
      "Epoch 2570: train loss: 0.07107092440128326\n",
      "Epoch 2580: train loss: 0.07091294974088669\n",
      "Epoch 2590: train loss: 0.0707571804523468\n",
      "Epoch 2600: train loss: 0.07060357928276062\n",
      "Epoch 2610: train loss: 0.07045212388038635\n",
      "Epoch 2620: train loss: 0.0703027993440628\n",
      "Epoch 2630: train loss: 0.07015562057495117\n",
      "Epoch 2640: train loss: 0.07001052051782608\n",
      "Epoch 2650: train loss: 0.06986752152442932\n",
      "Epoch 2660: train loss: 0.06972656399011612\n",
      "Epoch 2670: train loss: 0.06958766281604767\n",
      "Epoch 2680: train loss: 0.06945078819990158\n",
      "Epoch 2690: train loss: 0.06931590288877487\n",
      "Epoch 2700: train loss: 0.06918302923440933\n",
      "Epoch 2710: train loss: 0.06905210018157959\n",
      "Epoch 2720: train loss: 0.06892313063144684\n",
      "Epoch 2730: train loss: 0.0687960833311081\n",
      "Epoch 2740: train loss: 0.06867092847824097\n",
      "Epoch 2750: train loss: 0.06854769587516785\n",
      "Epoch 2760: train loss: 0.06842631846666336\n",
      "Epoch 2770: train loss: 0.06830678135156631\n",
      "Epoch 2780: train loss: 0.06818907707929611\n",
      "Epoch 2790: train loss: 0.06807318329811096\n",
      "Epoch 2800: train loss: 0.06795907765626907\n",
      "Epoch 2810: train loss: 0.06784674525260925\n",
      "Epoch 2820: train loss: 0.06773614883422852\n",
      "Epoch 2830: train loss: 0.06762728840112686\n",
      "Epoch 2840: train loss: 0.0675201416015625\n",
      "Epoch 2850: train loss: 0.06741466373205185\n",
      "Epoch 2860: train loss: 0.0673108771443367\n",
      "Epoch 2870: train loss: 0.06720872223377228\n",
      "Epoch 2880: train loss: 0.06710820645093918\n",
      "Epoch 2890: train loss: 0.06700928509235382\n",
      "Epoch 2900: train loss: 0.06691195070743561\n",
      "Epoch 2910: train loss: 0.06681619584560394\n",
      "Epoch 2920: train loss: 0.06672198325395584\n",
      "Epoch 2930: train loss: 0.06662929058074951\n",
      "Epoch 2940: train loss: 0.06653810292482376\n",
      "Epoch 2950: train loss: 0.0664484053850174\n",
      "Epoch 2960: train loss: 0.06636017560958862\n",
      "Epoch 2970: train loss: 0.06627339124679565\n",
      "Epoch 2980: train loss: 0.0661880299448967\n",
      "Epoch 2990: train loss: 0.06610408425331116\n",
      "Epoch 3000: train loss: 0.06602151691913605\n",
      "Epoch 3010: train loss: 0.06594032049179077\n",
      "Epoch 3020: train loss: 0.06586045026779175\n",
      "Epoch 3030: train loss: 0.06578194350004196\n",
      "Epoch 3040: train loss: 0.06570473313331604\n",
      "Epoch 3050: train loss: 0.06562879681587219\n",
      "Epoch 3060: train loss: 0.06555414199829102\n",
      "Epoch 3070: train loss: 0.06548073887825012\n",
      "Epoch 3080: train loss: 0.06540855020284653\n",
      "Epoch 3090: train loss: 0.06533759087324142\n",
      "Epoch 3100: train loss: 0.06526780873537064\n",
      "Epoch 3110: train loss: 0.06519920378923416\n",
      "Epoch 3120: train loss: 0.0651317685842514\n",
      "Epoch 3130: train loss: 0.06506545841693878\n",
      "Epoch 3140: train loss: 0.0650002658367157\n",
      "Epoch 3150: train loss: 0.06493619829416275\n",
      "Epoch 3160: train loss: 0.06487316638231277\n",
      "Epoch 3170: train loss: 0.06481122225522995\n",
      "Epoch 3180: train loss: 0.0647503137588501\n",
      "Epoch 3190: train loss: 0.06469044089317322\n",
      "Epoch 3200: train loss: 0.06463157385587692\n",
      "Epoch 3210: train loss: 0.06457369774580002\n",
      "Epoch 3220: train loss: 0.06451679766178131\n",
      "Epoch 3230: train loss: 0.0644608587026596\n",
      "Epoch 3240: train loss: 0.06440585106611252\n",
      "Epoch 3250: train loss: 0.06435177475214005\n",
      "Epoch 3260: train loss: 0.0642986074090004\n",
      "Epoch 3270: train loss: 0.06424631178379059\n",
      "Epoch 3280: train loss: 0.06419489532709122\n",
      "Epoch 3290: train loss: 0.06414434313774109\n",
      "Epoch 3300: train loss: 0.06409462541341782\n",
      "Epoch 3310: train loss: 0.06404571980237961\n",
      "Epoch 3320: train loss: 0.06399762630462646\n",
      "Epoch 3330: train loss: 0.06395033001899719\n",
      "Epoch 3340: train loss: 0.0639038160443306\n",
      "Epoch 3350: train loss: 0.0638580471277237\n",
      "Epoch 3360: train loss: 0.06381302326917648\n",
      "Epoch 3370: train loss: 0.06376874446868896\n",
      "Epoch 3380: train loss: 0.06372517347335815\n",
      "Epoch 3390: train loss: 0.06368230283260345\n",
      "Epoch 3400: train loss: 0.06364011764526367\n",
      "Epoch 3410: train loss: 0.06359860301017761\n",
      "Epoch 3420: train loss: 0.06355773657560349\n",
      "Epoch 3430: train loss: 0.0635175108909607\n",
      "Epoch 3440: train loss: 0.06347794085741043\n",
      "Epoch 3450: train loss: 0.06343896687030792\n",
      "Epoch 3460: train loss: 0.06340058892965317\n",
      "Epoch 3470: train loss: 0.06336281448602676\n",
      "Epoch 3480: train loss: 0.06332560628652573\n",
      "Epoch 3490: train loss: 0.06328896433115005\n",
      "Epoch 3500: train loss: 0.06325286626815796\n",
      "Epoch 3510: train loss: 0.06321730464696884\n",
      "Epoch 3520: train loss: 0.06318226456642151\n",
      "Epoch 3530: train loss: 0.06314773857593536\n",
      "Epoch 3540: train loss: 0.06311371922492981\n",
      "Epoch 3550: train loss: 0.06308017671108246\n",
      "Epoch 3560: train loss: 0.0630471259355545\n",
      "Epoch 3570: train loss: 0.06301453709602356\n",
      "Epoch 3580: train loss: 0.06298238784074783\n",
      "Epoch 3590: train loss: 0.06295069307088852\n",
      "Epoch 3600: train loss: 0.06291942298412323\n",
      "Epoch 3610: train loss: 0.06288858503103256\n",
      "Epoch 3620: train loss: 0.06285813450813293\n",
      "Epoch 3630: train loss: 0.06282809376716614\n",
      "Epoch 3640: train loss: 0.06279844790697098\n",
      "Epoch 3650: train loss: 0.06276917457580566\n",
      "Epoch 3660: train loss: 0.0627402737736702\n",
      "Epoch 3670: train loss: 0.06271173059940338\n",
      "Epoch 3680: train loss: 0.06268353760242462\n",
      "Epoch 3690: train loss: 0.06265568733215332\n",
      "Epoch 3700: train loss: 0.06262817978858948\n",
      "Epoch 3710: train loss: 0.06260096281766891\n",
      "Epoch 3720: train loss: 0.06257408857345581\n",
      "Epoch 3730: train loss: 0.06254750490188599\n",
      "Epoch 3740: train loss: 0.06252121925354004\n",
      "Epoch 3750: train loss: 0.06249522045254707\n",
      "Epoch 3760: train loss: 0.06246950477361679\n",
      "Epoch 3770: train loss: 0.062444064766168594\n",
      "Epoch 3780: train loss: 0.06241887807846069\n",
      "Epoch 3790: train loss: 0.06239395961165428\n",
      "Epoch 3800: train loss: 0.06236927956342697\n",
      "Epoch 3810: train loss: 0.06234484910964966\n",
      "Epoch 3820: train loss: 0.06232064589858055\n",
      "Epoch 3830: train loss: 0.06229666620492935\n",
      "Epoch 3840: train loss: 0.06227292865514755\n",
      "Epoch 3850: train loss: 0.06224937364459038\n",
      "Epoch 3860: train loss: 0.06222604587674141\n",
      "Epoch 3870: train loss: 0.06220293045043945\n",
      "Epoch 3880: train loss: 0.06217997893691063\n",
      "Epoch 3890: train loss: 0.06215722858905792\n",
      "Epoch 3900: train loss: 0.06213466450572014\n",
      "Epoch 3910: train loss: 0.062112268060445786\n",
      "Epoch 3920: train loss: 0.06209005042910576\n",
      "Epoch 3930: train loss: 0.06206801161170006\n",
      "Epoch 3940: train loss: 0.0620461069047451\n",
      "Epoch 3950: train loss: 0.062024373561143875\n",
      "Epoch 3960: train loss: 0.062002770602703094\n",
      "Epoch 3970: train loss: 0.061981331557035446\n",
      "Epoch 3980: train loss: 0.06196002662181854\n",
      "Epoch 3990: train loss: 0.06193886324763298\n",
      "Epoch 4000: train loss: 0.061917826533317566\n",
      "Epoch 4010: train loss: 0.0618969090282917\n",
      "Epoch 4020: train loss: 0.06187611445784569\n",
      "Epoch 4030: train loss: 0.061855439096689224\n",
      "Epoch 4040: train loss: 0.06183487921953201\n",
      "Epoch 4050: train loss: 0.061814434826374054\n",
      "Epoch 4060: train loss: 0.061794083565473557\n",
      "Epoch 4070: train loss: 0.06177383288741112\n",
      "Epoch 4080: train loss: 0.06175368279218674\n",
      "Epoch 4090: train loss: 0.061733637005090714\n",
      "Epoch 4100: train loss: 0.061713676899671555\n",
      "Epoch 4110: train loss: 0.06169379502534866\n",
      "Epoch 4120: train loss: 0.06167401000857353\n",
      "Epoch 4130: train loss: 0.06165429949760437\n",
      "Epoch 4140: train loss: 0.061634670943021774\n",
      "Epoch 4150: train loss: 0.06161511316895485\n",
      "Epoch 4160: train loss: 0.06159564480185509\n",
      "Epoch 4170: train loss: 0.06157622113823891\n",
      "Epoch 4180: train loss: 0.061556871980428696\n",
      "Epoch 4190: train loss: 0.061537597328424454\n",
      "Epoch 4200: train loss: 0.06151837855577469\n",
      "Epoch 4210: train loss: 0.06149922311306\n",
      "Epoch 4220: train loss: 0.061480119824409485\n",
      "Epoch 4230: train loss: 0.06146107241511345\n",
      "Epoch 4240: train loss: 0.06144207343459129\n",
      "Epoch 4250: train loss: 0.06142313778400421\n",
      "Epoch 4260: train loss: 0.06140425056219101\n",
      "Epoch 4270: train loss: 0.06138540059328079\n",
      "Epoch 4280: train loss: 0.06136661395430565\n",
      "Epoch 4290: train loss: 0.06134786084294319\n",
      "Epoch 4300: train loss: 0.06132914498448372\n",
      "Epoch 4310: train loss: 0.06131046265363693\n",
      "Epoch 4320: train loss: 0.061291832476854324\n",
      "Epoch 4330: train loss: 0.061273228377103806\n",
      "Epoch 4340: train loss: 0.061254676431417465\n",
      "Epoch 4350: train loss: 0.061236169189214706\n",
      "Epoch 4360: train loss: 0.06121767684817314\n",
      "Epoch 4370: train loss: 0.06119922176003456\n",
      "Epoch 4380: train loss: 0.06118078902363777\n",
      "Epoch 4390: train loss: 0.06116239354014397\n",
      "Epoch 4400: train loss: 0.06114403158426285\n",
      "Epoch 4410: train loss: 0.06112569570541382\n",
      "Epoch 4420: train loss: 0.06110739707946777\n",
      "Epoch 4430: train loss: 0.061089105904102325\n",
      "Epoch 4440: train loss: 0.06107084080576897\n",
      "Epoch 4450: train loss: 0.0610525980591774\n",
      "Epoch 4460: train loss: 0.061034392565488815\n",
      "Epoch 4470: train loss: 0.06101619079709053\n",
      "Epoch 4480: train loss: 0.060998037457466125\n",
      "Epoch 4490: train loss: 0.06097988784313202\n",
      "Epoch 4500: train loss: 0.06096176430583\n",
      "Epoch 4510: train loss: 0.06094367057085037\n",
      "Epoch 4520: train loss: 0.06092557683587074\n",
      "Epoch 4530: train loss: 0.060907524079084396\n",
      "Epoch 4540: train loss: 0.06088947504758835\n",
      "Epoch 4550: train loss: 0.06087145581841469\n",
      "Epoch 4560: train loss: 0.060853440314531326\n",
      "Epoch 4570: train loss: 0.06083546206355095\n",
      "Epoch 4580: train loss: 0.060817502439022064\n",
      "Epoch 4590: train loss: 0.060799550265073776\n",
      "Epoch 4600: train loss: 0.060781605541706085\n",
      "Epoch 4610: train loss: 0.06076370179653168\n",
      "Epoch 4620: train loss: 0.06074580177664757\n",
      "Epoch 4630: train loss: 0.06072792038321495\n",
      "Epoch 4640: train loss: 0.06071005389094353\n",
      "Epoch 4650: train loss: 0.060692209750413895\n",
      "Epoch 4660: train loss: 0.06067436933517456\n",
      "Epoch 4670: train loss: 0.06065656617283821\n",
      "Epoch 4680: train loss: 0.06063875928521156\n",
      "Epoch 4690: train loss: 0.060620974749326706\n",
      "Epoch 4700: train loss: 0.06060322746634483\n",
      "Epoch 4710: train loss: 0.060585469007492065\n",
      "Epoch 4720: train loss: 0.060567744076251984\n",
      "Epoch 4730: train loss: 0.0605500303208828\n",
      "Epoch 4740: train loss: 0.060532331466674805\n",
      "Epoch 4750: train loss: 0.0605146586894989\n",
      "Epoch 4760: train loss: 0.06049699708819389\n",
      "Epoch 4770: train loss: 0.06047935411334038\n",
      "Epoch 4780: train loss: 0.06046171113848686\n",
      "Epoch 4790: train loss: 0.06044410541653633\n",
      "Epoch 4800: train loss: 0.06042651832103729\n",
      "Epoch 4810: train loss: 0.06040894240140915\n",
      "Epoch 4820: train loss: 0.060391396284103394\n",
      "Epoch 4830: train loss: 0.060373853892087936\n",
      "Epoch 4840: train loss: 0.06035633757710457\n",
      "Epoch 4850: train loss: 0.06033883988857269\n",
      "Epoch 4860: train loss: 0.06032135710120201\n",
      "Epoch 4870: train loss: 0.06030389666557312\n",
      "Epoch 4880: train loss: 0.06028645858168602\n",
      "Epoch 4890: train loss: 0.06026903912425041\n",
      "Epoch 4900: train loss: 0.060251638293266296\n",
      "Epoch 4910: train loss: 0.06023425981402397\n",
      "Epoch 4920: train loss: 0.06021689251065254\n",
      "Epoch 4930: train loss: 0.060199566185474396\n",
      "Epoch 4940: train loss: 0.060182247310876846\n",
      "Epoch 4950: train loss: 0.06016495078802109\n",
      "Epoch 4960: train loss: 0.06014768034219742\n",
      "Epoch 4970: train loss: 0.06013043224811554\n",
      "Epoch 4980: train loss: 0.06011321395635605\n",
      "Epoch 4990: train loss: 0.060095999389886856\n",
      "Epoch 5000: train loss: 0.060078829526901245\n",
      "Epoch 5010: train loss: 0.06006167456507683\n",
      "Epoch 5020: train loss: 0.060044534504413605\n",
      "Epoch 5030: train loss: 0.060027435421943665\n",
      "Epoch 5040: train loss: 0.06001034751534462\n",
      "Epoch 5050: train loss: 0.05999329313635826\n",
      "Epoch 5060: train loss: 0.05997627228498459\n",
      "Epoch 5070: train loss: 0.05995926260948181\n",
      "Epoch 5080: train loss: 0.05994229018688202\n",
      "Epoch 5090: train loss: 0.059925343841314316\n",
      "Epoch 5100: train loss: 0.059908416122198105\n",
      "Epoch 5110: train loss: 0.05989152193069458\n",
      "Epoch 5120: train loss: 0.05987465754151344\n",
      "Epoch 5130: train loss: 0.05985783040523529\n",
      "Epoch 5140: train loss: 0.059841006994247437\n",
      "Epoch 5150: train loss: 0.05982423201203346\n",
      "Epoch 5160: train loss: 0.059807490557432175\n",
      "Epoch 5170: train loss: 0.05979076772928238\n",
      "Epoch 5180: train loss: 0.05977408215403557\n",
      "Epoch 5190: train loss: 0.059757426381111145\n",
      "Epoch 5200: train loss: 0.05974079295992851\n",
      "Epoch 5210: train loss: 0.05972420424222946\n",
      "Epoch 5220: train loss: 0.0597076453268528\n",
      "Epoch 5230: train loss: 0.05969112366437912\n",
      "Epoch 5240: train loss: 0.05967462435364723\n",
      "Epoch 5250: train loss: 0.05965815857052803\n",
      "Epoch 5260: train loss: 0.05964173749089241\n",
      "Epoch 5270: train loss: 0.05962533876299858\n",
      "Epoch 5280: train loss: 0.059608981013298035\n",
      "Epoch 5290: train loss: 0.05959266424179077\n",
      "Epoch 5300: train loss: 0.059576377272605896\n",
      "Epoch 5310: train loss: 0.059560131281614304\n",
      "Epoch 5320: train loss: 0.059543922543525696\n",
      "Epoch 5330: train loss: 0.05952772870659828\n",
      "Epoch 5340: train loss: 0.05951159819960594\n",
      "Epoch 5350: train loss: 0.05949549376964569\n",
      "Epoch 5360: train loss: 0.05947943031787872\n",
      "Epoch 5370: train loss: 0.05946340411901474\n",
      "Epoch 5380: train loss: 0.05944741517305374\n",
      "Epoch 5390: train loss: 0.059431467205286026\n",
      "Epoch 5400: train loss: 0.05941556394100189\n",
      "Epoch 5410: train loss: 0.059399690479040146\n",
      "Epoch 5420: train loss: 0.059383854269981384\n",
      "Epoch 5430: train loss: 0.0593680702149868\n",
      "Epoch 5440: train loss: 0.0593523308634758\n",
      "Epoch 5450: train loss: 0.059336621314287186\n",
      "Epoch 5460: train loss: 0.05932096391916275\n",
      "Epoch 5470: train loss: 0.0593053437769413\n",
      "Epoch 5480: train loss: 0.05928976088762283\n",
      "Epoch 5490: train loss: 0.05927422270178795\n",
      "Epoch 5500: train loss: 0.05925873667001724\n",
      "Epoch 5510: train loss: 0.05924328789114952\n",
      "Epoch 5520: train loss: 0.05922788381576538\n",
      "Epoch 5530: train loss: 0.059212520718574524\n",
      "Epoch 5540: train loss: 0.059197209775447845\n",
      "Epoch 5550: train loss: 0.05918193981051445\n",
      "Epoch 5560: train loss: 0.059166714549064636\n",
      "Epoch 5570: train loss: 0.059151533991098404\n",
      "Epoch 5580: train loss: 0.05913640931248665\n",
      "Epoch 5590: train loss: 0.059121325612068176\n",
      "Epoch 5600: train loss: 0.05910628288984299\n",
      "Epoch 5610: train loss: 0.059091296046972275\n",
      "Epoch 5620: train loss: 0.059076350182294846\n",
      "Epoch 5630: train loss: 0.05906146019697189\n",
      "Epoch 5640: train loss: 0.059046611189842224\n",
      "Epoch 5650: train loss: 0.05903181806206703\n",
      "Epoch 5660: train loss: 0.05901706591248512\n",
      "Epoch 5670: train loss: 0.05900236591696739\n",
      "Epoch 5680: train loss: 0.05898771062493324\n",
      "Epoch 5690: train loss: 0.05897311121225357\n",
      "Epoch 5700: train loss: 0.05895856395363808\n",
      "Epoch 5710: train loss: 0.058944061398506165\n",
      "Epoch 5720: train loss: 0.05892961472272873\n",
      "Epoch 5730: train loss: 0.058915216475725174\n",
      "Epoch 5740: train loss: 0.0589008703827858\n",
      "Epoch 5750: train loss: 0.0588865727186203\n",
      "Epoch 5760: train loss: 0.058872342109680176\n",
      "Epoch 5770: train loss: 0.058858148753643036\n",
      "Epoch 5780: train loss: 0.058844007551670074\n",
      "Epoch 5790: train loss: 0.05882992595434189\n",
      "Epoch 5800: train loss: 0.05881589651107788\n",
      "Epoch 5810: train loss: 0.05880191549658775\n",
      "Epoch 5820: train loss: 0.0587879940867424\n",
      "Epoch 5830: train loss: 0.05877411365509033\n",
      "Epoch 5840: train loss: 0.058760304003953934\n",
      "Epoch 5850: train loss: 0.05874653533101082\n",
      "Epoch 5860: train loss: 0.058732833713293076\n",
      "Epoch 5870: train loss: 0.058719176799058914\n",
      "Epoch 5880: train loss: 0.058705586940050125\n",
      "Epoch 5890: train loss: 0.05869203805923462\n",
      "Epoch 5900: train loss: 0.05867854878306389\n",
      "Epoch 5910: train loss: 0.05866512656211853\n",
      "Epoch 5920: train loss: 0.05865175649523735\n",
      "Epoch 5930: train loss: 0.05863843485713005\n",
      "Epoch 5940: train loss: 0.05862516909837723\n",
      "Epoch 5950: train loss: 0.05861196294426918\n",
      "Epoch 5960: train loss: 0.058598823845386505\n",
      "Epoch 5970: train loss: 0.058585718274116516\n",
      "Epoch 5980: train loss: 0.0585726723074913\n",
      "Epoch 5990: train loss: 0.05855970084667206\n",
      "Epoch 6000: train loss: 0.05854678153991699\n",
      "Epoch 6010: train loss: 0.058533925563097\n",
      "Epoch 6020: train loss: 0.05852111801505089\n",
      "Epoch 6030: train loss: 0.05850836634635925\n",
      "Epoch 6040: train loss: 0.05849567800760269\n",
      "Epoch 6050: train loss: 0.05848304554820061\n",
      "Epoch 6060: train loss: 0.0584704764187336\n",
      "Epoch 6070: train loss: 0.05845796316862106\n",
      "Epoch 6080: train loss: 0.058445509523153305\n",
      "Epoch 6090: train loss: 0.058433111757040024\n",
      "Epoch 6100: train loss: 0.05842076987028122\n",
      "Epoch 6110: train loss: 0.05840849131345749\n",
      "Epoch 6120: train loss: 0.05839627981185913\n",
      "Epoch 6130: train loss: 0.05838412046432495\n",
      "Epoch 6140: train loss: 0.05837201327085495\n",
      "Epoch 6150: train loss: 0.058359984308481216\n",
      "Epoch 6160: train loss: 0.058347996324300766\n",
      "Epoch 6170: train loss: 0.058336079120635986\n",
      "Epoch 6180: train loss: 0.05832421034574509\n",
      "Epoch 6190: train loss: 0.058312416076660156\n",
      "Epoch 6200: train loss: 0.05830068141222\n",
      "Epoch 6210: train loss: 0.05828899145126343\n",
      "Epoch 6220: train loss: 0.05827736854553223\n",
      "Epoch 6230: train loss: 0.0582658089697361\n",
      "Epoch 6240: train loss: 0.05825430899858475\n",
      "Epoch 6250: train loss: 0.05824287235736847\n",
      "Epoch 6260: train loss: 0.05823149532079697\n",
      "Epoch 6270: train loss: 0.05822017416357994\n",
      "Epoch 6280: train loss: 0.058208901435136795\n",
      "Epoch 6290: train loss: 0.05819770693778992\n",
      "Epoch 6300: train loss: 0.058186568319797516\n",
      "Epoch 6310: train loss: 0.05817548558115959\n",
      "Epoch 6320: train loss: 0.05816446989774704\n",
      "Epoch 6330: train loss: 0.05815352126955986\n",
      "Epoch 6340: train loss: 0.05814262852072716\n",
      "Epoch 6350: train loss: 0.058131784200668335\n",
      "Epoch 6360: train loss: 0.058121006935834885\n",
      "Epoch 6370: train loss: 0.05811029672622681\n",
      "Epoch 6380: train loss: 0.05809963867068291\n",
      "Epoch 6390: train loss: 0.05808904394507408\n",
      "Epoch 6400: train loss: 0.05807851627469063\n",
      "Epoch 6410: train loss: 0.058068037033081055\n",
      "Epoch 6420: train loss: 0.05805763602256775\n",
      "Epoch 6430: train loss: 0.058047275990247726\n",
      "Epoch 6440: train loss: 0.05803697928786278\n",
      "Epoch 6450: train loss: 0.0580267459154129\n",
      "Epoch 6460: train loss: 0.0580165758728981\n",
      "Epoch 6470: train loss: 0.05800645798444748\n",
      "Epoch 6480: train loss: 0.057996414601802826\n",
      "Epoch 6490: train loss: 0.05798641964793205\n",
      "Epoch 6500: train loss: 0.057976484298706055\n",
      "Epoch 6510: train loss: 0.05796661227941513\n",
      "Epoch 6520: train loss: 0.05795679986476898\n",
      "Epoch 6530: train loss: 0.05794704332947731\n",
      "Epoch 6540: train loss: 0.057937342673540115\n",
      "Epoch 06548: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 6550: train loss: 0.057928383350372314\n",
      "Epoch 06555: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 6560: train loss: 0.05792630836367607\n",
      "Epoch 06561: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 06567: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 6570: train loss: 0.05792596563696861\n",
      "Epoch 06573: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 06579: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 6580: train loss: 0.05792592838406563\n",
      "Epoch 06585: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 6590: train loss: 0.05792592093348503\n",
      "Epoch 06591: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 6591, loss: 0.05792592093348503\n",
      "Predictions saved, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(\"Predictions saved, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
