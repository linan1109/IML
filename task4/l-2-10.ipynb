{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    # x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-2-10-scale.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1962dd354f419bb62ba08642db063d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.5377904868174572, val loss: 0.24094640386104585\n",
      "Epoch 2: train loss: 0.20645261174562027, val loss: 0.1838669743537903\n",
      "Epoch 3: train loss: 0.15820234915918233, val loss: 0.17756114172935486\n",
      "Epoch 4: train loss: 0.1406023807476978, val loss: 0.12268793845176697\n",
      "Epoch 5: train loss: 0.12258228242640593, val loss: 0.10616911441087723\n",
      "Epoch 6: train loss: 0.10773998814699601, val loss: 0.09808802473545074\n",
      "Epoch 7: train loss: 0.09457663305559937, val loss: 0.09286013668775558\n",
      "Epoch 8: train loss: 0.08385430609936617, val loss: 0.0816337986588478\n",
      "Epoch 9: train loss: 0.0734488443519388, val loss: 0.06907666397094726\n",
      "Epoch 10: train loss: 0.06712417567506128, val loss: 0.07340779536962509\n",
      "Epoch 11: train loss: 0.05878503490771566, val loss: 0.054615820616483686\n",
      "Epoch 12: train loss: 0.05094934044018084, val loss: 0.04230550494790077\n",
      "Epoch 13: train loss: 0.04232055596247011, val loss: 0.038858816504478456\n",
      "Epoch 14: train loss: 0.03446472249894726, val loss: 0.0375212599337101\n",
      "Epoch 15: train loss: 0.02870903235004873, val loss: 0.02523854333162308\n",
      "Epoch 16: train loss: 0.023830982181490684, val loss: 0.024795562088489532\n",
      "Epoch 17: train loss: 0.018965520219535244, val loss: 0.01878578354418278\n",
      "Epoch 18: train loss: 0.015693623037514638, val loss: 0.015547532968223095\n",
      "Epoch 19: train loss: 0.012803021823265115, val loss: 0.0121831536591053\n",
      "Epoch 20: train loss: 0.010694033846715275, val loss: 0.010464776068925858\n",
      "Epoch 21: train loss: 0.009204632409677213, val loss: 0.008483366549015044\n",
      "Epoch 22: train loss: 0.008117121604480306, val loss: 0.00851747991144657\n",
      "Epoch 23: train loss: 0.007328704774075625, val loss: 0.007603463917970657\n",
      "Epoch 24: train loss: 0.006694214376639955, val loss: 0.0071196435578167435\n",
      "Epoch 25: train loss: 0.006406455278548659, val loss: 0.0069474300518631935\n",
      "Epoch 26: train loss: 0.00602569397630132, val loss: 0.0060563992634415625\n",
      "Epoch 27: train loss: 0.005841017454862594, val loss: 0.0066069444492459295\n",
      "Epoch 28: train loss: 0.0058727206947396, val loss: 0.006557782959192991\n",
      "Epoch 29: train loss: 0.005778728985816849, val loss: 0.006388019748032093\n",
      "Epoch 30: train loss: 0.005709656831865408, val loss: 0.006030099496245384\n",
      "Epoch 31: train loss: 0.005707775638267702, val loss: 0.006922186456620693\n",
      "Epoch 32: train loss: 0.00567260101141066, val loss: 0.007108111146837473\n",
      "Epoch 33: train loss: 0.005744528774248094, val loss: 0.007160562131553888\n",
      "Epoch 34: train loss: 0.005790356829306301, val loss: 0.005870180014520884\n",
      "Epoch 35: train loss: 0.005498253823619108, val loss: 0.006219474226236343\n",
      "Epoch 36: train loss: 0.005784942459543141, val loss: 0.006605211172252893\n",
      "Epoch 37: train loss: 0.005823390037049445, val loss: 0.005834089890122414\n",
      "Epoch 38: train loss: 0.00569723835557091, val loss: 0.005577413223683834\n",
      "Epoch 39: train loss: 0.005834965482810322, val loss: 0.005840170126408338\n",
      "Epoch 40: train loss: 0.005663377721242759, val loss: 0.00565249414369464\n",
      "Epoch 41: train loss: 0.005676694317511758, val loss: 0.006415658213198185\n",
      "Epoch 42: train loss: 0.00569974834616391, val loss: 0.006431690227240324\n",
      "Epoch 43: train loss: 0.005645563517526096, val loss: 0.005875397473573685\n",
      "Epoch 00044: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 44: train loss: 0.005875305502694481, val loss: 0.00640411439165473\n",
      "Epoch 45: train loss: 0.0040248191142264675, val loss: 0.004590480796992779\n",
      "Epoch 46: train loss: 0.003243245627503006, val loss: 0.004051314622163772\n",
      "Epoch 47: train loss: 0.003143167365912576, val loss: 0.00386319724842906\n",
      "Epoch 48: train loss: 0.0031161003930076044, val loss: 0.004581821985542774\n",
      "Epoch 49: train loss: 0.0030869978315444017, val loss: 0.004487204756587744\n",
      "Epoch 50: train loss: 0.00321770630054632, val loss: 0.0039355472065508365\n",
      "Epoch 51: train loss: 0.003242020641130452, val loss: 0.004279696231707931\n",
      "Epoch 52: train loss: 0.003218200037445949, val loss: 0.00393783095665276\n",
      "Epoch 00053: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 53: train loss: 0.003178366474503157, val loss: 0.004147838972508908\n",
      "Epoch 54: train loss: 0.0025913101479563177, val loss: 0.0033550867829471825\n",
      "Epoch 55: train loss: 0.0022569993128619937, val loss: 0.003325046591460705\n",
      "Epoch 56: train loss: 0.0021459205832370385, val loss: 0.0031500309128314257\n",
      "Epoch 57: train loss: 0.002102226069006993, val loss: 0.0030892542600631716\n",
      "Epoch 58: train loss: 0.0021132350748176783, val loss: 0.00317231659963727\n",
      "Epoch 59: train loss: 0.0020601400437251648, val loss: 0.0034325357154011728\n",
      "Epoch 60: train loss: 0.0020276804700949972, val loss: 0.0030828765388578178\n",
      "Epoch 61: train loss: 0.0020513337778725795, val loss: 0.0033504968658089637\n",
      "Epoch 62: train loss: 0.0020104894187985634, val loss: 0.0034231881238520143\n",
      "Epoch 63: train loss: 0.0019992435356526047, val loss: 0.0035144120026379825\n",
      "Epoch 64: train loss: 0.002001520015261307, val loss: 0.003290192186832428\n",
      "Epoch 65: train loss: 0.0020058487244900697, val loss: 0.003213672023266554\n",
      "Epoch 00066: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 66: train loss: 0.002006498243545695, val loss: 0.0031965750511735677\n",
      "Epoch 67: train loss: 0.0017975143622036794, val loss: 0.0031799167767167093\n",
      "Epoch 68: train loss: 0.0016925145045911172, val loss: 0.0030294514428824185\n",
      "Epoch 69: train loss: 0.001649920827405033, val loss: 0.0028202525489032268\n",
      "Epoch 70: train loss: 0.0016521329586489165, val loss: 0.002960494061931968\n",
      "Epoch 71: train loss: 0.0016277962890168538, val loss: 0.0030364534799009564\n",
      "Epoch 72: train loss: 0.0016072848166092013, val loss: 0.0028624832164496185\n",
      "Epoch 73: train loss: 0.0015971933168606187, val loss: 0.0030723455641418697\n",
      "Epoch 74: train loss: 0.0015676876084917055, val loss: 0.00300077298656106\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 75: train loss: 0.0015726853703927934, val loss: 0.0033036131989210844\n",
      "Epoch 76: train loss: 0.0015076686718254065, val loss: 0.0030121401492506266\n",
      "Epoch 77: train loss: 0.0015105288900358945, val loss: 0.0031192747727036476\n",
      "Epoch 78: train loss: 0.0014806776335082796, val loss: 0.00286154193431139\n",
      "Epoch 79: train loss: 0.0014906920058348654, val loss: 0.002873741613700986\n",
      "Epoch 80: train loss: 0.0014510963869338134, val loss: 0.003034590793773532\n",
      "Epoch 81: train loss: 0.0014496888551975087, val loss: 0.0027024078089743853\n",
      "Epoch 82: train loss: 0.0014522241778215583, val loss: 0.0027981684114784\n",
      "Epoch 83: train loss: 0.0014559192677617682, val loss: 0.002789607701823115\n",
      "Epoch 84: train loss: 0.0014352275707321812, val loss: 0.0029056938849389554\n",
      "Epoch 85: train loss: 0.0014373344286554017, val loss: 0.00302960055321455\n",
      "Epoch 86: train loss: 0.0014251789950138452, val loss: 0.002867739278823137\n",
      "Epoch 00087: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 87: train loss: 0.0014182704240843958, val loss: 0.0029031317718327047\n",
      "Epoch 88: train loss: 0.0014018388106202594, val loss: 0.0028829041738063096\n",
      "Epoch 89: train loss: 0.0013996695227046706, val loss: 0.0028464300502091646\n",
      "Epoch 90: train loss: 0.00138020731621821, val loss: 0.0027458007410168647\n",
      "Epoch 91: train loss: 0.001407333361460086, val loss: 0.0028869960531592367\n",
      "Epoch 92: train loss: 0.001398134981624174, val loss: 0.002839288769289851\n",
      "Epoch 00093: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 93: train loss: 0.001401510964584898, val loss: 0.0029317149575799704\n",
      "Early stop at epoch 93\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca7fac818584a5ab1f2d9076433fc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 6.539181232452393\n",
      "Epoch 20: train loss: 6.292253494262695\n",
      "Epoch 30: train loss: 6.056957721710205\n",
      "Epoch 40: train loss: 5.833811283111572\n",
      "Epoch 50: train loss: 5.622883319854736\n",
      "Epoch 60: train loss: 5.423953056335449\n",
      "Epoch 70: train loss: 5.236639499664307\n",
      "Epoch 80: train loss: 5.060482025146484\n",
      "Epoch 90: train loss: 4.894987106323242\n",
      "Epoch 100: train loss: 4.739650249481201\n",
      "Epoch 110: train loss: 4.593961715698242\n",
      "Epoch 120: train loss: 4.457414150238037\n",
      "Epoch 130: train loss: 4.3295063972473145\n",
      "Epoch 140: train loss: 4.209743499755859\n",
      "Epoch 150: train loss: 4.097639560699463\n",
      "Epoch 160: train loss: 3.992718458175659\n",
      "Epoch 170: train loss: 3.8945159912109375\n",
      "Epoch 180: train loss: 3.802579164505005\n",
      "Epoch 190: train loss: 3.7164716720581055\n",
      "Epoch 200: train loss: 3.6357688903808594\n",
      "Epoch 210: train loss: 3.560065507888794\n",
      "Epoch 220: train loss: 3.4889707565307617\n",
      "Epoch 230: train loss: 3.4221138954162598\n",
      "Epoch 240: train loss: 3.3591418266296387\n",
      "Epoch 250: train loss: 3.299720287322998\n",
      "Epoch 260: train loss: 3.2435357570648193\n",
      "Epoch 270: train loss: 3.190293550491333\n",
      "Epoch 280: train loss: 3.139719247817993\n",
      "Epoch 290: train loss: 3.0915565490722656\n",
      "Epoch 300: train loss: 3.0455715656280518\n",
      "Epoch 310: train loss: 3.001546621322632\n",
      "Epoch 320: train loss: 2.959282875061035\n",
      "Epoch 330: train loss: 2.918599843978882\n",
      "Epoch 340: train loss: 2.879333972930908\n",
      "Epoch 350: train loss: 2.841337203979492\n",
      "Epoch 360: train loss: 2.804476022720337\n",
      "Epoch 370: train loss: 2.768632173538208\n",
      "Epoch 380: train loss: 2.733699321746826\n",
      "Epoch 390: train loss: 2.699584484100342\n",
      "Epoch 400: train loss: 2.666205406188965\n",
      "Epoch 410: train loss: 2.6334893703460693\n",
      "Epoch 420: train loss: 2.601372718811035\n",
      "Epoch 430: train loss: 2.56980037689209\n",
      "Epoch 440: train loss: 2.5387256145477295\n",
      "Epoch 450: train loss: 2.508105754852295\n",
      "Epoch 460: train loss: 2.4779069423675537\n",
      "Epoch 470: train loss: 2.4480984210968018\n",
      "Epoch 480: train loss: 2.418654441833496\n",
      "Epoch 490: train loss: 2.3895528316497803\n",
      "Epoch 500: train loss: 2.3607749938964844\n",
      "Epoch 510: train loss: 2.332305908203125\n",
      "Epoch 520: train loss: 2.3041319847106934\n",
      "Epoch 530: train loss: 2.276242256164551\n",
      "Epoch 540: train loss: 2.2486276626586914\n",
      "Epoch 550: train loss: 2.221280097961426\n",
      "Epoch 560: train loss: 2.1941936016082764\n",
      "Epoch 570: train loss: 2.1673622131347656\n",
      "Epoch 580: train loss: 2.140781879425049\n",
      "Epoch 590: train loss: 2.1144485473632812\n",
      "Epoch 600: train loss: 2.0883591175079346\n",
      "Epoch 610: train loss: 2.0625102519989014\n",
      "Epoch 620: train loss: 2.036900520324707\n",
      "Epoch 630: train loss: 2.0115277767181396\n",
      "Epoch 640: train loss: 1.9863899946212769\n",
      "Epoch 650: train loss: 1.9614862203598022\n",
      "Epoch 660: train loss: 1.936814546585083\n",
      "Epoch 670: train loss: 1.9123737812042236\n",
      "Epoch 680: train loss: 1.8881629705429077\n",
      "Epoch 690: train loss: 1.8641809225082397\n",
      "Epoch 700: train loss: 1.8404265642166138\n",
      "Epoch 710: train loss: 1.8168989419937134\n",
      "Epoch 720: train loss: 1.7935973405838013\n",
      "Epoch 730: train loss: 1.770520567893982\n",
      "Epoch 740: train loss: 1.747667908668518\n",
      "Epoch 750: train loss: 1.7250382900238037\n",
      "Epoch 760: train loss: 1.702630877494812\n",
      "Epoch 770: train loss: 1.6804447174072266\n",
      "Epoch 780: train loss: 1.6584789752960205\n",
      "Epoch 790: train loss: 1.6367323398590088\n",
      "Epoch 800: train loss: 1.6152039766311646\n",
      "Epoch 810: train loss: 1.5938934087753296\n",
      "Epoch 820: train loss: 1.5727990865707397\n",
      "Epoch 830: train loss: 1.5519201755523682\n",
      "Epoch 840: train loss: 1.5312556028366089\n",
      "Epoch 850: train loss: 1.5108044147491455\n",
      "Epoch 860: train loss: 1.4905657768249512\n",
      "Epoch 870: train loss: 1.4705382585525513\n",
      "Epoch 880: train loss: 1.4507211446762085\n",
      "Epoch 890: train loss: 1.4311127662658691\n",
      "Epoch 900: train loss: 1.4117127656936646\n",
      "Epoch 910: train loss: 1.392519474029541\n",
      "Epoch 920: train loss: 1.3735324144363403\n",
      "Epoch 930: train loss: 1.3547494411468506\n",
      "Epoch 940: train loss: 1.3361701965332031\n",
      "Epoch 950: train loss: 1.317793369293213\n",
      "Epoch 960: train loss: 1.2996175289154053\n",
      "Epoch 970: train loss: 1.281641960144043\n",
      "Epoch 980: train loss: 1.2638652324676514\n",
      "Epoch 990: train loss: 1.2462855577468872\n",
      "Epoch 1000: train loss: 1.2289022207260132\n",
      "Epoch 1010: train loss: 1.2117141485214233\n",
      "Epoch 1020: train loss: 1.1947200298309326\n",
      "Epoch 1030: train loss: 1.1779189109802246\n",
      "Epoch 1040: train loss: 1.1613091230392456\n",
      "Epoch 1050: train loss: 1.144889235496521\n",
      "Epoch 1060: train loss: 1.128658413887024\n",
      "Epoch 1070: train loss: 1.1126152276992798\n",
      "Epoch 1080: train loss: 1.096758246421814\n",
      "Epoch 1090: train loss: 1.081086277961731\n",
      "Epoch 1100: train loss: 1.0655982494354248\n",
      "Epoch 1110: train loss: 1.0502926111221313\n",
      "Epoch 1120: train loss: 1.035168170928955\n",
      "Epoch 1130: train loss: 1.0202234983444214\n",
      "Epoch 1140: train loss: 1.0054571628570557\n",
      "Epoch 1150: train loss: 0.990868330001831\n",
      "Epoch 1160: train loss: 0.9764552116394043\n",
      "Epoch 1170: train loss: 0.9622167944908142\n",
      "Epoch 1180: train loss: 0.9481515288352966\n",
      "Epoch 1190: train loss: 0.9342581629753113\n",
      "Epoch 1200: train loss: 0.9205353856086731\n",
      "Epoch 1210: train loss: 0.9069819450378418\n",
      "Epoch 1220: train loss: 0.8935962319374084\n",
      "Epoch 1230: train loss: 0.8803771734237671\n",
      "Epoch 1240: train loss: 0.8673234581947327\n",
      "Epoch 1250: train loss: 0.8544334173202515\n",
      "Epoch 1260: train loss: 0.8417059779167175\n",
      "Epoch 1270: train loss: 0.8291395306587219\n",
      "Epoch 1280: train loss: 0.8167329430580139\n",
      "Epoch 1290: train loss: 0.8044849038124084\n",
      "Epoch 1300: train loss: 0.7923941612243652\n",
      "Epoch 1310: train loss: 0.7804588675498962\n",
      "Epoch 1320: train loss: 0.7686784267425537\n",
      "Epoch 1330: train loss: 0.7570507526397705\n",
      "Epoch 1340: train loss: 0.745574951171875\n",
      "Epoch 1350: train loss: 0.7342497110366821\n",
      "Epoch 1360: train loss: 0.7230735421180725\n",
      "Epoch 1370: train loss: 0.7120448350906372\n",
      "Epoch 1380: train loss: 0.7011622190475464\n",
      "Epoch 1390: train loss: 0.6904250979423523\n",
      "Epoch 1400: train loss: 0.679831862449646\n",
      "Epoch 1410: train loss: 0.6693810224533081\n",
      "Epoch 1420: train loss: 0.6590710282325745\n",
      "Epoch 1430: train loss: 0.6489006876945496\n",
      "Epoch 1440: train loss: 0.6388691663742065\n",
      "Epoch 1450: train loss: 0.628974199295044\n",
      "Epoch 1460: train loss: 0.6192154288291931\n",
      "Epoch 1470: train loss: 0.6095911860466003\n",
      "Epoch 1480: train loss: 0.6000999212265015\n",
      "Epoch 1490: train loss: 0.5907407402992249\n",
      "Epoch 1500: train loss: 0.581511914730072\n",
      "Epoch 1510: train loss: 0.5724125504493713\n",
      "Epoch 1520: train loss: 0.5634412169456482\n",
      "Epoch 1530: train loss: 0.554596483707428\n",
      "Epoch 1540: train loss: 0.5458773374557495\n",
      "Epoch 1550: train loss: 0.5372824668884277\n",
      "Epoch 1560: train loss: 0.528810441493988\n",
      "Epoch 1570: train loss: 0.5204598307609558\n",
      "Epoch 1580: train loss: 0.512229859828949\n",
      "Epoch 1590: train loss: 0.5041188597679138\n",
      "Epoch 1600: train loss: 0.4961259365081787\n",
      "Epoch 1610: train loss: 0.48824942111968994\n",
      "Epoch 1620: train loss: 0.4804884195327759\n",
      "Epoch 1630: train loss: 0.47284170985221863\n",
      "Epoch 1640: train loss: 0.4653080701828003\n",
      "Epoch 1650: train loss: 0.4578860402107239\n",
      "Epoch 1660: train loss: 0.4505743980407715\n",
      "Epoch 1670: train loss: 0.4433723986148834\n",
      "Epoch 1680: train loss: 0.4362785220146179\n",
      "Epoch 1690: train loss: 0.4292914867401123\n",
      "Epoch 1700: train loss: 0.4224103093147278\n",
      "Epoch 1710: train loss: 0.4156336784362793\n",
      "Epoch 1720: train loss: 0.40896040201187134\n",
      "Epoch 1730: train loss: 0.4023895859718323\n",
      "Epoch 1740: train loss: 0.39591991901397705\n",
      "Epoch 1750: train loss: 0.3895500898361206\n",
      "Epoch 1760: train loss: 0.3832791745662689\n",
      "Epoch 1770: train loss: 0.3771059811115265\n",
      "Epoch 1780: train loss: 0.3710292875766754\n",
      "Epoch 1790: train loss: 0.36504805088043213\n",
      "Epoch 1800: train loss: 0.35916125774383545\n",
      "Epoch 1810: train loss: 0.35336753726005554\n",
      "Epoch 1820: train loss: 0.3476662039756775\n",
      "Epoch 1830: train loss: 0.3420557975769043\n",
      "Epoch 1840: train loss: 0.33653539419174194\n",
      "Epoch 1850: train loss: 0.33110398054122925\n",
      "Epoch 1860: train loss: 0.3257603049278259\n",
      "Epoch 1870: train loss: 0.3205034136772156\n",
      "Epoch 1880: train loss: 0.3153322637081146\n",
      "Epoch 1890: train loss: 0.31024572253227234\n",
      "Epoch 1900: train loss: 0.3052429258823395\n",
      "Epoch 1910: train loss: 0.30032259225845337\n",
      "Epoch 1920: train loss: 0.2954840660095215\n",
      "Epoch 1930: train loss: 0.29072603583335876\n",
      "Epoch 1940: train loss: 0.28604766726493835\n",
      "Epoch 1950: train loss: 0.28144773840904236\n",
      "Epoch 1960: train loss: 0.2769254744052887\n",
      "Epoch 1970: train loss: 0.2724798023700714\n",
      "Epoch 1980: train loss: 0.2681097984313965\n",
      "Epoch 1990: train loss: 0.26381441950798035\n",
      "Epoch 2000: train loss: 0.2595926821231842\n",
      "Epoch 2010: train loss: 0.2554436922073364\n",
      "Epoch 2020: train loss: 0.25136658549308777\n",
      "Epoch 2030: train loss: 0.24736030399799347\n",
      "Epoch 2040: train loss: 0.24342401325702667\n",
      "Epoch 2050: train loss: 0.239556685090065\n",
      "Epoch 2060: train loss: 0.235757514834404\n",
      "Epoch 2070: train loss: 0.2320254147052765\n",
      "Epoch 2080: train loss: 0.22835971415042877\n",
      "Epoch 2090: train loss: 0.22475939989089966\n",
      "Epoch 2100: train loss: 0.22122357785701752\n",
      "Epoch 2110: train loss: 0.2177514284849167\n",
      "Epoch 2120: train loss: 0.21434204280376434\n",
      "Epoch 2130: train loss: 0.2109946459531784\n",
      "Epoch 2140: train loss: 0.2077081948518753\n",
      "Epoch 2150: train loss: 0.20448195934295654\n",
      "Epoch 2160: train loss: 0.20131510496139526\n",
      "Epoch 2170: train loss: 0.19820678234100342\n",
      "Epoch 2180: train loss: 0.19515618681907654\n",
      "Epoch 2190: train loss: 0.19216252863407135\n",
      "Epoch 2200: train loss: 0.1892249435186386\n",
      "Epoch 2210: train loss: 0.18634261190891266\n",
      "Epoch 2220: train loss: 0.18351474404335022\n",
      "Epoch 2230: train loss: 0.180740624666214\n",
      "Epoch 2240: train loss: 0.17801935970783234\n",
      "Epoch 2250: train loss: 0.1753503382205963\n",
      "Epoch 2260: train loss: 0.1727326363325119\n",
      "Epoch 2270: train loss: 0.17016561329364777\n",
      "Epoch 2280: train loss: 0.16764840483665466\n",
      "Epoch 2290: train loss: 0.16518035531044006\n",
      "Epoch 2300: train loss: 0.1627606749534607\n",
      "Epoch 2310: train loss: 0.16038861870765686\n",
      "Epoch 2320: train loss: 0.15806356072425842\n",
      "Epoch 2330: train loss: 0.1557846963405609\n",
      "Epoch 2340: train loss: 0.15355134010314941\n",
      "Epoch 2350: train loss: 0.15136277675628662\n",
      "Epoch 2360: train loss: 0.14921830594539642\n",
      "Epoch 2370: train loss: 0.1471172720193863\n",
      "Epoch 2380: train loss: 0.1450590044260025\n",
      "Epoch 2390: train loss: 0.14304278790950775\n",
      "Epoch 2400: train loss: 0.14106795191764832\n",
      "Epoch 2410: train loss: 0.13913381099700928\n",
      "Epoch 2420: train loss: 0.13723981380462646\n",
      "Epoch 2430: train loss: 0.13538523018360138\n",
      "Epoch 2440: train loss: 0.1335694044828415\n",
      "Epoch 2450: train loss: 0.13179181516170502\n",
      "Epoch 2460: train loss: 0.13005171716213226\n",
      "Epoch 2470: train loss: 0.1283484548330307\n",
      "Epoch 2480: train loss: 0.1266815960407257\n",
      "Epoch 2490: train loss: 0.125050351023674\n",
      "Epoch 2500: train loss: 0.12345422804355621\n",
      "Epoch 2510: train loss: 0.12189258635044098\n",
      "Epoch 2520: train loss: 0.12036483734846115\n",
      "Epoch 2530: train loss: 0.11887040734291077\n",
      "Epoch 2540: train loss: 0.11740875244140625\n",
      "Epoch 2550: train loss: 0.11597926914691925\n",
      "Epoch 2560: train loss: 0.1145813837647438\n",
      "Epoch 2570: train loss: 0.11321453750133514\n",
      "Epoch 2580: train loss: 0.11187820136547089\n",
      "Epoch 2590: train loss: 0.11057180166244507\n",
      "Epoch 2600: train loss: 0.10929489135742188\n",
      "Epoch 2610: train loss: 0.10804685205221176\n",
      "Epoch 2620: train loss: 0.10682723671197891\n",
      "Epoch 2630: train loss: 0.10563545674085617\n",
      "Epoch 2640: train loss: 0.10447107255458832\n",
      "Epoch 2650: train loss: 0.10333352535963058\n",
      "Epoch 2660: train loss: 0.10222228616476059\n",
      "Epoch 2670: train loss: 0.1011369526386261\n",
      "Epoch 2680: train loss: 0.10007695853710175\n",
      "Epoch 2690: train loss: 0.09904184192419052\n",
      "Epoch 2700: train loss: 0.09803116321563721\n",
      "Epoch 2710: train loss: 0.09704438596963882\n",
      "Epoch 2720: train loss: 0.09608108550310135\n",
      "Epoch 2730: train loss: 0.09514081478118896\n",
      "Epoch 2740: train loss: 0.09422317147254944\n",
      "Epoch 2750: train loss: 0.09332764148712158\n",
      "Epoch 2760: train loss: 0.09245383739471436\n",
      "Epoch 2770: train loss: 0.09160127490758896\n",
      "Epoch 2780: train loss: 0.09076955914497375\n",
      "Epoch 2790: train loss: 0.08995823562145233\n",
      "Epoch 2800: train loss: 0.08916693180799484\n",
      "Epoch 2810: train loss: 0.08839523047208786\n",
      "Epoch 2820: train loss: 0.08764274418354034\n",
      "Epoch 2830: train loss: 0.08690902590751648\n",
      "Epoch 2840: train loss: 0.0861937403678894\n",
      "Epoch 2850: train loss: 0.0854964405298233\n",
      "Epoch 2860: train loss: 0.08481680601835251\n",
      "Epoch 2870: train loss: 0.08415444195270538\n",
      "Epoch 2880: train loss: 0.0835089311003685\n",
      "Epoch 2890: train loss: 0.08287999033927917\n",
      "Epoch 2900: train loss: 0.08226721733808517\n",
      "Epoch 2910: train loss: 0.08167026191949844\n",
      "Epoch 2920: train loss: 0.08108879625797272\n",
      "Epoch 2930: train loss: 0.08052244037389755\n",
      "Epoch 2940: train loss: 0.07997095584869385\n",
      "Epoch 2950: train loss: 0.0794338583946228\n",
      "Epoch 2960: train loss: 0.0789109542965889\n",
      "Epoch 2970: train loss: 0.07840192317962646\n",
      "Epoch 2980: train loss: 0.07790637016296387\n",
      "Epoch 2990: train loss: 0.07742401212453842\n",
      "Epoch 3000: train loss: 0.07695458084344864\n",
      "Epoch 3010: train loss: 0.07649777829647064\n",
      "Epoch 3020: train loss: 0.07605312764644623\n",
      "Epoch 3030: train loss: 0.07562059909105301\n",
      "Epoch 3040: train loss: 0.07519979029893875\n",
      "Epoch 3050: train loss: 0.07479044795036316\n",
      "Epoch 3060: train loss: 0.07439228147268295\n",
      "Epoch 3070: train loss: 0.07400501519441605\n",
      "Epoch 3080: train loss: 0.07362838834524155\n",
      "Epoch 3090: train loss: 0.07326215505599976\n",
      "Epoch 3100: train loss: 0.07290605455636978\n",
      "Epoch 3110: train loss: 0.07255984097719193\n",
      "Epoch 3120: train loss: 0.07222326844930649\n",
      "Epoch 3130: train loss: 0.07189612090587616\n",
      "Epoch 3140: train loss: 0.07157813757658005\n",
      "Epoch 3150: train loss: 0.07126908004283905\n",
      "Epoch 3160: train loss: 0.07096872478723526\n",
      "Epoch 3170: train loss: 0.07067684084177017\n",
      "Epoch 3180: train loss: 0.07039324939250946\n",
      "Epoch 3190: train loss: 0.07011768966913223\n",
      "Epoch 3200: train loss: 0.06984998285770416\n",
      "Epoch 3210: train loss: 0.06958992034196854\n",
      "Epoch 3220: train loss: 0.06933729350566864\n",
      "Epoch 3230: train loss: 0.06909190118312836\n",
      "Epoch 3240: train loss: 0.06885355710983276\n",
      "Epoch 3250: train loss: 0.06862208992242813\n",
      "Epoch 3260: train loss: 0.06839730590581894\n",
      "Epoch 3270: train loss: 0.06817900389432907\n",
      "Epoch 3280: train loss: 0.0679670199751854\n",
      "Epoch 3290: train loss: 0.06776118278503418\n",
      "Epoch 3300: train loss: 0.0675613209605217\n",
      "Epoch 3310: train loss: 0.06736727803945541\n",
      "Epoch 3320: train loss: 0.067178875207901\n",
      "Epoch 3330: train loss: 0.06699599325656891\n",
      "Epoch 3340: train loss: 0.06681842356920242\n",
      "Epoch 3350: train loss: 0.06664606928825378\n",
      "Epoch 3360: train loss: 0.06647873669862747\n",
      "Epoch 3370: train loss: 0.06631631404161453\n",
      "Epoch 3380: train loss: 0.06615865230560303\n",
      "Epoch 3390: train loss: 0.06600560247898102\n",
      "Epoch 3400: train loss: 0.06585703045129776\n",
      "Epoch 3410: train loss: 0.0657128319144249\n",
      "Epoch 3420: train loss: 0.06557285040616989\n",
      "Epoch 3430: train loss: 0.06543698906898499\n",
      "Epoch 3440: train loss: 0.06530510634183884\n",
      "Epoch 3450: train loss: 0.0651770830154419\n",
      "Epoch 3460: train loss: 0.0650528073310852\n",
      "Epoch 3470: train loss: 0.06493218243122101\n",
      "Epoch 3480: train loss: 0.06481507420539856\n",
      "Epoch 3490: train loss: 0.0647013857960701\n",
      "Epoch 3500: train loss: 0.06459102779626846\n",
      "Epoch 3510: train loss: 0.0644838884472847\n",
      "Epoch 3520: train loss: 0.06437987089157104\n",
      "Epoch 3530: train loss: 0.06427887082099915\n",
      "Epoch 3540: train loss: 0.06418079882860184\n",
      "Epoch 3550: train loss: 0.06408556550741196\n",
      "Epoch 3560: train loss: 0.06399308145046234\n",
      "Epoch 3570: train loss: 0.06390325725078583\n",
      "Epoch 3580: train loss: 0.06381601095199585\n",
      "Epoch 3590: train loss: 0.06373127549886703\n",
      "Epoch 3600: train loss: 0.06364895403385162\n",
      "Epoch 3610: train loss: 0.06356897950172424\n",
      "Epoch 3620: train loss: 0.06349126994609833\n",
      "Epoch 3630: train loss: 0.06341574341058731\n",
      "Epoch 3640: train loss: 0.06334235519170761\n",
      "Epoch 3650: train loss: 0.06327103078365326\n",
      "Epoch 3660: train loss: 0.06320168823003769\n",
      "Epoch 3670: train loss: 0.06313424557447433\n",
      "Epoch 3680: train loss: 0.06306871026754379\n",
      "Epoch 3690: train loss: 0.06300496309995651\n",
      "Epoch 3700: train loss: 0.06294294446706772\n",
      "Epoch 3710: train loss: 0.06288260966539383\n",
      "Epoch 3720: train loss: 0.06282390654087067\n",
      "Epoch 3730: train loss: 0.06276679784059525\n",
      "Epoch 3740: train loss: 0.0627111867070198\n",
      "Epoch 3750: train loss: 0.06265706568956375\n",
      "Epoch 3760: train loss: 0.06260435283184052\n",
      "Epoch 3770: train loss: 0.0625530332326889\n",
      "Epoch 3780: train loss: 0.06250302493572235\n",
      "Epoch 3790: train loss: 0.06245431676506996\n",
      "Epoch 3800: train loss: 0.06240683048963547\n",
      "Epoch 3810: train loss: 0.06236056238412857\n",
      "Epoch 3820: train loss: 0.0623154453933239\n",
      "Epoch 3830: train loss: 0.06227143108844757\n",
      "Epoch 3840: train loss: 0.06222851574420929\n",
      "Epoch 3850: train loss: 0.062186650931835175\n",
      "Epoch 3860: train loss: 0.06214578449726105\n",
      "Epoch 3870: train loss: 0.06210588663816452\n",
      "Epoch 3880: train loss: 0.062066931277513504\n",
      "Epoch 3890: train loss: 0.06202888861298561\n",
      "Epoch 3900: train loss: 0.06199171021580696\n",
      "Epoch 3910: train loss: 0.06195538863539696\n",
      "Epoch 3920: train loss: 0.061919886618852615\n",
      "Epoch 3930: train loss: 0.06188517063856125\n",
      "Epoch 3940: train loss: 0.061851199716329575\n",
      "Epoch 3950: train loss: 0.06181797757744789\n",
      "Epoch 3960: train loss: 0.06178545951843262\n",
      "Epoch 3970: train loss: 0.06175362318754196\n",
      "Epoch 3980: train loss: 0.06172244995832443\n",
      "Epoch 3990: train loss: 0.06169192120432854\n",
      "Epoch 4000: train loss: 0.06166199594736099\n",
      "Epoch 4010: train loss: 0.061632659286260605\n",
      "Epoch 4020: train loss: 0.06160390377044678\n",
      "Epoch 4030: train loss: 0.061575695872306824\n",
      "Epoch 4040: train loss: 0.06154802814126015\n",
      "Epoch 4050: train loss: 0.061520855873823166\n",
      "Epoch 4060: train loss: 0.06149419769644737\n",
      "Epoch 4070: train loss: 0.061468008905649185\n",
      "Epoch 4080: train loss: 0.06144227832555771\n",
      "Epoch 4090: train loss: 0.06141699478030205\n",
      "Epoch 4100: train loss: 0.06139214336872101\n",
      "Epoch 4110: train loss: 0.061367690563201904\n",
      "Epoch 4120: train loss: 0.061343640089035034\n",
      "Epoch 4130: train loss: 0.0613199919462204\n",
      "Epoch 4140: train loss: 0.06129670515656471\n",
      "Epoch 4150: train loss: 0.06127377226948738\n",
      "Epoch 4160: train loss: 0.061251200735569\n",
      "Epoch 4170: train loss: 0.061228930950164795\n",
      "Epoch 4180: train loss: 0.061207015067338943\n",
      "Epoch 4190: train loss: 0.061185386031866074\n",
      "Epoch 4200: train loss: 0.061164066195487976\n",
      "Epoch 4210: train loss: 0.06114302575588226\n",
      "Epoch 4220: train loss: 0.061122264713048935\n",
      "Epoch 4230: train loss: 0.06110177934169769\n",
      "Epoch 4240: train loss: 0.06108153983950615\n",
      "Epoch 4250: train loss: 0.0610615536570549\n",
      "Epoch 4260: train loss: 0.061041805893182755\n",
      "Epoch 4270: train loss: 0.06102228909730911\n",
      "Epoch 4280: train loss: 0.06100299581885338\n",
      "Epoch 4290: train loss: 0.060983914881944656\n",
      "Epoch 4300: train loss: 0.06096504256129265\n",
      "Epoch 4310: train loss: 0.060946378856897354\n",
      "Epoch 4320: train loss: 0.06092787906527519\n",
      "Epoch 4330: train loss: 0.060909587889909744\n",
      "Epoch 4340: train loss: 0.06089146435260773\n",
      "Epoch 4350: train loss: 0.06087351590394974\n",
      "Epoch 4360: train loss: 0.06085573136806488\n",
      "Epoch 4370: train loss: 0.06083810329437256\n",
      "Epoch 4380: train loss: 0.06082063913345337\n",
      "Epoch 4390: train loss: 0.06080331653356552\n",
      "Epoch 4400: train loss: 0.06078613921999931\n",
      "Epoch 4410: train loss: 0.06076909974217415\n",
      "Epoch 4420: train loss: 0.06075219064950943\n",
      "Epoch 4430: train loss: 0.06073540449142456\n",
      "Epoch 4440: train loss: 0.06071874499320984\n",
      "Epoch 4450: train loss: 0.06070219725370407\n",
      "Epoch 4460: train loss: 0.060685764998197556\n",
      "Epoch 4470: train loss: 0.06066945940256119\n",
      "Epoch 4480: train loss: 0.060653235763311386\n",
      "Epoch 4490: train loss: 0.06063712760806084\n",
      "Epoch 4500: train loss: 0.06062111258506775\n",
      "Epoch 4510: train loss: 0.06060519069433212\n",
      "Epoch 4520: train loss: 0.06058936566114426\n",
      "Epoch 4530: train loss: 0.06057362258434296\n",
      "Epoch 4540: train loss: 0.06055796518921852\n",
      "Epoch 4550: train loss: 0.060542382299900055\n",
      "Epoch 4560: train loss: 0.06052687391638756\n",
      "Epoch 4570: train loss: 0.060511451214551926\n",
      "Epoch 4580: train loss: 0.060496099293231964\n",
      "Epoch 4590: train loss: 0.06048080325126648\n",
      "Epoch 4600: train loss: 0.06046559289097786\n",
      "Epoch 4610: train loss: 0.06045042723417282\n",
      "Epoch 4620: train loss: 0.06043533980846405\n",
      "Epoch 4630: train loss: 0.06042029708623886\n",
      "Epoch 4640: train loss: 0.060405321419239044\n",
      "Epoch 4650: train loss: 0.06039039418101311\n",
      "Epoch 4660: train loss: 0.06037551909685135\n",
      "Epoch 4670: train loss: 0.06036069244146347\n",
      "Epoch 4680: train loss: 0.06034592166543007\n",
      "Epoch 4690: train loss: 0.06033119186758995\n",
      "Epoch 4700: train loss: 0.060316503047943115\n",
      "Epoch 4710: train loss: 0.06030185893177986\n",
      "Epoch 4720: train loss: 0.06028726324439049\n",
      "Epoch 4730: train loss: 0.0602727048099041\n",
      "Epoch 4740: train loss: 0.0602581761777401\n",
      "Epoch 4750: train loss: 0.06024369224905968\n",
      "Epoch 4760: train loss: 0.06022924184799194\n",
      "Epoch 4770: train loss: 0.060214824974536896\n",
      "Epoch 4780: train loss: 0.060200441628694534\n",
      "Epoch 4790: train loss: 0.06018608435988426\n",
      "Epoch 4800: train loss: 0.060171764343976974\n",
      "Epoch 4810: train loss: 0.06015745922923088\n",
      "Epoch 4820: train loss: 0.06014319136738777\n",
      "Epoch 4830: train loss: 0.06012895330786705\n",
      "Epoch 4840: train loss: 0.06011473387479782\n",
      "Epoch 4850: train loss: 0.06010054424405098\n",
      "Epoch 4860: train loss: 0.06008637323975563\n",
      "Epoch 4870: train loss: 0.060072217136621475\n",
      "Epoch 4880: train loss: 0.060058098286390305\n",
      "Epoch 4890: train loss: 0.06004398688673973\n",
      "Epoch 4900: train loss: 0.060029901564121246\n",
      "Epoch 4910: train loss: 0.060015831142663956\n",
      "Epoch 4920: train loss: 0.06000179052352905\n",
      "Epoch 4930: train loss: 0.05998775735497475\n",
      "Epoch 4940: train loss: 0.05997374281287193\n",
      "Epoch 4950: train loss: 0.05995975062251091\n",
      "Epoch 4960: train loss: 0.05994576960802078\n",
      "Epoch 4970: train loss: 0.05993180721998215\n",
      "Epoch 4980: train loss: 0.059917859733104706\n",
      "Epoch 4990: train loss: 0.05990391597151756\n",
      "Epoch 5000: train loss: 0.05989000201225281\n",
      "Epoch 5010: train loss: 0.05987608805298805\n",
      "Epoch 5020: train loss: 0.05986219272017479\n",
      "Epoch 5030: train loss: 0.05984831228852272\n",
      "Epoch 5040: train loss: 0.05983443930745125\n",
      "Epoch 5050: train loss: 0.05982058495283127\n",
      "Epoch 5060: train loss: 0.059806738048791885\n",
      "Epoch 5070: train loss: 0.0597928985953331\n",
      "Epoch 5080: train loss: 0.05977907404303551\n",
      "Epoch 5090: train loss: 0.05976526066660881\n",
      "Epoch 5100: train loss: 0.05975146219134331\n",
      "Epoch 5110: train loss: 0.0597376711666584\n",
      "Epoch 5120: train loss: 0.05972388759255409\n",
      "Epoch 5130: train loss: 0.05971011146903038\n",
      "Epoch 5140: train loss: 0.05969635769724846\n",
      "Epoch 5150: train loss: 0.059682607650756836\n",
      "Epoch 5160: train loss: 0.05966885760426521\n",
      "Epoch 5170: train loss: 0.05965512990951538\n",
      "Epoch 5180: train loss: 0.05964139848947525\n",
      "Epoch 5190: train loss: 0.05962769314646721\n",
      "Epoch 5200: train loss: 0.05961398780345917\n",
      "Epoch 5210: train loss: 0.05960029363632202\n",
      "Epoch 5220: train loss: 0.05958661064505577\n",
      "Epoch 5230: train loss: 0.05957293510437012\n",
      "Epoch 5240: train loss: 0.05955926701426506\n",
      "Epoch 5250: train loss: 0.0595456063747406\n",
      "Epoch 5260: train loss: 0.05953195318579674\n",
      "Epoch 5270: train loss: 0.059518322348594666\n",
      "Epoch 5280: train loss: 0.05950469896197319\n",
      "Epoch 5290: train loss: 0.059491075575351715\n",
      "Epoch 5300: train loss: 0.05947745963931084\n",
      "Epoch 5310: train loss: 0.05946386232972145\n",
      "Epoch 5320: train loss: 0.05945027247071266\n",
      "Epoch 5330: train loss: 0.05943668633699417\n",
      "Epoch 5340: train loss: 0.05942312255501747\n",
      "Epoch 5350: train loss: 0.05940955877304077\n",
      "Epoch 5360: train loss: 0.059396013617515564\n",
      "Epoch 5370: train loss: 0.05938247591257095\n",
      "Epoch 5380: train loss: 0.05936894193291664\n",
      "Epoch 5390: train loss: 0.05935543030500412\n",
      "Epoch 5400: train loss: 0.0593419149518013\n",
      "Epoch 5410: train loss: 0.05932841822504997\n",
      "Epoch 5420: train loss: 0.05931493639945984\n",
      "Epoch 5430: train loss: 0.0593014620244503\n",
      "Epoch 5440: train loss: 0.05928799882531166\n",
      "Epoch 5450: train loss: 0.059274546802043915\n",
      "Epoch 5460: train loss: 0.059261102229356766\n",
      "Epoch 5470: train loss: 0.059247683733701706\n",
      "Epoch 5480: train loss: 0.059234265238046646\n",
      "Epoch 5490: train loss: 0.05922086164355278\n",
      "Epoch 5500: train loss: 0.059207476675510406\n",
      "Epoch 5510: train loss: 0.05919409543275833\n",
      "Epoch 5520: train loss: 0.05918073654174805\n",
      "Epoch 5530: train loss: 0.05916737765073776\n",
      "Epoch 5540: train loss: 0.05915404483675957\n",
      "Epoch 5550: train loss: 0.05914071947336197\n",
      "Epoch 5560: train loss: 0.05912741646170616\n",
      "Epoch 5570: train loss: 0.05911412090063095\n",
      "Epoch 5580: train loss: 0.05910084769129753\n",
      "Epoch 5590: train loss: 0.05908757075667381\n",
      "Epoch 5600: train loss: 0.05907432362437248\n",
      "Epoch 5610: train loss: 0.05906108766794205\n",
      "Epoch 5620: train loss: 0.059047870337963104\n",
      "Epoch 5630: train loss: 0.05903466045856476\n",
      "Epoch 5640: train loss: 0.0590214766561985\n",
      "Epoch 5650: train loss: 0.05900830775499344\n",
      "Epoch 5660: train loss: 0.05899515375494957\n",
      "Epoch 5670: train loss: 0.058982014656066895\n",
      "Epoch 5680: train loss: 0.05896889418363571\n",
      "Epoch 5690: train loss: 0.05895578861236572\n",
      "Epoch 5700: train loss: 0.05894269794225693\n",
      "Epoch 5710: train loss: 0.05892962962388992\n",
      "Epoch 5720: train loss: 0.05891657620668411\n",
      "Epoch 5730: train loss: 0.05890354886651039\n",
      "Epoch 5740: train loss: 0.05889052897691727\n",
      "Epoch 5750: train loss: 0.05887753516435623\n",
      "Epoch 5760: train loss: 0.05886455252766609\n",
      "Epoch 5770: train loss: 0.05885160341858864\n",
      "Epoch 5780: train loss: 0.05883867293596268\n",
      "Epoch 5790: train loss: 0.058825742453336716\n",
      "Epoch 5800: train loss: 0.058812856674194336\n",
      "Epoch 5810: train loss: 0.058799970895051956\n",
      "Epoch 5820: train loss: 0.058787114918231964\n",
      "Epoch 5830: train loss: 0.058774277567863464\n",
      "Epoch 5840: train loss: 0.058761462569236755\n",
      "Epoch 5850: train loss: 0.058748673647642136\n",
      "Epoch 5860: train loss: 0.05873590335249901\n",
      "Epoch 5870: train loss: 0.058723147958517075\n",
      "Epoch 5880: train loss: 0.05871042236685753\n",
      "Epoch 5890: train loss: 0.058697719126939774\n",
      "Epoch 5900: train loss: 0.05868503451347351\n",
      "Epoch 5910: train loss: 0.05867236852645874\n",
      "Epoch 5920: train loss: 0.05865973234176636\n",
      "Epoch 5930: train loss: 0.05864712595939636\n",
      "Epoch 5940: train loss: 0.058634527027606964\n",
      "Epoch 5950: train loss: 0.05862196162343025\n",
      "Epoch 5960: train loss: 0.05860942602157593\n",
      "Epoch 5970: train loss: 0.0585969015955925\n",
      "Epoch 5980: train loss: 0.05858440697193146\n",
      "Epoch 5990: train loss: 0.0585719458758831\n",
      "Epoch 6000: train loss: 0.05855950713157654\n",
      "Epoch 6010: train loss: 0.058547087013721466\n",
      "Epoch 6020: train loss: 0.05853468179702759\n",
      "Epoch 6030: train loss: 0.05852232500910759\n",
      "Epoch 6040: train loss: 0.058509986847639084\n",
      "Epoch 6050: train loss: 0.05849766731262207\n",
      "Epoch 6060: train loss: 0.058485377579927444\n",
      "Epoch 6070: train loss: 0.058473121374845505\n",
      "Epoch 6080: train loss: 0.05846088379621506\n",
      "Epoch 6090: train loss: 0.058448679745197296\n",
      "Epoch 6100: train loss: 0.058436498045921326\n",
      "Epoch 6110: train loss: 0.05842434614896774\n",
      "Epoch 6120: train loss: 0.05841221660375595\n",
      "Epoch 6130: train loss: 0.058400120586156845\n",
      "Epoch 6140: train loss: 0.05838805437088013\n",
      "Epoch 6150: train loss: 0.0583760142326355\n",
      "Epoch 6160: train loss: 0.05836399272084236\n",
      "Epoch 6170: train loss: 0.05835201218724251\n",
      "Epoch 6180: train loss: 0.05834006145596504\n",
      "Epoch 6190: train loss: 0.058328136801719666\n",
      "Epoch 6200: train loss: 0.05831623822450638\n",
      "Epoch 6210: train loss: 0.05830436944961548\n",
      "Epoch 6220: train loss: 0.05829254165291786\n",
      "Epoch 6230: train loss: 0.05828072503209114\n",
      "Epoch 6240: train loss: 0.0582689493894577\n",
      "Epoch 6250: train loss: 0.05825720354914665\n",
      "Epoch 6260: train loss: 0.058245494961738586\n",
      "Epoch 6270: train loss: 0.058233797550201416\n",
      "Epoch 6280: train loss: 0.058222148567438126\n",
      "Epoch 6290: train loss: 0.05821051821112633\n",
      "Epoch 6300: train loss: 0.05819893628358841\n",
      "Epoch 6310: train loss: 0.058187369257211685\n",
      "Epoch 6320: train loss: 0.05817583575844765\n",
      "Epoch 6330: train loss: 0.058164332062006\n",
      "Epoch 6340: train loss: 0.05815287306904793\n",
      "Epoch 6350: train loss: 0.05814144015312195\n",
      "Epoch 6360: train loss: 0.05813003331422806\n",
      "Epoch 6370: train loss: 0.058118660002946854\n",
      "Epoch 6380: train loss: 0.05810731649398804\n",
      "Epoch 6390: train loss: 0.0580960176885128\n",
      "Epoch 6400: train loss: 0.058084748685359955\n",
      "Epoch 6410: train loss: 0.0580735020339489\n",
      "Epoch 6420: train loss: 0.058062296360731125\n",
      "Epoch 6430: train loss: 0.05805111676454544\n",
      "Epoch 6440: train loss: 0.05803997814655304\n",
      "Epoch 6450: train loss: 0.058028869330883026\n",
      "Epoch 6460: train loss: 0.0580177865922451\n",
      "Epoch 6470: train loss: 0.05800674855709076\n",
      "Epoch 6480: train loss: 0.057995736598968506\n",
      "Epoch 6490: train loss: 0.057984765619039536\n",
      "Epoch 6500: train loss: 0.057973820716142654\n",
      "Epoch 6510: train loss: 0.057962920516729355\n",
      "Epoch 6520: train loss: 0.05795203894376755\n",
      "Epoch 6530: train loss: 0.05794120207428932\n",
      "Epoch 6540: train loss: 0.05793040245771408\n",
      "Epoch 6550: train loss: 0.05791962146759033\n",
      "Epoch 6560: train loss: 0.057908885180950165\n",
      "Epoch 6570: train loss: 0.05789817497134209\n",
      "Epoch 6580: train loss: 0.05788751319050789\n",
      "Epoch 6590: train loss: 0.05787688121199608\n",
      "Epoch 6600: train loss: 0.057866282761096954\n",
      "Epoch 6610: train loss: 0.05785571038722992\n",
      "Epoch 6620: train loss: 0.057845182716846466\n",
      "Epoch 6630: train loss: 0.0578346811234951\n",
      "Epoch 6640: train loss: 0.05782422050833702\n",
      "Epoch 6650: train loss: 0.057813797146081924\n",
      "Epoch 6660: train loss: 0.05780341103672981\n",
      "Epoch 6670: train loss: 0.05779304727911949\n",
      "Epoch 6680: train loss: 0.057782724499702454\n",
      "Epoch 6690: train loss: 0.057772450149059296\n",
      "Epoch 6700: train loss: 0.05776219069957733\n",
      "Epoch 6710: train loss: 0.05775197222828865\n",
      "Epoch 6720: train loss: 0.05774178355932236\n",
      "Epoch 6730: train loss: 0.05773164704442024\n",
      "Epoch 6740: train loss: 0.05772153288125992\n",
      "Epoch 6750: train loss: 0.05771144852042198\n",
      "Epoch 6760: train loss: 0.05770140886306763\n",
      "Epoch 6770: train loss: 0.057691410183906555\n",
      "Epoch 6780: train loss: 0.05768142640590668\n",
      "Epoch 6790: train loss: 0.057671498507261276\n",
      "Epoch 6800: train loss: 0.057661592960357666\n",
      "Epoch 6810: train loss: 0.05765172466635704\n",
      "Epoch 6820: train loss: 0.0576418973505497\n",
      "Epoch 6830: train loss: 0.05763209983706474\n",
      "Epoch 6840: train loss: 0.057622335851192474\n",
      "Epoch 6850: train loss: 0.05761261284351349\n",
      "Epoch 6860: train loss: 0.05760292708873749\n",
      "Epoch 6870: train loss: 0.057593267410993576\n",
      "Epoch 6880: train loss: 0.05758364498615265\n",
      "Epoch 06888: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 6890: train loss: 0.05757473036646843\n",
      "Epoch 06895: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 6900: train loss: 0.057572659105062485\n",
      "Epoch 06901: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 06907: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 6910: train loss: 0.057572316378355026\n",
      "Epoch 06913: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 06919: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 6920: train loss: 0.057572271674871445\n",
      "Epoch 06925: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 6930: train loss: 0.05757226422429085\n",
      "Epoch 06931: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 6931, loss: 0.05757226422429085\n",
      "Predictions saved to results-2-10-scale.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train).detach().cpu().numpy()\n",
    "scaler = StandardScaler()\n",
    "x_train_featured = scaler.fit_transform(x_train_featured)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy()).detach().cpu().numpy()\n",
    "x_test_featured = scaler.transform(x_test_featured)\n",
    "x_test_featured = torch.tensor(x_test_featured, dtype=torch.float).to(device)\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
