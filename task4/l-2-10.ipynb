{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 200\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=10, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-2-10----.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c78724489c486dbd2d4875e0163a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.593338571764985, val loss: 0.20773548460006713\n",
      "Epoch 2: train loss: 0.1848930295292212, val loss: 0.1635532375574112\n",
      "Epoch 3: train loss: 0.14851348034946285, val loss: 0.12730759930610658\n",
      "Epoch 4: train loss: 0.12711214770589557, val loss: 0.12456259059906005\n",
      "Epoch 5: train loss: 0.11078884008101055, val loss: 0.10769303387403488\n",
      "Epoch 6: train loss: 0.10153007074278228, val loss: 0.08991660994291306\n",
      "Epoch 7: train loss: 0.0886228990579138, val loss: 0.09236691677570343\n",
      "Epoch 8: train loss: 0.07970728954125424, val loss: 0.07234716492891312\n",
      "Epoch 9: train loss: 0.06952878614895198, val loss: 0.06930174261331558\n",
      "Epoch 10: train loss: 0.06195717613611903, val loss: 0.06008801412582397\n",
      "Epoch 11: train loss: 0.05385264152775005, val loss: 0.0491817626953125\n",
      "Epoch 12: train loss: 0.04342130570265711, val loss: 0.04446038433909416\n",
      "Epoch 13: train loss: 0.03493226637949749, val loss: 0.030469711393117906\n",
      "Epoch 14: train loss: 0.029017913381360016, val loss: 0.02823307625949383\n",
      "Epoch 15: train loss: 0.02469202127839838, val loss: 0.025028223931789398\n",
      "Epoch 16: train loss: 0.021236863673037412, val loss: 0.019114095196127893\n",
      "Epoch 17: train loss: 0.019170338749581455, val loss: 0.01854309318959713\n",
      "Epoch 18: train loss: 0.01631912820786238, val loss: 0.017185242369771005\n",
      "Epoch 19: train loss: 0.014458675147167274, val loss: 0.01402751685678959\n",
      "Epoch 20: train loss: 0.012423559460256781, val loss: 0.011493649527430535\n",
      "Epoch 21: train loss: 0.011224448431359262, val loss: 0.011800335384905339\n",
      "Epoch 22: train loss: 0.009854432200746877, val loss: 0.011086313404142857\n",
      "Epoch 23: train loss: 0.009209154909818756, val loss: 0.00932960633188486\n",
      "Epoch 24: train loss: 0.00800703217484513, val loss: 0.00853903215378523\n",
      "Epoch 25: train loss: 0.0076577499781032, val loss: 0.009164324298501015\n",
      "Epoch 26: train loss: 0.007099020755716732, val loss: 0.006930000733584165\n",
      "Epoch 27: train loss: 0.006670351416936942, val loss: 0.006729275602847338\n",
      "Epoch 28: train loss: 0.006314919431142661, val loss: 0.006269026074558496\n",
      "Epoch 29: train loss: 0.006166576405188867, val loss: 0.0069476356282830234\n",
      "Epoch 30: train loss: 0.006093365279007323, val loss: 0.006182224471122027\n",
      "Epoch 31: train loss: 0.006092464382703207, val loss: 0.006590175535529852\n",
      "Epoch 32: train loss: 0.00606592981661765, val loss: 0.006469682976603508\n",
      "Epoch 33: train loss: 0.00567308199968265, val loss: 0.006590089954435825\n",
      "Epoch 34: train loss: 0.005920255976900155, val loss: 0.006690348152071237\n",
      "Epoch 35: train loss: 0.005855612673625654, val loss: 0.005612229254096746\n",
      "Epoch 36: train loss: 0.005854546386444447, val loss: 0.006121569279581308\n",
      "Epoch 37: train loss: 0.005753973239842726, val loss: 0.0067143308036029335\n",
      "Epoch 38: train loss: 0.005819929502372231, val loss: 0.006272580876946449\n",
      "Epoch 39: train loss: 0.005649549100624055, val loss: 0.006822432242333889\n",
      "Epoch 40: train loss: 0.005837905200464385, val loss: 0.006597871158272028\n",
      "Epoch 00041: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 41: train loss: 0.00585911704523831, val loss: 0.006715684086084366\n",
      "Epoch 42: train loss: 0.003952532617641347, val loss: 0.003993032958358526\n",
      "Epoch 43: train loss: 0.0031689442323558793, val loss: 0.004028147775679827\n",
      "Epoch 44: train loss: 0.003131476780817825, val loss: 0.004282707158476114\n",
      "Epoch 45: train loss: 0.0030932525434740343, val loss: 0.004212635900825262\n",
      "Epoch 46: train loss: 0.0031086889911366967, val loss: 0.003952777849510312\n",
      "Epoch 47: train loss: 0.003238839231089366, val loss: 0.004052872490137816\n",
      "Epoch 48: train loss: 0.0031735439703476673, val loss: 0.004295214518904686\n",
      "Epoch 49: train loss: 0.0032371762293029803, val loss: 0.003929125934839249\n",
      "Epoch 50: train loss: 0.003166830798344953, val loss: 0.003575028842315078\n",
      "Epoch 51: train loss: 0.0032139230349614305, val loss: 0.003958185445517301\n",
      "Epoch 52: train loss: 0.0032208579014606623, val loss: 0.0038476599399000404\n",
      "Epoch 53: train loss: 0.0031260265168562835, val loss: 0.00452154066413641\n",
      "Epoch 54: train loss: 0.0030924755867418586, val loss: 0.0038716436214745044\n",
      "Epoch 55: train loss: 0.0030948687100744976, val loss: 0.0044898706339299675\n",
      "Epoch 00056: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 56: train loss: 0.003062393290693967, val loss: 0.004117341168224812\n",
      "Epoch 57: train loss: 0.0023482602336348927, val loss: 0.003155994167551398\n",
      "Epoch 58: train loss: 0.0020381316117258096, val loss: 0.0030593750346452\n",
      "Epoch 59: train loss: 0.001960751030290005, val loss: 0.003131951941177249\n",
      "Epoch 60: train loss: 0.0018877674701186467, val loss: 0.0031073562689125536\n",
      "Epoch 61: train loss: 0.0018827598449603028, val loss: 0.002784907095134258\n",
      "Epoch 62: train loss: 0.001847707649008656, val loss: 0.003030034497380257\n",
      "Epoch 63: train loss: 0.0018983966410160064, val loss: 0.0030927157830446957\n",
      "Epoch 64: train loss: 0.0018547736444149395, val loss: 0.0029612184800207617\n",
      "Epoch 65: train loss: 0.0018474225538345625, val loss: 0.003061442695558071\n",
      "Epoch 66: train loss: 0.0018265325619211915, val loss: 0.003074910653755069\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 67: train loss: 0.0018143425437488726, val loss: 0.003028634147718549\n",
      "Epoch 68: train loss: 0.0016488199886694854, val loss: 0.0029562278371304276\n",
      "Epoch 69: train loss: 0.0015559239607988572, val loss: 0.0031020155120640995\n",
      "Epoch 70: train loss: 0.0014814289189799099, val loss: 0.0028089785892516377\n",
      "Epoch 71: train loss: 0.0014751877146487941, val loss: 0.0029185081887990235\n",
      "Epoch 72: train loss: 0.0014670479057394728, val loss: 0.002868447767570615\n",
      "Epoch 73: train loss: 0.0014414760378398458, val loss: 0.0027504044380038976\n",
      "Epoch 74: train loss: 0.0014475306028652252, val loss: 0.0026480864770710467\n",
      "Epoch 75: train loss: 0.0014160389464576634, val loss: 0.0029113662037998436\n",
      "Epoch 76: train loss: 0.0014282747914976612, val loss: 0.0029498936887830496\n",
      "Epoch 77: train loss: 0.0014105852639150558, val loss: 0.0025713815595954656\n",
      "Epoch 78: train loss: 0.0013854897278456056, val loss: 0.0028315035086125136\n",
      "Epoch 79: train loss: 0.001405723969281024, val loss: 0.002888684308156371\n",
      "Epoch 80: train loss: 0.0013867554560645806, val loss: 0.002531701726838946\n",
      "Epoch 81: train loss: 0.0013773384347596034, val loss: 0.002560789152979851\n",
      "Epoch 82: train loss: 0.001376802995610906, val loss: 0.0026322015449404715\n",
      "Epoch 83: train loss: 0.0013621774761728486, val loss: 0.002506957748904824\n",
      "Epoch 84: train loss: 0.0013572236451187304, val loss: 0.002631504928693175\n",
      "Epoch 85: train loss: 0.0013464379581641787, val loss: 0.0027528952471911907\n",
      "Epoch 86: train loss: 0.0013379413426036434, val loss: 0.002562502248212695\n",
      "Epoch 87: train loss: 0.0013474034471057203, val loss: 0.002717016329988837\n",
      "Epoch 88: train loss: 0.0013270868922260646, val loss: 0.002730633419007063\n",
      "Epoch 00089: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 89: train loss: 0.0013233949774686172, val loss: 0.002857667563483119\n",
      "Epoch 90: train loss: 0.0012656581207767738, val loss: 0.0026829403806477785\n",
      "Epoch 91: train loss: 0.001246205456992041, val loss: 0.0025185514502227306\n",
      "Epoch 92: train loss: 0.001241570187618538, val loss: 0.002531515946611762\n",
      "Epoch 93: train loss: 0.001223780896381608, val loss: 0.0026221544109284877\n",
      "Epoch 94: train loss: 0.0012347067195104854, val loss: 0.0028188770618289708\n",
      "Epoch 00095: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 95: train loss: 0.0012001176800350755, val loss: 0.0027794140465557575\n",
      "Epoch 96: train loss: 0.0011874438856291224, val loss: 0.0025819445345550774\n",
      "Epoch 97: train loss: 0.0011917828370113762, val loss: 0.0026822299361228944\n",
      "Epoch 98: train loss: 0.0011858289065219613, val loss: 0.002657752467319369\n",
      "Epoch 99: train loss: 0.0011836460504338754, val loss: 0.0026806105487048627\n",
      "Epoch 100: train loss: 0.0011910920662767425, val loss: 0.0027448106780648233\n",
      "Epoch 00101: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 101: train loss: 0.0011919640957434871, val loss: 0.0026859346572309733\n",
      "Early stop at epoch 101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5f4468a3a040f687e125d7a6598c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 3.1186089515686035, val loss: 3.0650832653045654\n",
      "Epoch 2: train loss: 3.111391305923462, val loss: 3.0640416145324707\n",
      "Epoch 3: train loss: 3.104379653930664, val loss: 3.063145399093628\n",
      "Epoch 4: train loss: 3.0975770950317383, val loss: 3.062390089035034\n",
      "Epoch 5: train loss: 3.0909857749938965, val loss: 3.0617682933807373\n",
      "Epoch 6: train loss: 3.084608316421509, val loss: 3.0612728595733643\n",
      "Epoch 7: train loss: 3.0784459114074707, val loss: 3.06089448928833\n",
      "Epoch 8: train loss: 3.0724973678588867, val loss: 3.060621976852417\n",
      "Epoch 9: train loss: 3.0667619705200195, val loss: 3.0604441165924072\n",
      "Epoch 10: train loss: 3.0612380504608154, val loss: 3.060346841812134\n",
      "Epoch 11: train loss: 3.055922269821167, val loss: 3.060314655303955\n",
      "Epoch 12: train loss: 3.050809860229492, val loss: 3.060331344604492\n",
      "Epoch 13: train loss: 3.0458950996398926, val loss: 3.060378313064575\n",
      "Epoch 14: train loss: 3.041170597076416, val loss: 3.060436487197876\n",
      "Epoch 00015: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 15: train loss: 3.036628007888794, val loss: 3.0604844093322754\n",
      "Epoch 16: train loss: 3.0322582721710205, val loss: 3.060479164123535\n",
      "Epoch 17: train loss: 3.0309829711914062, val loss: 3.060445547103882\n",
      "Epoch 18: train loss: 3.029726505279541, val loss: 3.060384750366211\n",
      "Epoch 19: train loss: 3.0284879207611084, val loss: 3.060298442840576\n",
      "Epoch 20: train loss: 3.027265787124634, val loss: 3.0601863861083984\n",
      "Epoch 21: train loss: 3.026059150695801, val loss: 3.0600507259368896\n",
      "Epoch 22: train loss: 3.0248658657073975, val loss: 3.0598905086517334\n",
      "Epoch 23: train loss: 3.023686170578003, val loss: 3.0597081184387207\n",
      "Epoch 24: train loss: 3.0225179195404053, val loss: 3.059502363204956\n",
      "Epoch 25: train loss: 3.0213615894317627, val loss: 3.0592737197875977\n",
      "Epoch 26: train loss: 3.0202155113220215, val loss: 3.059023380279541\n",
      "Epoch 27: train loss: 3.0190796852111816, val loss: 3.058750867843628\n",
      "Epoch 28: train loss: 3.0179524421691895, val loss: 3.0584561824798584\n",
      "Epoch 29: train loss: 3.016834020614624, val loss: 3.0581400394439697\n",
      "Epoch 30: train loss: 3.015723466873169, val loss: 3.057802200317383\n",
      "Epoch 31: train loss: 3.014620304107666, val loss: 3.0574429035186768\n",
      "Epoch 32: train loss: 3.013524055480957, val loss: 3.0570626258850098\n",
      "Epoch 33: train loss: 3.012434244155884, val loss: 3.0566606521606445\n",
      "Epoch 34: train loss: 3.0113508701324463, val loss: 3.0562379360198975\n",
      "Epoch 35: train loss: 3.010272741317749, val loss: 3.0557937622070312\n",
      "Epoch 36: train loss: 3.009200096130371, val loss: 3.055328845977783\n",
      "Epoch 37: train loss: 3.008131980895996, val loss: 3.054842710494995\n",
      "Epoch 38: train loss: 3.007068634033203, val loss: 3.0543363094329834\n",
      "Epoch 39: train loss: 3.006009340286255, val loss: 3.05380916595459\n",
      "Epoch 40: train loss: 3.0049540996551514, val loss: 3.0532617568969727\n",
      "Epoch 41: train loss: 3.0039021968841553, val loss: 3.052694082260132\n",
      "Epoch 42: train loss: 3.0028533935546875, val loss: 3.0521063804626465\n",
      "Epoch 43: train loss: 3.0018081665039062, val loss: 3.051499128341675\n",
      "Epoch 44: train loss: 3.000765085220337, val loss: 3.050872564315796\n",
      "Epoch 45: train loss: 2.9997246265411377, val loss: 3.0502262115478516\n",
      "Epoch 46: train loss: 2.9986865520477295, val loss: 3.0495612621307373\n",
      "Epoch 47: train loss: 2.9976508617401123, val loss: 3.048877239227295\n",
      "Epoch 48: train loss: 2.9966166019439697, val loss: 3.048175096511841\n",
      "Epoch 49: train loss: 2.995584487915039, val loss: 3.047455310821533\n",
      "Epoch 50: train loss: 2.994553804397583, val loss: 3.046717405319214\n",
      "Epoch 51: train loss: 2.9935250282287598, val loss: 3.0459630489349365\n",
      "Epoch 52: train loss: 2.9924967288970947, val loss: 3.045191526412964\n",
      "Epoch 53: train loss: 2.9914700984954834, val loss: 3.044403553009033\n",
      "Epoch 54: train loss: 2.9904446601867676, val loss: 3.0435993671417236\n",
      "Epoch 55: train loss: 2.989419937133789, val loss: 3.0427803993225098\n",
      "Epoch 56: train loss: 2.988395929336548, val loss: 3.0419459342956543\n",
      "Epoch 57: train loss: 2.987372875213623, val loss: 3.0410966873168945\n",
      "Epoch 58: train loss: 2.9863505363464355, val loss: 3.040234088897705\n",
      "Epoch 59: train loss: 2.9853286743164062, val loss: 3.0393576622009277\n",
      "Epoch 60: train loss: 2.9843075275421143, val loss: 3.0384678840637207\n",
      "Epoch 61: train loss: 2.9832868576049805, val loss: 3.0375657081604004\n",
      "Epoch 62: train loss: 2.982266902923584, val loss: 3.036651372909546\n",
      "Epoch 63: train loss: 2.9812474250793457, val loss: 3.0357253551483154\n",
      "Epoch 64: train loss: 2.9802277088165283, val loss: 3.034788131713867\n",
      "Epoch 65: train loss: 2.9792087078094482, val loss: 3.0338404178619385\n",
      "Epoch 66: train loss: 2.9781901836395264, val loss: 3.0328826904296875\n",
      "Epoch 67: train loss: 2.9771716594696045, val loss: 3.0319154262542725\n",
      "Epoch 68: train loss: 2.976153612136841, val loss: 3.0309391021728516\n",
      "Epoch 69: train loss: 2.9751358032226562, val loss: 3.029954671859741\n",
      "Epoch 70: train loss: 2.9741177558898926, val loss: 3.028961181640625\n",
      "Epoch 71: train loss: 2.973100423812866, val loss: 3.0279605388641357\n",
      "Epoch 72: train loss: 2.972083568572998, val loss: 3.0269525051116943\n",
      "Epoch 73: train loss: 2.97106671333313, val loss: 3.025938034057617\n",
      "Epoch 74: train loss: 2.9700498580932617, val loss: 3.0249173641204834\n",
      "Epoch 75: train loss: 2.9690334796905518, val loss: 3.023890733718872\n",
      "Epoch 76: train loss: 2.9680168628692627, val loss: 3.0228583812713623\n",
      "Epoch 77: train loss: 2.967000961303711, val loss: 3.0218214988708496\n",
      "Epoch 78: train loss: 2.9659855365753174, val loss: 3.020780086517334\n",
      "Epoch 79: train loss: 2.9649698734283447, val loss: 3.0197341442108154\n",
      "Epoch 80: train loss: 2.9639546871185303, val loss: 3.0186843872070312\n",
      "Epoch 81: train loss: 2.962939500808716, val loss: 3.0176308155059814\n",
      "Epoch 82: train loss: 2.9619243144989014, val loss: 3.0165748596191406\n",
      "Epoch 83: train loss: 2.960909843444824, val loss: 3.0155155658721924\n",
      "Epoch 84: train loss: 2.959895610809326, val loss: 3.0144541263580322\n",
      "Epoch 85: train loss: 2.958881378173828, val loss: 3.013390064239502\n",
      "Epoch 86: train loss: 2.95786714553833, val loss: 3.012324571609497\n",
      "Epoch 87: train loss: 2.9568536281585693, val loss: 3.0112569332122803\n",
      "Epoch 88: train loss: 2.9558401107788086, val loss: 3.010188341140747\n",
      "Epoch 89: train loss: 2.954826831817627, val loss: 3.0091183185577393\n",
      "Epoch 90: train loss: 2.9538137912750244, val loss: 3.008047580718994\n",
      "Epoch 91: train loss: 2.952800989151001, val loss: 3.0069761276245117\n",
      "Epoch 92: train loss: 2.9517886638641357, val loss: 3.005903959274292\n",
      "Epoch 93: train loss: 2.9507765769958496, val loss: 3.004831314086914\n",
      "Epoch 94: train loss: 2.9497644901275635, val loss: 3.0037591457366943\n",
      "Epoch 95: train loss: 2.9487528800964355, val loss: 3.0026867389678955\n",
      "Epoch 96: train loss: 2.9477410316467285, val loss: 3.0016143321990967\n",
      "Epoch 97: train loss: 2.946730375289917, val loss: 3.000542402267456\n",
      "Epoch 98: train loss: 2.9457192420959473, val loss: 2.9994704723358154\n",
      "Epoch 99: train loss: 2.9447083473205566, val loss: 2.998399257659912\n",
      "Epoch 100: train loss: 2.943697929382324, val loss: 2.997328758239746\n",
      "Epoch 101: train loss: 2.94268798828125, val loss: 2.9962589740753174\n",
      "Epoch 102: train loss: 2.941678285598755, val loss: 2.995189666748047\n",
      "Epoch 103: train loss: 2.9406685829162598, val loss: 2.994121551513672\n",
      "Epoch 104: train loss: 2.9396586418151855, val loss: 2.9930543899536133\n",
      "Epoch 105: train loss: 2.938649892807007, val loss: 2.991988182067871\n",
      "Epoch 106: train loss: 2.937640428543091, val loss: 2.9909229278564453\n",
      "Epoch 107: train loss: 2.936631917953491, val loss: 2.989858865737915\n",
      "Epoch 108: train loss: 2.9356236457824707, val loss: 2.988795757293701\n",
      "Epoch 109: train loss: 2.93461537361145, val loss: 2.9877336025238037\n",
      "Epoch 110: train loss: 2.933607816696167, val loss: 2.9866726398468018\n",
      "Epoch 111: train loss: 2.9326000213623047, val loss: 2.9856128692626953\n",
      "Epoch 112: train loss: 2.9315924644470215, val loss: 2.9845540523529053\n",
      "Epoch 113: train loss: 2.9305853843688965, val loss: 2.9834964275360107\n",
      "Epoch 114: train loss: 2.9295787811279297, val loss: 2.9824397563934326\n",
      "Epoch 115: train loss: 2.928571939468384, val loss: 2.981384754180908\n",
      "Epoch 116: train loss: 2.927565813064575, val loss: 2.980330228805542\n",
      "Epoch 117: train loss: 2.9265599250793457, val loss: 2.9792773723602295\n",
      "Epoch 118: train loss: 2.925553560256958, val loss: 2.9782252311706543\n",
      "Epoch 119: train loss: 2.9245481491088867, val loss: 2.9771738052368164\n",
      "Epoch 120: train loss: 2.9235427379608154, val loss: 2.976123809814453\n",
      "Epoch 121: train loss: 2.9225375652313232, val loss: 2.975074529647827\n",
      "Epoch 122: train loss: 2.9215328693389893, val loss: 2.9740264415740967\n",
      "Epoch 123: train loss: 2.9205281734466553, val loss: 2.9729793071746826\n",
      "Epoch 124: train loss: 2.9195239543914795, val loss: 2.9719326496124268\n",
      "Epoch 125: train loss: 2.9185197353363037, val loss: 2.9708869457244873\n",
      "Epoch 126: train loss: 2.917515754699707, val loss: 2.9698421955108643\n",
      "Epoch 127: train loss: 2.9165124893188477, val loss: 2.9687981605529785\n",
      "Epoch 128: train loss: 2.915508985519409, val loss: 2.96775484085083\n",
      "Epoch 129: train loss: 2.914506196975708, val loss: 2.966712236404419\n",
      "Epoch 130: train loss: 2.9135031700134277, val loss: 2.965670347213745\n",
      "Epoch 131: train loss: 2.9125008583068848, val loss: 2.9646291732788086\n",
      "Epoch 132: train loss: 2.9114983081817627, val loss: 2.9635884761810303\n",
      "Epoch 133: train loss: 2.910496473312378, val loss: 2.9625484943389893\n",
      "Epoch 134: train loss: 2.909494400024414, val loss: 2.9615087509155273\n",
      "Epoch 135: train loss: 2.9084930419921875, val loss: 2.9604697227478027\n",
      "Epoch 136: train loss: 2.907491683959961, val loss: 2.9594311714172363\n",
      "Epoch 137: train loss: 2.9064905643463135, val loss: 2.958392858505249\n",
      "Epoch 138: train loss: 2.905489921569824, val loss: 2.957355499267578\n",
      "Epoch 139: train loss: 2.904489517211914, val loss: 2.9563181400299072\n",
      "Epoch 140: train loss: 2.903489351272583, val loss: 2.9552810192108154\n",
      "Epoch 141: train loss: 2.902488946914673, val loss: 2.954244613647461\n",
      "Epoch 142: train loss: 2.9014892578125, val loss: 2.9532082080841064\n",
      "Epoch 143: train loss: 2.9004898071289062, val loss: 2.95217227935791\n",
      "Epoch 144: train loss: 2.8994903564453125, val loss: 2.951136350631714\n",
      "Epoch 145: train loss: 2.898491144180298, val loss: 2.950101137161255\n",
      "Epoch 146: train loss: 2.8974928855895996, val loss: 2.949065923690796\n",
      "Epoch 147: train loss: 2.8964943885803223, val loss: 2.948030948638916\n",
      "Epoch 148: train loss: 2.895495653152466, val loss: 2.9469962120056152\n",
      "Epoch 149: train loss: 2.894498109817505, val loss: 2.9459614753723145\n",
      "Epoch 150: train loss: 2.8935000896453857, val loss: 2.9449269771575928\n",
      "Epoch 151: train loss: 2.892502546310425, val loss: 2.9438929557800293\n",
      "Epoch 152: train loss: 2.891505718231201, val loss: 2.942859172821045\n",
      "Epoch 153: train loss: 2.8905084133148193, val loss: 2.9418251514434814\n",
      "Epoch 154: train loss: 2.8895115852355957, val loss: 2.940791368484497\n",
      "Epoch 155: train loss: 2.8885152339935303, val loss: 2.939757823944092\n",
      "Epoch 156: train loss: 2.887519121170044, val loss: 2.9387245178222656\n",
      "Epoch 157: train loss: 2.8865232467651367, val loss: 2.9376914501190186\n",
      "Epoch 158: train loss: 2.8855273723602295, val loss: 2.9366581439971924\n",
      "Epoch 159: train loss: 2.8845317363739014, val loss: 2.9356253147125244\n",
      "Epoch 160: train loss: 2.8835368156433105, val loss: 2.9345920085906982\n",
      "Epoch 161: train loss: 2.8825418949127197, val loss: 2.9335594177246094\n",
      "Epoch 162: train loss: 2.881547212600708, val loss: 2.9325270652770996\n",
      "Epoch 163: train loss: 2.8805525302886963, val loss: 2.9314944744110107\n",
      "Epoch 164: train loss: 2.879558801651001, val loss: 2.93046236038208\n",
      "Epoch 165: train loss: 2.8785645961761475, val loss: 2.9294300079345703\n",
      "Epoch 166: train loss: 2.8775711059570312, val loss: 2.9283978939056396\n",
      "Epoch 167: train loss: 2.876577377319336, val loss: 2.927366018295288\n",
      "Epoch 168: train loss: 2.875584363937378, val loss: 2.9263341426849365\n",
      "Epoch 169: train loss: 2.874591588973999, val loss: 2.925302505493164\n",
      "Epoch 170: train loss: 2.873598575592041, val loss: 2.9242711067199707\n",
      "Epoch 171: train loss: 2.872606039047241, val loss: 2.9232399463653564\n",
      "Epoch 172: train loss: 2.8716139793395996, val loss: 2.922208547592163\n",
      "Epoch 173: train loss: 2.870622158050537, val loss: 2.921177625656128\n",
      "Epoch 174: train loss: 2.8696305751800537, val loss: 2.920146942138672\n",
      "Epoch 175: train loss: 2.868638753890991, val loss: 2.9191160202026367\n",
      "Epoch 176: train loss: 2.867648124694824, val loss: 2.9180855751037598\n",
      "Epoch 177: train loss: 2.866657257080078, val loss: 2.917055130004883\n",
      "Epoch 178: train loss: 2.865666389465332, val loss: 2.916024923324585\n",
      "Epoch 179: train loss: 2.864675998687744, val loss: 2.914994955062866\n",
      "Epoch 180: train loss: 2.8636858463287354, val loss: 2.9139652252197266\n",
      "Epoch 181: train loss: 2.8626961708068848, val loss: 2.912935733795166\n",
      "Epoch 182: train loss: 2.861706256866455, val loss: 2.9119064807891846\n",
      "Epoch 183: train loss: 2.8607170581817627, val loss: 2.910877227783203\n",
      "Epoch 184: train loss: 2.859727621078491, val loss: 2.90984845161438\n",
      "Epoch 185: train loss: 2.858738899230957, val loss: 2.9088196754455566\n",
      "Epoch 186: train loss: 2.857750415802002, val loss: 2.9077908992767334\n",
      "Epoch 187: train loss: 2.856762170791626, val loss: 2.9067623615264893\n",
      "Epoch 188: train loss: 2.85577392578125, val loss: 2.9057345390319824\n",
      "Epoch 189: train loss: 2.854785919189453, val loss: 2.9047067165374756\n",
      "Epoch 190: train loss: 2.8537983894348145, val loss: 2.903679370880127\n",
      "Epoch 191: train loss: 2.852811336517334, val loss: 2.90265154838562\n",
      "Epoch 192: train loss: 2.8518242835998535, val loss: 2.9016244411468506\n",
      "Epoch 193: train loss: 2.850837230682373, val loss: 2.900597333908081\n",
      "Epoch 194: train loss: 2.8498504161834717, val loss: 2.8995704650878906\n",
      "Epoch 195: train loss: 2.8488640785217285, val loss: 2.8985440731048584\n",
      "Epoch 196: train loss: 2.8478779792785645, val loss: 2.8975179195404053\n",
      "Epoch 197: train loss: 2.8468923568725586, val loss: 2.8964920043945312\n",
      "Epoch 198: train loss: 2.845906972885132, val loss: 2.895465850830078\n",
      "Epoch 199: train loss: 2.844921588897705, val loss: 2.8944404125213623\n",
      "Epoch 200: train loss: 2.8439366817474365, val loss: 2.8934152126312256\n",
      "Epoch 201: train loss: 2.842951774597168, val loss: 2.892390012741089\n",
      "Epoch 202: train loss: 2.8419671058654785, val loss: 2.8913652896881104\n",
      "Epoch 203: train loss: 2.8409829139709473, val loss: 2.890340805053711\n",
      "Epoch 204: train loss: 2.839998722076416, val loss: 2.8893165588378906\n",
      "Epoch 205: train loss: 2.839015007019043, val loss: 2.8882923126220703\n",
      "Epoch 206: train loss: 2.838031530380249, val loss: 2.887268543243408\n",
      "Epoch 207: train loss: 2.837048053741455, val loss: 2.886244535446167\n",
      "Epoch 208: train loss: 2.8360650539398193, val loss: 2.885221242904663\n",
      "Epoch 209: train loss: 2.8350822925567627, val loss: 2.8841986656188965\n",
      "Epoch 210: train loss: 2.834099769592285, val loss: 2.8831756114959717\n",
      "Epoch 211: train loss: 2.8331174850463867, val loss: 2.882153034210205\n",
      "Epoch 212: train loss: 2.8321356773376465, val loss: 2.8811309337615967\n",
      "Epoch 213: train loss: 2.8311538696289062, val loss: 2.880108594894409\n",
      "Epoch 214: train loss: 2.830172061920166, val loss: 2.87908673286438\n",
      "Epoch 215: train loss: 2.829190731048584, val loss: 2.878065347671509\n",
      "Epoch 216: train loss: 2.82820987701416, val loss: 2.8770437240600586\n",
      "Epoch 217: train loss: 2.8272287845611572, val loss: 2.876023054122925\n",
      "Epoch 218: train loss: 2.8262486457824707, val loss: 2.875002145767212\n",
      "Epoch 219: train loss: 2.825268268585205, val loss: 2.873981475830078\n",
      "Epoch 220: train loss: 2.8242878913879395, val loss: 2.8729610443115234\n",
      "Epoch 221: train loss: 2.8233084678649902, val loss: 2.871941089630127\n",
      "Epoch 222: train loss: 2.822329044342041, val loss: 2.8709211349487305\n",
      "Epoch 223: train loss: 2.821349859237671, val loss: 2.869901418685913\n",
      "Epoch 224: train loss: 2.82037091255188, val loss: 2.868882179260254\n",
      "Epoch 225: train loss: 2.819391965866089, val loss: 2.867863416671753\n",
      "Epoch 226: train loss: 2.818413496017456, val loss: 2.866844415664673\n",
      "Epoch 227: train loss: 2.8174350261688232, val loss: 2.86582612991333\n",
      "Epoch 228: train loss: 2.8164572715759277, val loss: 2.86480712890625\n",
      "Epoch 229: train loss: 2.8154795169830322, val loss: 2.8637890815734863\n",
      "Epoch 230: train loss: 2.814502000808716, val loss: 2.862771511077881\n",
      "Epoch 231: train loss: 2.8135247230529785, val loss: 2.861753463745117\n",
      "Epoch 232: train loss: 2.8125479221343994, val loss: 2.86073637008667\n",
      "Epoch 233: train loss: 2.811570882797241, val loss: 2.8597190380096436\n",
      "Epoch 234: train loss: 2.810594320297241, val loss: 2.8587021827697754\n",
      "Epoch 235: train loss: 2.8096184730529785, val loss: 2.8576855659484863\n",
      "Epoch 236: train loss: 2.8086423873901367, val loss: 2.8566691875457764\n",
      "Epoch 237: train loss: 2.807666540145874, val loss: 2.8556530475616455\n",
      "Epoch 238: train loss: 2.8066909313201904, val loss: 2.8546369075775146\n",
      "Epoch 239: train loss: 2.805715799331665, val loss: 2.853621482849121\n",
      "Epoch 240: train loss: 2.8047409057617188, val loss: 2.8526060581207275\n",
      "Epoch 241: train loss: 2.8037660121917725, val loss: 2.851590871810913\n",
      "Epoch 242: train loss: 2.8027918338775635, val loss: 2.8505759239196777\n",
      "Epoch 243: train loss: 2.8018174171447754, val loss: 2.8495614528656006\n",
      "Epoch 244: train loss: 2.8008437156677246, val loss: 2.8485469818115234\n",
      "Epoch 245: train loss: 2.7998697757720947, val loss: 2.8475329875946045\n",
      "Epoch 246: train loss: 2.798896551132202, val loss: 2.8465187549591064\n",
      "Epoch 247: train loss: 2.7979230880737305, val loss: 2.8455052375793457\n",
      "Epoch 248: train loss: 2.796950340270996, val loss: 2.844491958618164\n",
      "Epoch 249: train loss: 2.7959775924682617, val loss: 2.8434786796569824\n",
      "Epoch 250: train loss: 2.7950050830841064, val loss: 2.842465877532959\n",
      "Epoch 251: train loss: 2.7940328121185303, val loss: 2.8414533138275146\n",
      "Epoch 252: train loss: 2.793060779571533, val loss: 2.840440511703491\n",
      "Epoch 253: train loss: 2.7920889854431152, val loss: 2.839428663253784\n",
      "Epoch 254: train loss: 2.7911176681518555, val loss: 2.838416814804077\n",
      "Epoch 255: train loss: 2.790146589279175, val loss: 2.83740496635437\n",
      "Epoch 256: train loss: 2.7891757488250732, val loss: 2.8363938331604004\n",
      "Epoch 257: train loss: 2.7882046699523926, val loss: 2.8353824615478516\n",
      "Epoch 258: train loss: 2.787234306335449, val loss: 2.83437180519104\n",
      "Epoch 259: train loss: 2.786264181137085, val loss: 2.8333611488342285\n",
      "Epoch 260: train loss: 2.7852940559387207, val loss: 2.832350492477417\n",
      "Epoch 261: train loss: 2.7843244075775146, val loss: 2.8313403129577637\n",
      "Epoch 262: train loss: 2.7833549976348877, val loss: 2.8303303718566895\n",
      "Epoch 263: train loss: 2.7823855876922607, val loss: 2.8293209075927734\n",
      "Epoch 264: train loss: 2.781416654586792, val loss: 2.8283116817474365\n",
      "Epoch 265: train loss: 2.7804481983184814, val loss: 2.8273024559020996\n",
      "Epoch 266: train loss: 2.779479742050171, val loss: 2.826293706893921\n",
      "Epoch 267: train loss: 2.7785115242004395, val loss: 2.825284957885742\n",
      "Epoch 268: train loss: 2.777543783187866, val loss: 2.8242766857147217\n",
      "Epoch 269: train loss: 2.776576042175293, val loss: 2.8232686519622803\n",
      "Epoch 270: train loss: 2.7756083011627197, val loss: 2.822260618209839\n",
      "Epoch 271: train loss: 2.7746410369873047, val loss: 2.8212532997131348\n",
      "Epoch 272: train loss: 2.773674249649048, val loss: 2.8202457427978516\n",
      "Epoch 273: train loss: 2.772707462310791, val loss: 2.8192386627197266\n",
      "Epoch 274: train loss: 2.7717411518096924, val loss: 2.8182320594787598\n",
      "Epoch 275: train loss: 2.7707748413085938, val loss: 2.817225217819214\n",
      "Epoch 276: train loss: 2.769808769226074, val loss: 2.8162190914154053\n",
      "Epoch 277: train loss: 2.768843412399292, val loss: 2.815213203430176\n",
      "Epoch 278: train loss: 2.7678778171539307, val loss: 2.8142073154449463\n",
      "Epoch 279: train loss: 2.7669126987457275, val loss: 2.813201904296875\n",
      "Epoch 280: train loss: 2.7659475803375244, val loss: 2.8121964931488037\n",
      "Epoch 281: train loss: 2.7649829387664795, val loss: 2.8111915588378906\n",
      "Epoch 282: train loss: 2.7640187740325928, val loss: 2.8101871013641357\n",
      "Epoch 283: train loss: 2.763054370880127, val loss: 2.8091824054718018\n",
      "Epoch 284: train loss: 2.7620904445648193, val loss: 2.808178424835205\n",
      "Epoch 285: train loss: 2.7611265182495117, val loss: 2.8071744441986084\n",
      "Epoch 286: train loss: 2.7601630687713623, val loss: 2.806170701980591\n",
      "Epoch 287: train loss: 2.759199857711792, val loss: 2.8051674365997314\n",
      "Epoch 288: train loss: 2.75823712348938, val loss: 2.804164171218872\n",
      "Epoch 289: train loss: 2.7572741508483887, val loss: 2.803161144256592\n",
      "Epoch 290: train loss: 2.7563118934631348, val loss: 2.8021583557128906\n",
      "Epoch 291: train loss: 2.755349636077881, val loss: 2.8011562824249268\n",
      "Epoch 292: train loss: 2.754387617111206, val loss: 2.800153970718384\n",
      "Epoch 293: train loss: 2.7534258365631104, val loss: 2.799152135848999\n",
      "Epoch 294: train loss: 2.752464532852173, val loss: 2.7981507778167725\n",
      "Epoch 295: train loss: 2.7515032291412354, val loss: 2.797149419784546\n",
      "Epoch 296: train loss: 2.750542163848877, val loss: 2.7961485385894775\n",
      "Epoch 297: train loss: 2.7495815753936768, val loss: 2.795147657394409\n",
      "Epoch 298: train loss: 2.7486212253570557, val loss: 2.794147253036499\n",
      "Epoch 299: train loss: 2.7476611137390137, val loss: 2.7931466102600098\n",
      "Epoch 300: train loss: 2.7467010021209717, val loss: 2.792146682739258\n",
      "Epoch 301: train loss: 2.745741367340088, val loss: 2.791146993637085\n",
      "Epoch 302: train loss: 2.744781494140625, val loss: 2.790147542953491\n",
      "Epoch 303: train loss: 2.7438225746154785, val loss: 2.7891480922698975\n",
      "Epoch 304: train loss: 2.742863416671753, val loss: 2.788149356842041\n",
      "Epoch 305: train loss: 2.7419047355651855, val loss: 2.7871506214141846\n",
      "Epoch 306: train loss: 2.7409465312957764, val loss: 2.7861523628234863\n",
      "Epoch 307: train loss: 2.739988088607788, val loss: 2.785154104232788\n",
      "Epoch 308: train loss: 2.739030122756958, val loss: 2.784156322479248\n",
      "Epoch 309: train loss: 2.738072395324707, val loss: 2.783158779144287\n",
      "Epoch 310: train loss: 2.737114906311035, val loss: 2.782160997390747\n",
      "Epoch 311: train loss: 2.7361578941345215, val loss: 2.7811639308929443\n",
      "Epoch 312: train loss: 2.7352006435394287, val loss: 2.7801673412323\n",
      "Epoch 313: train loss: 2.7342441082000732, val loss: 2.7791707515716553\n",
      "Epoch 314: train loss: 2.7332875728607178, val loss: 2.77817440032959\n",
      "Epoch 315: train loss: 2.7323315143585205, val loss: 2.7771782875061035\n",
      "Epoch 316: train loss: 2.731375217437744, val loss: 2.7761826515197754\n",
      "Epoch 317: train loss: 2.730419635772705, val loss: 2.7751870155334473\n",
      "Epoch 318: train loss: 2.729464292526245, val loss: 2.7741916179656982\n",
      "Epoch 319: train loss: 2.728508949279785, val loss: 2.7731969356536865\n",
      "Epoch 320: train loss: 2.7275538444519043, val loss: 2.772202253341675\n",
      "Epoch 321: train loss: 2.7265992164611816, val loss: 2.771207571029663\n",
      "Epoch 322: train loss: 2.725644588470459, val loss: 2.7702136039733887\n",
      "Epoch 323: train loss: 2.7246901988983154, val loss: 2.7692196369171143\n",
      "Epoch 324: train loss: 2.72373628616333, val loss: 2.768225908279419\n",
      "Epoch 325: train loss: 2.7227823734283447, val loss: 2.767232656478882\n",
      "Epoch 326: train loss: 2.7218289375305176, val loss: 2.766239643096924\n",
      "Epoch 327: train loss: 2.7208755016326904, val loss: 2.765246629714966\n",
      "Epoch 328: train loss: 2.7199225425720215, val loss: 2.764254093170166\n",
      "Epoch 329: train loss: 2.7189698219299316, val loss: 2.7632617950439453\n",
      "Epoch 330: train loss: 2.718017339706421, val loss: 2.7622697353363037\n",
      "Epoch 331: train loss: 2.7170650959014893, val loss: 2.761277914047241\n",
      "Epoch 332: train loss: 2.7161130905151367, val loss: 2.760286331176758\n",
      "Epoch 333: train loss: 2.715161085128784, val loss: 2.7592952251434326\n",
      "Epoch 334: train loss: 2.714209794998169, val loss: 2.7583043575286865\n",
      "Epoch 335: train loss: 2.7132585048675537, val loss: 2.7573137283325195\n",
      "Epoch 336: train loss: 2.7123074531555176, val loss: 2.7563233375549316\n",
      "Epoch 337: train loss: 2.7113566398620605, val loss: 2.755333185195923\n",
      "Epoch 338: train loss: 2.7104060649871826, val loss: 2.754343271255493\n",
      "Epoch 339: train loss: 2.709455728530884, val loss: 2.7533538341522217\n",
      "Epoch 340: train loss: 2.708505630493164, val loss: 2.75236439704895\n",
      "Epoch 341: train loss: 2.7075560092926025, val loss: 2.751375436782837\n",
      "Epoch 342: train loss: 2.706606149673462, val loss: 2.7503862380981445\n",
      "Epoch 343: train loss: 2.7056570053100586, val loss: 2.7493979930877686\n",
      "Epoch 344: train loss: 2.7047078609466553, val loss: 2.7484097480773926\n",
      "Epoch 345: train loss: 2.703758716583252, val loss: 2.7474217414855957\n",
      "Epoch 346: train loss: 2.702810287475586, val loss: 2.746433973312378\n",
      "Epoch 347: train loss: 2.701862335205078, val loss: 2.7454466819763184\n",
      "Epoch 348: train loss: 2.700914144515991, val loss: 2.744459629058838\n",
      "Epoch 349: train loss: 2.6999664306640625, val loss: 2.7434725761413574\n",
      "Epoch 350: train loss: 2.6990184783935547, val loss: 2.742485761642456\n",
      "Epoch 351: train loss: 2.698071241378784, val loss: 2.741499423980713\n",
      "Epoch 352: train loss: 2.6971240043640137, val loss: 2.740513563156128\n",
      "Epoch 353: train loss: 2.6961774826049805, val loss: 2.739527463912964\n",
      "Epoch 354: train loss: 2.695230722427368, val loss: 2.738542079925537\n",
      "Epoch 355: train loss: 2.694284200668335, val loss: 2.7375569343566895\n",
      "Epoch 356: train loss: 2.69333815574646, val loss: 2.736571788787842\n",
      "Epoch 357: train loss: 2.692392110824585, val loss: 2.7355868816375732\n",
      "Epoch 358: train loss: 2.6914467811584473, val loss: 2.734602451324463\n",
      "Epoch 359: train loss: 2.6905012130737305, val loss: 2.7336184978485107\n",
      "Epoch 360: train loss: 2.689556360244751, val loss: 2.7326343059539795\n",
      "Epoch 361: train loss: 2.6886115074157715, val loss: 2.7316508293151855\n",
      "Epoch 362: train loss: 2.687666654586792, val loss: 2.7306671142578125\n",
      "Epoch 363: train loss: 2.6867222785949707, val loss: 2.7296836376190186\n",
      "Epoch 364: train loss: 2.6857781410217285, val loss: 2.72870135307312\n",
      "Epoch 365: train loss: 2.6848342418670654, val loss: 2.7277188301086426\n",
      "Epoch 366: train loss: 2.6838905811309814, val loss: 2.726736307144165\n",
      "Epoch 367: train loss: 2.6829469203948975, val loss: 2.725754499435425\n",
      "Epoch 368: train loss: 2.6820037364959717, val loss: 2.7247726917266846\n",
      "Epoch 369: train loss: 2.681060791015625, val loss: 2.7237911224365234\n",
      "Epoch 370: train loss: 2.6801183223724365, val loss: 2.7228095531463623\n",
      "Epoch 371: train loss: 2.679175615310669, val loss: 2.7218289375305176\n",
      "Epoch 372: train loss: 2.6782336235046387, val loss: 2.720848560333252\n",
      "Epoch 373: train loss: 2.6772916316986084, val loss: 2.7198679447174072\n",
      "Epoch 374: train loss: 2.6763498783111572, val loss: 2.7188878059387207\n",
      "Epoch 375: train loss: 2.675408363342285, val loss: 2.7179079055786133\n",
      "Epoch 376: train loss: 2.674467086791992, val loss: 2.716928243637085\n",
      "Epoch 377: train loss: 2.6735262870788574, val loss: 2.7159488201141357\n",
      "Epoch 378: train loss: 2.6725857257843018, val loss: 2.7149698734283447\n",
      "Epoch 379: train loss: 2.671645164489746, val loss: 2.7139909267425537\n",
      "Epoch 380: train loss: 2.6707046031951904, val loss: 2.713012456893921\n",
      "Epoch 381: train loss: 2.669764518737793, val loss: 2.7120344638824463\n",
      "Epoch 382: train loss: 2.6688249111175537, val loss: 2.7110564708709717\n",
      "Epoch 383: train loss: 2.6678855419158936, val loss: 2.7100789546966553\n",
      "Epoch 384: train loss: 2.6669464111328125, val loss: 2.709101438522339\n",
      "Epoch 385: train loss: 2.6660072803497314, val loss: 2.7081246376037598\n",
      "Epoch 386: train loss: 2.6650683879852295, val loss: 2.7071473598480225\n",
      "Epoch 387: train loss: 2.6641299724578857, val loss: 2.7061712741851807\n",
      "Epoch 388: train loss: 2.663191556930542, val loss: 2.7051944732666016\n",
      "Epoch 389: train loss: 2.6622536182403564, val loss: 2.704218626022339\n",
      "Epoch 390: train loss: 2.66131591796875, val loss: 2.7032430171966553\n",
      "Epoch 391: train loss: 2.6603782176971436, val loss: 2.7022674083709717\n",
      "Epoch 392: train loss: 2.6594409942626953, val loss: 2.701292037963867\n",
      "Epoch 393: train loss: 2.658503770828247, val loss: 2.700317144393921\n",
      "Epoch 394: train loss: 2.657567024230957, val loss: 2.699342727661133\n",
      "Epoch 395: train loss: 2.656630277633667, val loss: 2.6983680725097656\n",
      "Epoch 396: train loss: 2.6556942462921143, val loss: 2.6973941326141357\n",
      "Epoch 397: train loss: 2.6547582149505615, val loss: 2.696420192718506\n",
      "Epoch 398: train loss: 2.653822183609009, val loss: 2.695446491241455\n",
      "Epoch 399: train loss: 2.6528866291046143, val loss: 2.6944737434387207\n",
      "Epoch 400: train loss: 2.6519510746002197, val loss: 2.693500280380249\n",
      "Epoch 401: train loss: 2.6510159969329834, val loss: 2.6925277709960938\n",
      "Epoch 402: train loss: 2.650081157684326, val loss: 2.6915550231933594\n",
      "Epoch 403: train loss: 2.649146556854248, val loss: 2.6905829906463623\n",
      "Epoch 404: train loss: 2.64821195602417, val loss: 2.6896111965179443\n",
      "Epoch 405: train loss: 2.647278070449829, val loss: 2.6886394023895264\n",
      "Epoch 406: train loss: 2.646343946456909, val loss: 2.6876678466796875\n",
      "Epoch 407: train loss: 2.6454102993011475, val loss: 2.686696767807007\n",
      "Epoch 408: train loss: 2.644477128982544, val loss: 2.6857261657714844\n",
      "Epoch 409: train loss: 2.6435439586639404, val loss: 2.684755325317383\n",
      "Epoch 410: train loss: 2.642611026763916, val loss: 2.6837852001190186\n",
      "Epoch 411: train loss: 2.6416783332824707, val loss: 2.6828153133392334\n",
      "Epoch 412: train loss: 2.6407458782196045, val loss: 2.6818454265594482\n",
      "Epoch 413: train loss: 2.6398136615753174, val loss: 2.6808762550354004\n",
      "Epoch 414: train loss: 2.6388814449310303, val loss: 2.6799066066741943\n",
      "Epoch 415: train loss: 2.6379499435424805, val loss: 2.6789376735687256\n",
      "Epoch 416: train loss: 2.6370182037353516, val loss: 2.677969217300415\n",
      "Epoch 417: train loss: 2.63608717918396, val loss: 2.6770007610321045\n",
      "Epoch 418: train loss: 2.6351559162139893, val loss: 2.676032781600952\n",
      "Epoch 419: train loss: 2.634225606918335, val loss: 2.6750648021698\n",
      "Epoch 420: train loss: 2.6332948207855225, val loss: 2.6740975379943848\n",
      "Epoch 421: train loss: 2.6323647499084473, val loss: 2.6731300354003906\n",
      "Epoch 422: train loss: 2.631434440612793, val loss: 2.6721630096435547\n",
      "Epoch 423: train loss: 2.630504846572876, val loss: 2.671196222305298\n",
      "Epoch 424: train loss: 2.629575252532959, val loss: 2.6702301502227783\n",
      "Epoch 425: train loss: 2.628645896911621, val loss: 2.6692638397216797\n",
      "Epoch 426: train loss: 2.6277172565460205, val loss: 2.668297529220581\n",
      "Epoch 427: train loss: 2.6267881393432617, val loss: 2.667332172393799\n",
      "Epoch 428: train loss: 2.6258597373962402, val loss: 2.6663668155670166\n",
      "Epoch 429: train loss: 2.6249313354492188, val loss: 2.6654016971588135\n",
      "Epoch 430: train loss: 2.6240029335021973, val loss: 2.6644370555877686\n",
      "Epoch 431: train loss: 2.623075246810913, val loss: 2.6634724140167236\n",
      "Epoch 432: train loss: 2.622147798538208, val loss: 2.662508010864258\n",
      "Epoch 433: train loss: 2.621220111846924, val loss: 2.6615443229675293\n",
      "Epoch 434: train loss: 2.620293140411377, val loss: 2.6605803966522217\n",
      "Epoch 435: train loss: 2.619366407394409, val loss: 2.6596171855926514\n",
      "Epoch 436: train loss: 2.6184394359588623, val loss: 2.658653974533081\n",
      "Epoch 437: train loss: 2.6175131797790527, val loss: 2.657691240310669\n",
      "Epoch 438: train loss: 2.6165871620178223, val loss: 2.656728506088257\n",
      "Epoch 439: train loss: 2.615661382675171, val loss: 2.655766010284424\n",
      "Epoch 440: train loss: 2.6147353649139404, val loss: 2.654803991317749\n",
      "Epoch 441: train loss: 2.6138100624084473, val loss: 2.6538422107696533\n",
      "Epoch 442: train loss: 2.612884998321533, val loss: 2.652880907058716\n",
      "Epoch 443: train loss: 2.611959934234619, val loss: 2.651919364929199\n",
      "Epoch 444: train loss: 2.611035108566284, val loss: 2.650958776473999\n",
      "Epoch 445: train loss: 2.6101107597351074, val loss: 2.6499977111816406\n",
      "Epoch 446: train loss: 2.6091866493225098, val loss: 2.6490373611450195\n",
      "Epoch 447: train loss: 2.608262538909912, val loss: 2.6480770111083984\n",
      "Epoch 448: train loss: 2.6073386669158936, val loss: 2.6471173763275146\n",
      "Epoch 449: train loss: 2.606415271759033, val loss: 2.64615797996521\n",
      "Epoch 450: train loss: 2.605491876602173, val loss: 2.645198106765747\n",
      "Epoch 451: train loss: 2.60456919670105, val loss: 2.6442394256591797\n",
      "Epoch 452: train loss: 2.6036462783813477, val loss: 2.6432807445526123\n",
      "Epoch 453: train loss: 2.6027235984802246, val loss: 2.642322063446045\n",
      "Epoch 454: train loss: 2.6018013954162598, val loss: 2.6413638591766357\n",
      "Epoch 455: train loss: 2.600879192352295, val loss: 2.6404058933258057\n",
      "Epoch 456: train loss: 2.599957227706909, val loss: 2.6394481658935547\n",
      "Epoch 457: train loss: 2.5990357398986816, val loss: 2.638490676879883\n",
      "Epoch 458: train loss: 2.598114490509033, val loss: 2.637533664703369\n",
      "Epoch 459: train loss: 2.5971932411193848, val loss: 2.6365766525268555\n",
      "Epoch 460: train loss: 2.5962724685668945, val loss: 2.6356201171875\n",
      "Epoch 461: train loss: 2.5953519344329834, val loss: 2.6346640586853027\n",
      "Epoch 462: train loss: 2.5944314002990723, val loss: 2.6337077617645264\n",
      "Epoch 463: train loss: 2.5935113430023193, val loss: 2.6327521800994873\n",
      "Epoch 464: train loss: 2.5925915241241455, val loss: 2.6317965984344482\n",
      "Epoch 465: train loss: 2.5916717052459717, val loss: 2.6308412551879883\n",
      "Epoch 466: train loss: 2.590752124786377, val loss: 2.6298863887786865\n",
      "Epoch 467: train loss: 2.5898330211639404, val loss: 2.628931760787964\n",
      "Epoch 468: train loss: 2.588914155960083, val loss: 2.6279773712158203\n",
      "Epoch 469: train loss: 2.5879952907562256, val loss: 2.627023458480835\n",
      "Epoch 470: train loss: 2.5870769023895264, val loss: 2.6260693073272705\n",
      "Epoch 471: train loss: 2.5861587524414062, val loss: 2.6251161098480225\n",
      "Epoch 472: train loss: 2.5852408409118652, val loss: 2.624162435531616\n",
      "Epoch 473: train loss: 2.584322929382324, val loss: 2.6232094764709473\n",
      "Epoch 474: train loss: 2.5834054946899414, val loss: 2.6222565174102783\n",
      "Epoch 475: train loss: 2.5824882984161377, val loss: 2.6213042736053467\n",
      "Epoch 476: train loss: 2.581571102142334, val loss: 2.620351791381836\n",
      "Epoch 477: train loss: 2.5806543827056885, val loss: 2.6194000244140625\n",
      "Epoch 478: train loss: 2.579737663269043, val loss: 2.618448495864868\n",
      "Epoch 479: train loss: 2.5788216590881348, val loss: 2.617496967315674\n",
      "Epoch 480: train loss: 2.5779054164886475, val loss: 2.6165459156036377\n",
      "Epoch 481: train loss: 2.5769894123077393, val loss: 2.6155951023101807\n",
      "Epoch 482: train loss: 2.5760738849639893, val loss: 2.6146445274353027\n",
      "Epoch 483: train loss: 2.5751585960388184, val loss: 2.613694429397583\n",
      "Epoch 484: train loss: 2.5742435455322266, val loss: 2.612744092941284\n",
      "Epoch 485: train loss: 2.5733284950256348, val loss: 2.6117942333221436\n",
      "Epoch 486: train loss: 2.572413921356201, val loss: 2.610844612121582\n",
      "Epoch 487: train loss: 2.5714995861053467, val loss: 2.609895706176758\n",
      "Epoch 488: train loss: 2.570585250854492, val loss: 2.6089465618133545\n",
      "Epoch 489: train loss: 2.569671392440796, val loss: 2.6079978942871094\n",
      "Epoch 490: train loss: 2.5687577724456787, val loss: 2.6070497035980225\n",
      "Epoch 491: train loss: 2.5678439140319824, val loss: 2.6061017513275146\n",
      "Epoch 492: train loss: 2.5669310092926025, val loss: 2.605153799057007\n",
      "Epoch 493: train loss: 2.5660181045532227, val loss: 2.604206085205078\n",
      "Epoch 494: train loss: 2.5651051998138428, val loss: 2.6032588481903076\n",
      "Epoch 495: train loss: 2.564192771911621, val loss: 2.602311849594116\n",
      "Epoch 496: train loss: 2.5632805824279785, val loss: 2.601364850997925\n",
      "Epoch 497: train loss: 2.562368631362915, val loss: 2.60041880607605\n",
      "Epoch 498: train loss: 2.5614566802978516, val loss: 2.5994722843170166\n",
      "Epoch 499: train loss: 2.5605452060699463, val loss: 2.5985264778137207\n",
      "Epoch 500: train loss: 2.55963397026062, val loss: 2.597580909729004\n",
      "Epoch 501: train loss: 2.558722734451294, val loss: 2.596635341644287\n",
      "Epoch 502: train loss: 2.557811975479126, val loss: 2.5956902503967285\n",
      "Epoch 503: train loss: 2.556901454925537, val loss: 2.594745397567749\n",
      "Epoch 504: train loss: 2.5559909343719482, val loss: 2.5938007831573486\n",
      "Epoch 505: train loss: 2.5550808906555176, val loss: 2.5928561687469482\n",
      "Epoch 506: train loss: 2.554171085357666, val loss: 2.591912269592285\n",
      "Epoch 507: train loss: 2.5532612800598145, val loss: 2.5909688472747803\n",
      "Epoch 508: train loss: 2.552351951599121, val loss: 2.590024948120117\n",
      "Epoch 509: train loss: 2.551442861557007, val loss: 2.5890817642211914\n",
      "Epoch 510: train loss: 2.5505337715148926, val loss: 2.5881385803222656\n",
      "Epoch 511: train loss: 2.5496251583099365, val loss: 2.587196111679077\n",
      "Epoch 512: train loss: 2.5487165451049805, val loss: 2.5862538814544678\n",
      "Epoch 513: train loss: 2.5478084087371826, val loss: 2.5853116512298584\n",
      "Epoch 514: train loss: 2.546900510787964, val loss: 2.584369659423828\n",
      "Epoch 515: train loss: 2.545992374420166, val loss: 2.583428144454956\n",
      "Epoch 516: train loss: 2.5450849533081055, val loss: 2.582486629486084\n",
      "Epoch 517: train loss: 2.544177770614624, val loss: 2.58154559135437\n",
      "Epoch 518: train loss: 2.5432708263397217, val loss: 2.5806050300598145\n",
      "Epoch 519: train loss: 2.5423638820648193, val loss: 2.5796642303466797\n",
      "Epoch 520: train loss: 2.541457414627075, val loss: 2.5787243843078613\n",
      "Epoch 521: train loss: 2.54055118560791, val loss: 2.577784299850464\n",
      "Epoch 522: train loss: 2.539645195007324, val loss: 2.5768444538116455\n",
      "Epoch 523: train loss: 2.538738965988159, val loss: 2.5759053230285645\n",
      "Epoch 524: train loss: 2.5378334522247314, val loss: 2.574965715408325\n",
      "Epoch 525: train loss: 2.536928176879883, val loss: 2.5740268230438232\n",
      "Epoch 526: train loss: 2.536022901535034, val loss: 2.5730884075164795\n",
      "Epoch 527: train loss: 2.5351181030273438, val loss: 2.5721499919891357\n",
      "Epoch 528: train loss: 2.5342133045196533, val loss: 2.571211814880371\n",
      "Epoch 529: train loss: 2.533308982849121, val loss: 2.5702741146087646\n",
      "Epoch 530: train loss: 2.532404661178589, val loss: 2.569336414337158\n",
      "Epoch 531: train loss: 2.5315005779266357, val loss: 2.568399429321289\n",
      "Epoch 532: train loss: 2.530596971511841, val loss: 2.567462205886841\n",
      "Epoch 533: train loss: 2.529693603515625, val loss: 2.5665252208709717\n",
      "Epoch 534: train loss: 2.528790235519409, val loss: 2.56558895111084\n",
      "Epoch 535: train loss: 2.5278873443603516, val loss: 2.564652681350708\n",
      "Epoch 536: train loss: 2.526984453201294, val loss: 2.5637168884277344\n",
      "Epoch 537: train loss: 2.5260820388793945, val loss: 2.5627810955047607\n",
      "Epoch 538: train loss: 2.525179862976074, val loss: 2.5618457794189453\n",
      "Epoch 539: train loss: 2.524277448654175, val loss: 2.560910940170288\n",
      "Epoch 540: train loss: 2.5233757495880127, val loss: 2.5599756240844727\n",
      "Epoch 541: train loss: 2.5224740505218506, val loss: 2.5590412616729736\n",
      "Epoch 542: train loss: 2.5215725898742676, val loss: 2.5581068992614746\n",
      "Epoch 543: train loss: 2.5206713676452637, val loss: 2.5571727752685547\n",
      "Epoch 544: train loss: 2.519770622253418, val loss: 2.556239128112793\n",
      "Epoch 545: train loss: 2.5188698768615723, val loss: 2.5553054809570312\n",
      "Epoch 546: train loss: 2.5179696083068848, val loss: 2.554372549057007\n",
      "Epoch 547: train loss: 2.5170693397521973, val loss: 2.5534393787384033\n",
      "Epoch 548: train loss: 2.516169548034668, val loss: 2.552506685256958\n",
      "Epoch 549: train loss: 2.5152697563171387, val loss: 2.551574468612671\n",
      "Epoch 550: train loss: 2.5143702030181885, val loss: 2.5506420135498047\n",
      "Epoch 551: train loss: 2.5134708881378174, val loss: 2.549710273742676\n",
      "Epoch 552: train loss: 2.5125720500946045, val loss: 2.548779010772705\n",
      "Epoch 553: train loss: 2.5116729736328125, val loss: 2.547847270965576\n",
      "Epoch 554: train loss: 2.5107743740081787, val loss: 2.5469160079956055\n",
      "Epoch 555: train loss: 2.509876251220703, val loss: 2.545985460281372\n",
      "Epoch 556: train loss: 2.5089778900146484, val loss: 2.5450549125671387\n",
      "Epoch 557: train loss: 2.50808048248291, val loss: 2.5441243648529053\n",
      "Epoch 558: train loss: 2.5071825981140137, val loss: 2.543194532394409\n",
      "Epoch 559: train loss: 2.5062854290008545, val loss: 2.542264699935913\n",
      "Epoch 560: train loss: 2.505388021469116, val loss: 2.541335105895996\n",
      "Epoch 561: train loss: 2.5044913291931152, val loss: 2.5404059886932373\n",
      "Epoch 562: train loss: 2.5035946369171143, val loss: 2.5394768714904785\n",
      "Epoch 563: train loss: 2.5026981830596924, val loss: 2.538548231124878\n",
      "Epoch 564: train loss: 2.5018017292022705, val loss: 2.5376200675964355\n",
      "Epoch 565: train loss: 2.500905990600586, val loss: 2.536691665649414\n",
      "Epoch 566: train loss: 2.5000102519989014, val loss: 2.535763740539551\n",
      "Epoch 567: train loss: 2.499114513397217, val loss: 2.5348362922668457\n",
      "Epoch 568: train loss: 2.4982192516326904, val loss: 2.5339088439941406\n",
      "Epoch 569: train loss: 2.497324228286743, val loss: 2.5329816341400146\n",
      "Epoch 570: train loss: 2.496429443359375, val loss: 2.5320546627044678\n",
      "Epoch 571: train loss: 2.495534896850586, val loss: 2.531128406524658\n",
      "Epoch 572: train loss: 2.494640588760376, val loss: 2.5302019119262695\n",
      "Epoch 573: train loss: 2.493746280670166, val loss: 2.529276132583618\n",
      "Epoch 574: train loss: 2.4928524494171143, val loss: 2.528350591659546\n",
      "Epoch 575: train loss: 2.4919590950012207, val loss: 2.5274248123168945\n",
      "Epoch 576: train loss: 2.491065502166748, val loss: 2.5264995098114014\n",
      "Epoch 577: train loss: 2.4901723861694336, val loss: 2.5255744457244873\n",
      "Epoch 578: train loss: 2.4892795085906982, val loss: 2.5246498584747314\n",
      "Epoch 579: train loss: 2.488386869430542, val loss: 2.5237255096435547\n",
      "Epoch 580: train loss: 2.4874942302703857, val loss: 2.522801399230957\n",
      "Epoch 581: train loss: 2.4866020679473877, val loss: 2.5218775272369385\n",
      "Epoch 582: train loss: 2.4857101440429688, val loss: 2.520953893661499\n",
      "Epoch 583: train loss: 2.48481822013855, val loss: 2.5200302600860596\n",
      "Epoch 584: train loss: 2.483926773071289, val loss: 2.5191075801849365\n",
      "Epoch 585: train loss: 2.4830353260040283, val loss: 2.5181844234466553\n",
      "Epoch 586: train loss: 2.4821441173553467, val loss: 2.5172617435455322\n",
      "Epoch 587: train loss: 2.4812533855438232, val loss: 2.5163395404815674\n",
      "Epoch 588: train loss: 2.4803626537323, val loss: 2.5154178142547607\n",
      "Epoch 589: train loss: 2.4794726371765137, val loss: 2.514495849609375\n",
      "Epoch 590: train loss: 2.4785823822021484, val loss: 2.5135743618011475\n",
      "Epoch 591: train loss: 2.4776923656463623, val loss: 2.512653112411499\n",
      "Epoch 592: train loss: 2.4768025875091553, val loss: 2.5117318630218506\n",
      "Epoch 593: train loss: 2.4759132862091064, val loss: 2.5108113288879395\n",
      "Epoch 594: train loss: 2.4750242233276367, val loss: 2.5098907947540283\n",
      "Epoch 595: train loss: 2.474135160446167, val loss: 2.5089704990386963\n",
      "Epoch 596: train loss: 2.4732463359832764, val loss: 2.5080506801605225\n",
      "Epoch 597: train loss: 2.472357749938965, val loss: 2.5071310997009277\n",
      "Epoch 598: train loss: 2.4714694023132324, val loss: 2.506211757659912\n",
      "Epoch 599: train loss: 2.470581531524658, val loss: 2.5052924156188965\n",
      "Epoch 600: train loss: 2.469693660736084, val loss: 2.504373550415039\n",
      "Epoch 601: train loss: 2.468806028366089, val loss: 2.5034549236297607\n",
      "Epoch 602: train loss: 2.467918634414673, val loss: 2.5025365352630615\n",
      "Epoch 603: train loss: 2.467031478881836, val loss: 2.5016183853149414\n",
      "Epoch 604: train loss: 2.4661448001861572, val loss: 2.5007007122039795\n",
      "Epoch 605: train loss: 2.4652578830718994, val loss: 2.4997832775115967\n",
      "Epoch 606: train loss: 2.464371919631958, val loss: 2.498865842819214\n",
      "Epoch 607: train loss: 2.4634852409362793, val loss: 2.4979488849639893\n",
      "Epoch 608: train loss: 2.462599515914917, val loss: 2.4970319271087646\n",
      "Epoch 609: train loss: 2.4617137908935547, val loss: 2.4961154460906982\n",
      "Epoch 610: train loss: 2.4608280658721924, val loss: 2.495199203491211\n",
      "Epoch 611: train loss: 2.4599430561065674, val loss: 2.494283437728882\n",
      "Epoch 612: train loss: 2.459057569503784, val loss: 2.4933676719665527\n",
      "Epoch 613: train loss: 2.4581730365753174, val loss: 2.4924519062042236\n",
      "Epoch 614: train loss: 2.4572882652282715, val loss: 2.491536855697632\n",
      "Epoch 615: train loss: 2.456404209136963, val loss: 2.490622043609619\n",
      "Epoch 616: train loss: 2.455519914627075, val loss: 2.4897072315216064\n",
      "Epoch 617: train loss: 2.4546360969543457, val loss: 2.488793134689331\n",
      "Epoch 618: train loss: 2.4537525177001953, val loss: 2.4878787994384766\n",
      "Epoch 619: train loss: 2.452868938446045, val loss: 2.4869649410247803\n",
      "Epoch 620: train loss: 2.4519858360290527, val loss: 2.486051321029663\n",
      "Epoch 621: train loss: 2.4511032104492188, val loss: 2.485137939453125\n",
      "Epoch 622: train loss: 2.4502203464508057, val loss: 2.484224796295166\n",
      "Epoch 623: train loss: 2.4493377208709717, val loss: 2.483311891555786\n",
      "Epoch 624: train loss: 2.448455572128296, val loss: 2.4823994636535645\n",
      "Epoch 625: train loss: 2.447573661804199, val loss: 2.4814870357513428\n",
      "Epoch 626: train loss: 2.4466919898986816, val loss: 2.4805753231048584\n",
      "Epoch 627: train loss: 2.445810079574585, val loss: 2.479663133621216\n",
      "Epoch 628: train loss: 2.4449288845062256, val loss: 2.4787516593933105\n",
      "Epoch 629: train loss: 2.444047689437866, val loss: 2.4778404235839844\n",
      "Epoch 630: train loss: 2.443166971206665, val loss: 2.4769294261932373\n",
      "Epoch 631: train loss: 2.442286252975464, val loss: 2.4760189056396484\n",
      "Epoch 632: train loss: 2.441406011581421, val loss: 2.4751083850860596\n",
      "Epoch 633: train loss: 2.440525770187378, val loss: 2.47419810295105\n",
      "Epoch 634: train loss: 2.439645767211914, val loss: 2.473288059234619\n",
      "Epoch 635: train loss: 2.4387660026550293, val loss: 2.4723784923553467\n",
      "Epoch 636: train loss: 2.4378867149353027, val loss: 2.471468925476074\n",
      "Epoch 637: train loss: 2.437007427215576, val loss: 2.47055983543396\n",
      "Epoch 638: train loss: 2.4361283779144287, val loss: 2.469650983810425\n",
      "Epoch 639: train loss: 2.4352495670318604, val loss: 2.4687421321868896\n",
      "Epoch 640: train loss: 2.434370994567871, val loss: 2.4678337574005127\n",
      "Epoch 641: train loss: 2.433492660522461, val loss: 2.466925859451294\n",
      "Epoch 642: train loss: 2.432614326477051, val loss: 2.466017961502075\n",
      "Epoch 643: train loss: 2.431736707687378, val loss: 2.4651100635528564\n",
      "Epoch 644: train loss: 2.430858850479126, val loss: 2.464202880859375\n",
      "Epoch 645: train loss: 2.4299814701080322, val loss: 2.4632956981658936\n",
      "Epoch 646: train loss: 2.4291043281555176, val loss: 2.462388753890991\n",
      "Epoch 647: train loss: 2.428227424621582, val loss: 2.461482286453247\n",
      "Epoch 648: train loss: 2.4273505210876465, val loss: 2.460575819015503\n",
      "Epoch 649: train loss: 2.4264743328094482, val loss: 2.459669828414917\n",
      "Epoch 650: train loss: 2.42559814453125, val loss: 2.458763837814331\n",
      "Epoch 651: train loss: 2.4247219562530518, val loss: 2.4578585624694824\n",
      "Epoch 652: train loss: 2.4238462448120117, val loss: 2.4569530487060547\n",
      "Epoch 653: train loss: 2.4229705333709717, val loss: 2.456047773361206\n",
      "Epoch 654: train loss: 2.42209529876709, val loss: 2.4551432132720947\n",
      "Epoch 655: train loss: 2.421220064163208, val loss: 2.4542386531829834\n",
      "Epoch 656: train loss: 2.4203450679779053, val loss: 2.453334093093872\n",
      "Epoch 657: train loss: 2.4194705486297607, val loss: 2.452430486679077\n",
      "Epoch 658: train loss: 2.418596029281616, val loss: 2.451526403427124\n",
      "Epoch 659: train loss: 2.41772198677063, val loss: 2.450623035430908\n",
      "Epoch 660: train loss: 2.4168477058410645, val loss: 2.4497196674346924\n",
      "Epoch 661: train loss: 2.4159741401672363, val loss: 2.4488167762756348\n",
      "Epoch 662: train loss: 2.415100574493408, val loss: 2.447913885116577\n",
      "Epoch 663: train loss: 2.414227247238159, val loss: 2.447011709213257\n",
      "Epoch 664: train loss: 2.4133541584014893, val loss: 2.4461090564727783\n",
      "Epoch 665: train loss: 2.4124813079833984, val loss: 2.445207357406616\n",
      "Epoch 666: train loss: 2.4116086959838867, val loss: 2.444305419921875\n",
      "Epoch 667: train loss: 2.410736560821533, val loss: 2.443403959274292\n",
      "Epoch 668: train loss: 2.4098641872406006, val loss: 2.442502737045288\n",
      "Epoch 669: train loss: 2.408992290496826, val loss: 2.4416017532348633\n",
      "Epoch 670: train loss: 2.4081203937530518, val loss: 2.4407012462615967\n",
      "Epoch 671: train loss: 2.4072489738464355, val loss: 2.43980073928833\n",
      "Epoch 672: train loss: 2.4063777923583984, val loss: 2.4389007091522217\n",
      "Epoch 673: train loss: 2.4055068492889404, val loss: 2.4380009174346924\n",
      "Epoch 674: train loss: 2.4046359062194824, val loss: 2.437101125717163\n",
      "Epoch 675: train loss: 2.4037656784057617, val loss: 2.436201572418213\n",
      "Epoch 676: train loss: 2.402895212173462, val loss: 2.435302495956421\n",
      "Epoch 677: train loss: 2.402024984359741, val loss: 2.434403657913208\n",
      "Epoch 678: train loss: 2.4011552333831787, val loss: 2.433504819869995\n",
      "Epoch 679: train loss: 2.400285482406616, val loss: 2.4326064586639404\n",
      "Epoch 680: train loss: 2.399416208267212, val loss: 2.4317080974578857\n",
      "Epoch 681: train loss: 2.3985471725463867, val loss: 2.4308106899261475\n",
      "Epoch 682: train loss: 2.3976781368255615, val loss: 2.429912805557251\n",
      "Epoch 683: train loss: 2.3968093395233154, val loss: 2.4290153980255127\n",
      "Epoch 684: train loss: 2.3959407806396484, val loss: 2.4281184673309326\n",
      "Epoch 685: train loss: 2.3950724601745605, val loss: 2.4272217750549316\n",
      "Epoch 686: train loss: 2.394204616546631, val loss: 2.4263248443603516\n",
      "Epoch 687: train loss: 2.393336772918701, val loss: 2.425428867340088\n",
      "Epoch 688: train loss: 2.3924694061279297, val loss: 2.424532651901245\n",
      "Epoch 689: train loss: 2.391602039337158, val loss: 2.4236366748809814\n",
      "Epoch 690: train loss: 2.3907346725463867, val loss: 2.422741413116455\n",
      "Epoch 691: train loss: 2.3898677825927734, val loss: 2.4218459129333496\n",
      "Epoch 692: train loss: 2.3890011310577393, val loss: 2.4209506511688232\n",
      "Epoch 693: train loss: 2.388134479522705, val loss: 2.420056104660034\n",
      "Epoch 694: train loss: 2.387268543243408, val loss: 2.419161319732666\n",
      "Epoch 695: train loss: 2.3864023685455322, val loss: 2.418267011642456\n",
      "Epoch 696: train loss: 2.3855366706848145, val loss: 2.417372941970825\n",
      "Epoch 697: train loss: 2.3846709728240967, val loss: 2.4164791107177734\n",
      "Epoch 698: train loss: 2.383805751800537, val loss: 2.415585517883301\n",
      "Epoch 699: train loss: 2.3829407691955566, val loss: 2.414691925048828\n",
      "Epoch 700: train loss: 2.382075786590576, val loss: 2.413799285888672\n",
      "Epoch 701: train loss: 2.381211280822754, val loss: 2.4129064083099365\n",
      "Epoch 702: train loss: 2.3803467750549316, val loss: 2.4120137691497803\n",
      "Epoch 703: train loss: 2.3794825077056885, val loss: 2.411121129989624\n",
      "Epoch 704: train loss: 2.3786187171936035, val loss: 2.410229444503784\n",
      "Epoch 705: train loss: 2.3777551651000977, val loss: 2.4093377590179443\n",
      "Epoch 706: train loss: 2.3768913745880127, val loss: 2.4084460735321045\n",
      "Epoch 707: train loss: 2.376028060913086, val loss: 2.4075546264648438\n",
      "Epoch 708: train loss: 2.3751652240753174, val loss: 2.406663656234741\n",
      "Epoch 709: train loss: 2.3743021488189697, val loss: 2.4057729244232178\n",
      "Epoch 710: train loss: 2.3734397888183594, val loss: 2.4048824310302734\n",
      "Epoch 711: train loss: 2.37257719039917, val loss: 2.403992176055908\n",
      "Epoch 712: train loss: 2.3717150688171387, val loss: 2.403101682662964\n",
      "Epoch 713: train loss: 2.3708531856536865, val loss: 2.402212381362915\n",
      "Epoch 714: train loss: 2.3699915409088135, val loss: 2.401322603225708\n",
      "Epoch 715: train loss: 2.3691301345825195, val loss: 2.400433301925659\n",
      "Epoch 716: train loss: 2.3682689666748047, val loss: 2.3995442390441895\n",
      "Epoch 717: train loss: 2.367408037185669, val loss: 2.398655652999878\n",
      "Epoch 718: train loss: 2.366547107696533, val loss: 2.3977668285369873\n",
      "Epoch 719: train loss: 2.3656864166259766, val loss: 2.396878957748413\n",
      "Epoch 720: train loss: 2.364826202392578, val loss: 2.3959908485412598\n",
      "Epoch 721: train loss: 2.3639659881591797, val loss: 2.3951029777526855\n",
      "Epoch 722: train loss: 2.3631062507629395, val loss: 2.3942153453826904\n",
      "Epoch 723: train loss: 2.362246513366699, val loss: 2.3933284282684326\n",
      "Epoch 724: train loss: 2.361387252807617, val loss: 2.3924412727355957\n",
      "Epoch 725: train loss: 2.360527992248535, val loss: 2.391554355621338\n",
      "Epoch 726: train loss: 2.3596689701080322, val loss: 2.3906681537628174\n",
      "Epoch 727: train loss: 2.3588101863861084, val loss: 2.3897817134857178\n",
      "Epoch 728: train loss: 2.3579518795013428, val loss: 2.3888957500457764\n",
      "Epoch 729: train loss: 2.357093334197998, val loss: 2.388010025024414\n",
      "Epoch 730: train loss: 2.3562350273132324, val loss: 2.3871243000030518\n",
      "Epoch 731: train loss: 2.355377674102783, val loss: 2.3862392902374268\n",
      "Epoch 732: train loss: 2.3545196056365967, val loss: 2.3853540420532227\n",
      "Epoch 733: train loss: 2.3536624908447266, val loss: 2.3844692707061768\n",
      "Epoch 734: train loss: 2.3528053760528564, val loss: 2.38358473777771\n",
      "Epoch 735: train loss: 2.3519484996795654, val loss: 2.3827004432678223\n",
      "Epoch 736: train loss: 2.3510916233062744, val loss: 2.3818166255950928\n",
      "Epoch 737: train loss: 2.3502349853515625, val loss: 2.380932569503784\n",
      "Epoch 738: train loss: 2.349378824234009, val loss: 2.380049228668213\n",
      "Epoch 739: train loss: 2.348522663116455, val loss: 2.3791661262512207\n",
      "Epoch 740: train loss: 2.3476669788360596, val loss: 2.3782830238342285\n",
      "Epoch 741: train loss: 2.346811294555664, val loss: 2.3774001598358154\n",
      "Epoch 742: train loss: 2.3459560871124268, val loss: 2.3765175342559814\n",
      "Epoch 743: train loss: 2.3451008796691895, val loss: 2.3756351470947266\n",
      "Epoch 744: train loss: 2.3442459106445312, val loss: 2.374752998352051\n",
      "Epoch 745: train loss: 2.3433914184570312, val loss: 2.373871326446533\n",
      "Epoch 746: train loss: 2.3425369262695312, val loss: 2.3729896545410156\n",
      "Epoch 747: train loss: 2.3416824340820312, val loss: 2.3721084594726562\n",
      "Epoch 748: train loss: 2.3408284187316895, val loss: 2.371227264404297\n",
      "Epoch 749: train loss: 2.3399746417999268, val loss: 2.3703465461730957\n",
      "Epoch 750: train loss: 2.339121103286743, val loss: 2.3694660663604736\n",
      "Epoch 751: train loss: 2.3382678031921387, val loss: 2.3685855865478516\n",
      "Epoch 752: train loss: 2.337414503097534, val loss: 2.367705821990967\n",
      "Epoch 753: train loss: 2.336561918258667, val loss: 2.366825819015503\n",
      "Epoch 754: train loss: 2.3357090950012207, val loss: 2.3659462928771973\n",
      "Epoch 755: train loss: 2.3348565101623535, val loss: 2.3650670051574707\n",
      "Epoch 756: train loss: 2.3340044021606445, val loss: 2.3641879558563232\n",
      "Epoch 757: train loss: 2.3331522941589355, val loss: 2.363308906555176\n",
      "Epoch 758: train loss: 2.3323004245758057, val loss: 2.3624303340911865\n",
      "Epoch 759: train loss: 2.331449031829834, val loss: 2.3615520000457764\n",
      "Epoch 760: train loss: 2.3305976390838623, val loss: 2.3606739044189453\n",
      "Epoch 761: train loss: 2.3297464847564697, val loss: 2.3597962856292725\n",
      "Epoch 762: train loss: 2.3288955688476562, val loss: 2.3589184284210205\n",
      "Epoch 763: train loss: 2.328045129776001, val loss: 2.358041286468506\n",
      "Epoch 764: train loss: 2.3271946907043457, val loss: 2.357164144515991\n",
      "Epoch 765: train loss: 2.3263442516326904, val loss: 2.3562874794006348\n",
      "Epoch 766: train loss: 2.3254942893981934, val loss: 2.355410575866699\n",
      "Epoch 767: train loss: 2.3246445655822754, val loss: 2.354534387588501\n",
      "Epoch 768: train loss: 2.3237948417663574, val loss: 2.3536581993103027\n",
      "Epoch 769: train loss: 2.3229455947875977, val loss: 2.3527822494506836\n",
      "Epoch 770: train loss: 2.322096586227417, val loss: 2.3519067764282227\n",
      "Epoch 771: train loss: 2.3212478160858154, val loss: 2.3510310649871826\n",
      "Epoch 772: train loss: 2.320399284362793, val loss: 2.35015606880188\n",
      "Epoch 773: train loss: 2.3195507526397705, val loss: 2.349281072616577\n",
      "Epoch 774: train loss: 2.318702459335327, val loss: 2.3484063148498535\n",
      "Epoch 775: train loss: 2.317854404449463, val loss: 2.347532272338867\n",
      "Epoch 776: train loss: 2.317006826400757, val loss: 2.3466579914093018\n",
      "Epoch 777: train loss: 2.3161590099334717, val loss: 2.3457839488983154\n",
      "Epoch 778: train loss: 2.3153116703033447, val loss: 2.3449103832244873\n",
      "Epoch 779: train loss: 2.314464807510376, val loss: 2.344036817550659\n",
      "Epoch 780: train loss: 2.3136179447174072, val loss: 2.34316349029541\n",
      "Epoch 781: train loss: 2.3127710819244385, val loss: 2.3422906398773193\n",
      "Epoch 782: train loss: 2.311924695968628, val loss: 2.3414180278778076\n",
      "Epoch 783: train loss: 2.3110785484313965, val loss: 2.340545415878296\n",
      "Epoch 784: train loss: 2.310232639312744, val loss: 2.3396732807159424\n",
      "Epoch 785: train loss: 2.309386968612671, val loss: 2.338801145553589\n",
      "Epoch 786: train loss: 2.3085412979125977, val loss: 2.3379294872283936\n",
      "Epoch 787: train loss: 2.3076961040496826, val loss: 2.3370580673217773\n",
      "Epoch 788: train loss: 2.3068509101867676, val loss: 2.3361871242523193\n",
      "Epoch 789: train loss: 2.3060059547424316, val loss: 2.335315704345703\n",
      "Epoch 790: train loss: 2.305161476135254, val loss: 2.334444761276245\n",
      "Epoch 791: train loss: 2.304316997528076, val loss: 2.3335747718811035\n",
      "Epoch 792: train loss: 2.3034725189208984, val loss: 2.3327043056488037\n",
      "Epoch 793: train loss: 2.302628755569458, val loss: 2.331834316253662\n",
      "Epoch 794: train loss: 2.3017849922180176, val loss: 2.3309643268585205\n",
      "Epoch 795: train loss: 2.3009414672851562, val loss: 2.330095052719116\n",
      "Epoch 796: train loss: 2.300098180770874, val loss: 2.329225540161133\n",
      "Epoch 797: train loss: 2.299255132675171, val loss: 2.3283565044403076\n",
      "Epoch 798: train loss: 2.298412322998047, val loss: 2.3274879455566406\n",
      "Epoch 799: train loss: 2.297569513320923, val loss: 2.3266191482543945\n",
      "Epoch 800: train loss: 2.296727180480957, val loss: 2.3257508277893066\n",
      "Epoch 801: train loss: 2.295884847640991, val loss: 2.324882745742798\n",
      "Epoch 802: train loss: 2.2950429916381836, val loss: 2.324014902114868\n",
      "Epoch 803: train loss: 2.294201135635376, val loss: 2.3231472969055176\n",
      "Epoch 804: train loss: 2.2933597564697266, val loss: 2.322279691696167\n",
      "Epoch 805: train loss: 2.2925186157226562, val loss: 2.3214128017425537\n",
      "Epoch 806: train loss: 2.291677236557007, val loss: 2.3205459117889404\n",
      "Epoch 807: train loss: 2.2908365726470947, val loss: 2.3196792602539062\n",
      "Epoch 808: train loss: 2.2899956703186035, val loss: 2.318812608718872\n",
      "Epoch 809: train loss: 2.2891554832458496, val loss: 2.317946672439575\n",
      "Epoch 810: train loss: 2.2883152961730957, val loss: 2.3170807361602783\n",
      "Epoch 811: train loss: 2.287475347518921, val loss: 2.3162152767181396\n",
      "Epoch 812: train loss: 2.286635398864746, val loss: 2.315349817276001\n",
      "Epoch 813: train loss: 2.2857959270477295, val loss: 2.3144843578338623\n",
      "Epoch 814: train loss: 2.284956455230713, val loss: 2.313619375228882\n",
      "Epoch 815: train loss: 2.2841174602508545, val loss: 2.3127548694610596\n",
      "Epoch 816: train loss: 2.283278703689575, val loss: 2.3118903636932373\n",
      "Epoch 817: train loss: 2.282439947128296, val loss: 2.311026096343994\n",
      "Epoch 818: train loss: 2.2816014289855957, val loss: 2.31016206741333\n",
      "Epoch 819: train loss: 2.2807633876800537, val loss: 2.309298276901245\n",
      "Epoch 820: train loss: 2.2799253463745117, val loss: 2.3084347248077393\n",
      "Epoch 821: train loss: 2.2790873050689697, val loss: 2.3075718879699707\n",
      "Epoch 822: train loss: 2.278249979019165, val loss: 2.306708574295044\n",
      "Epoch 823: train loss: 2.2774126529693604, val loss: 2.3058457374572754\n",
      "Epoch 824: train loss: 2.2765755653381348, val loss: 2.304983139038086\n",
      "Epoch 825: train loss: 2.275738477706909, val loss: 2.3041207790374756\n",
      "Epoch 826: train loss: 2.274902105331421, val loss: 2.3032588958740234\n",
      "Epoch 827: train loss: 2.2740654945373535, val loss: 2.302396774291992\n",
      "Epoch 828: train loss: 2.2732293605804443, val loss: 2.301535129547119\n",
      "Epoch 829: train loss: 2.272393226623535, val loss: 2.3006742000579834\n",
      "Epoch 830: train loss: 2.271557569503784, val loss: 2.2998127937316895\n",
      "Epoch 831: train loss: 2.270721912384033, val loss: 2.2989518642425537\n",
      "Epoch 832: train loss: 2.2698867321014404, val loss: 2.2980916500091553\n",
      "Epoch 833: train loss: 2.2690515518188477, val loss: 2.2972309589385986\n",
      "Epoch 834: train loss: 2.268216609954834, val loss: 2.2963707447052\n",
      "Epoch 835: train loss: 2.2673819065093994, val loss: 2.29551100730896\n",
      "Epoch 836: train loss: 2.266547441482544, val loss: 2.2946512699127197\n",
      "Epoch 837: train loss: 2.2657132148742676, val loss: 2.2937915325164795\n",
      "Epoch 838: train loss: 2.264878988265991, val loss: 2.2929325103759766\n",
      "Epoch 839: train loss: 2.264045238494873, val loss: 2.2920734882354736\n",
      "Epoch 840: train loss: 2.263211727142334, val loss: 2.291214942932129\n",
      "Epoch 841: train loss: 2.262378215789795, val loss: 2.290356159210205\n",
      "Epoch 842: train loss: 2.261545181274414, val loss: 2.2894980907440186\n",
      "Epoch 843: train loss: 2.2607123851776123, val loss: 2.288640260696411\n",
      "Epoch 844: train loss: 2.2598795890808105, val loss: 2.2877819538116455\n",
      "Epoch 845: train loss: 2.259047031402588, val loss: 2.2869248390197754\n",
      "Epoch 846: train loss: 2.2582149505615234, val loss: 2.2860677242279053\n",
      "Epoch 847: train loss: 2.257382869720459, val loss: 2.285210371017456\n",
      "Epoch 848: train loss: 2.2565507888793945, val loss: 2.284353256225586\n",
      "Epoch 849: train loss: 2.2557191848754883, val loss: 2.283496618270874\n",
      "Epoch 850: train loss: 2.2548880577087402, val loss: 2.2826404571533203\n",
      "Epoch 851: train loss: 2.254056930541992, val loss: 2.2817842960357666\n",
      "Epoch 852: train loss: 2.253225803375244, val loss: 2.280928373336792\n",
      "Epoch 853: train loss: 2.252394914627075, val loss: 2.2800729274749756\n",
      "Epoch 854: train loss: 2.2515645027160645, val loss: 2.27921724319458\n",
      "Epoch 855: train loss: 2.2507340908050537, val loss: 2.278362274169922\n",
      "Epoch 856: train loss: 2.249904155731201, val loss: 2.2775075435638428\n",
      "Epoch 857: train loss: 2.2490742206573486, val loss: 2.2766525745391846\n",
      "Epoch 858: train loss: 2.248244524002075, val loss: 2.2757980823516846\n",
      "Epoch 859: train loss: 2.24741530418396, val loss: 2.2749438285827637\n",
      "Epoch 860: train loss: 2.2465860843658447, val loss: 2.274089813232422\n",
      "Epoch 861: train loss: 2.2457571029663086, val loss: 2.273236036300659\n",
      "Epoch 862: train loss: 2.2449281215667725, val loss: 2.2723824977874756\n",
      "Epoch 863: train loss: 2.2440996170043945, val loss: 2.271528959274292\n",
      "Epoch 864: train loss: 2.2432713508605957, val loss: 2.2706761360168457\n",
      "Epoch 865: train loss: 2.242443323135376, val loss: 2.2698233127593994\n",
      "Epoch 866: train loss: 2.2416152954101562, val loss: 2.2689709663391113\n",
      "Epoch 867: train loss: 2.2407877445220947, val loss: 2.268118381500244\n",
      "Epoch 868: train loss: 2.239960193634033, val loss: 2.267266035079956\n",
      "Epoch 869: train loss: 2.239132881164551, val loss: 2.2664144039154053\n",
      "Epoch 870: train loss: 2.2383060455322266, val loss: 2.2655627727508545\n",
      "Epoch 871: train loss: 2.2374789714813232, val loss: 2.264711380004883\n",
      "Epoch 872: train loss: 2.2366526126861572, val loss: 2.2638604640960693\n",
      "Epoch 873: train loss: 2.235826253890991, val loss: 2.2630093097686768\n",
      "Epoch 874: train loss: 2.2350001335144043, val loss: 2.2621586322784424\n",
      "Epoch 875: train loss: 2.2341742515563965, val loss: 2.261307954788208\n",
      "Epoch 876: train loss: 2.2333483695983887, val loss: 2.260457754135132\n",
      "Epoch 877: train loss: 2.232522964477539, val loss: 2.2596077919006348\n",
      "Epoch 878: train loss: 2.2316975593566895, val loss: 2.258758068084717\n",
      "Epoch 879: train loss: 2.230872631072998, val loss: 2.257908582687378\n",
      "Epoch 880: train loss: 2.2300479412078857, val loss: 2.257059335708618\n",
      "Epoch 881: train loss: 2.2292232513427734, val loss: 2.2562100887298584\n",
      "Epoch 882: train loss: 2.2283987998962402, val loss: 2.255361318588257\n",
      "Epoch 883: train loss: 2.2275748252868652, val loss: 2.2545127868652344\n",
      "Epoch 884: train loss: 2.2267508506774902, val loss: 2.253664493560791\n",
      "Epoch 885: train loss: 2.2259271144866943, val loss: 2.2528164386749268\n",
      "Epoch 886: train loss: 2.2251033782958984, val loss: 2.2519686222076416\n",
      "Epoch 887: train loss: 2.2242801189422607, val loss: 2.2511208057403564\n",
      "Epoch 888: train loss: 2.223457098007202, val loss: 2.2502734661102295\n",
      "Epoch 889: train loss: 2.2226343154907227, val loss: 2.2494258880615234\n",
      "Epoch 890: train loss: 2.221811532974243, val loss: 2.2485790252685547\n",
      "Epoch 891: train loss: 2.2209889888763428, val loss: 2.247732639312744\n",
      "Epoch 892: train loss: 2.2201666831970215, val loss: 2.2468860149383545\n",
      "Epoch 893: train loss: 2.2193448543548584, val loss: 2.246039867401123\n",
      "Epoch 894: train loss: 2.2185230255126953, val loss: 2.2451937198638916\n",
      "Epoch 895: train loss: 2.2177014350891113, val loss: 2.2443478107452393\n",
      "Epoch 896: train loss: 2.2168803215026855, val loss: 2.243502378463745\n",
      "Epoch 897: train loss: 2.2160589694976807, val loss: 2.24265718460083\n",
      "Epoch 898: train loss: 2.215238094329834, val loss: 2.241811752319336\n",
      "Epoch 899: train loss: 2.2144174575805664, val loss: 2.240967273712158\n",
      "Epoch 900: train loss: 2.213597059249878, val loss: 2.2401225566864014\n",
      "Epoch 901: train loss: 2.2127766609191895, val loss: 2.2392780780792236\n",
      "Epoch 902: train loss: 2.211956739425659, val loss: 2.238433837890625\n",
      "Epoch 903: train loss: 2.211136817932129, val loss: 2.2375900745391846\n",
      "Epoch 904: train loss: 2.210317373275757, val loss: 2.2367465496063232\n",
      "Epoch 905: train loss: 2.2094976902008057, val loss: 2.235903024673462\n",
      "Epoch 906: train loss: 2.208678722381592, val loss: 2.2350597381591797\n",
      "Epoch 907: train loss: 2.207859754562378, val loss: 2.2342166900634766\n",
      "Epoch 908: train loss: 2.207041025161743, val loss: 2.2333738803863525\n",
      "Epoch 909: train loss: 2.2062225341796875, val loss: 2.2325313091278076\n",
      "Epoch 910: train loss: 2.205404043197632, val loss: 2.231689214706421\n",
      "Epoch 911: train loss: 2.2045860290527344, val loss: 2.230847120285034\n",
      "Epoch 912: train loss: 2.203768014907837, val loss: 2.2300052642822266\n",
      "Epoch 913: train loss: 2.2029504776000977, val loss: 2.229163408279419\n",
      "Epoch 914: train loss: 2.2021331787109375, val loss: 2.2283217906951904\n",
      "Epoch 915: train loss: 2.2013158798217773, val loss: 2.2274811267852783\n",
      "Epoch 916: train loss: 2.200498580932617, val loss: 2.226640224456787\n",
      "Epoch 917: train loss: 2.1996819972991943, val loss: 2.225799560546875\n",
      "Epoch 918: train loss: 2.1988654136657715, val loss: 2.224958896636963\n",
      "Epoch 919: train loss: 2.1980490684509277, val loss: 2.224118709564209\n",
      "Epoch 920: train loss: 2.197232961654663, val loss: 2.223278760910034\n",
      "Epoch 921: train loss: 2.1964168548583984, val loss: 2.2224390506744385\n",
      "Epoch 922: train loss: 2.195601463317871, val loss: 2.221599578857422\n",
      "Epoch 923: train loss: 2.1947858333587646, val loss: 2.2207601070404053\n",
      "Epoch 924: train loss: 2.1939704418182373, val loss: 2.2199208736419678\n",
      "Epoch 925: train loss: 2.193155288696289, val loss: 2.2190823554992676\n",
      "Epoch 926: train loss: 2.19234037399292, val loss: 2.218243360519409\n",
      "Epoch 927: train loss: 2.191525936126709, val loss: 2.217405319213867\n",
      "Epoch 928: train loss: 2.190711498260498, val loss: 2.216567277908325\n",
      "Epoch 929: train loss: 2.189897298812866, val loss: 2.215729236602783\n",
      "Epoch 930: train loss: 2.1890833377838135, val loss: 2.2148914337158203\n",
      "Epoch 931: train loss: 2.1882693767547607, val loss: 2.2140538692474365\n",
      "Epoch 932: train loss: 2.187455892562866, val loss: 2.213216781616211\n",
      "Epoch 933: train loss: 2.1866424083709717, val loss: 2.2123794555664062\n",
      "Epoch 934: train loss: 2.1858294010162354, val loss: 2.211542844772339\n",
      "Epoch 935: train loss: 2.185016632080078, val loss: 2.2107064723968506\n",
      "Epoch 936: train loss: 2.184203863143921, val loss: 2.2098701000213623\n",
      "Epoch 937: train loss: 2.1833913326263428, val loss: 2.209033966064453\n",
      "Epoch 938: train loss: 2.1825790405273438, val loss: 2.208198308944702\n",
      "Epoch 939: train loss: 2.181766986846924, val loss: 2.207362413406372\n",
      "Epoch 940: train loss: 2.180955410003662, val loss: 2.2065272331237793\n",
      "Epoch 941: train loss: 2.1801435947418213, val loss: 2.2056918144226074\n",
      "Epoch 942: train loss: 2.1793320178985596, val loss: 2.204857349395752\n",
      "Epoch 943: train loss: 2.178520917892456, val loss: 2.2040226459503174\n",
      "Epoch 944: train loss: 2.1777100563049316, val loss: 2.203187942504883\n",
      "Epoch 945: train loss: 2.1768991947174072, val loss: 2.2023537158966064\n",
      "Epoch 946: train loss: 2.176088809967041, val loss: 2.201519727706909\n",
      "Epoch 947: train loss: 2.175278425216675, val loss: 2.200685977935791\n",
      "Epoch 948: train loss: 2.1744682788848877, val loss: 2.199852466583252\n",
      "Epoch 949: train loss: 2.1736583709716797, val loss: 2.199018955230713\n",
      "Epoch 950: train loss: 2.172848701477051, val loss: 2.198185682296753\n",
      "Epoch 951: train loss: 2.172039270401001, val loss: 2.1973531246185303\n",
      "Epoch 952: train loss: 2.171229839324951, val loss: 2.1965203285217285\n",
      "Epoch 953: train loss: 2.1704208850860596, val loss: 2.195688009262085\n",
      "Epoch 954: train loss: 2.169612169265747, val loss: 2.1948556900024414\n",
      "Epoch 955: train loss: 2.1688034534454346, val loss: 2.194023847579956\n",
      "Epoch 956: train loss: 2.1679952144622803, val loss: 2.19319224357605\n",
      "Epoch 957: train loss: 2.167186975479126, val loss: 2.1923606395721436\n",
      "Epoch 958: train loss: 2.166378974914551, val loss: 2.1915290355682373\n",
      "Epoch 959: train loss: 2.1655712127685547, val loss: 2.1906983852386475\n",
      "Epoch 960: train loss: 2.1647636890411377, val loss: 2.1898674964904785\n",
      "Epoch 961: train loss: 2.1639564037323, val loss: 2.1890366077423096\n",
      "Epoch 962: train loss: 2.163149118423462, val loss: 2.188206434249878\n",
      "Epoch 963: train loss: 2.1623423099517822, val loss: 2.1873764991760254\n",
      "Epoch 964: train loss: 2.1615357398986816, val loss: 2.186546564102173\n",
      "Epoch 965: train loss: 2.16072940826416, val loss: 2.1857168674468994\n",
      "Epoch 966: train loss: 2.1599230766296387, val loss: 2.184887409210205\n",
      "Epoch 967: train loss: 2.159116744995117, val loss: 2.1840579509735107\n",
      "Epoch 968: train loss: 2.158310890197754, val loss: 2.1832292079925537\n",
      "Epoch 969: train loss: 2.1575052738189697, val loss: 2.1824002265930176\n",
      "Epoch 970: train loss: 2.1567001342773438, val loss: 2.1815717220306396\n",
      "Epoch 971: train loss: 2.1558947563171387, val loss: 2.180743455886841\n",
      "Epoch 972: train loss: 2.155089855194092, val loss: 2.179915189743042\n",
      "Epoch 973: train loss: 2.154284954071045, val loss: 2.1790874004364014\n",
      "Epoch 974: train loss: 2.1534805297851562, val loss: 2.17825984954834\n",
      "Epoch 975: train loss: 2.1526761054992676, val loss: 2.1774322986602783\n",
      "Epoch 976: train loss: 2.151872158050537, val loss: 2.176605224609375\n",
      "Epoch 977: train loss: 2.1510682106018066, val loss: 2.1757781505584717\n",
      "Epoch 978: train loss: 2.1502645015716553, val loss: 2.1749515533447266\n",
      "Epoch 979: train loss: 2.149460792541504, val loss: 2.1741249561309814\n",
      "Epoch 980: train loss: 2.1486575603485107, val loss: 2.1732985973358154\n",
      "Epoch 981: train loss: 2.1478545665740967, val loss: 2.1724727153778076\n",
      "Epoch 982: train loss: 2.1470518112182617, val loss: 2.1716468334198\n",
      "Epoch 983: train loss: 2.1462490558624268, val loss: 2.170820951461792\n",
      "Epoch 984: train loss: 2.14544677734375, val loss: 2.1699957847595215\n",
      "Epoch 985: train loss: 2.1446444988250732, val loss: 2.169170618057251\n",
      "Epoch 986: train loss: 2.1438424587249756, val loss: 2.1683456897735596\n",
      "Epoch 987: train loss: 2.143040657043457, val loss: 2.1675209999084473\n",
      "Epoch 988: train loss: 2.1422390937805176, val loss: 2.166696548461914\n",
      "Epoch 989: train loss: 2.1414377689361572, val loss: 2.16587233543396\n",
      "Epoch 990: train loss: 2.140636682510376, val loss: 2.165048360824585\n",
      "Epoch 991: train loss: 2.1398355960845947, val loss: 2.16422438621521\n",
      "Epoch 992: train loss: 2.1390349864959717, val loss: 2.163400650024414\n",
      "Epoch 993: train loss: 2.1382346153259277, val loss: 2.1625773906707764\n",
      "Epoch 994: train loss: 2.137434244155884, val loss: 2.1617543697357178\n",
      "Epoch 995: train loss: 2.136634111404419, val loss: 2.160931348800659\n",
      "Epoch 996: train loss: 2.1358344554901123, val loss: 2.1601085662841797\n",
      "Epoch 997: train loss: 2.1350347995758057, val loss: 2.1592862606048584\n",
      "Epoch 998: train loss: 2.134235143661499, val loss: 2.158464193344116\n",
      "Epoch 999: train loss: 2.1334359645843506, val loss: 2.157642126083374\n",
      "Epoch 1000: train loss: 2.1326370239257812, val loss: 2.156820297241211\n",
      "Epoch 1001: train loss: 2.131838083267212, val loss: 2.155998706817627\n",
      "Epoch 1002: train loss: 2.1310393810272217, val loss: 2.155177354812622\n",
      "Epoch 1003: train loss: 2.1302411556243896, val loss: 2.1543562412261963\n",
      "Epoch 1004: train loss: 2.1294429302215576, val loss: 2.1535353660583496\n",
      "Epoch 1005: train loss: 2.1286449432373047, val loss: 2.152714490890503\n",
      "Epoch 1006: train loss: 2.12784743309021, val loss: 2.1518940925598145\n",
      "Epoch 1007: train loss: 2.1270499229431152, val loss: 2.151074171066284\n",
      "Epoch 1008: train loss: 2.1262521743774414, val loss: 2.150254011154175\n",
      "Epoch 1009: train loss: 2.125455379486084, val loss: 2.1494343280792236\n",
      "Epoch 1010: train loss: 2.1246583461761475, val loss: 2.1486146450042725\n",
      "Epoch 1011: train loss: 2.123861789703369, val loss: 2.1477954387664795\n",
      "Epoch 1012: train loss: 2.123065233230591, val loss: 2.1469762325286865\n",
      "Epoch 1013: train loss: 2.1222691535949707, val loss: 2.1461575031280518\n",
      "Epoch 1014: train loss: 2.1214728355407715, val loss: 2.145338773727417\n",
      "Epoch 1015: train loss: 2.1206769943237305, val loss: 2.1445202827453613\n",
      "Epoch 1016: train loss: 2.1198816299438477, val loss: 2.143702268600464\n",
      "Epoch 1017: train loss: 2.1190860271453857, val loss: 2.1428840160369873\n",
      "Epoch 1018: train loss: 2.118290662765503, val loss: 2.142066717147827\n",
      "Epoch 1019: train loss: 2.1174960136413574, val loss: 2.141248941421509\n",
      "Epoch 1020: train loss: 2.116701126098633, val loss: 2.1404316425323486\n",
      "Epoch 1021: train loss: 2.1159067153930664, val loss: 2.1396145820617676\n",
      "Epoch 1022: train loss: 2.115112066268921, val loss: 2.1387975215911865\n",
      "Epoch 1023: train loss: 2.1143181324005127, val loss: 2.1379806995391846\n",
      "Epoch 1024: train loss: 2.1135241985321045, val loss: 2.13716459274292\n",
      "Epoch 1025: train loss: 2.1127302646636963, val loss: 2.1363484859466553\n",
      "Epoch 1026: train loss: 2.1119368076324463, val loss: 2.1355326175689697\n",
      "Epoch 1027: train loss: 2.1111438274383545, val loss: 2.134716749191284\n",
      "Epoch 1028: train loss: 2.1103506088256836, val loss: 2.133901357650757\n",
      "Epoch 1029: train loss: 2.109557867050171, val loss: 2.1330859661102295\n",
      "Epoch 1030: train loss: 2.108765125274658, val loss: 2.1322708129882812\n",
      "Epoch 1031: train loss: 2.1079726219177246, val loss: 2.131456136703491\n",
      "Epoch 1032: train loss: 2.10718035697937, val loss: 2.130641222000122\n",
      "Epoch 1033: train loss: 2.1063883304595947, val loss: 2.1298270225524902\n",
      "Epoch 1034: train loss: 2.1055965423583984, val loss: 2.1290128231048584\n",
      "Epoch 1035: train loss: 2.1048049926757812, val loss: 2.1281986236572266\n",
      "Epoch 1036: train loss: 2.104013442993164, val loss: 2.127384901046753\n",
      "Epoch 1037: train loss: 2.103222131729126, val loss: 2.1265714168548584\n",
      "Epoch 1038: train loss: 2.102431297302246, val loss: 2.125757932662964\n",
      "Epoch 1039: train loss: 2.101640462875366, val loss: 2.1249449253082275\n",
      "Epoch 1040: train loss: 2.1008498668670654, val loss: 2.1241321563720703\n",
      "Epoch 1041: train loss: 2.1000595092773438, val loss: 2.123319625854492\n",
      "Epoch 1042: train loss: 2.099269390106201, val loss: 2.122507333755493\n",
      "Epoch 1043: train loss: 2.0984795093536377, val loss: 2.121694803237915\n",
      "Epoch 1044: train loss: 2.0976898670196533, val loss: 2.1208832263946533\n",
      "Epoch 1045: train loss: 2.096900224685669, val loss: 2.1200714111328125\n",
      "Epoch 1046: train loss: 2.0961108207702637, val loss: 2.1192595958709717\n",
      "Epoch 1047: train loss: 2.0953218936920166, val loss: 2.118448495864868\n",
      "Epoch 1048: train loss: 2.0945329666137695, val loss: 2.1176373958587646\n",
      "Epoch 1049: train loss: 2.0937442779541016, val loss: 2.1168267726898193\n",
      "Epoch 1050: train loss: 2.0929558277130127, val loss: 2.116016149520874\n",
      "Epoch 1051: train loss: 2.092167615890503, val loss: 2.1152055263519287\n",
      "Epoch 1052: train loss: 2.0913796424865723, val loss: 2.1143951416015625\n",
      "Epoch 1053: train loss: 2.0905919075012207, val loss: 2.1135852336883545\n",
      "Epoch 1054: train loss: 2.089804172515869, val loss: 2.1127755641937256\n",
      "Epoch 1055: train loss: 2.0890166759490967, val loss: 2.1119656562805176\n",
      "Epoch 1056: train loss: 2.0882294178009033, val loss: 2.111156702041626\n",
      "Epoch 1057: train loss: 2.087442636489868, val loss: 2.1103475093841553\n",
      "Epoch 1058: train loss: 2.086655616760254, val loss: 2.1095387935638428\n",
      "Epoch 1059: train loss: 2.085869073867798, val loss: 2.1087300777435303\n",
      "Epoch 1060: train loss: 2.0850830078125, val loss: 2.107921600341797\n",
      "Epoch 1061: train loss: 2.084296703338623, val loss: 2.1071133613586426\n",
      "Epoch 1062: train loss: 2.083510637283325, val loss: 2.1063055992126465\n",
      "Epoch 1063: train loss: 2.0827250480651855, val loss: 2.1054975986480713\n",
      "Epoch 1064: train loss: 2.081939458847046, val loss: 2.1046903133392334\n",
      "Epoch 1065: train loss: 2.0811543464660645, val loss: 2.1038827896118164\n",
      "Epoch 1066: train loss: 2.080368995666504, val loss: 2.1030755043029785\n",
      "Epoch 1067: train loss: 2.0795841217041016, val loss: 2.102268695831299\n",
      "Epoch 1068: train loss: 2.0787994861602783, val loss: 2.1014621257781982\n",
      "Epoch 1069: train loss: 2.078015089035034, val loss: 2.1006557941436768\n",
      "Epoch 1070: train loss: 2.077230930328369, val loss: 2.0998494625091553\n",
      "Epoch 1071: train loss: 2.076446771621704, val loss: 2.099043369293213\n",
      "Epoch 1072: train loss: 2.075662851333618, val loss: 2.0982377529144287\n",
      "Epoch 1073: train loss: 2.0748794078826904, val loss: 2.0974318981170654\n",
      "Epoch 1074: train loss: 2.0740957260131836, val loss: 2.0966267585754395\n",
      "Epoch 1075: train loss: 2.073312520980835, val loss: 2.0958213806152344\n",
      "Epoch 1076: train loss: 2.0725295543670654, val loss: 2.0950164794921875\n",
      "Epoch 1077: train loss: 2.071746587753296, val loss: 2.0942118167877197\n",
      "Epoch 1078: train loss: 2.0709640979766846, val loss: 2.093407392501831\n",
      "Epoch 1079: train loss: 2.0701818466186523, val loss: 2.0926034450531006\n",
      "Epoch 1080: train loss: 2.069399833679199, val loss: 2.091799259185791\n",
      "Epoch 1081: train loss: 2.068617820739746, val loss: 2.0909955501556396\n",
      "Epoch 1082: train loss: 2.067835807800293, val loss: 2.0901918411254883\n",
      "Epoch 1083: train loss: 2.067054510116577, val loss: 2.089388608932495\n",
      "Epoch 1084: train loss: 2.0662729740142822, val loss: 2.088585376739502\n",
      "Epoch 1085: train loss: 2.0654919147491455, val loss: 2.087782382965088\n",
      "Epoch 1086: train loss: 2.064710855484009, val loss: 2.086979627609253\n",
      "Epoch 1087: train loss: 2.0639302730560303, val loss: 2.086177110671997\n",
      "Epoch 1088: train loss: 2.0631496906280518, val loss: 2.0853748321533203\n",
      "Epoch 1089: train loss: 2.0623695850372314, val loss: 2.0845727920532227\n",
      "Epoch 1090: train loss: 2.0615897178649902, val loss: 2.083770990371704\n",
      "Epoch 1091: train loss: 2.06080961227417, val loss: 2.0829694271087646\n",
      "Epoch 1092: train loss: 2.0600297451019287, val loss: 2.082167863845825\n",
      "Epoch 1093: train loss: 2.0592503547668457, val loss: 2.081366777420044\n",
      "Epoch 1094: train loss: 2.0584709644317627, val loss: 2.080565929412842\n",
      "Epoch 1095: train loss: 2.057692289352417, val loss: 2.0797650814056396\n",
      "Epoch 1096: train loss: 2.056913375854492, val loss: 2.0789642333984375\n",
      "Epoch 1097: train loss: 2.0561344623565674, val loss: 2.0781641006469727\n",
      "Epoch 1098: train loss: 2.05535626411438, val loss: 2.077363967895508\n",
      "Epoch 1099: train loss: 2.0545780658721924, val loss: 2.076564311981201\n",
      "Epoch 1100: train loss: 2.053799867630005, val loss: 2.0757644176483154\n",
      "Epoch 1101: train loss: 2.0530221462249756, val loss: 2.074965238571167\n",
      "Epoch 1102: train loss: 2.0522449016571045, val loss: 2.0741658210754395\n",
      "Epoch 1103: train loss: 2.051467180252075, val loss: 2.07336688041687\n",
      "Epoch 1104: train loss: 2.050690174102783, val loss: 2.07256817817688\n",
      "Epoch 1105: train loss: 2.049912929534912, val loss: 2.0717694759368896\n",
      "Epoch 1106: train loss: 2.0491364002227783, val loss: 2.0709712505340576\n",
      "Epoch 1107: train loss: 2.0483598709106445, val loss: 2.0701732635498047\n",
      "Epoch 1108: train loss: 2.0475833415985107, val loss: 2.0693752765655518\n",
      "Epoch 1109: train loss: 2.046807289123535, val loss: 2.068577527999878\n",
      "Epoch 1110: train loss: 2.0460314750671387, val loss: 2.067780017852783\n",
      "Epoch 1111: train loss: 2.045255661010742, val loss: 2.0669829845428467\n",
      "Epoch 1112: train loss: 2.044480085372925, val loss: 2.066185712814331\n",
      "Epoch 1113: train loss: 2.0437047481536865, val loss: 2.065389394760132\n",
      "Epoch 1114: train loss: 2.0429296493530273, val loss: 2.0645923614501953\n",
      "Epoch 1115: train loss: 2.0421547889709473, val loss: 2.063795804977417\n",
      "Epoch 1116: train loss: 2.041379928588867, val loss: 2.062999725341797\n",
      "Epoch 1117: train loss: 2.0406057834625244, val loss: 2.062204122543335\n",
      "Epoch 1118: train loss: 2.0398311614990234, val loss: 2.061408042907715\n",
      "Epoch 1119: train loss: 2.0390572547912598, val loss: 2.060612440109253\n",
      "Epoch 1120: train loss: 2.038283348083496, val loss: 2.05981707572937\n",
      "Epoch 1121: train loss: 2.0375096797943115, val loss: 2.0590221881866455\n",
      "Epoch 1122: train loss: 2.036736249923706, val loss: 2.058227300643921\n",
      "Epoch 1123: train loss: 2.0359630584716797, val loss: 2.0574326515197754\n",
      "Epoch 1124: train loss: 2.0351898670196533, val loss: 2.05663800239563\n",
      "Epoch 1125: train loss: 2.034417152404785, val loss: 2.0558438301086426\n",
      "Epoch 1126: train loss: 2.033644437789917, val loss: 2.0550498962402344\n",
      "Epoch 1127: train loss: 2.032872200012207, val loss: 2.0542562007904053\n",
      "Epoch 1128: train loss: 2.032099962234497, val loss: 2.0534627437591553\n",
      "Epoch 1129: train loss: 2.031327962875366, val loss: 2.0526692867279053\n",
      "Epoch 1130: train loss: 2.0305562019348145, val loss: 2.0518760681152344\n",
      "Epoch 1131: train loss: 2.029784679412842, val loss: 2.0510830879211426\n",
      "Epoch 1132: train loss: 2.029013156890869, val loss: 2.050290584564209\n",
      "Epoch 1133: train loss: 2.0282421112060547, val loss: 2.0494980812072754\n",
      "Epoch 1134: train loss: 2.0274710655212402, val loss: 2.048705816268921\n",
      "Epoch 1135: train loss: 2.026700496673584, val loss: 2.0479137897491455\n",
      "Epoch 1136: train loss: 2.0259299278259277, val loss: 2.04712176322937\n",
      "Epoch 1137: train loss: 2.0251593589782715, val loss: 2.046330213546753\n",
      "Epoch 1138: train loss: 2.0243892669677734, val loss: 2.0455386638641357\n",
      "Epoch 1139: train loss: 2.0236194133758545, val loss: 2.0447475910186768\n",
      "Epoch 1140: train loss: 2.0228497982025146, val loss: 2.0439565181732178\n",
      "Epoch 1141: train loss: 2.022080183029175, val loss: 2.043165922164917\n",
      "Epoch 1142: train loss: 2.021311044692993, val loss: 2.042375326156616\n",
      "Epoch 1143: train loss: 2.0205416679382324, val loss: 2.0415849685668945\n",
      "Epoch 1144: train loss: 2.019773006439209, val loss: 2.040795087814331\n",
      "Epoch 1145: train loss: 2.0190041065216064, val loss: 2.0400052070617676\n",
      "Epoch 1146: train loss: 2.018235921859741, val loss: 2.039215564727783\n",
      "Epoch 1147: train loss: 2.017467498779297, val loss: 2.038425922393799\n",
      "Epoch 1148: train loss: 2.0166995525360107, val loss: 2.0376365184783936\n",
      "Epoch 1149: train loss: 2.0159313678741455, val loss: 2.0368475914001465\n",
      "Epoch 1150: train loss: 2.0151636600494385, val loss: 2.0360589027404785\n",
      "Epoch 1151: train loss: 2.0143964290618896, val loss: 2.0352699756622314\n",
      "Epoch 1152: train loss: 2.0136289596557617, val loss: 2.0344817638397217\n",
      "Epoch 1153: train loss: 2.012861967086792, val loss: 2.033693790435791\n",
      "Epoch 1154: train loss: 2.0120949745178223, val loss: 2.0329058170318604\n",
      "Epoch 1155: train loss: 2.0113284587860107, val loss: 2.0321178436279297\n",
      "Epoch 1156: train loss: 2.010561943054199, val loss: 2.0313305854797363\n",
      "Epoch 1157: train loss: 2.009795904159546, val loss: 2.030543327331543\n",
      "Epoch 1158: train loss: 2.0090296268463135, val loss: 2.0297560691833496\n",
      "Epoch 1159: train loss: 2.0082640647888184, val loss: 2.0289690494537354\n",
      "Epoch 1160: train loss: 2.0074985027313232, val loss: 2.0281822681427\n",
      "Epoch 1161: train loss: 2.006732940673828, val loss: 2.0273959636688232\n",
      "Epoch 1162: train loss: 2.005967855453491, val loss: 2.026609420776367\n",
      "Epoch 1163: train loss: 2.005202531814575, val loss: 2.0258233547210693\n",
      "Epoch 1164: train loss: 2.0044379234313965, val loss: 2.0250377655029297\n",
      "Epoch 1165: train loss: 2.0036733150482178, val loss: 2.024252414703369\n",
      "Epoch 1166: train loss: 2.002908945083618, val loss: 2.0234668254852295\n",
      "Epoch 1167: train loss: 2.0021448135375977, val loss: 2.022681713104248\n",
      "Epoch 1168: train loss: 2.001380681991577, val loss: 2.0218968391418457\n",
      "Epoch 1169: train loss: 2.0006167888641357, val loss: 2.0211122035980225\n",
      "Epoch 1170: train loss: 1.999853253364563, val loss: 2.02032732963562\n",
      "Epoch 1171: train loss: 1.9990899562835693, val loss: 2.019543170928955\n",
      "Epoch 1172: train loss: 1.9983267784118652, val loss: 2.018758773803711\n",
      "Epoch 1173: train loss: 1.9975638389587402, val loss: 2.017975330352783\n",
      "Epoch 1174: train loss: 1.9968011379241943, val loss: 2.0171914100646973\n",
      "Epoch 1175: train loss: 1.996038556098938, val loss: 2.0164079666137695\n",
      "Epoch 1176: train loss: 1.9952762126922607, val loss: 2.015624761581421\n",
      "Epoch 1177: train loss: 1.994513988494873, val loss: 2.0148417949676514\n",
      "Epoch 1178: train loss: 1.9937522411346436, val loss: 2.014058828353882\n",
      "Epoch 1179: train loss: 1.992990493774414, val loss: 2.0132763385772705\n",
      "Epoch 1180: train loss: 1.9922288656234741, val loss: 2.012493848800659\n",
      "Epoch 1181: train loss: 1.9914677143096924, val loss: 2.011711835861206\n",
      "Epoch 1182: train loss: 1.990706443786621, val loss: 2.010929822921753\n",
      "Epoch 1183: train loss: 1.9899455308914185, val loss: 2.0101478099823\n",
      "Epoch 1184: train loss: 1.9891847372055054, val loss: 2.009366512298584\n",
      "Epoch 1185: train loss: 1.9884244203567505, val loss: 2.008585214614868\n",
      "Epoch 1186: train loss: 1.9876642227172852, val loss: 2.0078039169311523\n",
      "Epoch 1187: train loss: 1.9869041442871094, val loss: 2.0070230960845947\n",
      "Epoch 1188: train loss: 1.9861440658569336, val loss: 2.006242513656616\n",
      "Epoch 1189: train loss: 1.985384464263916, val loss: 2.005462408065796\n",
      "Epoch 1190: train loss: 1.984624981880188, val loss: 2.0046818256378174\n",
      "Epoch 1191: train loss: 1.983865737915039, val loss: 2.003901958465576\n",
      "Epoch 1192: train loss: 1.9831067323684692, val loss: 2.003122091293335\n",
      "Epoch 1193: train loss: 1.9823479652404785, val loss: 2.002342462539673\n",
      "Epoch 1194: train loss: 1.9815890789031982, val loss: 2.0015628337860107\n",
      "Epoch 1195: train loss: 1.9808307886123657, val loss: 2.000783920288086\n",
      "Epoch 1196: train loss: 1.9800726175308228, val loss: 2.000005006790161\n",
      "Epoch 1197: train loss: 1.9793145656585693, val loss: 1.9992262125015259\n",
      "Epoch 1198: train loss: 1.978556752204895, val loss: 1.9984476566314697\n",
      "Epoch 1199: train loss: 1.9777991771697998, val loss: 1.9976692199707031\n",
      "Epoch 1200: train loss: 1.9770417213439941, val loss: 1.9968912601470947\n",
      "Epoch 1201: train loss: 1.9762845039367676, val loss: 1.9961134195327759\n",
      "Epoch 1202: train loss: 1.975527286529541, val loss: 1.995335578918457\n",
      "Epoch 1203: train loss: 1.9747705459594727, val loss: 1.9945582151412964\n",
      "Epoch 1204: train loss: 1.9740140438079834, val loss: 1.9937809705734253\n",
      "Epoch 1205: train loss: 1.9732575416564941, val loss: 1.9930038452148438\n",
      "Epoch 1206: train loss: 1.972501277923584, val loss: 1.9922269582748413\n",
      "Epoch 1207: train loss: 1.9717453718185425, val loss: 1.991450548171997\n",
      "Epoch 1208: train loss: 1.97098970413208, val loss: 1.9906740188598633\n",
      "Epoch 1209: train loss: 1.9702339172363281, val loss: 1.9898979663848877\n",
      "Epoch 1210: train loss: 1.9694786071777344, val loss: 1.9891220331192017\n",
      "Epoch 1211: train loss: 1.9687232971191406, val loss: 1.9883460998535156\n",
      "Epoch 1212: train loss: 1.967968463897705, val loss: 1.9875706434249878\n",
      "Epoch 1213: train loss: 1.9672136306762695, val loss: 1.9867950677871704\n",
      "Epoch 1214: train loss: 1.966459035873413, val loss: 1.9860199689865112\n",
      "Epoch 1215: train loss: 1.9657046794891357, val loss: 1.9852451086044312\n",
      "Epoch 1216: train loss: 1.964950442314148, val loss: 1.9844706058502197\n",
      "Epoch 1217: train loss: 1.9641964435577393, val loss: 1.9836959838867188\n",
      "Epoch 1218: train loss: 1.9634426832199097, val loss: 1.9829216003417969\n",
      "Epoch 1219: train loss: 1.9626890420913696, val loss: 1.982147455215454\n",
      "Epoch 1220: train loss: 1.9619357585906982, val loss: 1.98137366771698\n",
      "Epoch 1221: train loss: 1.9611825942993164, val loss: 1.9805999994277954\n",
      "Epoch 1222: train loss: 1.9604295492172241, val loss: 1.9798263311386108\n",
      "Epoch 1223: train loss: 1.9596768617630005, val loss: 1.9790534973144531\n",
      "Epoch 1224: train loss: 1.9589240550994873, val loss: 1.9782803058624268\n",
      "Epoch 1225: train loss: 1.9581718444824219, val loss: 1.977507472038269\n",
      "Epoch 1226: train loss: 1.957419753074646, val loss: 1.9767347574234009\n",
      "Epoch 1227: train loss: 1.9566677808761597, val loss: 1.975962519645691\n",
      "Epoch 1228: train loss: 1.955915927886963, val loss: 1.9751904010772705\n",
      "Epoch 1229: train loss: 1.9551643133163452, val loss: 1.97441828250885\n",
      "Epoch 1230: train loss: 1.9544129371643066, val loss: 1.9736465215682983\n",
      "Epoch 1231: train loss: 1.9536617994308472, val loss: 1.9728749990463257\n",
      "Epoch 1232: train loss: 1.9529109001159668, val loss: 1.972103476524353\n",
      "Epoch 1233: train loss: 1.9521600008010864, val loss: 1.9713325500488281\n",
      "Epoch 1234: train loss: 1.9514095783233643, val loss: 1.9705616235733032\n",
      "Epoch 1235: train loss: 1.9506592750549316, val loss: 1.9697906970977783\n",
      "Epoch 1236: train loss: 1.949908971786499, val loss: 1.9690202474594116\n",
      "Epoch 1237: train loss: 1.9491591453552246, val loss: 1.9682502746582031\n",
      "Epoch 1238: train loss: 1.9484094381332397, val loss: 1.967479944229126\n",
      "Epoch 1239: train loss: 1.9476598501205444, val loss: 1.966710090637207\n",
      "Epoch 1240: train loss: 1.9469106197357178, val loss: 1.9659404754638672\n",
      "Epoch 1241: train loss: 1.9461613893508911, val loss: 1.9651710987091064\n",
      "Epoch 1242: train loss: 1.9454126358032227, val loss: 1.9644018411636353\n",
      "Epoch 1243: train loss: 1.9446638822555542, val loss: 1.9636329412460327\n",
      "Epoch 1244: train loss: 1.9439151287078857, val loss: 1.9628639221191406\n",
      "Epoch 1245: train loss: 1.943166971206665, val loss: 1.9620952606201172\n",
      "Epoch 1246: train loss: 1.9424188137054443, val loss: 1.9613269567489624\n",
      "Epoch 1247: train loss: 1.9416707754135132, val loss: 1.9605587720870972\n",
      "Epoch 1248: train loss: 1.9409230947494507, val loss: 1.959790825843811\n",
      "Epoch 1249: train loss: 1.9401755332946777, val loss: 1.959023118019104\n",
      "Epoch 1250: train loss: 1.9394282102584839, val loss: 1.958255648612976\n",
      "Epoch 1251: train loss: 1.9386811256408691, val loss: 1.9574882984161377\n",
      "Epoch 1252: train loss: 1.9379342794418335, val loss: 1.9567211866378784\n",
      "Epoch 1253: train loss: 1.9371877908706665, val loss: 1.9559539556503296\n",
      "Epoch 1254: train loss: 1.9364410638809204, val loss: 1.9551876783370972\n",
      "Epoch 1255: train loss: 1.9356948137283325, val loss: 1.954421043395996\n",
      "Epoch 1256: train loss: 1.9349488019943237, val loss: 1.9536548852920532\n",
      "Epoch 1257: train loss: 1.934202790260315, val loss: 1.9528888463974\n",
      "Epoch 1258: train loss: 1.9334571361541748, val loss: 1.9521229267120361\n",
      "Epoch 1259: train loss: 1.9327114820480347, val loss: 1.951357126235962\n",
      "Epoch 1260: train loss: 1.9319661855697632, val loss: 1.9505919218063354\n",
      "Epoch 1261: train loss: 1.9312212467193604, val loss: 1.9498264789581299\n",
      "Epoch 1262: train loss: 1.9304763078689575, val loss: 1.949061393737793\n",
      "Epoch 1263: train loss: 1.9297316074371338, val loss: 1.9482967853546143\n",
      "Epoch 1264: train loss: 1.9289870262145996, val loss: 1.947532296180725\n",
      "Epoch 1265: train loss: 1.9282429218292236, val loss: 1.946767807006836\n",
      "Epoch 1266: train loss: 1.9274988174438477, val loss: 1.9460035562515259\n",
      "Epoch 1267: train loss: 1.9267548322677612, val loss: 1.9452396631240845\n",
      "Epoch 1268: train loss: 1.9260112047195435, val loss: 1.9444760084152222\n",
      "Epoch 1269: train loss: 1.9252675771713257, val loss: 1.9437122344970703\n",
      "Epoch 1270: train loss: 1.9245243072509766, val loss: 1.9429491758346558\n",
      "Epoch 1271: train loss: 1.923781394958496, val loss: 1.9421859979629517\n",
      "Epoch 1272: train loss: 1.9230384826660156, val loss: 1.9414230585098267\n",
      "Epoch 1273: train loss: 1.9222958087921143, val loss: 1.9406604766845703\n",
      "Epoch 1274: train loss: 1.9215532541275024, val loss: 1.939897894859314\n",
      "Epoch 1275: train loss: 1.9208110570907593, val loss: 1.9391354322433472\n",
      "Epoch 1276: train loss: 1.9200688600540161, val loss: 1.9383732080459595\n",
      "Epoch 1277: train loss: 1.9193270206451416, val loss: 1.93761146068573\n",
      "Epoch 1278: train loss: 1.9185853004455566, val loss: 1.93684983253479\n",
      "Epoch 1279: train loss: 1.9178438186645508, val loss: 1.93608820438385\n",
      "Epoch 1280: train loss: 1.9171024560928345, val loss: 1.935327172279358\n",
      "Epoch 1281: train loss: 1.9163613319396973, val loss: 1.9345661401748657\n",
      "Epoch 1282: train loss: 1.9156206846237183, val loss: 1.9338051080703735\n",
      "Epoch 1283: train loss: 1.9148797988891602, val loss: 1.933044672012329\n",
      "Epoch 1284: train loss: 1.9141395092010498, val loss: 1.9322842359542847\n",
      "Epoch 1285: train loss: 1.9133989810943604, val loss: 1.9315239191055298\n",
      "Epoch 1286: train loss: 1.912658929824829, val loss: 1.930764079093933\n",
      "Epoch 1287: train loss: 1.9119192361831665, val loss: 1.9300041198730469\n",
      "Epoch 1288: train loss: 1.9111794233322144, val loss: 1.9292446374893188\n",
      "Epoch 1289: train loss: 1.9104399681091309, val loss: 1.9284851551055908\n",
      "Epoch 1290: train loss: 1.9097005128860474, val loss: 1.9277260303497314\n",
      "Epoch 1291: train loss: 1.908961534500122, val loss: 1.9269670248031616\n",
      "Epoch 1292: train loss: 1.9082225561141968, val loss: 1.9262083768844604\n",
      "Epoch 1293: train loss: 1.9074839353561401, val loss: 1.9254497289657593\n",
      "Epoch 1294: train loss: 1.906745433807373, val loss: 1.9246914386749268\n",
      "Epoch 1295: train loss: 1.9060072898864746, val loss: 1.9239333868026733\n",
      "Epoch 1296: train loss: 1.9052690267562866, val loss: 1.9231754541397095\n",
      "Epoch 1297: train loss: 1.9045312404632568, val loss: 1.9224178791046143\n",
      "Epoch 1298: train loss: 1.9037935733795166, val loss: 1.921660304069519\n",
      "Epoch 1299: train loss: 1.903056025505066, val loss: 1.9209030866622925\n",
      "Epoch 1300: train loss: 1.9023187160491943, val loss: 1.920145869255066\n",
      "Epoch 1301: train loss: 1.9015815258026123, val loss: 1.919389009475708\n",
      "Epoch 1302: train loss: 1.900844693183899, val loss: 1.9186325073242188\n",
      "Epoch 1303: train loss: 1.9001082181930542, val loss: 1.917876124382019\n",
      "Epoch 1304: train loss: 1.8993717432022095, val loss: 1.9171196222305298\n",
      "Epoch 1305: train loss: 1.8986353874206543, val loss: 1.9163637161254883\n",
      "Epoch 1306: train loss: 1.8978992700576782, val loss: 1.9156076908111572\n",
      "Epoch 1307: train loss: 1.8971632719039917, val loss: 1.9148523807525635\n",
      "Epoch 1308: train loss: 1.8964276313781738, val loss: 1.9140968322753906\n",
      "Epoch 1309: train loss: 1.8956921100616455, val loss: 1.9133415222167969\n",
      "Epoch 1310: train loss: 1.8949568271636963, val loss: 1.9125865697860718\n",
      "Epoch 1311: train loss: 1.8942216634750366, val loss: 1.9118318557739258\n",
      "Epoch 1312: train loss: 1.8934868574142456, val loss: 1.9110773801803589\n",
      "Epoch 1313: train loss: 1.8927521705627441, val loss: 1.9103230237960815\n",
      "Epoch 1314: train loss: 1.8920177221298218, val loss: 1.909568428993225\n",
      "Epoch 1315: train loss: 1.8912831544876099, val loss: 1.908814787864685\n",
      "Epoch 1316: train loss: 1.8905494213104248, val loss: 1.908060908317566\n",
      "Epoch 1317: train loss: 1.889815330505371, val loss: 1.907307505607605\n",
      "Epoch 1318: train loss: 1.8890814781188965, val loss: 1.906554102897644\n",
      "Epoch 1319: train loss: 1.88834810256958, val loss: 1.9058008193969727\n",
      "Epoch 1320: train loss: 1.8876148462295532, val loss: 1.9050480127334595\n",
      "Epoch 1321: train loss: 1.886881709098816, val loss: 1.9042953252792358\n",
      "Epoch 1322: train loss: 1.8861488103866577, val loss: 1.9035428762435913\n",
      "Epoch 1323: train loss: 1.8854161500930786, val loss: 1.9027904272079468\n",
      "Epoch 1324: train loss: 1.8846834897994995, val loss: 1.9020384550094604\n",
      "Epoch 1325: train loss: 1.883951187133789, val loss: 1.9012864828109741\n",
      "Epoch 1326: train loss: 1.8832191228866577, val loss: 1.9005348682403564\n",
      "Epoch 1327: train loss: 1.882487416267395, val loss: 1.8997833728790283\n",
      "Epoch 1328: train loss: 1.8817554712295532, val loss: 1.8990319967269897\n",
      "Epoch 1329: train loss: 1.8810240030288696, val loss: 1.8982810974121094\n",
      "Epoch 1330: train loss: 1.8802927732467651, val loss: 1.897530436515808\n",
      "Epoch 1331: train loss: 1.8795617818832397, val loss: 1.8967796564102173\n",
      "Epoch 1332: train loss: 1.8788306713104248, val loss: 1.8960291147232056\n",
      "Epoch 1333: train loss: 1.8780999183654785, val loss: 1.8952789306640625\n",
      "Epoch 1334: train loss: 1.8773692846298218, val loss: 1.8945289850234985\n",
      "Epoch 1335: train loss: 1.8766391277313232, val loss: 1.8937790393829346\n",
      "Epoch 1336: train loss: 1.8759087324142456, val loss: 1.8930294513702393\n",
      "Epoch 1337: train loss: 1.8751789331436157, val loss: 1.8922799825668335\n",
      "Epoch 1338: train loss: 1.874449372291565, val loss: 1.8915308713912964\n",
      "Epoch 1339: train loss: 1.8737196922302246, val loss: 1.8907817602157593\n",
      "Epoch 1340: train loss: 1.8729902505874634, val loss: 1.8900330066680908\n",
      "Epoch 1341: train loss: 1.8722612857818604, val loss: 1.8892844915390015\n",
      "Epoch 1342: train loss: 1.8715322017669678, val loss: 1.8885360956192017\n",
      "Epoch 1343: train loss: 1.8708035945892334, val loss: 1.8877880573272705\n",
      "Epoch 1344: train loss: 1.8700748682022095, val loss: 1.8870400190353394\n",
      "Epoch 1345: train loss: 1.8693466186523438, val loss: 1.8862923383712769\n",
      "Epoch 1346: train loss: 1.8686184883117676, val loss: 1.8855446577072144\n",
      "Epoch 1347: train loss: 1.8678905963897705, val loss: 1.8847973346710205\n",
      "Epoch 1348: train loss: 1.867162823677063, val loss: 1.8840502500534058\n",
      "Epoch 1349: train loss: 1.866435170173645, val loss: 1.8833035230636597\n",
      "Epoch 1350: train loss: 1.8657077550888062, val loss: 1.8825569152832031\n",
      "Epoch 1351: train loss: 1.8649808168411255, val loss: 1.8818104267120361\n",
      "Epoch 1352: train loss: 1.8642537593841553, val loss: 1.8810638189315796\n",
      "Epoch 1353: train loss: 1.8635271787643433, val loss: 1.8803179264068604\n",
      "Epoch 1354: train loss: 1.8628004789352417, val loss: 1.8795719146728516\n",
      "Epoch 1355: train loss: 1.8620741367340088, val loss: 1.878826379776001\n",
      "Epoch 1356: train loss: 1.861348032951355, val loss: 1.8780807256698608\n",
      "Epoch 1357: train loss: 1.8606218099594116, val loss: 1.877335548400879\n",
      "Epoch 1358: train loss: 1.859896183013916, val loss: 1.8765901327133179\n",
      "Epoch 1359: train loss: 1.8591704368591309, val loss: 1.8758455514907837\n",
      "Epoch 1360: train loss: 1.8584452867507935, val loss: 1.8751007318496704\n",
      "Epoch 1361: train loss: 1.8577200174331665, val loss: 1.8743561506271362\n",
      "Epoch 1362: train loss: 1.85699462890625, val loss: 1.8736120462417603\n",
      "Epoch 1363: train loss: 1.8562700748443604, val loss: 1.8728679418563843\n",
      "Epoch 1364: train loss: 1.8555454015731812, val loss: 1.8721240758895874\n",
      "Epoch 1365: train loss: 1.854820966720581, val loss: 1.8713806867599487\n",
      "Epoch 1366: train loss: 1.8540968894958496, val loss: 1.8706371784210205\n",
      "Epoch 1367: train loss: 1.8533726930618286, val loss: 1.8698936700820923\n",
      "Epoch 1368: train loss: 1.8526489734649658, val loss: 1.869150996208191\n",
      "Epoch 1369: train loss: 1.851925253868103, val loss: 1.8684080839157104\n",
      "Epoch 1370: train loss: 1.8512017726898193, val loss: 1.8676655292510986\n",
      "Epoch 1371: train loss: 1.8504785299301147, val loss: 1.8669229745864868\n",
      "Epoch 1372: train loss: 1.8497555255889893, val loss: 1.8661807775497437\n",
      "Epoch 1373: train loss: 1.8490327596664429, val loss: 1.8654388189315796\n",
      "Epoch 1374: train loss: 1.8483099937438965, val loss: 1.8646968603134155\n",
      "Epoch 1375: train loss: 1.8475878238677979, val loss: 1.8639553785324097\n",
      "Epoch 1376: train loss: 1.8468655347824097, val loss: 1.863214135169983\n",
      "Epoch 1377: train loss: 1.8461432456970215, val loss: 1.8624727725982666\n",
      "Epoch 1378: train loss: 1.845421552658081, val loss: 1.861731767654419\n",
      "Epoch 1379: train loss: 1.8446998596191406, val loss: 1.86099112033844\n",
      "Epoch 1380: train loss: 1.8439784049987793, val loss: 1.860250473022461\n",
      "Epoch 1381: train loss: 1.843257188796997, val loss: 1.859510064125061\n",
      "Epoch 1382: train loss: 1.8425360918045044, val loss: 1.8587700128555298\n",
      "Epoch 1383: train loss: 1.8418152332305908, val loss: 1.8580302000045776\n",
      "Epoch 1384: train loss: 1.841094732284546, val loss: 1.857290267944336\n",
      "Epoch 1385: train loss: 1.8403741121292114, val loss: 1.8565508127212524\n",
      "Epoch 1386: train loss: 1.839653730392456, val loss: 1.8558114767074585\n",
      "Epoch 1387: train loss: 1.8389337062835693, val loss: 1.8550723791122437\n",
      "Epoch 1388: train loss: 1.8382140398025513, val loss: 1.854333519935608\n",
      "Epoch 1389: train loss: 1.8374942541122437, val loss: 1.8535946607589722\n",
      "Epoch 1390: train loss: 1.8367747068405151, val loss: 1.8528562784194946\n",
      "Epoch 1391: train loss: 1.8360553979873657, val loss: 1.852117896080017\n",
      "Epoch 1392: train loss: 1.8353362083435059, val loss: 1.851379632949829\n",
      "Epoch 1393: train loss: 1.8346171379089355, val loss: 1.8506420850753784\n",
      "Epoch 1394: train loss: 1.833898663520813, val loss: 1.8499040603637695\n",
      "Epoch 1395: train loss: 1.8331800699234009, val loss: 1.8491665124893188\n",
      "Epoch 1396: train loss: 1.8324615955352783, val loss: 1.8484290838241577\n",
      "Epoch 1397: train loss: 1.8317433595657349, val loss: 1.8476921319961548\n",
      "Epoch 1398: train loss: 1.8310256004333496, val loss: 1.8469551801681519\n",
      "Epoch 1399: train loss: 1.8303077220916748, val loss: 1.846218466758728\n",
      "Epoch 1400: train loss: 1.8295902013778687, val loss: 1.8454822301864624\n",
      "Epoch 1401: train loss: 1.8288730382919312, val loss: 1.8447456359863281\n",
      "Epoch 1402: train loss: 1.828155517578125, val loss: 1.8440097570419312\n",
      "Epoch 1403: train loss: 1.8274387121200562, val loss: 1.8432739973068237\n",
      "Epoch 1404: train loss: 1.8267220258712769, val loss: 1.8425382375717163\n",
      "Epoch 1405: train loss: 1.826005458831787, val loss: 1.8418028354644775\n",
      "Epoch 1406: train loss: 1.8252888917922974, val loss: 1.8410675525665283\n",
      "Epoch 1407: train loss: 1.8245728015899658, val loss: 1.8403323888778687\n",
      "Epoch 1408: train loss: 1.8238569498062134, val loss: 1.8395977020263672\n",
      "Epoch 1409: train loss: 1.8231412172317505, val loss: 1.8388630151748657\n",
      "Epoch 1410: train loss: 1.8224257230758667, val loss: 1.8381284475326538\n",
      "Epoch 1411: train loss: 1.821710228919983, val loss: 1.8373943567276\n",
      "Epoch 1412: train loss: 1.8209950923919678, val loss: 1.836660385131836\n",
      "Epoch 1413: train loss: 1.8202799558639526, val loss: 1.8359264135360718\n",
      "Epoch 1414: train loss: 1.8195651769638062, val loss: 1.8351930379867554\n",
      "Epoch 1415: train loss: 1.8188507556915283, val loss: 1.834459662437439\n",
      "Epoch 1416: train loss: 1.8181363344192505, val loss: 1.8337265253067017\n",
      "Epoch 1417: train loss: 1.8174220323562622, val loss: 1.8329933881759644\n",
      "Epoch 1418: train loss: 1.816707968711853, val loss: 1.8322607278823853\n",
      "Epoch 1419: train loss: 1.8159940242767334, val loss: 1.8315277099609375\n",
      "Epoch 1420: train loss: 1.815280556678772, val loss: 1.8307952880859375\n",
      "Epoch 1421: train loss: 1.814566969871521, val loss: 1.8300632238388062\n",
      "Epoch 1422: train loss: 1.8138536214828491, val loss: 1.8293312788009644\n",
      "Epoch 1423: train loss: 1.8131407499313354, val loss: 1.828599214553833\n",
      "Epoch 1424: train loss: 1.8124276399612427, val loss: 1.827867865562439\n",
      "Epoch 1425: train loss: 1.811715006828308, val loss: 1.8271362781524658\n",
      "Epoch 1426: train loss: 1.8110026121139526, val loss: 1.82640540599823\n",
      "Epoch 1427: train loss: 1.8102902173995972, val loss: 1.825674295425415\n",
      "Epoch 1428: train loss: 1.8095781803131104, val loss: 1.8249434232711792\n",
      "Epoch 1429: train loss: 1.808866262435913, val loss: 1.824212908744812\n",
      "Epoch 1430: train loss: 1.8081547021865845, val loss: 1.8234825134277344\n",
      "Epoch 1431: train loss: 1.8074431419372559, val loss: 1.8227519989013672\n",
      "Epoch 1432: train loss: 1.806731939315796, val loss: 1.8220223188400269\n",
      "Epoch 1433: train loss: 1.8060206174850464, val loss: 1.821292757987976\n",
      "Epoch 1434: train loss: 1.805309772491455, val loss: 1.8205631971359253\n",
      "Epoch 1435: train loss: 1.8045990467071533, val loss: 1.8198336362838745\n",
      "Epoch 1436: train loss: 1.8038884401321411, val loss: 1.819104552268982\n",
      "Epoch 1437: train loss: 1.803178310394287, val loss: 1.818375825881958\n",
      "Epoch 1438: train loss: 1.802468180656433, val loss: 1.8176469802856445\n",
      "Epoch 1439: train loss: 1.8017581701278687, val loss: 1.8169183731079102\n",
      "Epoch 1440: train loss: 1.8010485172271729, val loss: 1.8161901235580444\n",
      "Epoch 1441: train loss: 1.8003387451171875, val loss: 1.8154619932174683\n",
      "Epoch 1442: train loss: 1.7996294498443604, val loss: 1.814733862876892\n",
      "Epoch 1443: train loss: 1.7989203929901123, val loss: 1.8140062093734741\n",
      "Epoch 1444: train loss: 1.7982113361358643, val loss: 1.8132785558700562\n",
      "Epoch 1445: train loss: 1.7975023984909058, val loss: 1.8125513792037964\n",
      "Epoch 1446: train loss: 1.7967936992645264, val loss: 1.811824083328247\n",
      "Epoch 1447: train loss: 1.7960854768753052, val loss: 1.8110971450805664\n",
      "Epoch 1448: train loss: 1.795377254486084, val loss: 1.8103704452514648\n",
      "Epoch 1449: train loss: 1.7946691513061523, val loss: 1.8096439838409424\n",
      "Epoch 1450: train loss: 1.7939612865447998, val loss: 1.8089176416397095\n",
      "Epoch 1451: train loss: 1.793253779411316, val loss: 1.8081912994384766\n",
      "Epoch 1452: train loss: 1.792546272277832, val loss: 1.8074655532836914\n",
      "Epoch 1453: train loss: 1.7918390035629272, val loss: 1.8067398071289062\n",
      "Epoch 1454: train loss: 1.7911319732666016, val loss: 1.8060142993927002\n",
      "Epoch 1455: train loss: 1.7904253005981445, val loss: 1.8052891492843628\n",
      "Epoch 1456: train loss: 1.7897186279296875, val loss: 1.8045638799667358\n",
      "Epoch 1457: train loss: 1.78901207447052, val loss: 1.8038389682769775\n",
      "Epoch 1458: train loss: 1.7883057594299316, val loss: 1.8031142950057983\n",
      "Epoch 1459: train loss: 1.7875996828079224, val loss: 1.8023899793624878\n",
      "Epoch 1460: train loss: 1.7868939638137817, val loss: 1.8016655445098877\n",
      "Epoch 1461: train loss: 1.7861881256103516, val loss: 1.8009413480758667\n",
      "Epoch 1462: train loss: 1.7854828834533691, val loss: 1.8002175092697144\n",
      "Epoch 1463: train loss: 1.7847774028778076, val loss: 1.7994937896728516\n",
      "Epoch 1464: train loss: 1.7840722799301147, val loss: 1.7987703084945679\n",
      "Epoch 1465: train loss: 1.783367395401001, val loss: 1.7980473041534424\n",
      "Epoch 1466: train loss: 1.7826626300811768, val loss: 1.7973240613937378\n",
      "Epoch 1467: train loss: 1.7819582223892212, val loss: 1.7966011762619019\n",
      "Epoch 1468: train loss: 1.7812538146972656, val loss: 1.7958784103393555\n",
      "Epoch 1469: train loss: 1.7805496454238892, val loss: 1.7951558828353882\n",
      "Epoch 1470: train loss: 1.7798457145690918, val loss: 1.79443359375\n",
      "Epoch 1471: train loss: 1.779141902923584, val loss: 1.793711543083191\n",
      "Epoch 1472: train loss: 1.7784382104873657, val loss: 1.792989730834961\n",
      "Epoch 1473: train loss: 1.7777348756790161, val loss: 1.7922677993774414\n",
      "Epoch 1474: train loss: 1.7770317792892456, val loss: 1.7915467023849487\n",
      "Epoch 1475: train loss: 1.7763288021087646, val loss: 1.7908251285552979\n",
      "Epoch 1476: train loss: 1.7756259441375732, val loss: 1.7901039123535156\n",
      "Epoch 1477: train loss: 1.7749234437942505, val loss: 1.7893831729888916\n",
      "Epoch 1478: train loss: 1.7742208242416382, val loss: 1.7886625528335571\n",
      "Epoch 1479: train loss: 1.7735188007354736, val loss: 1.7879419326782227\n",
      "Epoch 1480: train loss: 1.7728166580200195, val loss: 1.7872217893600464\n",
      "Epoch 1481: train loss: 1.7721147537231445, val loss: 1.7865017652511597\n",
      "Epoch 1482: train loss: 1.7714132070541382, val loss: 1.7857818603515625\n",
      "Epoch 1483: train loss: 1.7707117795944214, val loss: 1.7850624322891235\n",
      "Epoch 1484: train loss: 1.7700107097625732, val loss: 1.784342646598816\n",
      "Epoch 1485: train loss: 1.7693095207214355, val loss: 1.7836233377456665\n",
      "Epoch 1486: train loss: 1.7686086893081665, val loss: 1.7829042673110962\n",
      "Epoch 1487: train loss: 1.767907977104187, val loss: 1.7821855545043945\n",
      "Epoch 1488: train loss: 1.7672075033187866, val loss: 1.7814667224884033\n",
      "Epoch 1489: train loss: 1.7665071487426758, val loss: 1.7807482481002808\n",
      "Epoch 1490: train loss: 1.7658071517944336, val loss: 1.7800301313400269\n",
      "Epoch 1491: train loss: 1.7651070356369019, val loss: 1.779312014579773\n",
      "Epoch 1492: train loss: 1.7644073963165283, val loss: 1.7785940170288086\n",
      "Epoch 1493: train loss: 1.7637078762054443, val loss: 1.7778762578964233\n",
      "Epoch 1494: train loss: 1.76300847530365, val loss: 1.7771587371826172\n",
      "Epoch 1495: train loss: 1.7623094320297241, val loss: 1.7764415740966797\n",
      "Epoch 1496: train loss: 1.7616102695465088, val loss: 1.7757246494293213\n",
      "Epoch 1497: train loss: 1.7609118223190308, val loss: 1.7750076055526733\n",
      "Epoch 1498: train loss: 1.7602131366729736, val loss: 1.7742910385131836\n",
      "Epoch 1499: train loss: 1.7595146894454956, val loss: 1.7735748291015625\n",
      "Epoch 1500: train loss: 1.7588165998458862, val loss: 1.7728585004806519\n",
      "Epoch 1501: train loss: 1.7581183910369873, val loss: 1.7721422910690308\n",
      "Epoch 1502: train loss: 1.7574207782745361, val loss: 1.7714264392852783\n",
      "Epoch 1503: train loss: 1.7567230463027954, val loss: 1.770710825920105\n",
      "Epoch 1504: train loss: 1.7560259103775024, val loss: 1.769995093345642\n",
      "Epoch 1505: train loss: 1.7553284168243408, val loss: 1.7692800760269165\n",
      "Epoch 1506: train loss: 1.7546316385269165, val loss: 1.768565058708191\n",
      "Epoch 1507: train loss: 1.7539347410202026, val loss: 1.7678501605987549\n",
      "Epoch 1508: train loss: 1.7532379627227783, val loss: 1.7671352624893188\n",
      "Epoch 1509: train loss: 1.752541422843933, val loss: 1.7664207220077515\n",
      "Epoch 1510: train loss: 1.751845359802246, val loss: 1.7657068967819214\n",
      "Epoch 1511: train loss: 1.7511491775512695, val loss: 1.764992594718933\n",
      "Epoch 1512: train loss: 1.750453233718872, val loss: 1.764278769493103\n",
      "Epoch 1513: train loss: 1.7497576475143433, val loss: 1.7635650634765625\n",
      "Epoch 1514: train loss: 1.749062180519104, val loss: 1.762851357460022\n",
      "Epoch 1515: train loss: 1.7483667135238647, val loss: 1.7621383666992188\n",
      "Epoch 1516: train loss: 1.7476716041564941, val loss: 1.7614250183105469\n",
      "Epoch 1517: train loss: 1.7469767332077026, val loss: 1.7607122659683228\n",
      "Epoch 1518: train loss: 1.7462819814682007, val loss: 1.7599995136260986\n",
      "Epoch 1519: train loss: 1.7455875873565674, val loss: 1.7592872381210327\n",
      "Epoch 1520: train loss: 1.7448930740356445, val loss: 1.7585750818252563\n",
      "Epoch 1521: train loss: 1.7441987991333008, val loss: 1.75786292552948\n",
      "Epoch 1522: train loss: 1.7435048818588257, val loss: 1.7571512460708618\n",
      "Epoch 1523: train loss: 1.7428110837936401, val loss: 1.7564395666122437\n",
      "Epoch 1524: train loss: 1.7421175241470337, val loss: 1.755727767944336\n",
      "Epoch 1525: train loss: 1.7414240837097168, val loss: 1.7550166845321655\n",
      "Epoch 1526: train loss: 1.740730881690979, val loss: 1.7543057203292847\n",
      "Epoch 1527: train loss: 1.7400377988815308, val loss: 1.7535946369171143\n",
      "Epoch 1528: train loss: 1.7393449544906616, val loss: 1.7528839111328125\n",
      "Epoch 1529: train loss: 1.738652229309082, val loss: 1.7521734237670898\n",
      "Epoch 1530: train loss: 1.737959861755371, val loss: 1.7514632940292358\n",
      "Epoch 1531: train loss: 1.7372673749923706, val loss: 1.7507530450820923\n",
      "Epoch 1532: train loss: 1.7365754842758179, val loss: 1.750043272972107\n",
      "Epoch 1533: train loss: 1.7358835935592651, val loss: 1.7493336200714111\n",
      "Epoch 1534: train loss: 1.7351919412612915, val loss: 1.7486240863800049\n",
      "Epoch 1535: train loss: 1.734500527381897, val loss: 1.7479145526885986\n",
      "Epoch 1536: train loss: 1.733809232711792, val loss: 1.7472059726715088\n",
      "Epoch 1537: train loss: 1.733117938041687, val loss: 1.7464970350265503\n",
      "Epoch 1538: train loss: 1.7324270009994507, val loss: 1.7457882165908813\n",
      "Epoch 1539: train loss: 1.7317363023757935, val loss: 1.7450798749923706\n",
      "Epoch 1540: train loss: 1.7310459613800049, val loss: 1.7443714141845703\n",
      "Epoch 1541: train loss: 1.7303555011749268, val loss: 1.7436634302139282\n",
      "Epoch 1542: train loss: 1.7296655178070068, val loss: 1.7429555654525757\n",
      "Epoch 1543: train loss: 1.7289752960205078, val loss: 1.7422478199005127\n",
      "Epoch 1544: train loss: 1.7282854318618774, val loss: 1.7415403127670288\n",
      "Epoch 1545: train loss: 1.7275960445404053, val loss: 1.7408329248428345\n",
      "Epoch 1546: train loss: 1.726906418800354, val loss: 1.7401258945465088\n",
      "Epoch 1547: train loss: 1.7262173891067505, val loss: 1.739418864250183\n",
      "Epoch 1548: train loss: 1.725528359413147, val loss: 1.7387123107910156\n",
      "Epoch 1549: train loss: 1.7248393297195435, val loss: 1.7380056381225586\n",
      "Epoch 1550: train loss: 1.7241506576538086, val loss: 1.7372993230819702\n",
      "Epoch 1551: train loss: 1.7234623432159424, val loss: 1.736593246459961\n",
      "Epoch 1552: train loss: 1.7227741479873657, val loss: 1.7358874082565308\n",
      "Epoch 1553: train loss: 1.722085952758789, val loss: 1.7351816892623901\n",
      "Epoch 1554: train loss: 1.7213979959487915, val loss: 1.7344763278961182\n",
      "Epoch 1555: train loss: 1.7207105159759521, val loss: 1.7337709665298462\n",
      "Epoch 1556: train loss: 1.7200229167938232, val loss: 1.7330658435821533\n",
      "Epoch 1557: train loss: 1.719335675239563, val loss: 1.73236083984375\n",
      "Epoch 1558: train loss: 1.7186484336853027, val loss: 1.7316563129425049\n",
      "Epoch 1559: train loss: 1.7179615497589111, val loss: 1.7309516668319702\n",
      "Epoch 1560: train loss: 1.7172746658325195, val loss: 1.730247139930725\n",
      "Epoch 1561: train loss: 1.716588020324707, val loss: 1.7295430898666382\n",
      "Epoch 1562: train loss: 1.7159017324447632, val loss: 1.7288391590118408\n",
      "Epoch 1563: train loss: 1.7152154445648193, val loss: 1.7281354665756226\n",
      "Epoch 1564: train loss: 1.7145295143127441, val loss: 1.7274318933486938\n",
      "Epoch 1565: train loss: 1.7138437032699585, val loss: 1.7267284393310547\n",
      "Epoch 1566: train loss: 1.7131580114364624, val loss: 1.7260254621505737\n",
      "Epoch 1567: train loss: 1.7124725580215454, val loss: 1.7253223657608032\n",
      "Epoch 1568: train loss: 1.7117873430252075, val loss: 1.724619746208191\n",
      "Epoch 1569: train loss: 1.7111023664474487, val loss: 1.7239173650741577\n",
      "Epoch 1570: train loss: 1.7104175090789795, val loss: 1.7232149839401245\n",
      "Epoch 1571: train loss: 1.7097327709197998, val loss: 1.7225128412246704\n",
      "Epoch 1572: train loss: 1.7090483903884888, val loss: 1.7218109369277954\n",
      "Epoch 1573: train loss: 1.7083642482757568, val loss: 1.7211092710494995\n",
      "Epoch 1574: train loss: 1.7076799869537354, val loss: 1.720407485961914\n",
      "Epoch 1575: train loss: 1.7069960832595825, val loss: 1.719706416130066\n",
      "Epoch 1576: train loss: 1.7063124179840088, val loss: 1.7190052270889282\n",
      "Epoch 1577: train loss: 1.7056289911270142, val loss: 1.7183042764663696\n",
      "Epoch 1578: train loss: 1.7049455642700195, val loss: 1.717603325843811\n",
      "Epoch 1579: train loss: 1.704262375831604, val loss: 1.7169029712677002\n",
      "Epoch 1580: train loss: 1.7035794258117676, val loss: 1.7162023782730103\n",
      "Epoch 1581: train loss: 1.7028967142105103, val loss: 1.715502142906189\n",
      "Epoch 1582: train loss: 1.7022138833999634, val loss: 1.7148023843765259\n",
      "Epoch 1583: train loss: 1.7015316486358643, val loss: 1.7141026258468628\n",
      "Epoch 1584: train loss: 1.7008494138717651, val loss: 1.7134029865264893\n",
      "Epoch 1585: train loss: 1.700167179107666, val loss: 1.7127037048339844\n",
      "Epoch 1586: train loss: 1.6994856595993042, val loss: 1.71200430393219\n",
      "Epoch 1587: train loss: 1.6988039016723633, val loss: 1.7113054990768433\n",
      "Epoch 1588: train loss: 1.698122501373291, val loss: 1.710606575012207\n",
      "Epoch 1589: train loss: 1.6974413394927979, val loss: 1.709908127784729\n",
      "Epoch 1590: train loss: 1.6967600584030151, val loss: 1.7092097997665405\n",
      "Epoch 1591: train loss: 1.6960792541503906, val loss: 1.7085113525390625\n",
      "Epoch 1592: train loss: 1.6953983306884766, val loss: 1.7078135013580322\n",
      "Epoch 1593: train loss: 1.6947180032730103, val loss: 1.7071155309677124\n",
      "Epoch 1594: train loss: 1.6940375566482544, val loss: 1.7064179182052612\n",
      "Epoch 1595: train loss: 1.6933573484420776, val loss: 1.7057205438613892\n",
      "Epoch 1596: train loss: 1.6926774978637695, val loss: 1.705023169517517\n",
      "Epoch 1597: train loss: 1.691997766494751, val loss: 1.7043260335922241\n",
      "Epoch 1598: train loss: 1.691318154335022, val loss: 1.7036293745040894\n",
      "Epoch 1599: train loss: 1.6906386613845825, val loss: 1.7029327154159546\n",
      "Epoch 1600: train loss: 1.6899596452713013, val loss: 1.7022364139556885\n",
      "Epoch 1601: train loss: 1.689280390739441, val loss: 1.7015399932861328\n",
      "Epoch 1602: train loss: 1.6886017322540283, val loss: 1.700844168663025\n",
      "Epoch 1603: train loss: 1.6879231929779053, val loss: 1.7001482248306274\n",
      "Epoch 1604: train loss: 1.6872445344924927, val loss: 1.6994526386260986\n",
      "Epoch 1605: train loss: 1.6865665912628174, val loss: 1.6987571716308594\n",
      "Epoch 1606: train loss: 1.685888409614563, val loss: 1.6980621814727783\n",
      "Epoch 1607: train loss: 1.6852105855941772, val loss: 1.6973669528961182\n",
      "Epoch 1608: train loss: 1.6845327615737915, val loss: 1.6966720819473267\n",
      "Epoch 1609: train loss: 1.6838551759719849, val loss: 1.6959774494171143\n",
      "Epoch 1610: train loss: 1.6831779479980469, val loss: 1.6952828168869019\n",
      "Epoch 1611: train loss: 1.6825006008148193, val loss: 1.6945886611938477\n",
      "Epoch 1612: train loss: 1.68182373046875, val loss: 1.693894624710083\n",
      "Epoch 1613: train loss: 1.6811469793319702, val loss: 1.693200707435608\n",
      "Epoch 1614: train loss: 1.6804704666137695, val loss: 1.692507028579712\n",
      "Epoch 1615: train loss: 1.6797939538955688, val loss: 1.6918137073516846\n",
      "Epoch 1616: train loss: 1.6791179180145264, val loss: 1.6911201477050781\n",
      "Epoch 1617: train loss: 1.6784418821334839, val loss: 1.6904274225234985\n",
      "Epoch 1618: train loss: 1.677765965461731, val loss: 1.6897343397140503\n",
      "Epoch 1619: train loss: 1.6770905256271362, val loss: 1.6890414953231812\n",
      "Epoch 1620: train loss: 1.6764150857925415, val loss: 1.6883491277694702\n",
      "Epoch 1621: train loss: 1.6757398843765259, val loss: 1.6876567602157593\n",
      "Epoch 1622: train loss: 1.6750648021697998, val loss: 1.6869648694992065\n",
      "Epoch 1623: train loss: 1.6743897199630737, val loss: 1.6862728595733643\n",
      "Epoch 1624: train loss: 1.6737151145935059, val loss: 1.6855812072753906\n",
      "Epoch 1625: train loss: 1.673040509223938, val loss: 1.6848896741867065\n",
      "Epoch 1626: train loss: 1.6723660230636597, val loss: 1.684198260307312\n",
      "Epoch 1627: train loss: 1.671692132949829, val loss: 1.683506965637207\n",
      "Epoch 1628: train loss: 1.671018123626709, val loss: 1.6828159093856812\n",
      "Epoch 1629: train loss: 1.6703444719314575, val loss: 1.6821253299713135\n",
      "Epoch 1630: train loss: 1.6696707010269165, val loss: 1.6814348697662354\n",
      "Epoch 1631: train loss: 1.6689972877502441, val loss: 1.6807444095611572\n",
      "Epoch 1632: train loss: 1.6683241128921509, val loss: 1.6800543069839478\n",
      "Epoch 1633: train loss: 1.6676510572433472, val loss: 1.6793644428253174\n",
      "Epoch 1634: train loss: 1.666978359222412, val loss: 1.6786746978759766\n",
      "Epoch 1635: train loss: 1.6663057804107666, val loss: 1.6779850721359253\n",
      "Epoch 1636: train loss: 1.665633201599121, val loss: 1.6772955656051636\n",
      "Epoch 1637: train loss: 1.6649607419967651, val loss: 1.6766064167022705\n",
      "Epoch 1638: train loss: 1.6642887592315674, val loss: 1.6759175062179565\n",
      "Epoch 1639: train loss: 1.6636170148849487, val loss: 1.675228476524353\n",
      "Epoch 1640: train loss: 1.66294527053833, val loss: 1.6745399236679077\n",
      "Epoch 1641: train loss: 1.662273645401001, val loss: 1.6738518476486206\n",
      "Epoch 1642: train loss: 1.66160249710083, val loss: 1.6731634140014648\n",
      "Epoch 1643: train loss: 1.66093111038208, val loss: 1.6724752187728882\n",
      "Epoch 1644: train loss: 1.6602604389190674, val loss: 1.6717876195907593\n",
      "Epoch 1645: train loss: 1.6595895290374756, val loss: 1.6710999011993408\n",
      "Epoch 1646: train loss: 1.6589189767837524, val loss: 1.6704126596450806\n",
      "Epoch 1647: train loss: 1.6582485437393188, val loss: 1.6697254180908203\n",
      "Epoch 1648: train loss: 1.6575783491134644, val loss: 1.6690384149551392\n",
      "Epoch 1649: train loss: 1.656908392906189, val loss: 1.6683515310287476\n",
      "Epoch 1650: train loss: 1.6562385559082031, val loss: 1.6676651239395142\n",
      "Epoch 1651: train loss: 1.6555688381195068, val loss: 1.6669784784317017\n",
      "Epoch 1652: train loss: 1.6548994779586792, val loss: 1.6662921905517578\n",
      "Epoch 1653: train loss: 1.654229998588562, val loss: 1.665606141090393\n",
      "Epoch 1654: train loss: 1.6535608768463135, val loss: 1.664920449256897\n",
      "Epoch 1655: train loss: 1.6528921127319336, val loss: 1.6642345190048218\n",
      "Epoch 1656: train loss: 1.6522234678268433, val loss: 1.6635493040084839\n",
      "Epoch 1657: train loss: 1.6515547037124634, val loss: 1.6628639698028564\n",
      "Epoch 1658: train loss: 1.6508864164352417, val loss: 1.6621789932250977\n",
      "Epoch 1659: train loss: 1.6502182483673096, val loss: 1.6614941358566284\n",
      "Epoch 1660: train loss: 1.649550437927246, val loss: 1.6608093976974487\n",
      "Epoch 1661: train loss: 1.6488823890686035, val loss: 1.6601250171661377\n",
      "Epoch 1662: train loss: 1.6482148170471191, val loss: 1.6594406366348267\n",
      "Epoch 1663: train loss: 1.6475474834442139, val loss: 1.6587566137313843\n",
      "Epoch 1664: train loss: 1.6468801498413086, val loss: 1.6580727100372314\n",
      "Epoch 1665: train loss: 1.6462132930755615, val loss: 1.657388687133789\n",
      "Epoch 1666: train loss: 1.645546317100525, val loss: 1.6567052602767944\n",
      "Epoch 1667: train loss: 1.6448795795440674, val loss: 1.656022071838379\n",
      "Epoch 1668: train loss: 1.6442131996154785, val loss: 1.6553386449813843\n",
      "Epoch 1669: train loss: 1.6435467004776, val loss: 1.6546558141708374\n",
      "Epoch 1670: train loss: 1.6428807973861694, val loss: 1.6539729833602905\n",
      "Epoch 1671: train loss: 1.6422146558761597, val loss: 1.6532903909683228\n",
      "Epoch 1672: train loss: 1.641548991203308, val loss: 1.6526081562042236\n",
      "Epoch 1673: train loss: 1.640883445739746, val loss: 1.651926040649414\n",
      "Epoch 1674: train loss: 1.6402180194854736, val loss: 1.651244044303894\n",
      "Epoch 1675: train loss: 1.6395528316497803, val loss: 1.6505622863769531\n",
      "Epoch 1676: train loss: 1.638887882232666, val loss: 1.6498806476593018\n",
      "Epoch 1677: train loss: 1.6382231712341309, val loss: 1.649199366569519\n",
      "Epoch 1678: train loss: 1.6375583410263062, val loss: 1.6485179662704468\n",
      "Epoch 1679: train loss: 1.63689386844635, val loss: 1.6478370428085327\n",
      "Epoch 1680: train loss: 1.6362296342849731, val loss: 1.6471565961837769\n",
      "Epoch 1681: train loss: 1.6355655193328857, val loss: 1.6464756727218628\n",
      "Epoch 1682: train loss: 1.6349016427993774, val loss: 1.6457951068878174\n",
      "Epoch 1683: train loss: 1.6342380046844482, val loss: 1.6451151371002197\n",
      "Epoch 1684: train loss: 1.6335744857788086, val loss: 1.644435167312622\n",
      "Epoch 1685: train loss: 1.632911205291748, val loss: 1.643755316734314\n",
      "Epoch 1686: train loss: 1.6322479248046875, val loss: 1.6430755853652954\n",
      "Epoch 1687: train loss: 1.6315850019454956, val loss: 1.6423962116241455\n",
      "Epoch 1688: train loss: 1.6309221982955933, val loss: 1.6417169570922852\n",
      "Epoch 1689: train loss: 1.63025963306427, val loss: 1.641037940979004\n",
      "Epoch 1690: train loss: 1.6295973062515259, val loss: 1.6403591632843018\n",
      "Epoch 1691: train loss: 1.6289349794387817, val loss: 1.6396801471710205\n",
      "Epoch 1692: train loss: 1.6282726526260376, val loss: 1.639001727104187\n",
      "Epoch 1693: train loss: 1.6276110410690308, val loss: 1.6383236646652222\n",
      "Epoch 1694: train loss: 1.6269493103027344, val loss: 1.6376453638076782\n",
      "Epoch 1695: train loss: 1.6262876987457275, val loss: 1.6369673013687134\n",
      "Epoch 1696: train loss: 1.6256264448165894, val loss: 1.6362895965576172\n",
      "Epoch 1697: train loss: 1.6249654293060303, val loss: 1.6356121301651\n",
      "Epoch 1698: train loss: 1.6243044137954712, val loss: 1.6349347829818726\n",
      "Epoch 1699: train loss: 1.6236437559127808, val loss: 1.6342575550079346\n",
      "Epoch 1700: train loss: 1.6229832172393799, val loss: 1.6335808038711548\n",
      "Epoch 1701: train loss: 1.622322678565979, val loss: 1.632904052734375\n",
      "Epoch 1702: train loss: 1.6216623783111572, val loss: 1.6322277784347534\n",
      "Epoch 1703: train loss: 1.6210025548934937, val loss: 1.6315513849258423\n",
      "Epoch 1704: train loss: 1.62034273147583, val loss: 1.6308749914169312\n",
      "Epoch 1705: train loss: 1.619683027267456, val loss: 1.6301990747451782\n",
      "Epoch 1706: train loss: 1.6190235614776611, val loss: 1.6295232772827148\n",
      "Epoch 1707: train loss: 1.6183642148971558, val loss: 1.6288479566574097\n",
      "Epoch 1708: train loss: 1.61770498752594, val loss: 1.6281722784042358\n",
      "Epoch 1709: train loss: 1.6170461177825928, val loss: 1.6274970769882202\n",
      "Epoch 1710: train loss: 1.6163874864578247, val loss: 1.6268221139907837\n",
      "Epoch 1711: train loss: 1.6157289743423462, val loss: 1.6261475086212158\n",
      "Epoch 1712: train loss: 1.6150705814361572, val loss: 1.6254726648330688\n",
      "Epoch 1713: train loss: 1.6144124269485474, val loss: 1.6247981786727905\n",
      "Epoch 1714: train loss: 1.6137542724609375, val loss: 1.6241241693496704\n",
      "Epoch 1715: train loss: 1.613096833229065, val loss: 1.6234501600265503\n",
      "Epoch 1716: train loss: 1.6124391555786133, val loss: 1.6227762699127197\n",
      "Epoch 1717: train loss: 1.6117814779281616, val loss: 1.6221023797988892\n",
      "Epoch 1718: train loss: 1.6111243963241577, val loss: 1.6214290857315063\n",
      "Epoch 1719: train loss: 1.6104671955108643, val loss: 1.6207557916641235\n",
      "Epoch 1720: train loss: 1.609810471534729, val loss: 1.6200827360153198\n",
      "Epoch 1721: train loss: 1.6091536283493042, val loss: 1.6194097995758057\n",
      "Epoch 1722: train loss: 1.608497142791748, val loss: 1.6187371015548706\n",
      "Epoch 1723: train loss: 1.607840657234192, val loss: 1.618064522743225\n",
      "Epoch 1724: train loss: 1.6071845293045044, val loss: 1.6173921823501587\n",
      "Epoch 1725: train loss: 1.6065287590026855, val loss: 1.616720199584961\n",
      "Epoch 1726: train loss: 1.6058729887008667, val loss: 1.6160484552383423\n",
      "Epoch 1727: train loss: 1.6052172183990479, val loss: 1.6153764724731445\n",
      "Epoch 1728: train loss: 1.604561686515808, val loss: 1.6147053241729736\n",
      "Epoch 1729: train loss: 1.6039066314697266, val loss: 1.6140339374542236\n",
      "Epoch 1730: train loss: 1.603251576423645, val loss: 1.6133623123168945\n",
      "Epoch 1731: train loss: 1.6025965213775635, val loss: 1.6126915216445923\n",
      "Epoch 1732: train loss: 1.6019419431686401, val loss: 1.6120208501815796\n",
      "Epoch 1733: train loss: 1.6012874841690063, val loss: 1.6113500595092773\n",
      "Epoch 1734: train loss: 1.600633144378662, val loss: 1.6106798648834229\n",
      "Epoch 1735: train loss: 1.599979043006897, val loss: 1.6100095510482788\n",
      "Epoch 1736: train loss: 1.5993249416351318, val loss: 1.6093395948410034\n",
      "Epoch 1737: train loss: 1.5986711978912354, val loss: 1.608669638633728\n",
      "Epoch 1738: train loss: 1.5980178117752075, val loss: 1.6080001592636108\n",
      "Epoch 1739: train loss: 1.5973643064498901, val loss: 1.6073306798934937\n",
      "Epoch 1740: train loss: 1.5967110395431519, val loss: 1.6066614389419556\n",
      "Epoch 1741: train loss: 1.5960580110549927, val loss: 1.6059925556182861\n",
      "Epoch 1742: train loss: 1.595405101776123, val loss: 1.6053234338760376\n",
      "Epoch 1743: train loss: 1.5947524309158325, val loss: 1.6046546697616577\n",
      "Epoch 1744: train loss: 1.594099998474121, val loss: 1.603986382484436\n",
      "Epoch 1745: train loss: 1.5934475660324097, val loss: 1.6033180952072144\n",
      "Epoch 1746: train loss: 1.592795491218567, val loss: 1.6026499271392822\n",
      "Epoch 1747: train loss: 1.5921434164047241, val loss: 1.6019821166992188\n",
      "Epoch 1748: train loss: 1.59149169921875, val loss: 1.6013145446777344\n",
      "Epoch 1749: train loss: 1.5908403396606445, val loss: 1.60064697265625\n",
      "Epoch 1750: train loss: 1.590188980102539, val loss: 1.5999795198440552\n",
      "Epoch 1751: train loss: 1.5895379781723022, val loss: 1.599312424659729\n",
      "Epoch 1752: train loss: 1.5888867378234863, val loss: 1.598645567893982\n",
      "Epoch 1753: train loss: 1.588235855102539, val loss: 1.597978949546814\n",
      "Epoch 1754: train loss: 1.5875850915908813, val loss: 1.597312331199646\n",
      "Epoch 1755: train loss: 1.5869348049163818, val loss: 1.5966460704803467\n",
      "Epoch 1756: train loss: 1.5862843990325928, val loss: 1.5959796905517578\n",
      "Epoch 1757: train loss: 1.5856342315673828, val loss: 1.5953139066696167\n",
      "Epoch 1758: train loss: 1.5849844217300415, val loss: 1.5946482419967651\n",
      "Epoch 1759: train loss: 1.5843347311019897, val loss: 1.5939823389053345\n",
      "Epoch 1760: train loss: 1.583685040473938, val loss: 1.5933170318603516\n",
      "Epoch 1761: train loss: 1.5830357074737549, val loss: 1.5926518440246582\n",
      "Epoch 1762: train loss: 1.5823866128921509, val loss: 1.5919870138168335\n",
      "Epoch 1763: train loss: 1.5817375183105469, val loss: 1.5913219451904297\n",
      "Epoch 1764: train loss: 1.581088662147522, val loss: 1.590657353401184\n",
      "Epoch 1765: train loss: 1.5804399251937866, val loss: 1.589992880821228\n",
      "Epoch 1766: train loss: 1.5797914266586304, val loss: 1.589328646659851\n",
      "Epoch 1767: train loss: 1.5791431665420532, val loss: 1.5886644124984741\n",
      "Epoch 1768: train loss: 1.5784952640533447, val loss: 1.5880006551742554\n",
      "Epoch 1769: train loss: 1.5778471231460571, val loss: 1.5873370170593262\n",
      "Epoch 1770: train loss: 1.5771993398666382, val loss: 1.586673617362976\n",
      "Epoch 1771: train loss: 1.576551914215088, val loss: 1.5860103368759155\n",
      "Epoch 1772: train loss: 1.5759044885635376, val loss: 1.5853471755981445\n",
      "Epoch 1773: train loss: 1.5752571821212769, val loss: 1.5846842527389526\n",
      "Epoch 1774: train loss: 1.5746104717254639, val loss: 1.5840214490890503\n",
      "Epoch 1775: train loss: 1.5739634037017822, val loss: 1.5833590030670166\n",
      "Epoch 1776: train loss: 1.5733168125152588, val loss: 1.582696557044983\n",
      "Epoch 1777: train loss: 1.572670340538025, val loss: 1.5820344686508179\n",
      "Epoch 1778: train loss: 1.5720241069793701, val loss: 1.581372618675232\n",
      "Epoch 1779: train loss: 1.5713779926300049, val loss: 1.5807106494903564\n",
      "Epoch 1780: train loss: 1.5707319974899292, val loss: 1.5800491571426392\n",
      "Epoch 1781: train loss: 1.5700863599777222, val loss: 1.5793876647949219\n",
      "Epoch 1782: train loss: 1.5694407224655151, val loss: 1.5787266492843628\n",
      "Epoch 1783: train loss: 1.5687953233718872, val loss: 1.5780657529830933\n",
      "Epoch 1784: train loss: 1.5681504011154175, val loss: 1.5774048566818237\n",
      "Epoch 1785: train loss: 1.5675052404403687, val loss: 1.5767443180084229\n",
      "Epoch 1786: train loss: 1.5668604373931885, val loss: 1.5760835409164429\n",
      "Epoch 1787: train loss: 1.5662157535552979, val loss: 1.5754234790802002\n",
      "Epoch 1788: train loss: 1.5655710697174072, val loss: 1.574763298034668\n",
      "Epoch 1789: train loss: 1.5649268627166748, val loss: 1.5741033554077148\n",
      "Epoch 1790: train loss: 1.5642828941345215, val loss: 1.5734437704086304\n",
      "Epoch 1791: train loss: 1.5636388063430786, val loss: 1.5727843046188354\n",
      "Epoch 1792: train loss: 1.5629950761795044, val loss: 1.57212495803833\n",
      "Epoch 1793: train loss: 1.5623517036437988, val loss: 1.5714658498764038\n",
      "Epoch 1794: train loss: 1.5617083311080933, val loss: 1.5708069801330566\n",
      "Epoch 1795: train loss: 1.5610650777816772, val loss: 1.5701483488082886\n",
      "Epoch 1796: train loss: 1.5604219436645508, val loss: 1.56948983669281\n",
      "Epoch 1797: train loss: 1.559779167175293, val loss: 1.5688315629959106\n",
      "Epoch 1798: train loss: 1.5591365098953247, val loss: 1.5681732892990112\n",
      "Epoch 1799: train loss: 1.558494210243225, val loss: 1.5675153732299805\n",
      "Epoch 1800: train loss: 1.5578519105911255, val loss: 1.5668576955795288\n",
      "Epoch 1801: train loss: 1.557209849357605, val loss: 1.5662001371383667\n",
      "Epoch 1802: train loss: 1.5565677881240845, val loss: 1.5655425786972046\n",
      "Epoch 1803: train loss: 1.5559260845184326, val loss: 1.5648854970932007\n",
      "Epoch 1804: train loss: 1.5552845001220703, val loss: 1.5642284154891968\n",
      "Epoch 1805: train loss: 1.554643154144287, val loss: 1.563571572303772\n",
      "Epoch 1806: train loss: 1.5540019273757935, val loss: 1.5629150867462158\n",
      "Epoch 1807: train loss: 1.5533608198165894, val loss: 1.5622586011886597\n",
      "Epoch 1808: train loss: 1.5527199506759644, val loss: 1.5616023540496826\n",
      "Epoch 1809: train loss: 1.552079439163208, val loss: 1.5609461069107056\n",
      "Epoch 1810: train loss: 1.551438808441162, val loss: 1.5602903366088867\n",
      "Epoch 1811: train loss: 1.5507985353469849, val loss: 1.5596345663070679\n",
      "Epoch 1812: train loss: 1.5501583814620972, val loss: 1.5589792728424072\n",
      "Epoch 1813: train loss: 1.5495185852050781, val loss: 1.5583237409591675\n",
      "Epoch 1814: train loss: 1.5488786697387695, val loss: 1.557668685913086\n",
      "Epoch 1815: train loss: 1.5482392311096191, val loss: 1.557013750076294\n",
      "Epoch 1816: train loss: 1.5475997924804688, val loss: 1.5563589334487915\n",
      "Epoch 1817: train loss: 1.546960711479187, val loss: 1.5557044744491577\n",
      "Epoch 1818: train loss: 1.5463216304779053, val loss: 1.5550501346588135\n",
      "Epoch 1819: train loss: 1.545682668685913, val loss: 1.5543956756591797\n",
      "Epoch 1820: train loss: 1.5450439453125, val loss: 1.5537419319152832\n",
      "Epoch 1821: train loss: 1.544405460357666, val loss: 1.5530879497528076\n",
      "Epoch 1822: train loss: 1.5437672138214111, val loss: 1.5524344444274902\n",
      "Epoch 1823: train loss: 1.5431289672851562, val loss: 1.5517809391021729\n",
      "Epoch 1824: train loss: 1.5424911975860596, val loss: 1.5511277914047241\n",
      "Epoch 1825: train loss: 1.541853427886963, val loss: 1.5504746437072754\n",
      "Epoch 1826: train loss: 1.5412157773971558, val loss: 1.5498218536376953\n",
      "Epoch 1827: train loss: 1.5405782461166382, val loss: 1.5491694211959839\n",
      "Epoch 1828: train loss: 1.5399410724639893, val loss: 1.5485167503356934\n",
      "Epoch 1829: train loss: 1.5393040180206299, val loss: 1.5478644371032715\n",
      "Epoch 1830: train loss: 1.5386672019958496, val loss: 1.5472123622894287\n",
      "Epoch 1831: train loss: 1.5380303859710693, val loss: 1.546560525894165\n",
      "Epoch 1832: train loss: 1.5373939275741577, val loss: 1.545908808708191\n",
      "Epoch 1833: train loss: 1.536757469177246, val loss: 1.545257329940796\n",
      "Epoch 1834: train loss: 1.5361214876174927, val loss: 1.54460608959198\n",
      "Epoch 1835: train loss: 1.5354855060577393, val loss: 1.543954849243164\n",
      "Epoch 1836: train loss: 1.5348496437072754, val loss: 1.5433039665222168\n",
      "Epoch 1837: train loss: 1.5342140197753906, val loss: 1.542653203010559\n",
      "Epoch 1838: train loss: 1.5335787534713745, val loss: 1.542002558708191\n",
      "Epoch 1839: train loss: 1.5329433679580688, val loss: 1.5413522720336914\n",
      "Epoch 1840: train loss: 1.5323083400726318, val loss: 1.540701985359192\n",
      "Epoch 1841: train loss: 1.531673550605774, val loss: 1.5400519371032715\n",
      "Epoch 1842: train loss: 1.531038761138916, val loss: 1.5394020080566406\n",
      "Epoch 1843: train loss: 1.5304042100906372, val loss: 1.5387524366378784\n",
      "Epoch 1844: train loss: 1.5297698974609375, val loss: 1.5381031036376953\n",
      "Epoch 1845: train loss: 1.529135823249817, val loss: 1.5374537706375122\n",
      "Epoch 1846: train loss: 1.5285017490386963, val loss: 1.5368046760559082\n",
      "Epoch 1847: train loss: 1.5278680324554443, val loss: 1.5361558198928833\n",
      "Epoch 1848: train loss: 1.5272343158721924, val loss: 1.535507082939148\n",
      "Epoch 1849: train loss: 1.5266008377075195, val loss: 1.5348585844039917\n",
      "Epoch 1850: train loss: 1.5259675979614258, val loss: 1.5342100858688354\n",
      "Epoch 1851: train loss: 1.5253345966339111, val loss: 1.5335620641708374\n",
      "Epoch 1852: train loss: 1.5247015953063965, val loss: 1.532914161682129\n",
      "Epoch 1853: train loss: 1.5240689516067505, val loss: 1.5322664976119995\n",
      "Epoch 1854: train loss: 1.523436188697815, val loss: 1.5316187143325806\n",
      "Epoch 1855: train loss: 1.5228039026260376, val loss: 1.5309715270996094\n",
      "Epoch 1856: train loss: 1.5221717357635498, val loss: 1.5303243398666382\n",
      "Epoch 1857: train loss: 1.5215396881103516, val loss: 1.5296775102615356\n",
      "Epoch 1858: train loss: 1.520907998085022, val loss: 1.529030442237854\n",
      "Epoch 1859: train loss: 1.5202760696411133, val loss: 1.5283840894699097\n",
      "Epoch 1860: train loss: 1.5196447372436523, val loss: 1.5277374982833862\n",
      "Epoch 1861: train loss: 1.519013524055481, val loss: 1.5270912647247314\n",
      "Epoch 1862: train loss: 1.5183824300765991, val loss: 1.5264451503753662\n",
      "Epoch 1863: train loss: 1.5177514553070068, val loss: 1.52579927444458\n",
      "Epoch 1864: train loss: 1.517120599746704, val loss: 1.525153636932373\n",
      "Epoch 1865: train loss: 1.5164899826049805, val loss: 1.524507999420166\n",
      "Epoch 1866: train loss: 1.5158594846725464, val loss: 1.5238628387451172\n",
      "Epoch 1867: train loss: 1.515229344367981, val loss: 1.5232175588607788\n",
      "Epoch 1868: train loss: 1.514599323272705, val loss: 1.5225727558135986\n",
      "Epoch 1869: train loss: 1.5139694213867188, val loss: 1.521928071975708\n",
      "Epoch 1870: train loss: 1.5133399963378906, val loss: 1.521283507347107\n",
      "Epoch 1871: train loss: 1.5127102136611938, val loss: 1.5206390619277954\n",
      "Epoch 1872: train loss: 1.5120809078216553, val loss: 1.5199949741363525\n",
      "Epoch 1873: train loss: 1.5114517211914062, val loss: 1.5193508863449097\n",
      "Epoch 1874: train loss: 1.5108227729797363, val loss: 1.5187071561813354\n",
      "Epoch 1875: train loss: 1.5101940631866455, val loss: 1.5180634260177612\n",
      "Epoch 1876: train loss: 1.5095652341842651, val loss: 1.5174200534820557\n",
      "Epoch 1877: train loss: 1.508936882019043, val loss: 1.5167769193649292\n",
      "Epoch 1878: train loss: 1.5083086490631104, val loss: 1.5161336660385132\n",
      "Epoch 1879: train loss: 1.5076806545257568, val loss: 1.5154911279678345\n",
      "Epoch 1880: train loss: 1.5070527791976929, val loss: 1.514848232269287\n",
      "Epoch 1881: train loss: 1.5064250230789185, val loss: 1.514205813407898\n",
      "Epoch 1882: train loss: 1.5057975053787231, val loss: 1.5135635137557983\n",
      "Epoch 1883: train loss: 1.5051701068878174, val loss: 1.5129213333129883\n",
      "Epoch 1884: train loss: 1.5045429468154907, val loss: 1.5122793912887573\n",
      "Epoch 1885: train loss: 1.503915786743164, val loss: 1.5116376876831055\n",
      "Epoch 1886: train loss: 1.503288984298706, val loss: 1.5109962224960327\n",
      "Epoch 1887: train loss: 1.5026625394821167, val loss: 1.5103548765182495\n",
      "Epoch 1888: train loss: 1.5020358562469482, val loss: 1.5097135305404663\n",
      "Epoch 1889: train loss: 1.501409649848938, val loss: 1.5090726613998413\n",
      "Epoch 1890: train loss: 1.5007834434509277, val loss: 1.5084317922592163\n",
      "Epoch 1891: train loss: 1.5001575946807861, val loss: 1.5077911615371704\n",
      "Epoch 1892: train loss: 1.4995317459106445, val loss: 1.507150650024414\n",
      "Epoch 1893: train loss: 1.498906135559082, val loss: 1.506510615348816\n",
      "Epoch 1894: train loss: 1.4982807636260986, val loss: 1.5058703422546387\n",
      "Epoch 1895: train loss: 1.4976556301116943, val loss: 1.5052305459976196\n",
      "Epoch 1896: train loss: 1.49703049659729, val loss: 1.5045908689498901\n",
      "Epoch 1897: train loss: 1.4964057207107544, val loss: 1.5039514303207397\n",
      "Epoch 1898: train loss: 1.4957809448242188, val loss: 1.5033119916915894\n",
      "Epoch 1899: train loss: 1.4951565265655518, val loss: 1.5026730298995972\n",
      "Epoch 1900: train loss: 1.4945323467254639, val loss: 1.502034068107605\n",
      "Epoch 1901: train loss: 1.4939080476760864, val loss: 1.5013952255249023\n",
      "Epoch 1902: train loss: 1.493283987045288, val loss: 1.5007566213607788\n",
      "Epoch 1903: train loss: 1.4926601648330688, val loss: 1.500118374824524\n",
      "Epoch 1904: train loss: 1.4920367002487183, val loss: 1.4994802474975586\n",
      "Epoch 1905: train loss: 1.4914131164550781, val loss: 1.4988422393798828\n",
      "Epoch 1906: train loss: 1.4907898902893066, val loss: 1.498204231262207\n",
      "Epoch 1907: train loss: 1.4901667833328247, val loss: 1.497566819190979\n",
      "Epoch 1908: train loss: 1.4895439147949219, val loss: 1.4969290494918823\n",
      "Epoch 1909: train loss: 1.4889211654663086, val loss: 1.496291995048523\n",
      "Epoch 1910: train loss: 1.4882985353469849, val loss: 1.4956550598144531\n",
      "Epoch 1911: train loss: 1.4876761436462402, val loss: 1.4950181245803833\n",
      "Epoch 1912: train loss: 1.4870539903640747, val loss: 1.4943814277648926\n",
      "Epoch 1913: train loss: 1.4864319562911987, val loss: 1.4937448501586914\n",
      "Epoch 1914: train loss: 1.4858100414276123, val loss: 1.4931085109710693\n",
      "Epoch 1915: train loss: 1.4851884841918945, val loss: 1.4924722909927368\n",
      "Epoch 1916: train loss: 1.4845669269561768, val loss: 1.491836428642273\n",
      "Epoch 1917: train loss: 1.483945369720459, val loss: 1.4912006855010986\n",
      "Epoch 1918: train loss: 1.483324408531189, val loss: 1.4905650615692139\n",
      "Epoch 1919: train loss: 1.4827033281326294, val loss: 1.4899297952651978\n",
      "Epoch 1920: train loss: 1.4820828437805176, val loss: 1.489294409751892\n",
      "Epoch 1921: train loss: 1.4814621210098267, val loss: 1.4886596202850342\n",
      "Epoch 1922: train loss: 1.4808417558670044, val loss: 1.4880245923995972\n",
      "Epoch 1923: train loss: 1.4802212715148926, val loss: 1.487390160560608\n",
      "Epoch 1924: train loss: 1.4796013832092285, val loss: 1.4867554903030396\n",
      "Epoch 1925: train loss: 1.478981614112854, val loss: 1.4861212968826294\n",
      "Epoch 1926: train loss: 1.47836172580719, val loss: 1.4854872226715088\n",
      "Epoch 1927: train loss: 1.4777421951293945, val loss: 1.4848533868789673\n",
      "Epoch 1928: train loss: 1.4771229028701782, val loss: 1.4842195510864258\n",
      "Epoch 1929: train loss: 1.4765037298202515, val loss: 1.4835861921310425\n",
      "Epoch 1930: train loss: 1.4758846759796143, val loss: 1.4829529523849487\n",
      "Epoch 1931: train loss: 1.4752658605575562, val loss: 1.482319712638855\n",
      "Epoch 1932: train loss: 1.4746471643447876, val loss: 1.4816865921020508\n",
      "Epoch 1933: train loss: 1.4740287065505981, val loss: 1.4810539484024048\n",
      "Epoch 1934: train loss: 1.4734103679656982, val loss: 1.4804213047027588\n",
      "Epoch 1935: train loss: 1.4727922677993774, val loss: 1.4797887802124023\n",
      "Epoch 1936: train loss: 1.4721742868423462, val loss: 1.479156732559204\n",
      "Epoch 1937: train loss: 1.4715564250946045, val loss: 1.4785246849060059\n",
      "Epoch 1938: train loss: 1.470938801765442, val loss: 1.4778926372528076\n",
      "Epoch 1939: train loss: 1.4703212976455688, val loss: 1.4772611856460571\n",
      "Epoch 1940: train loss: 1.469704270362854, val loss: 1.476629614830017\n",
      "Epoch 1941: train loss: 1.4690872430801392, val loss: 1.4759985208511353\n",
      "Epoch 1942: train loss: 1.4684703350067139, val loss: 1.4753671884536743\n",
      "Epoch 1943: train loss: 1.4678535461425781, val loss: 1.4747364521026611\n",
      "Epoch 1944: train loss: 1.4672369956970215, val loss: 1.4741054773330688\n",
      "Epoch 1945: train loss: 1.4666208028793335, val loss: 1.4734748601913452\n",
      "Epoch 1946: train loss: 1.466004490852356, val loss: 1.4728446006774902\n",
      "Epoch 1947: train loss: 1.465388536453247, val loss: 1.4722145795822144\n",
      "Epoch 1948: train loss: 1.4647727012634277, val loss: 1.4715843200683594\n",
      "Epoch 1949: train loss: 1.464156985282898, val loss: 1.4709545373916626\n",
      "Epoch 1950: train loss: 1.4635413885116577, val loss: 1.4703251123428345\n",
      "Epoch 1951: train loss: 1.4629262685775757, val loss: 1.4696954488754272\n",
      "Epoch 1952: train loss: 1.4623111486434937, val loss: 1.4690661430358887\n",
      "Epoch 1953: train loss: 1.4616960287094116, val loss: 1.4684370756149292\n",
      "Epoch 1954: train loss: 1.4610812664031982, val loss: 1.4678083658218384\n",
      "Epoch 1955: train loss: 1.4604668617248535, val loss: 1.467179536819458\n",
      "Epoch 1956: train loss: 1.4598523378372192, val loss: 1.4665510654449463\n",
      "Epoch 1957: train loss: 1.4592381715774536, val loss: 1.4659227132797241\n",
      "Epoch 1958: train loss: 1.458624005317688, val loss: 1.4652947187423706\n",
      "Epoch 1959: train loss: 1.458010196685791, val loss: 1.464666724205017\n",
      "Epoch 1960: train loss: 1.4573962688446045, val loss: 1.4640387296676636\n",
      "Epoch 1961: train loss: 1.4567826986312866, val loss: 1.4634112119674683\n",
      "Epoch 1962: train loss: 1.4561693668365479, val loss: 1.462783694267273\n",
      "Epoch 1963: train loss: 1.4555561542510986, val loss: 1.4621565341949463\n",
      "Epoch 1964: train loss: 1.4549431800842285, val loss: 1.4615296125411987\n",
      "Epoch 1965: train loss: 1.4543304443359375, val loss: 1.4609025716781616\n",
      "Epoch 1966: train loss: 1.4537177085876465, val loss: 1.4602760076522827\n",
      "Epoch 1967: train loss: 1.4531052112579346, val loss: 1.4596494436264038\n",
      "Epoch 1968: train loss: 1.4524929523468018, val loss: 1.459023356437683\n",
      "Epoch 1969: train loss: 1.4518808126449585, val loss: 1.4583971500396729\n",
      "Epoch 1970: train loss: 1.4512686729431152, val loss: 1.4577711820602417\n",
      "Epoch 1971: train loss: 1.4506570100784302, val loss: 1.4571455717086792\n",
      "Epoch 1972: train loss: 1.4500454664230347, val loss: 1.4565199613571167\n",
      "Epoch 1973: train loss: 1.4494338035583496, val loss: 1.4558944702148438\n",
      "Epoch 1974: train loss: 1.4488227367401123, val loss: 1.4552693367004395\n",
      "Epoch 1975: train loss: 1.448211669921875, val loss: 1.4546443223953247\n",
      "Epoch 1976: train loss: 1.4476007223129272, val loss: 1.4540194272994995\n",
      "Epoch 1977: train loss: 1.4469900131225586, val loss: 1.453394889831543\n",
      "Epoch 1978: train loss: 1.4463794231414795, val loss: 1.452770471572876\n",
      "Epoch 1979: train loss: 1.4457690715789795, val loss: 1.4521461725234985\n",
      "Epoch 1980: train loss: 1.4451589584350586, val loss: 1.4515219926834106\n",
      "Epoch 1981: train loss: 1.4445489645004272, val loss: 1.4508981704711914\n",
      "Epoch 1982: train loss: 1.4439390897750854, val loss: 1.4502743482589722\n",
      "Epoch 1983: train loss: 1.4433292150497437, val loss: 1.4496510028839111\n",
      "Epoch 1984: train loss: 1.4427196979522705, val loss: 1.44902765750885\n",
      "Epoch 1985: train loss: 1.4421104192733765, val loss: 1.448404312133789\n",
      "Epoch 1986: train loss: 1.441501259803772, val loss: 1.4477814435958862\n",
      "Epoch 1987: train loss: 1.440892219543457, val loss: 1.4471585750579834\n",
      "Epoch 1988: train loss: 1.4402835369110107, val loss: 1.4465358257293701\n",
      "Epoch 1989: train loss: 1.439674973487854, val loss: 1.445913553237915\n",
      "Epoch 1990: train loss: 1.4390664100646973, val loss: 1.4452913999557495\n",
      "Epoch 1991: train loss: 1.4384580850601196, val loss: 1.4446691274642944\n",
      "Epoch 1992: train loss: 1.437849998474121, val loss: 1.4440473318099976\n",
      "Epoch 1993: train loss: 1.437242031097412, val loss: 1.4434257745742798\n",
      "Epoch 1994: train loss: 1.4366345405578613, val loss: 1.442804217338562\n",
      "Epoch 1995: train loss: 1.436026692390442, val loss: 1.4421828985214233\n",
      "Epoch 1996: train loss: 1.4354194402694702, val loss: 1.4415615797042847\n",
      "Epoch 1997: train loss: 1.434812068939209, val loss: 1.4409408569335938\n",
      "Epoch 1998: train loss: 1.434205174446106, val loss: 1.4403200149536133\n",
      "Epoch 1999: train loss: 1.4335981607437134, val loss: 1.439699411392212\n",
      "Epoch 2000: train loss: 1.432991623878479, val loss: 1.4390789270401\n",
      "Epoch 2001: train loss: 1.432384967803955, val loss: 1.4384586811065674\n",
      "Epoch 2002: train loss: 1.4317786693572998, val loss: 1.4378387928009033\n",
      "Epoch 2003: train loss: 1.4311726093292236, val loss: 1.4372189044952393\n",
      "Epoch 2004: train loss: 1.430566430091858, val loss: 1.4365993738174438\n",
      "Epoch 2005: train loss: 1.4299606084823608, val loss: 1.4359798431396484\n",
      "Epoch 2006: train loss: 1.4293549060821533, val loss: 1.435360312461853\n",
      "Epoch 2007: train loss: 1.428749442100525, val loss: 1.4347413778305054\n",
      "Epoch 2008: train loss: 1.4281442165374756, val loss: 1.4341224431991577\n",
      "Epoch 2009: train loss: 1.4275391101837158, val loss: 1.4335037469863892\n",
      "Epoch 2010: train loss: 1.426934003829956, val loss: 1.4328851699829102\n",
      "Epoch 2011: train loss: 1.426329255104065, val loss: 1.4322667121887207\n",
      "Epoch 2012: train loss: 1.4257246255874634, val loss: 1.4316484928131104\n",
      "Epoch 2013: train loss: 1.4251201152801514, val loss: 1.431030511856079\n",
      "Epoch 2014: train loss: 1.424516201019287, val loss: 1.4304126501083374\n",
      "Epoch 2015: train loss: 1.4239119291305542, val loss: 1.4297951459884644\n",
      "Epoch 2016: train loss: 1.42330801486969, val loss: 1.4291776418685913\n",
      "Epoch 2017: train loss: 1.4227042198181152, val loss: 1.428560495376587\n",
      "Epoch 2018: train loss: 1.4221006631851196, val loss: 1.427943229675293\n",
      "Epoch 2019: train loss: 1.4214974641799927, val loss: 1.4273262023925781\n",
      "Epoch 2020: train loss: 1.4208940267562866, val loss: 1.4267096519470215\n",
      "Epoch 2021: train loss: 1.4202910661697388, val loss: 1.4260931015014648\n",
      "Epoch 2022: train loss: 1.4196882247924805, val loss: 1.4254766702651978\n",
      "Epoch 2023: train loss: 1.4190856218338013, val loss: 1.4248605966567993\n",
      "Epoch 2024: train loss: 1.418483018875122, val loss: 1.4242445230484009\n",
      "Epoch 2025: train loss: 1.417880654335022, val loss: 1.423628807067871\n",
      "Epoch 2026: train loss: 1.417278528213501, val loss: 1.4230129718780518\n",
      "Epoch 2027: train loss: 1.41667640209198, val loss: 1.4223977327346802\n",
      "Epoch 2028: train loss: 1.4160747528076172, val loss: 1.421782374382019\n",
      "Epoch 2029: train loss: 1.4154729843139648, val loss: 1.421167254447937\n",
      "Epoch 2030: train loss: 1.4148714542388916, val loss: 1.4205524921417236\n",
      "Epoch 2031: train loss: 1.414270281791687, val loss: 1.4199374914169312\n",
      "Epoch 2032: train loss: 1.413669228553772, val loss: 1.419323205947876\n",
      "Epoch 2033: train loss: 1.413068175315857, val loss: 1.4187086820602417\n",
      "Epoch 2034: train loss: 1.412467360496521, val loss: 1.4180946350097656\n",
      "Epoch 2035: train loss: 1.4118669033050537, val loss: 1.417480707168579\n",
      "Epoch 2036: train loss: 1.4112664461135864, val loss: 1.4168667793273926\n",
      "Epoch 2037: train loss: 1.4106661081314087, val loss: 1.4162530899047852\n",
      "Epoch 2038: train loss: 1.4100661277770996, val loss: 1.4156396389007568\n",
      "Epoch 2039: train loss: 1.4094661474227905, val loss: 1.4150265455245972\n",
      "Epoch 2040: train loss: 1.408866286277771, val loss: 1.414413332939148\n",
      "Epoch 2041: train loss: 1.4082667827606201, val loss: 1.413800597190857\n",
      "Epoch 2042: train loss: 1.4076673984527588, val loss: 1.4131876230239868\n",
      "Epoch 2043: train loss: 1.407068133354187, val loss: 1.412575125694275\n",
      "Epoch 2044: train loss: 1.4064691066741943, val loss: 1.411962866783142\n",
      "Epoch 2045: train loss: 1.4058703184127808, val loss: 1.4113507270812988\n",
      "Epoch 2046: train loss: 1.4052716493606567, val loss: 1.4107387065887451\n",
      "Epoch 2047: train loss: 1.4046730995178223, val loss: 1.4101271629333496\n",
      "Epoch 2048: train loss: 1.4040746688842773, val loss: 1.4095155000686646\n",
      "Epoch 2049: train loss: 1.4034767150878906, val loss: 1.408903956413269\n",
      "Epoch 2050: train loss: 1.4028785228729248, val loss: 1.4082927703857422\n",
      "Epoch 2051: train loss: 1.4022806882858276, val loss: 1.4076817035675049\n",
      "Epoch 2052: train loss: 1.40168297290802, val loss: 1.4070708751678467\n",
      "Epoch 2053: train loss: 1.4010854959487915, val loss: 1.4064600467681885\n",
      "Epoch 2054: train loss: 1.400488257408142, val loss: 1.4058496952056885\n",
      "Epoch 2055: train loss: 1.3998910188674927, val loss: 1.4052391052246094\n",
      "Epoch 2056: train loss: 1.3992942571640015, val loss: 1.4046289920806885\n",
      "Epoch 2057: train loss: 1.3986973762512207, val loss: 1.4040191173553467\n",
      "Epoch 2058: train loss: 1.3981008529663086, val loss: 1.4034093618392944\n",
      "Epoch 2059: train loss: 1.397504448890686, val loss: 1.4027998447418213\n",
      "Epoch 2060: train loss: 1.3969080448150635, val loss: 1.4021902084350586\n",
      "Epoch 2061: train loss: 1.3963119983673096, val loss: 1.401581048965454\n",
      "Epoch 2062: train loss: 1.3957161903381348, val loss: 1.4009720087051392\n",
      "Epoch 2063: train loss: 1.3951202630996704, val loss: 1.4003629684448242\n",
      "Epoch 2064: train loss: 1.3945246934890747, val loss: 1.3997544050216675\n",
      "Epoch 2065: train loss: 1.393929362297058, val loss: 1.3991459608078003\n",
      "Epoch 2066: train loss: 1.393334150314331, val loss: 1.398537516593933\n",
      "Epoch 2067: train loss: 1.3927390575408936, val loss: 1.3979294300079346\n",
      "Epoch 2068: train loss: 1.3921440839767456, val loss: 1.3973214626312256\n",
      "Epoch 2069: train loss: 1.3915493488311768, val loss: 1.3967134952545166\n",
      "Epoch 2070: train loss: 1.3909549713134766, val loss: 1.3961060047149658\n",
      "Epoch 2071: train loss: 1.3903604745864868, val loss: 1.3954986333847046\n",
      "Epoch 2072: train loss: 1.3897663354873657, val loss: 1.394891381263733\n",
      "Epoch 2073: train loss: 1.3891723155975342, val loss: 1.3942843675613403\n",
      "Epoch 2074: train loss: 1.3885782957077026, val loss: 1.3936775922775269\n",
      "Epoch 2075: train loss: 1.3879847526550293, val loss: 1.393070936203003\n",
      "Epoch 2076: train loss: 1.3873913288116455, val loss: 1.392464280128479\n",
      "Epoch 2077: train loss: 1.3867980241775513, val loss: 1.3918579816818237\n",
      "Epoch 2078: train loss: 1.3862048387527466, val loss: 1.391251802444458\n",
      "Epoch 2079: train loss: 1.385611891746521, val loss: 1.3906458616256714\n",
      "Epoch 2080: train loss: 1.3850189447402954, val loss: 1.3900399208068848\n",
      "Epoch 2081: train loss: 1.3844263553619385, val loss: 1.3894344568252563\n",
      "Epoch 2082: train loss: 1.383833885192871, val loss: 1.3888291120529175\n",
      "Epoch 2083: train loss: 1.3832416534423828, val loss: 1.3882237672805786\n",
      "Epoch 2084: train loss: 1.3826494216918945, val loss: 1.3876186609268188\n",
      "Epoch 2085: train loss: 1.3820574283599854, val loss: 1.3870137929916382\n",
      "Epoch 2086: train loss: 1.3814656734466553, val loss: 1.386409044265747\n",
      "Epoch 2087: train loss: 1.3808740377426147, val loss: 1.385804533958435\n",
      "Epoch 2088: train loss: 1.3802826404571533, val loss: 1.3852002620697021\n",
      "Epoch 2089: train loss: 1.379691481590271, val loss: 1.3845959901809692\n",
      "Epoch 2090: train loss: 1.3791002035140991, val loss: 1.383992075920105\n",
      "Epoch 2091: train loss: 1.378509283065796, val loss: 1.3833884000778198\n",
      "Epoch 2092: train loss: 1.3779187202453613, val loss: 1.3827847242355347\n",
      "Epoch 2093: train loss: 1.3773281574249268, val loss: 1.3821814060211182\n",
      "Epoch 2094: train loss: 1.3767377138137817, val loss: 1.381577968597412\n",
      "Epoch 2095: train loss: 1.3761472702026367, val loss: 1.3809750080108643\n",
      "Epoch 2096: train loss: 1.37555730342865, val loss: 1.3803719282150269\n",
      "Epoch 2097: train loss: 1.3749674558639526, val loss: 1.3797693252563477\n",
      "Epoch 2098: train loss: 1.3743776082992554, val loss: 1.379166841506958\n",
      "Epoch 2099: train loss: 1.3737881183624268, val loss: 1.3785645961761475\n",
      "Epoch 2100: train loss: 1.3731988668441772, val loss: 1.377962350845337\n",
      "Epoch 2101: train loss: 1.3726096153259277, val loss: 1.3773603439331055\n",
      "Epoch 2102: train loss: 1.3720206022262573, val loss: 1.3767585754394531\n",
      "Epoch 2103: train loss: 1.3714317083358765, val loss: 1.3761570453643799\n",
      "Epoch 2104: train loss: 1.3708430528640747, val loss: 1.3755552768707275\n",
      "Epoch 2105: train loss: 1.3702545166015625, val loss: 1.374954104423523\n",
      "Epoch 2106: train loss: 1.3696660995483398, val loss: 1.3743531703948975\n",
      "Epoch 2107: train loss: 1.3690779209136963, val loss: 1.373752236366272\n",
      "Epoch 2108: train loss: 1.3684899806976318, val loss: 1.3731515407562256\n",
      "Epoch 2109: train loss: 1.367902159690857, val loss: 1.3725510835647583\n",
      "Epoch 2110: train loss: 1.3673145771026611, val loss: 1.3719505071640015\n",
      "Epoch 2111: train loss: 1.3667271137237549, val loss: 1.3713505268096924\n",
      "Epoch 2112: train loss: 1.3661396503448486, val loss: 1.3707503080368042\n",
      "Epoch 2113: train loss: 1.3655524253845215, val loss: 1.3701505661010742\n",
      "Epoch 2114: train loss: 1.364965558052063, val loss: 1.3695508241653442\n",
      "Epoch 2115: train loss: 1.3643786907196045, val loss: 1.368951439857483\n",
      "Epoch 2116: train loss: 1.363792061805725, val loss: 1.3683522939682007\n",
      "Epoch 2117: train loss: 1.3632056713104248, val loss: 1.3677531480789185\n",
      "Epoch 2118: train loss: 1.362619400024414, val loss: 1.3671541213989258\n",
      "Epoch 2119: train loss: 1.3620332479476929, val loss: 1.3665555715560913\n",
      "Epoch 2120: train loss: 1.3614473342895508, val loss: 1.3659567832946777\n",
      "Epoch 2121: train loss: 1.3608615398406982, val loss: 1.365358591079712\n",
      "Epoch 2122: train loss: 1.3602758646011353, val loss: 1.3647602796554565\n",
      "Epoch 2123: train loss: 1.3596904277801514, val loss: 1.3641623258590698\n",
      "Epoch 2124: train loss: 1.3591053485870361, val loss: 1.363564372062683\n",
      "Epoch 2125: train loss: 1.3585201501846313, val loss: 1.3629668951034546\n",
      "Epoch 2126: train loss: 1.3579353094100952, val loss: 1.362369418144226\n",
      "Epoch 2127: train loss: 1.3573505878448486, val loss: 1.361772060394287\n",
      "Epoch 2128: train loss: 1.3567659854888916, val loss: 1.3611749410629272\n",
      "Epoch 2129: train loss: 1.3561815023422241, val loss: 1.3605780601501465\n",
      "Epoch 2130: train loss: 1.3555972576141357, val loss: 1.3599814176559448\n",
      "Epoch 2131: train loss: 1.3550132513046265, val loss: 1.3593848943710327\n",
      "Epoch 2132: train loss: 1.3544293642044067, val loss: 1.3587884902954102\n",
      "Epoch 2133: train loss: 1.3538455963134766, val loss: 1.3581922054290771\n",
      "Epoch 2134: train loss: 1.3532620668411255, val loss: 1.3575962781906128\n",
      "Epoch 2135: train loss: 1.352678656578064, val loss: 1.3570002317428589\n",
      "Epoch 2136: train loss: 1.3520954847335815, val loss: 1.3564046621322632\n",
      "Epoch 2137: train loss: 1.3515123128890991, val loss: 1.3558090925216675\n",
      "Epoch 2138: train loss: 1.3509294986724854, val loss: 1.3552137613296509\n",
      "Epoch 2139: train loss: 1.3503468036651611, val loss: 1.3546186685562134\n",
      "Epoch 2140: train loss: 1.349764347076416, val loss: 1.3540236949920654\n",
      "Epoch 2141: train loss: 1.3491820096969604, val loss: 1.353428840637207\n",
      "Epoch 2142: train loss: 1.3485997915267944, val loss: 1.3528342247009277\n",
      "Epoch 2143: train loss: 1.348017692565918, val loss: 1.3522398471832275\n",
      "Epoch 2144: train loss: 1.3474358320236206, val loss: 1.351645588874817\n",
      "Epoch 2145: train loss: 1.3468542098999023, val loss: 1.3510515689849854\n",
      "Epoch 2146: train loss: 1.3462727069854736, val loss: 1.3504575490951538\n",
      "Epoch 2147: train loss: 1.3456913232803345, val loss: 1.349863886833191\n",
      "Epoch 2148: train loss: 1.3451100587844849, val loss: 1.3492703437805176\n",
      "Epoch 2149: train loss: 1.3445290327072144, val loss: 1.3486770391464233\n",
      "Epoch 2150: train loss: 1.3439483642578125, val loss: 1.3480840921401978\n",
      "Epoch 2151: train loss: 1.3433676958084106, val loss: 1.3474911451339722\n",
      "Epoch 2152: train loss: 1.342787265777588, val loss: 1.3468983173370361\n",
      "Epoch 2153: train loss: 1.3422068357467651, val loss: 1.3463054895401\n",
      "Epoch 2154: train loss: 1.341626763343811, val loss: 1.3457131385803223\n",
      "Epoch 2155: train loss: 1.3410468101501465, val loss: 1.345120906829834\n",
      "Epoch 2156: train loss: 1.3404669761657715, val loss: 1.3445289134979248\n",
      "Epoch 2157: train loss: 1.3398873805999756, val loss: 1.343936800956726\n",
      "Epoch 2158: train loss: 1.3393079042434692, val loss: 1.343345284461975\n",
      "Epoch 2159: train loss: 1.338728666305542, val loss: 1.342753529548645\n",
      "Epoch 2160: train loss: 1.3381494283676147, val loss: 1.3421624898910522\n",
      "Epoch 2161: train loss: 1.3375704288482666, val loss: 1.3415712118148804\n",
      "Epoch 2162: train loss: 1.336991786956787, val loss: 1.3409799337387085\n",
      "Epoch 2163: train loss: 1.336413025856018, val loss: 1.3403892517089844\n",
      "Epoch 2164: train loss: 1.3358346223831177, val loss: 1.3397985696792603\n",
      "Epoch 2165: train loss: 1.3352563381195068, val loss: 1.3392081260681152\n",
      "Epoch 2166: train loss: 1.334678292274475, val loss: 1.3386178016662598\n",
      "Epoch 2167: train loss: 1.3341004848480225, val loss: 1.338027834892273\n",
      "Epoch 2168: train loss: 1.3335226774215698, val loss: 1.3374378681182861\n",
      "Epoch 2169: train loss: 1.3329451084136963, val loss: 1.3368481397628784\n",
      "Epoch 2170: train loss: 1.3323675394058228, val loss: 1.3362585306167603\n",
      "Epoch 2171: train loss: 1.3317903280258179, val loss: 1.3356691598892212\n",
      "Epoch 2172: train loss: 1.3312132358551025, val loss: 1.3350799083709717\n",
      "Epoch 2173: train loss: 1.3306362628936768, val loss: 1.3344908952713013\n",
      "Epoch 2174: train loss: 1.3300596475601196, val loss: 1.3339018821716309\n",
      "Epoch 2175: train loss: 1.3294830322265625, val loss: 1.3333133459091187\n",
      "Epoch 2176: train loss: 1.3289066553115845, val loss: 1.332724690437317\n",
      "Epoch 2177: train loss: 1.328330397605896, val loss: 1.3321365118026733\n",
      "Epoch 2178: train loss: 1.327754259109497, val loss: 1.3315483331680298\n",
      "Epoch 2179: train loss: 1.3271784782409668, val loss: 1.3309602737426758\n",
      "Epoch 2180: train loss: 1.326602578163147, val loss: 1.3303725719451904\n",
      "Epoch 2181: train loss: 1.3260271549224854, val loss: 1.3297847509384155\n",
      "Epoch 2182: train loss: 1.3254518508911133, val loss: 1.3291975259780884\n",
      "Epoch 2183: train loss: 1.3248765468597412, val loss: 1.3286101818084717\n",
      "Epoch 2184: train loss: 1.3243013620376587, val loss: 1.3280231952667236\n",
      "Epoch 2185: train loss: 1.3237266540527344, val loss: 1.3274363279342651\n",
      "Epoch 2186: train loss: 1.3231518268585205, val loss: 1.3268495798110962\n",
      "Epoch 2187: train loss: 1.3225772380828857, val loss: 1.3262630701065063\n",
      "Epoch 2188: train loss: 1.32200288772583, val loss: 1.3256767988204956\n",
      "Epoch 2189: train loss: 1.3214287757873535, val loss: 1.3250904083251953\n",
      "Epoch 2190: train loss: 1.320854663848877, val loss: 1.3245044946670532\n",
      "Epoch 2191: train loss: 1.3202807903289795, val loss: 1.3239185810089111\n",
      "Epoch 2192: train loss: 1.3197071552276611, val loss: 1.3233330249786377\n",
      "Epoch 2193: train loss: 1.3191335201263428, val loss: 1.3227475881576538\n",
      "Epoch 2194: train loss: 1.318560242652893, val loss: 1.3221622705459595\n",
      "Epoch 2195: train loss: 1.317987084388733, val loss: 1.3215770721435547\n",
      "Epoch 2196: train loss: 1.3174140453338623, val loss: 1.320992112159729\n",
      "Epoch 2197: train loss: 1.3168411254882812, val loss: 1.3204073905944824\n",
      "Epoch 2198: train loss: 1.3162685632705688, val loss: 1.3198230266571045\n",
      "Epoch 2199: train loss: 1.3156960010528564, val loss: 1.3192384243011475\n",
      "Epoch 2200: train loss: 1.3151235580444336, val loss: 1.3186544179916382\n",
      "Epoch 2201: train loss: 1.3145513534545898, val loss: 1.3180701732635498\n",
      "Epoch 2202: train loss: 1.3139795064926147, val loss: 1.3174861669540405\n",
      "Epoch 2203: train loss: 1.31340754032135, val loss: 1.3169025182724\n",
      "Epoch 2204: train loss: 1.312835931777954, val loss: 1.3163191080093384\n",
      "Epoch 2205: train loss: 1.312264323234558, val loss: 1.3157358169555664\n",
      "Epoch 2206: train loss: 1.3116931915283203, val loss: 1.3151527643203735\n",
      "Epoch 2207: train loss: 1.311121940612793, val loss: 1.3145694732666016\n",
      "Epoch 2208: train loss: 1.3105509281158447, val loss: 1.3139870166778564\n",
      "Epoch 2209: train loss: 1.3099799156188965, val loss: 1.3134040832519531\n",
      "Epoch 2210: train loss: 1.309409260749817, val loss: 1.3128217458724976\n",
      "Epoch 2211: train loss: 1.3088388442993164, val loss: 1.3122395277023315\n",
      "Epoch 2212: train loss: 1.3082685470581055, val loss: 1.3116573095321655\n",
      "Epoch 2213: train loss: 1.3076984882354736, val loss: 1.3110754489898682\n",
      "Epoch 2214: train loss: 1.3071284294128418, val loss: 1.3104937076568604\n",
      "Epoch 2215: train loss: 1.306558609008789, val loss: 1.309912085533142\n",
      "Epoch 2216: train loss: 1.3059889078140259, val loss: 1.309330940246582\n",
      "Epoch 2217: train loss: 1.3054195642471313, val loss: 1.3087496757507324\n",
      "Epoch 2218: train loss: 1.3048501014709473, val loss: 1.308168649673462\n",
      "Epoch 2219: train loss: 1.3042809963226318, val loss: 1.307587742805481\n",
      "Epoch 2220: train loss: 1.3037121295928955, val loss: 1.307007074356079\n",
      "Epoch 2221: train loss: 1.3031431436538696, val loss: 1.3064266443252563\n",
      "Epoch 2222: train loss: 1.3025747537612915, val loss: 1.3058462142944336\n",
      "Epoch 2223: train loss: 1.3020062446594238, val loss: 1.305266261100769\n",
      "Epoch 2224: train loss: 1.3014379739761353, val loss: 1.304686188697815\n",
      "Epoch 2225: train loss: 1.3008695840835571, val loss: 1.3041064739227295\n",
      "Epoch 2226: train loss: 1.3003017902374268, val loss: 1.303526759147644\n",
      "Epoch 2227: train loss: 1.2997338771820068, val loss: 1.3029474020004272\n",
      "Epoch 2228: train loss: 1.299166202545166, val loss: 1.3023680448532104\n",
      "Epoch 2229: train loss: 1.2985988855361938, val loss: 1.3017891645431519\n",
      "Epoch 2230: train loss: 1.2980316877365112, val loss: 1.3012101650238037\n",
      "Epoch 2231: train loss: 1.297464370727539, val loss: 1.3006314039230347\n",
      "Epoch 2232: train loss: 1.2968974113464355, val loss: 1.3000528812408447\n",
      "Epoch 2233: train loss: 1.2963306903839111, val loss: 1.2994745969772339\n",
      "Epoch 2234: train loss: 1.2957639694213867, val loss: 1.2988961935043335\n",
      "Epoch 2235: train loss: 1.2951974868774414, val loss: 1.2983182668685913\n",
      "Epoch 2236: train loss: 1.2946313619613647, val loss: 1.2977403402328491\n",
      "Epoch 2237: train loss: 1.294065237045288, val loss: 1.297162652015686\n",
      "Epoch 2238: train loss: 1.293499231338501, val loss: 1.2965854406356812\n",
      "Epoch 2239: train loss: 1.292933464050293, val loss: 1.2960081100463867\n",
      "Epoch 2240: train loss: 1.2923678159713745, val loss: 1.2954310178756714\n",
      "Epoch 2241: train loss: 1.2918022871017456, val loss: 1.2948540449142456\n",
      "Epoch 2242: train loss: 1.2912371158599854, val loss: 1.2942771911621094\n",
      "Epoch 2243: train loss: 1.290671944618225, val loss: 1.2937005758285522\n",
      "Epoch 2244: train loss: 1.2901071310043335, val loss: 1.2931241989135742\n",
      "Epoch 2245: train loss: 1.289542317390442, val loss: 1.2925480604171753\n",
      "Epoch 2246: train loss: 1.2889776229858398, val loss: 1.291972041130066\n",
      "Epoch 2247: train loss: 1.288413405418396, val loss: 1.2913960218429565\n",
      "Epoch 2248: train loss: 1.2878490686416626, val loss: 1.2908202409744263\n",
      "Epoch 2249: train loss: 1.2872848510742188, val loss: 1.2902448177337646\n",
      "Epoch 2250: train loss: 1.2867209911346436, val loss: 1.289669394493103\n",
      "Epoch 2251: train loss: 1.2861571311950684, val loss: 1.2890942096710205\n",
      "Epoch 2252: train loss: 1.2855935096740723, val loss: 1.2885191440582275\n",
      "Epoch 2253: train loss: 1.2850301265716553, val loss: 1.2879444360733032\n",
      "Epoch 2254: train loss: 1.2844669818878174, val loss: 1.287369728088379\n",
      "Epoch 2255: train loss: 1.2839038372039795, val loss: 1.2867951393127441\n",
      "Epoch 2256: train loss: 1.2833408117294312, val loss: 1.286220908164978\n",
      "Epoch 2257: train loss: 1.2827781438827515, val loss: 1.2856467962265015\n",
      "Epoch 2258: train loss: 1.2822153568267822, val loss: 1.285072922706604\n",
      "Epoch 2259: train loss: 1.2816530466079712, val loss: 1.2844990491867065\n",
      "Epoch 2260: train loss: 1.2810907363891602, val loss: 1.2839256525039673\n",
      "Epoch 2261: train loss: 1.2805286645889282, val loss: 1.2833521366119385\n",
      "Epoch 2262: train loss: 1.2799667119979858, val loss: 1.2827787399291992\n",
      "Epoch 2263: train loss: 1.279404878616333, val loss: 1.2822058200836182\n",
      "Epoch 2264: train loss: 1.2788431644439697, val loss: 1.281632900238037\n",
      "Epoch 2265: train loss: 1.278281807899475, val loss: 1.2810600996017456\n",
      "Epoch 2266: train loss: 1.2777204513549805, val loss: 1.2804874181747437\n",
      "Epoch 2267: train loss: 1.2771594524383545, val loss: 1.2799152135849\n",
      "Epoch 2268: train loss: 1.2765984535217285, val loss: 1.2793430089950562\n",
      "Epoch 2269: train loss: 1.2760378122329712, val loss: 1.2787710428237915\n",
      "Epoch 2270: train loss: 1.2754771709442139, val loss: 1.278199315071106\n",
      "Epoch 2271: train loss: 1.2749167680740356, val loss: 1.2776275873184204\n",
      "Epoch 2272: train loss: 1.274356484413147, val loss: 1.277056097984314\n",
      "Epoch 2273: train loss: 1.2737963199615479, val loss: 1.2764848470687866\n",
      "Epoch 2274: train loss: 1.2732365131378174, val loss: 1.2759135961532593\n",
      "Epoch 2275: train loss: 1.2726768255233765, val loss: 1.2753427028656006\n",
      "Epoch 2276: train loss: 1.272117018699646, val loss: 1.274772047996521\n",
      "Epoch 2277: train loss: 1.2715576887130737, val loss: 1.274201512336731\n",
      "Epoch 2278: train loss: 1.270998477935791, val loss: 1.273630976676941\n",
      "Epoch 2279: train loss: 1.2704393863677979, val loss: 1.27306067943573\n",
      "Epoch 2280: train loss: 1.2698804140090942, val loss: 1.2724907398223877\n",
      "Epoch 2281: train loss: 1.2693215608596802, val loss: 1.2719205617904663\n",
      "Epoch 2282: train loss: 1.2687630653381348, val loss: 1.2713509798049927\n",
      "Epoch 2283: train loss: 1.2682045698165894, val loss: 1.2707815170288086\n",
      "Epoch 2284: train loss: 1.2676464319229126, val loss: 1.2702120542526245\n",
      "Epoch 2285: train loss: 1.2670882940292358, val loss: 1.269642949104309\n",
      "Epoch 2286: train loss: 1.2665303945541382, val loss: 1.2690738439559937\n",
      "Epoch 2287: train loss: 1.2659724950790405, val loss: 1.2685050964355469\n",
      "Epoch 2288: train loss: 1.2654149532318115, val loss: 1.2679365873336792\n",
      "Epoch 2289: train loss: 1.264857530593872, val loss: 1.2673677206039429\n",
      "Epoch 2290: train loss: 1.2643002271652222, val loss: 1.2667995691299438\n",
      "Epoch 2291: train loss: 1.2637431621551514, val loss: 1.2662314176559448\n",
      "Epoch 2292: train loss: 1.2631862163543701, val loss: 1.265663504600525\n",
      "Epoch 2293: train loss: 1.2626293897628784, val loss: 1.2650957107543945\n",
      "Epoch 2294: train loss: 1.2620728015899658, val loss: 1.2645279169082642\n",
      "Epoch 2295: train loss: 1.2615163326263428, val loss: 1.263960599899292\n",
      "Epoch 2296: train loss: 1.2609601020812988, val loss: 1.2633932828903198\n",
      "Epoch 2297: train loss: 1.2604039907455444, val loss: 1.2628263235092163\n",
      "Epoch 2298: train loss: 1.2598479986190796, val loss: 1.2622593641281128\n",
      "Epoch 2299: train loss: 1.2592922449111938, val loss: 1.2616924047470093\n",
      "Epoch 2300: train loss: 1.258736491203308, val loss: 1.2611260414123535\n",
      "Epoch 2301: train loss: 1.258181095123291, val loss: 1.2605594396591187\n",
      "Epoch 2302: train loss: 1.2576260566711426, val loss: 1.2599934339523315\n",
      "Epoch 2303: train loss: 1.2570708990097046, val loss: 1.2594273090362549\n",
      "Epoch 2304: train loss: 1.2565158605575562, val loss: 1.2588615417480469\n",
      "Epoch 2305: train loss: 1.2559611797332764, val loss: 1.2582958936691284\n",
      "Epoch 2306: train loss: 1.2554064989089966, val loss: 1.2577303647994995\n",
      "Epoch 2307: train loss: 1.254852056503296, val loss: 1.2571649551391602\n",
      "Epoch 2308: train loss: 1.2542979717254639, val loss: 1.2565999031066895\n",
      "Epoch 2309: train loss: 1.2537437677383423, val loss: 1.2560349702835083\n",
      "Epoch 2310: train loss: 1.2531898021697998, val loss: 1.2554701566696167\n",
      "Epoch 2311: train loss: 1.2526359558105469, val loss: 1.2549055814743042\n",
      "Epoch 2312: train loss: 1.2520824670791626, val loss: 1.2543410062789917\n",
      "Epoch 2313: train loss: 1.2515289783477783, val loss: 1.2537767887115479\n",
      "Epoch 2314: train loss: 1.2509756088256836, val loss: 1.253212571144104\n",
      "Epoch 2315: train loss: 1.250422477722168, val loss: 1.2526485919952393\n",
      "Epoch 2316: train loss: 1.249869704246521, val loss: 1.252084732055664\n",
      "Epoch 2317: train loss: 1.2493168115615845, val loss: 1.251521348953247\n",
      "Epoch 2318: train loss: 1.2487642765045166, val loss: 1.2509578466415405\n",
      "Epoch 2319: train loss: 1.2482117414474487, val loss: 1.250394582748413\n",
      "Epoch 2320: train loss: 1.24765944480896, val loss: 1.2498314380645752\n",
      "Epoch 2321: train loss: 1.2471072673797607, val loss: 1.2492685317993164\n",
      "Epoch 2322: train loss: 1.2465553283691406, val loss: 1.2487058639526367\n",
      "Epoch 2323: train loss: 1.2460036277770996, val loss: 1.248143196105957\n",
      "Epoch 2324: train loss: 1.2454519271850586, val loss: 1.247580885887146\n",
      "Epoch 2325: train loss: 1.2449003458023071, val loss: 1.247018814086914\n",
      "Epoch 2326: train loss: 1.2443492412567139, val loss: 1.2464567422866821\n",
      "Epoch 2327: train loss: 1.243798017501831, val loss: 1.2458949089050293\n",
      "Epoch 2328: train loss: 1.2432470321655273, val loss: 1.2453330755233765\n",
      "Epoch 2329: train loss: 1.2426962852478027, val loss: 1.2447718381881714\n",
      "Epoch 2330: train loss: 1.2421456575393677, val loss: 1.2442104816436768\n",
      "Epoch 2331: train loss: 1.2415951490402222, val loss: 1.2436493635177612\n",
      "Epoch 2332: train loss: 1.2410447597503662, val loss: 1.2430883646011353\n",
      "Epoch 2333: train loss: 1.2404948472976685, val loss: 1.2425276041030884\n",
      "Epoch 2334: train loss: 1.2399446964263916, val loss: 1.2419668436050415\n",
      "Epoch 2335: train loss: 1.2393949031829834, val loss: 1.2414064407348633\n",
      "Epoch 2336: train loss: 1.2388453483581543, val loss: 1.240846037864685\n",
      "Epoch 2337: train loss: 1.2382959127426147, val loss: 1.2402862310409546\n",
      "Epoch 2338: train loss: 1.2377465963363647, val loss: 1.2397263050079346\n",
      "Epoch 2339: train loss: 1.2371973991394043, val loss: 1.239166498184204\n",
      "Epoch 2340: train loss: 1.236648440361023, val loss: 1.2386068105697632\n",
      "Epoch 2341: train loss: 1.2360996007919312, val loss: 1.238047480583191\n",
      "Epoch 2342: train loss: 1.235550880432129, val loss: 1.2374881505966187\n",
      "Epoch 2343: train loss: 1.2350025177001953, val loss: 1.236929178237915\n",
      "Epoch 2344: train loss: 1.2344540357589722, val loss: 1.236370325088501\n",
      "Epoch 2345: train loss: 1.2339059114456177, val loss: 1.2358115911483765\n",
      "Epoch 2346: train loss: 1.2333580255508423, val loss: 1.2352532148361206\n",
      "Epoch 2347: train loss: 1.232810139656067, val loss: 1.2346947193145752\n",
      "Epoch 2348: train loss: 1.2322624921798706, val loss: 1.2341364622116089\n",
      "Epoch 2349: train loss: 1.2317149639129639, val loss: 1.2335785627365112\n",
      "Epoch 2350: train loss: 1.2311676740646362, val loss: 1.2330206632614136\n",
      "Epoch 2351: train loss: 1.2306203842163086, val loss: 1.2324628829956055\n",
      "Epoch 2352: train loss: 1.2300734519958496, val loss: 1.2319055795669556\n",
      "Epoch 2353: train loss: 1.2295266389846802, val loss: 1.2313482761383057\n",
      "Epoch 2354: train loss: 1.2289799451828003, val loss: 1.2307910919189453\n",
      "Epoch 2355: train loss: 1.22843337059021, val loss: 1.2302340269088745\n",
      "Epoch 2356: train loss: 1.2278870344161987, val loss: 1.2296772003173828\n",
      "Epoch 2357: train loss: 1.2273409366607666, val loss: 1.2291206121444702\n",
      "Epoch 2358: train loss: 1.2267948389053345, val loss: 1.2285641431808472\n",
      "Epoch 2359: train loss: 1.226248860359192, val loss: 1.2280080318450928\n",
      "Epoch 2360: train loss: 1.225703239440918, val loss: 1.2274519205093384\n",
      "Epoch 2361: train loss: 1.2251577377319336, val loss: 1.2268959283828735\n",
      "Epoch 2362: train loss: 1.2246123552322388, val loss: 1.2263401746749878\n",
      "Epoch 2363: train loss: 1.224067211151123, val loss: 1.2257845401763916\n",
      "Epoch 2364: train loss: 1.2235221862792969, val loss: 1.2252291440963745\n",
      "Epoch 2365: train loss: 1.2229772806167603, val loss: 1.224673867225647\n",
      "Epoch 2366: train loss: 1.2224324941635132, val loss: 1.2241190671920776\n",
      "Epoch 2367: train loss: 1.2218879461288452, val loss: 1.2235640287399292\n",
      "Epoch 2368: train loss: 1.2213436365127563, val loss: 1.2230093479156494\n",
      "Epoch 2369: train loss: 1.220799446105957, val loss: 1.2224549055099487\n",
      "Epoch 2370: train loss: 1.2202554941177368, val loss: 1.2219005823135376\n",
      "Epoch 2371: train loss: 1.2197115421295166, val loss: 1.2213462591171265\n",
      "Epoch 2372: train loss: 1.2191678285598755, val loss: 1.2207921743392944\n",
      "Epoch 2373: train loss: 1.218624234199524, val loss: 1.220238447189331\n",
      "Epoch 2374: train loss: 1.218080759048462, val loss: 1.2196846008300781\n",
      "Epoch 2375: train loss: 1.2175376415252686, val loss: 1.2191312313079834\n",
      "Epoch 2376: train loss: 1.2169945240020752, val loss: 1.2185778617858887\n",
      "Epoch 2377: train loss: 1.2164517641067505, val loss: 1.218024730682373\n",
      "Epoch 2378: train loss: 1.2159090042114258, val loss: 1.2174718379974365\n",
      "Epoch 2379: train loss: 1.2153663635253906, val loss: 1.2169189453125\n",
      "Epoch 2380: train loss: 1.2148239612579346, val loss: 1.2163662910461426\n",
      "Epoch 2381: train loss: 1.2142817974090576, val loss: 1.2158137559890747\n",
      "Epoch 2382: train loss: 1.2137396335601807, val loss: 1.2152615785598755\n",
      "Epoch 2383: train loss: 1.2131977081298828, val loss: 1.2147094011306763\n",
      "Epoch 2384: train loss: 1.212656021118164, val loss: 1.2141575813293457\n",
      "Epoch 2385: train loss: 1.2121144533157349, val loss: 1.2136057615280151\n",
      "Epoch 2386: train loss: 1.2115730047225952, val loss: 1.2130541801452637\n",
      "Epoch 2387: train loss: 1.2110316753387451, val loss: 1.2125028371810913\n",
      "Epoch 2388: train loss: 1.2104905843734741, val loss: 1.2119516134262085\n",
      "Epoch 2389: train loss: 1.2099496126174927, val loss: 1.2114003896713257\n",
      "Epoch 2390: train loss: 1.2094088792800903, val loss: 1.2108495235443115\n",
      "Epoch 2391: train loss: 1.208868384361267, val loss: 1.210298776626587\n",
      "Epoch 2392: train loss: 1.2083278894424438, val loss: 1.2097482681274414\n",
      "Epoch 2393: train loss: 1.2077875137329102, val loss: 1.2091978788375854\n",
      "Epoch 2394: train loss: 1.207247257232666, val loss: 1.208647608757019\n",
      "Epoch 2395: train loss: 1.2067075967788696, val loss: 1.2080976963043213\n",
      "Epoch 2396: train loss: 1.2061675786972046, val loss: 1.2075475454330444\n",
      "Epoch 2397: train loss: 1.2056280374526978, val loss: 1.2069979906082153\n",
      "Epoch 2398: train loss: 1.2050886154174805, val loss: 1.2064484357833862\n",
      "Epoch 2399: train loss: 1.2045493125915527, val loss: 1.2058992385864258\n",
      "Epoch 2400: train loss: 1.204010248184204, val loss: 1.2053498029708862\n",
      "Epoch 2401: train loss: 1.2034711837768555, val loss: 1.2048008441925049\n",
      "Epoch 2402: train loss: 1.202932357788086, val loss: 1.2042521238327026\n",
      "Epoch 2403: train loss: 1.202393651008606, val loss: 1.2037034034729004\n",
      "Epoch 2404: train loss: 1.201855182647705, val loss: 1.2031549215316772\n",
      "Epoch 2405: train loss: 1.2013168334960938, val loss: 1.2026065587997437\n",
      "Epoch 2406: train loss: 1.2007787227630615, val loss: 1.2020584344863892\n",
      "Epoch 2407: train loss: 1.2002407312393188, val loss: 1.2015104293823242\n",
      "Epoch 2408: train loss: 1.1997028589248657, val loss: 1.2009625434875488\n",
      "Epoch 2409: train loss: 1.1991652250289917, val loss: 1.200415015220642\n",
      "Epoch 2410: train loss: 1.1986277103424072, val loss: 1.1998674869537354\n",
      "Epoch 2411: train loss: 1.1980903148651123, val loss: 1.1993201971054077\n",
      "Epoch 2412: train loss: 1.1975531578063965, val loss: 1.1987732648849487\n",
      "Epoch 2413: train loss: 1.1970161199569702, val loss: 1.1982262134552002\n",
      "Epoch 2414: train loss: 1.196479320526123, val loss: 1.1976795196533203\n",
      "Epoch 2415: train loss: 1.1959425210952759, val loss: 1.19713294506073\n",
      "Epoch 2416: train loss: 1.1954060792922974, val loss: 1.1965864896774292\n",
      "Epoch 2417: train loss: 1.1948696374893188, val loss: 1.1960402727127075\n",
      "Epoch 2418: train loss: 1.194333553314209, val loss: 1.1954941749572754\n",
      "Epoch 2419: train loss: 1.1937974691390991, val loss: 1.1949483156204224\n",
      "Epoch 2420: train loss: 1.1932616233825684, val loss: 1.1944025754928589\n",
      "Epoch 2421: train loss: 1.1927257776260376, val loss: 1.1938570737838745\n",
      "Epoch 2422: train loss: 1.192190408706665, val loss: 1.1933116912841797\n",
      "Epoch 2423: train loss: 1.1916550397872925, val loss: 1.1927664279937744\n",
      "Epoch 2424: train loss: 1.1911197900772095, val loss: 1.1922214031219482\n",
      "Epoch 2425: train loss: 1.190584659576416, val loss: 1.1916764974594116\n",
      "Epoch 2426: train loss: 1.1900497674942017, val loss: 1.191131830215454\n",
      "Epoch 2427: train loss: 1.1895149946212769, val loss: 1.1905874013900757\n",
      "Epoch 2428: train loss: 1.1889804601669312, val loss: 1.1900428533554077\n",
      "Epoch 2429: train loss: 1.188446044921875, val loss: 1.189498782157898\n",
      "Epoch 2430: train loss: 1.187911868095398, val loss: 1.1889545917510986\n",
      "Epoch 2431: train loss: 1.1873778104782104, val loss: 1.1884106397628784\n",
      "Epoch 2432: train loss: 1.186843752861023, val loss: 1.1878670454025269\n",
      "Epoch 2433: train loss: 1.186310052871704, val loss: 1.1873235702514648\n",
      "Epoch 2434: train loss: 1.1857764720916748, val loss: 1.186780333518982\n",
      "Epoch 2435: train loss: 1.1852428913116455, val loss: 1.1862372159957886\n",
      "Epoch 2436: train loss: 1.1847097873687744, val loss: 1.1856942176818848\n",
      "Epoch 2437: train loss: 1.1841766834259033, val loss: 1.18515145778656\n",
      "Epoch 2438: train loss: 1.1836436986923218, val loss: 1.184608817100525\n",
      "Epoch 2439: train loss: 1.1831107139587402, val loss: 1.1840661764144897\n",
      "Epoch 2440: train loss: 1.182578206062317, val loss: 1.1835240125656128\n",
      "Epoch 2441: train loss: 1.182045817375183, val loss: 1.1829817295074463\n",
      "Epoch 2442: train loss: 1.1815135478973389, val loss: 1.1824400424957275\n",
      "Epoch 2443: train loss: 1.1809813976287842, val loss: 1.1818979978561401\n",
      "Epoch 2444: train loss: 1.1804494857788086, val loss: 1.181356430053711\n",
      "Epoch 2445: train loss: 1.179917573928833, val loss: 1.1808148622512817\n",
      "Epoch 2446: train loss: 1.1793859004974365, val loss: 1.1802736520767212\n",
      "Epoch 2447: train loss: 1.1788543462753296, val loss: 1.1797326803207397\n",
      "Epoch 2448: train loss: 1.1783230304718018, val loss: 1.1791914701461792\n",
      "Epoch 2449: train loss: 1.1777918338775635, val loss: 1.178650975227356\n",
      "Epoch 2450: train loss: 1.1772608757019043, val loss: 1.1781102418899536\n",
      "Epoch 2451: train loss: 1.1767300367355347, val loss: 1.17756986618042\n",
      "Epoch 2452: train loss: 1.1761993169784546, val loss: 1.1770294904708862\n",
      "Epoch 2453: train loss: 1.175668716430664, val loss: 1.1764893531799316\n",
      "Epoch 2454: train loss: 1.1751384735107422, val loss: 1.1759496927261353\n",
      "Epoch 2455: train loss: 1.1746082305908203, val loss: 1.1754097938537598\n",
      "Epoch 2456: train loss: 1.174078106880188, val loss: 1.1748703718185425\n",
      "Epoch 2457: train loss: 1.1735483407974243, val loss: 1.1743310689926147\n",
      "Epoch 2458: train loss: 1.1730186939239502, val loss: 1.173791766166687\n",
      "Epoch 2459: train loss: 1.172489047050476, val loss: 1.1732527017593384\n",
      "Epoch 2460: train loss: 1.171959638595581, val loss: 1.1727136373519897\n",
      "Epoch 2461: train loss: 1.1714304685592651, val loss: 1.1721750497817993\n",
      "Epoch 2462: train loss: 1.1709012985229492, val loss: 1.1716364622116089\n",
      "Epoch 2463: train loss: 1.1703723669052124, val loss: 1.1710981130599976\n",
      "Epoch 2464: train loss: 1.1698436737060547, val loss: 1.1705598831176758\n",
      "Epoch 2465: train loss: 1.1693150997161865, val loss: 1.170021891593933\n",
      "Epoch 2466: train loss: 1.1687867641448975, val loss: 1.1694841384887695\n",
      "Epoch 2467: train loss: 1.168258547782898, val loss: 1.1689465045928955\n",
      "Epoch 2468: train loss: 1.167730450630188, val loss: 1.168408989906311\n",
      "Epoch 2469: train loss: 1.1672024726867676, val loss: 1.1678714752197266\n",
      "Epoch 2470: train loss: 1.1666746139526367, val loss: 1.1673344373703003\n",
      "Epoch 2471: train loss: 1.166146993637085, val loss: 1.1667975187301636\n",
      "Epoch 2472: train loss: 1.1656196117401123, val loss: 1.1662606000900269\n",
      "Epoch 2473: train loss: 1.1650923490524292, val loss: 1.1657239198684692\n",
      "Epoch 2474: train loss: 1.1645653247833252, val loss: 1.1651875972747803\n",
      "Epoch 2475: train loss: 1.1640383005142212, val loss: 1.1646511554718018\n",
      "Epoch 2476: train loss: 1.1635113954544067, val loss: 1.1641149520874023\n",
      "Epoch 2477: train loss: 1.162984848022461, val loss: 1.163578987121582\n",
      "Epoch 2478: train loss: 1.1624583005905151, val loss: 1.1630432605743408\n",
      "Epoch 2479: train loss: 1.161932110786438, val loss: 1.1625076532363892\n",
      "Epoch 2480: train loss: 1.1614058017730713, val loss: 1.1619720458984375\n",
      "Epoch 2481: train loss: 1.1608798503875732, val loss: 1.161436915397644\n",
      "Epoch 2482: train loss: 1.1603541374206543, val loss: 1.160901665687561\n",
      "Epoch 2483: train loss: 1.1598284244537354, val loss: 1.1603666543960571\n",
      "Epoch 2484: train loss: 1.159302830696106, val loss: 1.1598320007324219\n",
      "Epoch 2485: train loss: 1.1587774753570557, val loss: 1.1592973470687866\n",
      "Epoch 2486: train loss: 1.1582523584365845, val loss: 1.158762812614441\n",
      "Epoch 2487: train loss: 1.1577272415161133, val loss: 1.1582285165786743\n",
      "Epoch 2488: train loss: 1.1572024822235107, val loss: 1.1576945781707764\n",
      "Epoch 2489: train loss: 1.1566778421401978, val loss: 1.1571606397628784\n",
      "Epoch 2490: train loss: 1.1561532020568848, val loss: 1.15662682056427\n",
      "Epoch 2491: train loss: 1.1556288003921509, val loss: 1.1560932397842407\n",
      "Epoch 2492: train loss: 1.155104637145996, val loss: 1.1555596590042114\n",
      "Epoch 2493: train loss: 1.1545804738998413, val loss: 1.1550264358520508\n",
      "Epoch 2494: train loss: 1.1540565490722656, val loss: 1.1544934511184692\n",
      "Epoch 2495: train loss: 1.153532862663269, val loss: 1.1539605855941772\n",
      "Epoch 2496: train loss: 1.1530091762542725, val loss: 1.1534277200698853\n",
      "Epoch 2497: train loss: 1.1524858474731445, val loss: 1.152895212173462\n",
      "Epoch 2498: train loss: 1.1519625186920166, val loss: 1.1523627042770386\n",
      "Epoch 2499: train loss: 1.1514394283294678, val loss: 1.1518306732177734\n",
      "Epoch 2500: train loss: 1.150916576385498, val loss: 1.1512985229492188\n",
      "Epoch 2501: train loss: 1.1503937244415283, val loss: 1.1507666110992432\n",
      "Epoch 2502: train loss: 1.1498712301254272, val loss: 1.1502348184585571\n",
      "Epoch 2503: train loss: 1.1493486166000366, val loss: 1.1497032642364502\n",
      "Epoch 2504: train loss: 1.1488264799118042, val loss: 1.1491717100143433\n",
      "Epoch 2505: train loss: 1.1483041048049927, val loss: 1.1486406326293945\n",
      "Epoch 2506: train loss: 1.1477822065353394, val loss: 1.1481095552444458\n",
      "Epoch 2507: train loss: 1.1472604274749756, val loss: 1.1475785970687866\n",
      "Epoch 2508: train loss: 1.1467387676239014, val loss: 1.147047996520996\n",
      "Epoch 2509: train loss: 1.1462173461914062, val loss: 1.1465175151824951\n",
      "Epoch 2510: train loss: 1.1456959247589111, val loss: 1.1459871530532837\n",
      "Epoch 2511: train loss: 1.1451747417449951, val loss: 1.1454567909240723\n",
      "Epoch 2512: train loss: 1.1446536779403687, val loss: 1.1449267864227295\n",
      "Epoch 2513: train loss: 1.1441328525543213, val loss: 1.1443969011306763\n",
      "Epoch 2514: train loss: 1.1436121463775635, val loss: 1.1438672542572021\n",
      "Epoch 2515: train loss: 1.1430915594100952, val loss: 1.143337607383728\n",
      "Epoch 2516: train loss: 1.142571210861206, val loss: 1.1428085565567017\n",
      "Epoch 2517: train loss: 1.142051100730896, val loss: 1.1422791481018066\n",
      "Epoch 2518: train loss: 1.1415308713912964, val loss: 1.1417500972747803\n",
      "Epoch 2519: train loss: 1.1410108804702759, val loss: 1.1412211656570435\n",
      "Epoch 2520: train loss: 1.140491247177124, val loss: 1.1406925916671753\n",
      "Epoch 2521: train loss: 1.1399717330932617, val loss: 1.1401640176773071\n",
      "Epoch 2522: train loss: 1.139452338218689, val loss: 1.1396359205245972\n",
      "Epoch 2523: train loss: 1.1389329433441162, val loss: 1.139107584953308\n",
      "Epoch 2524: train loss: 1.138413906097412, val loss: 1.1385797262191772\n",
      "Epoch 2525: train loss: 1.137894868850708, val loss: 1.1380518674850464\n",
      "Epoch 2526: train loss: 1.137376070022583, val loss: 1.1375240087509155\n",
      "Epoch 2527: train loss: 1.136857509613037, val loss: 1.1369966268539429\n",
      "Epoch 2528: train loss: 1.1363390684127808, val loss: 1.1364692449569702\n",
      "Epoch 2529: train loss: 1.135820746421814, val loss: 1.1359421014785767\n",
      "Epoch 2530: train loss: 1.1353026628494263, val loss: 1.1354150772094727\n",
      "Epoch 2531: train loss: 1.1347846984863281, val loss: 1.1348880529403687\n",
      "Epoch 2532: train loss: 1.1342668533325195, val loss: 1.1343616247177124\n",
      "Epoch 2533: train loss: 1.13374924659729, val loss: 1.1338351964950562\n",
      "Epoch 2534: train loss: 1.13323175907135, val loss: 1.1333087682724\n",
      "Epoch 2535: train loss: 1.1327145099639893, val loss: 1.1327828168869019\n",
      "Epoch 2536: train loss: 1.1321972608566284, val loss: 1.1322567462921143\n",
      "Epoch 2537: train loss: 1.1316802501678467, val loss: 1.1317309141159058\n",
      "Epoch 2538: train loss: 1.1311633586883545, val loss: 1.1312052011489868\n",
      "Epoch 2539: train loss: 1.1306465864181519, val loss: 1.130679726600647\n",
      "Epoch 2540: train loss: 1.1301301717758179, val loss: 1.1301544904708862\n",
      "Epoch 2541: train loss: 1.1296137571334839, val loss: 1.1296294927597046\n",
      "Epoch 2542: train loss: 1.129097580909729, val loss: 1.129104495048523\n",
      "Epoch 2543: train loss: 1.1285815238952637, val loss: 1.1285796165466309\n",
      "Epoch 2544: train loss: 1.1280657052993774, val loss: 1.1280549764633179\n",
      "Epoch 2545: train loss: 1.1275498867034912, val loss: 1.127530574798584\n",
      "Epoch 2546: train loss: 1.127034306526184, val loss: 1.1270064115524292\n",
      "Epoch 2547: train loss: 1.126518964767456, val loss: 1.1264822483062744\n",
      "Epoch 2548: train loss: 1.126003623008728, val loss: 1.1259583234786987\n",
      "Epoch 2549: train loss: 1.125488519668579, val loss: 1.1254345178604126\n",
      "Epoch 2550: train loss: 1.1249735355377197, val loss: 1.124910831451416\n",
      "Epoch 2551: train loss: 1.1244587898254395, val loss: 1.1243876218795776\n",
      "Epoch 2552: train loss: 1.1239441633224487, val loss: 1.1238640546798706\n",
      "Epoch 2553: train loss: 1.123429775238037, val loss: 1.1233409643173218\n",
      "Epoch 2554: train loss: 1.1229153871536255, val loss: 1.1228182315826416\n",
      "Epoch 2555: train loss: 1.122401237487793, val loss: 1.1222953796386719\n",
      "Epoch 2556: train loss: 1.121887445449829, val loss: 1.1217727661132812\n",
      "Epoch 2557: train loss: 1.1213734149932861, val loss: 1.1212503910064697\n",
      "Epoch 2558: train loss: 1.1208598613739014, val loss: 1.1207278966903687\n",
      "Epoch 2559: train loss: 1.1203463077545166, val loss: 1.1202057600021362\n",
      "Epoch 2560: train loss: 1.1198328733444214, val loss: 1.1196839809417725\n",
      "Epoch 2561: train loss: 1.1193197965621948, val loss: 1.1191624402999878\n",
      "Epoch 2562: train loss: 1.1188068389892578, val loss: 1.118640661239624\n",
      "Epoch 2563: train loss: 1.1182938814163208, val loss: 1.118119239807129\n",
      "Epoch 2564: train loss: 1.1177810430526733, val loss: 1.1175979375839233\n",
      "Epoch 2565: train loss: 1.1172685623168945, val loss: 1.1170768737792969\n",
      "Epoch 2566: train loss: 1.1167562007904053, val loss: 1.1165560483932495\n",
      "Epoch 2567: train loss: 1.1162439584732056, val loss: 1.1160353422164917\n",
      "Epoch 2568: train loss: 1.115731954574585, val loss: 1.1155146360397339\n",
      "Epoch 2569: train loss: 1.1152199506759644, val loss: 1.1149942874908447\n",
      "Epoch 2570: train loss: 1.1147081851959229, val loss: 1.1144739389419556\n",
      "Epoch 2571: train loss: 1.1141964197158813, val loss: 1.1139538288116455\n",
      "Epoch 2572: train loss: 1.113685131072998, val loss: 1.113433837890625\n",
      "Epoch 2573: train loss: 1.1131738424301147, val loss: 1.1129140853881836\n",
      "Epoch 2574: train loss: 1.112662672996521, val loss: 1.1123945713043213\n",
      "Epoch 2575: train loss: 1.1121517419815063, val loss: 1.1118751764297485\n",
      "Epoch 2576: train loss: 1.1116409301757812, val loss: 1.1113560199737549\n",
      "Epoch 2577: train loss: 1.1111302375793457, val loss: 1.1108368635177612\n",
      "Epoch 2578: train loss: 1.1106197834014893, val loss: 1.1103179454803467\n",
      "Epoch 2579: train loss: 1.1101094484329224, val loss: 1.1097992658615112\n",
      "Epoch 2580: train loss: 1.1095993518829346, val loss: 1.1092805862426758\n",
      "Epoch 2581: train loss: 1.1090891361236572, val loss: 1.1087621450424194\n",
      "Epoch 2582: train loss: 1.108579397201538, val loss: 1.1082439422607422\n",
      "Epoch 2583: train loss: 1.1080697774887085, val loss: 1.1077258586883545\n",
      "Epoch 2584: train loss: 1.1075602769851685, val loss: 1.107208013534546\n",
      "Epoch 2585: train loss: 1.1070507764816284, val loss: 1.1066902875900269\n",
      "Epoch 2586: train loss: 1.1065415143966675, val loss: 1.1061726808547974\n",
      "Epoch 2587: train loss: 1.1060324907302856, val loss: 1.1056551933288574\n",
      "Epoch 2588: train loss: 1.1055235862731934, val loss: 1.1051379442214966\n",
      "Epoch 2589: train loss: 1.1050148010253906, val loss: 1.1046209335327148\n",
      "Epoch 2590: train loss: 1.104506254196167, val loss: 1.1041040420532227\n",
      "Epoch 2591: train loss: 1.103997826576233, val loss: 1.10358726978302\n",
      "Epoch 2592: train loss: 1.1034895181655884, val loss: 1.103070855140686\n",
      "Epoch 2593: train loss: 1.1029813289642334, val loss: 1.102554202079773\n",
      "Epoch 2594: train loss: 1.102473497390747, val loss: 1.102038025856018\n",
      "Epoch 2595: train loss: 1.1019655466079712, val loss: 1.1015219688415527\n",
      "Epoch 2596: train loss: 1.101457953453064, val loss: 1.1010061502456665\n",
      "Epoch 2597: train loss: 1.1009504795074463, val loss: 1.1004904508590698\n",
      "Epoch 2598: train loss: 1.1004431247711182, val loss: 1.0999748706817627\n",
      "Epoch 2599: train loss: 1.0999361276626587, val loss: 1.0994595289230347\n",
      "Epoch 2600: train loss: 1.0994290113449097, val loss: 1.0989443063735962\n",
      "Epoch 2601: train loss: 1.0989222526550293, val loss: 1.0984292030334473\n",
      "Epoch 2602: train loss: 1.098415493965149, val loss: 1.0979143381118774\n",
      "Epoch 2603: train loss: 1.097908854484558, val loss: 1.0973995923995972\n",
      "Epoch 2604: train loss: 1.097402572631836, val loss: 1.0968849658966064\n",
      "Epoch 2605: train loss: 1.0968962907791138, val loss: 1.0963706970214844\n",
      "Epoch 2606: train loss: 1.0963903665542603, val loss: 1.0958565473556519\n",
      "Epoch 2607: train loss: 1.0958844423294067, val loss: 1.0953425168991089\n",
      "Epoch 2608: train loss: 1.0953787565231323, val loss: 1.094828486442566\n",
      "Epoch 2609: train loss: 1.094873070716858, val loss: 1.0943149328231812\n",
      "Epoch 2610: train loss: 1.0943676233291626, val loss: 1.0938013792037964\n",
      "Epoch 2611: train loss: 1.0938624143600464, val loss: 1.0932878255844116\n",
      "Epoch 2612: train loss: 1.0933572053909302, val loss: 1.0927746295928955\n",
      "Epoch 2613: train loss: 1.0928523540496826, val loss: 1.0922616720199585\n",
      "Epoch 2614: train loss: 1.0923476219177246, val loss: 1.0917487144470215\n",
      "Epoch 2615: train loss: 1.0918428897857666, val loss: 1.0912361145019531\n",
      "Epoch 2616: train loss: 1.0913385152816772, val loss: 1.0907233953475952\n",
      "Epoch 2617: train loss: 1.090834140777588, val loss: 1.0902109146118164\n",
      "Epoch 2618: train loss: 1.0903300046920776, val loss: 1.0896989107131958\n",
      "Epoch 2619: train loss: 1.089825987815857, val loss: 1.0891869068145752\n",
      "Epoch 2620: train loss: 1.0893220901489258, val loss: 1.0886750221252441\n",
      "Epoch 2621: train loss: 1.0888184309005737, val loss: 1.0881632566452026\n",
      "Epoch 2622: train loss: 1.0883150100708008, val loss: 1.0876516103744507\n",
      "Epoch 2623: train loss: 1.0878114700317383, val loss: 1.0871400833129883\n",
      "Epoch 2624: train loss: 1.0873082876205444, val loss: 1.086629033088684\n",
      "Epoch 2625: train loss: 1.0868052244186401, val loss: 1.0861179828643799\n",
      "Epoch 2626: train loss: 1.0863022804260254, val loss: 1.0856071710586548\n",
      "Epoch 2627: train loss: 1.0857995748519897, val loss: 1.0850963592529297\n",
      "Epoch 2628: train loss: 1.0852969884872437, val loss: 1.0845859050750732\n",
      "Epoch 2629: train loss: 1.084794521331787, val loss: 1.0840753316879272\n",
      "Epoch 2630: train loss: 1.0842922925949097, val loss: 1.08356511592865\n",
      "Epoch 2631: train loss: 1.0837901830673218, val loss: 1.0830551385879517\n",
      "Epoch 2632: train loss: 1.083288311958313, val loss: 1.082545280456543\n",
      "Epoch 2633: train loss: 1.0827864408493042, val loss: 1.0820354223251343\n",
      "Epoch 2634: train loss: 1.0822848081588745, val loss: 1.0815259218215942\n",
      "Epoch 2635: train loss: 1.0817832946777344, val loss: 1.0810165405273438\n",
      "Epoch 2636: train loss: 1.0812820196151733, val loss: 1.0805071592330933\n",
      "Epoch 2637: train loss: 1.0807807445526123, val loss: 1.079998254776001\n",
      "Epoch 2638: train loss: 1.0802797079086304, val loss: 1.0794894695281982\n",
      "Epoch 2639: train loss: 1.0797789096832275, val loss: 1.0789806842803955\n",
      "Epoch 2640: train loss: 1.0792782306671143, val loss: 1.0784720182418823\n",
      "Epoch 2641: train loss: 1.0787776708602905, val loss: 1.0779637098312378\n",
      "Epoch 2642: train loss: 1.0782772302627563, val loss: 1.0774555206298828\n",
      "Epoch 2643: train loss: 1.0777771472930908, val loss: 1.0769474506378174\n",
      "Epoch 2644: train loss: 1.0772768259048462, val loss: 1.0764394998550415\n",
      "Epoch 2645: train loss: 1.0767769813537598, val loss: 1.0759317874908447\n",
      "Epoch 2646: train loss: 1.0762771368026733, val loss: 1.0754241943359375\n",
      "Epoch 2647: train loss: 1.075777530670166, val loss: 1.0749168395996094\n",
      "Epoch 2648: train loss: 1.0752780437469482, val loss: 1.0744097232818604\n",
      "Epoch 2649: train loss: 1.0747789144515991, val loss: 1.0739024877548218\n",
      "Epoch 2650: train loss: 1.07427978515625, val loss: 1.0733956098556519\n",
      "Epoch 2651: train loss: 1.0737806558609009, val loss: 1.0728888511657715\n",
      "Epoch 2652: train loss: 1.0732818841934204, val loss: 1.0723822116851807\n",
      "Epoch 2653: train loss: 1.0727832317352295, val loss: 1.071875810623169\n",
      "Epoch 2654: train loss: 1.0722846984863281, val loss: 1.0713696479797363\n",
      "Epoch 2655: train loss: 1.0717864036560059, val loss: 1.0708636045455933\n",
      "Epoch 2656: train loss: 1.0712882280349731, val loss: 1.0703575611114502\n",
      "Epoch 2657: train loss: 1.07079017162323, val loss: 1.0698518753051758\n",
      "Epoch 2658: train loss: 1.0702922344207764, val loss: 1.069346308708191\n",
      "Epoch 2659: train loss: 1.0697944164276123, val loss: 1.0688408613204956\n",
      "Epoch 2660: train loss: 1.069296956062317, val loss: 1.0683355331420898\n",
      "Epoch 2661: train loss: 1.0687994956970215, val loss: 1.0678305625915527\n",
      "Epoch 2662: train loss: 1.0683022737503052, val loss: 1.0673258304595947\n",
      "Epoch 2663: train loss: 1.0678051710128784, val loss: 1.0668209791183472\n",
      "Epoch 2664: train loss: 1.0673081874847412, val loss: 1.0663162469863892\n",
      "Epoch 2665: train loss: 1.0668113231658936, val loss: 1.0658118724822998\n",
      "Epoch 2666: train loss: 1.066314697265625, val loss: 1.0653074979782104\n",
      "Epoch 2667: train loss: 1.0658183097839355, val loss: 1.0648034811019897\n",
      "Epoch 2668: train loss: 1.065321922302246, val loss: 1.0642995834350586\n",
      "Epoch 2669: train loss: 1.0648257732391357, val loss: 1.0637959241867065\n",
      "Epoch 2670: train loss: 1.0643298625946045, val loss: 1.0632922649383545\n",
      "Epoch 2671: train loss: 1.0638339519500732, val loss: 1.0627888441085815\n",
      "Epoch 2672: train loss: 1.063338279724121, val loss: 1.0622854232788086\n",
      "Epoch 2673: train loss: 1.062842845916748, val loss: 1.0617824792861938\n",
      "Epoch 2674: train loss: 1.062347412109375, val loss: 1.061279535293579\n",
      "Epoch 2675: train loss: 1.061851978302002, val loss: 1.0607768297195435\n",
      "Epoch 2676: train loss: 1.0613571405410767, val loss: 1.0602741241455078\n",
      "Epoch 2677: train loss: 1.0608623027801514, val loss: 1.0597716569900513\n",
      "Epoch 2678: train loss: 1.0603673458099365, val loss: 1.0592695474624634\n",
      "Epoch 2679: train loss: 1.0598728656768799, val loss: 1.058767318725586\n",
      "Epoch 2680: train loss: 1.0593783855438232, val loss: 1.0582653284072876\n",
      "Epoch 2681: train loss: 1.0588841438293457, val loss: 1.057763695716858\n",
      "Epoch 2682: train loss: 1.0583900213241577, val loss: 1.0572620630264282\n",
      "Epoch 2683: train loss: 1.0578961372375488, val loss: 1.0567606687545776\n",
      "Epoch 2684: train loss: 1.05740225315094, val loss: 1.056259274482727\n",
      "Epoch 2685: train loss: 1.0569084882736206, val loss: 1.0557583570480347\n",
      "Epoch 2686: train loss: 1.0564152002334595, val loss: 1.0552572011947632\n",
      "Epoch 2687: train loss: 1.0559217929840088, val loss: 1.05475652217865\n",
      "Epoch 2688: train loss: 1.0554286241531372, val loss: 1.0542558431625366\n",
      "Epoch 2689: train loss: 1.0549354553222656, val loss: 1.0537554025650024\n",
      "Epoch 2690: train loss: 1.0544427633285522, val loss: 1.0532552003860474\n",
      "Epoch 2691: train loss: 1.0539499521255493, val loss: 1.0527549982070923\n",
      "Epoch 2692: train loss: 1.053457260131836, val loss: 1.0522550344467163\n",
      "Epoch 2693: train loss: 1.0529649257659912, val loss: 1.0517551898956299\n",
      "Epoch 2694: train loss: 1.052472710609436, val loss: 1.0512555837631226\n",
      "Epoch 2695: train loss: 1.0519806146621704, val loss: 1.0507560968399048\n",
      "Epoch 2696: train loss: 1.0514886379241943, val loss: 1.0502568483352661\n",
      "Epoch 2697: train loss: 1.0509967803955078, val loss: 1.0497575998306274\n",
      "Epoch 2698: train loss: 1.0505051612854004, val loss: 1.0492585897445679\n",
      "Epoch 2699: train loss: 1.0500136613845825, val loss: 1.0487598180770874\n",
      "Epoch 2700: train loss: 1.0495223999023438, val loss: 1.0482615232467651\n",
      "Epoch 2701: train loss: 1.0490312576293945, val loss: 1.0477628707885742\n",
      "Epoch 2702: train loss: 1.0485402345657349, val loss: 1.0472644567489624\n",
      "Epoch 2703: train loss: 1.0480493307113647, val loss: 1.0467664003372192\n",
      "Epoch 2704: train loss: 1.0475586652755737, val loss: 1.046268343925476\n",
      "Epoch 2705: train loss: 1.0470679998397827, val loss: 1.045770525932312\n",
      "Epoch 2706: train loss: 1.0465776920318604, val loss: 1.045272946357727\n",
      "Epoch 2707: train loss: 1.0460875034332275, val loss: 1.044775366783142\n",
      "Epoch 2708: train loss: 1.0455974340438843, val loss: 1.0442780256271362\n",
      "Epoch 2709: train loss: 1.0451074838638306, val loss: 1.04378080368042\n",
      "Epoch 2710: train loss: 1.0446175336837769, val loss: 1.0432838201522827\n",
      "Epoch 2711: train loss: 1.0441279411315918, val loss: 1.042786955833435\n",
      "Epoch 2712: train loss: 1.0436385869979858, val loss: 1.0422903299331665\n",
      "Epoch 2713: train loss: 1.0431492328643799, val loss: 1.0417938232421875\n",
      "Epoch 2714: train loss: 1.0426599979400635, val loss: 1.0412975549697876\n",
      "Epoch 2715: train loss: 1.0421711206436157, val loss: 1.0408012866973877\n",
      "Epoch 2716: train loss: 1.041682243347168, val loss: 1.040305256843567\n",
      "Epoch 2717: train loss: 1.0411936044692993, val loss: 1.0398093461990356\n",
      "Epoch 2718: train loss: 1.0407050848007202, val loss: 1.0393136739730835\n",
      "Epoch 2719: train loss: 1.0402165651321411, val loss: 1.0388182401657104\n",
      "Epoch 2720: train loss: 1.0397284030914307, val loss: 1.0383228063583374\n",
      "Epoch 2721: train loss: 1.0392403602600098, val loss: 1.037827730178833\n",
      "Epoch 2722: train loss: 1.0387524366378784, val loss: 1.0373326539993286\n",
      "Epoch 2723: train loss: 1.0382646322250366, val loss: 1.0368376970291138\n",
      "Epoch 2724: train loss: 1.037777066230774, val loss: 1.036342978477478\n",
      "Epoch 2725: train loss: 1.0372896194458008, val loss: 1.0358484983444214\n",
      "Epoch 2726: train loss: 1.0368024110794067, val loss: 1.0353540182113647\n",
      "Epoch 2727: train loss: 1.0363152027130127, val loss: 1.0348598957061768\n",
      "Epoch 2728: train loss: 1.0358282327651978, val loss: 1.0343660116195679\n",
      "Epoch 2729: train loss: 1.0353413820266724, val loss: 1.0338720083236694\n",
      "Epoch 2730: train loss: 1.0348546504974365, val loss: 1.03337824344635\n",
      "Epoch 2731: train loss: 1.0343682765960693, val loss: 1.0328845977783203\n",
      "Epoch 2732: train loss: 1.0338817834854126, val loss: 1.0323911905288696\n",
      "Epoch 2733: train loss: 1.033395528793335, val loss: 1.0318979024887085\n",
      "Epoch 2734: train loss: 1.0329095125198364, val loss: 1.0314048528671265\n",
      "Epoch 2735: train loss: 1.0324236154556274, val loss: 1.0309120416641235\n",
      "Epoch 2736: train loss: 1.0319379568099976, val loss: 1.0304192304611206\n",
      "Epoch 2737: train loss: 1.0314524173736572, val loss: 1.0299266576766968\n",
      "Epoch 2738: train loss: 1.030966877937317, val loss: 1.029434323310852\n",
      "Epoch 2739: train loss: 1.0304816961288452, val loss: 1.0289421081542969\n",
      "Epoch 2740: train loss: 1.029996633529663, val loss: 1.0284498929977417\n",
      "Epoch 2741: train loss: 1.029511570930481, val loss: 1.0279581546783447\n",
      "Epoch 2742: train loss: 1.029026746749878, val loss: 1.0274664163589478\n",
      "Epoch 2743: train loss: 1.0285420417785645, val loss: 1.0269746780395508\n",
      "Epoch 2744: train loss: 1.02805757522583, val loss: 1.026483416557312\n",
      "Epoch 2745: train loss: 1.0275733470916748, val loss: 1.0259920358657837\n",
      "Epoch 2746: train loss: 1.0270891189575195, val loss: 1.025501012802124\n",
      "Epoch 2747: train loss: 1.0266051292419434, val loss: 1.0250099897384644\n",
      "Epoch 2748: train loss: 1.0261211395263672, val loss: 1.0245192050933838\n",
      "Epoch 2749: train loss: 1.0256373882293701, val loss: 1.0240286588668823\n",
      "Epoch 2750: train loss: 1.0251537561416626, val loss: 1.0235381126403809\n",
      "Epoch 2751: train loss: 1.0246703624725342, val loss: 1.023047924041748\n",
      "Epoch 2752: train loss: 1.0241872072219849, val loss: 1.0225579738616943\n",
      "Epoch 2753: train loss: 1.023703932762146, val loss: 1.022067904472351\n",
      "Epoch 2754: train loss: 1.0232211351394653, val loss: 1.021578073501587\n",
      "Epoch 2755: train loss: 1.0227382183074951, val loss: 1.0210883617401123\n",
      "Epoch 2756: train loss: 1.0222556591033936, val loss: 1.0205988883972168\n",
      "Epoch 2757: train loss: 1.021773099899292, val loss: 1.0201095342636108\n",
      "Epoch 2758: train loss: 1.0212907791137695, val loss: 1.0196205377578735\n",
      "Epoch 2759: train loss: 1.0208085775375366, val loss: 1.0191315412521362\n",
      "Epoch 2760: train loss: 1.0203266143798828, val loss: 1.0186426639556885\n",
      "Epoch 2761: train loss: 1.0198447704315186, val loss: 1.0181540250778198\n",
      "Epoch 2762: train loss: 1.0193629264831543, val loss: 1.0176655054092407\n",
      "Epoch 2763: train loss: 1.0188814401626587, val loss: 1.0171772241592407\n",
      "Epoch 2764: train loss: 1.018399953842163, val loss: 1.0166889429092407\n",
      "Epoch 2765: train loss: 1.0179187059402466, val loss: 1.0162010192871094\n",
      "Epoch 2766: train loss: 1.0174375772476196, val loss: 1.015713095664978\n",
      "Epoch 2767: train loss: 1.0169565677642822, val loss: 1.0152254104614258\n",
      "Epoch 2768: train loss: 1.016475796699524, val loss: 1.014737844467163\n",
      "Epoch 2769: train loss: 1.0159950256347656, val loss: 1.014250636100769\n",
      "Epoch 2770: train loss: 1.015514612197876, val loss: 1.0137633085250854\n",
      "Epoch 2771: train loss: 1.0150343179702759, val loss: 1.0132763385772705\n",
      "Epoch 2772: train loss: 1.0145540237426758, val loss: 1.0127894878387451\n",
      "Epoch 2773: train loss: 1.0140740871429443, val loss: 1.0123027563095093\n",
      "Epoch 2774: train loss: 1.0135942697525024, val loss: 1.011816143989563\n",
      "Epoch 2775: train loss: 1.0131144523620605, val loss: 1.0113297700881958\n",
      "Epoch 2776: train loss: 1.0126348733901978, val loss: 1.0108436346054077\n",
      "Epoch 2777: train loss: 1.0121554136276245, val loss: 1.0103576183319092\n",
      "Epoch 2778: train loss: 1.0116761922836304, val loss: 1.009871482849121\n",
      "Epoch 2779: train loss: 1.0111969709396362, val loss: 1.0093859434127808\n",
      "Epoch 2780: train loss: 1.0107181072235107, val loss: 1.0089004039764404\n",
      "Epoch 2781: train loss: 1.0102393627166748, val loss: 1.0084148645401\n",
      "Epoch 2782: train loss: 1.0097607374191284, val loss: 1.0079295635223389\n",
      "Epoch 2783: train loss: 1.009282112121582, val loss: 1.0074446201324463\n",
      "Epoch 2784: train loss: 1.0088038444519043, val loss: 1.0069595575332642\n",
      "Epoch 2785: train loss: 1.0083255767822266, val loss: 1.0064748525619507\n",
      "Epoch 2786: train loss: 1.0078474283218384, val loss: 1.0059903860092163\n",
      "Epoch 2787: train loss: 1.0073696374893188, val loss: 1.005505919456482\n",
      "Epoch 2788: train loss: 1.0068919658660889, val loss: 1.005021572113037\n",
      "Epoch 2789: train loss: 1.0064142942428589, val loss: 1.004537582397461\n",
      "Epoch 2790: train loss: 1.005937099456787, val loss: 1.0040534734725952\n",
      "Epoch 2791: train loss: 1.0054597854614258, val loss: 1.0035698413848877\n",
      "Epoch 2792: train loss: 1.0049824714660645, val loss: 1.0030860900878906\n",
      "Epoch 2793: train loss: 1.0045056343078613, val loss: 1.0026026964187622\n",
      "Epoch 2794: train loss: 1.0040287971496582, val loss: 1.0021194219589233\n",
      "Epoch 2795: train loss: 1.0035520792007446, val loss: 1.001636266708374\n",
      "Epoch 2796: train loss: 1.0030755996704102, val loss: 1.0011534690856934\n",
      "Epoch 2797: train loss: 1.0025992393493652, val loss: 1.0006706714630127\n",
      "Epoch 2798: train loss: 1.002123236656189, val loss: 1.000187873840332\n",
      "Epoch 2799: train loss: 1.0016471147537231, val loss: 0.9997053146362305\n",
      "Epoch 2800: train loss: 1.0011711120605469, val loss: 0.9992231726646423\n",
      "Epoch 2801: train loss: 1.0006954669952393, val loss: 0.9987409710884094\n",
      "Epoch 2802: train loss: 1.0002199411392212, val loss: 0.9982587695121765\n",
      "Epoch 2803: train loss: 0.9997443556785583, val loss: 0.9977772831916809\n",
      "Epoch 2804: train loss: 0.9992691278457642, val loss: 0.9972954988479614\n",
      "Epoch 2805: train loss: 0.9987940788269043, val loss: 0.9968137741088867\n",
      "Epoch 2806: train loss: 0.9983190894126892, val loss: 0.9963324666023254\n",
      "Epoch 2807: train loss: 0.9978442788124084, val loss: 0.9958513379096985\n",
      "Epoch 2808: train loss: 0.9973695874214172, val loss: 0.9953703880310059\n",
      "Epoch 2809: train loss: 0.9968950152397156, val loss: 0.9948894381523132\n",
      "Epoch 2810: train loss: 0.9964206218719482, val loss: 0.9944087862968445\n",
      "Epoch 2811: train loss: 0.9959465861320496, val loss: 0.9939281344413757\n",
      "Epoch 2812: train loss: 0.9954724907875061, val loss: 0.9934478998184204\n",
      "Epoch 2813: train loss: 0.9949985146522522, val loss: 0.9929675459861755\n",
      "Epoch 2814: train loss: 0.9945248365402222, val loss: 0.9924875497817993\n",
      "Epoch 2815: train loss: 0.9940511584281921, val loss: 0.9920076727867126\n",
      "Epoch 2816: train loss: 0.993577778339386, val loss: 0.9915277361869812\n",
      "Epoch 2817: train loss: 0.9931043982505798, val loss: 0.9910484552383423\n",
      "Epoch 2818: train loss: 0.9926313161849976, val loss: 0.9905689358711243\n",
      "Epoch 2819: train loss: 0.9921583533287048, val loss: 0.9900895953178406\n",
      "Epoch 2820: train loss: 0.9916854500770569, val loss: 0.989610493183136\n",
      "Epoch 2821: train loss: 0.9912127256393433, val loss: 0.9891315698623657\n",
      "Epoch 2822: train loss: 0.9907402992248535, val loss: 0.9886528253555298\n",
      "Epoch 2823: train loss: 0.9902678728103638, val loss: 0.9881742596626282\n",
      "Epoch 2824: train loss: 0.9897956252098083, val loss: 0.9876958131790161\n",
      "Epoch 2825: train loss: 0.989323616027832, val loss: 0.9872175455093384\n",
      "Epoch 2826: train loss: 0.9888516068458557, val loss: 0.986739456653595\n",
      "Epoch 2827: train loss: 0.988379955291748, val loss: 0.9862613677978516\n",
      "Epoch 2828: train loss: 0.9879083037376404, val loss: 0.9857836961746216\n",
      "Epoch 2829: train loss: 0.9874367713928223, val loss: 0.9853059649467468\n",
      "Epoch 2830: train loss: 0.986965537071228, val loss: 0.9848284125328064\n",
      "Epoch 2831: train loss: 0.9864943027496338, val loss: 0.9843511581420898\n",
      "Epoch 2832: train loss: 0.9860234260559082, val loss: 0.9838741421699524\n",
      "Epoch 2833: train loss: 0.9855524897575378, val loss: 0.9833970069885254\n",
      "Epoch 2834: train loss: 0.9850817322731018, val loss: 0.9829201698303223\n",
      "Epoch 2835: train loss: 0.9846112132072449, val loss: 0.9824434518814087\n",
      "Epoch 2836: train loss: 0.9841406941413879, val loss: 0.9819669723510742\n",
      "Epoch 2837: train loss: 0.9836705923080444, val loss: 0.9814905524253845\n",
      "Epoch 2838: train loss: 0.9832004308700562, val loss: 0.9810142517089844\n",
      "Epoch 2839: train loss: 0.9827304482460022, val loss: 0.9805383682250977\n",
      "Epoch 2840: train loss: 0.9822607636451721, val loss: 0.9800624251365662\n",
      "Epoch 2841: train loss: 0.9817910194396973, val loss: 0.9795867800712585\n",
      "Epoch 2842: train loss: 0.9813216328620911, val loss: 0.9791113138198853\n",
      "Epoch 2843: train loss: 0.9808523058891296, val loss: 0.9786359071731567\n",
      "Epoch 2844: train loss: 0.9803831577301025, val loss: 0.9781606793403625\n",
      "Epoch 2845: train loss: 0.9799140095710754, val loss: 0.9776856303215027\n",
      "Epoch 2846: train loss: 0.979445219039917, val loss: 0.9772106409072876\n",
      "Epoch 2847: train loss: 0.9789764881134033, val loss: 0.9767358899116516\n",
      "Epoch 2848: train loss: 0.978507936000824, val loss: 0.97626131772995\n",
      "Epoch 2849: train loss: 0.978039562702179, val loss: 0.9757869839668274\n",
      "Epoch 2850: train loss: 0.9775713086128235, val loss: 0.9753127098083496\n",
      "Epoch 2851: train loss: 0.9771031737327576, val loss: 0.9748386740684509\n",
      "Epoch 2852: train loss: 0.9766350984573364, val loss: 0.9743645787239075\n",
      "Epoch 2853: train loss: 0.9761673808097839, val loss: 0.9738909602165222\n",
      "Epoch 2854: train loss: 0.9756997227668762, val loss: 0.9734172821044922\n",
      "Epoch 2855: train loss: 0.9752321839332581, val loss: 0.972943902015686\n",
      "Epoch 2856: train loss: 0.974764883518219, val loss: 0.9724705815315247\n",
      "Epoch 2857: train loss: 0.9742977619171143, val loss: 0.9719974398612976\n",
      "Epoch 2858: train loss: 0.9738306999206543, val loss: 0.9715244174003601\n",
      "Epoch 2859: train loss: 0.9733636975288391, val loss: 0.9710516333580017\n",
      "Epoch 2860: train loss: 0.9728970527648926, val loss: 0.9705789685249329\n",
      "Epoch 2861: train loss: 0.9724304676055908, val loss: 0.970106303691864\n",
      "Epoch 2862: train loss: 0.9719638824462891, val loss: 0.9696341753005981\n",
      "Epoch 2863: train loss: 0.9714975953102112, val loss: 0.9691619873046875\n",
      "Epoch 2864: train loss: 0.9710315465927124, val loss: 0.9686898589134216\n",
      "Epoch 2865: train loss: 0.9705654978752136, val loss: 0.9682179689407349\n",
      "Epoch 2866: train loss: 0.9700995683670044, val loss: 0.967746376991272\n",
      "Epoch 2867: train loss: 0.9696339964866638, val loss: 0.9672747850418091\n",
      "Epoch 2868: train loss: 0.9691684246063232, val loss: 0.9668033719062805\n",
      "Epoch 2869: train loss: 0.9687029719352722, val loss: 0.9663322567939758\n",
      "Epoch 2870: train loss: 0.9682378172874451, val loss: 0.9658611416816711\n",
      "Epoch 2871: train loss: 0.9677727818489075, val loss: 0.9653902053833008\n",
      "Epoch 2872: train loss: 0.9673078060150146, val loss: 0.9649195075035095\n",
      "Epoch 2873: train loss: 0.9668430089950562, val loss: 0.9644489288330078\n",
      "Epoch 2874: train loss: 0.966378390789032, val loss: 0.9639785885810852\n",
      "Epoch 2875: train loss: 0.9659139513969421, val loss: 0.9635082483291626\n",
      "Epoch 2876: train loss: 0.9654495716094971, val loss: 0.9630381464958191\n",
      "Epoch 2877: train loss: 0.9649854302406311, val loss: 0.9625682830810547\n",
      "Epoch 2878: train loss: 0.9645214080810547, val loss: 0.9620985388755798\n",
      "Epoch 2879: train loss: 0.964057445526123, val loss: 0.9616289138793945\n",
      "Epoch 2880: train loss: 0.9635937213897705, val loss: 0.9611594080924988\n",
      "Epoch 2881: train loss: 0.9631302952766418, val loss: 0.9606901407241821\n",
      "Epoch 2882: train loss: 0.9626668691635132, val loss: 0.9602209329605103\n",
      "Epoch 2883: train loss: 0.9622035026550293, val loss: 0.959752082824707\n",
      "Epoch 2884: train loss: 0.9617404937744141, val loss: 0.9592832922935486\n",
      "Epoch 2885: train loss: 0.9612774848937988, val loss: 0.9588145613670349\n",
      "Epoch 2886: train loss: 0.9608145952224731, val loss: 0.9583461880683899\n",
      "Epoch 2887: train loss: 0.9603520035743713, val loss: 0.9578777551651001\n",
      "Epoch 2888: train loss: 0.9598895907402039, val loss: 0.957409679889679\n",
      "Epoch 2889: train loss: 0.9594271779060364, val loss: 0.9569416046142578\n",
      "Epoch 2890: train loss: 0.958965003490448, val loss: 0.9564736485481262\n",
      "Epoch 2891: train loss: 0.9585028886795044, val loss: 0.9560060501098633\n",
      "Epoch 2892: train loss: 0.9580410718917847, val loss: 0.9555384516716003\n",
      "Epoch 2893: train loss: 0.9575792551040649, val loss: 0.9550710916519165\n",
      "Epoch 2894: train loss: 0.9571176767349243, val loss: 0.9546039700508118\n",
      "Epoch 2895: train loss: 0.9566561579704285, val loss: 0.954136848449707\n",
      "Epoch 2896: train loss: 0.9561949372291565, val loss: 0.9536699652671814\n",
      "Epoch 2897: train loss: 0.9557336568832397, val loss: 0.9532032012939453\n",
      "Epoch 2898: train loss: 0.9552727937698364, val loss: 0.9527365565299988\n",
      "Epoch 2899: train loss: 0.9548118710517883, val loss: 0.9522701501846313\n",
      "Epoch 2900: train loss: 0.9543511271476746, val loss: 0.951803982257843\n",
      "Epoch 2901: train loss: 0.9538906812667847, val loss: 0.9513378143310547\n",
      "Epoch 2902: train loss: 0.9534303545951843, val loss: 0.9508719444274902\n",
      "Epoch 2903: train loss: 0.9529700875282288, val loss: 0.9504060745239258\n",
      "Epoch 2904: train loss: 0.9525098204612732, val loss: 0.9499405026435852\n",
      "Epoch 2905: train loss: 0.952049970626831, val loss: 0.9494749307632446\n",
      "Epoch 2906: train loss: 0.9515901803970337, val loss: 0.9490097165107727\n",
      "Epoch 2907: train loss: 0.9511305093765259, val loss: 0.9485446214675903\n",
      "Epoch 2908: train loss: 0.9506710767745972, val loss: 0.9480795860290527\n",
      "Epoch 2909: train loss: 0.9502117037773132, val loss: 0.9476147890090942\n",
      "Epoch 2910: train loss: 0.9497525095939636, val loss: 0.9471500515937805\n",
      "Epoch 2911: train loss: 0.9492933750152588, val loss: 0.9466856122016907\n",
      "Epoch 2912: train loss: 0.9488344192504883, val loss: 0.9462210536003113\n",
      "Epoch 2913: train loss: 0.9483757019042969, val loss: 0.9457569122314453\n",
      "Epoch 2914: train loss: 0.9479172229766846, val loss: 0.9452929496765137\n",
      "Epoch 2915: train loss: 0.9474587440490723, val loss: 0.9448291063308716\n",
      "Epoch 2916: train loss: 0.9470005035400391, val loss: 0.9443654417991638\n",
      "Epoch 2917: train loss: 0.9465422034263611, val loss: 0.9439018368721008\n",
      "Epoch 2918: train loss: 0.9460843801498413, val loss: 0.9434383511543274\n",
      "Epoch 2919: train loss: 0.9456264972686768, val loss: 0.9429752230644226\n",
      "Epoch 2920: train loss: 0.9451688528060913, val loss: 0.9425121545791626\n",
      "Epoch 2921: train loss: 0.9447112679481506, val loss: 0.9420492053031921\n",
      "Epoch 2922: train loss: 0.9442538022994995, val loss: 0.941586434841156\n",
      "Epoch 2923: train loss: 0.9437965750694275, val loss: 0.941123902797699\n",
      "Epoch 2924: train loss: 0.943339467048645, val loss: 0.9406614303588867\n",
      "Epoch 2925: train loss: 0.9428826570510864, val loss: 0.940199077129364\n",
      "Epoch 2926: train loss: 0.9424259066581726, val loss: 0.9397370219230652\n",
      "Epoch 2927: train loss: 0.9419692158699036, val loss: 0.9392749667167664\n",
      "Epoch 2928: train loss: 0.9415126442909241, val loss: 0.9388132095336914\n",
      "Epoch 2929: train loss: 0.9410563111305237, val loss: 0.938351571559906\n",
      "Epoch 2930: train loss: 0.9406002759933472, val loss: 0.9378900527954102\n",
      "Epoch 2931: train loss: 0.9401440024375916, val loss: 0.9374287724494934\n",
      "Epoch 2932: train loss: 0.9396883249282837, val loss: 0.9369675517082214\n",
      "Epoch 2933: train loss: 0.939232587814331, val loss: 0.936506450176239\n",
      "Epoch 2934: train loss: 0.9387769103050232, val loss: 0.93604576587677\n",
      "Epoch 2935: train loss: 0.9383214712142944, val loss: 0.9355850219726562\n",
      "Epoch 2936: train loss: 0.9378662109375, val loss: 0.9351245164871216\n",
      "Epoch 2937: train loss: 0.9374110102653503, val loss: 0.9346639513969421\n",
      "Epoch 2938: train loss: 0.9369560480117798, val loss: 0.9342039227485657\n",
      "Epoch 2939: train loss: 0.9365012645721436, val loss: 0.9337437748908997\n",
      "Epoch 2940: train loss: 0.9360465407371521, val loss: 0.933283805847168\n",
      "Epoch 2941: train loss: 0.935591995716095, val loss: 0.9328240752220154\n",
      "Epoch 2942: train loss: 0.9351376295089722, val loss: 0.9323644638061523\n",
      "Epoch 2943: train loss: 0.9346834421157837, val loss: 0.9319050908088684\n",
      "Epoch 2944: train loss: 0.9342291951179504, val loss: 0.9314457178115845\n",
      "Epoch 2945: train loss: 0.9337753653526306, val loss: 0.9309867024421692\n",
      "Epoch 2946: train loss: 0.933321475982666, val loss: 0.9305276870727539\n",
      "Epoch 2947: train loss: 0.9328678846359253, val loss: 0.9300687909126282\n",
      "Epoch 2948: train loss: 0.9324144124984741, val loss: 0.9296102523803711\n",
      "Epoch 2949: train loss: 0.9319610595703125, val loss: 0.9291519522666931\n",
      "Epoch 2950: train loss: 0.9315078854560852, val loss: 0.9286934733390808\n",
      "Epoch 2951: train loss: 0.9310548901557922, val loss: 0.9282353520393372\n",
      "Epoch 2952: train loss: 0.930601954460144, val loss: 0.9277774095535278\n",
      "Epoch 2953: train loss: 0.9301491975784302, val loss: 0.9273195266723633\n",
      "Epoch 2954: train loss: 0.9296966791152954, val loss: 0.9268617630004883\n",
      "Epoch 2955: train loss: 0.9292441606521606, val loss: 0.9264042973518372\n",
      "Epoch 2956: train loss: 0.9287919402122498, val loss: 0.9259468913078308\n",
      "Epoch 2957: train loss: 0.9283398389816284, val loss: 0.925489604473114\n",
      "Epoch 2958: train loss: 0.9278878569602966, val loss: 0.9250326156616211\n",
      "Epoch 2959: train loss: 0.9274359345436096, val loss: 0.9245756268501282\n",
      "Epoch 2960: train loss: 0.9269842505455017, val loss: 0.9241189360618591\n",
      "Epoch 2961: train loss: 0.9265326857566833, val loss: 0.9236623644828796\n",
      "Epoch 2962: train loss: 0.9260811805725098, val loss: 0.9232059717178345\n",
      "Epoch 2963: train loss: 0.9256300330162048, val loss: 0.9227496981620789\n",
      "Epoch 2964: train loss: 0.9251788854598999, val loss: 0.922293484210968\n",
      "Epoch 2965: train loss: 0.9247279763221741, val loss: 0.9218375086784363\n",
      "Epoch 2966: train loss: 0.9242771863937378, val loss: 0.9213817715644836\n",
      "Epoch 2967: train loss: 0.9238265156745911, val loss: 0.9209260940551758\n",
      "Epoch 2968: train loss: 0.9233759641647339, val loss: 0.920470654964447\n",
      "Epoch 2969: train loss: 0.9229256510734558, val loss: 0.9200153350830078\n",
      "Epoch 2970: train loss: 0.9224754571914673, val loss: 0.9195601344108582\n",
      "Epoch 2971: train loss: 0.9220253825187683, val loss: 0.9191051721572876\n",
      "Epoch 2972: train loss: 0.9215754270553589, val loss: 0.9186501502990723\n",
      "Epoch 2973: train loss: 0.9211256504058838, val loss: 0.9181955456733704\n",
      "Epoch 2974: train loss: 0.9206761121749878, val loss: 0.9177410006523132\n",
      "Epoch 2975: train loss: 0.9202265739440918, val loss: 0.9172865748405457\n",
      "Epoch 2976: train loss: 0.9197772741317749, val loss: 0.9168323874473572\n",
      "Epoch 2977: train loss: 0.9193281531333923, val loss: 0.9163784384727478\n",
      "Epoch 2978: train loss: 0.9188791513442993, val loss: 0.9159244894981384\n",
      "Epoch 2979: train loss: 0.9184302687644958, val loss: 0.9154707193374634\n",
      "Epoch 2980: train loss: 0.9179815649986267, val loss: 0.9150171279907227\n",
      "Epoch 2981: train loss: 0.9175330400466919, val loss: 0.9145636558532715\n",
      "Epoch 2982: train loss: 0.9170846343040466, val loss: 0.9141103029251099\n",
      "Epoch 2983: train loss: 0.9166362285614014, val loss: 0.9136573672294617\n",
      "Epoch 2984: train loss: 0.9161882400512695, val loss: 0.9132043719291687\n",
      "Epoch 2985: train loss: 0.9157403111457825, val loss: 0.9127514958381653\n",
      "Epoch 2986: train loss: 0.9152923822402954, val loss: 0.912298858165741\n",
      "Epoch 2987: train loss: 0.914844810962677, val loss: 0.9118464589118958\n",
      "Epoch 2988: train loss: 0.9143971800804138, val loss: 0.9113941192626953\n",
      "Epoch 2989: train loss: 0.9139499664306641, val loss: 0.9109418988227844\n",
      "Epoch 2990: train loss: 0.9135025143623352, val loss: 0.9104898571968079\n",
      "Epoch 2991: train loss: 0.9130555391311646, val loss: 0.9100379347801208\n",
      "Epoch 2992: train loss: 0.9126086235046387, val loss: 0.9095863699913025\n",
      "Epoch 2993: train loss: 0.9121617674827576, val loss: 0.9091346859931946\n",
      "Epoch 2994: train loss: 0.9117152094841003, val loss: 0.9086834192276001\n",
      "Epoch 2995: train loss: 0.9112688302993774, val loss: 0.9082321524620056\n",
      "Epoch 2996: train loss: 0.9108223915100098, val loss: 0.9077809453010559\n",
      "Epoch 2997: train loss: 0.9103761911392212, val loss: 0.9073299765586853\n",
      "Epoch 2998: train loss: 0.9099301695823669, val loss: 0.9068793654441833\n",
      "Epoch 2999: train loss: 0.9094842076301575, val loss: 0.9064285159111023\n",
      "Epoch 3000: train loss: 0.9090384840965271, val loss: 0.9059783220291138\n",
      "Epoch 3001: train loss: 0.9085928201675415, val loss: 0.9055278897285461\n",
      "Epoch 3002: train loss: 0.9081474542617798, val loss: 0.9050777554512024\n",
      "Epoch 3003: train loss: 0.9077021479606628, val loss: 0.904627799987793\n",
      "Epoch 3004: train loss: 0.9072570204734802, val loss: 0.9041780829429626\n",
      "Epoch 3005: train loss: 0.9068120718002319, val loss: 0.9037283062934875\n",
      "Epoch 3006: train loss: 0.9063671827316284, val loss: 0.9032788276672363\n",
      "Epoch 3007: train loss: 0.9059224724769592, val loss: 0.9028295874595642\n",
      "Epoch 3008: train loss: 0.9054779410362244, val loss: 0.9023804068565369\n",
      "Epoch 3009: train loss: 0.905033528804779, val loss: 0.90193110704422\n",
      "Epoch 3010: train loss: 0.9045892953872681, val loss: 0.9014822840690613\n",
      "Epoch 3011: train loss: 0.9041451811790466, val loss: 0.9010336995124817\n",
      "Epoch 3012: train loss: 0.9037011861801147, val loss: 0.9005849957466125\n",
      "Epoch 3013: train loss: 0.903257429599762, val loss: 0.9001366496086121\n",
      "Epoch 3014: train loss: 0.902813732624054, val loss: 0.8996884226799011\n",
      "Epoch 3015: train loss: 0.9023702144622803, val loss: 0.8992403149604797\n",
      "Epoch 3016: train loss: 0.9019268751144409, val loss: 0.8987922668457031\n",
      "Epoch 3017: train loss: 0.9014835357666016, val loss: 0.8983444571495056\n",
      "Epoch 3018: train loss: 0.9010404944419861, val loss: 0.897896945476532\n",
      "Epoch 3019: train loss: 0.9005976319313049, val loss: 0.8974494338035583\n",
      "Epoch 3020: train loss: 0.9001548290252686, val loss: 0.8970020413398743\n",
      "Epoch 3021: train loss: 0.8997121453285217, val loss: 0.8965549468994141\n",
      "Epoch 3022: train loss: 0.899269700050354, val loss: 0.8961078524589539\n",
      "Epoch 3023: train loss: 0.898827314376831, val loss: 0.8956609964370728\n",
      "Epoch 3024: train loss: 0.8983851075172424, val loss: 0.8952144980430603\n",
      "Epoch 3025: train loss: 0.8979431390762329, val loss: 0.8947677612304688\n",
      "Epoch 3026: train loss: 0.8975012302398682, val loss: 0.8943214416503906\n",
      "Epoch 3027: train loss: 0.8970595002174377, val loss: 0.8938751220703125\n",
      "Epoch 3028: train loss: 0.8966180086135864, val loss: 0.8934291005134583\n",
      "Epoch 3029: train loss: 0.8961765170097351, val loss: 0.8929832577705383\n",
      "Epoch 3030: train loss: 0.8957352042198181, val loss: 0.892537534236908\n",
      "Epoch 3031: train loss: 0.8952939510345459, val loss: 0.8920917510986328\n",
      "Epoch 3032: train loss: 0.8948530554771423, val loss: 0.8916465044021606\n",
      "Epoch 3033: train loss: 0.8944121599197388, val loss: 0.8912011981010437\n",
      "Epoch 3034: train loss: 0.8939713835716248, val loss: 0.8907560706138611\n",
      "Epoch 3035: train loss: 0.8935309052467346, val loss: 0.890311062335968\n",
      "Epoch 3036: train loss: 0.8930904865264893, val loss: 0.889866292476654\n",
      "Epoch 3037: train loss: 0.8926501870155334, val loss: 0.8894214630126953\n",
      "Epoch 3038: train loss: 0.892210066318512, val loss: 0.88897705078125\n",
      "Epoch 3039: train loss: 0.8917701244354248, val loss: 0.8885326385498047\n",
      "Epoch 3040: train loss: 0.8913302421569824, val loss: 0.8880884051322937\n",
      "Epoch 3041: train loss: 0.8908905386924744, val loss: 0.8876444101333618\n",
      "Epoch 3042: train loss: 0.8904510736465454, val loss: 0.8872005343437195\n",
      "Epoch 3043: train loss: 0.8900117874145508, val loss: 0.8867568373680115\n",
      "Epoch 3044: train loss: 0.8895724415779114, val loss: 0.886313259601593\n",
      "Epoch 3045: train loss: 0.8891333937644958, val loss: 0.8858696818351746\n",
      "Epoch 3046: train loss: 0.8886944651603699, val loss: 0.8854266405105591\n",
      "Epoch 3047: train loss: 0.8882555961608887, val loss: 0.8849833607673645\n",
      "Epoch 3048: train loss: 0.8878169059753418, val loss: 0.8845404982566833\n",
      "Epoch 3049: train loss: 0.887378454208374, val loss: 0.884097695350647\n",
      "Epoch 3050: train loss: 0.8869401216506958, val loss: 0.8836550712585449\n",
      "Epoch 3051: train loss: 0.8865019083023071, val loss: 0.8832125067710876\n",
      "Epoch 3052: train loss: 0.8860639333724976, val loss: 0.8827703595161438\n",
      "Epoch 3053: train loss: 0.885625958442688, val loss: 0.8823280334472656\n",
      "Epoch 3054: train loss: 0.8851882219314575, val loss: 0.8818859457969666\n",
      "Epoch 3055: train loss: 0.8847505450248718, val loss: 0.8814440965652466\n",
      "Epoch 3056: train loss: 0.88431316614151, val loss: 0.8810025453567505\n",
      "Epoch 3057: train loss: 0.8838759064674377, val loss: 0.8805608749389648\n",
      "Epoch 3058: train loss: 0.8834386467933655, val loss: 0.8801195025444031\n",
      "Epoch 3059: train loss: 0.8830016255378723, val loss: 0.8796782493591309\n",
      "Epoch 3060: train loss: 0.8825648427009583, val loss: 0.879237174987793\n",
      "Epoch 3061: train loss: 0.8821280598640442, val loss: 0.8787962794303894\n",
      "Epoch 3062: train loss: 0.8816913366317749, val loss: 0.8783555030822754\n",
      "Epoch 3063: train loss: 0.8812549114227295, val loss: 0.8779147267341614\n",
      "Epoch 3064: train loss: 0.8808186054229736, val loss: 0.8774742484092712\n",
      "Epoch 3065: train loss: 0.8803824782371521, val loss: 0.8770341873168945\n",
      "Epoch 3066: train loss: 0.8799465894699097, val loss: 0.8765940070152283\n",
      "Epoch 3067: train loss: 0.8795107007026672, val loss: 0.8761539459228516\n",
      "Epoch 3068: train loss: 0.8790749907493591, val loss: 0.8757140040397644\n",
      "Epoch 3069: train loss: 0.8786393404006958, val loss: 0.8752743005752563\n",
      "Epoch 3070: train loss: 0.8782040476799011, val loss: 0.8748346567153931\n",
      "Epoch 3071: train loss: 0.8777687549591064, val loss: 0.8743953704833984\n",
      "Epoch 3072: train loss: 0.8773335218429565, val loss: 0.8739562034606934\n",
      "Epoch 3073: train loss: 0.8768985271453857, val loss: 0.8735170364379883\n",
      "Epoch 3074: train loss: 0.8764638304710388, val loss: 0.8730781674385071\n",
      "Epoch 3075: train loss: 0.8760291337966919, val loss: 0.8726394772529602\n",
      "Epoch 3076: train loss: 0.8755946159362793, val loss: 0.8722007870674133\n",
      "Epoch 3077: train loss: 0.8751602172851562, val loss: 0.8717624545097351\n",
      "Epoch 3078: train loss: 0.8747259378433228, val loss: 0.8713240623474121\n",
      "Epoch 3079: train loss: 0.8742918372154236, val loss: 0.8708858489990234\n",
      "Epoch 3080: train loss: 0.8738579154014587, val loss: 0.8704479336738586\n",
      "Epoch 3081: train loss: 0.8734241127967834, val loss: 0.8700100779533386\n",
      "Epoch 3082: train loss: 0.8729904294013977, val loss: 0.8695724606513977\n",
      "Epoch 3083: train loss: 0.8725569248199463, val loss: 0.8691349029541016\n",
      "Epoch 3084: train loss: 0.872123658657074, val loss: 0.8686976432800293\n",
      "Epoch 3085: train loss: 0.8716903924942017, val loss: 0.8682603240013123\n",
      "Epoch 3086: train loss: 0.8712573647499084, val loss: 0.8678233027458191\n",
      "Epoch 3087: train loss: 0.87082439661026, val loss: 0.8673862814903259\n",
      "Epoch 3088: train loss: 0.8703916072845459, val loss: 0.8669494986534119\n",
      "Epoch 3089: train loss: 0.8699589967727661, val loss: 0.8665129542350769\n",
      "Epoch 3090: train loss: 0.8695265054702759, val loss: 0.8660764694213867\n",
      "Epoch 3091: train loss: 0.86909419298172, val loss: 0.8656402826309204\n",
      "Epoch 3092: train loss: 0.8686619997024536, val loss: 0.8652041554450989\n",
      "Epoch 3093: train loss: 0.8682300448417664, val loss: 0.8647682070732117\n",
      "Epoch 3094: train loss: 0.8677982091903687, val loss: 0.8643323183059692\n",
      "Epoch 3095: train loss: 0.8673663139343262, val loss: 0.8638967871665955\n",
      "Epoch 3096: train loss: 0.8669347167015076, val loss: 0.8634611368179321\n",
      "Epoch 3097: train loss: 0.8665032386779785, val loss: 0.8630258440971375\n",
      "Epoch 3098: train loss: 0.8660719990730286, val loss: 0.8625906109809875\n",
      "Epoch 3099: train loss: 0.8656407594680786, val loss: 0.862155556678772\n",
      "Epoch 3100: train loss: 0.8652098178863525, val loss: 0.8617206811904907\n",
      "Epoch 3101: train loss: 0.864778995513916, val loss: 0.8612859845161438\n",
      "Epoch 3102: train loss: 0.8643481731414795, val loss: 0.8608512878417969\n",
      "Epoch 3103: train loss: 0.8639175891876221, val loss: 0.8604170083999634\n",
      "Epoch 3104: train loss: 0.863487184047699, val loss: 0.8599824905395508\n",
      "Epoch 3105: train loss: 0.8630569577217102, val loss: 0.8595485091209412\n",
      "Epoch 3106: train loss: 0.8626266717910767, val loss: 0.8591144680976868\n",
      "Epoch 3107: train loss: 0.862196683883667, val loss: 0.8586807250976562\n",
      "Epoch 3108: train loss: 0.8617669343948364, val loss: 0.8582471013069153\n",
      "Epoch 3109: train loss: 0.8613372445106506, val loss: 0.8578134775161743\n",
      "Epoch 3110: train loss: 0.8609076738357544, val loss: 0.857380211353302\n",
      "Epoch 3111: train loss: 0.8604783415794373, val loss: 0.8569470643997192\n",
      "Epoch 3112: train loss: 0.8600489497184753, val loss: 0.8565139174461365\n",
      "Epoch 3113: train loss: 0.8596198558807373, val loss: 0.8560810089111328\n",
      "Epoch 3114: train loss: 0.8591908812522888, val loss: 0.8556483387947083\n",
      "Epoch 3115: train loss: 0.8587621450424194, val loss: 0.8552156686782837\n",
      "Epoch 3116: train loss: 0.8583333492279053, val loss: 0.8547832369804382\n",
      "Epoch 3117: train loss: 0.857904851436615, val loss: 0.8543511629104614\n",
      "Epoch 3118: train loss: 0.8574764728546143, val loss: 0.8539188504219055\n",
      "Epoch 3119: train loss: 0.8570482134819031, val loss: 0.853486955165863\n",
      "Epoch 3120: train loss: 0.8566201329231262, val loss: 0.8530550003051758\n",
      "Epoch 3121: train loss: 0.8561922311782837, val loss: 0.8526234030723572\n",
      "Epoch 3122: train loss: 0.8557643890380859, val loss: 0.8521919250488281\n",
      "Epoch 3123: train loss: 0.8553367257118225, val loss: 0.8517606854438782\n",
      "Epoch 3124: train loss: 0.8549091815948486, val loss: 0.8513294458389282\n",
      "Epoch 3125: train loss: 0.8544817566871643, val loss: 0.8508983850479126\n",
      "Epoch 3126: train loss: 0.8540545105934143, val loss: 0.8504675030708313\n",
      "Epoch 3127: train loss: 0.8536275625228882, val loss: 0.8500367999076843\n",
      "Epoch 3128: train loss: 0.8532006144523621, val loss: 0.8496061563491821\n",
      "Epoch 3129: train loss: 0.852773904800415, val loss: 0.8491756319999695\n",
      "Epoch 3130: train loss: 0.8523471355438232, val loss: 0.8487454652786255\n",
      "Epoch 3131: train loss: 0.8519207835197449, val loss: 0.8483152389526367\n",
      "Epoch 3132: train loss: 0.8514943718910217, val loss: 0.8478853106498718\n",
      "Epoch 3133: train loss: 0.8510681390762329, val loss: 0.847455620765686\n",
      "Epoch 3134: train loss: 0.8506420850753784, val loss: 0.8470258712768555\n",
      "Epoch 3135: train loss: 0.8502162098884583, val loss: 0.8465965390205383\n",
      "Epoch 3136: train loss: 0.8497902750968933, val loss: 0.8461670875549316\n",
      "Epoch 3137: train loss: 0.8493647575378418, val loss: 0.845737874507904\n",
      "Epoch 3138: train loss: 0.8489391803741455, val loss: 0.845308780670166\n",
      "Epoch 3139: train loss: 0.8485139012336731, val loss: 0.8448797464370728\n",
      "Epoch 3140: train loss: 0.8480886816978455, val loss: 0.8444511294364929\n",
      "Epoch 3141: train loss: 0.8476637005805969, val loss: 0.8440225720405579\n",
      "Epoch 3142: train loss: 0.8472388386726379, val loss: 0.8435940146446228\n",
      "Epoch 3143: train loss: 0.846813976764679, val loss: 0.8431658148765564\n",
      "Epoch 3144: train loss: 0.8463894724845886, val loss: 0.84273761510849\n",
      "Epoch 3145: train loss: 0.8459650278091431, val loss: 0.8423096537590027\n",
      "Epoch 3146: train loss: 0.8455407023429871, val loss: 0.8418818712234497\n",
      "Epoch 3147: train loss: 0.8451164364814758, val loss: 0.8414542078971863\n",
      "Epoch 3148: train loss: 0.8446925282478333, val loss: 0.8410267233848572\n",
      "Epoch 3149: train loss: 0.8442685604095459, val loss: 0.8405992388725281\n",
      "Epoch 3150: train loss: 0.8438447713851929, val loss: 0.8401721119880676\n",
      "Epoch 3151: train loss: 0.843421220779419, val loss: 0.839745044708252\n",
      "Epoch 3152: train loss: 0.8429978489875793, val loss: 0.8393180966377258\n",
      "Epoch 3153: train loss: 0.8425744771957397, val loss: 0.8388914465904236\n",
      "Epoch 3154: train loss: 0.8421513438224792, val loss: 0.8384649157524109\n",
      "Epoch 3155: train loss: 0.8417283296585083, val loss: 0.8380383849143982\n",
      "Epoch 3156: train loss: 0.8413053750991821, val loss: 0.8376121520996094\n",
      "Epoch 3157: train loss: 0.8408826589584351, val loss: 0.8371859788894653\n",
      "Epoch 3158: train loss: 0.8404601216316223, val loss: 0.8367599844932556\n",
      "Epoch 3159: train loss: 0.8400377035140991, val loss: 0.8363340497016907\n",
      "Epoch 3160: train loss: 0.8396154046058655, val loss: 0.8359085321426392\n",
      "Epoch 3161: train loss: 0.8391932249069214, val loss: 0.8354830145835876\n",
      "Epoch 3162: train loss: 0.8387713432312012, val loss: 0.8350575566291809\n",
      "Epoch 3163: train loss: 0.8383494019508362, val loss: 0.8346323370933533\n",
      "Epoch 3164: train loss: 0.8379275798797607, val loss: 0.8342072367668152\n",
      "Epoch 3165: train loss: 0.837506115436554, val loss: 0.8337821960449219\n",
      "Epoch 3166: train loss: 0.8370847105979919, val loss: 0.8333576321601868\n",
      "Epoch 3167: train loss: 0.8366634249687195, val loss: 0.8329330682754517\n",
      "Epoch 3168: train loss: 0.8362423777580261, val loss: 0.8325085639953613\n",
      "Epoch 3169: train loss: 0.835821270942688, val loss: 0.8320841789245605\n",
      "Epoch 3170: train loss: 0.835400402545929, val loss: 0.8316599130630493\n",
      "Epoch 3171: train loss: 0.834979772567749, val loss: 0.8312360048294067\n",
      "Epoch 3172: train loss: 0.8345591425895691, val loss: 0.8308122754096985\n",
      "Epoch 3173: train loss: 0.8341387510299683, val loss: 0.8303883671760559\n",
      "Epoch 3174: train loss: 0.833718478679657, val loss: 0.8299649357795715\n",
      "Epoch 3175: train loss: 0.8332984447479248, val loss: 0.8295415043830872\n",
      "Epoch 3176: train loss: 0.8328784108161926, val loss: 0.8291181921958923\n",
      "Epoch 3177: train loss: 0.83245849609375, val loss: 0.8286952376365662\n",
      "Epoch 3178: train loss: 0.8320388793945312, val loss: 0.8282724618911743\n",
      "Epoch 3179: train loss: 0.831619381904602, val loss: 0.8278493881225586\n",
      "Epoch 3180: train loss: 0.8312000036239624, val loss: 0.8274268507957458\n",
      "Epoch 3181: train loss: 0.8307807445526123, val loss: 0.8270043730735779\n",
      "Epoch 3182: train loss: 0.8303616046905518, val loss: 0.8265820741653442\n",
      "Epoch 3183: train loss: 0.8299425840377808, val loss: 0.8261597752571106\n",
      "Epoch 3184: train loss: 0.8295237421989441, val loss: 0.8257378935813904\n",
      "Epoch 3185: train loss: 0.8291051387786865, val loss: 0.8253159523010254\n",
      "Epoch 3186: train loss: 0.828686535358429, val loss: 0.824894368648529\n",
      "Epoch 3187: train loss: 0.8282681703567505, val loss: 0.8244726061820984\n",
      "Epoch 3188: train loss: 0.8278498649597168, val loss: 0.8240513205528259\n",
      "Epoch 3189: train loss: 0.827431857585907, val loss: 0.8236299753189087\n",
      "Epoch 3190: train loss: 0.8270138502120972, val loss: 0.8232088088989258\n",
      "Epoch 3191: train loss: 0.8265959024429321, val loss: 0.822787880897522\n",
      "Epoch 3192: train loss: 0.8261783123016357, val loss: 0.8223671317100525\n",
      "Epoch 3193: train loss: 0.8257608413696289, val loss: 0.8219464421272278\n",
      "Epoch 3194: train loss: 0.8253434300422668, val loss: 0.8215258717536926\n",
      "Epoch 3195: train loss: 0.8249262571334839, val loss: 0.8211055994033813\n",
      "Epoch 3196: train loss: 0.8245092034339905, val loss: 0.8206853270530701\n",
      "Epoch 3197: train loss: 0.8240921497344971, val loss: 0.8202652931213379\n",
      "Epoch 3198: train loss: 0.823675274848938, val loss: 0.8198453783988953\n",
      "Epoch 3199: train loss: 0.823258638381958, val loss: 0.8194257616996765\n",
      "Epoch 3200: train loss: 0.8228421807289124, val loss: 0.819006085395813\n",
      "Epoch 3201: train loss: 0.8224256634712219, val loss: 0.8185867667198181\n",
      "Epoch 3202: train loss: 0.8220095634460449, val loss: 0.818167507648468\n",
      "Epoch 3203: train loss: 0.8215935230255127, val loss: 0.8177482485771179\n",
      "Epoch 3204: train loss: 0.8211775422096252, val loss: 0.8173293471336365\n",
      "Epoch 3205: train loss: 0.8207617998123169, val loss: 0.8169103860855103\n",
      "Epoch 3206: train loss: 0.8203461170196533, val loss: 0.8164917230606079\n",
      "Epoch 3207: train loss: 0.8199306130409241, val loss: 0.8160732388496399\n",
      "Epoch 3208: train loss: 0.8195151686668396, val loss: 0.8156548738479614\n",
      "Epoch 3209: train loss: 0.8190999627113342, val loss: 0.8152366876602173\n",
      "Epoch 3210: train loss: 0.8186849355697632, val loss: 0.8148185014724731\n",
      "Epoch 3211: train loss: 0.8182699680328369, val loss: 0.8144006133079529\n",
      "Epoch 3212: train loss: 0.817855179309845, val loss: 0.8139827847480774\n",
      "Epoch 3213: train loss: 0.8174404501914978, val loss: 0.813565194606781\n",
      "Epoch 3214: train loss: 0.8170259594917297, val loss: 0.8131477236747742\n",
      "Epoch 3215: train loss: 0.8166115880012512, val loss: 0.8127304315567017\n",
      "Epoch 3216: train loss: 0.816197395324707, val loss: 0.8123132586479187\n",
      "Epoch 3217: train loss: 0.8157833814620972, val loss: 0.8118961453437805\n",
      "Epoch 3218: train loss: 0.8153693675994873, val loss: 0.8114792704582214\n",
      "Epoch 3219: train loss: 0.8149556517601013, val loss: 0.8110626339912415\n",
      "Epoch 3220: train loss: 0.8145419955253601, val loss: 0.8106460571289062\n",
      "Epoch 3221: train loss: 0.8141284584999084, val loss: 0.8102297186851501\n",
      "Epoch 3222: train loss: 0.8137151002883911, val loss: 0.8098134398460388\n",
      "Epoch 3223: train loss: 0.8133019804954529, val loss: 0.8093973398208618\n",
      "Epoch 3224: train loss: 0.8128888010978699, val loss: 0.8089813590049744\n",
      "Epoch 3225: train loss: 0.8124758005142212, val loss: 0.8085655570030212\n",
      "Epoch 3226: train loss: 0.8120630979537964, val loss: 0.8081498146057129\n",
      "Epoch 3227: train loss: 0.8116504549980164, val loss: 0.8077344298362732\n",
      "Epoch 3228: train loss: 0.8112379312515259, val loss: 0.8073191046714783\n",
      "Epoch 3229: train loss: 0.8108256459236145, val loss: 0.8069039583206177\n",
      "Epoch 3230: train loss: 0.8104133009910583, val loss: 0.8064888119697571\n",
      "Epoch 3231: train loss: 0.8100013136863708, val loss: 0.8060739636421204\n",
      "Epoch 3232: train loss: 0.8095893859863281, val loss: 0.8056592345237732\n",
      "Epoch 3233: train loss: 0.8091776967048645, val loss: 0.8052446246147156\n",
      "Epoch 3234: train loss: 0.8087660074234009, val loss: 0.8048300743103027\n",
      "Epoch 3235: train loss: 0.8083544373512268, val loss: 0.8044158816337585\n",
      "Epoch 3236: train loss: 0.8079431653022766, val loss: 0.8040017485618591\n",
      "Epoch 3237: train loss: 0.8075319528579712, val loss: 0.8035877346992493\n",
      "Epoch 3238: train loss: 0.8071208000183105, val loss: 0.803173840045929\n",
      "Epoch 3239: train loss: 0.806709885597229, val loss: 0.8027602434158325\n",
      "Epoch 3240: train loss: 0.8062992095947266, val loss: 0.8023467063903809\n",
      "Epoch 3241: train loss: 0.8058884739875793, val loss: 0.8019332885742188\n",
      "Epoch 3242: train loss: 0.8054780960083008, val loss: 0.801520049571991\n",
      "Epoch 3243: train loss: 0.8050677180290222, val loss: 0.8011070489883423\n",
      "Epoch 3244: train loss: 0.804657518863678, val loss: 0.8006939888000488\n",
      "Epoch 3245: train loss: 0.8042473793029785, val loss: 0.8002813458442688\n",
      "Epoch 3246: train loss: 0.8038374185562134, val loss: 0.7998687028884888\n",
      "Epoch 3247: train loss: 0.8034276366233826, val loss: 0.7994560599327087\n",
      "Epoch 3248: train loss: 0.8030180335044861, val loss: 0.7990438342094421\n",
      "Epoch 3249: train loss: 0.8026086091995239, val loss: 0.7986316680908203\n",
      "Epoch 3250: train loss: 0.8021992444992065, val loss: 0.798219621181488\n",
      "Epoch 3251: train loss: 0.8017900586128235, val loss: 0.7978077530860901\n",
      "Epoch 3252: train loss: 0.8013809323310852, val loss: 0.7973960041999817\n",
      "Epoch 3253: train loss: 0.8009719848632812, val loss: 0.7969844937324524\n",
      "Epoch 3254: train loss: 0.8005632162094116, val loss: 0.7965729236602783\n",
      "Epoch 3255: train loss: 0.800154447555542, val loss: 0.7961616516113281\n",
      "Epoch 3256: train loss: 0.799746036529541, val loss: 0.7957505583763123\n",
      "Epoch 3257: train loss: 0.7993376851081848, val loss: 0.7953395247459412\n",
      "Epoch 3258: train loss: 0.7989294528961182, val loss: 0.7949287295341492\n",
      "Epoch 3259: train loss: 0.7985214591026306, val loss: 0.7945181727409363\n",
      "Epoch 3260: train loss: 0.7981135249137878, val loss: 0.7941076159477234\n",
      "Epoch 3261: train loss: 0.7977057695388794, val loss: 0.7936971783638\n",
      "Epoch 3262: train loss: 0.797298014163971, val loss: 0.7932869791984558\n",
      "Epoch 3263: train loss: 0.7968906164169312, val loss: 0.7928768396377563\n",
      "Epoch 3264: train loss: 0.7964832186698914, val loss: 0.792466938495636\n",
      "Epoch 3265: train loss: 0.7960759401321411, val loss: 0.7920571565628052\n",
      "Epoch 3266: train loss: 0.79566890001297, val loss: 0.7916474342346191\n",
      "Epoch 3267: train loss: 0.7952619791030884, val loss: 0.791238009929657\n",
      "Epoch 3268: train loss: 0.7948551774024963, val loss: 0.7908288240432739\n",
      "Epoch 3269: train loss: 0.7944485545158386, val loss: 0.7904195189476013\n",
      "Epoch 3270: train loss: 0.79404217004776, val loss: 0.7900105714797974\n",
      "Epoch 3271: train loss: 0.7936357259750366, val loss: 0.789601743221283\n",
      "Epoch 3272: train loss: 0.7932295203208923, val loss: 0.7891928553581238\n",
      "Epoch 3273: train loss: 0.7928233742713928, val loss: 0.7887843251228333\n",
      "Epoch 3274: train loss: 0.7924174070358276, val loss: 0.7883760333061218\n",
      "Epoch 3275: train loss: 0.7920116186141968, val loss: 0.7879676818847656\n",
      "Epoch 3276: train loss: 0.7916060090065002, val loss: 0.7875595688819885\n",
      "Epoch 3277: train loss: 0.7912006378173828, val loss: 0.7871515154838562\n",
      "Epoch 3278: train loss: 0.7907951474189758, val loss: 0.7867437601089478\n",
      "Epoch 3279: train loss: 0.7903899550437927, val loss: 0.7863361239433289\n",
      "Epoch 3280: train loss: 0.7899850010871887, val loss: 0.7859286665916443\n",
      "Epoch 3281: train loss: 0.7895800471305847, val loss: 0.7855212688446045\n",
      "Epoch 3282: train loss: 0.7891750931739807, val loss: 0.7851139903068542\n",
      "Epoch 3283: train loss: 0.7887705564498901, val loss: 0.7847068905830383\n",
      "Epoch 3284: train loss: 0.7883660197257996, val loss: 0.784299910068512\n",
      "Epoch 3285: train loss: 0.7879616022109985, val loss: 0.7838931679725647\n",
      "Epoch 3286: train loss: 0.7875574231147766, val loss: 0.783486545085907\n",
      "Epoch 3287: train loss: 0.7871533036231995, val loss: 0.7830801010131836\n",
      "Epoch 3288: train loss: 0.7867493629455566, val loss: 0.7826737761497498\n",
      "Epoch 3289: train loss: 0.7863456010818481, val loss: 0.7822676301002502\n",
      "Epoch 3290: train loss: 0.7859418988227844, val loss: 0.7818616628646851\n",
      "Epoch 3291: train loss: 0.7855384349822998, val loss: 0.7814556956291199\n",
      "Epoch 3292: train loss: 0.7851349711418152, val loss: 0.7810500264167786\n",
      "Epoch 3293: train loss: 0.7847318053245544, val loss: 0.780644416809082\n",
      "Epoch 3294: train loss: 0.7843287587165833, val loss: 0.7802390456199646\n",
      "Epoch 3295: train loss: 0.7839256525039673, val loss: 0.7798336148262024\n",
      "Epoch 3296: train loss: 0.7835229635238647, val loss: 0.7794286012649536\n",
      "Epoch 3297: train loss: 0.783120334148407, val loss: 0.7790235877037048\n",
      "Epoch 3298: train loss: 0.782717764377594, val loss: 0.7786188125610352\n",
      "Epoch 3299: train loss: 0.7823154330253601, val loss: 0.778214156627655\n",
      "Epoch 3300: train loss: 0.7819132804870605, val loss: 0.7778095602989197\n",
      "Epoch 3301: train loss: 0.781511127948761, val loss: 0.7774052023887634\n",
      "Epoch 3302: train loss: 0.7811090350151062, val loss: 0.7770010828971863\n",
      "Epoch 3303: train loss: 0.7807072997093201, val loss: 0.7765969634056091\n",
      "Epoch 3304: train loss: 0.7803055644035339, val loss: 0.7761930823326111\n",
      "Epoch 3305: train loss: 0.7799040079116821, val loss: 0.775789201259613\n",
      "Epoch 3306: train loss: 0.7795026898384094, val loss: 0.7753854393959045\n",
      "Epoch 3307: train loss: 0.7791014313697815, val loss: 0.7749819159507751\n",
      "Epoch 3308: train loss: 0.7787001729011536, val loss: 0.7745785117149353\n",
      "Epoch 3309: train loss: 0.7782991528511047, val loss: 0.774175226688385\n",
      "Epoch 3310: train loss: 0.7778982520103455, val loss: 0.7737723588943481\n",
      "Epoch 3311: train loss: 0.7774977087974548, val loss: 0.7733694911003113\n",
      "Epoch 3312: train loss: 0.777097225189209, val loss: 0.772966742515564\n",
      "Epoch 3313: train loss: 0.7766968607902527, val loss: 0.7725641131401062\n",
      "Epoch 3314: train loss: 0.7762966156005859, val loss: 0.7721616625785828\n",
      "Epoch 3315: train loss: 0.7758964896202087, val loss: 0.7717593312263489\n",
      "Epoch 3316: train loss: 0.7754964232444763, val loss: 0.7713571786880493\n",
      "Epoch 3317: train loss: 0.7750964760780334, val loss: 0.7709552049636841\n",
      "Epoch 3318: train loss: 0.7746968865394592, val loss: 0.7705534100532532\n",
      "Epoch 3319: train loss: 0.7742973566055298, val loss: 0.7701517343521118\n",
      "Epoch 3320: train loss: 0.7738980054855347, val loss: 0.7697502374649048\n",
      "Epoch 3321: train loss: 0.7734987139701843, val loss: 0.7693487405776978\n",
      "Epoch 3322: train loss: 0.773099422454834, val loss: 0.768947422504425\n",
      "Epoch 3323: train loss: 0.7727003693580627, val loss: 0.7685464024543762\n",
      "Epoch 3324: train loss: 0.7723016142845154, val loss: 0.7681455612182617\n",
      "Epoch 3325: train loss: 0.7719030976295471, val loss: 0.7677448391914368\n",
      "Epoch 3326: train loss: 0.7715045809745789, val loss: 0.7673441767692566\n",
      "Epoch 3327: train loss: 0.7711061239242554, val loss: 0.7669435739517212\n",
      "Epoch 3328: train loss: 0.7707077860832214, val loss: 0.7665432691574097\n",
      "Epoch 3329: train loss: 0.7703096270561218, val loss: 0.7661429047584534\n",
      "Epoch 3330: train loss: 0.769911527633667, val loss: 0.7657429575920105\n",
      "Epoch 3331: train loss: 0.769513726234436, val loss: 0.7653431296348572\n",
      "Epoch 3332: train loss: 0.7691161632537842, val loss: 0.7649433016777039\n",
      "Epoch 3333: train loss: 0.7687184810638428, val loss: 0.7645436525344849\n",
      "Epoch 3334: train loss: 0.7683210968971252, val loss: 0.7641441822052002\n",
      "Epoch 3335: train loss: 0.7679237723350525, val loss: 0.7637448906898499\n",
      "Epoch 3336: train loss: 0.7675266265869141, val loss: 0.76334547996521\n",
      "Epoch 3337: train loss: 0.7671293616294861, val loss: 0.762946605682373\n",
      "Epoch 3338: train loss: 0.7667326331138611, val loss: 0.7625477313995361\n",
      "Epoch 3339: train loss: 0.7663360834121704, val loss: 0.7621491551399231\n",
      "Epoch 3340: train loss: 0.7659394145011902, val loss: 0.7617504000663757\n",
      "Epoch 3341: train loss: 0.7655429840087891, val loss: 0.761352002620697\n",
      "Epoch 3342: train loss: 0.7651466727256775, val loss: 0.7609536051750183\n",
      "Epoch 3343: train loss: 0.7647503614425659, val loss: 0.760555624961853\n",
      "Epoch 3344: train loss: 0.7643544673919678, val loss: 0.7601577043533325\n",
      "Epoch 3345: train loss: 0.7639585733413696, val loss: 0.7597599029541016\n",
      "Epoch 3346: train loss: 0.7635629773139954, val loss: 0.7593622207641602\n",
      "Epoch 3347: train loss: 0.7631673216819763, val loss: 0.7589647173881531\n",
      "Epoch 3348: train loss: 0.7627717852592468, val loss: 0.7585672736167908\n",
      "Epoch 3349: train loss: 0.7623764276504517, val loss: 0.7581698894500732\n",
      "Epoch 3350: train loss: 0.7619812488555908, val loss: 0.7577729225158691\n",
      "Epoch 3351: train loss: 0.7615861892700195, val loss: 0.7573760151863098\n",
      "Epoch 3352: train loss: 0.7611913084983826, val loss: 0.7569792866706848\n",
      "Epoch 3353: train loss: 0.7607966065406799, val loss: 0.7565825581550598\n",
      "Epoch 3354: train loss: 0.7604020237922668, val loss: 0.7561860680580139\n",
      "Epoch 3355: train loss: 0.760007381439209, val loss: 0.7557896971702576\n",
      "Epoch 3356: train loss: 0.759613037109375, val loss: 0.7553933262825012\n",
      "Epoch 3357: train loss: 0.759218692779541, val loss: 0.7549973726272583\n",
      "Epoch 3358: train loss: 0.7588247656822205, val loss: 0.7546014189720154\n",
      "Epoch 3359: train loss: 0.7584310173988342, val loss: 0.7542057037353516\n",
      "Epoch 3360: train loss: 0.7580371499061584, val loss: 0.7538100481033325\n",
      "Epoch 3361: train loss: 0.757643461227417, val loss: 0.7534146308898926\n",
      "Epoch 3362: train loss: 0.7572499513626099, val loss: 0.7530190944671631\n",
      "Epoch 3363: train loss: 0.7568565607070923, val loss: 0.7526241540908813\n",
      "Epoch 3364: train loss: 0.7564634680747986, val loss: 0.7522291541099548\n",
      "Epoch 3365: train loss: 0.7560703158378601, val loss: 0.7518343925476074\n",
      "Epoch 3366: train loss: 0.7556774616241455, val loss: 0.7514396905899048\n",
      "Epoch 3367: train loss: 0.7552846670150757, val loss: 0.7510450482368469\n",
      "Epoch 3368: train loss: 0.7548919916152954, val loss: 0.7506504058837891\n",
      "Epoch 3369: train loss: 0.7544994354248047, val loss: 0.7502561211585999\n",
      "Epoch 3370: train loss: 0.7541069984436035, val loss: 0.7498621344566345\n",
      "Epoch 3371: train loss: 0.7537147998809814, val loss: 0.7494681477546692\n",
      "Epoch 3372: train loss: 0.7533227801322937, val loss: 0.749074399471283\n",
      "Epoch 3373: train loss: 0.7529308795928955, val loss: 0.7486806511878967\n",
      "Epoch 3374: train loss: 0.7525389790534973, val loss: 0.7482870221138\n",
      "Epoch 3375: train loss: 0.7521472573280334, val loss: 0.7478935718536377\n",
      "Epoch 3376: train loss: 0.7517556548118591, val loss: 0.7475003004074097\n",
      "Epoch 3377: train loss: 0.7513641715049744, val loss: 0.7471072673797607\n",
      "Epoch 3378: train loss: 0.7509730458259583, val loss: 0.7467142939567566\n",
      "Epoch 3379: train loss: 0.7505818605422974, val loss: 0.7463213801383972\n",
      "Epoch 3380: train loss: 0.7501909136772156, val loss: 0.7459287643432617\n",
      "Epoch 3381: train loss: 0.7498001456260681, val loss: 0.745536208152771\n",
      "Epoch 3382: train loss: 0.7494093179702759, val loss: 0.7451438307762146\n",
      "Epoch 3383: train loss: 0.7490187883377075, val loss: 0.7447516322135925\n",
      "Epoch 3384: train loss: 0.7486284375190735, val loss: 0.7443596720695496\n",
      "Epoch 3385: train loss: 0.7482381463050842, val loss: 0.7439677119255066\n",
      "Epoch 3386: train loss: 0.7478480935096741, val loss: 0.7435759902000427\n",
      "Epoch 3387: train loss: 0.7474581003189087, val loss: 0.7431842684745789\n",
      "Epoch 3388: train loss: 0.7470681071281433, val loss: 0.7427927851676941\n",
      "Epoch 3389: train loss: 0.7466784119606018, val loss: 0.7424014210700989\n",
      "Epoch 3390: train loss: 0.7462887167930603, val loss: 0.7420102953910828\n",
      "Epoch 3391: train loss: 0.7458993196487427, val loss: 0.7416192889213562\n",
      "Epoch 3392: train loss: 0.7455102205276489, val loss: 0.7412284016609192\n",
      "Epoch 3393: train loss: 0.7451209425926208, val loss: 0.740837574005127\n",
      "Epoch 3394: train loss: 0.7447320222854614, val loss: 0.7404469847679138\n",
      "Epoch 3395: train loss: 0.7443429827690125, val loss: 0.7400563955307007\n",
      "Epoch 3396: train loss: 0.7439543008804321, val loss: 0.7396661639213562\n",
      "Epoch 3397: train loss: 0.7435657382011414, val loss: 0.7392761707305908\n",
      "Epoch 3398: train loss: 0.7431772947311401, val loss: 0.7388862371444702\n",
      "Epoch 3399: train loss: 0.7427890300750732, val loss: 0.7384962439537048\n",
      "Epoch 3400: train loss: 0.7424008250236511, val loss: 0.7381065487861633\n",
      "Epoch 3401: train loss: 0.7420127987861633, val loss: 0.7377169728279114\n",
      "Epoch 3402: train loss: 0.7416248321533203, val loss: 0.7373273968696594\n",
      "Epoch 3403: train loss: 0.7412369847297668, val loss: 0.7369381785392761\n",
      "Epoch 3404: train loss: 0.740849494934082, val loss: 0.7365493178367615\n",
      "Epoch 3405: train loss: 0.7404620051383972, val loss: 0.7361602187156677\n",
      "Epoch 3406: train loss: 0.7400746941566467, val loss: 0.7357713580131531\n",
      "Epoch 3407: train loss: 0.739687442779541, val loss: 0.7353826761245728\n",
      "Epoch 3408: train loss: 0.7393003702163696, val loss: 0.7349939346313477\n",
      "Epoch 3409: train loss: 0.7389134168624878, val loss: 0.734605610370636\n",
      "Epoch 3410: train loss: 0.7385265231132507, val loss: 0.7342172861099243\n",
      "Epoch 3411: train loss: 0.7381398677825928, val loss: 0.7338292598724365\n",
      "Epoch 3412: train loss: 0.7377535104751587, val loss: 0.7334412932395935\n",
      "Epoch 3413: train loss: 0.7373670339584351, val loss: 0.7330535054206848\n",
      "Epoch 3414: train loss: 0.7369807958602905, val loss: 0.7326658368110657\n",
      "Epoch 3415: train loss: 0.7365946769714355, val loss: 0.7322781682014465\n",
      "Epoch 3416: train loss: 0.7362086772918701, val loss: 0.7318909764289856\n",
      "Epoch 3417: train loss: 0.7358229756355286, val loss: 0.7315036654472351\n",
      "Epoch 3418: train loss: 0.7354373931884766, val loss: 0.7311167120933533\n",
      "Epoch 3419: train loss: 0.7350518107414246, val loss: 0.7307297587394714\n",
      "Epoch 3420: train loss: 0.7346664667129517, val loss: 0.7303429841995239\n",
      "Epoch 3421: train loss: 0.734281063079834, val loss: 0.7299562096595764\n",
      "Epoch 3422: train loss: 0.7338958978652954, val loss: 0.7295696139335632\n",
      "Epoch 3423: train loss: 0.7335108518600464, val loss: 0.7291834354400635\n",
      "Epoch 3424: train loss: 0.733126163482666, val loss: 0.7287973761558533\n",
      "Epoch 3425: train loss: 0.7327414751052856, val loss: 0.7284111976623535\n",
      "Epoch 3426: train loss: 0.7323569655418396, val loss: 0.7280252575874329\n",
      "Epoch 3427: train loss: 0.7319724559783936, val loss: 0.7276394367218018\n",
      "Epoch 3428: train loss: 0.7315880656242371, val loss: 0.7272538542747498\n",
      "Epoch 3429: train loss: 0.7312039136886597, val loss: 0.7268684506416321\n",
      "Epoch 3430: train loss: 0.7308200001716614, val loss: 0.7264832258224487\n",
      "Epoch 3431: train loss: 0.7304362058639526, val loss: 0.7260980606079102\n",
      "Epoch 3432: train loss: 0.7300525307655334, val loss: 0.7257129549980164\n",
      "Epoch 3433: train loss: 0.7296688556671143, val loss: 0.7253280878067017\n",
      "Epoch 3434: train loss: 0.7292854189872742, val loss: 0.7249431610107422\n",
      "Epoch 3435: train loss: 0.7289020419120789, val loss: 0.7245587110519409\n",
      "Epoch 3436: train loss: 0.7285188436508179, val loss: 0.7241743206977844\n",
      "Epoch 3437: train loss: 0.728135883808136, val loss: 0.7237899303436279\n",
      "Epoch 3438: train loss: 0.7277530431747437, val loss: 0.7234058380126953\n",
      "Epoch 3439: train loss: 0.7273702025413513, val loss: 0.723021924495697\n",
      "Epoch 3440: train loss: 0.7269876599311829, val loss: 0.722637951374054\n",
      "Epoch 3441: train loss: 0.7266050577163696, val loss: 0.72225421667099\n",
      "Epoch 3442: train loss: 0.7262226939201355, val loss: 0.7218704223632812\n",
      "Epoch 3443: train loss: 0.7258404493331909, val loss: 0.7214872241020203\n",
      "Epoch 3444: train loss: 0.7254583835601807, val loss: 0.721103847026825\n",
      "Epoch 3445: train loss: 0.7250765562057495, val loss: 0.7207207083702087\n",
      "Epoch 3446: train loss: 0.7246947288513184, val loss: 0.7203377485275269\n",
      "Epoch 3447: train loss: 0.7243130207061768, val loss: 0.7199549078941345\n",
      "Epoch 3448: train loss: 0.7239315509796143, val loss: 0.7195720672607422\n",
      "Epoch 3449: train loss: 0.723550021648407, val loss: 0.7191895842552185\n",
      "Epoch 3450: train loss: 0.7231688499450684, val loss: 0.7188073396682739\n",
      "Epoch 3451: train loss: 0.7227879166603088, val loss: 0.7184250950813293\n",
      "Epoch 3452: train loss: 0.7224069833755493, val loss: 0.7180429697036743\n",
      "Epoch 3453: train loss: 0.7220260500907898, val loss: 0.7176610827445984\n",
      "Epoch 3454: train loss: 0.7216453552246094, val loss: 0.7172790765762329\n",
      "Epoch 3455: train loss: 0.7212648391723633, val loss: 0.7168972492218018\n",
      "Epoch 3456: train loss: 0.7208843231201172, val loss: 0.716515839099884\n",
      "Epoch 3457: train loss: 0.720504105091095, val loss: 0.7161344289779663\n",
      "Epoch 3458: train loss: 0.7201240062713623, val loss: 0.7157532572746277\n",
      "Epoch 3459: train loss: 0.7197440266609192, val loss: 0.7153720855712891\n",
      "Epoch 3460: train loss: 0.7193641662597656, val loss: 0.7149911522865295\n",
      "Epoch 3461: train loss: 0.7189844250679016, val loss: 0.7146102786064148\n",
      "Epoch 3462: train loss: 0.7186047434806824, val loss: 0.7142296433448792\n",
      "Epoch 3463: train loss: 0.718225359916687, val loss: 0.7138491272926331\n",
      "Epoch 3464: train loss: 0.7178462147712708, val loss: 0.7134687304496765\n",
      "Epoch 3465: train loss: 0.7174669504165649, val loss: 0.7130886912345886\n",
      "Epoch 3466: train loss: 0.717087984085083, val loss: 0.712708592414856\n",
      "Epoch 3467: train loss: 0.7167090773582458, val loss: 0.7123286128044128\n",
      "Epoch 3468: train loss: 0.7163302302360535, val loss: 0.711948573589325\n",
      "Epoch 3469: train loss: 0.7159515619277954, val loss: 0.71156907081604\n",
      "Epoch 3470: train loss: 0.7155731320381165, val loss: 0.7111896872520447\n",
      "Epoch 3471: train loss: 0.7151948809623718, val loss: 0.7108103036880493\n",
      "Epoch 3472: train loss: 0.7148167490959167, val loss: 0.7104310393333435\n",
      "Epoch 3473: train loss: 0.7144385576248169, val loss: 0.710051953792572\n",
      "Epoch 3474: train loss: 0.7140606641769409, val loss: 0.7096730470657349\n",
      "Epoch 3475: train loss: 0.7136828303337097, val loss: 0.7092942595481873\n",
      "Epoch 3476: train loss: 0.7133052349090576, val loss: 0.708915650844574\n",
      "Epoch 3477: train loss: 0.7129277586936951, val loss: 0.7085372805595398\n",
      "Epoch 3478: train loss: 0.7125503420829773, val loss: 0.7081589698791504\n",
      "Epoch 3479: train loss: 0.7121732234954834, val loss: 0.707780659198761\n",
      "Epoch 3480: train loss: 0.7117961049079895, val loss: 0.7074025273323059\n",
      "Epoch 3481: train loss: 0.7114190459251404, val loss: 0.7070246934890747\n",
      "Epoch 3482: train loss: 0.7110422253608704, val loss: 0.7066469192504883\n",
      "Epoch 3483: train loss: 0.7106655836105347, val loss: 0.706269383430481\n",
      "Epoch 3484: train loss: 0.7102890610694885, val loss: 0.7058919072151184\n",
      "Epoch 3485: train loss: 0.7099126577377319, val loss: 0.7055146098136902\n",
      "Epoch 3486: train loss: 0.7095364332199097, val loss: 0.7051374316215515\n",
      "Epoch 3487: train loss: 0.7091602683067322, val loss: 0.7047602534294128\n",
      "Epoch 3488: train loss: 0.7087841629981995, val loss: 0.7043834924697876\n",
      "Epoch 3489: train loss: 0.7084084749221802, val loss: 0.7040068507194519\n",
      "Epoch 3490: train loss: 0.7080327272415161, val loss: 0.7036301493644714\n",
      "Epoch 3491: train loss: 0.7076572179794312, val loss: 0.7032538056373596\n",
      "Epoch 3492: train loss: 0.7072817087173462, val loss: 0.7028774619102478\n",
      "Epoch 3493: train loss: 0.7069063782691956, val loss: 0.7025011777877808\n",
      "Epoch 3494: train loss: 0.7065312266349792, val loss: 0.702125072479248\n",
      "Epoch 3495: train loss: 0.7061560750007629, val loss: 0.701749324798584\n",
      "Epoch 3496: train loss: 0.7057812809944153, val loss: 0.7013736963272095\n",
      "Epoch 3497: train loss: 0.7054065465927124, val loss: 0.7009980082511902\n",
      "Epoch 3498: train loss: 0.7050319910049438, val loss: 0.7006226778030396\n",
      "Epoch 3499: train loss: 0.7046574354171753, val loss: 0.7002472281455994\n",
      "Epoch 3500: train loss: 0.7042831182479858, val loss: 0.6998721361160278\n",
      "Epoch 3501: train loss: 0.7039088010787964, val loss: 0.6994971632957458\n",
      "Epoch 3502: train loss: 0.7035348415374756, val loss: 0.6991223692893982\n",
      "Epoch 3503: train loss: 0.7031609416007996, val loss: 0.6987476944923401\n",
      "Epoch 3504: train loss: 0.7027872204780579, val loss: 0.6983730792999268\n",
      "Epoch 3505: train loss: 0.7024136185646057, val loss: 0.6979987025260925\n",
      "Epoch 3506: train loss: 0.7020400166511536, val loss: 0.6976242065429688\n",
      "Epoch 3507: train loss: 0.7016665935516357, val loss: 0.6972500085830688\n",
      "Epoch 3508: train loss: 0.7012931704521179, val loss: 0.6968761682510376\n",
      "Epoch 3509: train loss: 0.7009202241897583, val loss: 0.6965023279190063\n",
      "Epoch 3510: train loss: 0.7005472779273987, val loss: 0.6961286067962646\n",
      "Epoch 3511: train loss: 0.7001744508743286, val loss: 0.6957550048828125\n",
      "Epoch 3512: train loss: 0.6998018026351929, val loss: 0.6953815221786499\n",
      "Epoch 3513: train loss: 0.6994291543960571, val loss: 0.6950082778930664\n",
      "Epoch 3514: train loss: 0.6990566849708557, val loss: 0.6946352124214172\n",
      "Epoch 3515: train loss: 0.6986844539642334, val loss: 0.6942622065544128\n",
      "Epoch 3516: train loss: 0.6983124017715454, val loss: 0.6938894391059875\n",
      "Epoch 3517: train loss: 0.6979404091835022, val loss: 0.6935166716575623\n",
      "Epoch 3518: train loss: 0.6975684762001038, val loss: 0.6931441426277161\n",
      "Epoch 3519: train loss: 0.6971967816352844, val loss: 0.6927717328071594\n",
      "Epoch 3520: train loss: 0.6968250870704651, val loss: 0.6923993229866028\n",
      "Epoch 3521: train loss: 0.6964536309242249, val loss: 0.6920272707939148\n",
      "Epoch 3522: train loss: 0.6960822939872742, val loss: 0.6916553974151611\n",
      "Epoch 3523: train loss: 0.6957111954689026, val loss: 0.6912835240364075\n",
      "Epoch 3524: train loss: 0.6953401565551758, val loss: 0.6909118890762329\n",
      "Epoch 3525: train loss: 0.6949692368507385, val loss: 0.6905402541160583\n",
      "Epoch 3526: train loss: 0.694598376750946, val loss: 0.6901687979698181\n",
      "Epoch 3527: train loss: 0.6942276358604431, val loss: 0.6897976994514465\n",
      "Epoch 3528: train loss: 0.6938573122024536, val loss: 0.6894264817237854\n",
      "Epoch 3529: train loss: 0.6934869885444641, val loss: 0.6890555620193481\n",
      "Epoch 3530: train loss: 0.6931167840957642, val loss: 0.6886847615242004\n",
      "Epoch 3531: train loss: 0.692746639251709, val loss: 0.6883140802383423\n",
      "Epoch 3532: train loss: 0.6923766732215881, val loss: 0.6879434585571289\n",
      "Epoch 3533: train loss: 0.6920067667961121, val loss: 0.6875729560852051\n",
      "Epoch 3534: train loss: 0.6916370391845703, val loss: 0.6872028112411499\n",
      "Epoch 3535: train loss: 0.6912676095962524, val loss: 0.68683260679245\n",
      "Epoch 3536: train loss: 0.6908981800079346, val loss: 0.6864628195762634\n",
      "Epoch 3537: train loss: 0.6905289888381958, val loss: 0.6860929727554321\n",
      "Epoch 3538: train loss: 0.690159797668457, val loss: 0.6857232451438904\n",
      "Epoch 3539: train loss: 0.6897907257080078, val loss: 0.685353696346283\n",
      "Epoch 3540: train loss: 0.6894218325614929, val loss: 0.6849843859672546\n",
      "Epoch 3541: train loss: 0.6890531778335571, val loss: 0.6846151351928711\n",
      "Epoch 3542: train loss: 0.6886845827102661, val loss: 0.6842460632324219\n",
      "Epoch 3543: train loss: 0.6883161664009094, val loss: 0.6838771104812622\n",
      "Epoch 3544: train loss: 0.6879478693008423, val loss: 0.6835083365440369\n",
      "Epoch 3545: train loss: 0.6875796914100647, val loss: 0.6831395030021667\n",
      "Epoch 3546: train loss: 0.6872115731239319, val loss: 0.682770848274231\n",
      "Epoch 3547: train loss: 0.6868435740470886, val loss: 0.6824026107788086\n",
      "Epoch 3548: train loss: 0.6864758133888245, val loss: 0.6820343136787415\n",
      "Epoch 3549: train loss: 0.6861083507537842, val loss: 0.6816663146018982\n",
      "Epoch 3550: train loss: 0.6857407689094543, val loss: 0.6812981963157654\n",
      "Epoch 3551: train loss: 0.6853733658790588, val loss: 0.6809304356575012\n",
      "Epoch 3552: train loss: 0.6850061416625977, val loss: 0.6805626749992371\n",
      "Epoch 3553: train loss: 0.6846389770507812, val loss: 0.6801952719688416\n",
      "Epoch 3554: train loss: 0.6842721104621887, val loss: 0.679827868938446\n",
      "Epoch 3555: train loss: 0.6839053630828857, val loss: 0.6794607043266296\n",
      "Epoch 3556: train loss: 0.6835386753082275, val loss: 0.679093599319458\n",
      "Epoch 3557: train loss: 0.6831721067428589, val loss: 0.6787266135215759\n",
      "Epoch 3558: train loss: 0.6828057169914246, val loss: 0.6783596873283386\n",
      "Epoch 3559: train loss: 0.6824393272399902, val loss: 0.6779929399490356\n",
      "Epoch 3560: train loss: 0.6820731163024902, val loss: 0.6776265501976013\n",
      "Epoch 3561: train loss: 0.6817072033882141, val loss: 0.6772601008415222\n",
      "Epoch 3562: train loss: 0.6813413500785828, val loss: 0.6768938302993774\n",
      "Epoch 3563: train loss: 0.680975615978241, val loss: 0.6765276789665222\n",
      "Epoch 3564: train loss: 0.6806100010871887, val loss: 0.6761617064476013\n",
      "Epoch 3565: train loss: 0.680244505405426, val loss: 0.6757959127426147\n",
      "Epoch 3566: train loss: 0.6798790693283081, val loss: 0.6754302382469177\n",
      "Epoch 3567: train loss: 0.6795140504837036, val loss: 0.6750648617744446\n",
      "Epoch 3568: train loss: 0.6791489720344543, val loss: 0.6746994853019714\n",
      "Epoch 3569: train loss: 0.6787841320037842, val loss: 0.6743342280387878\n",
      "Epoch 3570: train loss: 0.678419291973114, val loss: 0.6739691495895386\n",
      "Epoch 3571: train loss: 0.6780546307563782, val loss: 0.6736040115356445\n",
      "Epoch 3572: train loss: 0.6776900887489319, val loss: 0.6732391715049744\n",
      "Epoch 3573: train loss: 0.6773256063461304, val loss: 0.6728745698928833\n",
      "Epoch 3574: train loss: 0.6769614219665527, val loss: 0.672510027885437\n",
      "Epoch 3575: train loss: 0.6765973567962646, val loss: 0.6721455454826355\n",
      "Epoch 3576: train loss: 0.6762334108352661, val loss: 0.6717814207077026\n",
      "Epoch 3577: train loss: 0.6758695840835571, val loss: 0.671417236328125\n",
      "Epoch 3578: train loss: 0.6755058169364929, val loss: 0.6710532307624817\n",
      "Epoch 3579: train loss: 0.6751421689987183, val loss: 0.6706895232200623\n",
      "Epoch 3580: train loss: 0.6747788190841675, val loss: 0.6703259348869324\n",
      "Epoch 3581: train loss: 0.6744155883789062, val loss: 0.6699623465538025\n",
      "Epoch 3582: train loss: 0.6740524172782898, val loss: 0.6695989966392517\n",
      "Epoch 3583: train loss: 0.6736894845962524, val loss: 0.6692357063293457\n",
      "Epoch 3584: train loss: 0.6733265519142151, val loss: 0.6688725352287292\n",
      "Epoch 3585: train loss: 0.6729636192321777, val loss: 0.6685096025466919\n",
      "Epoch 3586: train loss: 0.6726011633872986, val loss: 0.6681469082832336\n",
      "Epoch 3587: train loss: 0.6722387671470642, val loss: 0.6677842140197754\n",
      "Epoch 3588: train loss: 0.6718764305114746, val loss: 0.6674217581748962\n",
      "Epoch 3589: train loss: 0.6715142130851746, val loss: 0.6670592427253723\n",
      "Epoch 3590: train loss: 0.6711521744728088, val loss: 0.6666969656944275\n",
      "Epoch 3591: train loss: 0.6707901954650879, val loss: 0.6663347482681274\n",
      "Epoch 3592: train loss: 0.6704283356666565, val loss: 0.6659728288650513\n",
      "Epoch 3593: train loss: 0.6700666546821594, val loss: 0.6656110882759094\n",
      "Epoch 3594: train loss: 0.6697052717208862, val loss: 0.6652494668960571\n",
      "Epoch 3595: train loss: 0.6693438291549683, val loss: 0.6648879647254944\n",
      "Epoch 3596: train loss: 0.6689825654029846, val loss: 0.6645264029502869\n",
      "Epoch 3597: train loss: 0.6686214208602905, val loss: 0.6641650199890137\n",
      "Epoch 3598: train loss: 0.668260395526886, val loss: 0.6638041138648987\n",
      "Epoch 3599: train loss: 0.6678996086120605, val loss: 0.6634432077407837\n",
      "Epoch 3600: train loss: 0.6675389409065247, val loss: 0.6630824208259583\n",
      "Epoch 3601: train loss: 0.6671783328056335, val loss: 0.6627216935157776\n",
      "Epoch 3602: train loss: 0.6668179035186768, val loss: 0.6623610854148865\n",
      "Epoch 3603: train loss: 0.6664576530456543, val loss: 0.6620005965232849\n",
      "Epoch 3604: train loss: 0.6660973429679871, val loss: 0.6616403460502625\n",
      "Epoch 3605: train loss: 0.6657372713088989, val loss: 0.6612802147865295\n",
      "Epoch 3606: train loss: 0.6653774380683899, val loss: 0.660920262336731\n",
      "Epoch 3607: train loss: 0.6650177240371704, val loss: 0.6605604290962219\n",
      "Epoch 3608: train loss: 0.6646580100059509, val loss: 0.6602006554603577\n",
      "Epoch 3609: train loss: 0.6642985343933105, val loss: 0.6598411798477173\n",
      "Epoch 3610: train loss: 0.6639391183853149, val loss: 0.6594817042350769\n",
      "Epoch 3611: train loss: 0.6635798215866089, val loss: 0.6591224670410156\n",
      "Epoch 3612: train loss: 0.6632208228111267, val loss: 0.6587634086608887\n",
      "Epoch 3613: train loss: 0.6628618836402893, val loss: 0.6584044694900513\n",
      "Epoch 3614: train loss: 0.6625030636787415, val loss: 0.6580456495285034\n",
      "Epoch 3615: train loss: 0.6621444225311279, val loss: 0.6576868891716003\n",
      "Epoch 3616: train loss: 0.6617857813835144, val loss: 0.6573282480239868\n",
      "Epoch 3617: train loss: 0.6614273190498352, val loss: 0.6569697260856628\n",
      "Epoch 3618: train loss: 0.6610689759254456, val loss: 0.656611442565918\n",
      "Epoch 3619: train loss: 0.660710871219635, val loss: 0.6562533378601074\n",
      "Epoch 3620: train loss: 0.660352885723114, val loss: 0.6558953523635864\n",
      "Epoch 3621: train loss: 0.6599949598312378, val loss: 0.6555376052856445\n",
      "Epoch 3622: train loss: 0.6596372127532959, val loss: 0.6551797986030579\n",
      "Epoch 3623: train loss: 0.6592795848846436, val loss: 0.6548221707344055\n",
      "Epoch 3624: train loss: 0.658922016620636, val loss: 0.6544647812843323\n",
      "Epoch 3625: train loss: 0.6585647463798523, val loss: 0.6541075110435486\n",
      "Epoch 3626: train loss: 0.6582075953483582, val loss: 0.6537503600120544\n",
      "Epoch 3627: train loss: 0.6578505635261536, val loss: 0.6533933877944946\n",
      "Epoch 3628: train loss: 0.6574935913085938, val loss: 0.6530364155769348\n",
      "Epoch 3629: train loss: 0.6571367383003235, val loss: 0.6526797413825989\n",
      "Epoch 3630: train loss: 0.6567800045013428, val loss: 0.6523231863975525\n",
      "Epoch 3631: train loss: 0.6564235687255859, val loss: 0.6519667506217957\n",
      "Epoch 3632: train loss: 0.6560672521591187, val loss: 0.6516105532646179\n",
      "Epoch 3633: train loss: 0.6557109951972961, val loss: 0.6512542963027954\n",
      "Epoch 3634: train loss: 0.6553548574447632, val loss: 0.650898277759552\n",
      "Epoch 3635: train loss: 0.654998779296875, val loss: 0.6505423784255981\n",
      "Epoch 3636: train loss: 0.6546428799629211, val loss: 0.6501865386962891\n",
      "Epoch 3637: train loss: 0.6542870402336121, val loss: 0.6498309969902039\n",
      "Epoch 3638: train loss: 0.6539314985275269, val loss: 0.6494755744934082\n",
      "Epoch 3639: train loss: 0.653576135635376, val loss: 0.6491202712059021\n",
      "Epoch 3640: train loss: 0.6532208323478699, val loss: 0.6487652063369751\n",
      "Epoch 3641: train loss: 0.6528655886650085, val loss: 0.6484100222587585\n",
      "Epoch 3642: train loss: 0.6525105237960815, val loss: 0.6480550765991211\n",
      "Epoch 3643: train loss: 0.6521555185317993, val loss: 0.6477004885673523\n",
      "Epoch 3644: train loss: 0.651800811290741, val loss: 0.6473459601402283\n",
      "Epoch 3645: train loss: 0.6514462232589722, val loss: 0.6469914317131042\n",
      "Epoch 3646: train loss: 0.6510916948318481, val loss: 0.6466371417045593\n",
      "Epoch 3647: train loss: 0.6507372856140137, val loss: 0.6462828516960144\n",
      "Epoch 3648: train loss: 0.6503829956054688, val loss: 0.6459288001060486\n",
      "Epoch 3649: train loss: 0.6500288248062134, val loss: 0.6455749869346619\n",
      "Epoch 3650: train loss: 0.6496748924255371, val loss: 0.6452212333679199\n",
      "Epoch 3651: train loss: 0.6493211388587952, val loss: 0.6448675990104675\n",
      "Epoch 3652: train loss: 0.6489675045013428, val loss: 0.6445142030715942\n",
      "Epoch 3653: train loss: 0.6486139297485352, val loss: 0.6441608667373657\n",
      "Epoch 3654: train loss: 0.6482604146003723, val loss: 0.643807590007782\n",
      "Epoch 3655: train loss: 0.6479070782661438, val loss: 0.6434544920921326\n",
      "Epoch 3656: train loss: 0.6475538015365601, val loss: 0.6431014537811279\n",
      "Epoch 3657: train loss: 0.6472007632255554, val loss: 0.6427488327026367\n",
      "Epoch 3658: train loss: 0.6468479633331299, val loss: 0.6423962712287903\n",
      "Epoch 3659: train loss: 0.6464952230453491, val loss: 0.6420437097549438\n",
      "Epoch 3660: train loss: 0.6461424827575684, val loss: 0.6416913270950317\n",
      "Epoch 3661: train loss: 0.6457899212837219, val loss: 0.6413390040397644\n",
      "Epoch 3662: train loss: 0.645437479019165, val loss: 0.6409870386123657\n",
      "Epoch 3663: train loss: 0.6450853943824768, val loss: 0.6406351923942566\n",
      "Epoch 3664: train loss: 0.6447332501411438, val loss: 0.6402833461761475\n",
      "Epoch 3665: train loss: 0.6443813443183899, val loss: 0.6399316787719727\n",
      "Epoch 3666: train loss: 0.6440294981002808, val loss: 0.6395801901817322\n",
      "Epoch 3667: train loss: 0.6436777710914612, val loss: 0.6392287611961365\n",
      "Epoch 3668: train loss: 0.6433261632919312, val loss: 0.6388776898384094\n",
      "Epoch 3669: train loss: 0.6429747939109802, val loss: 0.638526439666748\n",
      "Epoch 3670: train loss: 0.6426234841346741, val loss: 0.6381756663322449\n",
      "Epoch 3671: train loss: 0.6422724723815918, val loss: 0.6378248333930969\n",
      "Epoch 3672: train loss: 0.6419214010238647, val loss: 0.6374741792678833\n",
      "Epoch 3673: train loss: 0.6415704488754272, val loss: 0.6371235847473145\n",
      "Epoch 3674: train loss: 0.6412196159362793, val loss: 0.6367730498313904\n",
      "Epoch 3675: train loss: 0.6408689022064209, val loss: 0.6364229321479797\n",
      "Epoch 3676: train loss: 0.6405184864997864, val loss: 0.6360728144645691\n",
      "Epoch 3677: train loss: 0.6401681303977966, val loss: 0.6357228755950928\n",
      "Epoch 3678: train loss: 0.6398178935050964, val loss: 0.635373055934906\n",
      "Epoch 3679: train loss: 0.6394678354263306, val loss: 0.635023295879364\n",
      "Epoch 3680: train loss: 0.6391177773475647, val loss: 0.6346737146377563\n",
      "Epoch 3681: train loss: 0.6387678980827332, val loss: 0.6343243718147278\n",
      "Epoch 3682: train loss: 0.6384182572364807, val loss: 0.6339752078056335\n",
      "Epoch 3683: train loss: 0.6380687355995178, val loss: 0.6336259841918945\n",
      "Epoch 3684: train loss: 0.6377193331718445, val loss: 0.6332770586013794\n",
      "Epoch 3685: train loss: 0.6373699903488159, val loss: 0.6329281330108643\n",
      "Epoch 3686: train loss: 0.6370208263397217, val loss: 0.6325793266296387\n",
      "Epoch 3687: train loss: 0.6366717219352722, val loss: 0.6322308778762817\n",
      "Epoch 3688: train loss: 0.6363229155540466, val loss: 0.6318826079368591\n",
      "Epoch 3689: train loss: 0.6359742879867554, val loss: 0.6315342783927917\n",
      "Epoch 3690: train loss: 0.6356255412101746, val loss: 0.6311861872673035\n",
      "Epoch 3691: train loss: 0.6352771520614624, val loss: 0.6308380961418152\n",
      "Epoch 3692: train loss: 0.6349287629127502, val loss: 0.630490243434906\n",
      "Epoch 3693: train loss: 0.6345804333686829, val loss: 0.6301424503326416\n",
      "Epoch 3694: train loss: 0.634232223033905, val loss: 0.6297948956489563\n",
      "Epoch 3695: train loss: 0.6338843703269958, val loss: 0.6294474601745605\n",
      "Epoch 3696: train loss: 0.6335365176200867, val loss: 0.6291002631187439\n",
      "Epoch 3697: train loss: 0.6331888437271118, val loss: 0.6287529468536377\n",
      "Epoch 3698: train loss: 0.6328412294387817, val loss: 0.6284058690071106\n",
      "Epoch 3699: train loss: 0.6324938535690308, val loss: 0.6280589699745178\n",
      "Epoch 3700: train loss: 0.632146418094635, val loss: 0.6277122497558594\n",
      "Epoch 3701: train loss: 0.6317992806434631, val loss: 0.6273656487464905\n",
      "Epoch 3702: train loss: 0.6314523220062256, val loss: 0.6270192265510559\n",
      "Epoch 3703: train loss: 0.6311054825782776, val loss: 0.6266729235649109\n",
      "Epoch 3704: train loss: 0.6307586431503296, val loss: 0.6263266801834106\n",
      "Epoch 3705: train loss: 0.6304119825363159, val loss: 0.6259805560112\n",
      "Epoch 3706: train loss: 0.6300654411315918, val loss: 0.6256347894668579\n",
      "Epoch 3707: train loss: 0.6297191381454468, val loss: 0.6252889633178711\n",
      "Epoch 3708: train loss: 0.6293729543685913, val loss: 0.6249433755874634\n",
      "Epoch 3709: train loss: 0.6290268898010254, val loss: 0.6245979070663452\n",
      "Epoch 3710: train loss: 0.628680944442749, val loss: 0.624252438545227\n",
      "Epoch 3711: train loss: 0.6283350586891174, val loss: 0.6239072680473328\n",
      "Epoch 3712: train loss: 0.6279892921447754, val loss: 0.6235620379447937\n",
      "Epoch 3713: train loss: 0.6276436448097229, val loss: 0.6232172846794128\n",
      "Epoch 3714: train loss: 0.6272982358932495, val loss: 0.6228723526000977\n",
      "Epoch 3715: train loss: 0.6269529461860657, val loss: 0.6225277781486511\n",
      "Epoch 3716: train loss: 0.6266078352928162, val loss: 0.6221832633018494\n",
      "Epoch 3717: train loss: 0.6262627840042114, val loss: 0.6218388080596924\n",
      "Epoch 3718: train loss: 0.6259178519248962, val loss: 0.621494472026825\n",
      "Epoch 3719: train loss: 0.625572919845581, val loss: 0.6211504340171814\n",
      "Epoch 3720: train loss: 0.6252284049987793, val loss: 0.6208065152168274\n",
      "Epoch 3721: train loss: 0.6248839497566223, val loss: 0.6204627156257629\n",
      "Epoch 3722: train loss: 0.6245395541191101, val loss: 0.620119035243988\n",
      "Epoch 3723: train loss: 0.6241952776908875, val loss: 0.6197754740715027\n",
      "Epoch 3724: train loss: 0.6238511204719543, val loss: 0.6194319128990173\n",
      "Epoch 3725: train loss: 0.623507022857666, val loss: 0.6190888285636902\n",
      "Epoch 3726: train loss: 0.6231633424758911, val loss: 0.618745744228363\n",
      "Epoch 3727: train loss: 0.6228196620941162, val loss: 0.6184027791023254\n",
      "Epoch 3728: train loss: 0.6224761009216309, val loss: 0.6180599331855774\n",
      "Epoch 3729: train loss: 0.6221326589584351, val loss: 0.6177171468734741\n",
      "Epoch 3730: train loss: 0.621789276599884, val loss: 0.6173745393753052\n",
      "Epoch 3731: train loss: 0.6214460134506226, val loss: 0.6170322299003601\n",
      "Epoch 3732: train loss: 0.6211031079292297, val loss: 0.6166898608207703\n",
      "Epoch 3733: train loss: 0.6207602620124817, val loss: 0.6163477897644043\n",
      "Epoch 3734: train loss: 0.6204174757003784, val loss: 0.6160058379173279\n",
      "Epoch 3735: train loss: 0.6200748085975647, val loss: 0.6156639456748962\n",
      "Epoch 3736: train loss: 0.6197322607040405, val loss: 0.6153221130371094\n",
      "Epoch 3737: train loss: 0.6193898320198059, val loss: 0.6149803996086121\n",
      "Epoch 3738: train loss: 0.6190474629402161, val loss: 0.6146391034126282\n",
      "Epoch 3739: train loss: 0.6187054514884949, val loss: 0.6142978072166443\n",
      "Epoch 3740: train loss: 0.6183634400367737, val loss: 0.6139564514160156\n",
      "Epoch 3741: train loss: 0.618021547794342, val loss: 0.6136154532432556\n",
      "Epoch 3742: train loss: 0.6176798939704895, val loss: 0.6132743954658508\n",
      "Epoch 3743: train loss: 0.6173381805419922, val loss: 0.6129336357116699\n",
      "Epoch 3744: train loss: 0.616996705532074, val loss: 0.6125931143760681\n",
      "Epoch 3745: train loss: 0.6166554689407349, val loss: 0.6122527122497559\n",
      "Epoch 3746: train loss: 0.6163142919540405, val loss: 0.611912190914154\n",
      "Epoch 3747: train loss: 0.6159732341766357, val loss: 0.6115720868110657\n",
      "Epoch 3748: train loss: 0.6156322360038757, val loss: 0.6112319231033325\n",
      "Epoch 3749: train loss: 0.61529141664505, val loss: 0.6108919978141785\n",
      "Epoch 3750: train loss: 0.6149506568908691, val loss: 0.6105522513389587\n",
      "Epoch 3751: train loss: 0.6146101951599121, val loss: 0.6102127432823181\n",
      "Epoch 3752: train loss: 0.6142698526382446, val loss: 0.6098732352256775\n",
      "Epoch 3753: train loss: 0.6139295697212219, val loss: 0.6095337271690369\n",
      "Epoch 3754: train loss: 0.6135894656181335, val loss: 0.6091945171356201\n",
      "Epoch 3755: train loss: 0.6132494807243347, val loss: 0.6088553071022034\n",
      "Epoch 3756: train loss: 0.6129094362258911, val loss: 0.6085164546966553\n",
      "Epoch 3757: train loss: 0.6125698089599609, val loss: 0.608177661895752\n",
      "Epoch 3758: train loss: 0.6122303009033203, val loss: 0.6078389286994934\n",
      "Epoch 3759: train loss: 0.6118907928466797, val loss: 0.6075004935264587\n",
      "Epoch 3760: train loss: 0.6115515232086182, val loss: 0.6071619987487793\n",
      "Epoch 3761: train loss: 0.6112121939659119, val loss: 0.6068238019943237\n",
      "Epoch 3762: train loss: 0.6108731031417847, val loss: 0.6064854264259338\n",
      "Epoch 3763: train loss: 0.6105340719223022, val loss: 0.6061475872993469\n",
      "Epoch 3764: train loss: 0.6101952791213989, val loss: 0.60580974817276\n",
      "Epoch 3765: train loss: 0.6098566055297852, val loss: 0.6054719686508179\n",
      "Epoch 3766: train loss: 0.6095180511474609, val loss: 0.6051344275474548\n",
      "Epoch 3767: train loss: 0.6091796159744263, val loss: 0.604796826839447\n",
      "Epoch 3768: train loss: 0.6088413000106812, val loss: 0.6044594049453735\n",
      "Epoch 3769: train loss: 0.608502984046936, val loss: 0.6041223406791687\n",
      "Epoch 3770: train loss: 0.6081650853157043, val loss: 0.6037854552268982\n",
      "Epoch 3771: train loss: 0.6078272461891174, val loss: 0.6034485697746277\n",
      "Epoch 3772: train loss: 0.6074894070625305, val loss: 0.6031116843223572\n",
      "Epoch 3773: train loss: 0.6071518063545227, val loss: 0.6027750372886658\n",
      "Epoch 3774: train loss: 0.6068142652511597, val loss: 0.6024385690689087\n",
      "Epoch 3775: train loss: 0.6064767837524414, val loss: 0.6021022200584412\n",
      "Epoch 3776: train loss: 0.6061395406723022, val loss: 0.601766049861908\n",
      "Epoch 3777: train loss: 0.6058025360107422, val loss: 0.6014299988746643\n",
      "Epoch 3778: train loss: 0.6054655909538269, val loss: 0.6010940670967102\n",
      "Epoch 3779: train loss: 0.6051287651062012, val loss: 0.6007582545280457\n",
      "Epoch 3780: train loss: 0.6047919988632202, val loss: 0.6004225015640259\n",
      "Epoch 3781: train loss: 0.6044553518295288, val loss: 0.6000871062278748\n",
      "Epoch 3782: train loss: 0.6041190028190613, val loss: 0.5997517704963684\n",
      "Epoch 3783: train loss: 0.6037826538085938, val loss: 0.5994165539741516\n",
      "Epoch 3784: train loss: 0.6034465432167053, val loss: 0.59908127784729\n",
      "Epoch 3785: train loss: 0.6031104326248169, val loss: 0.5987464189529419\n",
      "Epoch 3786: train loss: 0.6027745008468628, val loss: 0.598411500453949\n",
      "Epoch 3787: train loss: 0.6024386286735535, val loss: 0.5980767607688904\n",
      "Epoch 3788: train loss: 0.6021028161048889, val loss: 0.5977422595024109\n",
      "Epoch 3789: train loss: 0.6017674207687378, val loss: 0.5974078178405762\n",
      "Epoch 3790: train loss: 0.6014320254325867, val loss: 0.5970736145973206\n",
      "Epoch 3791: train loss: 0.6010967493057251, val loss: 0.5967394113540649\n",
      "Epoch 3792: train loss: 0.6007615923881531, val loss: 0.5964053273200989\n",
      "Epoch 3793: train loss: 0.6004265546798706, val loss: 0.5960714221000671\n",
      "Epoch 3794: train loss: 0.6000915765762329, val loss: 0.5957377552986145\n",
      "Epoch 3795: train loss: 0.5997568964958191, val loss: 0.5954041481018066\n",
      "Epoch 3796: train loss: 0.59942227602005, val loss: 0.5950706601142883\n",
      "Epoch 3797: train loss: 0.5990878343582153, val loss: 0.5947373509407043\n",
      "Epoch 3798: train loss: 0.5987534523010254, val loss: 0.5944041609764099\n",
      "Epoch 3799: train loss: 0.598419189453125, val loss: 0.5940710306167603\n",
      "Epoch 3800: train loss: 0.5980849862098694, val loss: 0.5937381386756897\n",
      "Epoch 3801: train loss: 0.5977510213851929, val loss: 0.5934054851531982\n",
      "Epoch 3802: train loss: 0.5974172949790955, val loss: 0.5930728316307068\n",
      "Epoch 3803: train loss: 0.5970836281776428, val loss: 0.5927403569221497\n",
      "Epoch 3804: train loss: 0.5967500805854797, val loss: 0.5924078226089478\n",
      "Epoch 3805: train loss: 0.5964165925979614, val loss: 0.5920756459236145\n",
      "Epoch 3806: train loss: 0.5960831046104431, val loss: 0.5917435884475708\n",
      "Epoch 3807: train loss: 0.595750093460083, val loss: 0.5914116501808167\n",
      "Epoch 3808: train loss: 0.5954170227050781, val loss: 0.591079831123352\n",
      "Epoch 3809: train loss: 0.5950841903686523, val loss: 0.5907482504844666\n",
      "Epoch 3810: train loss: 0.5947514176368713, val loss: 0.5904166102409363\n",
      "Epoch 3811: train loss: 0.5944187045097351, val loss: 0.5900852680206299\n",
      "Epoch 3812: train loss: 0.5940861105918884, val loss: 0.589754045009613\n",
      "Epoch 3813: train loss: 0.5937537550926208, val loss: 0.5894230008125305\n",
      "Epoch 3814: train loss: 0.5934215188026428, val loss: 0.5890920162200928\n",
      "Epoch 3815: train loss: 0.5930894613265991, val loss: 0.5887610912322998\n",
      "Epoch 3816: train loss: 0.5927574038505554, val loss: 0.5884303450584412\n",
      "Epoch 3817: train loss: 0.592425525188446, val loss: 0.5880997776985168\n",
      "Epoch 3818: train loss: 0.5920937657356262, val loss: 0.5877691507339478\n",
      "Epoch 3819: train loss: 0.5917620658874512, val loss: 0.5874388813972473\n",
      "Epoch 3820: train loss: 0.5914306640625, val loss: 0.5871087908744812\n",
      "Epoch 3821: train loss: 0.5910993218421936, val loss: 0.5867787599563599\n",
      "Epoch 3822: train loss: 0.5907680988311768, val loss: 0.5864488482475281\n",
      "Epoch 3823: train loss: 0.5904370546340942, val loss: 0.5861190557479858\n",
      "Epoch 3824: train loss: 0.5901059508323669, val loss: 0.5857893228530884\n",
      "Epoch 3825: train loss: 0.5897750854492188, val loss: 0.5854599475860596\n",
      "Epoch 3826: train loss: 0.5894444584846497, val loss: 0.5851306319236755\n",
      "Epoch 3827: train loss: 0.5891138911247253, val loss: 0.5848013758659363\n",
      "Epoch 3828: train loss: 0.5887834429740906, val loss: 0.5844722390174866\n",
      "Epoch 3829: train loss: 0.5884531736373901, val loss: 0.584143340587616\n",
      "Epoch 3830: train loss: 0.5881229043006897, val loss: 0.5838143229484558\n",
      "Epoch 3831: train loss: 0.5877928137779236, val loss: 0.5834857821464539\n",
      "Epoch 3832: train loss: 0.5874629020690918, val loss: 0.5831572413444519\n",
      "Epoch 3833: train loss: 0.5871332287788391, val loss: 0.5828288197517395\n",
      "Epoch 3834: train loss: 0.5868035554885864, val loss: 0.5825005769729614\n",
      "Epoch 3835: train loss: 0.5864740014076233, val loss: 0.5821724534034729\n",
      "Epoch 3836: train loss: 0.5861445069313049, val loss: 0.5818444490432739\n",
      "Epoch 3837: train loss: 0.5858152508735657, val loss: 0.5815165638923645\n",
      "Epoch 3838: train loss: 0.5854861736297607, val loss: 0.5811888575553894\n",
      "Epoch 3839: train loss: 0.5851572155952454, val loss: 0.5808613896369934\n",
      "Epoch 3840: train loss: 0.5848283767700195, val loss: 0.5805339217185974\n",
      "Epoch 3841: train loss: 0.5844995379447937, val loss: 0.580206573009491\n",
      "Epoch 3842: train loss: 0.5841709971427917, val loss: 0.5798792839050293\n",
      "Epoch 3843: train loss: 0.583842396736145, val loss: 0.5795523524284363\n",
      "Epoch 3844: train loss: 0.5835141539573669, val loss: 0.579225480556488\n",
      "Epoch 3845: train loss: 0.5831859111785889, val loss: 0.5788988471031189\n",
      "Epoch 3846: train loss: 0.5828578472137451, val loss: 0.578572154045105\n",
      "Epoch 3847: train loss: 0.5825299024581909, val loss: 0.5782455801963806\n",
      "Epoch 3848: train loss: 0.5822020173072815, val loss: 0.5779191255569458\n",
      "Epoch 3849: train loss: 0.5818742513656616, val loss: 0.5775930285453796\n",
      "Epoch 3850: train loss: 0.5815467834472656, val loss: 0.5772669315338135\n",
      "Epoch 3851: train loss: 0.5812194347381592, val loss: 0.5769410729408264\n",
      "Epoch 3852: train loss: 0.5808920860290527, val loss: 0.5766152739524841\n",
      "Epoch 3853: train loss: 0.5805649161338806, val loss: 0.5762895941734314\n",
      "Epoch 3854: train loss: 0.580237865447998, val loss: 0.5759639739990234\n",
      "Epoch 3855: train loss: 0.5799108147621155, val loss: 0.5756387114524841\n",
      "Epoch 3856: train loss: 0.5795841217041016, val loss: 0.5753135085105896\n",
      "Epoch 3857: train loss: 0.5792574882507324, val loss: 0.5749883651733398\n",
      "Epoch 3858: train loss: 0.5789310336112976, val loss: 0.5746633410453796\n",
      "Epoch 3859: train loss: 0.5786045789718628, val loss: 0.574338436126709\n",
      "Epoch 3860: train loss: 0.5782783031463623, val loss: 0.5740135908126831\n",
      "Epoch 3861: train loss: 0.5779520869255066, val loss: 0.5736889839172363\n",
      "Epoch 3862: train loss: 0.5776259899139404, val loss: 0.5733645558357239\n",
      "Epoch 3863: train loss: 0.5773001909255981, val loss: 0.5730403065681458\n",
      "Epoch 3864: train loss: 0.5769743919372559, val loss: 0.5727160573005676\n",
      "Epoch 3865: train loss: 0.5766488313674927, val loss: 0.5723921060562134\n",
      "Epoch 3866: train loss: 0.5763232707977295, val loss: 0.5720680952072144\n",
      "Epoch 3867: train loss: 0.5759978294372559, val loss: 0.5717442035675049\n",
      "Epoch 3868: train loss: 0.5756725668907166, val loss: 0.5714206099510193\n",
      "Epoch 3869: train loss: 0.5753474831581116, val loss: 0.571097195148468\n",
      "Epoch 3870: train loss: 0.5750225782394409, val loss: 0.570773720741272\n",
      "Epoch 3871: train loss: 0.574697732925415, val loss: 0.5704505443572998\n",
      "Epoch 3872: train loss: 0.5743729472160339, val loss: 0.5701273679733276\n",
      "Epoch 3873: train loss: 0.5740483403205872, val loss: 0.5698043704032898\n",
      "Epoch 3874: train loss: 0.5737237930297852, val loss: 0.5694816708564758\n",
      "Epoch 3875: train loss: 0.573399543762207, val loss: 0.5691589117050171\n",
      "Epoch 3876: train loss: 0.5730753540992737, val loss: 0.5688363909721375\n",
      "Epoch 3877: train loss: 0.5727512836456299, val loss: 0.5685139894485474\n",
      "Epoch 3878: train loss: 0.5724272727966309, val loss: 0.5681915879249573\n",
      "Epoch 3879: train loss: 0.5721034407615662, val loss: 0.5678694248199463\n",
      "Epoch 3880: train loss: 0.5717796683311462, val loss: 0.5675474405288696\n",
      "Epoch 3881: train loss: 0.5714561939239502, val loss: 0.5672256350517273\n",
      "Epoch 3882: train loss: 0.5711327791213989, val loss: 0.5669038891792297\n",
      "Epoch 3883: train loss: 0.570809543132782, val loss: 0.5665822625160217\n",
      "Epoch 3884: train loss: 0.570486307144165, val loss: 0.5662607550621033\n",
      "Epoch 3885: train loss: 0.5701632499694824, val loss: 0.5659393668174744\n",
      "Epoch 3886: train loss: 0.5698402523994446, val loss: 0.5656182169914246\n",
      "Epoch 3887: train loss: 0.5695176124572754, val loss: 0.5652971267700195\n",
      "Epoch 3888: train loss: 0.5691949725151062, val loss: 0.5649762749671936\n",
      "Epoch 3889: train loss: 0.5688724517822266, val loss: 0.5646554231643677\n",
      "Epoch 3890: train loss: 0.5685500502586365, val loss: 0.5643347501754761\n",
      "Epoch 3891: train loss: 0.5682277679443359, val loss: 0.5640141367912292\n",
      "Epoch 3892: train loss: 0.5679055452346802, val loss: 0.5636937618255615\n",
      "Epoch 3893: train loss: 0.5675835609436035, val loss: 0.5633735656738281\n",
      "Epoch 3894: train loss: 0.5672617554664612, val loss: 0.563053548336029\n",
      "Epoch 3895: train loss: 0.5669400691986084, val loss: 0.5627334713935852\n",
      "Epoch 3896: train loss: 0.5666183829307556, val loss: 0.5624136328697205\n",
      "Epoch 3897: train loss: 0.5662968754768372, val loss: 0.5620937347412109\n",
      "Epoch 3898: train loss: 0.5659754276275635, val loss: 0.5617741942405701\n",
      "Epoch 3899: train loss: 0.5656542778015137, val loss: 0.5614549517631531\n",
      "Epoch 3900: train loss: 0.5653332471847534, val loss: 0.5611357092857361\n",
      "Epoch 3901: train loss: 0.5650122165679932, val loss: 0.5608164668083191\n",
      "Epoch 3902: train loss: 0.564691424369812, val loss: 0.5604973435401917\n",
      "Epoch 3903: train loss: 0.5643706321716309, val loss: 0.5601783394813538\n",
      "Epoch 3904: train loss: 0.564050018787384, val loss: 0.559859573841095\n",
      "Epoch 3905: train loss: 0.5637296438217163, val loss: 0.5595410466194153\n",
      "Epoch 3906: train loss: 0.5634093284606934, val loss: 0.5592225790023804\n",
      "Epoch 3907: train loss: 0.5630891919136047, val loss: 0.558904230594635\n",
      "Epoch 3908: train loss: 0.5627690553665161, val loss: 0.5585858225822449\n",
      "Epoch 3909: train loss: 0.562449038028717, val loss: 0.5582677721977234\n",
      "Epoch 3910: train loss: 0.5621291995048523, val loss: 0.5579498410224915\n",
      "Epoch 3911: train loss: 0.5618096590042114, val loss: 0.5576320290565491\n",
      "Epoch 3912: train loss: 0.5614901185035706, val loss: 0.5573143362998962\n",
      "Epoch 3913: train loss: 0.5611706972122192, val loss: 0.5569968223571777\n",
      "Epoch 3914: train loss: 0.5608513951301575, val loss: 0.556679368019104\n",
      "Epoch 3915: train loss: 0.5605321526527405, val loss: 0.556361973285675\n",
      "Epoch 3916: train loss: 0.5602130889892578, val loss: 0.5560449957847595\n",
      "Epoch 3917: train loss: 0.5598942637443542, val loss: 0.5557279586791992\n",
      "Epoch 3918: train loss: 0.5595754981040955, val loss: 0.5554110407829285\n",
      "Epoch 3919: train loss: 0.559256911277771, val loss: 0.5550943613052368\n",
      "Epoch 3920: train loss: 0.5589383840560913, val loss: 0.5547776222229004\n",
      "Epoch 3921: train loss: 0.5586198568344116, val loss: 0.5544610023498535\n",
      "Epoch 3922: train loss: 0.558301568031311, val loss: 0.5541449189186096\n",
      "Epoch 3923: train loss: 0.5579835176467896, val loss: 0.5538285374641418\n",
      "Epoch 3924: train loss: 0.5576655268669128, val loss: 0.5535125732421875\n",
      "Epoch 3925: train loss: 0.5573476552963257, val loss: 0.5531966090202332\n",
      "Epoch 3926: train loss: 0.5570299029350281, val loss: 0.5528808832168579\n",
      "Epoch 3927: train loss: 0.5567122101783752, val loss: 0.5525650382041931\n",
      "Epoch 3928: train loss: 0.5563946962356567, val loss: 0.552249550819397\n",
      "Epoch 3929: train loss: 0.5560773611068726, val loss: 0.5519341826438904\n",
      "Epoch 3930: train loss: 0.5557602047920227, val loss: 0.5516190528869629\n",
      "Epoch 3931: train loss: 0.5554431080818176, val loss: 0.5513038039207458\n",
      "Epoch 3932: train loss: 0.5551261305809021, val loss: 0.5509887337684631\n",
      "Epoch 3933: train loss: 0.5548092722892761, val loss: 0.5506737232208252\n",
      "Epoch 3934: train loss: 0.5544924736022949, val loss: 0.5503592491149902\n",
      "Epoch 3935: train loss: 0.5541759133338928, val loss: 0.550044596195221\n",
      "Epoch 3936: train loss: 0.5538594722747803, val loss: 0.5497302412986755\n",
      "Epoch 3937: train loss: 0.553543210029602, val loss: 0.5494157075881958\n",
      "Epoch 3938: train loss: 0.553226888179779, val loss: 0.5491015315055847\n",
      "Epoch 3939: train loss: 0.5529108047485352, val loss: 0.5487873554229736\n",
      "Epoch 3940: train loss: 0.552594780921936, val loss: 0.548473596572876\n",
      "Epoch 3941: train loss: 0.552278995513916, val loss: 0.5481597781181335\n",
      "Epoch 3942: train loss: 0.5519633889198303, val loss: 0.5478461980819702\n",
      "Epoch 3943: train loss: 0.5516477823257446, val loss: 0.5475325584411621\n",
      "Epoch 3944: train loss: 0.551332414150238, val loss: 0.5472192168235779\n",
      "Epoch 3945: train loss: 0.5510169863700867, val loss: 0.5469058752059937\n",
      "Epoch 3946: train loss: 0.5507017374038696, val loss: 0.5465928316116333\n",
      "Epoch 3947: train loss: 0.5503868460655212, val loss: 0.5462798476219177\n",
      "Epoch 3948: train loss: 0.5500718951225281, val loss: 0.5459670424461365\n",
      "Epoch 3949: train loss: 0.5497571229934692, val loss: 0.5456542372703552\n",
      "Epoch 3950: train loss: 0.5494424104690552, val loss: 0.5453416705131531\n",
      "Epoch 3951: train loss: 0.5491278171539307, val loss: 0.5450291037559509\n",
      "Epoch 3952: train loss: 0.5488132834434509, val loss: 0.5447167754173279\n",
      "Epoch 3953: train loss: 0.5484991669654846, val loss: 0.5444046854972839\n",
      "Epoch 3954: train loss: 0.5481850504875183, val loss: 0.5440926551818848\n",
      "Epoch 3955: train loss: 0.5478709936141968, val loss: 0.5437807440757751\n",
      "Epoch 3956: train loss: 0.5475570559501648, val loss: 0.5434688925743103\n",
      "Epoch 3957: train loss: 0.5472432374954224, val loss: 0.5431572198867798\n",
      "Epoch 3958: train loss: 0.5469295382499695, val loss: 0.5428456664085388\n",
      "Epoch 3959: train loss: 0.5466160774230957, val loss: 0.542534351348877\n",
      "Epoch 3960: train loss: 0.5463027358055115, val loss: 0.5422230958938599\n",
      "Epoch 3961: train loss: 0.5459895133972168, val loss: 0.5419120192527771\n",
      "Epoch 3962: train loss: 0.5456763505935669, val loss: 0.5416008830070496\n",
      "Epoch 3963: train loss: 0.5453633069992065, val loss: 0.5412899255752563\n",
      "Epoch 3964: train loss: 0.545050323009491, val loss: 0.5409793257713318\n",
      "Epoch 3965: train loss: 0.5447375774383545, val loss: 0.5406686663627625\n",
      "Epoch 3966: train loss: 0.5444250702857971, val loss: 0.5403582453727722\n",
      "Epoch 3967: train loss: 0.5441125631332397, val loss: 0.5400480628013611\n",
      "Epoch 3968: train loss: 0.5438001751899719, val loss: 0.5397377014160156\n",
      "Epoch 3969: train loss: 0.5434878468513489, val loss: 0.5394275784492493\n",
      "Epoch 3970: train loss: 0.5431756973266602, val loss: 0.5391177535057068\n",
      "Epoch 3971: train loss: 0.5428637862205505, val loss: 0.5388080477714539\n",
      "Epoch 3972: train loss: 0.5425519347190857, val loss: 0.5384984016418457\n",
      "Epoch 3973: train loss: 0.5422402620315552, val loss: 0.5381888151168823\n",
      "Epoch 3974: train loss: 0.5419285893440247, val loss: 0.5378794074058533\n",
      "Epoch 3975: train loss: 0.5416170358657837, val loss: 0.5375701189041138\n",
      "Epoch 3976: train loss: 0.541305661201477, val loss: 0.5372610092163086\n",
      "Epoch 3977: train loss: 0.5409944653511047, val loss: 0.5369519591331482\n",
      "Epoch 3978: train loss: 0.5406834483146667, val loss: 0.5366432070732117\n",
      "Epoch 3979: train loss: 0.5403724312782288, val loss: 0.5363343358039856\n",
      "Epoch 3980: train loss: 0.5400616526603699, val loss: 0.5360258221626282\n",
      "Epoch 3981: train loss: 0.5397508144378662, val loss: 0.5357173085212708\n",
      "Epoch 3982: train loss: 0.5394401550292969, val loss: 0.5354089736938477\n",
      "Epoch 3983: train loss: 0.5391297340393066, val loss: 0.5351008772850037\n",
      "Epoch 3984: train loss: 0.5388194918632507, val loss: 0.5347927212715149\n",
      "Epoch 3985: train loss: 0.5385093092918396, val loss: 0.5344848036766052\n",
      "Epoch 3986: train loss: 0.5381991267204285, val loss: 0.5341770052909851\n",
      "Epoch 3987: train loss: 0.5378891229629517, val loss: 0.5338692665100098\n",
      "Epoch 3988: train loss: 0.5375792384147644, val loss: 0.5335617065429688\n",
      "Epoch 3989: train loss: 0.537269651889801, val loss: 0.5332543849945068\n",
      "Epoch 3990: train loss: 0.5369601249694824, val loss: 0.5329471826553345\n",
      "Epoch 3991: train loss: 0.5366505980491638, val loss: 0.5326399207115173\n",
      "Epoch 3992: train loss: 0.5363412499427795, val loss: 0.5323328971862793\n",
      "Epoch 3993: train loss: 0.5360320210456848, val loss: 0.5320260524749756\n",
      "Epoch 3994: train loss: 0.5357229709625244, val loss: 0.5317193269729614\n",
      "Epoch 3995: train loss: 0.5354140400886536, val loss: 0.5314127802848816\n",
      "Epoch 3996: train loss: 0.5351052284240723, val loss: 0.5311062932014465\n",
      "Epoch 3997: train loss: 0.5347965955734253, val loss: 0.530799925327301\n",
      "Epoch 3998: train loss: 0.5344880223274231, val loss: 0.5304936766624451\n",
      "Epoch 3999: train loss: 0.5341795086860657, val loss: 0.5301875472068787\n",
      "Epoch 4000: train loss: 0.5338711142539978, val loss: 0.5298815965652466\n",
      "Epoch 4001: train loss: 0.5335630774497986, val loss: 0.5295758247375488\n",
      "Epoch 4002: train loss: 0.5332550406455994, val loss: 0.5292701721191406\n",
      "Epoch 4003: train loss: 0.5329470634460449, val loss: 0.5289645791053772\n",
      "Epoch 4004: train loss: 0.5326392650604248, val loss: 0.5286590456962585\n",
      "Epoch 4005: train loss: 0.5323315262794495, val loss: 0.5283538699150085\n",
      "Epoch 4006: train loss: 0.5320239067077637, val loss: 0.5280486941337585\n",
      "Epoch 4007: train loss: 0.5317165851593018, val loss: 0.5277437567710876\n",
      "Epoch 4008: train loss: 0.5314093232154846, val loss: 0.5274388790130615\n",
      "Epoch 4009: train loss: 0.531102180480957, val loss: 0.5271339416503906\n",
      "Epoch 4010: train loss: 0.530795156955719, val loss: 0.5268294215202332\n",
      "Epoch 4011: train loss: 0.530488133430481, val loss: 0.5265249013900757\n",
      "Epoch 4012: train loss: 0.5301812887191772, val loss: 0.5262205600738525\n",
      "Epoch 4013: train loss: 0.5298746228218079, val loss: 0.5259162783622742\n",
      "Epoch 4014: train loss: 0.5295681357383728, val loss: 0.5256121754646301\n",
      "Epoch 4015: train loss: 0.5292617678642273, val loss: 0.5253083109855652\n",
      "Epoch 4016: train loss: 0.5289554595947266, val loss: 0.5250043869018555\n",
      "Epoch 4017: train loss: 0.5286492705345154, val loss: 0.5247005820274353\n",
      "Epoch 4018: train loss: 0.528343141078949, val loss: 0.524397075176239\n",
      "Epoch 4019: train loss: 0.5280373096466064, val loss: 0.5240936279296875\n",
      "Epoch 4020: train loss: 0.5277315378189087, val loss: 0.5237903594970703\n",
      "Epoch 4021: train loss: 0.5274258852005005, val loss: 0.5234872102737427\n",
      "Epoch 4022: train loss: 0.5271204113960266, val loss: 0.523184061050415\n",
      "Epoch 4023: train loss: 0.5268149375915527, val loss: 0.5228811502456665\n",
      "Epoch 4024: train loss: 0.5265095233917236, val loss: 0.5225784182548523\n",
      "Epoch 4025: train loss: 0.5262044668197632, val loss: 0.5222757458686829\n",
      "Epoch 4026: train loss: 0.5258995294570923, val loss: 0.5219732522964478\n",
      "Epoch 4027: train loss: 0.5255945920944214, val loss: 0.5216708779335022\n",
      "Epoch 4028: train loss: 0.52528977394104, val loss: 0.5213685035705566\n",
      "Epoch 4029: train loss: 0.5249850749969482, val loss: 0.5210663676261902\n",
      "Epoch 4030: train loss: 0.524680495262146, val loss: 0.5207644701004028\n",
      "Epoch 4031: train loss: 0.5243761539459229, val loss: 0.520462691783905\n",
      "Epoch 4032: train loss: 0.5240719318389893, val loss: 0.5201608538627625\n",
      "Epoch 4033: train loss: 0.5237677693367004, val loss: 0.5198593139648438\n",
      "Epoch 4034: train loss: 0.5234637260437012, val loss: 0.5195576548576355\n",
      "Epoch 4035: train loss: 0.5231598019599915, val loss: 0.5192562937736511\n",
      "Epoch 4036: train loss: 0.5228559970855713, val loss: 0.5189551711082458\n",
      "Epoch 4037: train loss: 0.5225523710250854, val loss: 0.5186541080474854\n",
      "Epoch 4038: train loss: 0.5222489237785339, val loss: 0.5183531641960144\n",
      "Epoch 4039: train loss: 0.5219455361366272, val loss: 0.5180523991584778\n",
      "Epoch 4040: train loss: 0.5216422080993652, val loss: 0.5177516341209412\n",
      "Epoch 4041: train loss: 0.5213390588760376, val loss: 0.5174509882926941\n",
      "Epoch 4042: train loss: 0.5210359692573547, val loss: 0.5171507000923157\n",
      "Epoch 4043: train loss: 0.520733118057251, val loss: 0.5168503522872925\n",
      "Epoch 4044: train loss: 0.5204303860664368, val loss: 0.5165501832962036\n",
      "Epoch 4045: train loss: 0.5201277732849121, val loss: 0.5162501931190491\n",
      "Epoch 4046: train loss: 0.5198252201080322, val loss: 0.5159501433372498\n",
      "Epoch 4047: train loss: 0.5195228457450867, val loss: 0.5156505107879639\n",
      "Epoch 4048: train loss: 0.5192205905914307, val loss: 0.5153509974479675\n",
      "Epoch 4049: train loss: 0.518918514251709, val loss: 0.515051543712616\n",
      "Epoch 4050: train loss: 0.5186165571212769, val loss: 0.5147521495819092\n",
      "Epoch 4051: train loss: 0.5183147192001343, val loss: 0.5144528150558472\n",
      "Epoch 4052: train loss: 0.5180129408836365, val loss: 0.514153778553009\n",
      "Epoch 4053: train loss: 0.5177112221717834, val loss: 0.5138548016548157\n",
      "Epoch 4054: train loss: 0.5174098014831543, val loss: 0.5135560035705566\n",
      "Epoch 4055: train loss: 0.5171084403991699, val loss: 0.5132573843002319\n",
      "Epoch 4056: train loss: 0.5168072581291199, val loss: 0.5129587054252625\n",
      "Epoch 4057: train loss: 0.5165061354637146, val loss: 0.5126603245735168\n",
      "Epoch 4058: train loss: 0.5162050724029541, val loss: 0.5123618245124817\n",
      "Epoch 4059: train loss: 0.5159041881561279, val loss: 0.5120638012886047\n",
      "Epoch 4060: train loss: 0.5156035423278809, val loss: 0.5117657780647278\n",
      "Epoch 4061: train loss: 0.5153028964996338, val loss: 0.5114678740501404\n",
      "Epoch 4062: train loss: 0.515002429485321, val loss: 0.5111700296401978\n",
      "Epoch 4063: train loss: 0.5147020816802979, val loss: 0.5108723640441895\n",
      "Epoch 4064: train loss: 0.5144017338752747, val loss: 0.5105747580528259\n",
      "Epoch 4065: train loss: 0.5141015648841858, val loss: 0.5102774500846863\n",
      "Epoch 4066: train loss: 0.5138016939163208, val loss: 0.5099801421165466\n",
      "Epoch 4067: train loss: 0.5135018229484558, val loss: 0.5096830129623413\n",
      "Epoch 4068: train loss: 0.5132020711898804, val loss: 0.5093861222267151\n",
      "Epoch 4069: train loss: 0.5129024386405945, val loss: 0.5090891122817993\n",
      "Epoch 4070: train loss: 0.5126029253005981, val loss: 0.5087922811508179\n",
      "Epoch 4071: train loss: 0.5123035311698914, val loss: 0.5084957480430603\n",
      "Epoch 4072: train loss: 0.5120043754577637, val loss: 0.5081993341445923\n",
      "Epoch 4073: train loss: 0.511705219745636, val loss: 0.5079030394554138\n",
      "Epoch 4074: train loss: 0.5114062428474426, val loss: 0.5076066851615906\n",
      "Epoch 4075: train loss: 0.511107325553894, val loss: 0.5073105692863464\n",
      "Epoch 4076: train loss: 0.5108085870742798, val loss: 0.5070145726203918\n",
      "Epoch 4077: train loss: 0.5105099081993103, val loss: 0.5067186951637268\n",
      "Epoch 4078: train loss: 0.5102114677429199, val loss: 0.5064231157302856\n",
      "Epoch 4079: train loss: 0.5099131464958191, val loss: 0.5061275362968445\n",
      "Epoch 4080: train loss: 0.509614884853363, val loss: 0.5058321356773376\n",
      "Epoch 4081: train loss: 0.5093168020248413, val loss: 0.5055367350578308\n",
      "Epoch 4082: train loss: 0.5090187191963196, val loss: 0.5052413940429688\n",
      "Epoch 4083: train loss: 0.5087207555770874, val loss: 0.5049464702606201\n",
      "Epoch 4084: train loss: 0.5084230899810791, val loss: 0.5046515464782715\n",
      "Epoch 4085: train loss: 0.5081255435943604, val loss: 0.5043568015098572\n",
      "Epoch 4086: train loss: 0.5078279972076416, val loss: 0.5040621161460876\n",
      "Epoch 4087: train loss: 0.507530689239502, val loss: 0.5037674903869629\n",
      "Epoch 4088: train loss: 0.5072333812713623, val loss: 0.5034732222557068\n",
      "Epoch 4089: train loss: 0.5069363117218018, val loss: 0.5031788945198059\n",
      "Epoch 4090: train loss: 0.5066393613815308, val loss: 0.5028848648071289\n",
      "Epoch 4091: train loss: 0.5063425302505493, val loss: 0.5025908350944519\n",
      "Epoch 4092: train loss: 0.5060458183288574, val loss: 0.5022971034049988\n",
      "Epoch 4093: train loss: 0.5057491660118103, val loss: 0.5020031929016113\n",
      "Epoch 4094: train loss: 0.505452573299408, val loss: 0.5017096400260925\n",
      "Epoch 4095: train loss: 0.5051562786102295, val loss: 0.5014161467552185\n",
      "Epoch 4096: train loss: 0.5048601031303406, val loss: 0.5011228919029236\n",
      "Epoch 4097: train loss: 0.5045639872550964, val loss: 0.5008296370506287\n",
      "Epoch 4098: train loss: 0.5042679905891418, val loss: 0.5005364418029785\n",
      "Epoch 4099: train loss: 0.5039721131324768, val loss: 0.5002433657646179\n",
      "Epoch 4100: train loss: 0.5036762952804565, val loss: 0.49995073676109314\n",
      "Epoch 4101: train loss: 0.5033807754516602, val loss: 0.4996580183506012\n",
      "Epoch 4102: train loss: 0.5030853152275085, val loss: 0.4993654787540436\n",
      "Epoch 4103: train loss: 0.5027899146080017, val loss: 0.49907293915748596\n",
      "Epoch 4104: train loss: 0.5024946331977844, val loss: 0.49878057837486267\n",
      "Epoch 4105: train loss: 0.5021994709968567, val loss: 0.49848848581314087\n",
      "Epoch 4106: train loss: 0.5019044876098633, val loss: 0.4981963336467743\n",
      "Epoch 4107: train loss: 0.5016096234321594, val loss: 0.497904509305954\n",
      "Epoch 4108: train loss: 0.5013149976730347, val loss: 0.49761277437210083\n",
      "Epoch 4109: train loss: 0.5010203123092651, val loss: 0.4973210394382477\n",
      "Epoch 4110: train loss: 0.5007258057594299, val loss: 0.4970293939113617\n",
      "Epoch 4111: train loss: 0.5004314184188843, val loss: 0.4967379570007324\n",
      "Epoch 4112: train loss: 0.5001370906829834, val loss: 0.4964466989040375\n",
      "Epoch 4113: train loss: 0.4998430013656616, val loss: 0.4961555600166321\n",
      "Epoch 4114: train loss: 0.4995490610599518, val loss: 0.49586454033851624\n",
      "Epoch 4115: train loss: 0.4992551803588867, val loss: 0.4955737292766571\n",
      "Epoch 4116: train loss: 0.4989613890647888, val loss: 0.4952828586101532\n",
      "Epoch 4117: train loss: 0.49866774678230286, val loss: 0.49499234557151794\n",
      "Epoch 4118: train loss: 0.4983742833137512, val loss: 0.4947018623352051\n",
      "Epoch 4119: train loss: 0.4980809688568115, val loss: 0.49441152811050415\n",
      "Epoch 4120: train loss: 0.497787743806839, val loss: 0.4941212832927704\n",
      "Epoch 4121: train loss: 0.4974946081638336, val loss: 0.4938311278820038\n",
      "Epoch 4122: train loss: 0.4972015619277954, val loss: 0.4935411512851715\n",
      "Epoch 4123: train loss: 0.49690866470336914, val loss: 0.49325132369995117\n",
      "Epoch 4124: train loss: 0.4966159462928772, val loss: 0.4929616451263428\n",
      "Epoch 4125: train loss: 0.4963234066963196, val loss: 0.49267202615737915\n",
      "Epoch 4126: train loss: 0.49603089690208435, val loss: 0.49238261580467224\n",
      "Epoch 4127: train loss: 0.4957384765148163, val loss: 0.49209317564964294\n",
      "Epoch 4128: train loss: 0.49544620513916016, val loss: 0.4918038845062256\n",
      "Epoch 4129: train loss: 0.4951539933681488, val loss: 0.49151498079299927\n",
      "Epoch 4130: train loss: 0.49486204981803894, val loss: 0.49122604727745056\n",
      "Epoch 4131: train loss: 0.49457022547721863, val loss: 0.490937203168869\n",
      "Epoch 4132: train loss: 0.4942784607410431, val loss: 0.490648478269577\n",
      "Epoch 4133: train loss: 0.4939868152141571, val loss: 0.4903598725795746\n",
      "Epoch 4134: train loss: 0.4936952590942383, val loss: 0.49007129669189453\n",
      "Epoch 4135: train loss: 0.4934037923812866, val loss: 0.4897831082344055\n",
      "Epoch 4136: train loss: 0.4931125342845917, val loss: 0.4894949495792389\n",
      "Epoch 4137: train loss: 0.49282148480415344, val loss: 0.48920702934265137\n",
      "Epoch 4138: train loss: 0.4925304651260376, val loss: 0.4889190196990967\n",
      "Epoch 4139: train loss: 0.49223950505256653, val loss: 0.48863115906715393\n",
      "Epoch 4140: train loss: 0.4919487237930298, val loss: 0.4883435368537903\n",
      "Epoch 4141: train loss: 0.4916580021381378, val loss: 0.4880560040473938\n",
      "Epoch 4142: train loss: 0.49136751890182495, val loss: 0.48776865005493164\n",
      "Epoch 4143: train loss: 0.49107712507247925, val loss: 0.48748141527175903\n",
      "Epoch 4144: train loss: 0.4907868504524231, val loss: 0.4871942102909088\n",
      "Epoch 4145: train loss: 0.4904967248439789, val loss: 0.48690709471702576\n",
      "Epoch 4146: train loss: 0.49020665884017944, val loss: 0.48662033677101135\n",
      "Epoch 4147: train loss: 0.4899168014526367, val loss: 0.48633357882499695\n",
      "Epoch 4148: train loss: 0.48962709307670593, val loss: 0.4860469400882721\n",
      "Epoch 4149: train loss: 0.48933741450309753, val loss: 0.4857605993747711\n",
      "Epoch 4150: train loss: 0.4890478551387787, val loss: 0.4854741096496582\n",
      "Epoch 4151: train loss: 0.4887584447860718, val loss: 0.485187828540802\n",
      "Epoch 4152: train loss: 0.48846906423568726, val loss: 0.4849017560482025\n",
      "Epoch 4153: train loss: 0.488180011510849, val loss: 0.4846158027648926\n",
      "Epoch 4154: train loss: 0.48789098858833313, val loss: 0.4843299984931946\n",
      "Epoch 4155: train loss: 0.4876020848751068, val loss: 0.48404428362846375\n",
      "Epoch 4156: train loss: 0.48731327056884766, val loss: 0.4837585389614105\n",
      "Epoch 4157: train loss: 0.48702457547187805, val loss: 0.48347312211990356\n",
      "Epoch 4158: train loss: 0.4867359697818756, val loss: 0.4831877648830414\n",
      "Epoch 4159: train loss: 0.4864475727081299, val loss: 0.4829026162624359\n",
      "Epoch 4160: train loss: 0.4861593246459961, val loss: 0.48261746764183044\n",
      "Epoch 4161: train loss: 0.48587116599082947, val loss: 0.48233267664909363\n",
      "Epoch 4162: train loss: 0.48558309674263, val loss: 0.4820477068424225\n",
      "Epoch 4163: train loss: 0.4852950870990753, val loss: 0.48176297545433044\n",
      "Epoch 4164: train loss: 0.48500722646713257, val loss: 0.4814784526824951\n",
      "Epoch 4165: train loss: 0.4847196042537689, val loss: 0.48119401931762695\n",
      "Epoch 4166: train loss: 0.48443207144737244, val loss: 0.4809097349643707\n",
      "Epoch 4167: train loss: 0.48414456844329834, val loss: 0.4806254506111145\n",
      "Epoch 4168: train loss: 0.48385727405548096, val loss: 0.48034143447875977\n",
      "Epoch 4169: train loss: 0.48357003927230835, val loss: 0.48005756735801697\n",
      "Epoch 4170: train loss: 0.48328307271003723, val loss: 0.4797738194465637\n",
      "Epoch 4171: train loss: 0.4829961359500885, val loss: 0.4794900119304657\n",
      "Epoch 4172: train loss: 0.4827093482017517, val loss: 0.47920656204223633\n",
      "Epoch 4173: train loss: 0.4824226498603821, val loss: 0.47892317175865173\n",
      "Epoch 4174: train loss: 0.4821360409259796, val loss: 0.4786398112773895\n",
      "Epoch 4175: train loss: 0.4818495213985443, val loss: 0.47835665941238403\n",
      "Epoch 4176: train loss: 0.4815632700920105, val loss: 0.47807368636131287\n",
      "Epoch 4177: train loss: 0.48127710819244385, val loss: 0.4777907431125641\n",
      "Epoch 4178: train loss: 0.48099103569984436, val loss: 0.4775080680847168\n",
      "Epoch 4179: train loss: 0.4807051122188568, val loss: 0.4772253930568695\n",
      "Epoch 4180: train loss: 0.4804191589355469, val loss: 0.4769427478313446\n",
      "Epoch 4181: train loss: 0.4801333546638489, val loss: 0.47666046023368835\n",
      "Epoch 4182: train loss: 0.47984784841537476, val loss: 0.47637826204299927\n",
      "Epoch 4183: train loss: 0.4795624613761902, val loss: 0.4760960638523102\n",
      "Epoch 4184: train loss: 0.479277104139328, val loss: 0.47581395506858826\n",
      "Epoch 4185: train loss: 0.478991836309433, val loss: 0.4755321145057678\n",
      "Epoch 4186: train loss: 0.4787067174911499, val loss: 0.4752504527568817\n",
      "Epoch 4187: train loss: 0.47842180728912354, val loss: 0.47496888041496277\n",
      "Epoch 4188: train loss: 0.4781370162963867, val loss: 0.4746874272823334\n",
      "Epoch 4189: train loss: 0.47785231471061707, val loss: 0.47440609335899353\n",
      "Epoch 4190: train loss: 0.4775676429271698, val loss: 0.4741247296333313\n",
      "Epoch 4191: train loss: 0.47728314995765686, val loss: 0.4738435447216034\n",
      "Epoch 4192: train loss: 0.4769987463951111, val loss: 0.47356268763542175\n",
      "Epoch 4193: train loss: 0.4767145812511444, val loss: 0.4732818603515625\n",
      "Epoch 4194: train loss: 0.4764305353164673, val loss: 0.4730011522769928\n",
      "Epoch 4195: train loss: 0.47614654898643494, val loss: 0.47272053360939026\n",
      "Epoch 4196: train loss: 0.47586265206336975, val loss: 0.47244006395339966\n",
      "Epoch 4197: train loss: 0.47557881474494934, val loss: 0.47215962409973145\n",
      "Epoch 4198: train loss: 0.47529518604278564, val loss: 0.4718795418739319\n",
      "Epoch 4199: train loss: 0.4750117063522339, val loss: 0.47159940004348755\n",
      "Epoch 4200: train loss: 0.47472837567329407, val loss: 0.4713194966316223\n",
      "Epoch 4201: train loss: 0.474445104598999, val loss: 0.47103968262672424\n",
      "Epoch 4202: train loss: 0.47416195273399353, val loss: 0.47075986862182617\n",
      "Epoch 4203: train loss: 0.4738789200782776, val loss: 0.470480352640152\n",
      "Epoch 4204: train loss: 0.47359612584114075, val loss: 0.47020092606544495\n",
      "Epoch 4205: train loss: 0.4733133614063263, val loss: 0.469921737909317\n",
      "Epoch 4206: train loss: 0.4730307459831238, val loss: 0.46964240074157715\n",
      "Epoch 4207: train loss: 0.4727482497692108, val loss: 0.46936336159706116\n",
      "Epoch 4208: train loss: 0.4724658131599426, val loss: 0.46908435225486755\n",
      "Epoch 4209: train loss: 0.4721834361553192, val loss: 0.4688056409358978\n",
      "Epoch 4210: train loss: 0.47190138697624207, val loss: 0.4685269296169281\n",
      "Epoch 4211: train loss: 0.4716194272041321, val loss: 0.4682484567165375\n",
      "Epoch 4212: train loss: 0.4713374674320221, val loss: 0.46796995401382446\n",
      "Epoch 4213: train loss: 0.47105568647384644, val loss: 0.4676916301250458\n",
      "Epoch 4214: train loss: 0.47077399492263794, val loss: 0.46741342544555664\n",
      "Epoch 4215: train loss: 0.47049230337142944, val loss: 0.46713533997535706\n",
      "Epoch 4216: train loss: 0.4702110290527344, val loss: 0.46685758233070374\n",
      "Epoch 4217: train loss: 0.4699297845363617, val loss: 0.4665798246860504\n",
      "Epoch 4218: train loss: 0.4696485698223114, val loss: 0.46630212664604187\n",
      "Epoch 4219: train loss: 0.46936747431755066, val loss: 0.46602439880371094\n",
      "Epoch 4220: train loss: 0.46908649802207947, val loss: 0.4657471776008606\n",
      "Epoch 4221: train loss: 0.46880578994750977, val loss: 0.46546992659568787\n",
      "Epoch 4222: train loss: 0.46852514147758484, val loss: 0.4651927947998047\n",
      "Epoch 4223: train loss: 0.46824461221694946, val loss: 0.46491575241088867\n",
      "Epoch 4224: train loss: 0.46796417236328125, val loss: 0.4646388590335846\n",
      "Epoch 4225: train loss: 0.4676837623119354, val loss: 0.4643619656562805\n",
      "Epoch 4226: train loss: 0.4674035608768463, val loss: 0.4640854001045227\n",
      "Epoch 4227: train loss: 0.46712350845336914, val loss: 0.46380892395973206\n",
      "Epoch 4228: train loss: 0.4668436348438263, val loss: 0.46353253722190857\n",
      "Epoch 4229: train loss: 0.4665638208389282, val loss: 0.4632563292980194\n",
      "Epoch 4230: train loss: 0.4662840664386749, val loss: 0.4629800021648407\n",
      "Epoch 4231: train loss: 0.46600446105003357, val loss: 0.46270403265953064\n",
      "Epoch 4232: train loss: 0.465724915266037, val loss: 0.46242815256118774\n",
      "Epoch 4233: train loss: 0.4654456377029419, val loss: 0.4621523916721344\n",
      "Epoch 4234: train loss: 0.4651663899421692, val loss: 0.46187686920166016\n",
      "Epoch 4235: train loss: 0.4648873209953308, val loss: 0.46160125732421875\n",
      "Epoch 4236: train loss: 0.4646083414554596, val loss: 0.4613257944583893\n",
      "Epoch 4237: train loss: 0.46432945132255554, val loss: 0.4610506594181061\n",
      "Epoch 4238: train loss: 0.4640507698059082, val loss: 0.46077561378479004\n",
      "Epoch 4239: train loss: 0.4637722074985504, val loss: 0.460500568151474\n",
      "Epoch 4240: train loss: 0.4634937047958374, val loss: 0.4602256715297699\n",
      "Epoch 4241: train loss: 0.46321535110473633, val loss: 0.45995092391967773\n",
      "Epoch 4242: train loss: 0.4629370868206024, val loss: 0.45967626571655273\n",
      "Epoch 4243: train loss: 0.4626588821411133, val loss: 0.45940181612968445\n",
      "Epoch 4244: train loss: 0.46238094568252563, val loss: 0.4591274857521057\n",
      "Epoch 4245: train loss: 0.46210309863090515, val loss: 0.45885321497917175\n",
      "Epoch 4246: train loss: 0.46182531118392944, val loss: 0.4585791528224945\n",
      "Epoch 4247: train loss: 0.46154770255088806, val loss: 0.45830512046813965\n",
      "Epoch 4248: train loss: 0.46127012372016907, val loss: 0.4580313265323639\n",
      "Epoch 4249: train loss: 0.4609927535057068, val loss: 0.4577576220035553\n",
      "Epoch 4250: train loss: 0.46071553230285645, val loss: 0.45748406648635864\n",
      "Epoch 4251: train loss: 0.46043846011161804, val loss: 0.45721060037612915\n",
      "Epoch 4252: train loss: 0.46016135811805725, val loss: 0.4569372236728668\n",
      "Epoch 4253: train loss: 0.45988449454307556, val loss: 0.4566638469696045\n",
      "Epoch 4254: train loss: 0.45960763096809387, val loss: 0.4563908278942108\n",
      "Epoch 4255: train loss: 0.4593310058116913, val loss: 0.4561179578304291\n",
      "Epoch 4256: train loss: 0.45905449986457825, val loss: 0.45584508776664734\n",
      "Epoch 4257: train loss: 0.45877814292907715, val loss: 0.45557236671447754\n",
      "Epoch 4258: train loss: 0.45850178599357605, val loss: 0.4552996754646301\n",
      "Epoch 4259: train loss: 0.4582255482673645, val loss: 0.45502716302871704\n",
      "Epoch 4260: train loss: 0.4579494595527649, val loss: 0.45475491881370544\n",
      "Epoch 4261: train loss: 0.457673579454422, val loss: 0.454482764005661\n",
      "Epoch 4262: train loss: 0.4573977589607239, val loss: 0.4542106091976166\n",
      "Epoch 4263: train loss: 0.45712214708328247, val loss: 0.4539385735988617\n",
      "Epoch 4264: train loss: 0.4568465054035187, val loss: 0.45366668701171875\n",
      "Epoch 4265: train loss: 0.45657095313072205, val loss: 0.45339512825012207\n",
      "Epoch 4266: train loss: 0.4562956988811493, val loss: 0.4531234800815582\n",
      "Epoch 4267: train loss: 0.4560205340385437, val loss: 0.4528522193431854\n",
      "Epoch 4268: train loss: 0.4557454586029053, val loss: 0.4525807499885559\n",
      "Epoch 4269: train loss: 0.4554704427719116, val loss: 0.45230957865715027\n",
      "Epoch 4270: train loss: 0.4551956355571747, val loss: 0.45203837752342224\n",
      "Epoch 4271: train loss: 0.45492085814476013, val loss: 0.4517675042152405\n",
      "Epoch 4272: train loss: 0.4546463191509247, val loss: 0.4514966905117035\n",
      "Epoch 4273: train loss: 0.4543718099594116, val loss: 0.45122599601745605\n",
      "Epoch 4274: train loss: 0.4540975093841553, val loss: 0.45095545053482056\n",
      "Epoch 4275: train loss: 0.4538232386112213, val loss: 0.4506848454475403\n",
      "Epoch 4276: train loss: 0.45354902744293213, val loss: 0.4504147171974182\n",
      "Epoch 4277: train loss: 0.4532751441001892, val loss: 0.45014452934265137\n",
      "Epoch 4278: train loss: 0.4530012905597687, val loss: 0.44987449049949646\n",
      "Epoch 4279: train loss: 0.4527275562286377, val loss: 0.44960451126098633\n",
      "Epoch 4280: train loss: 0.4524539113044739, val loss: 0.44933468103408813\n",
      "Epoch 4281: train loss: 0.452180415391922, val loss: 0.4490649402141571\n",
      "Epoch 4282: train loss: 0.45190688967704773, val loss: 0.4487954080104828\n",
      "Epoch 4283: train loss: 0.4516337215900421, val loss: 0.44852596521377563\n",
      "Epoch 4284: train loss: 0.4513605833053589, val loss: 0.4482567012310028\n",
      "Epoch 4285: train loss: 0.4510875940322876, val loss: 0.44798746705055237\n",
      "Epoch 4286: train loss: 0.4508146643638611, val loss: 0.4477182924747467\n",
      "Epoch 4287: train loss: 0.45054179430007935, val loss: 0.447449266910553\n",
      "Epoch 4288: train loss: 0.45026907324790955, val loss: 0.4471805691719055\n",
      "Epoch 4289: train loss: 0.44999659061431885, val loss: 0.44691190123558044\n",
      "Epoch 4290: train loss: 0.4497241973876953, val loss: 0.44664326310157776\n",
      "Epoch 4291: train loss: 0.44945183396339417, val loss: 0.4463748633861542\n",
      "Epoch 4292: train loss: 0.44917958974838257, val loss: 0.446106493473053\n",
      "Epoch 4293: train loss: 0.4489074945449829, val loss: 0.4458382725715637\n",
      "Epoch 4294: train loss: 0.44863566756248474, val loss: 0.44557031989097595\n",
      "Epoch 4295: train loss: 0.4483638405799866, val loss: 0.44530245661735535\n",
      "Epoch 4296: train loss: 0.44809219241142273, val loss: 0.4450345039367676\n",
      "Epoch 4297: train loss: 0.4478205442428589, val loss: 0.4447668194770813\n",
      "Epoch 4298: train loss: 0.44754907488822937, val loss: 0.4444991648197174\n",
      "Epoch 4299: train loss: 0.44727766513824463, val loss: 0.44423189759254456\n",
      "Epoch 4300: train loss: 0.447006493806839, val loss: 0.4439643919467926\n",
      "Epoch 4301: train loss: 0.4467353820800781, val loss: 0.44369736313819885\n",
      "Epoch 4302: train loss: 0.4464643895626068, val loss: 0.44343024492263794\n",
      "Epoch 4303: train loss: 0.44619354605674744, val loss: 0.4431632161140442\n",
      "Epoch 4304: train loss: 0.44592273235321045, val loss: 0.44289645552635193\n",
      "Epoch 4305: train loss: 0.44565218687057495, val loss: 0.4426298141479492\n",
      "Epoch 4306: train loss: 0.4453817307949066, val loss: 0.44236332178115845\n",
      "Epoch 4307: train loss: 0.44511133432388306, val loss: 0.4420967996120453\n",
      "Epoch 4308: train loss: 0.44484108686447144, val loss: 0.44183045625686646\n",
      "Epoch 4309: train loss: 0.4445708990097046, val loss: 0.4415642321109772\n",
      "Epoch 4310: train loss: 0.4443008601665497, val loss: 0.44129809737205505\n",
      "Epoch 4311: train loss: 0.4440309703350067, val loss: 0.4410322606563568\n",
      "Epoch 4312: train loss: 0.4437612295150757, val loss: 0.4407665431499481\n",
      "Epoch 4313: train loss: 0.4434915781021118, val loss: 0.44050079584121704\n",
      "Epoch 4314: train loss: 0.4432220160961151, val loss: 0.4402351379394531\n",
      "Epoch 4315: train loss: 0.4429525136947632, val loss: 0.43996986746788025\n",
      "Epoch 4316: train loss: 0.4426833391189575, val loss: 0.43970462679862976\n",
      "Epoch 4317: train loss: 0.44241419434547424, val loss: 0.4394393563270569\n",
      "Epoch 4318: train loss: 0.4421451687812805, val loss: 0.43917426466941833\n",
      "Epoch 4319: train loss: 0.44187620282173157, val loss: 0.43890926241874695\n",
      "Epoch 4320: train loss: 0.4416073262691498, val loss: 0.4386444091796875\n",
      "Epoch 4321: train loss: 0.44133856892585754, val loss: 0.43837982416152954\n",
      "Epoch 4322: train loss: 0.441070020198822, val loss: 0.43811526894569397\n",
      "Epoch 4323: train loss: 0.44080162048339844, val loss: 0.4378507733345032\n",
      "Epoch 4324: train loss: 0.44053328037261963, val loss: 0.4375864565372467\n",
      "Epoch 4325: train loss: 0.44026505947113037, val loss: 0.4373222291469574\n",
      "Epoch 4326: train loss: 0.4399968981742859, val loss: 0.4370581805706024\n",
      "Epoch 4327: train loss: 0.4397289752960205, val loss: 0.43679437041282654\n",
      "Epoch 4328: train loss: 0.43946120142936707, val loss: 0.43653059005737305\n",
      "Epoch 4329: train loss: 0.4391934871673584, val loss: 0.43626680970191956\n",
      "Epoch 4330: train loss: 0.4389258027076721, val loss: 0.4360032081604004\n",
      "Epoch 4331: train loss: 0.43865832686424255, val loss: 0.43573981523513794\n",
      "Epoch 4332: train loss: 0.4383908808231354, val loss: 0.4354764521121979\n",
      "Epoch 4333: train loss: 0.4381236732006073, val loss: 0.435213178396225\n",
      "Epoch 4334: train loss: 0.4378565549850464, val loss: 0.4349502623081207\n",
      "Epoch 4335: train loss: 0.43758952617645264, val loss: 0.4346873462200165\n",
      "Epoch 4336: train loss: 0.43732258677482605, val loss: 0.43442437052726746\n",
      "Epoch 4337: train loss: 0.4370557963848114, val loss: 0.4341617226600647\n",
      "Epoch 4338: train loss: 0.4367891550064087, val loss: 0.4338992238044739\n",
      "Epoch 4339: train loss: 0.4365226924419403, val loss: 0.43363675475120544\n",
      "Epoch 4340: train loss: 0.4362562894821167, val loss: 0.4333744943141937\n",
      "Epoch 4341: train loss: 0.43598997592926025, val loss: 0.433112233877182\n",
      "Epoch 4342: train loss: 0.4357237219810486, val loss: 0.43285009264945984\n",
      "Epoch 4343: train loss: 0.43545758724212646, val loss: 0.4325881898403168\n",
      "Epoch 4344: train loss: 0.4351917505264282, val loss: 0.43232640624046326\n",
      "Epoch 4345: train loss: 0.43492594361305237, val loss: 0.4320646822452545\n",
      "Epoch 4346: train loss: 0.4346601665019989, val loss: 0.4318031370639801\n",
      "Epoch 4347: train loss: 0.43439459800720215, val loss: 0.4315415322780609\n",
      "Epoch 4348: train loss: 0.43412911891937256, val loss: 0.43128034472465515\n",
      "Epoch 4349: train loss: 0.4338638484477997, val loss: 0.431019127368927\n",
      "Epoch 4350: train loss: 0.4335986375808716, val loss: 0.4307580590248108\n",
      "Epoch 4351: train loss: 0.43333354592323303, val loss: 0.4304971396923065\n",
      "Epoch 4352: train loss: 0.43306857347488403, val loss: 0.43023625016212463\n",
      "Epoch 4353: train loss: 0.4328036606311798, val loss: 0.4299754202365875\n",
      "Epoch 4354: train loss: 0.43253886699676514, val loss: 0.4297149181365967\n",
      "Epoch 4355: train loss: 0.4322742223739624, val loss: 0.4294544756412506\n",
      "Epoch 4356: train loss: 0.432009756565094, val loss: 0.42919421195983887\n",
      "Epoch 4357: train loss: 0.431745320558548, val loss: 0.4289338290691376\n",
      "Epoch 4358: train loss: 0.43148109316825867, val loss: 0.428673654794693\n",
      "Epoch 4359: train loss: 0.43121686577796936, val loss: 0.4284137785434723\n",
      "Epoch 4360: train loss: 0.43095287680625916, val loss: 0.42815396189689636\n",
      "Epoch 4361: train loss: 0.4306890070438385, val loss: 0.42789432406425476\n",
      "Epoch 4362: train loss: 0.4304252862930298, val loss: 0.42763471603393555\n",
      "Epoch 4363: train loss: 0.4301615357398987, val loss: 0.4273752272129059\n",
      "Epoch 4364: train loss: 0.4298979640007019, val loss: 0.4271159768104553\n",
      "Epoch 4365: train loss: 0.42963460087776184, val loss: 0.42685672640800476\n",
      "Epoch 4366: train loss: 0.42937132716178894, val loss: 0.4265976548194885\n",
      "Epoch 4367: train loss: 0.4291081428527832, val loss: 0.42633867263793945\n",
      "Epoch 4368: train loss: 0.42884504795074463, val loss: 0.42607975006103516\n",
      "Epoch 4369: train loss: 0.428582102060318, val loss: 0.425820916891098\n",
      "Epoch 4370: train loss: 0.42831918597221375, val loss: 0.42556238174438477\n",
      "Epoch 4371: train loss: 0.4280565679073334, val loss: 0.42530402541160583\n",
      "Epoch 4372: train loss: 0.4277939796447754, val loss: 0.42504557967185974\n",
      "Epoch 4373: train loss: 0.42753154039382935, val loss: 0.42478737235069275\n",
      "Epoch 4374: train loss: 0.4272691607475281, val loss: 0.42452922463417053\n",
      "Epoch 4375: train loss: 0.4270068407058716, val loss: 0.4242713451385498\n",
      "Epoch 4376: train loss: 0.4267447590827942, val loss: 0.42401352524757385\n",
      "Epoch 4377: train loss: 0.42648279666900635, val loss: 0.42375585436820984\n",
      "Epoch 4378: train loss: 0.42622095346450806, val loss: 0.42349812388420105\n",
      "Epoch 4379: train loss: 0.42595914006233215, val loss: 0.42324066162109375\n",
      "Epoch 4380: train loss: 0.4256974756717682, val loss: 0.4229832589626312\n",
      "Epoch 4381: train loss: 0.4254359006881714, val loss: 0.4227260649204254\n",
      "Epoch 4382: train loss: 0.4251745045185089, val loss: 0.422468900680542\n",
      "Epoch 4383: train loss: 0.4249132573604584, val loss: 0.4222119450569153\n",
      "Epoch 4384: train loss: 0.4246520698070526, val loss: 0.42195501923561096\n",
      "Epoch 4385: train loss: 0.4243910014629364, val loss: 0.42169833183288574\n",
      "Epoch 4386: train loss: 0.42412999272346497, val loss: 0.4214416444301605\n",
      "Epoch 4387: train loss: 0.423869252204895, val loss: 0.4211852252483368\n",
      "Epoch 4388: train loss: 0.4236086308956146, val loss: 0.42092886567115784\n",
      "Epoch 4389: train loss: 0.423348069190979, val loss: 0.42067256569862366\n",
      "Epoch 4390: train loss: 0.42308756709098816, val loss: 0.42041635513305664\n",
      "Epoch 4391: train loss: 0.4228271543979645, val loss: 0.42016029357910156\n",
      "Epoch 4392: train loss: 0.42256686091423035, val loss: 0.4199044704437256\n",
      "Epoch 4393: train loss: 0.4223068058490753, val loss: 0.4196487367153168\n",
      "Epoch 4394: train loss: 0.42204684019088745, val loss: 0.41939306259155273\n",
      "Epoch 4395: train loss: 0.42178696393966675, val loss: 0.41913753747940063\n",
      "Epoch 4396: train loss: 0.4215272068977356, val loss: 0.4188821017742157\n",
      "Epoch 4397: train loss: 0.4212675392627716, val loss: 0.4186268746852875\n",
      "Epoch 4398: train loss: 0.42100805044174194, val loss: 0.41837167739868164\n",
      "Epoch 4399: train loss: 0.4207487404346466, val loss: 0.41811665892601013\n",
      "Epoch 4400: train loss: 0.42048943042755127, val loss: 0.4178617000579834\n",
      "Epoch 4401: train loss: 0.4202302396297455, val loss: 0.417606920003891\n",
      "Epoch 4402: train loss: 0.41997113823890686, val loss: 0.4173522889614105\n",
      "Epoch 4403: train loss: 0.4197123348712921, val loss: 0.41709786653518677\n",
      "Epoch 4404: train loss: 0.41945356130599976, val loss: 0.4168434143066406\n",
      "Epoch 4405: train loss: 0.41919487714767456, val loss: 0.41658908128738403\n",
      "Epoch 4406: train loss: 0.41893625259399414, val loss: 0.416334867477417\n",
      "Epoch 4407: train loss: 0.4186778664588928, val loss: 0.4160807728767395\n",
      "Epoch 4408: train loss: 0.41841939091682434, val loss: 0.4158268868923187\n",
      "Epoch 4409: train loss: 0.4181612730026245, val loss: 0.4155731201171875\n",
      "Epoch 4410: train loss: 0.41790318489074707, val loss: 0.41531944274902344\n",
      "Epoch 4411: train loss: 0.4176451861858368, val loss: 0.4150659143924713\n",
      "Epoch 4412: train loss: 0.41738730669021606, val loss: 0.4148123264312744\n",
      "Epoch 4413: train loss: 0.4171295166015625, val loss: 0.41455918550491333\n",
      "Epoch 4414: train loss: 0.4168719947338104, val loss: 0.41430598497390747\n",
      "Epoch 4415: train loss: 0.4166145324707031, val loss: 0.41405293345451355\n",
      "Epoch 4416: train loss: 0.4163571894168854, val loss: 0.4137999713420868\n",
      "Epoch 4417: train loss: 0.4160998463630676, val loss: 0.4135471284389496\n",
      "Epoch 4418: train loss: 0.4158427119255066, val loss: 0.41329440474510193\n",
      "Epoch 4419: train loss: 0.41558557748794556, val loss: 0.4130418002605438\n",
      "Epoch 4420: train loss: 0.4153287410736084, val loss: 0.4127894341945648\n",
      "Epoch 4421: train loss: 0.41507193446159363, val loss: 0.4125370681285858\n",
      "Epoch 4422: train loss: 0.4148152470588684, val loss: 0.4122847616672516\n",
      "Epoch 4423: train loss: 0.41455867886543274, val loss: 0.41203269362449646\n",
      "Epoch 4424: train loss: 0.41430211067199707, val loss: 0.4117807447910309\n",
      "Epoch 4425: train loss: 0.41404590010643005, val loss: 0.41152891516685486\n",
      "Epoch 4426: train loss: 0.4137897193431854, val loss: 0.4112772047519684\n",
      "Epoch 4427: train loss: 0.41353362798690796, val loss: 0.41102561354637146\n",
      "Epoch 4428: train loss: 0.41327762603759766, val loss: 0.4107741415500641\n",
      "Epoch 4429: train loss: 0.4130217432975769, val loss: 0.4105229079723358\n",
      "Epoch 4430: train loss: 0.41276606917381287, val loss: 0.41027164459228516\n",
      "Epoch 4431: train loss: 0.4125104248523712, val loss: 0.4100204408168793\n",
      "Epoch 4432: train loss: 0.4122549891471863, val loss: 0.4097695052623749\n",
      "Epoch 4433: train loss: 0.41199955344200134, val loss: 0.40951862931251526\n",
      "Epoch 4434: train loss: 0.41174426674842834, val loss: 0.4092678129673004\n",
      "Epoch 4435: train loss: 0.4114890396595001, val loss: 0.4090171754360199\n",
      "Epoch 4436: train loss: 0.4112341105937958, val loss: 0.4087667167186737\n",
      "Epoch 4437: train loss: 0.4109792113304138, val loss: 0.4085163176059723\n",
      "Epoch 4438: train loss: 0.410724401473999, val loss: 0.4082660675048828\n",
      "Epoch 4439: train loss: 0.4104697108268738, val loss: 0.40801581740379333\n",
      "Epoch 4440: train loss: 0.4102150499820709, val loss: 0.40776586532592773\n",
      "Epoch 4441: train loss: 0.40996068716049194, val loss: 0.4075160026550293\n",
      "Epoch 4442: train loss: 0.40970638394355774, val loss: 0.407266229391098\n",
      "Epoch 4443: train loss: 0.4094522297382355, val loss: 0.4070165753364563\n",
      "Epoch 4444: train loss: 0.4091981053352356, val loss: 0.40676698088645935\n",
      "Epoch 4445: train loss: 0.4089440405368805, val loss: 0.40651771426200867\n",
      "Epoch 4446: train loss: 0.4086902439594269, val loss: 0.4062684178352356\n",
      "Epoch 4447: train loss: 0.4084365963935852, val loss: 0.4060192108154297\n",
      "Epoch 4448: train loss: 0.40818294882774353, val loss: 0.40577030181884766\n",
      "Epoch 4449: train loss: 0.4079294502735138, val loss: 0.40552130341529846\n",
      "Epoch 4450: train loss: 0.4076760411262512, val loss: 0.40527239441871643\n",
      "Epoch 4451: train loss: 0.4074226915836334, val loss: 0.40502387285232544\n",
      "Epoch 4452: train loss: 0.4071696102619171, val loss: 0.4047752916812897\n",
      "Epoch 4453: train loss: 0.4069165587425232, val loss: 0.404526948928833\n",
      "Epoch 4454: train loss: 0.4066636562347412, val loss: 0.40427857637405396\n",
      "Epoch 4455: train loss: 0.4064107835292816, val loss: 0.4040302336215973\n",
      "Epoch 4456: train loss: 0.40615811944007874, val loss: 0.40378227829933167\n",
      "Epoch 4457: train loss: 0.4059055745601654, val loss: 0.4035343825817108\n",
      "Epoch 4458: train loss: 0.4056531488895416, val loss: 0.40328654646873474\n",
      "Epoch 4459: train loss: 0.4054008722305298, val loss: 0.4030388295650482\n",
      "Epoch 4460: train loss: 0.40514856576919556, val loss: 0.40279123187065125\n",
      "Epoch 4461: train loss: 0.40489646792411804, val loss: 0.4025437831878662\n",
      "Epoch 4462: train loss: 0.40464451909065247, val loss: 0.4022965133190155\n",
      "Epoch 4463: train loss: 0.40439268946647644, val loss: 0.4020492732524872\n",
      "Epoch 4464: train loss: 0.40414097905158997, val loss: 0.4018021523952484\n",
      "Epoch 4465: train loss: 0.4038892984390259, val loss: 0.40155521035194397\n",
      "Epoch 4466: train loss: 0.4036377966403961, val loss: 0.4013083577156067\n",
      "Epoch 4467: train loss: 0.4033864438533783, val loss: 0.40106162428855896\n",
      "Epoch 4468: train loss: 0.40313518047332764, val loss: 0.40081509947776794\n",
      "Epoch 4469: train loss: 0.4028840959072113, val loss: 0.40056854486465454\n",
      "Epoch 4470: train loss: 0.40263301134109497, val loss: 0.40032219886779785\n",
      "Epoch 4471: train loss: 0.4023820757865906, val loss: 0.4000759720802307\n",
      "Epoch 4472: train loss: 0.4021311402320862, val loss: 0.39982983469963074\n",
      "Epoch 4473: train loss: 0.40188056230545044, val loss: 0.3995838761329651\n",
      "Epoch 4474: train loss: 0.4016299843788147, val loss: 0.3993380665779114\n",
      "Epoch 4475: train loss: 0.4013795256614685, val loss: 0.3990921676158905\n",
      "Epoch 4476: train loss: 0.4011291563510895, val loss: 0.3988465368747711\n",
      "Epoch 4477: train loss: 0.4008788764476776, val loss: 0.3986010253429413\n",
      "Epoch 4478: train loss: 0.40062883496284485, val loss: 0.3983556926250458\n",
      "Epoch 4479: train loss: 0.400378942489624, val loss: 0.39811035990715027\n",
      "Epoch 4480: train loss: 0.4001290202140808, val loss: 0.3978652358055115\n",
      "Epoch 4481: train loss: 0.39987921714782715, val loss: 0.39762020111083984\n",
      "Epoch 4482: train loss: 0.39962950348854065, val loss: 0.39737531542778015\n",
      "Epoch 4483: train loss: 0.399380087852478, val loss: 0.3971305787563324\n",
      "Epoch 4484: train loss: 0.3991306722164154, val loss: 0.3968859612941742\n",
      "Epoch 4485: train loss: 0.3988814651966095, val loss: 0.3966412842273712\n",
      "Epoch 4486: train loss: 0.3986322581768036, val loss: 0.3963968753814697\n",
      "Epoch 4487: train loss: 0.39838317036628723, val loss: 0.3961525559425354\n",
      "Epoch 4488: train loss: 0.3981342017650604, val loss: 0.39590832591056824\n",
      "Epoch 4489: train loss: 0.39788535237312317, val loss: 0.39566436409950256\n",
      "Epoch 4490: train loss: 0.39763668179512024, val loss: 0.3954203724861145\n",
      "Epoch 4491: train loss: 0.39738813042640686, val loss: 0.395176500082016\n",
      "Epoch 4492: train loss: 0.3971395790576935, val loss: 0.39493274688720703\n",
      "Epoch 4493: train loss: 0.39689111709594727, val loss: 0.39468929171562195\n",
      "Epoch 4494: train loss: 0.3966429531574249, val loss: 0.3944457173347473\n",
      "Epoch 4495: train loss: 0.39639487862586975, val loss: 0.39420247077941895\n",
      "Epoch 4496: train loss: 0.39614686369895935, val loss: 0.3939591646194458\n",
      "Epoch 4497: train loss: 0.3958989083766937, val loss: 0.39371609687805176\n",
      "Epoch 4498: train loss: 0.39565107226371765, val loss: 0.3934731185436249\n",
      "Epoch 4499: train loss: 0.39540350437164307, val loss: 0.39323022961616516\n",
      "Epoch 4500: train loss: 0.39515602588653564, val loss: 0.3929874897003174\n",
      "Epoch 4501: train loss: 0.3949085772037506, val loss: 0.39274492859840393\n",
      "Epoch 4502: train loss: 0.3946612477302551, val loss: 0.39250239729881287\n",
      "Epoch 4503: train loss: 0.3944139778614044, val loss: 0.39226004481315613\n",
      "Epoch 4504: train loss: 0.3941670060157776, val loss: 0.39201781153678894\n",
      "Epoch 4505: train loss: 0.39392006397247314, val loss: 0.3917757570743561\n",
      "Epoch 4506: train loss: 0.39367321133613586, val loss: 0.3915337026119232\n",
      "Epoch 4507: train loss: 0.39342644810676575, val loss: 0.3912917375564575\n",
      "Epoch 4508: train loss: 0.3931798040866852, val loss: 0.39104992151260376\n",
      "Epoch 4509: train loss: 0.3929332494735718, val loss: 0.3908082842826843\n",
      "Epoch 4510: train loss: 0.3926869332790375, val loss: 0.39056679606437683\n",
      "Epoch 4511: train loss: 0.39244064688682556, val loss: 0.39032530784606934\n",
      "Epoch 4512: train loss: 0.3921944499015808, val loss: 0.39008405804634094\n",
      "Epoch 4513: train loss: 0.3919484615325928, val loss: 0.3898427486419678\n",
      "Epoch 4514: train loss: 0.39170244336128235, val loss: 0.38960179686546326\n",
      "Epoch 4515: train loss: 0.3914566934108734, val loss: 0.38936081528663635\n",
      "Epoch 4516: train loss: 0.39121103286743164, val loss: 0.38912007212638855\n",
      "Epoch 4517: train loss: 0.39096546173095703, val loss: 0.38887932896614075\n",
      "Epoch 4518: train loss: 0.3907199501991272, val loss: 0.3886387348175049\n",
      "Epoch 4519: train loss: 0.3904745280742645, val loss: 0.38839825987815857\n",
      "Epoch 4520: train loss: 0.3902294337749481, val loss: 0.38815799355506897\n",
      "Epoch 4521: train loss: 0.3899843394756317, val loss: 0.3879178762435913\n",
      "Epoch 4522: train loss: 0.38973933458328247, val loss: 0.38767769932746887\n",
      "Epoch 4523: train loss: 0.3894944190979004, val loss: 0.3874377906322479\n",
      "Epoch 4524: train loss: 0.38924965262413025, val loss: 0.387197881937027\n",
      "Epoch 4525: train loss: 0.38900506496429443, val loss: 0.38695812225341797\n",
      "Epoch 4526: train loss: 0.3887605667114258, val loss: 0.38671866059303284\n",
      "Epoch 4527: train loss: 0.3885161280632019, val loss: 0.38647904992103577\n",
      "Epoch 4528: train loss: 0.38827183842658997, val loss: 0.3862396776676178\n",
      "Epoch 4529: train loss: 0.3880276381969452, val loss: 0.38600051403045654\n",
      "Epoch 4530: train loss: 0.38778355717658997, val loss: 0.3857613503932953\n",
      "Epoch 4531: train loss: 0.38753968477249146, val loss: 0.38552242517471313\n",
      "Epoch 4532: train loss: 0.3872958719730377, val loss: 0.3852834701538086\n",
      "Epoch 4533: train loss: 0.38705217838287354, val loss: 0.3850446939468384\n",
      "Epoch 4534: train loss: 0.3868085443973541, val loss: 0.38480594754219055\n",
      "Epoch 4535: train loss: 0.3865649104118347, val loss: 0.38456735014915466\n",
      "Epoch 4536: train loss: 0.3863216042518616, val loss: 0.3843289911746979\n",
      "Epoch 4537: train loss: 0.3860783874988556, val loss: 0.38409072160720825\n",
      "Epoch 4538: train loss: 0.385835200548172, val loss: 0.3838525712490082\n",
      "Epoch 4539: train loss: 0.38559216260910034, val loss: 0.3836143910884857\n",
      "Epoch 4540: train loss: 0.38534918427467346, val loss: 0.3833765387535095\n",
      "Epoch 4541: train loss: 0.3851064145565033, val loss: 0.3831387460231781\n",
      "Epoch 4542: train loss: 0.38486379384994507, val loss: 0.38290101289749146\n",
      "Epoch 4543: train loss: 0.3846212327480316, val loss: 0.38266339898109436\n",
      "Epoch 4544: train loss: 0.38437873125076294, val loss: 0.38242587447166443\n",
      "Epoch 4545: train loss: 0.3841363787651062, val loss: 0.3821885883808136\n",
      "Epoch 4546: train loss: 0.3838941752910614, val loss: 0.3819514513015747\n",
      "Epoch 4547: train loss: 0.38365209102630615, val loss: 0.38171443343162537\n",
      "Epoch 4548: train loss: 0.38341015577316284, val loss: 0.38147735595703125\n",
      "Epoch 4549: train loss: 0.3831682503223419, val loss: 0.3812403976917267\n",
      "Epoch 4550: train loss: 0.38292640447616577, val loss: 0.38100358843803406\n",
      "Epoch 4551: train loss: 0.3826848566532135, val loss: 0.3807670772075653\n",
      "Epoch 4552: train loss: 0.382443368434906, val loss: 0.38053062558174133\n",
      "Epoch 4553: train loss: 0.3822019100189209, val loss: 0.38029423356056213\n",
      "Epoch 4554: train loss: 0.38196060061454773, val loss: 0.3800579607486725\n",
      "Epoch 4555: train loss: 0.3817193806171417, val loss: 0.3798218369483948\n",
      "Epoch 4556: train loss: 0.38147836923599243, val loss: 0.37958580255508423\n",
      "Epoch 4557: train loss: 0.3812374770641327, val loss: 0.3793500065803528\n",
      "Epoch 4558: train loss: 0.38099661469459534, val loss: 0.3791141211986542\n",
      "Epoch 4559: train loss: 0.38075587153434753, val loss: 0.3788784444332123\n",
      "Epoch 4560: train loss: 0.3805152475833893, val loss: 0.37864282727241516\n",
      "Epoch 4561: train loss: 0.3802746832370758, val loss: 0.37840744853019714\n",
      "Epoch 4562: train loss: 0.38003429770469666, val loss: 0.3781720995903015\n",
      "Epoch 4563: train loss: 0.37979406118392944, val loss: 0.3779369294643402\n",
      "Epoch 4564: train loss: 0.3795539140701294, val loss: 0.3777017593383789\n",
      "Epoch 4565: train loss: 0.3793138265609741, val loss: 0.37746676802635193\n",
      "Epoch 4566: train loss: 0.3790738582611084, val loss: 0.37723207473754883\n",
      "Epoch 4567: train loss: 0.3788341283798218, val loss: 0.37699729204177856\n",
      "Epoch 4568: train loss: 0.37859442830085754, val loss: 0.3767626881599426\n",
      "Epoch 4569: train loss: 0.3783548176288605, val loss: 0.37652823328971863\n",
      "Epoch 4570: train loss: 0.37811535596847534, val loss: 0.37629374861717224\n",
      "Epoch 4571: train loss: 0.377875953912735, val loss: 0.3760595917701721\n",
      "Epoch 4572: train loss: 0.37763676047325134, val loss: 0.3758254945278168\n",
      "Epoch 4573: train loss: 0.37739771604537964, val loss: 0.375591516494751\n",
      "Epoch 4574: train loss: 0.3771587014198303, val loss: 0.3753575086593628\n",
      "Epoch 4575: train loss: 0.37691977620124817, val loss: 0.37512367963790894\n",
      "Epoch 4576: train loss: 0.3766809403896332, val loss: 0.3748900890350342\n",
      "Epoch 4577: train loss: 0.3764423131942749, val loss: 0.3746565580368042\n",
      "Epoch 4578: train loss: 0.3762038052082062, val loss: 0.3744232654571533\n",
      "Epoch 4579: train loss: 0.3759653866291046, val loss: 0.3741898238658905\n",
      "Epoch 4580: train loss: 0.37572699785232544, val loss: 0.3739566504955292\n",
      "Epoch 4581: train loss: 0.3754887878894806, val loss: 0.3737236261367798\n",
      "Epoch 4582: train loss: 0.37525075674057007, val loss: 0.37349066138267517\n",
      "Epoch 4583: train loss: 0.3750128149986267, val loss: 0.3732578456401825\n",
      "Epoch 4584: train loss: 0.3747749328613281, val loss: 0.37302517890930176\n",
      "Epoch 4585: train loss: 0.3745371699333191, val loss: 0.37279248237609863\n",
      "Epoch 4586: train loss: 0.374299556016922, val loss: 0.3725601136684418\n",
      "Epoch 4587: train loss: 0.37406206130981445, val loss: 0.37232786417007446\n",
      "Epoch 4588: train loss: 0.37382471561431885, val loss: 0.37209564447402954\n",
      "Epoch 4589: train loss: 0.373587429523468, val loss: 0.371863454580307\n",
      "Epoch 4590: train loss: 0.37335020303726196, val loss: 0.371631383895874\n",
      "Epoch 4591: train loss: 0.37311312556266785, val loss: 0.3713996112346649\n",
      "Epoch 4592: train loss: 0.37287622690200806, val loss: 0.37116795778274536\n",
      "Epoch 4593: train loss: 0.3726394772529602, val loss: 0.3709363639354706\n",
      "Epoch 4594: train loss: 0.37240275740623474, val loss: 0.37070488929748535\n",
      "Epoch 4595: train loss: 0.37216612696647644, val loss: 0.3704734146595001\n",
      "Epoch 4596: train loss: 0.3719296157360077, val loss: 0.3702421188354492\n",
      "Epoch 4597: train loss: 0.37169331312179565, val loss: 0.3700110614299774\n",
      "Epoch 4598: train loss: 0.3714570999145508, val loss: 0.36977994441986084\n",
      "Epoch 4599: train loss: 0.3712209165096283, val loss: 0.369549036026001\n",
      "Epoch 4600: train loss: 0.37098491191864014, val loss: 0.3693181872367859\n",
      "Epoch 4601: train loss: 0.37074899673461914, val loss: 0.36908742785453796\n",
      "Epoch 4602: train loss: 0.37051308155059814, val loss: 0.36885687708854675\n",
      "Epoch 4603: train loss: 0.37027740478515625, val loss: 0.3686264455318451\n",
      "Epoch 4604: train loss: 0.3700418770313263, val loss: 0.3683961033821106\n",
      "Epoch 4605: train loss: 0.3698064386844635, val loss: 0.36816588044166565\n",
      "Epoch 4606: train loss: 0.3695710599422455, val loss: 0.3679356575012207\n",
      "Epoch 4607: train loss: 0.36933574080467224, val loss: 0.3677057921886444\n",
      "Epoch 4608: train loss: 0.3691006600856781, val loss: 0.36747586727142334\n",
      "Epoch 4609: train loss: 0.3688656687736511, val loss: 0.3672461211681366\n",
      "Epoch 4610: train loss: 0.3686307668685913, val loss: 0.36701640486717224\n",
      "Epoch 4611: train loss: 0.36839595437049866, val loss: 0.3667868375778198\n",
      "Epoch 4612: train loss: 0.36816123127937317, val loss: 0.3665575683116913\n",
      "Epoch 4613: train loss: 0.3679267466068268, val loss: 0.36632832884788513\n",
      "Epoch 4614: train loss: 0.36769232153892517, val loss: 0.366099089384079\n",
      "Epoch 4615: train loss: 0.36745795607566833, val loss: 0.36587005853652954\n",
      "Epoch 4616: train loss: 0.3672237992286682, val loss: 0.3656410276889801\n",
      "Epoch 4617: train loss: 0.3669895827770233, val loss: 0.36541232466697693\n",
      "Epoch 4618: train loss: 0.3667556345462799, val loss: 0.36518365144729614\n",
      "Epoch 4619: train loss: 0.36652180552482605, val loss: 0.3649551272392273\n",
      "Epoch 4620: train loss: 0.36628809571266174, val loss: 0.36472660303115845\n",
      "Epoch 4621: train loss: 0.3660544157028198, val loss: 0.3644981384277344\n",
      "Epoch 4622: train loss: 0.36582082509994507, val loss: 0.36427003145217896\n",
      "Epoch 4623: train loss: 0.365587443113327, val loss: 0.36404192447662354\n",
      "Epoch 4624: train loss: 0.3653542101383209, val loss: 0.36381402611732483\n",
      "Epoch 4625: train loss: 0.36512094736099243, val loss: 0.36358603835105896\n",
      "Epoch 4626: train loss: 0.36488789319992065, val loss: 0.3633582592010498\n",
      "Epoch 4627: train loss: 0.36465489864349365, val loss: 0.363130658864975\n",
      "Epoch 4628: train loss: 0.36442211270332336, val loss: 0.3629032075405121\n",
      "Epoch 4629: train loss: 0.36418935656547546, val loss: 0.36267581582069397\n",
      "Epoch 4630: train loss: 0.3639567494392395, val loss: 0.36244845390319824\n",
      "Epoch 4631: train loss: 0.3637242317199707, val loss: 0.36222127079963684\n",
      "Epoch 4632: train loss: 0.36349183320999146, val loss: 0.36199429631233215\n",
      "Epoch 4633: train loss: 0.36325952410697937, val loss: 0.361767441034317\n",
      "Epoch 4634: train loss: 0.363027423620224, val loss: 0.36154061555862427\n",
      "Epoch 4635: train loss: 0.3627953827381134, val loss: 0.3613138794898987\n",
      "Epoch 4636: train loss: 0.36256346106529236, val loss: 0.36108723282814026\n",
      "Epoch 4637: train loss: 0.3623315393924713, val loss: 0.3608608841896057\n",
      "Epoch 4638: train loss: 0.36209985613822937, val loss: 0.36063453555107117\n",
      "Epoch 4639: train loss: 0.361868292093277, val loss: 0.36040836572647095\n",
      "Epoch 4640: train loss: 0.36163678765296936, val loss: 0.36018216609954834\n",
      "Epoch 4641: train loss: 0.3614054024219513, val loss: 0.35995614528656006\n",
      "Epoch 4642: train loss: 0.361174076795578, val loss: 0.35973021388053894\n",
      "Epoch 4643: train loss: 0.3609430193901062, val loss: 0.3595045506954193\n",
      "Epoch 4644: train loss: 0.3607120215892792, val loss: 0.35927900671958923\n",
      "Epoch 4645: train loss: 0.3604811131954193, val loss: 0.35905343294143677\n",
      "Epoch 4646: train loss: 0.3602502644062042, val loss: 0.35882803797721863\n",
      "Epoch 4647: train loss: 0.3600195348262787, val loss: 0.35860276222229004\n",
      "Epoch 4648: train loss: 0.3597889840602875, val loss: 0.358377605676651\n",
      "Epoch 4649: train loss: 0.3595585823059082, val loss: 0.3581526577472687\n",
      "Epoch 4650: train loss: 0.3593282103538513, val loss: 0.357927531003952\n",
      "Epoch 4651: train loss: 0.35909798741340637, val loss: 0.35770270228385925\n",
      "Epoch 4652: train loss: 0.3588677942752838, val loss: 0.3574781119823456\n",
      "Epoch 4653: train loss: 0.35863783955574036, val loss: 0.3572535216808319\n",
      "Epoch 4654: train loss: 0.3584079444408417, val loss: 0.3570290505886078\n",
      "Epoch 4655: train loss: 0.35817813873291016, val loss: 0.3568046987056732\n",
      "Epoch 4656: train loss: 0.3579484522342682, val loss: 0.3565804064273834\n",
      "Epoch 4657: train loss: 0.357718825340271, val loss: 0.3563563823699951\n",
      "Epoch 4658: train loss: 0.3574894368648529, val loss: 0.3561323881149292\n",
      "Epoch 4659: train loss: 0.357260137796402, val loss: 0.35590848326683044\n",
      "Epoch 4660: train loss: 0.3570309281349182, val loss: 0.3556847870349884\n",
      "Epoch 4661: train loss: 0.3568017780780792, val loss: 0.3554610311985016\n",
      "Epoch 4662: train loss: 0.3565727174282074, val loss: 0.35523751378059387\n",
      "Epoch 4663: train loss: 0.3563438951969147, val loss: 0.3550141453742981\n",
      "Epoch 4664: train loss: 0.3561151921749115, val loss: 0.3547908365726471\n",
      "Epoch 4665: train loss: 0.3558865189552307, val loss: 0.35456764698028564\n",
      "Epoch 4666: train loss: 0.3556579649448395, val loss: 0.35434451699256897\n",
      "Epoch 4667: train loss: 0.355429470539093, val loss: 0.3541216552257538\n",
      "Epoch 4668: train loss: 0.35520118474960327, val loss: 0.3538988530635834\n",
      "Epoch 4669: train loss: 0.3549729883670807, val loss: 0.3536761403083801\n",
      "Epoch 4670: train loss: 0.35474494099617004, val loss: 0.35345351696014404\n",
      "Epoch 4671: train loss: 0.3545168936252594, val loss: 0.35323095321655273\n",
      "Epoch 4672: train loss: 0.3542889654636383, val loss: 0.3530086576938629\n",
      "Epoch 4673: train loss: 0.3540612757205963, val loss: 0.3527865409851074\n",
      "Epoch 4674: train loss: 0.3538336455821991, val loss: 0.3525643050670624\n",
      "Epoch 4675: train loss: 0.35360613465309143, val loss: 0.35234227776527405\n",
      "Epoch 4676: train loss: 0.35337865352630615, val loss: 0.3521203398704529\n",
      "Epoch 4677: train loss: 0.3531512916088104, val loss: 0.35189855098724365\n",
      "Epoch 4678: train loss: 0.3529241979122162, val loss: 0.35167694091796875\n",
      "Epoch 4679: train loss: 0.35269710421562195, val loss: 0.351455420255661\n",
      "Epoch 4680: train loss: 0.35247015953063965, val loss: 0.35123395919799805\n",
      "Epoch 4681: train loss: 0.3522432744503021, val loss: 0.35101255774497986\n",
      "Epoch 4682: train loss: 0.3520164489746094, val loss: 0.35079142451286316\n",
      "Epoch 4683: train loss: 0.3517899215221405, val loss: 0.35057032108306885\n",
      "Epoch 4684: train loss: 0.35156336426734924, val loss: 0.3503493368625641\n",
      "Epoch 4685: train loss: 0.3513369560241699, val loss: 0.3501284122467041\n",
      "Epoch 4686: train loss: 0.3511106073856354, val loss: 0.3499077260494232\n",
      "Epoch 4687: train loss: 0.35088440775871277, val loss: 0.3496870696544647\n",
      "Epoch 4688: train loss: 0.3506583571434021, val loss: 0.3494666516780853\n",
      "Epoch 4689: train loss: 0.350432425737381, val loss: 0.34924623370170593\n",
      "Epoch 4690: train loss: 0.350206583738327, val loss: 0.34902602434158325\n",
      "Epoch 4691: train loss: 0.3499808609485626, val loss: 0.3488057553768158\n",
      "Epoch 4692: train loss: 0.3497551381587982, val loss: 0.34858569502830505\n",
      "Epoch 4693: train loss: 0.3495296835899353, val loss: 0.3483658730983734\n",
      "Epoch 4694: train loss: 0.34930431842803955, val loss: 0.3481460213661194\n",
      "Epoch 4695: train loss: 0.3490790128707886, val loss: 0.34792622923851013\n",
      "Epoch 4696: train loss: 0.34885379672050476, val loss: 0.3477066457271576\n",
      "Epoch 4697: train loss: 0.3486286699771881, val loss: 0.347487211227417\n",
      "Epoch 4698: train loss: 0.34840378165245056, val loss: 0.3472679555416107\n",
      "Epoch 4699: train loss: 0.3481789529323578, val loss: 0.3470486104488373\n",
      "Epoch 4700: train loss: 0.34795427322387695, val loss: 0.34682953357696533\n",
      "Epoch 4701: train loss: 0.3477295935153961, val loss: 0.34661048650741577\n",
      "Epoch 4702: train loss: 0.34750500321388245, val loss: 0.3463915288448334\n",
      "Epoch 4703: train loss: 0.34728068113327026, val loss: 0.34617286920547485\n",
      "Epoch 4704: train loss: 0.34705644845962524, val loss: 0.34595420956611633\n",
      "Epoch 4705: train loss: 0.3468322455883026, val loss: 0.345735639333725\n",
      "Epoch 4706: train loss: 0.34660816192626953, val loss: 0.34551721811294556\n",
      "Epoch 4707: train loss: 0.3463841378688812, val loss: 0.3452989161014557\n",
      "Epoch 4708: train loss: 0.3461603820323944, val loss: 0.34508076310157776\n",
      "Epoch 4709: train loss: 0.34593668580055237, val loss: 0.3448626697063446\n",
      "Epoch 4710: train loss: 0.3457130193710327, val loss: 0.3446446359157562\n",
      "Epoch 4711: train loss: 0.345489501953125, val loss: 0.34442681074142456\n",
      "Epoch 4712: train loss: 0.34526607394218445, val loss: 0.34420910477638245\n",
      "Epoch 4713: train loss: 0.3450428247451782, val loss: 0.34399160742759705\n",
      "Epoch 4714: train loss: 0.34481969475746155, val loss: 0.3437739312648773\n",
      "Epoch 4715: train loss: 0.34459662437438965, val loss: 0.34355655312538147\n",
      "Epoch 4716: train loss: 0.3443736135959625, val loss: 0.3433391749858856\n",
      "Epoch 4717: train loss: 0.34415075182914734, val loss: 0.34312209486961365\n",
      "Epoch 4718: train loss: 0.3439280390739441, val loss: 0.34290504455566406\n",
      "Epoch 4719: train loss: 0.3437054753303528, val loss: 0.3426881432533264\n",
      "Epoch 4720: train loss: 0.34348297119140625, val loss: 0.34247127175331116\n",
      "Epoch 4721: train loss: 0.3432605266571045, val loss: 0.34225454926490784\n",
      "Epoch 4722: train loss: 0.34303826093673706, val loss: 0.34203800559043884\n",
      "Epoch 4723: train loss: 0.3428161144256592, val loss: 0.3418215215206146\n",
      "Epoch 4724: train loss: 0.34259405732154846, val loss: 0.3416052460670471\n",
      "Epoch 4725: train loss: 0.3423720896244049, val loss: 0.3413889706134796\n",
      "Epoch 4726: train loss: 0.3421502113342285, val loss: 0.3411727547645569\n",
      "Epoch 4727: train loss: 0.3419284522533417, val loss: 0.3409568965435028\n",
      "Epoch 4728: train loss: 0.34170690178871155, val loss: 0.34074097871780396\n",
      "Epoch 4729: train loss: 0.3414853811264038, val loss: 0.34052515029907227\n",
      "Epoch 4730: train loss: 0.3412640392780304, val loss: 0.3403094410896301\n",
      "Epoch 4731: train loss: 0.34104272723197937, val loss: 0.34009385108947754\n",
      "Epoch 4732: train loss: 0.3408214747905731, val loss: 0.3398784101009369\n",
      "Epoch 4733: train loss: 0.34060046076774597, val loss: 0.3396630883216858\n",
      "Epoch 4734: train loss: 0.340379536151886, val loss: 0.339447945356369\n",
      "Epoch 4735: train loss: 0.3401586711406708, val loss: 0.33923277258872986\n",
      "Epoch 4736: train loss: 0.3399379253387451, val loss: 0.339017778635025\n",
      "Epoch 4737: train loss: 0.33971723914146423, val loss: 0.33880293369293213\n",
      "Epoch 4738: train loss: 0.33949679136276245, val loss: 0.3385882079601288\n",
      "Epoch 4739: train loss: 0.33927637338638306, val loss: 0.3383735716342926\n",
      "Epoch 4740: train loss: 0.3390561044216156, val loss: 0.3381589949131012\n",
      "Epoch 4741: train loss: 0.3388359248638153, val loss: 0.3379446566104889\n",
      "Epoch 4742: train loss: 0.33861589431762695, val loss: 0.33773040771484375\n",
      "Epoch 4743: train loss: 0.33839601278305054, val loss: 0.3375162184238434\n",
      "Epoch 4744: train loss: 0.3381761610507965, val loss: 0.33730217814445496\n",
      "Epoch 4745: train loss: 0.33795642852783203, val loss: 0.3370882570743561\n",
      "Epoch 4746: train loss: 0.3377367854118347, val loss: 0.33687445521354675\n",
      "Epoch 4747: train loss: 0.33751732110977173, val loss: 0.3366607129573822\n",
      "Epoch 4748: train loss: 0.3372979760169983, val loss: 0.336447149515152\n",
      "Epoch 4749: train loss: 0.33707869052886963, val loss: 0.3362336754798889\n",
      "Epoch 4750: train loss: 0.3368595242500305, val loss: 0.33602023124694824\n",
      "Epoch 4751: train loss: 0.3366404175758362, val loss: 0.33580705523490906\n",
      "Epoch 4752: train loss: 0.33642151951789856, val loss: 0.33559390902519226\n",
      "Epoch 4753: train loss: 0.3362026810646057, val loss: 0.3353809118270874\n",
      "Epoch 4754: train loss: 0.3359839618206024, val loss: 0.3351680338382721\n",
      "Epoch 4755: train loss: 0.3357653021812439, val loss: 0.3349551856517792\n",
      "Epoch 4756: train loss: 0.3355467915534973, val loss: 0.3347424864768982\n",
      "Epoch 4757: train loss: 0.3353284001350403, val loss: 0.33452993631362915\n",
      "Epoch 4758: train loss: 0.33511021733283997, val loss: 0.33431753516197205\n",
      "Epoch 4759: train loss: 0.33489203453063965, val loss: 0.3341052234172821\n",
      "Epoch 4760: train loss: 0.3346739113330841, val loss: 0.333892822265625\n",
      "Epoch 4761: train loss: 0.3344559073448181, val loss: 0.33368074893951416\n",
      "Epoch 4762: train loss: 0.33423811197280884, val loss: 0.33346888422966003\n",
      "Epoch 4763: train loss: 0.3340204060077667, val loss: 0.33325687050819397\n",
      "Epoch 4764: train loss: 0.3338027894496918, val loss: 0.3330451548099518\n",
      "Epoch 4765: train loss: 0.3335852324962616, val loss: 0.33283349871635437\n",
      "Epoch 4766: train loss: 0.33336779475212097, val loss: 0.33262190222740173\n",
      "Epoch 4767: train loss: 0.33315056562423706, val loss: 0.3324105143547058\n",
      "Epoch 4768: train loss: 0.3329333961009979, val loss: 0.33219924569129944\n",
      "Epoch 4769: train loss: 0.33271628618240356, val loss: 0.3319879472255707\n",
      "Epoch 4770: train loss: 0.33249932527542114, val loss: 0.3317767381668091\n",
      "Epoch 4771: train loss: 0.3322824239730835, val loss: 0.33156588673591614\n",
      "Epoch 4772: train loss: 0.3320657014846802, val loss: 0.33135509490966797\n",
      "Epoch 4773: train loss: 0.3318490982055664, val loss: 0.33114439249038696\n",
      "Epoch 4774: train loss: 0.331632524728775, val loss: 0.33093366026878357\n",
      "Epoch 4775: train loss: 0.33141613006591797, val loss: 0.33072301745414734\n",
      "Epoch 4776: train loss: 0.3311997652053833, val loss: 0.330512672662735\n",
      "Epoch 4777: train loss: 0.33098360896110535, val loss: 0.3303024470806122\n",
      "Epoch 4778: train loss: 0.3307674825191498, val loss: 0.330092191696167\n",
      "Epoch 4779: train loss: 0.33055153489112854, val loss: 0.3298821449279785\n",
      "Epoch 4780: train loss: 0.33033567667007446, val loss: 0.329672247171402\n",
      "Epoch 4781: train loss: 0.3301199674606323, val loss: 0.3294624984264374\n",
      "Epoch 4782: train loss: 0.32990434765815735, val loss: 0.3292527496814728\n",
      "Epoch 4783: train loss: 0.32968881726264954, val loss: 0.3290430009365082\n",
      "Epoch 4784: train loss: 0.3294733762741089, val loss: 0.32883358001708984\n",
      "Epoch 4785: train loss: 0.3292580246925354, val loss: 0.3286242187023163\n",
      "Epoch 4786: train loss: 0.32904288172721863, val loss: 0.3284149765968323\n",
      "Epoch 4787: train loss: 0.3288278579711914, val loss: 0.32820582389831543\n",
      "Epoch 4788: train loss: 0.3286128640174866, val loss: 0.3279968500137329\n",
      "Epoch 4789: train loss: 0.3283979594707489, val loss: 0.327787846326828\n",
      "Epoch 4790: train loss: 0.3281831741333008, val loss: 0.3275790214538574\n",
      "Epoch 4791: train loss: 0.327968567609787, val loss: 0.3273703157901764\n",
      "Epoch 4792: train loss: 0.32775405049324036, val loss: 0.3271617889404297\n",
      "Epoch 4793: train loss: 0.3275396227836609, val loss: 0.326953262090683\n",
      "Epoch 4794: train loss: 0.3273252844810486, val loss: 0.32674482464790344\n",
      "Epoch 4795: train loss: 0.32711100578308105, val loss: 0.32653674483299255\n",
      "Epoch 4796: train loss: 0.3268969655036926, val loss: 0.3263285458087921\n",
      "Epoch 4797: train loss: 0.3266829550266266, val loss: 0.3261205554008484\n",
      "Epoch 4798: train loss: 0.3264690935611725, val loss: 0.32591262459754944\n",
      "Epoch 4799: train loss: 0.32625529170036316, val loss: 0.32570475339889526\n",
      "Epoch 4800: train loss: 0.326041579246521, val loss: 0.3254971206188202\n",
      "Epoch 4801: train loss: 0.32582804560661316, val loss: 0.32528960704803467\n",
      "Epoch 4802: train loss: 0.3256146311759949, val loss: 0.3250821530818939\n",
      "Epoch 4803: train loss: 0.32540130615234375, val loss: 0.32487472891807556\n",
      "Epoch 4804: train loss: 0.325188010931015, val loss: 0.3246676027774811\n",
      "Epoch 4805: train loss: 0.3249749541282654, val loss: 0.324460506439209\n",
      "Epoch 4806: train loss: 0.3247619867324829, val loss: 0.3242534399032593\n",
      "Epoch 4807: train loss: 0.3245490789413452, val loss: 0.32404667139053345\n",
      "Epoch 4808: train loss: 0.32433632016181946, val loss: 0.32383981347084045\n",
      "Epoch 4809: train loss: 0.3241235613822937, val loss: 0.3236331641674042\n",
      "Epoch 4810: train loss: 0.32391107082366943, val loss: 0.3234266936779022\n",
      "Epoch 4811: train loss: 0.32369863986968994, val loss: 0.3232203423976898\n",
      "Epoch 4812: train loss: 0.323486328125, val loss: 0.32301393151283264\n",
      "Epoch 4813: train loss: 0.32327401638031006, val loss: 0.3228077292442322\n",
      "Epoch 4814: train loss: 0.32306185364723206, val loss: 0.3226015567779541\n",
      "Epoch 4815: train loss: 0.32284992933273315, val loss: 0.3223956227302551\n",
      "Epoch 4816: train loss: 0.32263797521591187, val loss: 0.3221898078918457\n",
      "Epoch 4817: train loss: 0.3224261999130249, val loss: 0.3219839930534363\n",
      "Epoch 4818: train loss: 0.3222144842147827, val loss: 0.321778267621994\n",
      "Epoch 4819: train loss: 0.3220028281211853, val loss: 0.32157284021377563\n",
      "Epoch 4820: train loss: 0.321791410446167, val loss: 0.32136741280555725\n",
      "Epoch 4821: train loss: 0.32158005237579346, val loss: 0.32116207480430603\n",
      "Epoch 4822: train loss: 0.3213687837123871, val loss: 0.32095685601234436\n",
      "Epoch 4823: train loss: 0.3211576044559479, val loss: 0.32075169682502747\n",
      "Epoch 4824: train loss: 0.32094651460647583, val loss: 0.3205467760562897\n",
      "Epoch 4825: train loss: 0.3207356333732605, val loss: 0.32034197449684143\n",
      "Epoch 4826: train loss: 0.32052481174468994, val loss: 0.32013723254203796\n",
      "Epoch 4827: train loss: 0.32031410932540894, val loss: 0.31993260979652405\n",
      "Epoch 4828: train loss: 0.3201034665107727, val loss: 0.31972813606262207\n",
      "Epoch 4829: train loss: 0.3198930323123932, val loss: 0.31952378153800964\n",
      "Epoch 4830: train loss: 0.31968268752098083, val loss: 0.3193194270133972\n",
      "Epoch 4831: train loss: 0.31947243213653564, val loss: 0.31911522150039673\n",
      "Epoch 4832: train loss: 0.31926223635673523, val loss: 0.3189111351966858\n",
      "Epoch 4833: train loss: 0.3190521001815796, val loss: 0.3187071979045868\n",
      "Epoch 4834: train loss: 0.31884223222732544, val loss: 0.3185034692287445\n",
      "Epoch 4835: train loss: 0.31863242387771606, val loss: 0.3182997405529022\n",
      "Epoch 4836: train loss: 0.31842267513275146, val loss: 0.31809601187705994\n",
      "Epoch 4837: train loss: 0.3182130455970764, val loss: 0.3178924024105072\n",
      "Epoch 4838: train loss: 0.31800344586372375, val loss: 0.3176891505718231\n",
      "Epoch 4839: train loss: 0.3177941143512726, val loss: 0.31748586893081665\n",
      "Epoch 4840: train loss: 0.3175848424434662, val loss: 0.31728267669677734\n",
      "Epoch 4841: train loss: 0.31737563014030457, val loss: 0.3170795738697052\n",
      "Epoch 4842: train loss: 0.3171665370464325, val loss: 0.3168765604496002\n",
      "Epoch 4843: train loss: 0.3169575035572052, val loss: 0.3166738450527191\n",
      "Epoch 4844: train loss: 0.3167486786842346, val loss: 0.3164711594581604\n",
      "Epoch 4845: train loss: 0.3165399432182312, val loss: 0.3162685036659241\n",
      "Epoch 4846: train loss: 0.31633126735687256, val loss: 0.31606602668762207\n",
      "Epoch 4847: train loss: 0.31612271070480347, val loss: 0.3158636689186096\n",
      "Epoch 4848: train loss: 0.3159143328666687, val loss: 0.31566140055656433\n",
      "Epoch 4849: train loss: 0.3157060444355011, val loss: 0.315459281206131\n",
      "Epoch 4850: train loss: 0.31549784541130066, val loss: 0.3152572810649872\n",
      "Epoch 4851: train loss: 0.3152897357940674, val loss: 0.3150552809238434\n",
      "Epoch 4852: train loss: 0.3150816857814789, val loss: 0.31485339999198914\n",
      "Epoch 4853: train loss: 0.3148738443851471, val loss: 0.3146516978740692\n",
      "Epoch 4854: train loss: 0.31466612219810486, val loss: 0.3144501745700836\n",
      "Epoch 4855: train loss: 0.314458429813385, val loss: 0.314248651266098\n",
      "Epoch 4856: train loss: 0.3142508268356323, val loss: 0.3140471875667572\n",
      "Epoch 4857: train loss: 0.3140433430671692, val loss: 0.3138459622859955\n",
      "Epoch 4858: train loss: 0.3138360381126404, val loss: 0.3136449456214905\n",
      "Epoch 4859: train loss: 0.31362882256507874, val loss: 0.3134438395500183\n",
      "Epoch 4860: train loss: 0.31342166662216187, val loss: 0.31324276328086853\n",
      "Epoch 4861: train loss: 0.31321465969085693, val loss: 0.3130420744419098\n",
      "Epoch 4862: train loss: 0.3130076825618744, val loss: 0.31284135580062866\n",
      "Epoch 4863: train loss: 0.31280091404914856, val loss: 0.31264081597328186\n",
      "Epoch 4864: train loss: 0.3125942647457123, val loss: 0.31244030594825745\n",
      "Epoch 4865: train loss: 0.3123876452445984, val loss: 0.31223979592323303\n",
      "Epoch 4866: train loss: 0.3121810853481293, val loss: 0.3120396137237549\n",
      "Epoch 4867: train loss: 0.31197476387023926, val loss: 0.3118395507335663\n",
      "Epoch 4868: train loss: 0.3117685914039612, val loss: 0.3116395175457001\n",
      "Epoch 4869: train loss: 0.3115624487400055, val loss: 0.3114396035671234\n",
      "Epoch 4870: train loss: 0.3113563060760498, val loss: 0.31123968958854675\n",
      "Epoch 4871: train loss: 0.31115034222602844, val loss: 0.3110400140285492\n",
      "Epoch 4872: train loss: 0.3109445869922638, val loss: 0.3108404576778412\n",
      "Epoch 4873: train loss: 0.3107389211654663, val loss: 0.3106410503387451\n",
      "Epoch 4874: train loss: 0.3105332851409912, val loss: 0.3104415833950043\n",
      "Epoch 4875: train loss: 0.3103277087211609, val loss: 0.31024232506752014\n",
      "Epoch 4876: train loss: 0.3101223111152649, val loss: 0.31004321575164795\n",
      "Epoch 4877: train loss: 0.30991703271865845, val loss: 0.30984416604042053\n",
      "Epoch 4878: train loss: 0.30971187353134155, val loss: 0.30964523553848267\n",
      "Epoch 4879: train loss: 0.30950677394866943, val loss: 0.30944645404815674\n",
      "Epoch 4880: train loss: 0.3093017637729645, val loss: 0.309247761964798\n",
      "Epoch 4881: train loss: 0.30909696221351624, val loss: 0.30904921889305115\n",
      "Epoch 4882: train loss: 0.30889225006103516, val loss: 0.3088507354259491\n",
      "Epoch 4883: train loss: 0.30868762731552124, val loss: 0.3086523413658142\n",
      "Epoch 4884: train loss: 0.3084830343723297, val loss: 0.3084540069103241\n",
      "Epoch 4885: train loss: 0.3082785904407501, val loss: 0.30825600028038025\n",
      "Epoch 4886: train loss: 0.30807432532310486, val loss: 0.30805790424346924\n",
      "Epoch 4887: train loss: 0.30787011981010437, val loss: 0.30786004662513733\n",
      "Epoch 4888: train loss: 0.30766603350639343, val loss: 0.30766209959983826\n",
      "Epoch 4889: train loss: 0.30746200680732727, val loss: 0.3074644207954407\n",
      "Epoch 4890: train loss: 0.3072580099105835, val loss: 0.30726686120033264\n",
      "Epoch 4891: train loss: 0.3070543110370636, val loss: 0.30706948041915894\n",
      "Epoch 4892: train loss: 0.3068506419658661, val loss: 0.30687206983566284\n",
      "Epoch 4893: train loss: 0.30664706230163574, val loss: 0.3066748082637787\n",
      "Epoch 4894: train loss: 0.30644357204437256, val loss: 0.30647772550582886\n",
      "Epoch 4895: train loss: 0.3062402904033661, val loss: 0.3062806725502014\n",
      "Epoch 4896: train loss: 0.3060370981693268, val loss: 0.3060837686061859\n",
      "Epoch 4897: train loss: 0.30583399534225464, val loss: 0.30588698387145996\n",
      "Epoch 4898: train loss: 0.3056308925151825, val loss: 0.3056902289390564\n",
      "Epoch 4899: train loss: 0.3054279386997223, val loss: 0.30549362301826477\n",
      "Epoch 4900: train loss: 0.3052251636981964, val loss: 0.30529722571372986\n",
      "Epoch 4901: train loss: 0.3050224781036377, val loss: 0.3051008880138397\n",
      "Epoch 4902: train loss: 0.30481988191604614, val loss: 0.30490460991859436\n",
      "Epoch 4903: train loss: 0.30461734533309937, val loss: 0.3047083616256714\n",
      "Epoch 4904: train loss: 0.30441492795944214, val loss: 0.3045124113559723\n",
      "Epoch 4905: train loss: 0.3042127192020416, val loss: 0.3043164908885956\n",
      "Epoch 4906: train loss: 0.3040105402469635, val loss: 0.3041206896305084\n",
      "Epoch 4907: train loss: 0.30380842089653015, val loss: 0.3039248287677765\n",
      "Epoch 4908: train loss: 0.30360645055770874, val loss: 0.3037293553352356\n",
      "Epoch 4909: train loss: 0.30340468883514404, val loss: 0.3035339415073395\n",
      "Epoch 4910: train loss: 0.30320295691490173, val loss: 0.30333855748176575\n",
      "Epoch 4911: train loss: 0.3030013144016266, val loss: 0.3031432330608368\n",
      "Epoch 4912: train loss: 0.302799791097641, val loss: 0.3029482066631317\n",
      "Epoch 4913: train loss: 0.30259832739830017, val loss: 0.3027530610561371\n",
      "Epoch 4914: train loss: 0.3023970425128937, val loss: 0.30255815386772156\n",
      "Epoch 4915: train loss: 0.30219584703445435, val loss: 0.3023633658885956\n",
      "Epoch 4916: train loss: 0.3019947409629822, val loss: 0.30216866731643677\n",
      "Epoch 4917: train loss: 0.3017937242984772, val loss: 0.30197399854660034\n",
      "Epoch 4918: train loss: 0.30159279704093933, val loss: 0.301779568195343\n",
      "Epoch 4919: train loss: 0.3013920783996582, val loss: 0.30158519744873047\n",
      "Epoch 4920: train loss: 0.30119138956069946, val loss: 0.3013908565044403\n",
      "Epoch 4921: train loss: 0.3009908199310303, val loss: 0.30119672417640686\n",
      "Epoch 4922: train loss: 0.30079033970832825, val loss: 0.30100271105766296\n",
      "Epoch 4923: train loss: 0.30059000849723816, val loss: 0.30080875754356384\n",
      "Epoch 4924: train loss: 0.30038982629776, val loss: 0.30061501264572144\n",
      "Epoch 4925: train loss: 0.30018967390060425, val loss: 0.30042120814323425\n",
      "Epoch 4926: train loss: 0.29998964071273804, val loss: 0.300227552652359\n",
      "Epoch 4927: train loss: 0.2997896373271942, val loss: 0.30003413558006287\n",
      "Epoch 4928: train loss: 0.2995898723602295, val loss: 0.2998407781124115\n",
      "Epoch 4929: train loss: 0.29939016699790955, val loss: 0.2996475398540497\n",
      "Epoch 4930: train loss: 0.29919055104255676, val loss: 0.29945430159568787\n",
      "Epoch 4931: train loss: 0.29899102449417114, val loss: 0.2992611825466156\n",
      "Epoch 4932: train loss: 0.2987915873527527, val loss: 0.2990683615207672\n",
      "Epoch 4933: train loss: 0.29859232902526855, val loss: 0.29887545108795166\n",
      "Epoch 4934: train loss: 0.2983931601047516, val loss: 0.2986827790737152\n",
      "Epoch 4935: train loss: 0.2981940805912018, val loss: 0.29849010705947876\n",
      "Epoch 4936: train loss: 0.29799506068229675, val loss: 0.298297643661499\n",
      "Epoch 4937: train loss: 0.2977962791919708, val loss: 0.29810526967048645\n",
      "Epoch 4938: train loss: 0.2975974977016449, val loss: 0.29791295528411865\n",
      "Epoch 4939: train loss: 0.2973988950252533, val loss: 0.2977207601070404\n",
      "Epoch 4940: train loss: 0.29720035195350647, val loss: 0.2975286543369293\n",
      "Epoch 4941: train loss: 0.29700183868408203, val loss: 0.2973366677761078\n",
      "Epoch 4942: train loss: 0.2968035340309143, val loss: 0.2971448600292206\n",
      "Epoch 4943: train loss: 0.29660534858703613, val loss: 0.2969531714916229\n",
      "Epoch 4944: train loss: 0.2964072525501251, val loss: 0.29676151275634766\n",
      "Epoch 4945: train loss: 0.2962091863155365, val loss: 0.2965700328350067\n",
      "Epoch 4946: train loss: 0.296011358499527, val loss: 0.2963786721229553\n",
      "Epoch 4947: train loss: 0.29581356048583984, val loss: 0.2961873710155487\n",
      "Epoch 4948: train loss: 0.29561588168144226, val loss: 0.29599615931510925\n",
      "Epoch 4949: train loss: 0.29541829228401184, val loss: 0.29580503702163696\n",
      "Epoch 4950: train loss: 0.2952207326889038, val loss: 0.295614093542099\n",
      "Epoch 4951: train loss: 0.29502344131469727, val loss: 0.2954232692718506\n",
      "Epoch 4952: train loss: 0.2948261797428131, val loss: 0.29523253440856934\n",
      "Epoch 4953: train loss: 0.2946290075778961, val loss: 0.29504185914993286\n",
      "Epoch 4954: train loss: 0.2944319248199463, val loss: 0.29485127329826355\n",
      "Epoch 4955: train loss: 0.2942349314689636, val loss: 0.29466086626052856\n",
      "Epoch 4956: train loss: 0.2940381169319153, val loss: 0.29447054862976074\n",
      "Epoch 4957: train loss: 0.2938413918018341, val loss: 0.2942802906036377\n",
      "Epoch 4958: train loss: 0.2936447560787201, val loss: 0.29409024119377136\n",
      "Epoch 4959: train loss: 0.29344820976257324, val loss: 0.29390034079551697\n",
      "Epoch 4960: train loss: 0.29325181245803833, val loss: 0.2937104403972626\n",
      "Epoch 4961: train loss: 0.29305553436279297, val loss: 0.2935206890106201\n",
      "Epoch 4962: train loss: 0.2928593158721924, val loss: 0.29333099722862244\n",
      "Epoch 4963: train loss: 0.29266318678855896, val loss: 0.29314133524894714\n",
      "Epoch 4964: train loss: 0.2924671769142151, val loss: 0.2929520308971405\n",
      "Epoch 4965: train loss: 0.29227131605148315, val loss: 0.2927626967430115\n",
      "Epoch 4966: train loss: 0.292075514793396, val loss: 0.2925734221935272\n",
      "Epoch 4967: train loss: 0.2918798625469208, val loss: 0.2923843562602997\n",
      "Epoch 4968: train loss: 0.29168424010276794, val loss: 0.2921953499317169\n",
      "Epoch 4969: train loss: 0.2914888262748718, val loss: 0.2920064628124237\n",
      "Epoch 4970: train loss: 0.29129350185394287, val loss: 0.29181769490242004\n",
      "Epoch 4971: train loss: 0.2910982370376587, val loss: 0.29162904620170593\n",
      "Epoch 4972: train loss: 0.29090309143066406, val loss: 0.2914404571056366\n",
      "Epoch 4973: train loss: 0.2907080054283142, val loss: 0.291252076625824\n",
      "Epoch 4974: train loss: 0.2905130982398987, val loss: 0.2910636365413666\n",
      "Epoch 4975: train loss: 0.2903183102607727, val loss: 0.2908753454685211\n",
      "Epoch 4976: train loss: 0.2901235520839691, val loss: 0.2906871736049652\n",
      "Epoch 4977: train loss: 0.2899288833141327, val loss: 0.290499210357666\n",
      "Epoch 4978: train loss: 0.28973445296287537, val loss: 0.29031139612197876\n",
      "Epoch 4979: train loss: 0.28954005241394043, val loss: 0.2901235818862915\n",
      "Epoch 4980: train loss: 0.28934577107429504, val loss: 0.2899358868598938\n",
      "Epoch 4981: train loss: 0.28915154933929443, val loss: 0.28974825143814087\n",
      "Epoch 4982: train loss: 0.2889574468135834, val loss: 0.28956079483032227\n",
      "Epoch 4983: train loss: 0.28876346349716187, val loss: 0.2893734276294708\n",
      "Epoch 4984: train loss: 0.2885695993900299, val loss: 0.28918614983558655\n",
      "Epoch 4985: train loss: 0.2883758246898651, val loss: 0.28899893164634705\n",
      "Epoch 4986: train loss: 0.2881821095943451, val loss: 0.2888120710849762\n",
      "Epoch 4987: train loss: 0.2879886329174042, val loss: 0.2886250913143158\n",
      "Epoch 4988: train loss: 0.28779518604278564, val loss: 0.2884382903575897\n",
      "Epoch 4989: train loss: 0.2876018285751343, val loss: 0.2882515490055084\n",
      "Epoch 4990: train loss: 0.2874085605144501, val loss: 0.2880649268627167\n",
      "Epoch 4991: train loss: 0.28721538186073303, val loss: 0.28787851333618164\n",
      "Epoch 4992: train loss: 0.2870223820209503, val loss: 0.28769204020500183\n",
      "Epoch 4993: train loss: 0.28682947158813477, val loss: 0.2875058352947235\n",
      "Epoch 4994: train loss: 0.286636620759964, val loss: 0.2873196005821228\n",
      "Epoch 4995: train loss: 0.2864438593387604, val loss: 0.28713351488113403\n",
      "Epoch 4996: train loss: 0.28625133633613586, val loss: 0.28694769740104675\n",
      "Epoch 4997: train loss: 0.28605884313583374, val loss: 0.2867618203163147\n",
      "Epoch 4998: train loss: 0.28586646914482117, val loss: 0.2865760326385498\n",
      "Epoch 4999: train loss: 0.2856740951538086, val loss: 0.28639039397239685\n",
      "Epoch 5000: train loss: 0.28548189997673035, val loss: 0.286204993724823\n",
      "Epoch 5001: train loss: 0.28528982400894165, val loss: 0.28601956367492676\n",
      "Epoch 5002: train loss: 0.2850978672504425, val loss: 0.28583428263664246\n",
      "Epoch 5003: train loss: 0.2849059998989105, val loss: 0.28564903140068054\n",
      "Epoch 5004: train loss: 0.2847141623497009, val loss: 0.2854638695716858\n",
      "Epoch 5005: train loss: 0.2845224440097809, val loss: 0.28527888655662537\n",
      "Epoch 5006: train loss: 0.28433093428611755, val loss: 0.28509408235549927\n",
      "Epoch 5007: train loss: 0.2841394245624542, val loss: 0.2849092185497284\n",
      "Epoch 5008: train loss: 0.2839480936527252, val loss: 0.28472456336021423\n",
      "Epoch 5009: train loss: 0.2837567925453186, val loss: 0.2845400273799896\n",
      "Epoch 5010: train loss: 0.2835657000541687, val loss: 0.28435564041137695\n",
      "Epoch 5011: train loss: 0.2833746671676636, val loss: 0.28417128324508667\n",
      "Epoch 5012: train loss: 0.2831837236881256, val loss: 0.28398704528808594\n",
      "Epoch 5013: train loss: 0.2829928398132324, val loss: 0.2838028073310852\n",
      "Epoch 5014: train loss: 0.2828020453453064, val loss: 0.28361886739730835\n",
      "Epoch 5015: train loss: 0.2826114594936371, val loss: 0.2834349572658539\n",
      "Epoch 5016: train loss: 0.28242096304893494, val loss: 0.2832511067390442\n",
      "Epoch 5017: train loss: 0.2822304964065552, val loss: 0.2830674350261688\n",
      "Epoch 5018: train loss: 0.28204014897346497, val loss: 0.2828839421272278\n",
      "Epoch 5019: train loss: 0.28185001015663147, val loss: 0.2827005088329315\n",
      "Epoch 5020: train loss: 0.28165993094444275, val loss: 0.28251713514328003\n",
      "Epoch 5021: train loss: 0.2814699411392212, val loss: 0.2823337912559509\n",
      "Epoch 5022: train loss: 0.2812800109386444, val loss: 0.28215065598487854\n",
      "Epoch 5023: train loss: 0.2810901701450348, val loss: 0.2819676995277405\n",
      "Epoch 5024: train loss: 0.2809005379676819, val loss: 0.2817847430706024\n",
      "Epoch 5025: train loss: 0.28071093559265137, val loss: 0.28160184621810913\n",
      "Epoch 5026: train loss: 0.2805214524269104, val loss: 0.2814190983772278\n",
      "Epoch 5027: train loss: 0.2803320288658142, val loss: 0.2812364995479584\n",
      "Epoch 5028: train loss: 0.28014281392097473, val loss: 0.28105399012565613\n",
      "Epoch 5029: train loss: 0.2799536883831024, val loss: 0.2808716297149658\n",
      "Epoch 5030: train loss: 0.2797645926475525, val loss: 0.2806893289089203\n",
      "Epoch 5031: train loss: 0.2795756161212921, val loss: 0.2805071771144867\n",
      "Epoch 5032: train loss: 0.27938681840896606, val loss: 0.28032517433166504\n",
      "Epoch 5033: train loss: 0.2791981101036072, val loss: 0.28014329075813293\n",
      "Epoch 5034: train loss: 0.27900946140289307, val loss: 0.27996140718460083\n",
      "Epoch 5035: train loss: 0.2788209319114685, val loss: 0.2797795236110687\n",
      "Epoch 5036: train loss: 0.2786324620246887, val loss: 0.2795979082584381\n",
      "Epoch 5037: train loss: 0.27844417095184326, val loss: 0.2794165015220642\n",
      "Epoch 5038: train loss: 0.27825596928596497, val loss: 0.27923503518104553\n",
      "Epoch 5039: train loss: 0.27806785702705383, val loss: 0.2790536880493164\n",
      "Epoch 5040: train loss: 0.2778798043727875, val loss: 0.2788724899291992\n",
      "Epoch 5041: train loss: 0.27769193053245544, val loss: 0.2786914110183716\n",
      "Epoch 5042: train loss: 0.27750417590141296, val loss: 0.2785104215145111\n",
      "Epoch 5043: train loss: 0.27731645107269287, val loss: 0.278329461812973\n",
      "Epoch 5044: train loss: 0.27712884545326233, val loss: 0.2781486511230469\n",
      "Epoch 5045: train loss: 0.27694129943847656, val loss: 0.27796807885169983\n",
      "Epoch 5046: train loss: 0.2767539620399475, val loss: 0.27778753638267517\n",
      "Epoch 5047: train loss: 0.2765667140483856, val loss: 0.2776070535182953\n",
      "Epoch 5048: train loss: 0.2763794958591461, val loss: 0.27742668986320496\n",
      "Epoch 5049: train loss: 0.27619239687919617, val loss: 0.27724647521972656\n",
      "Epoch 5050: train loss: 0.27600544691085815, val loss: 0.2770664095878601\n",
      "Epoch 5051: train loss: 0.2758185863494873, val loss: 0.27688631415367126\n",
      "Epoch 5052: train loss: 0.2756318151950836, val loss: 0.27670639753341675\n",
      "Epoch 5053: train loss: 0.2754451334476471, val loss: 0.27652648091316223\n",
      "Epoch 5054: train loss: 0.2752585709095001, val loss: 0.2763468325138092\n",
      "Epoch 5055: train loss: 0.2750721275806427, val loss: 0.27616721391677856\n",
      "Epoch 5056: train loss: 0.27488577365875244, val loss: 0.2759877145290375\n",
      "Epoch 5057: train loss: 0.27469950914382935, val loss: 0.27580833435058594\n",
      "Epoch 5058: train loss: 0.2745133340358734, val loss: 0.27562910318374634\n",
      "Epoch 5059: train loss: 0.2743273377418518, val loss: 0.2754499018192291\n",
      "Epoch 5060: train loss: 0.27414143085479736, val loss: 0.27527084946632385\n",
      "Epoch 5061: train loss: 0.2739556133747101, val loss: 0.27509188652038574\n",
      "Epoch 5062: train loss: 0.2737698256969452, val loss: 0.2749130129814148\n",
      "Epoch 5063: train loss: 0.27358412742614746, val loss: 0.2747341990470886\n",
      "Epoch 5064: train loss: 0.27339866757392883, val loss: 0.2745555341243744\n",
      "Epoch 5065: train loss: 0.2732132375240326, val loss: 0.2743770182132721\n",
      "Epoch 5066: train loss: 0.2730278968811035, val loss: 0.2741985619068146\n",
      "Epoch 5067: train loss: 0.2728426158428192, val loss: 0.2740202248096466\n",
      "Epoch 5068: train loss: 0.272657573223114, val loss: 0.27384209632873535\n",
      "Epoch 5069: train loss: 0.2724725902080536, val loss: 0.2736639678478241\n",
      "Epoch 5070: train loss: 0.27228766679763794, val loss: 0.2734859585762024\n",
      "Epoch 5071: train loss: 0.27210286259651184, val loss: 0.273308128118515\n",
      "Epoch 5072: train loss: 0.2719182074069977, val loss: 0.2731303572654724\n",
      "Epoch 5073: train loss: 0.2717336416244507, val loss: 0.2729526460170746\n",
      "Epoch 5074: train loss: 0.27154916524887085, val loss: 0.2727750241756439\n",
      "Epoch 5075: train loss: 0.2713647782802582, val loss: 0.2725975513458252\n",
      "Epoch 5076: train loss: 0.2711804211139679, val loss: 0.272420197725296\n",
      "Epoch 5077: train loss: 0.2709963023662567, val loss: 0.2722429931163788\n",
      "Epoch 5078: train loss: 0.2708122432231903, val loss: 0.2720657289028168\n",
      "Epoch 5079: train loss: 0.2706282436847687, val loss: 0.27188873291015625\n",
      "Epoch 5080: train loss: 0.2704443335533142, val loss: 0.2717117667198181\n",
      "Epoch 5081: train loss: 0.27026060223579407, val loss: 0.2715350091457367\n",
      "Epoch 5082: train loss: 0.2700769901275635, val loss: 0.27135828137397766\n",
      "Epoch 5083: train loss: 0.2698934078216553, val loss: 0.2711816728115082\n",
      "Epoch 5084: train loss: 0.26970991492271423, val loss: 0.2710050046443939\n",
      "Epoch 5085: train loss: 0.26952651143074036, val loss: 0.2708286941051483\n",
      "Epoch 5086: train loss: 0.2693432867527008, val loss: 0.2706523835659027\n",
      "Epoch 5087: train loss: 0.2691601812839508, val loss: 0.27047616243362427\n",
      "Epoch 5088: train loss: 0.2689770758152008, val loss: 0.2703000605106354\n",
      "Epoch 5089: train loss: 0.26879408955574036, val loss: 0.2701241672039032\n",
      "Epoch 5090: train loss: 0.2686113119125366, val loss: 0.26994824409484863\n",
      "Epoch 5091: train loss: 0.26842859387397766, val loss: 0.269772469997406\n",
      "Epoch 5092: train loss: 0.26824596524238586, val loss: 0.26959675550460815\n",
      "Epoch 5093: train loss: 0.26806339621543884, val loss: 0.26942119002342224\n",
      "Epoch 5094: train loss: 0.26788100600242615, val loss: 0.26924583315849304\n",
      "Epoch 5095: train loss: 0.2676987051963806, val loss: 0.26907047629356384\n",
      "Epoch 5096: train loss: 0.26751649379730225, val loss: 0.2688951790332794\n",
      "Epoch 5097: train loss: 0.26733434200286865, val loss: 0.26872003078460693\n",
      "Epoch 5098: train loss: 0.2671522796154022, val loss: 0.2685450613498688\n",
      "Epoch 5099: train loss: 0.2669704258441925, val loss: 0.26837021112442017\n",
      "Epoch 5100: train loss: 0.2667886018753052, val loss: 0.26819539070129395\n",
      "Epoch 5101: train loss: 0.266606867313385, val loss: 0.2680205702781677\n",
      "Epoch 5102: train loss: 0.2664252519607544, val loss: 0.26784607768058777\n",
      "Epoch 5103: train loss: 0.2662437856197357, val loss: 0.26767149567604065\n",
      "Epoch 5104: train loss: 0.2660624384880066, val loss: 0.26749706268310547\n",
      "Epoch 5105: train loss: 0.26588109135627747, val loss: 0.2673227787017822\n",
      "Epoch 5106: train loss: 0.2656998634338379, val loss: 0.26714855432510376\n",
      "Epoch 5107: train loss: 0.26551875472068787, val loss: 0.26697444915771484\n",
      "Epoch 5108: train loss: 0.2653377652168274, val loss: 0.26680052280426025\n",
      "Epoch 5109: train loss: 0.2651568651199341, val loss: 0.26662665605545044\n",
      "Epoch 5110: train loss: 0.2649760842323303, val loss: 0.266452819108963\n",
      "Epoch 5111: train loss: 0.26479533314704895, val loss: 0.2662792205810547\n",
      "Epoch 5112: train loss: 0.2646148204803467, val loss: 0.26610562205314636\n",
      "Epoch 5113: train loss: 0.2644343376159668, val loss: 0.2659322917461395\n",
      "Epoch 5114: train loss: 0.26425397396087646, val loss: 0.2657589018344879\n",
      "Epoch 5115: train loss: 0.2640736401081085, val loss: 0.265585720539093\n",
      "Epoch 5116: train loss: 0.2638935148715973, val loss: 0.2654125988483429\n",
      "Epoch 5117: train loss: 0.26371344923973083, val loss: 0.26523956656455994\n",
      "Epoch 5118: train loss: 0.26353350281715393, val loss: 0.26506665349006653\n",
      "Epoch 5119: train loss: 0.2633536159992218, val loss: 0.2648937404155731\n",
      "Epoch 5120: train loss: 0.26317375898361206, val loss: 0.2647210657596588\n",
      "Epoch 5121: train loss: 0.2629941403865814, val loss: 0.26454851031303406\n",
      "Epoch 5122: train loss: 0.26281458139419556, val loss: 0.26437586545944214\n",
      "Epoch 5123: train loss: 0.26263514161109924, val loss: 0.26420342922210693\n",
      "Epoch 5124: train loss: 0.26245570182800293, val loss: 0.2640313506126404\n",
      "Epoch 5125: train loss: 0.2622765004634857, val loss: 0.2638590931892395\n",
      "Epoch 5126: train loss: 0.26209738850593567, val loss: 0.26368704438209534\n",
      "Epoch 5127: train loss: 0.261918306350708, val loss: 0.2635149657726288\n",
      "Epoch 5128: train loss: 0.2617393136024475, val loss: 0.2633431851863861\n",
      "Epoch 5129: train loss: 0.2615605294704437, val loss: 0.26317140460014343\n",
      "Epoch 5130: train loss: 0.2613818049430847, val loss: 0.26299983263015747\n",
      "Epoch 5131: train loss: 0.2612031400203705, val loss: 0.2628282308578491\n",
      "Epoch 5132: train loss: 0.2610245645046234, val loss: 0.26265671849250793\n",
      "Epoch 5133: train loss: 0.2608460783958435, val loss: 0.2624853551387787\n",
      "Epoch 5134: train loss: 0.2606678009033203, val loss: 0.26231417059898376\n",
      "Epoch 5135: train loss: 0.2604895532131195, val loss: 0.2621431052684784\n",
      "Epoch 5136: train loss: 0.26031142473220825, val loss: 0.26197192072868347\n",
      "Epoch 5137: train loss: 0.2601333558559418, val loss: 0.2618010342121124\n",
      "Epoch 5138: train loss: 0.2599554657936096, val loss: 0.26163026690483093\n",
      "Epoch 5139: train loss: 0.25977763533592224, val loss: 0.2614595293998718\n",
      "Epoch 5140: train loss: 0.259599894285202, val loss: 0.2612888514995575\n",
      "Epoch 5141: train loss: 0.2594222128391266, val loss: 0.26111841201782227\n",
      "Epoch 5142: train loss: 0.25924476981163025, val loss: 0.2609480321407318\n",
      "Epoch 5143: train loss: 0.2590673863887787, val loss: 0.26077771186828613\n",
      "Epoch 5144: train loss: 0.2588900625705719, val loss: 0.2606075704097748\n",
      "Epoch 5145: train loss: 0.2587128281593323, val loss: 0.2604374587535858\n",
      "Epoch 5146: train loss: 0.25853562355041504, val loss: 0.2602674663066864\n",
      "Epoch 5147: train loss: 0.2583586573600769, val loss: 0.2600976526737213\n",
      "Epoch 5148: train loss: 0.25818172097206116, val loss: 0.25992774963378906\n",
      "Epoch 5149: train loss: 0.25800490379333496, val loss: 0.2597580850124359\n",
      "Epoch 5150: train loss: 0.2578281760215759, val loss: 0.2595885396003723\n",
      "Epoch 5151: train loss: 0.25765159726142883, val loss: 0.25941914319992065\n",
      "Epoch 5152: train loss: 0.2574751079082489, val loss: 0.259249746799469\n",
      "Epoch 5153: train loss: 0.25729867815971375, val loss: 0.2590804696083069\n",
      "Epoch 5154: train loss: 0.25712233781814575, val loss: 0.2589113414287567\n",
      "Epoch 5155: train loss: 0.2569461762905121, val loss: 0.2587423324584961\n",
      "Epoch 5156: train loss: 0.2567701041698456, val loss: 0.25857341289520264\n",
      "Epoch 5157: train loss: 0.25659409165382385, val loss: 0.2584044635295868\n",
      "Epoch 5158: train loss: 0.2564181685447693, val loss: 0.25823578238487244\n",
      "Epoch 5159: train loss: 0.25624242424964905, val loss: 0.2580673098564148\n",
      "Epoch 5160: train loss: 0.2560667395591736, val loss: 0.2578986883163452\n",
      "Epoch 5161: train loss: 0.2558911144733429, val loss: 0.25773030519485474\n",
      "Epoch 5162: train loss: 0.25571563839912415, val loss: 0.2575620114803314\n",
      "Epoch 5163: train loss: 0.2555401921272278, val loss: 0.2573937475681305\n",
      "Epoch 5164: train loss: 0.25536495447158813, val loss: 0.2572256624698639\n",
      "Epoch 5165: train loss: 0.2551897466182709, val loss: 0.25705763697624207\n",
      "Epoch 5166: train loss: 0.2550146281719208, val loss: 0.2568897306919098\n",
      "Epoch 5167: train loss: 0.25483962893486023, val loss: 0.25672194361686707\n",
      "Epoch 5168: train loss: 0.2546647787094116, val loss: 0.25655433535575867\n",
      "Epoch 5169: train loss: 0.2544900178909302, val loss: 0.2563866972923279\n",
      "Epoch 5170: train loss: 0.2543153166770935, val loss: 0.2562192380428314\n",
      "Epoch 5171: train loss: 0.2541406750679016, val loss: 0.2560518980026245\n",
      "Epoch 5172: train loss: 0.25396624207496643, val loss: 0.2558845281600952\n",
      "Epoch 5173: train loss: 0.2537918984889984, val loss: 0.2557174861431122\n",
      "Epoch 5174: train loss: 0.2536175549030304, val loss: 0.2555503845214844\n",
      "Epoch 5175: train loss: 0.2534433901309967, val loss: 0.25538337230682373\n",
      "Epoch 5176: train loss: 0.2532692849636078, val loss: 0.25521644949913025\n",
      "Epoch 5177: train loss: 0.2530952990055084, val loss: 0.25504976511001587\n",
      "Epoch 5178: train loss: 0.2529214322566986, val loss: 0.2548830807209015\n",
      "Epoch 5179: train loss: 0.25274765491485596, val loss: 0.2547164559364319\n",
      "Epoch 5180: train loss: 0.2525738775730133, val loss: 0.25455012917518616\n",
      "Epoch 5181: train loss: 0.25240036845207214, val loss: 0.25438380241394043\n",
      "Epoch 5182: train loss: 0.25222694873809814, val loss: 0.2542174756526947\n",
      "Epoch 5183: train loss: 0.25205349922180176, val loss: 0.25405141711235046\n",
      "Epoch 5184: train loss: 0.2518802285194397, val loss: 0.2538853883743286\n",
      "Epoch 5185: train loss: 0.2517070472240448, val loss: 0.2537195086479187\n",
      "Epoch 5186: train loss: 0.25153401494026184, val loss: 0.2535536587238312\n",
      "Epoch 5187: train loss: 0.25136104226112366, val loss: 0.2533878982067108\n",
      "Epoch 5188: train loss: 0.25118812918663025, val loss: 0.25322234630584717\n",
      "Epoch 5189: train loss: 0.25101539492607117, val loss: 0.2530568540096283\n",
      "Epoch 5190: train loss: 0.25084275007247925, val loss: 0.252891480922699\n",
      "Epoch 5191: train loss: 0.2506701648235321, val loss: 0.25272613763809204\n",
      "Epoch 5192: train loss: 0.2504976987838745, val loss: 0.25256094336509705\n",
      "Epoch 5193: train loss: 0.2503252625465393, val loss: 0.2523958384990692\n",
      "Epoch 5194: train loss: 0.2501530051231384, val loss: 0.2522308826446533\n",
      "Epoch 5195: train loss: 0.2499808520078659, val loss: 0.2520659863948822\n",
      "Epoch 5196: train loss: 0.24980878829956055, val loss: 0.25190111994743347\n",
      "Epoch 5197: train loss: 0.2496367245912552, val loss: 0.25173646211624146\n",
      "Epoch 5198: train loss: 0.24946491420269012, val loss: 0.2515718936920166\n",
      "Epoch 5199: train loss: 0.24929317831993103, val loss: 0.2514074742794037\n",
      "Epoch 5200: train loss: 0.24912148714065552, val loss: 0.2512430250644684\n",
      "Epoch 5201: train loss: 0.24894985556602478, val loss: 0.2510788142681122\n",
      "Epoch 5202: train loss: 0.24877846240997314, val loss: 0.25091463327407837\n",
      "Epoch 5203: train loss: 0.2486070841550827, val loss: 0.25075051188468933\n",
      "Epoch 5204: train loss: 0.24843581020832062, val loss: 0.25058647990226746\n",
      "Epoch 5205: train loss: 0.2482646107673645, val loss: 0.25042277574539185\n",
      "Epoch 5206: train loss: 0.2480935901403427, val loss: 0.25025901198387146\n",
      "Epoch 5207: train loss: 0.2479226291179657, val loss: 0.25009533762931824\n",
      "Epoch 5208: train loss: 0.24775177240371704, val loss: 0.24993176758289337\n",
      "Epoch 5209: train loss: 0.24758097529411316, val loss: 0.2497684210538864\n",
      "Epoch 5210: train loss: 0.2474103569984436, val loss: 0.24960504472255707\n",
      "Epoch 5211: train loss: 0.24723981320858002, val loss: 0.24944184720516205\n",
      "Epoch 5212: train loss: 0.2470693439245224, val loss: 0.24927866458892822\n",
      "Epoch 5213: train loss: 0.24689894914627075, val loss: 0.24911557137966156\n",
      "Epoch 5214: train loss: 0.24672864377498627, val loss: 0.2489527314901352\n",
      "Epoch 5215: train loss: 0.2465585172176361, val loss: 0.2487899363040924\n",
      "Epoch 5216: train loss: 0.24638846516609192, val loss: 0.24862714111804962\n",
      "Epoch 5217: train loss: 0.2462184727191925, val loss: 0.24846449494361877\n",
      "Epoch 5218: train loss: 0.24604856967926025, val loss: 0.24830202758312225\n",
      "Epoch 5219: train loss: 0.24587884545326233, val loss: 0.2481396198272705\n",
      "Epoch 5220: train loss: 0.24570916593074799, val loss: 0.24797730147838593\n",
      "Epoch 5221: train loss: 0.245539590716362, val loss: 0.24781499803066254\n",
      "Epoch 5222: train loss: 0.24537009000778198, val loss: 0.24765293300151825\n",
      "Epoch 5223: train loss: 0.24520078301429749, val loss: 0.2474909871816635\n",
      "Epoch 5224: train loss: 0.24503150582313538, val loss: 0.24732907116413116\n",
      "Epoch 5225: train loss: 0.24486233294010162, val loss: 0.24716722965240479\n",
      "Epoch 5226: train loss: 0.24469320476055145, val loss: 0.2470056116580963\n",
      "Epoch 5227: train loss: 0.2445243000984192, val loss: 0.24684403836727142\n",
      "Epoch 5228: train loss: 0.2443554550409317, val loss: 0.24668245017528534\n",
      "Epoch 5229: train loss: 0.2441866546869278, val loss: 0.24652110040187836\n",
      "Epoch 5230: train loss: 0.24401797354221344, val loss: 0.24635981023311615\n",
      "Epoch 5231: train loss: 0.24384945631027222, val loss: 0.24619868397712708\n",
      "Epoch 5232: train loss: 0.24368099868297577, val loss: 0.2460375875234604\n",
      "Epoch 5233: train loss: 0.24351264536380768, val loss: 0.24587659537792206\n",
      "Epoch 5234: train loss: 0.24334433674812317, val loss: 0.24571572244167328\n",
      "Epoch 5235: train loss: 0.24317613244056702, val loss: 0.24555495381355286\n",
      "Epoch 5236: train loss: 0.243008092045784, val loss: 0.2453942596912384\n",
      "Epoch 5237: train loss: 0.24284011125564575, val loss: 0.24523372948169708\n",
      "Epoch 5238: train loss: 0.24267221987247467, val loss: 0.24507324397563934\n",
      "Epoch 5239: train loss: 0.24250437319278717, val loss: 0.24491284787654877\n",
      "Epoch 5240: train loss: 0.24233675003051758, val loss: 0.24475257098674774\n",
      "Epoch 5241: train loss: 0.24216920137405396, val loss: 0.2445923537015915\n",
      "Epoch 5242: train loss: 0.24200168251991272, val loss: 0.24443228542804718\n",
      "Epoch 5243: train loss: 0.24183426797389984, val loss: 0.2442723512649536\n",
      "Epoch 5244: train loss: 0.2416670173406601, val loss: 0.24411249160766602\n",
      "Epoch 5245: train loss: 0.24149984121322632, val loss: 0.24395279586315155\n",
      "Epoch 5246: train loss: 0.2413327842950821, val loss: 0.24379311501979828\n",
      "Epoch 5247: train loss: 0.24116571247577667, val loss: 0.24363361299037933\n",
      "Epoch 5248: train loss: 0.24099890887737274, val loss: 0.24347424507141113\n",
      "Epoch 5249: train loss: 0.24083212018013, val loss: 0.24331490695476532\n",
      "Epoch 5250: train loss: 0.24066543579101562, val loss: 0.2431555837392807\n",
      "Epoch 5251: train loss: 0.24049882590770721, val loss: 0.2429964542388916\n",
      "Epoch 5252: train loss: 0.24033237993717194, val loss: 0.24283741414546967\n",
      "Epoch 5253: train loss: 0.24016596376895905, val loss: 0.24267840385437012\n",
      "Epoch 5254: train loss: 0.2399996966123581, val loss: 0.24251960217952728\n",
      "Epoch 5255: train loss: 0.23983347415924072, val loss: 0.2423609346151352\n",
      "Epoch 5256: train loss: 0.23966743052005768, val loss: 0.2422022670507431\n",
      "Epoch 5257: train loss: 0.2395014464855194, val loss: 0.2420438528060913\n",
      "Epoch 5258: train loss: 0.2393355816602707, val loss: 0.24188533425331116\n",
      "Epoch 5259: train loss: 0.23916976153850555, val loss: 0.24172699451446533\n",
      "Epoch 5260: train loss: 0.2390040159225464, val loss: 0.24156875908374786\n",
      "Epoch 5261: train loss: 0.23883844912052155, val loss: 0.24141068756580353\n",
      "Epoch 5262: train loss: 0.23867294192314148, val loss: 0.24125266075134277\n",
      "Epoch 5263: train loss: 0.23850752413272858, val loss: 0.24109458923339844\n",
      "Epoch 5264: train loss: 0.23834221065044403, val loss: 0.24093683063983917\n",
      "Epoch 5265: train loss: 0.23817698657512665, val loss: 0.24077916145324707\n",
      "Epoch 5266: train loss: 0.2380119115114212, val loss: 0.24062147736549377\n",
      "Epoch 5267: train loss: 0.23784691095352173, val loss: 0.24046392738819122\n",
      "Epoch 5268: train loss: 0.23768194019794464, val loss: 0.2403065413236618\n",
      "Epoch 5269: train loss: 0.23751717805862427, val loss: 0.24014924466609955\n",
      "Epoch 5270: train loss: 0.23735246062278748, val loss: 0.23999202251434326\n",
      "Epoch 5271: train loss: 0.23718783259391785, val loss: 0.23983488976955414\n",
      "Epoch 5272: train loss: 0.23702329397201538, val loss: 0.23967795073986053\n",
      "Epoch 5273: train loss: 0.23685888946056366, val loss: 0.2395210713148117\n",
      "Epoch 5274: train loss: 0.23669461905956268, val loss: 0.23936425149440765\n",
      "Epoch 5275: train loss: 0.2365303784608841, val loss: 0.23920749127864838\n",
      "Epoch 5276: train loss: 0.23636624217033386, val loss: 0.23905090987682343\n",
      "Epoch 5277: train loss: 0.23620223999023438, val loss: 0.23889444768428802\n",
      "Epoch 5278: train loss: 0.23603831231594086, val loss: 0.23873798549175262\n",
      "Epoch 5279: train loss: 0.23587451875209808, val loss: 0.23858173191547394\n",
      "Epoch 5280: train loss: 0.2357107400894165, val loss: 0.23842549324035645\n",
      "Epoch 5281: train loss: 0.23554715514183044, val loss: 0.23826943337917328\n",
      "Epoch 5282: train loss: 0.23538362979888916, val loss: 0.2381134331226349\n",
      "Epoch 5283: train loss: 0.23522020876407623, val loss: 0.23795752227306366\n",
      "Epoch 5284: train loss: 0.23505684733390808, val loss: 0.23780174553394318\n",
      "Epoch 5285: train loss: 0.23489364981651306, val loss: 0.23764614760875702\n",
      "Epoch 5286: train loss: 0.2347305417060852, val loss: 0.2374904900789261\n",
      "Epoch 5287: train loss: 0.23456749320030212, val loss: 0.23733492195606232\n",
      "Epoch 5288: train loss: 0.2344045341014862, val loss: 0.2371797114610672\n",
      "Epoch 5289: train loss: 0.23424172401428223, val loss: 0.23702438175678253\n",
      "Epoch 5290: train loss: 0.2340790033340454, val loss: 0.2368692010641098\n",
      "Epoch 5291: train loss: 0.23391635715961456, val loss: 0.23671412467956543\n",
      "Epoch 5292: train loss: 0.23375380039215088, val loss: 0.23655906319618225\n",
      "Epoch 5293: train loss: 0.23359130322933197, val loss: 0.2364041805267334\n",
      "Epoch 5294: train loss: 0.233428955078125, val loss: 0.23624944686889648\n",
      "Epoch 5295: train loss: 0.23326672613620758, val loss: 0.2360946387052536\n",
      "Epoch 5296: train loss: 0.23310454189777374, val loss: 0.235940083861351\n",
      "Epoch 5297: train loss: 0.23294243216514587, val loss: 0.23578567802906036\n",
      "Epoch 5298: train loss: 0.23278048634529114, val loss: 0.23563118278980255\n",
      "Epoch 5299: train loss: 0.23261862993240356, val loss: 0.23547695577144623\n",
      "Epoch 5300: train loss: 0.23245686292648315, val loss: 0.2353227585554123\n",
      "Epoch 5301: train loss: 0.23229514062404633, val loss: 0.2351686954498291\n",
      "Epoch 5302: train loss: 0.23213356733322144, val loss: 0.23501470685005188\n",
      "Epoch 5303: train loss: 0.2319721281528473, val loss: 0.23486080765724182\n",
      "Epoch 5304: train loss: 0.23181071877479553, val loss: 0.23470692336559296\n",
      "Epoch 5305: train loss: 0.23164942860603333, val loss: 0.23455333709716797\n",
      "Epoch 5306: train loss: 0.23148825764656067, val loss: 0.2343997061252594\n",
      "Epoch 5307: train loss: 0.23132719099521637, val loss: 0.23424629867076874\n",
      "Epoch 5308: train loss: 0.23116621375083923, val loss: 0.2340928614139557\n",
      "Epoch 5309: train loss: 0.23100526630878448, val loss: 0.23393963277339935\n",
      "Epoch 5310: train loss: 0.23084452748298645, val loss: 0.23378638923168182\n",
      "Epoch 5311: train loss: 0.2306838482618332, val loss: 0.2336333841085434\n",
      "Epoch 5312: train loss: 0.23052319884300232, val loss: 0.23348037898540497\n",
      "Epoch 5313: train loss: 0.2303626835346222, val loss: 0.2333274930715561\n",
      "Epoch 5314: train loss: 0.23020236194133759, val loss: 0.23317472636699677\n",
      "Epoch 5315: train loss: 0.23004204034805298, val loss: 0.233022078871727\n",
      "Epoch 5316: train loss: 0.22988182306289673, val loss: 0.23286938667297363\n",
      "Epoch 5317: train loss: 0.22972166538238525, val loss: 0.23271699249744415\n",
      "Epoch 5318: train loss: 0.2295617014169693, val loss: 0.23256464302539825\n",
      "Epoch 5319: train loss: 0.2294018268585205, val loss: 0.23241238296031952\n",
      "Epoch 5320: train loss: 0.2292419970035553, val loss: 0.23226013779640198\n",
      "Epoch 5321: train loss: 0.22908224165439606, val loss: 0.23210807144641876\n",
      "Epoch 5322: train loss: 0.22892263531684875, val loss: 0.2319561094045639\n",
      "Epoch 5323: train loss: 0.22876311838626862, val loss: 0.2318042367696762\n",
      "Epoch 5324: train loss: 0.22860369086265564, val loss: 0.2316524088382721\n",
      "Epoch 5325: train loss: 0.22844433784484863, val loss: 0.23150081932544708\n",
      "Epoch 5326: train loss: 0.22828513383865356, val loss: 0.23134927451610565\n",
      "Epoch 5327: train loss: 0.22812601923942566, val loss: 0.23119774460792542\n",
      "Epoch 5328: train loss: 0.22796696424484253, val loss: 0.23104631900787354\n",
      "Epoch 5329: train loss: 0.22780799865722656, val loss: 0.23089511692523956\n",
      "Epoch 5330: train loss: 0.22764922678470612, val loss: 0.2307438850402832\n",
      "Epoch 5331: train loss: 0.22749045491218567, val loss: 0.23059284687042236\n",
      "Epoch 5332: train loss: 0.22733181715011597, val loss: 0.2304418534040451\n",
      "Epoch 5333: train loss: 0.22717323899269104, val loss: 0.2302909940481186\n",
      "Epoch 5334: train loss: 0.22701479494571686, val loss: 0.23014025390148163\n",
      "Epoch 5335: train loss: 0.22685648500919342, val loss: 0.22998957335948944\n",
      "Epoch 5336: train loss: 0.22669821977615356, val loss: 0.2298389971256256\n",
      "Epoch 5337: train loss: 0.2265399992465973, val loss: 0.22968855500221252\n",
      "Epoch 5338: train loss: 0.22638198733329773, val loss: 0.22953815758228302\n",
      "Epoch 5339: train loss: 0.22622404992580414, val loss: 0.22938786447048187\n",
      "Epoch 5340: train loss: 0.22606612741947174, val loss: 0.2292376607656479\n",
      "Epoch 5341: train loss: 0.2259083241224289, val loss: 0.22908759117126465\n",
      "Epoch 5342: train loss: 0.22575071454048157, val loss: 0.22893765568733215\n",
      "Epoch 5343: train loss: 0.22559314966201782, val loss: 0.22878776490688324\n",
      "Epoch 5344: train loss: 0.22543564438819885, val loss: 0.22863797843456268\n",
      "Epoch 5345: train loss: 0.22527822852134705, val loss: 0.22848835587501526\n",
      "Epoch 5346: train loss: 0.22512097656726837, val loss: 0.228338822722435\n",
      "Epoch 5347: train loss: 0.22496381402015686, val loss: 0.22818930447101593\n",
      "Epoch 5348: train loss: 0.22480668127536774, val loss: 0.22803989052772522\n",
      "Epoch 5349: train loss: 0.22464965283870697, val loss: 0.22789064049720764\n",
      "Epoch 5350: train loss: 0.22449277341365814, val loss: 0.22774147987365723\n",
      "Epoch 5351: train loss: 0.22433599829673767, val loss: 0.2275923490524292\n",
      "Epoch 5352: train loss: 0.22417929768562317, val loss: 0.2274433672428131\n",
      "Epoch 5353: train loss: 0.22402264177799225, val loss: 0.22729448974132538\n",
      "Epoch 5354: train loss: 0.22386614978313446, val loss: 0.22714577615261078\n",
      "Epoch 5355: train loss: 0.22370974719524384, val loss: 0.22699704766273499\n",
      "Epoch 5356: train loss: 0.22355343401432037, val loss: 0.22684846818447113\n",
      "Epoch 5357: train loss: 0.2233971804380417, val loss: 0.22669993340969086\n",
      "Epoch 5358: train loss: 0.22324107587337494, val loss: 0.22655163705348969\n",
      "Epoch 5359: train loss: 0.22308503091335297, val loss: 0.22640328109264374\n",
      "Epoch 5360: train loss: 0.22292909026145935, val loss: 0.22625505924224854\n",
      "Epoch 5361: train loss: 0.2227732390165329, val loss: 0.22610703110694885\n",
      "Epoch 5362: train loss: 0.2226175218820572, val loss: 0.22595906257629395\n",
      "Epoch 5363: train loss: 0.22246187925338745, val loss: 0.22581127285957336\n",
      "Epoch 5364: train loss: 0.22230632603168488, val loss: 0.22566340863704681\n",
      "Epoch 5365: train loss: 0.22215083241462708, val loss: 0.225515678524971\n",
      "Epoch 5366: train loss: 0.2219955027103424, val loss: 0.2253681719303131\n",
      "Epoch 5367: train loss: 0.2218402922153473, val loss: 0.22522056102752686\n",
      "Epoch 5368: train loss: 0.22168508172035217, val loss: 0.2250732034444809\n",
      "Epoch 5369: train loss: 0.2215299904346466, val loss: 0.22492587566375732\n",
      "Epoch 5370: train loss: 0.2213750183582306, val loss: 0.22477875649929047\n",
      "Epoch 5371: train loss: 0.22122016549110413, val loss: 0.22463159263134003\n",
      "Epoch 5372: train loss: 0.22106535732746124, val loss: 0.22448454797267914\n",
      "Epoch 5373: train loss: 0.2209106683731079, val loss: 0.22433769702911377\n",
      "Epoch 5374: train loss: 0.22075608372688293, val loss: 0.22419090569019318\n",
      "Epoch 5375: train loss: 0.22060158848762512, val loss: 0.22404412925243378\n",
      "Epoch 5376: train loss: 0.22044721245765686, val loss: 0.2238975614309311\n",
      "Epoch 5377: train loss: 0.220292866230011, val loss: 0.22375105321407318\n",
      "Epoch 5378: train loss: 0.22013869881629944, val loss: 0.22360463440418243\n",
      "Epoch 5379: train loss: 0.21998460590839386, val loss: 0.22345833480358124\n",
      "Epoch 5380: train loss: 0.21983054280281067, val loss: 0.22331205010414124\n",
      "Epoch 5381: train loss: 0.21967659890651703, val loss: 0.22316598892211914\n",
      "Epoch 5382: train loss: 0.21952281892299652, val loss: 0.22301998734474182\n",
      "Epoch 5383: train loss: 0.2193690836429596, val loss: 0.22287407517433167\n",
      "Epoch 5384: train loss: 0.21921543776988983, val loss: 0.2227282077074051\n",
      "Epoch 5385: train loss: 0.21906188130378723, val loss: 0.22258253395557404\n",
      "Epoch 5386: train loss: 0.21890848875045776, val loss: 0.22243690490722656\n",
      "Epoch 5387: train loss: 0.21875514090061188, val loss: 0.22229142487049103\n",
      "Epoch 5388: train loss: 0.21860186755657196, val loss: 0.2221459150314331\n",
      "Epoch 5389: train loss: 0.218448668718338, val loss: 0.2220006287097931\n",
      "Epoch 5390: train loss: 0.2182956337928772, val loss: 0.22185544669628143\n",
      "Epoch 5391: train loss: 0.21814268827438354, val loss: 0.221710205078125\n",
      "Epoch 5392: train loss: 0.21798980236053467, val loss: 0.22156520187854767\n",
      "Epoch 5393: train loss: 0.21783697605133057, val loss: 0.2214203178882599\n",
      "Epoch 5394: train loss: 0.21768434345722198, val loss: 0.2212754786014557\n",
      "Epoch 5395: train loss: 0.21753175556659698, val loss: 0.22113065421581268\n",
      "Epoch 5396: train loss: 0.21737925708293915, val loss: 0.2209860384464264\n",
      "Epoch 5397: train loss: 0.21722683310508728, val loss: 0.22084152698516846\n",
      "Epoch 5398: train loss: 0.21707457304000854, val loss: 0.22069703042507172\n",
      "Epoch 5399: train loss: 0.21692238748073578, val loss: 0.22055268287658691\n",
      "Epoch 5400: train loss: 0.2167702615261078, val loss: 0.22040843963623047\n",
      "Epoch 5401: train loss: 0.21661821007728577, val loss: 0.22026430070400238\n",
      "Epoch 5402: train loss: 0.21646632254123688, val loss: 0.22012034058570862\n",
      "Epoch 5403: train loss: 0.21631449460983276, val loss: 0.2199762910604477\n",
      "Epoch 5404: train loss: 0.2161627560853958, val loss: 0.2198324203491211\n",
      "Epoch 5405: train loss: 0.21601104736328125, val loss: 0.21968860924243927\n",
      "Epoch 5406: train loss: 0.2158595621585846, val loss: 0.21954503655433655\n",
      "Epoch 5407: train loss: 0.21570812165737152, val loss: 0.21940135955810547\n",
      "Epoch 5408: train loss: 0.2155567854642868, val loss: 0.2192579060792923\n",
      "Epoch 5409: train loss: 0.21540549397468567, val loss: 0.21911457180976868\n",
      "Epoch 5410: train loss: 0.21525433659553528, val loss: 0.21897125244140625\n",
      "Epoch 5411: train loss: 0.21510329842567444, val loss: 0.21882811188697815\n",
      "Epoch 5412: train loss: 0.21495230495929718, val loss: 0.21868494153022766\n",
      "Epoch 5413: train loss: 0.2148013859987259, val loss: 0.2185419648885727\n",
      "Epoch 5414: train loss: 0.21465064585208893, val loss: 0.21839912235736847\n",
      "Epoch 5415: train loss: 0.21449995040893555, val loss: 0.21825627982616425\n",
      "Epoch 5416: train loss: 0.21434937417507172, val loss: 0.21811357140541077\n",
      "Epoch 5417: train loss: 0.21419881284236908, val loss: 0.21797099709510803\n",
      "Epoch 5418: train loss: 0.21404846012592316, val loss: 0.21782851219177246\n",
      "Epoch 5419: train loss: 0.2138981819152832, val loss: 0.21768608689308167\n",
      "Epoch 5420: train loss: 0.21374791860580444, val loss: 0.21754376590251923\n",
      "Epoch 5421: train loss: 0.21359774470329285, val loss: 0.21740157902240753\n",
      "Epoch 5422: train loss: 0.21344776451587677, val loss: 0.2172595113515854\n",
      "Epoch 5423: train loss: 0.21329782903194427, val loss: 0.21711735427379608\n",
      "Epoch 5424: train loss: 0.21314799785614014, val loss: 0.21697543561458588\n",
      "Epoch 5425: train loss: 0.2129981815814972, val loss: 0.21683369576931\n",
      "Epoch 5426: train loss: 0.21284858882427216, val loss: 0.21669192612171173\n",
      "Epoch 5427: train loss: 0.2126990258693695, val loss: 0.2165503352880478\n",
      "Epoch 5428: train loss: 0.21254953742027283, val loss: 0.21640877425670624\n",
      "Epoch 5429: train loss: 0.2124001383781433, val loss: 0.21626730263233185\n",
      "Epoch 5430: train loss: 0.21225090324878693, val loss: 0.2161259949207306\n",
      "Epoch 5431: train loss: 0.2121017575263977, val loss: 0.2159847468137741\n",
      "Epoch 5432: train loss: 0.21195264160633087, val loss: 0.2158435434103012\n",
      "Epoch 5433: train loss: 0.2118036150932312, val loss: 0.21570256352424622\n",
      "Epoch 5434: train loss: 0.21165475249290466, val loss: 0.21556158363819122\n",
      "Epoch 5435: train loss: 0.2115059345960617, val loss: 0.2154206782579422\n",
      "Epoch 5436: train loss: 0.2113572061061859, val loss: 0.21527992188930511\n",
      "Epoch 5437: train loss: 0.2112085372209549, val loss: 0.2151392698287964\n",
      "Epoch 5438: train loss: 0.2110600620508194, val loss: 0.21499867737293243\n",
      "Epoch 5439: train loss: 0.21091163158416748, val loss: 0.21485821902751923\n",
      "Epoch 5440: train loss: 0.21076326072216034, val loss: 0.21471789479255676\n",
      "Epoch 5441: train loss: 0.21061508357524872, val loss: 0.21457770466804504\n",
      "Epoch 5442: train loss: 0.21046698093414307, val loss: 0.21443746984004974\n",
      "Epoch 5443: train loss: 0.2103189378976822, val loss: 0.21429741382598877\n",
      "Epoch 5444: train loss: 0.21017096936702728, val loss: 0.21415749192237854\n",
      "Epoch 5445: train loss: 0.21002312004566193, val loss: 0.21401751041412354\n",
      "Epoch 5446: train loss: 0.20987538993358612, val loss: 0.21387779712677002\n",
      "Epoch 5447: train loss: 0.2097277194261551, val loss: 0.2137380689382553\n",
      "Epoch 5448: train loss: 0.20958010852336884, val loss: 0.2135985642671585\n",
      "Epoch 5449: train loss: 0.2094326764345169, val loss: 0.2134590446949005\n",
      "Epoch 5450: train loss: 0.20928531885147095, val loss: 0.21331964433193207\n",
      "Epoch 5451: train loss: 0.20913799107074738, val loss: 0.2131803035736084\n",
      "Epoch 5452: train loss: 0.20899078249931335, val loss: 0.21304111182689667\n",
      "Epoch 5453: train loss: 0.20884370803833008, val loss: 0.2129019945859909\n",
      "Epoch 5454: train loss: 0.20869670808315277, val loss: 0.21276290714740753\n",
      "Epoch 5455: train loss: 0.20854975283145905, val loss: 0.21262402832508087\n",
      "Epoch 5456: train loss: 0.20840291678905487, val loss: 0.21248526871204376\n",
      "Epoch 5457: train loss: 0.20825621485710144, val loss: 0.21234653890132904\n",
      "Epoch 5458: train loss: 0.20810963213443756, val loss: 0.21220794320106506\n",
      "Epoch 5459: train loss: 0.2079630345106125, val loss: 0.21206927299499512\n",
      "Epoch 5460: train loss: 0.20781652629375458, val loss: 0.21193090081214905\n",
      "Epoch 5461: train loss: 0.20767024159431458, val loss: 0.21179266273975372\n",
      "Epoch 5462: train loss: 0.20752398669719696, val loss: 0.21165430545806885\n",
      "Epoch 5463: train loss: 0.20737780630588531, val loss: 0.2115161269903183\n",
      "Epoch 5464: train loss: 0.20723167061805725, val loss: 0.21137814223766327\n",
      "Epoch 5465: train loss: 0.2070857137441635, val loss: 0.21124012768268585\n",
      "Epoch 5466: train loss: 0.20693983137607574, val loss: 0.21110229194164276\n",
      "Epoch 5467: train loss: 0.20679402351379395, val loss: 0.21096444129943848\n",
      "Epoch 5468: train loss: 0.20664826035499573, val loss: 0.2108267992734909\n",
      "Epoch 5469: train loss: 0.20650270581245422, val loss: 0.2106892317533493\n",
      "Epoch 5470: train loss: 0.2063571959733963, val loss: 0.21055172383785248\n",
      "Epoch 5471: train loss: 0.20621173083782196, val loss: 0.2104143649339676\n",
      "Epoch 5472: train loss: 0.20606645941734314, val loss: 0.21027718484401703\n",
      "Epoch 5473: train loss: 0.2059212177991867, val loss: 0.2101399004459381\n",
      "Epoch 5474: train loss: 0.20577611029148102, val loss: 0.2100028544664383\n",
      "Epoch 5475: train loss: 0.2056310474872589, val loss: 0.20986585319042206\n",
      "Epoch 5476: train loss: 0.20548608899116516, val loss: 0.20972898602485657\n",
      "Epoch 5477: train loss: 0.20534127950668335, val loss: 0.20959225296974182\n",
      "Epoch 5478: train loss: 0.20519648492336273, val loss: 0.20945541560649872\n",
      "Epoch 5479: train loss: 0.20505176484584808, val loss: 0.2093188315629959\n",
      "Epoch 5480: train loss: 0.20490725338459015, val loss: 0.20918230712413788\n",
      "Epoch 5481: train loss: 0.2047627866268158, val loss: 0.2090458869934082\n",
      "Epoch 5482: train loss: 0.20461837947368622, val loss: 0.2089095115661621\n",
      "Epoch 5483: train loss: 0.2044740468263626, val loss: 0.20877335965633392\n",
      "Epoch 5484: train loss: 0.20432986319065094, val loss: 0.20863723754882812\n",
      "Epoch 5485: train loss: 0.20418578386306763, val loss: 0.2085012048482895\n",
      "Epoch 5486: train loss: 0.2040417343378067, val loss: 0.20836511254310608\n",
      "Epoch 5487: train loss: 0.20389777421951294, val loss: 0.2082294076681137\n",
      "Epoch 5488: train loss: 0.20375396311283112, val loss: 0.20809359848499298\n",
      "Epoch 5489: train loss: 0.20361019670963287, val loss: 0.20795796811580658\n",
      "Epoch 5490: train loss: 0.20346654951572418, val loss: 0.20782236754894257\n",
      "Epoch 5491: train loss: 0.20332291722297668, val loss: 0.2076869010925293\n",
      "Epoch 5492: train loss: 0.2031795084476471, val loss: 0.20755158364772797\n",
      "Epoch 5493: train loss: 0.2030361294746399, val loss: 0.20741629600524902\n",
      "Epoch 5494: train loss: 0.20289285480976105, val loss: 0.20728114247322083\n",
      "Epoch 5495: train loss: 0.20274972915649414, val loss: 0.2071460485458374\n",
      "Epoch 5496: train loss: 0.20260658860206604, val loss: 0.20701101422309875\n",
      "Epoch 5497: train loss: 0.20246364176273346, val loss: 0.20687603950500488\n",
      "Epoch 5498: train loss: 0.20232069492340088, val loss: 0.2067413628101349\n",
      "Epoch 5499: train loss: 0.20217789709568024, val loss: 0.2066067010164261\n",
      "Epoch 5500: train loss: 0.20203520357608795, val loss: 0.2064719945192337\n",
      "Epoch 5501: train loss: 0.20189255475997925, val loss: 0.20633748173713684\n",
      "Epoch 5502: train loss: 0.2017500251531601, val loss: 0.2062031477689743\n",
      "Epoch 5503: train loss: 0.2016075998544693, val loss: 0.20606878399848938\n",
      "Epoch 5504: train loss: 0.20146524906158447, val loss: 0.20593450963497162\n",
      "Epoch 5505: train loss: 0.2013229876756668, val loss: 0.205800399184227\n",
      "Epoch 5506: train loss: 0.20118078589439392, val loss: 0.20566630363464355\n",
      "Epoch 5507: train loss: 0.20103874802589417, val loss: 0.20553243160247803\n",
      "Epoch 5508: train loss: 0.200896754860878, val loss: 0.20539860427379608\n",
      "Epoch 5509: train loss: 0.20075486600399017, val loss: 0.20526476204395294\n",
      "Epoch 5510: train loss: 0.20061302185058594, val loss: 0.20513109862804413\n",
      "Epoch 5511: train loss: 0.20047132670879364, val loss: 0.20499761402606964\n",
      "Epoch 5512: train loss: 0.2003297209739685, val loss: 0.20486405491828918\n",
      "Epoch 5513: train loss: 0.20018818974494934, val loss: 0.20473070442676544\n",
      "Epoch 5514: train loss: 0.20004671812057495, val loss: 0.20459745824337006\n",
      "Epoch 5515: train loss: 0.1999054253101349, val loss: 0.20446419715881348\n",
      "Epoch 5516: train loss: 0.1997641772031784, val loss: 0.2043311595916748\n",
      "Epoch 5517: train loss: 0.1996229887008667, val loss: 0.20419812202453613\n",
      "Epoch 5518: train loss: 0.19948196411132812, val loss: 0.20406527817249298\n",
      "Epoch 5519: train loss: 0.1993410438299179, val loss: 0.20393238961696625\n",
      "Epoch 5520: train loss: 0.19920015335083008, val loss: 0.20379972457885742\n",
      "Epoch 5521: train loss: 0.19905933737754822, val loss: 0.20366711914539337\n",
      "Epoch 5522: train loss: 0.1989186853170395, val loss: 0.20353467762470245\n",
      "Epoch 5523: train loss: 0.19877810776233673, val loss: 0.20340216159820557\n",
      "Epoch 5524: train loss: 0.19863756000995636, val loss: 0.20326972007751465\n",
      "Epoch 5525: train loss: 0.19849711656570435, val loss: 0.2031375914812088\n",
      "Epoch 5526: train loss: 0.19835685193538666, val loss: 0.20300541818141937\n",
      "Epoch 5527: train loss: 0.19821663200855255, val loss: 0.20287342369556427\n",
      "Epoch 5528: train loss: 0.19807642698287964, val loss: 0.20274138450622559\n",
      "Epoch 5529: train loss: 0.19793635606765747, val loss: 0.20260953903198242\n",
      "Epoch 5530: train loss: 0.19779644906520844, val loss: 0.2024778127670288\n",
      "Epoch 5531: train loss: 0.19765658676624298, val loss: 0.20234611630439758\n",
      "Epoch 5532: train loss: 0.1975167840719223, val loss: 0.20221452414989471\n",
      "Epoch 5533: train loss: 0.19737714529037476, val loss: 0.2020830363035202\n",
      "Epoch 5534: train loss: 0.19723759591579437, val loss: 0.20195169746875763\n",
      "Epoch 5535: train loss: 0.19709810614585876, val loss: 0.20182041823863983\n",
      "Epoch 5536: train loss: 0.19695864617824554, val loss: 0.20168916881084442\n",
      "Epoch 5537: train loss: 0.19681937992572784, val loss: 0.20155806839466095\n",
      "Epoch 5538: train loss: 0.19668015837669373, val loss: 0.20142708718776703\n",
      "Epoch 5539: train loss: 0.19654105603694916, val loss: 0.2012961208820343\n",
      "Epoch 5540: train loss: 0.1964019387960434, val loss: 0.20116539299488068\n",
      "Epoch 5541: train loss: 0.19626304507255554, val loss: 0.20103462040424347\n",
      "Epoch 5542: train loss: 0.19612421095371246, val loss: 0.20090392231941223\n",
      "Epoch 5543: train loss: 0.19598542153835297, val loss: 0.20077334344387054\n",
      "Epoch 5544: train loss: 0.19584670662879944, val loss: 0.200642928481102\n",
      "Epoch 5545: train loss: 0.19570815563201904, val loss: 0.2005126029253006\n",
      "Epoch 5546: train loss: 0.1955696940422058, val loss: 0.2003823071718216\n",
      "Epoch 5547: train loss: 0.19543126225471497, val loss: 0.20025217533111572\n",
      "Epoch 5548: train loss: 0.19529300928115845, val loss: 0.2001221626996994\n",
      "Epoch 5549: train loss: 0.19515478610992432, val loss: 0.1999921351671219\n",
      "Epoch 5550: train loss: 0.19501665234565735, val loss: 0.19986221194267273\n",
      "Epoch 5551: train loss: 0.19487865269184113, val loss: 0.1997324526309967\n",
      "Epoch 5552: train loss: 0.19474069774150848, val loss: 0.19960276782512665\n",
      "Epoch 5553: train loss: 0.19460290670394897, val loss: 0.19947312772274017\n",
      "Epoch 5554: train loss: 0.19446513056755066, val loss: 0.19934363663196564\n",
      "Epoch 5555: train loss: 0.1943274289369583, val loss: 0.1992141753435135\n",
      "Epoch 5556: train loss: 0.1941898763179779, val loss: 0.19908487796783447\n",
      "Epoch 5557: train loss: 0.19405238330364227, val loss: 0.19895561039447784\n",
      "Epoch 5558: train loss: 0.1939150094985962, val loss: 0.19882647693157196\n",
      "Epoch 5559: train loss: 0.1937776654958725, val loss: 0.19869743287563324\n",
      "Epoch 5560: train loss: 0.19364045560359955, val loss: 0.19856850802898407\n",
      "Epoch 5561: train loss: 0.19350336492061615, val loss: 0.19843968749046326\n",
      "Epoch 5562: train loss: 0.19336630403995514, val loss: 0.19831089675426483\n",
      "Epoch 5563: train loss: 0.19322942197322845, val loss: 0.19818222522735596\n",
      "Epoch 5564: train loss: 0.19309259951114655, val loss: 0.19805368781089783\n",
      "Epoch 5565: train loss: 0.1929558664560318, val loss: 0.1979251354932785\n",
      "Epoch 5566: train loss: 0.19281914830207825, val loss: 0.1977967768907547\n",
      "Epoch 5567: train loss: 0.19268260896205902, val loss: 0.19766849279403687\n",
      "Epoch 5568: train loss: 0.19254614412784576, val loss: 0.197540283203125\n",
      "Epoch 5569: train loss: 0.19240973889827728, val loss: 0.19741208851337433\n",
      "Epoch 5570: train loss: 0.19227339327335358, val loss: 0.19728414714336395\n",
      "Epoch 5571: train loss: 0.192137211561203, val loss: 0.19715619087219238\n",
      "Epoch 5572: train loss: 0.1920010894536972, val loss: 0.19702838361263275\n",
      "Epoch 5573: train loss: 0.19186507165431976, val loss: 0.19690056145191193\n",
      "Epoch 5574: train loss: 0.1917290836572647, val loss: 0.19677303731441498\n",
      "Epoch 5575: train loss: 0.1915932595729828, val loss: 0.19664539396762848\n",
      "Epoch 5576: train loss: 0.19145749509334564, val loss: 0.19651800394058228\n",
      "Epoch 5577: train loss: 0.19132182002067566, val loss: 0.19639058411121368\n",
      "Epoch 5578: train loss: 0.19118627905845642, val loss: 0.19626334309577942\n",
      "Epoch 5579: train loss: 0.19105081260204315, val loss: 0.1961362361907959\n",
      "Epoch 5580: train loss: 0.19091543555259705, val loss: 0.19600903987884521\n",
      "Epoch 5581: train loss: 0.19078004360198975, val loss: 0.19588196277618408\n",
      "Epoch 5582: train loss: 0.19064490497112274, val loss: 0.19575512409210205\n",
      "Epoch 5583: train loss: 0.19050979614257812, val loss: 0.1956283152103424\n",
      "Epoch 5584: train loss: 0.1903747320175171, val loss: 0.19550155103206635\n",
      "Epoch 5585: train loss: 0.19023974239826202, val loss: 0.1953749805688858\n",
      "Epoch 5586: train loss: 0.19010496139526367, val loss: 0.19524846971035004\n",
      "Epoch 5587: train loss: 0.1899701952934265, val loss: 0.19512201845645905\n",
      "Epoch 5588: train loss: 0.18983548879623413, val loss: 0.19499576091766357\n",
      "Epoch 5589: train loss: 0.18970096111297607, val loss: 0.19486944377422333\n",
      "Epoch 5590: train loss: 0.1895664632320404, val loss: 0.19474327564239502\n",
      "Epoch 5591: train loss: 0.1894320696592331, val loss: 0.19461719691753387\n",
      "Epoch 5592: train loss: 0.18929775059223175, val loss: 0.1944912225008011\n",
      "Epoch 5593: train loss: 0.18916355073451996, val loss: 0.19436533749103546\n",
      "Epoch 5594: train loss: 0.18902944028377533, val loss: 0.1942395120859146\n",
      "Epoch 5595: train loss: 0.18889537453651428, val loss: 0.19411373138427734\n",
      "Epoch 5596: train loss: 0.1887614130973816, val loss: 0.19398820400238037\n",
      "Epoch 5597: train loss: 0.18862757086753845, val loss: 0.19386272132396698\n",
      "Epoch 5598: train loss: 0.18849381804466248, val loss: 0.19373735785484314\n",
      "Epoch 5599: train loss: 0.1883600950241089, val loss: 0.1936119794845581\n",
      "Epoch 5600: train loss: 0.18822647631168365, val loss: 0.19348669052124023\n",
      "Epoch 5601: train loss: 0.18809300661087036, val loss: 0.1933615803718567\n",
      "Epoch 5602: train loss: 0.18795959651470184, val loss: 0.19323651492595673\n",
      "Epoch 5603: train loss: 0.1878262460231781, val loss: 0.1931116282939911\n",
      "Epoch 5604: train loss: 0.18769307434558868, val loss: 0.1929866522550583\n",
      "Epoch 5605: train loss: 0.18755990266799927, val loss: 0.19286192953586578\n",
      "Epoch 5606: train loss: 0.1874268501996994, val loss: 0.1927371770143509\n",
      "Epoch 5607: train loss: 0.1872938871383667, val loss: 0.1926126778125763\n",
      "Epoch 5608: train loss: 0.18716104328632355, val loss: 0.19248807430267334\n",
      "Epoch 5609: train loss: 0.18702825903892517, val loss: 0.1923637092113495\n",
      "Epoch 5610: train loss: 0.18689554929733276, val loss: 0.19223935902118683\n",
      "Epoch 5611: train loss: 0.18676289916038513, val loss: 0.1921151876449585\n",
      "Epoch 5612: train loss: 0.18663042783737183, val loss: 0.19199101626873016\n",
      "Epoch 5613: train loss: 0.1864980161190033, val loss: 0.19186700880527496\n",
      "Epoch 5614: train loss: 0.18636563420295715, val loss: 0.19174304604530334\n",
      "Epoch 5615: train loss: 0.18623344600200653, val loss: 0.19161920249462128\n",
      "Epoch 5616: train loss: 0.1861012876033783, val loss: 0.19149546325206757\n",
      "Epoch 5617: train loss: 0.18596920371055603, val loss: 0.19137173891067505\n",
      "Epoch 5618: train loss: 0.18583722412586212, val loss: 0.19124813377857208\n",
      "Epoch 5619: train loss: 0.18570536375045776, val loss: 0.19112461805343628\n",
      "Epoch 5620: train loss: 0.18557357788085938, val loss: 0.19100122153759003\n",
      "Epoch 5621: train loss: 0.18544185161590576, val loss: 0.19087791442871094\n",
      "Epoch 5622: train loss: 0.18531019985675812, val loss: 0.1907547414302826\n",
      "Epoch 5623: train loss: 0.1851786971092224, val loss: 0.19063155353069305\n",
      "Epoch 5624: train loss: 0.18504725396633148, val loss: 0.19050849974155426\n",
      "Epoch 5625: train loss: 0.18491587042808533, val loss: 0.19038565456867218\n",
      "Epoch 5626: train loss: 0.1847846657037735, val loss: 0.1902628093957901\n",
      "Epoch 5627: train loss: 0.18465352058410645, val loss: 0.19014009833335876\n",
      "Epoch 5628: train loss: 0.18452240526676178, val loss: 0.19001737236976624\n",
      "Epoch 5629: train loss: 0.18439140915870667, val loss: 0.18989482522010803\n",
      "Epoch 5630: train loss: 0.1842605173587799, val loss: 0.18977227807044983\n",
      "Epoch 5631: train loss: 0.1841297149658203, val loss: 0.18964987993240356\n",
      "Epoch 5632: train loss: 0.1839989572763443, val loss: 0.1895275115966797\n",
      "Epoch 5633: train loss: 0.18386828899383545, val loss: 0.18940532207489014\n",
      "Epoch 5634: train loss: 0.18373778462409973, val loss: 0.18928320705890656\n",
      "Epoch 5635: train loss: 0.1836072951555252, val loss: 0.18916121125221252\n",
      "Epoch 5636: train loss: 0.18347692489624023, val loss: 0.18903933465480804\n",
      "Epoch 5637: train loss: 0.1833466738462448, val loss: 0.18891751766204834\n",
      "Epoch 5638: train loss: 0.18321652710437775, val loss: 0.1887957900762558\n",
      "Epoch 5639: train loss: 0.18308638036251068, val loss: 0.18867407739162445\n",
      "Epoch 5640: train loss: 0.18295632302761078, val loss: 0.18855246901512146\n",
      "Epoch 5641: train loss: 0.1828264445066452, val loss: 0.18843097984790802\n",
      "Epoch 5642: train loss: 0.18269659578800201, val loss: 0.18830959498882294\n",
      "Epoch 5643: train loss: 0.182566836476326, val loss: 0.1881883144378662\n",
      "Epoch 5644: train loss: 0.1824372261762619, val loss: 0.18806712329387665\n",
      "Epoch 5645: train loss: 0.18230769038200378, val loss: 0.18794594705104828\n",
      "Epoch 5646: train loss: 0.18217819929122925, val loss: 0.18782491981983185\n",
      "Epoch 5647: train loss: 0.18204879760742188, val loss: 0.18770404160022736\n",
      "Epoch 5648: train loss: 0.18191953003406525, val loss: 0.18758323788642883\n",
      "Epoch 5649: train loss: 0.1817903071641922, val loss: 0.18746249377727509\n",
      "Epoch 5650: train loss: 0.1816611886024475, val loss: 0.18734176456928253\n",
      "Epoch 5651: train loss: 0.1815321296453476, val loss: 0.1872212439775467\n",
      "Epoch 5652: train loss: 0.18140318989753723, val loss: 0.18710075318813324\n",
      "Epoch 5653: train loss: 0.18127433955669403, val loss: 0.18698035180568695\n",
      "Epoch 5654: train loss: 0.18114551901817322, val loss: 0.186860129237175\n",
      "Epoch 5655: train loss: 0.18101687729358673, val loss: 0.18673984706401825\n",
      "Epoch 5656: train loss: 0.18088828027248383, val loss: 0.18661974370479584\n",
      "Epoch 5657: train loss: 0.18075981736183167, val loss: 0.1864997148513794\n",
      "Epoch 5658: train loss: 0.1806313395500183, val loss: 0.18637989461421967\n",
      "Epoch 5659: train loss: 0.18050307035446167, val loss: 0.18625997006893158\n",
      "Epoch 5660: train loss: 0.18037483096122742, val loss: 0.18614022433757782\n",
      "Epoch 5661: train loss: 0.18024665117263794, val loss: 0.18602047860622406\n",
      "Epoch 5662: train loss: 0.18011851608753204, val loss: 0.1859009563922882\n",
      "Epoch 5663: train loss: 0.17999057471752167, val loss: 0.18578146398067474\n",
      "Epoch 5664: train loss: 0.17986272275447845, val loss: 0.18566207587718964\n",
      "Epoch 5665: train loss: 0.17973487079143524, val loss: 0.18554268777370453\n",
      "Epoch 5666: train loss: 0.17960722744464874, val loss: 0.1854235976934433\n",
      "Epoch 5667: train loss: 0.17947959899902344, val loss: 0.1853044331073761\n",
      "Epoch 5668: train loss: 0.1793520599603653, val loss: 0.1851852983236313\n",
      "Epoch 5669: train loss: 0.17922456562519073, val loss: 0.18506649136543274\n",
      "Epoch 5670: train loss: 0.1790972501039505, val loss: 0.18494759500026703\n",
      "Epoch 5671: train loss: 0.17896997928619385, val loss: 0.18482878804206848\n",
      "Epoch 5672: train loss: 0.17884276807308197, val loss: 0.18471021950244904\n",
      "Epoch 5673: train loss: 0.17871573567390442, val loss: 0.18459157645702362\n",
      "Epoch 5674: train loss: 0.17858874797821045, val loss: 0.18447312712669373\n",
      "Epoch 5675: train loss: 0.17846181988716125, val loss: 0.18435466289520264\n",
      "Epoch 5676: train loss: 0.17833496630191803, val loss: 0.18423645198345184\n",
      "Epoch 5677: train loss: 0.17820824682712555, val loss: 0.18411824107170105\n",
      "Epoch 5678: train loss: 0.17808160185813904, val loss: 0.18400001525878906\n",
      "Epoch 5679: train loss: 0.1779550015926361, val loss: 0.183881938457489\n",
      "Epoch 5680: train loss: 0.17782849073410034, val loss: 0.1837640255689621\n",
      "Epoch 5681: train loss: 0.1777021288871765, val loss: 0.18364618718624115\n",
      "Epoch 5682: train loss: 0.17757581174373627, val loss: 0.18352845311164856\n",
      "Epoch 5683: train loss: 0.177449569106102, val loss: 0.18341071903705597\n",
      "Epoch 5684: train loss: 0.17732349038124084, val loss: 0.1832932084798813\n",
      "Epoch 5685: train loss: 0.1771974414587021, val loss: 0.1831756830215454\n",
      "Epoch 5686: train loss: 0.1770714819431305, val loss: 0.18305829167366028\n",
      "Epoch 5687: train loss: 0.17694556713104248, val loss: 0.1829409897327423\n",
      "Epoch 5688: train loss: 0.17681984603405, val loss: 0.18282374739646912\n",
      "Epoch 5689: train loss: 0.17669415473937988, val loss: 0.18270646035671234\n",
      "Epoch 5690: train loss: 0.17656852304935455, val loss: 0.18258947134017944\n",
      "Epoch 5691: train loss: 0.17644305527210236, val loss: 0.18247255682945251\n",
      "Epoch 5692: train loss: 0.17631764709949493, val loss: 0.18235564231872559\n",
      "Epoch 5693: train loss: 0.1761922836303711, val loss: 0.18223883211612701\n",
      "Epoch 5694: train loss: 0.17606699466705322, val loss: 0.18212220072746277\n",
      "Epoch 5695: train loss: 0.1759418547153473, val loss: 0.1820056289434433\n",
      "Epoch 5696: train loss: 0.17581681907176971, val loss: 0.18188901245594025\n",
      "Epoch 5697: train loss: 0.17569178342819214, val loss: 0.18177269399166107\n",
      "Epoch 5698: train loss: 0.1755669265985489, val loss: 0.1816563457250595\n",
      "Epoch 5699: train loss: 0.17544209957122803, val loss: 0.18154017627239227\n",
      "Epoch 5700: train loss: 0.17531739175319672, val loss: 0.18142393231391907\n",
      "Epoch 5701: train loss: 0.1751927137374878, val loss: 0.18130789697170258\n",
      "Epoch 5702: train loss: 0.175068199634552, val loss: 0.18119202554225922\n",
      "Epoch 5703: train loss: 0.1749437302350998, val loss: 0.18107599020004272\n",
      "Epoch 5704: train loss: 0.17481933534145355, val loss: 0.1809602528810501\n",
      "Epoch 5705: train loss: 0.17469510436058044, val loss: 0.18084466457366943\n",
      "Epoch 5706: train loss: 0.17457090318202972, val loss: 0.18072901666164398\n",
      "Epoch 5707: train loss: 0.17444679141044617, val loss: 0.18061338365077972\n",
      "Epoch 5708: train loss: 0.17432275414466858, val loss: 0.18049800395965576\n",
      "Epoch 5709: train loss: 0.17419880628585815, val loss: 0.1803826093673706\n",
      "Epoch 5710: train loss: 0.17407497763633728, val loss: 0.18026737868785858\n",
      "Epoch 5711: train loss: 0.1739511787891388, val loss: 0.18015213310718536\n",
      "Epoch 5712: train loss: 0.17382746934890747, val loss: 0.18003715574741364\n",
      "Epoch 5713: train loss: 0.1737038791179657, val loss: 0.17992211878299713\n",
      "Epoch 5714: train loss: 0.17358040809631348, val loss: 0.1798071712255478\n",
      "Epoch 5715: train loss: 0.17345695197582245, val loss: 0.179692342877388\n",
      "Epoch 5716: train loss: 0.17333364486694336, val loss: 0.17957761883735657\n",
      "Epoch 5717: train loss: 0.17321041226387024, val loss: 0.1794629693031311\n",
      "Epoch 5718: train loss: 0.1730872392654419, val loss: 0.1793484389781952\n",
      "Epoch 5719: train loss: 0.17296412587165833, val loss: 0.17923402786254883\n",
      "Epoch 5720: train loss: 0.1728411763906479, val loss: 0.17911960184574127\n",
      "Epoch 5721: train loss: 0.17271824181079865, val loss: 0.17900530993938446\n",
      "Epoch 5722: train loss: 0.17259542644023895, val loss: 0.17889109253883362\n",
      "Epoch 5723: train loss: 0.1724727302789688, val loss: 0.1787770837545395\n",
      "Epoch 5724: train loss: 0.17235010862350464, val loss: 0.1786630004644394\n",
      "Epoch 5725: train loss: 0.17222754657268524, val loss: 0.17854903638362885\n",
      "Epoch 5726: train loss: 0.17210505902767181, val loss: 0.17843519151210785\n",
      "Epoch 5727: train loss: 0.17198269069194794, val loss: 0.17832152545452118\n",
      "Epoch 5728: train loss: 0.17186039686203003, val loss: 0.17820778489112854\n",
      "Epoch 5729: train loss: 0.1717381775379181, val loss: 0.1780943125486374\n",
      "Epoch 5730: train loss: 0.1716160774230957, val loss: 0.17798085510730743\n",
      "Epoch 5731: train loss: 0.1714940369129181, val loss: 0.1778673380613327\n",
      "Epoch 5732: train loss: 0.17137208580970764, val loss: 0.1777539998292923\n",
      "Epoch 5733: train loss: 0.17125019431114197, val loss: 0.17764079570770264\n",
      "Epoch 5734: train loss: 0.17112845182418823, val loss: 0.17752763628959656\n",
      "Epoch 5735: train loss: 0.17100675404071808, val loss: 0.1774146556854248\n",
      "Epoch 5736: train loss: 0.1708851456642151, val loss: 0.17730174958705902\n",
      "Epoch 5737: train loss: 0.17076365649700165, val loss: 0.17718885838985443\n",
      "Epoch 5738: train loss: 0.17064224183559418, val loss: 0.17707610130310059\n",
      "Epoch 5739: train loss: 0.1705208718776703, val loss: 0.17696337401866913\n",
      "Epoch 5740: train loss: 0.17039959132671356, val loss: 0.17685075104236603\n",
      "Epoch 5741: train loss: 0.17027844488620758, val loss: 0.17673826217651367\n",
      "Epoch 5742: train loss: 0.17015735805034637, val loss: 0.1766258329153061\n",
      "Epoch 5743: train loss: 0.17003634572029114, val loss: 0.17651353776454926\n",
      "Epoch 5744: train loss: 0.16991545259952545, val loss: 0.1764012724161148\n",
      "Epoch 5745: train loss: 0.16979464888572693, val loss: 0.17628911137580872\n",
      "Epoch 5746: train loss: 0.16967391967773438, val loss: 0.17617692053318024\n",
      "Epoch 5747: train loss: 0.16955320537090302, val loss: 0.17606496810913086\n",
      "Epoch 5748: train loss: 0.16943268477916718, val loss: 0.17595313489437103\n",
      "Epoch 5749: train loss: 0.16931217908859253, val loss: 0.17584128677845\n",
      "Epoch 5750: train loss: 0.16919176280498505, val loss: 0.1757296323776245\n",
      "Epoch 5751: train loss: 0.1690714955329895, val loss: 0.17561796307563782\n",
      "Epoch 5752: train loss: 0.16895128786563873, val loss: 0.17550645768642426\n",
      "Epoch 5753: train loss: 0.16883115470409393, val loss: 0.1753949671983719\n",
      "Epoch 5754: train loss: 0.16871105134487152, val loss: 0.1752835363149643\n",
      "Epoch 5755: train loss: 0.16859109699726105, val loss: 0.1751723289489746\n",
      "Epoch 5756: train loss: 0.16847121715545654, val loss: 0.1750611811876297\n",
      "Epoch 5757: train loss: 0.16835139691829681, val loss: 0.17495004832744598\n",
      "Epoch 5758: train loss: 0.16823174059391022, val loss: 0.17483898997306824\n",
      "Epoch 5759: train loss: 0.1681121289730072, val loss: 0.17472802102565765\n",
      "Epoch 5760: train loss: 0.16799257695674896, val loss: 0.174617201089859\n",
      "Epoch 5761: train loss: 0.1678730696439743, val loss: 0.17450641095638275\n",
      "Epoch 5762: train loss: 0.16775372624397278, val loss: 0.17439579963684082\n",
      "Epoch 5763: train loss: 0.16763444244861603, val loss: 0.17428520321846008\n",
      "Epoch 5764: train loss: 0.16751523315906525, val loss: 0.17417478561401367\n",
      "Epoch 5765: train loss: 0.16739614307880402, val loss: 0.17406436800956726\n",
      "Epoch 5766: train loss: 0.16727714240550995, val loss: 0.17395401000976562\n",
      "Epoch 5767: train loss: 0.16715820133686066, val loss: 0.17384369671344757\n",
      "Epoch 5768: train loss: 0.16703927516937256, val loss: 0.1737336367368698\n",
      "Epoch 5769: train loss: 0.16692054271697998, val loss: 0.17362353205680847\n",
      "Epoch 5770: train loss: 0.16680186986923218, val loss: 0.17351357638835907\n",
      "Epoch 5771: train loss: 0.16668321192264557, val loss: 0.17340373992919922\n",
      "Epoch 5772: train loss: 0.16656474769115448, val loss: 0.17329393327236176\n",
      "Epoch 5773: train loss: 0.16644634306430817, val loss: 0.17318418622016907\n",
      "Epoch 5774: train loss: 0.16632798314094543, val loss: 0.17307452857494354\n",
      "Epoch 5775: train loss: 0.16620968282222748, val loss: 0.17296503484249115\n",
      "Epoch 5776: train loss: 0.16609151661396027, val loss: 0.17285564541816711\n",
      "Epoch 5777: train loss: 0.16597343981266022, val loss: 0.1727462112903595\n",
      "Epoch 5778: train loss: 0.16585537791252136, val loss: 0.172636941075325\n",
      "Epoch 5779: train loss: 0.16573750972747803, val loss: 0.1725277602672577\n",
      "Epoch 5780: train loss: 0.16561970114707947, val loss: 0.1724187135696411\n",
      "Epoch 5781: train loss: 0.1655019223690033, val loss: 0.17230962216854095\n",
      "Epoch 5782: train loss: 0.1653842180967331, val loss: 0.17220067977905273\n",
      "Epoch 5783: train loss: 0.16526666283607483, val loss: 0.17209190130233765\n",
      "Epoch 5784: train loss: 0.16514918208122253, val loss: 0.17198313772678375\n",
      "Epoch 5785: train loss: 0.16503173112869263, val loss: 0.17187447845935822\n",
      "Epoch 5786: train loss: 0.16491442918777466, val loss: 0.17176592350006104\n",
      "Epoch 5787: train loss: 0.16479723155498505, val loss: 0.17165736854076385\n",
      "Epoch 5788: train loss: 0.16468003392219543, val loss: 0.17154908180236816\n",
      "Epoch 5789: train loss: 0.16456298530101776, val loss: 0.17144076526165009\n",
      "Epoch 5790: train loss: 0.16444604098796844, val loss: 0.17133250832557678\n",
      "Epoch 5791: train loss: 0.16432912647724152, val loss: 0.17122428119182587\n",
      "Epoch 5792: train loss: 0.16421230137348175, val loss: 0.17111627757549286\n",
      "Epoch 5793: train loss: 0.16409558057785034, val loss: 0.17100834846496582\n",
      "Epoch 5794: train loss: 0.1639789640903473, val loss: 0.1709003895521164\n",
      "Epoch 5795: train loss: 0.16386233270168304, val loss: 0.17079268395900726\n",
      "Epoch 5796: train loss: 0.1637459099292755, val loss: 0.17068496346473694\n",
      "Epoch 5797: train loss: 0.16362956166267395, val loss: 0.17057733237743378\n",
      "Epoch 5798: train loss: 0.16351325809955597, val loss: 0.17046982049942017\n",
      "Epoch 5799: train loss: 0.16339699923992157, val loss: 0.17036230862140656\n",
      "Epoch 5800: train loss: 0.1632808893918991, val loss: 0.17025499045848846\n",
      "Epoch 5801: train loss: 0.16316482424736023, val loss: 0.17014770209789276\n",
      "Epoch 5802: train loss: 0.16304883360862732, val loss: 0.17004048824310303\n",
      "Epoch 5803: train loss: 0.16293297708034515, val loss: 0.16993343830108643\n",
      "Epoch 5804: train loss: 0.16281718015670776, val loss: 0.16982634365558624\n",
      "Epoch 5805: train loss: 0.16270145773887634, val loss: 0.16971942782402039\n",
      "Epoch 5806: train loss: 0.1625857651233673, val loss: 0.1696125566959381\n",
      "Epoch 5807: train loss: 0.1624702364206314, val loss: 0.16950590908527374\n",
      "Epoch 5808: train loss: 0.16235478222370148, val loss: 0.16939912736415863\n",
      "Epoch 5809: train loss: 0.16223940253257751, val loss: 0.16929256916046143\n",
      "Epoch 5810: train loss: 0.1621241271495819, val loss: 0.16918614506721497\n",
      "Epoch 5811: train loss: 0.16200894117355347, val loss: 0.16907969117164612\n",
      "Epoch 5812: train loss: 0.16189377009868622, val loss: 0.16897331178188324\n",
      "Epoch 5813: train loss: 0.16177870333194733, val loss: 0.1688670516014099\n",
      "Epoch 5814: train loss: 0.1616637408733368, val loss: 0.16876091063022614\n",
      "Epoch 5815: train loss: 0.1615488976240158, val loss: 0.16865482926368713\n",
      "Epoch 5816: train loss: 0.16143405437469482, val loss: 0.1685488224029541\n",
      "Epoch 5817: train loss: 0.16131936013698578, val loss: 0.16844291985034943\n",
      "Epoch 5818: train loss: 0.1612047553062439, val loss: 0.16833710670471191\n",
      "Epoch 5819: train loss: 0.1610901951789856, val loss: 0.16823135316371918\n",
      "Epoch 5820: train loss: 0.16097578406333923, val loss: 0.16812576353549957\n",
      "Epoch 5821: train loss: 0.16086141765117645, val loss: 0.16802020370960236\n",
      "Epoch 5822: train loss: 0.16074712574481964, val loss: 0.16791470348834991\n",
      "Epoch 5823: train loss: 0.16063286364078522, val loss: 0.1678094118833542\n",
      "Epoch 5824: train loss: 0.1605188250541687, val loss: 0.16770407557487488\n",
      "Epoch 5825: train loss: 0.1604047566652298, val loss: 0.16759884357452393\n",
      "Epoch 5826: train loss: 0.16029077768325806, val loss: 0.16749368607997894\n",
      "Epoch 5827: train loss: 0.16017694771289825, val loss: 0.1673886626958847\n",
      "Epoch 5828: train loss: 0.16006317734718323, val loss: 0.16728374361991882\n",
      "Epoch 5829: train loss: 0.15994945168495178, val loss: 0.16717879474163055\n",
      "Epoch 5830: train loss: 0.1598358005285263, val loss: 0.16707400977611542\n",
      "Epoch 5831: train loss: 0.15972228348255157, val loss: 0.16696931421756744\n",
      "Epoch 5832: train loss: 0.1596088409423828, val loss: 0.16686467826366425\n",
      "Epoch 5833: train loss: 0.15949544310569763, val loss: 0.16676020622253418\n",
      "Epoch 5834: train loss: 0.1593821942806244, val loss: 0.16665570437908173\n",
      "Epoch 5835: train loss: 0.15926899015903473, val loss: 0.1665513664484024\n",
      "Epoch 5836: train loss: 0.15915587544441223, val loss: 0.16644708812236786\n",
      "Epoch 5837: train loss: 0.1590428650379181, val loss: 0.16634295880794525\n",
      "Epoch 5838: train loss: 0.15892992913722992, val loss: 0.16623876988887787\n",
      "Epoch 5839: train loss: 0.15881703794002533, val loss: 0.1661347597837448\n",
      "Epoch 5840: train loss: 0.1587042510509491, val loss: 0.1660308688879013\n",
      "Epoch 5841: train loss: 0.15859155356884003, val loss: 0.16592703759670258\n",
      "Epoch 5842: train loss: 0.15847894549369812, val loss: 0.16582325100898743\n",
      "Epoch 5843: train loss: 0.1583663821220398, val loss: 0.16571952402591705\n",
      "Epoch 5844: train loss: 0.1582539677619934, val loss: 0.1656159609556198\n",
      "Epoch 5845: train loss: 0.1581416130065918, val loss: 0.16551244258880615\n",
      "Epoch 5846: train loss: 0.15802931785583496, val loss: 0.16540908813476562\n",
      "Epoch 5847: train loss: 0.15791712701320648, val loss: 0.1653057187795639\n",
      "Epoch 5848: train loss: 0.15780505537986755, val loss: 0.16520249843597412\n",
      "Epoch 5849: train loss: 0.157693013548851, val loss: 0.16509930789470673\n",
      "Epoch 5850: train loss: 0.15758101642131805, val loss: 0.16499625146389008\n",
      "Epoch 5851: train loss: 0.15746919810771942, val loss: 0.1648932546377182\n",
      "Epoch 5852: train loss: 0.15735740959644318, val loss: 0.16479039192199707\n",
      "Epoch 5853: train loss: 0.1572457104921341, val loss: 0.16468754410743713\n",
      "Epoch 5854: train loss: 0.15713410079479218, val loss: 0.1645849049091339\n",
      "Epoch 5855: train loss: 0.15702258050441742, val loss: 0.16448208689689636\n",
      "Epoch 5856: train loss: 0.15691110491752625, val loss: 0.16437946259975433\n",
      "Epoch 5857: train loss: 0.15679971873760223, val loss: 0.16427703201770782\n",
      "Epoch 5858: train loss: 0.15668845176696777, val loss: 0.1641746610403061\n",
      "Epoch 5859: train loss: 0.1565772294998169, val loss: 0.16407226026058197\n",
      "Epoch 5860: train loss: 0.15646608173847198, val loss: 0.16397006809711456\n",
      "Epoch 5861: train loss: 0.156355082988739, val loss: 0.16386793553829193\n",
      "Epoch 5862: train loss: 0.15624412894248962, val loss: 0.1637657880783081\n",
      "Epoch 5863: train loss: 0.1561332494020462, val loss: 0.16366396844387054\n",
      "Epoch 5864: train loss: 0.15602250397205353, val loss: 0.16356199979782104\n",
      "Epoch 5865: train loss: 0.15591178834438324, val loss: 0.16346020996570587\n",
      "Epoch 5866: train loss: 0.15580116212368011, val loss: 0.16335844993591309\n",
      "Epoch 5867: train loss: 0.15569056570529938, val loss: 0.16325683891773224\n",
      "Epoch 5868: train loss: 0.15558013319969177, val loss: 0.16315524280071259\n",
      "Epoch 5869: train loss: 0.15546974539756775, val loss: 0.1630537360906601\n",
      "Epoch 5870: train loss: 0.1553594470024109, val loss: 0.16295234858989716\n",
      "Epoch 5871: train loss: 0.15524922311306, val loss: 0.162851020693779\n",
      "Epoch 5872: train loss: 0.15513914823532104, val loss: 0.16274990141391754\n",
      "Epoch 5873: train loss: 0.1550290584564209, val loss: 0.16264872252941132\n",
      "Epoch 5874: train loss: 0.1549191176891327, val loss: 0.16254772245883942\n",
      "Epoch 5875: train loss: 0.15480925142765045, val loss: 0.16244672238826752\n",
      "Epoch 5876: train loss: 0.15469945967197418, val loss: 0.16234588623046875\n",
      "Epoch 5877: train loss: 0.1545897126197815, val loss: 0.1622450202703476\n",
      "Epoch 5878: train loss: 0.15448008477687836, val loss: 0.16214434802532196\n",
      "Epoch 5879: train loss: 0.15437054634094238, val loss: 0.16204361617565155\n",
      "Epoch 5880: train loss: 0.1542610377073288, val loss: 0.16194307804107666\n",
      "Epoch 5881: train loss: 0.15415170788764954, val loss: 0.16184261441230774\n",
      "Epoch 5882: train loss: 0.15404239296913147, val loss: 0.16174225509166718\n",
      "Epoch 5883: train loss: 0.15393315255641937, val loss: 0.16164205968379974\n",
      "Epoch 5884: train loss: 0.15382404625415802, val loss: 0.16154177486896515\n",
      "Epoch 5885: train loss: 0.15371501445770264, val loss: 0.1614416390657425\n",
      "Epoch 5886: train loss: 0.15360599756240845, val loss: 0.1613415628671646\n",
      "Epoch 5887: train loss: 0.15349707007408142, val loss: 0.16124169528484344\n",
      "Epoch 5888: train loss: 0.15338827669620514, val loss: 0.1611417680978775\n",
      "Epoch 5889: train loss: 0.15327954292297363, val loss: 0.1610419601202011\n",
      "Epoch 5890: train loss: 0.1531708687543869, val loss: 0.16094224154949188\n",
      "Epoch 5891: train loss: 0.1530623584985733, val loss: 0.1608426421880722\n",
      "Epoch 5892: train loss: 0.1529538482427597, val loss: 0.16074307262897491\n",
      "Epoch 5893: train loss: 0.15284542739391327, val loss: 0.1606435775756836\n",
      "Epoch 5894: train loss: 0.15273714065551758, val loss: 0.1605442613363266\n",
      "Epoch 5895: train loss: 0.15262889862060547, val loss: 0.1604449301958084\n",
      "Epoch 5896: train loss: 0.15252076089382172, val loss: 0.16034571826457977\n",
      "Epoch 5897: train loss: 0.15241262316703796, val loss: 0.1602465957403183\n",
      "Epoch 5898: train loss: 0.15230464935302734, val loss: 0.16014748811721802\n",
      "Epoch 5899: train loss: 0.1521967351436615, val loss: 0.16004858911037445\n",
      "Epoch 5900: train loss: 0.15208888053894043, val loss: 0.15994970500469208\n",
      "Epoch 5901: train loss: 0.1519811600446701, val loss: 0.15985092520713806\n",
      "Epoch 5902: train loss: 0.15187348425388336, val loss: 0.15975217521190643\n",
      "Epoch 5903: train loss: 0.1517658829689026, val loss: 0.15965358912944794\n",
      "Epoch 5904: train loss: 0.15165841579437256, val loss: 0.15955501794815063\n",
      "Epoch 5905: train loss: 0.1515509933233261, val loss: 0.15945661067962646\n",
      "Epoch 5906: train loss: 0.15144364535808563, val loss: 0.15935812890529633\n",
      "Epoch 5907: train loss: 0.15133632719516754, val loss: 0.15925994515419006\n",
      "Epoch 5908: train loss: 0.1512291580438614, val loss: 0.15916167199611664\n",
      "Epoch 5909: train loss: 0.1511220782995224, val loss: 0.15906357765197754\n",
      "Epoch 5910: train loss: 0.15101505815982819, val loss: 0.15896549820899963\n",
      "Epoch 5911: train loss: 0.15090811252593994, val loss: 0.15886759757995605\n",
      "Epoch 5912: train loss: 0.15080125629901886, val loss: 0.15876969695091248\n",
      "Epoch 5913: train loss: 0.15069447457790375, val loss: 0.15867188572883606\n",
      "Epoch 5914: train loss: 0.15058781206607819, val loss: 0.15857426822185516\n",
      "Epoch 5915: train loss: 0.1504812240600586, val loss: 0.15847653150558472\n",
      "Epoch 5916: train loss: 0.1503746658563614, val loss: 0.1583789885044098\n",
      "Epoch 5917: train loss: 0.15026818215847015, val loss: 0.15828156471252441\n",
      "Epoch 5918: train loss: 0.15016183257102966, val loss: 0.15818408131599426\n",
      "Epoch 5919: train loss: 0.15005554258823395, val loss: 0.1580868363380432\n",
      "Epoch 5920: train loss: 0.149949312210083, val loss: 0.15798959136009216\n",
      "Epoch 5921: train loss: 0.14984320104122162, val loss: 0.15789246559143066\n",
      "Epoch 5922: train loss: 0.1497371345758438, val loss: 0.15779542922973633\n",
      "Epoch 5923: train loss: 0.14963117241859436, val loss: 0.15769846737384796\n",
      "Epoch 5924: train loss: 0.14952531456947327, val loss: 0.15760163962841034\n",
      "Epoch 5925: train loss: 0.14941951632499695, val loss: 0.15750478208065033\n",
      "Epoch 5926: train loss: 0.1493137776851654, val loss: 0.15740810334682465\n",
      "Epoch 5927: train loss: 0.1492081731557846, val loss: 0.1573113650083542\n",
      "Epoch 5928: train loss: 0.1491026133298874, val loss: 0.15721489489078522\n",
      "Epoch 5929: train loss: 0.14899718761444092, val loss: 0.15711839497089386\n",
      "Epoch 5930: train loss: 0.14889171719551086, val loss: 0.15702198445796967\n",
      "Epoch 5931: train loss: 0.14878639578819275, val loss: 0.1569257527589798\n",
      "Epoch 5932: train loss: 0.14868119359016418, val loss: 0.15682946145534515\n",
      "Epoch 5933: train loss: 0.14857599139213562, val loss: 0.15673328936100006\n",
      "Epoch 5934: train loss: 0.1484709531068802, val loss: 0.1566372960805893\n",
      "Epoch 5935: train loss: 0.14836597442626953, val loss: 0.15654130280017853\n",
      "Epoch 5936: train loss: 0.14826102554798126, val loss: 0.1564454287290573\n",
      "Epoch 5937: train loss: 0.14815622568130493, val loss: 0.15634958446025848\n",
      "Epoch 5938: train loss: 0.14805147051811218, val loss: 0.15625379979610443\n",
      "Epoch 5939: train loss: 0.1479467898607254, val loss: 0.15615816414356232\n",
      "Epoch 5940: train loss: 0.1478421837091446, val loss: 0.15606258809566498\n",
      "Epoch 5941: train loss: 0.14773765206336975, val loss: 0.15596714615821838\n",
      "Epoch 5942: train loss: 0.14763320982456207, val loss: 0.1558716744184494\n",
      "Epoch 5943: train loss: 0.14752885699272156, val loss: 0.15577642619609833\n",
      "Epoch 5944: train loss: 0.1474245935678482, val loss: 0.15568117797374725\n",
      "Epoch 5945: train loss: 0.14732040464878082, val loss: 0.15558604896068573\n",
      "Epoch 5946: train loss: 0.1472162902355194, val loss: 0.15549099445343018\n",
      "Epoch 5947: train loss: 0.14711228013038635, val loss: 0.15539591014385223\n",
      "Epoch 5948: train loss: 0.14700831472873688, val loss: 0.1553010195493698\n",
      "Epoch 5949: train loss: 0.14690442383289337, val loss: 0.1552061289548874\n",
      "Epoch 5950: train loss: 0.146800696849823, val loss: 0.1551114320755005\n",
      "Epoch 5951: train loss: 0.146696999669075, val loss: 0.15501676499843597\n",
      "Epoch 5952: train loss: 0.1465933471918106, val loss: 0.15492214262485504\n",
      "Epoch 5953: train loss: 0.14648975431919098, val loss: 0.15482769906520844\n",
      "Epoch 5954: train loss: 0.14638632535934448, val loss: 0.15473321080207825\n",
      "Epoch 5955: train loss: 0.14628292620182037, val loss: 0.15463896095752716\n",
      "Epoch 5956: train loss: 0.14617958664894104, val loss: 0.1545446217060089\n",
      "Epoch 5957: train loss: 0.14607638120651245, val loss: 0.154450461268425\n",
      "Epoch 5958: train loss: 0.14597323536872864, val loss: 0.15435634553432465\n",
      "Epoch 5959: train loss: 0.1458701342344284, val loss: 0.15426237881183624\n",
      "Epoch 5960: train loss: 0.1457671821117401, val loss: 0.15416839718818665\n",
      "Epoch 5961: train loss: 0.1456642895936966, val loss: 0.15407446026802063\n",
      "Epoch 5962: train loss: 0.14556141197681427, val loss: 0.15398073196411133\n",
      "Epoch 5963: train loss: 0.14545871317386627, val loss: 0.153887078166008\n",
      "Epoch 5964: train loss: 0.14535605907440186, val loss: 0.15379349887371063\n",
      "Epoch 5965: train loss: 0.14525346457958221, val loss: 0.15369994938373566\n",
      "Epoch 5966: train loss: 0.14515089988708496, val loss: 0.15360651910305023\n",
      "Epoch 5967: train loss: 0.14504849910736084, val loss: 0.1535130888223648\n",
      "Epoch 5968: train loss: 0.1449461281299591, val loss: 0.15341974794864655\n",
      "Epoch 5969: train loss: 0.14484384655952454, val loss: 0.15332667529582977\n",
      "Epoch 5970: train loss: 0.14474165439605713, val loss: 0.15323346853256226\n",
      "Epoch 5971: train loss: 0.14463955163955688, val loss: 0.15314039587974548\n",
      "Epoch 5972: train loss: 0.14453747868537903, val loss: 0.15304754674434662\n",
      "Epoch 5973: train loss: 0.1444355696439743, val loss: 0.152954563498497\n",
      "Epoch 5974: train loss: 0.14433369040489197, val loss: 0.1528617888689041\n",
      "Epoch 5975: train loss: 0.1442318707704544, val loss: 0.152769073843956\n",
      "Epoch 5976: train loss: 0.14413020014762878, val loss: 0.15267649292945862\n",
      "Epoch 5977: train loss: 0.14402857422828674, val loss: 0.1525839865207672\n",
      "Epoch 5978: train loss: 0.14392700791358948, val loss: 0.1524914801120758\n",
      "Epoch 5979: train loss: 0.14382556080818176, val loss: 0.15239916741847992\n",
      "Epoch 5980: train loss: 0.14372417330741882, val loss: 0.15230681002140045\n",
      "Epoch 5981: train loss: 0.14362287521362305, val loss: 0.15221451222896576\n",
      "Epoch 5982: train loss: 0.14352159202098846, val loss: 0.1521223932504654\n",
      "Epoch 5983: train loss: 0.14342044293880463, val loss: 0.15203030407428741\n",
      "Epoch 5984: train loss: 0.14331936836242676, val loss: 0.151938334107399\n",
      "Epoch 5985: train loss: 0.14321832358837128, val loss: 0.15184639394283295\n",
      "Epoch 5986: train loss: 0.14311742782592773, val loss: 0.15175460278987885\n",
      "Epoch 5987: train loss: 0.14301659166812897, val loss: 0.15166285634040833\n",
      "Epoch 5988: train loss: 0.14291581511497498, val loss: 0.15157122910022736\n",
      "Epoch 5989: train loss: 0.14281512796878815, val loss: 0.15147963166236877\n",
      "Epoch 5990: train loss: 0.14271456003189087, val loss: 0.15138810873031616\n",
      "Epoch 5991: train loss: 0.14261400699615479, val loss: 0.1512966901063919\n",
      "Epoch 5992: train loss: 0.14251360297203064, val loss: 0.1512053906917572\n",
      "Epoch 5993: train loss: 0.14241322875022888, val loss: 0.1511140912771225\n",
      "Epoch 5994: train loss: 0.1423129290342331, val loss: 0.15102286636829376\n",
      "Epoch 5995: train loss: 0.14221270382404327, val loss: 0.15093176066875458\n",
      "Epoch 5996: train loss: 0.1421125829219818, val loss: 0.15084078907966614\n",
      "Epoch 5997: train loss: 0.14201250672340393, val loss: 0.15074972808361053\n",
      "Epoch 5998: train loss: 0.1419125199317932, val loss: 0.1506589651107788\n",
      "Epoch 5999: train loss: 0.14181263744831085, val loss: 0.15056820213794708\n",
      "Epoch 6000: train loss: 0.14171279966831207, val loss: 0.15047743916511536\n",
      "Epoch 6001: train loss: 0.14161303639411926, val loss: 0.15038682520389557\n",
      "Epoch 6002: train loss: 0.1415134072303772, val loss: 0.15029628574848175\n",
      "Epoch 6003: train loss: 0.1414138227701187, val loss: 0.1502058058977127\n",
      "Epoch 6004: train loss: 0.141314297914505, val loss: 0.15011535584926605\n",
      "Epoch 6005: train loss: 0.14121489226818085, val loss: 0.15002509951591492\n",
      "Epoch 6006: train loss: 0.14111554622650146, val loss: 0.14993491768836975\n",
      "Epoch 6007: train loss: 0.14101627469062805, val loss: 0.14984479546546936\n",
      "Epoch 6008: train loss: 0.14091713726520538, val loss: 0.14975471794605255\n",
      "Epoch 6009: train loss: 0.1408180296421051, val loss: 0.14966478943824768\n",
      "Epoch 6010: train loss: 0.1407189965248108, val loss: 0.1495748609304428\n",
      "Epoch 6011: train loss: 0.14062000811100006, val loss: 0.14948499202728271\n",
      "Epoch 6012: train loss: 0.14052113890647888, val loss: 0.14939533174037933\n",
      "Epoch 6013: train loss: 0.14042231440544128, val loss: 0.14930562674999237\n",
      "Epoch 6014: train loss: 0.14032359421253204, val loss: 0.14921604096889496\n",
      "Epoch 6015: train loss: 0.14022494852542877, val loss: 0.14912647008895874\n",
      "Epoch 6016: train loss: 0.14012637734413147, val loss: 0.14903710782527924\n",
      "Epoch 6017: train loss: 0.14002788066864014, val loss: 0.14894770085811615\n",
      "Epoch 6018: train loss: 0.13992947340011597, val loss: 0.1488584578037262\n",
      "Epoch 6019: train loss: 0.13983117043972015, val loss: 0.14876924455165863\n",
      "Epoch 6020: train loss: 0.13973289728164673, val loss: 0.14868023991584778\n",
      "Epoch 6021: train loss: 0.13963474333286285, val loss: 0.14859120547771454\n",
      "Epoch 6022: train loss: 0.13953666388988495, val loss: 0.14850223064422607\n",
      "Epoch 6023: train loss: 0.13943861424922943, val loss: 0.14841340482234955\n",
      "Epoch 6024: train loss: 0.13934072852134705, val loss: 0.14832453429698944\n",
      "Epoch 6025: train loss: 0.13924287259578705, val loss: 0.14823590219020844\n",
      "Epoch 6026: train loss: 0.13914504647254944, val loss: 0.14814725518226624\n",
      "Epoch 6027: train loss: 0.13904739916324615, val loss: 0.1480587273836136\n",
      "Epoch 6028: train loss: 0.13894979655742645, val loss: 0.14797024428844452\n",
      "Epoch 6029: train loss: 0.13885222375392914, val loss: 0.14788179099559784\n",
      "Epoch 6030: train loss: 0.1387547105550766, val loss: 0.14779356122016907\n",
      "Epoch 6031: train loss: 0.1386573612689972, val loss: 0.14770527184009552\n",
      "Epoch 6032: train loss: 0.13856002688407898, val loss: 0.1476171314716339\n",
      "Epoch 6033: train loss: 0.13846276700496674, val loss: 0.14752905070781708\n",
      "Epoch 6034: train loss: 0.13836562633514404, val loss: 0.14744101464748383\n",
      "Epoch 6035: train loss: 0.13826853036880493, val loss: 0.14735311269760132\n",
      "Epoch 6036: train loss: 0.13817152380943298, val loss: 0.14726530015468597\n",
      "Epoch 6037: train loss: 0.138074591755867, val loss: 0.1471775621175766\n",
      "Epoch 6038: train loss: 0.137977734208107, val loss: 0.1470898687839508\n",
      "Epoch 6039: train loss: 0.13788096606731415, val loss: 0.14700229465961456\n",
      "Epoch 6040: train loss: 0.13778430223464966, val loss: 0.1469147503376007\n",
      "Epoch 6041: train loss: 0.13768766820430756, val loss: 0.1468273252248764\n",
      "Epoch 6042: train loss: 0.13759110867977142, val loss: 0.14673997461795807\n",
      "Epoch 6043: train loss: 0.13749468326568604, val loss: 0.1466526985168457\n",
      "Epoch 6044: train loss: 0.13739830255508423, val loss: 0.14656546711921692\n",
      "Epoch 6045: train loss: 0.1373019963502884, val loss: 0.14647841453552246\n",
      "Epoch 6046: train loss: 0.13720577955245972, val loss: 0.14639133214950562\n",
      "Epoch 6047: train loss: 0.137109637260437, val loss: 0.14630435407161713\n",
      "Epoch 6048: train loss: 0.13701355457305908, val loss: 0.14621739089488983\n",
      "Epoch 6049: train loss: 0.13691751658916473, val loss: 0.14613059163093567\n",
      "Epoch 6050: train loss: 0.13682162761688232, val loss: 0.14604388177394867\n",
      "Epoch 6051: train loss: 0.1367257684469223, val loss: 0.14595723152160645\n",
      "Epoch 6052: train loss: 0.13662998378276825, val loss: 0.1458706110715866\n",
      "Epoch 6053: train loss: 0.13653428852558136, val loss: 0.1457841396331787\n",
      "Epoch 6054: train loss: 0.13643866777420044, val loss: 0.1456977128982544\n",
      "Epoch 6055: train loss: 0.1363431066274643, val loss: 0.14561139047145844\n",
      "Epoch 6056: train loss: 0.1362476795911789, val loss: 0.14552508294582367\n",
      "Epoch 6057: train loss: 0.13615228235721588, val loss: 0.14543889462947845\n",
      "Epoch 6058: train loss: 0.13605695962905884, val loss: 0.14535287022590637\n",
      "Epoch 6059: train loss: 0.13596175611019135, val loss: 0.1452668309211731\n",
      "Epoch 6060: train loss: 0.13586659729480743, val loss: 0.1451808363199234\n",
      "Epoch 6061: train loss: 0.1357714980840683, val loss: 0.14509497582912445\n",
      "Epoch 6062: train loss: 0.1356765329837799, val loss: 0.14500920474529266\n",
      "Epoch 6063: train loss: 0.1355816274881363, val loss: 0.14492347836494446\n",
      "Epoch 6064: train loss: 0.13548678159713745, val loss: 0.14483784139156342\n",
      "Epoch 6065: train loss: 0.13539202511310577, val loss: 0.14475230872631073\n",
      "Epoch 6066: train loss: 0.13529732823371887, val loss: 0.14466683566570282\n",
      "Epoch 6067: train loss: 0.13520270586013794, val loss: 0.1445813924074173\n",
      "Epoch 6068: train loss: 0.13510820269584656, val loss: 0.14449606835842133\n",
      "Epoch 6069: train loss: 0.13501375913619995, val loss: 0.14441080391407013\n",
      "Epoch 6070: train loss: 0.13491936028003693, val loss: 0.14432571828365326\n",
      "Epoch 6071: train loss: 0.13482508063316345, val loss: 0.1442405879497528\n",
      "Epoch 6072: train loss: 0.13473089039325714, val loss: 0.1441555768251419\n",
      "Epoch 6073: train loss: 0.13463668525218964, val loss: 0.1440706104040146\n",
      "Epoch 6074: train loss: 0.13454259932041168, val loss: 0.1439858078956604\n",
      "Epoch 6075: train loss: 0.1344486027956009, val loss: 0.1439010351896286\n",
      "Epoch 6076: train loss: 0.13435469567775726, val loss: 0.143816277384758\n",
      "Epoch 6077: train loss: 0.13426081836223602, val loss: 0.14373162388801575\n",
      "Epoch 6078: train loss: 0.13416704535484314, val loss: 0.14364711940288544\n",
      "Epoch 6079: train loss: 0.13407334685325623, val loss: 0.14356262981891632\n",
      "Epoch 6080: train loss: 0.13397973775863647, val loss: 0.14347827434539795\n",
      "Epoch 6081: train loss: 0.1338861882686615, val loss: 0.14339399337768555\n",
      "Epoch 6082: train loss: 0.1337927132844925, val loss: 0.14330971240997314\n",
      "Epoch 6083: train loss: 0.13369928300380707, val loss: 0.1432255059480667\n",
      "Epoch 6084: train loss: 0.13360600173473358, val loss: 0.14314137399196625\n",
      "Epoch 6085: train loss: 0.13351276516914368, val loss: 0.14305739104747772\n",
      "Epoch 6086: train loss: 0.13341958820819855, val loss: 0.14297352731227875\n",
      "Epoch 6087: train loss: 0.13332651555538177, val loss: 0.14288978278636932\n",
      "Epoch 6088: train loss: 0.13323353230953217, val loss: 0.14280588924884796\n",
      "Epoch 6089: train loss: 0.13314056396484375, val loss: 0.1427222043275833\n",
      "Epoch 6090: train loss: 0.13304772973060608, val loss: 0.14263859391212463\n",
      "Epoch 6091: train loss: 0.132954940199852, val loss: 0.14255507290363312\n",
      "Epoch 6092: train loss: 0.13286222517490387, val loss: 0.1424715667963028\n",
      "Epoch 6093: train loss: 0.1327696293592453, val loss: 0.1423882097005844\n",
      "Epoch 6094: train loss: 0.1326770931482315, val loss: 0.1423049420118332\n",
      "Epoch 6095: train loss: 0.1325846016407013, val loss: 0.1422216147184372\n",
      "Epoch 6096: train loss: 0.13249221444129944, val loss: 0.14213858544826508\n",
      "Epoch 6097: train loss: 0.13239990174770355, val loss: 0.14205549657344818\n",
      "Epoch 6098: train loss: 0.13230763375759125, val loss: 0.14197243750095367\n",
      "Epoch 6099: train loss: 0.1322154998779297, val loss: 0.14188960194587708\n",
      "Epoch 6100: train loss: 0.13212339580059052, val loss: 0.1418067216873169\n",
      "Epoch 6101: train loss: 0.1320313662290573, val loss: 0.14172397553920746\n",
      "Epoch 6102: train loss: 0.13193945586681366, val loss: 0.14164122939109802\n",
      "Epoch 6103: train loss: 0.1318475902080536, val loss: 0.1415586918592453\n",
      "Epoch 6104: train loss: 0.1317557692527771, val loss: 0.1414760798215866\n",
      "Epoch 6105: train loss: 0.13166405260562897, val loss: 0.14139366149902344\n",
      "Epoch 6106: train loss: 0.1315723955631256, val loss: 0.14131130278110504\n",
      "Epoch 6107: train loss: 0.13148082792758942, val loss: 0.14122894406318665\n",
      "Epoch 6108: train loss: 0.1313893049955368, val loss: 0.1411467045545578\n",
      "Epoch 6109: train loss: 0.13129790127277374, val loss: 0.14106450974941254\n",
      "Epoch 6110: train loss: 0.13120655715465546, val loss: 0.14098243415355682\n",
      "Epoch 6111: train loss: 0.13111524283885956, val loss: 0.1409004181623459\n",
      "Epoch 6112: train loss: 0.1310240626335144, val loss: 0.1408185064792633\n",
      "Epoch 6113: train loss: 0.1309329718351364, val loss: 0.1407366544008255\n",
      "Epoch 6114: train loss: 0.13084186613559723, val loss: 0.14065492153167725\n",
      "Epoch 6115: train loss: 0.13075093924999237, val loss: 0.140573188662529\n",
      "Epoch 6116: train loss: 0.1306600421667099, val loss: 0.1404915601015091\n",
      "Epoch 6117: train loss: 0.1305692046880722, val loss: 0.14041003584861755\n",
      "Epoch 6118: train loss: 0.13047845661640167, val loss: 0.1403285562992096\n",
      "Epoch 6119: train loss: 0.1303878128528595, val loss: 0.14024721086025238\n",
      "Epoch 6120: train loss: 0.1302972137928009, val loss: 0.14016583561897278\n",
      "Epoch 6121: train loss: 0.13020670413970947, val loss: 0.14008459448814392\n",
      "Epoch 6122: train loss: 0.13011625409126282, val loss: 0.14000341296195984\n",
      "Epoch 6123: train loss: 0.13002586364746094, val loss: 0.1399223357439041\n",
      "Epoch 6124: train loss: 0.1299356073141098, val loss: 0.13984127342700958\n",
      "Epoch 6125: train loss: 0.12984539568424225, val loss: 0.13976037502288818\n",
      "Epoch 6126: train loss: 0.12975522875785828, val loss: 0.13967964053153992\n",
      "Epoch 6127: train loss: 0.12966518104076385, val loss: 0.13959884643554688\n",
      "Epoch 6128: train loss: 0.12957517802715302, val loss: 0.13951809704303741\n",
      "Epoch 6129: train loss: 0.12948526442050934, val loss: 0.1394375115633011\n",
      "Epoch 6130: train loss: 0.12939544022083282, val loss: 0.13935686647891998\n",
      "Epoch 6131: train loss: 0.12930569052696228, val loss: 0.1392764151096344\n",
      "Epoch 6132: train loss: 0.12921597063541412, val loss: 0.13919606804847717\n",
      "Epoch 6133: train loss: 0.12912636995315552, val loss: 0.13911572098731995\n",
      "Epoch 6134: train loss: 0.12903684377670288, val loss: 0.1390353888273239\n",
      "Epoch 6135: train loss: 0.12894736230373383, val loss: 0.1389552801847458\n",
      "Epoch 6136: train loss: 0.12885800004005432, val loss: 0.13887524604797363\n",
      "Epoch 6137: train loss: 0.1287686675786972, val loss: 0.1387951821088791\n",
      "Epoch 6138: train loss: 0.12867937982082367, val loss: 0.1387152224779129\n",
      "Epoch 6139: train loss: 0.12859027087688446, val loss: 0.13863544166088104\n",
      "Epoch 6140: train loss: 0.12850117683410645, val loss: 0.1385556012392044\n",
      "Epoch 6141: train loss: 0.128412127494812, val loss: 0.13847588002681732\n",
      "Epoch 6142: train loss: 0.12832321226596832, val loss: 0.1383962333202362\n",
      "Epoch 6143: train loss: 0.12823434174060822, val loss: 0.13831667602062225\n",
      "Epoch 6144: train loss: 0.12814554572105408, val loss: 0.13823716342449188\n",
      "Epoch 6145: train loss: 0.1280568540096283, val loss: 0.13815775513648987\n",
      "Epoch 6146: train loss: 0.1279682070016861, val loss: 0.1380784958600998\n",
      "Epoch 6147: train loss: 0.12787960469722748, val loss: 0.13799916207790375\n",
      "Epoch 6148: train loss: 0.1277911514043808, val loss: 0.13791999220848083\n",
      "Epoch 6149: train loss: 0.1277027428150177, val loss: 0.13784083724021912\n",
      "Epoch 6150: train loss: 0.127614364027977, val loss: 0.1377619355916977\n",
      "Epoch 6151: train loss: 0.12752611935138702, val loss: 0.1376829594373703\n",
      "Epoch 6152: train loss: 0.12743793427944183, val loss: 0.13760404288768768\n",
      "Epoch 6153: train loss: 0.12734979391098022, val loss: 0.13752524554729462\n",
      "Epoch 6154: train loss: 0.12726177275180817, val loss: 0.13744644820690155\n",
      "Epoch 6155: train loss: 0.1271737813949585, val loss: 0.13736779987812042\n",
      "Epoch 6156: train loss: 0.1270858496427536, val loss: 0.13728931546211243\n",
      "Epoch 6157: train loss: 0.12699805200099945, val loss: 0.13721074163913727\n",
      "Epoch 6158: train loss: 0.12691031396389008, val loss: 0.13713227212429047\n",
      "Epoch 6159: train loss: 0.1268226057291031, val loss: 0.13705390691757202\n",
      "Epoch 6160: train loss: 0.12673498690128326, val loss: 0.13697563111782074\n",
      "Epoch 6161: train loss: 0.1266474425792694, val loss: 0.1368974894285202\n",
      "Epoch 6162: train loss: 0.12655997276306152, val loss: 0.1368192434310913\n",
      "Epoch 6163: train loss: 0.1264725774526596, val loss: 0.13674116134643555\n",
      "Epoch 6164: train loss: 0.12638524174690247, val loss: 0.13666318356990814\n",
      "Epoch 6165: train loss: 0.1262979954481125, val loss: 0.13658519089221954\n",
      "Epoch 6166: train loss: 0.12621082365512848, val loss: 0.13650740683078766\n",
      "Epoch 6167: train loss: 0.12612371146678925, val loss: 0.13642960786819458\n",
      "Epoch 6168: train loss: 0.12603668868541718, val loss: 0.13635192811489105\n",
      "Epoch 6169: train loss: 0.1259496957063675, val loss: 0.1362743079662323\n",
      "Epoch 6170: train loss: 0.12586285173892975, val loss: 0.13619671761989594\n",
      "Epoch 6171: train loss: 0.1257760375738144, val loss: 0.13611921668052673\n",
      "Epoch 6172: train loss: 0.12568926811218262, val loss: 0.13604190945625305\n",
      "Epoch 6173: train loss: 0.1256026327610016, val loss: 0.13596458733081818\n",
      "Epoch 6174: train loss: 0.12551604211330414, val loss: 0.13588730990886688\n",
      "Epoch 6175: train loss: 0.12542951107025146, val loss: 0.13581013679504395\n",
      "Epoch 6176: train loss: 0.12534306943416595, val loss: 0.13573302328586578\n",
      "Epoch 6177: train loss: 0.1252567023038864, val loss: 0.13565592467784882\n",
      "Epoch 6178: train loss: 0.12517039477825165, val loss: 0.13557903468608856\n",
      "Epoch 6179: train loss: 0.12508419156074524, val loss: 0.13550211489200592\n",
      "Epoch 6180: train loss: 0.12499803304672241, val loss: 0.13542531430721283\n",
      "Epoch 6181: train loss: 0.12491193413734436, val loss: 0.1353486031293869\n",
      "Epoch 6182: train loss: 0.12482593953609467, val loss: 0.13527201116085052\n",
      "Epoch 6183: train loss: 0.12474003434181213, val loss: 0.13519537448883057\n",
      "Epoch 6184: train loss: 0.12465415894985199, val loss: 0.13511888682842255\n",
      "Epoch 6185: train loss: 0.1245683804154396, val loss: 0.1350424438714981\n",
      "Epoch 6186: train loss: 0.1244826689362526, val loss: 0.13496606051921844\n",
      "Epoch 6187: train loss: 0.12439700216054916, val loss: 0.13488975167274475\n",
      "Epoch 6188: train loss: 0.12431149184703827, val loss: 0.13481353223323822\n",
      "Epoch 6189: train loss: 0.12422598153352737, val loss: 0.13473740220069885\n",
      "Epoch 6190: train loss: 0.12414055317640305, val loss: 0.13466134667396545\n",
      "Epoch 6191: train loss: 0.12405520677566528, val loss: 0.1345853954553604\n",
      "Epoch 6192: train loss: 0.12396994233131409, val loss: 0.13450947403907776\n",
      "Epoch 6193: train loss: 0.12388471513986588, val loss: 0.13443370163440704\n",
      "Epoch 6194: train loss: 0.12379961460828781, val loss: 0.13435785472393036\n",
      "Epoch 6195: train loss: 0.12371455878019333, val loss: 0.1342822164297104\n",
      "Epoch 6196: train loss: 0.12362955510616302, val loss: 0.13420651853084564\n",
      "Epoch 6197: train loss: 0.12354466319084167, val loss: 0.13413099944591522\n",
      "Epoch 6198: train loss: 0.12345980852842331, val loss: 0.1340555101633072\n",
      "Epoch 6199: train loss: 0.12337502837181091, val loss: 0.13398008048534393\n",
      "Epoch 6200: train loss: 0.12329035997390747, val loss: 0.13390478491783142\n",
      "Epoch 6201: train loss: 0.1232057511806488, val loss: 0.1338295042514801\n",
      "Epoch 6202: train loss: 0.12312117964029312, val loss: 0.13375437259674072\n",
      "Epoch 6203: train loss: 0.12303672730922699, val loss: 0.13367919623851776\n",
      "Epoch 6204: train loss: 0.12295231223106384, val loss: 0.13360419869422913\n",
      "Epoch 6205: train loss: 0.12286795675754547, val loss: 0.13352924585342407\n",
      "Epoch 6206: train loss: 0.12278373539447784, val loss: 0.1334543079137802\n",
      "Epoch 6207: train loss: 0.12269957363605499, val loss: 0.13337960839271545\n",
      "Epoch 6208: train loss: 0.12261546403169632, val loss: 0.1333049088716507\n",
      "Epoch 6209: train loss: 0.1225314512848854, val loss: 0.13323020935058594\n",
      "Epoch 6210: train loss: 0.12244748324155807, val loss: 0.13315558433532715\n",
      "Epoch 6211: train loss: 0.1223636344075203, val loss: 0.1330811232328415\n",
      "Epoch 6212: train loss: 0.12227983772754669, val loss: 0.13300664722919464\n",
      "Epoch 6213: train loss: 0.12219607084989548, val loss: 0.13293235003948212\n",
      "Epoch 6214: train loss: 0.12211242318153381, val loss: 0.13285811245441437\n",
      "Epoch 6215: train loss: 0.12202884256839752, val loss: 0.13278378546237946\n",
      "Epoch 6216: train loss: 0.121945321559906, val loss: 0.13270968198776245\n",
      "Epoch 6217: train loss: 0.12186188250780106, val loss: 0.1326356828212738\n",
      "Epoch 6218: train loss: 0.12177851051092148, val loss: 0.132561594247818\n",
      "Epoch 6219: train loss: 0.12169518321752548, val loss: 0.1324876993894577\n",
      "Epoch 6220: train loss: 0.12161196023225784, val loss: 0.1324138641357422\n",
      "Epoch 6221: train loss: 0.12152881920337677, val loss: 0.13234005868434906\n",
      "Epoch 6222: train loss: 0.12144571542739868, val loss: 0.13226637244224548\n",
      "Epoch 6223: train loss: 0.12136270850896835, val loss: 0.13219277560710907\n",
      "Epoch 6224: train loss: 0.12127978354692459, val loss: 0.13211923837661743\n",
      "Epoch 6225: train loss: 0.12119689583778381, val loss: 0.13204573094844818\n",
      "Epoch 6226: train loss: 0.1211140975356102, val loss: 0.13197240233421326\n",
      "Epoch 6227: train loss: 0.12103140354156494, val loss: 0.13189901411533356\n",
      "Epoch 6228: train loss: 0.12094870209693909, val loss: 0.1318257451057434\n",
      "Epoch 6229: train loss: 0.12086614221334457, val loss: 0.13175252079963684\n",
      "Epoch 6230: train loss: 0.12078362703323364, val loss: 0.13167934119701385\n",
      "Epoch 6231: train loss: 0.12070115655660629, val loss: 0.13160626590251923\n",
      "Epoch 6232: train loss: 0.12061882019042969, val loss: 0.13153330981731415\n",
      "Epoch 6233: train loss: 0.12053649127483368, val loss: 0.13146033883094788\n",
      "Epoch 6234: train loss: 0.12045425921678543, val loss: 0.13138768076896667\n",
      "Epoch 6235: train loss: 0.12037213891744614, val loss: 0.13131488859653473\n",
      "Epoch 6236: train loss: 0.12029004096984863, val loss: 0.13124218583106995\n",
      "Epoch 6237: train loss: 0.12020798027515411, val loss: 0.13116958737373352\n",
      "Epoch 6238: train loss: 0.12012606114149094, val loss: 0.1310969740152359\n",
      "Epoch 6239: train loss: 0.12004420161247253, val loss: 0.13102449476718903\n",
      "Epoch 6240: train loss: 0.11996237933635712, val loss: 0.13095210492610931\n",
      "Epoch 6241: train loss: 0.11988066136837006, val loss: 0.13087980449199677\n",
      "Epoch 6242: train loss: 0.11979901045560837, val loss: 0.1308075189590454\n",
      "Epoch 6243: train loss: 0.11971738189458847, val loss: 0.13073532283306122\n",
      "Epoch 6244: train loss: 0.11963589489459991, val loss: 0.13066329061985016\n",
      "Epoch 6245: train loss: 0.11955443769693375, val loss: 0.13059116899967194\n",
      "Epoch 6246: train loss: 0.11947305500507355, val loss: 0.13051925599575043\n",
      "Epoch 6247: train loss: 0.11939176172018051, val loss: 0.13044731318950653\n",
      "Epoch 6248: train loss: 0.11931050568819046, val loss: 0.13037550449371338\n",
      "Epoch 6249: train loss: 0.11922933161258698, val loss: 0.13030369579792023\n",
      "Epoch 6250: train loss: 0.11914825439453125, val loss: 0.13023202121257782\n",
      "Epoch 6251: train loss: 0.1190672293305397, val loss: 0.130160391330719\n",
      "Epoch 6252: train loss: 0.11898627132177353, val loss: 0.13008885085582733\n",
      "Epoch 6253: train loss: 0.11890538781881332, val loss: 0.13001741468906403\n",
      "Epoch 6254: train loss: 0.11882457882165909, val loss: 0.12994591891765594\n",
      "Epoch 6255: train loss: 0.11874382197856903, val loss: 0.12987464666366577\n",
      "Epoch 6256: train loss: 0.11866317689418793, val loss: 0.12980344891548157\n",
      "Epoch 6257: train loss: 0.11858254671096802, val loss: 0.12973226606845856\n",
      "Epoch 6258: train loss: 0.11850201338529587, val loss: 0.12966111302375793\n",
      "Epoch 6259: train loss: 0.11842157691717148, val loss: 0.1295901983976364\n",
      "Epoch 6260: train loss: 0.11834117025136948, val loss: 0.12951909005641937\n",
      "Epoch 6261: train loss: 0.11826083064079285, val loss: 0.12944823503494263\n",
      "Epoch 6262: train loss: 0.11818060278892517, val loss: 0.1293773353099823\n",
      "Epoch 6263: train loss: 0.11810042709112167, val loss: 0.12930670380592346\n",
      "Epoch 6264: train loss: 0.11802034825086594, val loss: 0.1292359083890915\n",
      "Epoch 6265: train loss: 0.11794033646583557, val loss: 0.1291653960943222\n",
      "Epoch 6266: train loss: 0.11786036193370819, val loss: 0.12909483909606934\n",
      "Epoch 6267: train loss: 0.11778049916028976, val loss: 0.1290244162082672\n",
      "Epoch 6268: train loss: 0.11770068854093552, val loss: 0.12895405292510986\n",
      "Epoch 6269: train loss: 0.11762091517448425, val loss: 0.1288837194442749\n",
      "Epoch 6270: train loss: 0.11754129827022552, val loss: 0.12881344556808472\n",
      "Epoch 6271: train loss: 0.1174616739153862, val loss: 0.1287432461977005\n",
      "Epoch 6272: train loss: 0.11738213896751404, val loss: 0.12867319583892822\n",
      "Epoch 6273: train loss: 0.11730266362428665, val loss: 0.12860316038131714\n",
      "Epoch 6274: train loss: 0.11722328513860703, val loss: 0.12853315472602844\n",
      "Epoch 6275: train loss: 0.11714394390583038, val loss: 0.1284632682800293\n",
      "Epoch 6276: train loss: 0.1170647144317627, val loss: 0.12839345633983612\n",
      "Epoch 6277: train loss: 0.11698552221059799, val loss: 0.12832368910312653\n",
      "Epoch 6278: train loss: 0.11690640449523926, val loss: 0.1282539665699005\n",
      "Epoch 6279: train loss: 0.11682737618684769, val loss: 0.12818436324596405\n",
      "Epoch 6280: train loss: 0.1167484000325203, val loss: 0.12811480462551117\n",
      "Epoch 6281: train loss: 0.11666946858167648, val loss: 0.12804538011550903\n",
      "Epoch 6282: train loss: 0.11659064888954163, val loss: 0.12797600030899048\n",
      "Epoch 6283: train loss: 0.11651187390089035, val loss: 0.1279066652059555\n",
      "Epoch 6284: train loss: 0.11643316596746445, val loss: 0.1278374046087265\n",
      "Epoch 6285: train loss: 0.1163545623421669, val loss: 0.12776820361614227\n",
      "Epoch 6286: train loss: 0.11627600342035294, val loss: 0.127699077129364\n",
      "Epoch 6287: train loss: 0.11619748175144196, val loss: 0.12762999534606934\n",
      "Epoch 6288: train loss: 0.11611909419298172, val loss: 0.12756109237670898\n",
      "Epoch 6289: train loss: 0.11604074388742447, val loss: 0.12749214470386505\n",
      "Epoch 6290: train loss: 0.11596241593360901, val loss: 0.1274232268333435\n",
      "Epoch 6291: train loss: 0.11588422954082489, val loss: 0.12735453248023987\n",
      "Epoch 6292: train loss: 0.11580609530210495, val loss: 0.12728585302829742\n",
      "Epoch 6293: train loss: 0.11572805047035217, val loss: 0.12721727788448334\n",
      "Epoch 6294: train loss: 0.11565007269382477, val loss: 0.12714864313602448\n",
      "Epoch 6295: train loss: 0.11557214707136154, val loss: 0.12708021700382233\n",
      "Epoch 6296: train loss: 0.11549428850412369, val loss: 0.12701182067394257\n",
      "Epoch 6297: train loss: 0.115416519343853, val loss: 0.12694349884986877\n",
      "Epoch 6298: train loss: 0.11533878743648529, val loss: 0.12687519192695618\n",
      "Epoch 6299: train loss: 0.11526116728782654, val loss: 0.1268070787191391\n",
      "Epoch 6300: train loss: 0.11518359184265137, val loss: 0.12673881649971008\n",
      "Epoch 6301: train loss: 0.11510605365037918, val loss: 0.12667077779769897\n",
      "Epoch 6302: train loss: 0.11502864956855774, val loss: 0.12660281360149384\n",
      "Epoch 6303: train loss: 0.11495126038789749, val loss: 0.1265348643064499\n",
      "Epoch 6304: train loss: 0.1148739755153656, val loss: 0.12646706402301788\n",
      "Epoch 6305: train loss: 0.11479675024747849, val loss: 0.12639929354190826\n",
      "Epoch 6306: train loss: 0.11471957713365555, val loss: 0.12633150815963745\n",
      "Epoch 6307: train loss: 0.11464246362447739, val loss: 0.12626396119594574\n",
      "Epoch 6308: train loss: 0.11456546932458878, val loss: 0.12619632482528687\n",
      "Epoch 6309: train loss: 0.11448849737644196, val loss: 0.12612879276275635\n",
      "Epoch 6310: train loss: 0.11441159248352051, val loss: 0.12606142461299896\n",
      "Epoch 6311: train loss: 0.11433478444814682, val loss: 0.12599407136440277\n",
      "Epoch 6312: train loss: 0.11425803601741791, val loss: 0.12592670321464539\n",
      "Epoch 6313: train loss: 0.11418133229017258, val loss: 0.12585954368114471\n",
      "Epoch 6314: train loss: 0.11410471796989441, val loss: 0.12579236924648285\n",
      "Epoch 6315: train loss: 0.1140281930565834, val loss: 0.12572529911994934\n",
      "Epoch 6316: train loss: 0.11395172774791718, val loss: 0.1256582885980606\n",
      "Epoch 6317: train loss: 0.11387535184621811, val loss: 0.12559138238430023\n",
      "Epoch 6318: train loss: 0.11379899084568024, val loss: 0.12552443146705627\n",
      "Epoch 6319: train loss: 0.11372276395559311, val loss: 0.1254577487707138\n",
      "Epoch 6320: train loss: 0.11364655941724777, val loss: 0.12539097666740417\n",
      "Epoch 6321: train loss: 0.1135704293847084, val loss: 0.1253242939710617\n",
      "Epoch 6322: train loss: 0.1134943962097168, val loss: 0.12525777518749237\n",
      "Epoch 6323: train loss: 0.11341840028762817, val loss: 0.12519113719463348\n",
      "Epoch 6324: train loss: 0.11334247887134552, val loss: 0.1251247227191925\n",
      "Epoch 6325: train loss: 0.11326663941144943, val loss: 0.1250583529472351\n",
      "Epoch 6326: train loss: 0.11319084465503693, val loss: 0.1249919906258583\n",
      "Epoch 6327: train loss: 0.11311512440443039, val loss: 0.12492576986551285\n",
      "Epoch 6328: train loss: 0.11303949356079102, val loss: 0.12485964596271515\n",
      "Epoch 6329: train loss: 0.11296393722295761, val loss: 0.12479346990585327\n",
      "Epoch 6330: train loss: 0.1128883808851242, val loss: 0.12472746521234512\n",
      "Epoch 6331: train loss: 0.11281296610832214, val loss: 0.12466149777173996\n",
      "Epoch 6332: train loss: 0.11273755133152008, val loss: 0.12459557503461838\n",
      "Epoch 6333: train loss: 0.11266224086284637, val loss: 0.12452971935272217\n",
      "Epoch 6334: train loss: 0.11258701980113983, val loss: 0.1244639903306961\n",
      "Epoch 6335: train loss: 0.11251185089349747, val loss: 0.12439832836389542\n",
      "Epoch 6336: train loss: 0.11243677139282227, val loss: 0.12433270364999771\n",
      "Epoch 6337: train loss: 0.11236174404621124, val loss: 0.12426712363958359\n",
      "Epoch 6338: train loss: 0.1122867688536644, val loss: 0.1242016926407814\n",
      "Epoch 6339: train loss: 0.1122119277715683, val loss: 0.12413626164197922\n",
      "Epoch 6340: train loss: 0.1121370866894722, val loss: 0.12407088279724121\n",
      "Epoch 6341: train loss: 0.11206230521202087, val loss: 0.12400554865598679\n",
      "Epoch 6342: train loss: 0.1119876429438591, val loss: 0.1239403486251831\n",
      "Epoch 6343: train loss: 0.1119130477309227, val loss: 0.123875193297863\n",
      "Epoch 6344: train loss: 0.1118384525179863, val loss: 0.12381010502576828\n",
      "Epoch 6345: train loss: 0.11176399141550064, val loss: 0.1237451583147049\n",
      "Epoch 6346: train loss: 0.11168956756591797, val loss: 0.12368025630712509\n",
      "Epoch 6347: train loss: 0.11161519587039948, val loss: 0.12361540645360947\n",
      "Epoch 6348: train loss: 0.11154093593358994, val loss: 0.12355058640241623\n",
      "Epoch 6349: train loss: 0.11146670579910278, val loss: 0.12348581850528717\n",
      "Epoch 6350: train loss: 0.1113925501704216, val loss: 0.12342105060815811\n",
      "Epoch 6351: train loss: 0.11131848394870758, val loss: 0.12335648387670517\n",
      "Epoch 6352: train loss: 0.11124447733163834, val loss: 0.12329193204641342\n",
      "Epoch 6353: train loss: 0.11117048561573029, val loss: 0.12322745472192764\n",
      "Epoch 6354: train loss: 0.11109663546085358, val loss: 0.12316305935382843\n",
      "Epoch 6355: train loss: 0.11102282255887985, val loss: 0.12309887260198593\n",
      "Epoch 6356: train loss: 0.1109490916132927, val loss: 0.12303458899259567\n",
      "Epoch 6357: train loss: 0.1108754426240921, val loss: 0.12297041714191437\n",
      "Epoch 6358: train loss: 0.1108018159866333, val loss: 0.12290628254413605\n",
      "Epoch 6359: train loss: 0.11072828620672226, val loss: 0.12284224480390549\n",
      "Epoch 6360: train loss: 0.11065483093261719, val loss: 0.12277819961309433\n",
      "Epoch 6361: train loss: 0.1105814203619957, val loss: 0.12271427363157272\n",
      "Epoch 6362: train loss: 0.11050809174776077, val loss: 0.12265042215585709\n",
      "Epoch 6363: train loss: 0.11043484508991241, val loss: 0.12258666753768921\n",
      "Epoch 6364: train loss: 0.11036161333322525, val loss: 0.12252297252416611\n",
      "Epoch 6365: train loss: 0.11028848588466644, val loss: 0.12245932966470718\n",
      "Epoch 6366: train loss: 0.11021542549133301, val loss: 0.12239570915699005\n",
      "Epoch 6367: train loss: 0.11014241725206375, val loss: 0.12233223766088486\n",
      "Epoch 6368: train loss: 0.11006949096918106, val loss: 0.12226881831884384\n",
      "Epoch 6369: train loss: 0.10999661684036255, val loss: 0.12220543622970581\n",
      "Epoch 6370: train loss: 0.10992385447025299, val loss: 0.12214206904172897\n",
      "Epoch 6371: train loss: 0.10985112190246582, val loss: 0.12207885086536407\n",
      "Epoch 6372: train loss: 0.10977847129106522, val loss: 0.12201571464538574\n",
      "Epoch 6373: train loss: 0.1097058653831482, val loss: 0.1219526082277298\n",
      "Epoch 6374: train loss: 0.10963336378335953, val loss: 0.12188959121704102\n",
      "Epoch 6375: train loss: 0.10956090688705444, val loss: 0.12182670831680298\n",
      "Epoch 6376: train loss: 0.10948850959539413, val loss: 0.12176376581192017\n",
      "Epoch 6377: train loss: 0.1094161793589592, val loss: 0.12170088291168213\n",
      "Epoch 6378: train loss: 0.10934391617774963, val loss: 0.12163817882537842\n",
      "Epoch 6379: train loss: 0.10927172750234604, val loss: 0.12157545238733292\n",
      "Epoch 6380: train loss: 0.10919961333274841, val loss: 0.12151283025741577\n",
      "Epoch 6381: train loss: 0.10912752151489258, val loss: 0.12145018577575684\n",
      "Epoch 6382: train loss: 0.10905556380748749, val loss: 0.12138773500919342\n",
      "Epoch 6383: train loss: 0.108983613550663, val loss: 0.12132531404495239\n",
      "Epoch 6384: train loss: 0.10891178995370865, val loss: 0.12126298993825912\n",
      "Epoch 6385: train loss: 0.10884000360965729, val loss: 0.12120070308446884\n",
      "Epoch 6386: train loss: 0.1087682768702507, val loss: 0.12113852798938751\n",
      "Epoch 6387: train loss: 0.10869664698839188, val loss: 0.12107639759778976\n",
      "Epoch 6388: train loss: 0.10862505435943604, val loss: 0.12101428955793381\n",
      "Epoch 6389: train loss: 0.10855352133512497, val loss: 0.12095224857330322\n",
      "Epoch 6390: train loss: 0.10848207026720047, val loss: 0.12089022248983383\n",
      "Epoch 6391: train loss: 0.10841068625450134, val loss: 0.12082827091217041\n",
      "Epoch 6392: train loss: 0.1083393543958664, val loss: 0.12076659500598907\n",
      "Epoch 6393: train loss: 0.10826810449361801, val loss: 0.12070479243993759\n",
      "Epoch 6394: train loss: 0.108196921646595, val loss: 0.12064315378665924\n",
      "Epoch 6395: train loss: 0.10812577605247498, val loss: 0.1205815002322197\n",
      "Epoch 6396: train loss: 0.10805471241474152, val loss: 0.12051993608474731\n",
      "Epoch 6397: train loss: 0.10798371583223343, val loss: 0.12045847624540329\n",
      "Epoch 6398: train loss: 0.1079128161072731, val loss: 0.12039702385663986\n",
      "Epoch 6399: train loss: 0.10784193873405457, val loss: 0.120335653424263\n",
      "Epoch 6400: train loss: 0.10777115821838379, val loss: 0.12027440220117569\n",
      "Epoch 6401: train loss: 0.10770047456026077, val loss: 0.12021315097808838\n",
      "Epoch 6402: train loss: 0.10762979835271835, val loss: 0.12015199661254883\n",
      "Epoch 6403: train loss: 0.10755917429924011, val loss: 0.12009098380804062\n",
      "Epoch 6404: train loss: 0.10748867690563202, val loss: 0.12002992630004883\n",
      "Epoch 6405: train loss: 0.1074182316660881, val loss: 0.1199689656496048\n",
      "Epoch 6406: train loss: 0.10734780877828598, val loss: 0.11990811675786972\n",
      "Epoch 6407: train loss: 0.10727749019861221, val loss: 0.11984726041555405\n",
      "Epoch 6408: train loss: 0.10720719397068024, val loss: 0.11978647857904434\n",
      "Epoch 6409: train loss: 0.10713697969913483, val loss: 0.1197257861495018\n",
      "Epoch 6410: train loss: 0.10706685483455658, val loss: 0.11966512352228165\n",
      "Epoch 6411: train loss: 0.10699678957462311, val loss: 0.11960458755493164\n",
      "Epoch 6412: train loss: 0.1069268137216568, val loss: 0.1195441484451294\n",
      "Epoch 6413: train loss: 0.10685689002275467, val loss: 0.11948372423648834\n",
      "Epoch 6414: train loss: 0.10678700357675552, val loss: 0.11942338943481445\n",
      "Epoch 6415: train loss: 0.10671722143888474, val loss: 0.11936312168836594\n",
      "Epoch 6416: train loss: 0.10664749145507812, val loss: 0.11930286139249802\n",
      "Epoch 6417: train loss: 0.1065777912735939, val loss: 0.11924266815185547\n",
      "Epoch 6418: train loss: 0.10650820285081863, val loss: 0.11918264627456665\n",
      "Epoch 6419: train loss: 0.10643867403268814, val loss: 0.11912255734205246\n",
      "Epoch 6420: train loss: 0.10636914521455765, val loss: 0.11906253546476364\n",
      "Epoch 6421: train loss: 0.10629976540803909, val loss: 0.11900260299444199\n",
      "Epoch 6422: train loss: 0.10623042285442352, val loss: 0.11894283443689346\n",
      "Epoch 6423: train loss: 0.10616116225719452, val loss: 0.11888308823108673\n",
      "Epoch 6424: train loss: 0.10609196126461029, val loss: 0.11882343143224716\n",
      "Epoch 6425: train loss: 0.10602279752492905, val loss: 0.11876382678747177\n",
      "Epoch 6426: train loss: 0.10595374554395676, val loss: 0.11870425194501877\n",
      "Epoch 6427: train loss: 0.10588472336530685, val loss: 0.11864477396011353\n",
      "Epoch 6428: train loss: 0.10581576079130173, val loss: 0.11858532577753067\n",
      "Epoch 6429: train loss: 0.10574688762426376, val loss: 0.1185259148478508\n",
      "Epoch 6430: train loss: 0.10567807406187057, val loss: 0.11846654862165451\n",
      "Epoch 6431: train loss: 0.10560930520296097, val loss: 0.11840736120939255\n",
      "Epoch 6432: train loss: 0.10554061830043793, val loss: 0.11834815889596939\n",
      "Epoch 6433: train loss: 0.10547199845314026, val loss: 0.11828909069299698\n",
      "Epoch 6434: train loss: 0.10540346056222916, val loss: 0.11823005974292755\n",
      "Epoch 6435: train loss: 0.10533495992422104, val loss: 0.1181710734963417\n",
      "Epoch 6436: train loss: 0.10526653379201889, val loss: 0.11811219900846481\n",
      "Epoch 6437: train loss: 0.10519818216562271, val loss: 0.1180533692240715\n",
      "Epoch 6438: train loss: 0.1051299050450325, val loss: 0.117994524538517\n",
      "Epoch 6439: train loss: 0.10506165027618408, val loss: 0.11793587356805801\n",
      "Epoch 6440: train loss: 0.10499348491430283, val loss: 0.11787720024585724\n",
      "Epoch 6441: train loss: 0.10492539405822754, val loss: 0.11781859397888184\n",
      "Epoch 6442: train loss: 0.10485731810331345, val loss: 0.1177600622177124\n",
      "Epoch 6443: train loss: 0.1047893539071083, val loss: 0.11770161241292953\n",
      "Epoch 6444: train loss: 0.10472144186496735, val loss: 0.11764325946569443\n",
      "Epoch 6445: train loss: 0.10465362668037415, val loss: 0.11758502572774887\n",
      "Epoch 6446: train loss: 0.10458586364984512, val loss: 0.11752667278051376\n",
      "Epoch 6447: train loss: 0.10451812297105789, val loss: 0.11746849864721298\n",
      "Epoch 6448: train loss: 0.10445049405097961, val loss: 0.11741039901971817\n",
      "Epoch 6449: train loss: 0.10438290983438492, val loss: 0.11735229939222336\n",
      "Epoch 6450: train loss: 0.1043153777718544, val loss: 0.1172943040728569\n",
      "Epoch 6451: train loss: 0.10424794256687164, val loss: 0.11723633110523224\n",
      "Epoch 6452: train loss: 0.10418053716421127, val loss: 0.11717849224805832\n",
      "Epoch 6453: train loss: 0.10411319881677628, val loss: 0.11712074279785156\n",
      "Epoch 6454: train loss: 0.10404595732688904, val loss: 0.11706297844648361\n",
      "Epoch 6455: train loss: 0.103978730738163, val loss: 0.11700534075498581\n",
      "Epoch 6456: train loss: 0.1039116382598877, val loss: 0.11694767326116562\n",
      "Epoch 6457: train loss: 0.10384458303451538, val loss: 0.11689016968011856\n",
      "Epoch 6458: train loss: 0.10377757251262665, val loss: 0.1168326660990715\n",
      "Epoch 6459: train loss: 0.10371065139770508, val loss: 0.11677523702383041\n",
      "Epoch 6460: train loss: 0.1036437600851059, val loss: 0.1167178750038147\n",
      "Epoch 6461: train loss: 0.10357693582773209, val loss: 0.11666055768728256\n",
      "Epoch 6462: train loss: 0.10351020097732544, val loss: 0.11660336703062057\n",
      "Epoch 6463: train loss: 0.10344351083040237, val loss: 0.11654623597860336\n",
      "Epoch 6464: train loss: 0.1033768579363823, val loss: 0.11648912727832794\n",
      "Epoch 6465: train loss: 0.10331033170223236, val loss: 0.11643212288618088\n",
      "Epoch 6466: train loss: 0.10324384272098541, val loss: 0.11637517064809799\n",
      "Epoch 6467: train loss: 0.10317743569612503, val loss: 0.1163182258605957\n",
      "Epoch 6468: train loss: 0.10311108082532883, val loss: 0.11626140028238297\n",
      "Epoch 6469: train loss: 0.10304476320743561, val loss: 0.11620459705591202\n",
      "Epoch 6470: train loss: 0.10297852754592896, val loss: 0.11614792793989182\n",
      "Epoch 6471: train loss: 0.10291236639022827, val loss: 0.1160912737250328\n",
      "Epoch 6472: train loss: 0.10284623503684998, val loss: 0.11603474617004395\n",
      "Epoch 6473: train loss: 0.10278021544218063, val loss: 0.11597820371389389\n",
      "Epoch 6474: train loss: 0.10271422564983368, val loss: 0.11592181026935577\n",
      "Epoch 6475: train loss: 0.10264834761619568, val loss: 0.11586540192365646\n",
      "Epoch 6476: train loss: 0.10258249193429947, val loss: 0.11580904573202133\n",
      "Epoch 6477: train loss: 0.10251668840646744, val loss: 0.11575279384851456\n",
      "Epoch 6478: train loss: 0.10245098918676376, val loss: 0.11569662392139435\n",
      "Epoch 6479: train loss: 0.10238533467054367, val loss: 0.11564041674137115\n",
      "Epoch 6480: train loss: 0.10231971740722656, val loss: 0.11558439582586288\n",
      "Epoch 6481: train loss: 0.10225421190261841, val loss: 0.11552844196557999\n",
      "Epoch 6482: train loss: 0.10218872874975204, val loss: 0.1154724508523941\n",
      "Epoch 6483: train loss: 0.10212330520153046, val loss: 0.11541657894849777\n",
      "Epoch 6484: train loss: 0.10205797851085663, val loss: 0.11536077409982681\n",
      "Epoch 6485: train loss: 0.10199269652366638, val loss: 0.11530504375696182\n",
      "Epoch 6486: train loss: 0.1019274964928627, val loss: 0.11524932831525803\n",
      "Epoch 6487: train loss: 0.1018623411655426, val loss: 0.11519373953342438\n",
      "Epoch 6488: train loss: 0.10179725289344788, val loss: 0.1151382103562355\n",
      "Epoch 6489: train loss: 0.10173223167657852, val loss: 0.11508271843194962\n",
      "Epoch 6490: train loss: 0.10166727751493454, val loss: 0.11502726376056671\n",
      "Epoch 6491: train loss: 0.10160236805677414, val loss: 0.11497180908918381\n",
      "Epoch 6492: train loss: 0.1015375405550003, val loss: 0.11491646617650986\n",
      "Epoch 6493: train loss: 0.10147275030612946, val loss: 0.11486132442951202\n",
      "Epoch 6494: train loss: 0.10140807181596756, val loss: 0.1148061752319336\n",
      "Epoch 6495: train loss: 0.10134343057870865, val loss: 0.11475104093551636\n",
      "Epoch 6496: train loss: 0.10127882659435272, val loss: 0.11469604074954987\n",
      "Epoch 6497: train loss: 0.10121432691812515, val loss: 0.11464106291532516\n",
      "Epoch 6498: train loss: 0.10114990174770355, val loss: 0.11458611488342285\n",
      "Epoch 6499: train loss: 0.10108547657728195, val loss: 0.1145312562584877\n",
      "Epoch 6500: train loss: 0.1010211631655693, val loss: 0.11447644233703613\n",
      "Epoch 6501: train loss: 0.10095689445734024, val loss: 0.11442170292139053\n",
      "Epoch 6502: train loss: 0.10089270770549774, val loss: 0.11436708271503448\n",
      "Epoch 6503: train loss: 0.10082858800888062, val loss: 0.11431238800287247\n",
      "Epoch 6504: train loss: 0.10076449066400528, val loss: 0.11425787210464478\n",
      "Epoch 6505: train loss: 0.1007004901766777, val loss: 0.11420344561338425\n",
      "Epoch 6506: train loss: 0.10063653439283371, val loss: 0.11414898931980133\n",
      "Epoch 6507: train loss: 0.1005726307630539, val loss: 0.11409466713666916\n",
      "Epoch 6508: train loss: 0.10050883144140244, val loss: 0.11404037475585938\n",
      "Epoch 6509: train loss: 0.10044506192207336, val loss: 0.11398613452911377\n",
      "Epoch 6510: train loss: 0.10038134455680847, val loss: 0.11393194645643234\n",
      "Epoch 6511: train loss: 0.10031767934560776, val loss: 0.11387784779071808\n",
      "Epoch 6512: train loss: 0.100254125893116, val loss: 0.1138237714767456\n",
      "Epoch 6513: train loss: 0.10019063204526901, val loss: 0.11376980692148209\n",
      "Epoch 6514: train loss: 0.10012718290090561, val loss: 0.11371586471796036\n",
      "Epoch 6515: train loss: 0.10006377100944519, val loss: 0.1136619821190834\n",
      "Epoch 6516: train loss: 0.10000045597553253, val loss: 0.11360819637775421\n",
      "Epoch 6517: train loss: 0.09993720799684525, val loss: 0.11355450004339218\n",
      "Epoch 6518: train loss: 0.09987398236989975, val loss: 0.11350085586309433\n",
      "Epoch 6519: train loss: 0.09981084614992142, val loss: 0.11344726383686066\n",
      "Epoch 6520: train loss: 0.09974776953458786, val loss: 0.11339373886585236\n",
      "Epoch 6521: train loss: 0.09968478232622147, val loss: 0.11334028095006943\n",
      "Epoch 6522: train loss: 0.09962182492017746, val loss: 0.11328677088022232\n",
      "Epoch 6523: train loss: 0.09955890476703644, val loss: 0.11323340237140656\n",
      "Epoch 6524: train loss: 0.09949610382318497, val loss: 0.11318011581897736\n",
      "Epoch 6525: train loss: 0.09943334013223648, val loss: 0.11312689632177353\n",
      "Epoch 6526: train loss: 0.09937062114477158, val loss: 0.11307372152805328\n",
      "Epoch 6527: train loss: 0.09930797666311264, val loss: 0.11302062124013901\n",
      "Epoch 6528: train loss: 0.09924539923667908, val loss: 0.11296763271093369\n",
      "Epoch 6529: train loss: 0.09918289631605148, val loss: 0.11291459947824478\n",
      "Epoch 6530: train loss: 0.09912046045064926, val loss: 0.11286173015832901\n",
      "Epoch 6531: train loss: 0.09905804693698883, val loss: 0.11280886083841324\n",
      "Epoch 6532: train loss: 0.09899573773145676, val loss: 0.11275602877140045\n",
      "Epoch 6533: train loss: 0.09893345832824707, val loss: 0.11270325630903244\n",
      "Epoch 6534: train loss: 0.09887123107910156, val loss: 0.11265057325363159\n",
      "Epoch 6535: train loss: 0.09880910813808441, val loss: 0.1125980019569397\n",
      "Epoch 6536: train loss: 0.09874700009822845, val loss: 0.1125454530119896\n",
      "Epoch 6537: train loss: 0.09868501126766205, val loss: 0.11249303072690964\n",
      "Epoch 6538: train loss: 0.09862305223941803, val loss: 0.11244060099124908\n",
      "Epoch 6539: train loss: 0.09856115281581879, val loss: 0.1123882532119751\n",
      "Epoch 6540: train loss: 0.09849932044744492, val loss: 0.1123359426856041\n",
      "Epoch 6541: train loss: 0.09843754768371582, val loss: 0.11228363960981369\n",
      "Epoch 6542: train loss: 0.0983758196234703, val loss: 0.11223145574331284\n",
      "Epoch 6543: train loss: 0.09831415861845016, val loss: 0.11217931658029556\n",
      "Epoch 6544: train loss: 0.09825257211923599, val loss: 0.11212718486785889\n",
      "Epoch 6545: train loss: 0.09819107502698898, val loss: 0.11207524687051773\n",
      "Epoch 6546: train loss: 0.09812960028648376, val loss: 0.11202339082956314\n",
      "Epoch 6547: train loss: 0.09806820005178452, val loss: 0.11197149753570557\n",
      "Epoch 6548: train loss: 0.09800685197114944, val loss: 0.11191972345113754\n",
      "Epoch 6549: train loss: 0.09794556349515915, val loss: 0.11186796426773071\n",
      "Epoch 6550: train loss: 0.09788434207439423, val loss: 0.11181624233722687\n",
      "Epoch 6551: train loss: 0.09782318025827408, val loss: 0.11176462471485138\n",
      "Epoch 6552: train loss: 0.09776207059621811, val loss: 0.1117129921913147\n",
      "Epoch 6553: train loss: 0.0977010503411293, val loss: 0.11166150867938995\n",
      "Epoch 6554: train loss: 0.09764008224010468, val loss: 0.11160998791456223\n",
      "Epoch 6555: train loss: 0.09757912904024124, val loss: 0.1115586906671524\n",
      "Epoch 6556: train loss: 0.09751830995082855, val loss: 0.11150746792554855\n",
      "Epoch 6557: train loss: 0.09745750576257706, val loss: 0.11145623028278351\n",
      "Epoch 6558: train loss: 0.09739682823419571, val loss: 0.11140507459640503\n",
      "Epoch 6559: train loss: 0.09733613580465317, val loss: 0.11135385185480118\n",
      "Epoch 6560: train loss: 0.09727552533149719, val loss: 0.11130273342132568\n",
      "Epoch 6561: train loss: 0.09721499681472778, val loss: 0.11125178635120392\n",
      "Epoch 6562: train loss: 0.09715452790260315, val loss: 0.1112007275223732\n",
      "Epoch 6563: train loss: 0.09709407389163971, val loss: 0.11114981025457382\n",
      "Epoch 6564: train loss: 0.09703371673822403, val loss: 0.11109901964664459\n",
      "Epoch 6565: train loss: 0.09697341173887253, val loss: 0.11104830354452133\n",
      "Epoch 6566: train loss: 0.09691318869590759, val loss: 0.11099763214588165\n",
      "Epoch 6567: train loss: 0.09685301780700684, val loss: 0.11094701290130615\n",
      "Epoch 6568: train loss: 0.09679286926984787, val loss: 0.11089644581079483\n",
      "Epoch 6569: train loss: 0.09673283994197845, val loss: 0.11084580421447754\n",
      "Epoch 6570: train loss: 0.09667284786701202, val loss: 0.11079533398151398\n",
      "Epoch 6571: train loss: 0.09661287069320679, val loss: 0.110744908452034\n",
      "Epoch 6572: train loss: 0.09655299782752991, val loss: 0.11069457978010178\n",
      "Epoch 6573: train loss: 0.09649316966533661, val loss: 0.11064429581165314\n",
      "Epoch 6574: train loss: 0.09643345326185226, val loss: 0.11059407144784927\n",
      "Epoch 6575: train loss: 0.0963737741112709, val loss: 0.11054392904043198\n",
      "Epoch 6576: train loss: 0.09631409496068954, val loss: 0.11049383133649826\n",
      "Epoch 6577: train loss: 0.09625454246997833, val loss: 0.11044378578662872\n",
      "Epoch 6578: train loss: 0.09619501233100891, val loss: 0.11039374023675919\n",
      "Epoch 6579: train loss: 0.09613554179668427, val loss: 0.1103438287973404\n",
      "Epoch 6580: train loss: 0.09607617557048798, val loss: 0.11029399931430817\n",
      "Epoch 6581: train loss: 0.09601682424545288, val loss: 0.11024420708417892\n",
      "Epoch 6582: train loss: 0.09595755487680435, val loss: 0.11019446700811386\n",
      "Epoch 6583: train loss: 0.0958983525633812, val loss: 0.11014475673437119\n",
      "Epoch 6584: train loss: 0.09583918750286102, val loss: 0.1100950762629509\n",
      "Epoch 6585: train loss: 0.09578011929988861, val loss: 0.11004552990198135\n",
      "Epoch 6586: train loss: 0.0957210436463356, val loss: 0.1099960207939148\n",
      "Epoch 6587: train loss: 0.09566210955381393, val loss: 0.10994663089513779\n",
      "Epoch 6588: train loss: 0.09560320526361465, val loss: 0.10989727824926376\n",
      "Epoch 6589: train loss: 0.09554431587457657, val loss: 0.10984790325164795\n",
      "Epoch 6590: train loss: 0.09548555314540863, val loss: 0.10979866236448288\n",
      "Epoch 6591: train loss: 0.09542681276798248, val loss: 0.10974941402673721\n",
      "Epoch 6592: train loss: 0.0953681468963623, val loss: 0.10970029979944229\n",
      "Epoch 6593: train loss: 0.0953095331788063, val loss: 0.10965120792388916\n",
      "Epoch 6594: train loss: 0.09525097161531448, val loss: 0.1096021756529808\n",
      "Epoch 6595: train loss: 0.09519248455762863, val loss: 0.10955321043729782\n",
      "Epoch 6596: train loss: 0.09513407200574875, val loss: 0.10950439423322678\n",
      "Epoch 6597: train loss: 0.09507567435503006, val loss: 0.10945554077625275\n",
      "Epoch 6598: train loss: 0.09501737356185913, val loss: 0.10940667241811752\n",
      "Epoch 6599: train loss: 0.09495912492275238, val loss: 0.10935795307159424\n",
      "Epoch 6600: train loss: 0.0949009358882904, val loss: 0.10930925607681274\n",
      "Epoch 6601: train loss: 0.094842828810215, val loss: 0.10926073044538498\n",
      "Epoch 6602: train loss: 0.09478471428155899, val loss: 0.10921216011047363\n",
      "Epoch 6603: train loss: 0.09472674131393433, val loss: 0.10916371643543243\n",
      "Epoch 6604: train loss: 0.09466877579689026, val loss: 0.10911530256271362\n",
      "Epoch 6605: train loss: 0.09461086243391037, val loss: 0.10906694084405899\n",
      "Epoch 6606: train loss: 0.09455303102731705, val loss: 0.10901854187250137\n",
      "Epoch 6607: train loss: 0.0944952443242073, val loss: 0.10897030681371689\n",
      "Epoch 6608: train loss: 0.09443756937980652, val loss: 0.10892214626073837\n",
      "Epoch 6609: train loss: 0.09437990188598633, val loss: 0.10887396335601807\n",
      "Epoch 6610: train loss: 0.09432227909564972, val loss: 0.10882585495710373\n",
      "Epoch 6611: train loss: 0.09426477551460266, val loss: 0.10877787321805954\n",
      "Epoch 6612: train loss: 0.0942072793841362, val loss: 0.10872995108366013\n",
      "Epoch 6613: train loss: 0.0941498801112175, val loss: 0.10868209600448608\n",
      "Epoch 6614: train loss: 0.09409251809120178, val loss: 0.10863427072763443\n",
      "Epoch 6615: train loss: 0.09403520077466965, val loss: 0.10858652740716934\n",
      "Epoch 6616: train loss: 0.09397799521684647, val loss: 0.10853873938322067\n",
      "Epoch 6617: train loss: 0.09392079710960388, val loss: 0.10849101841449738\n",
      "Epoch 6618: train loss: 0.09386365860700607, val loss: 0.10844341665506363\n",
      "Epoch 6619: train loss: 0.09380659461021423, val loss: 0.10839586704969406\n",
      "Epoch 6620: train loss: 0.09374959021806717, val loss: 0.10834848880767822\n",
      "Epoch 6621: train loss: 0.09369263052940369, val loss: 0.10830094665288925\n",
      "Epoch 6622: train loss: 0.09363575279712677, val loss: 0.10825367271900177\n",
      "Epoch 6623: train loss: 0.09357892721891403, val loss: 0.1082063540816307\n",
      "Epoch 6624: train loss: 0.09352216869592667, val loss: 0.108159139752388\n",
      "Epoch 6625: train loss: 0.09346543997526169, val loss: 0.10811193287372589\n",
      "Epoch 6626: train loss: 0.09340883046388626, val loss: 0.10806476324796677\n",
      "Epoch 6627: train loss: 0.09335223585367203, val loss: 0.10801766067743301\n",
      "Epoch 6628: train loss: 0.09329568594694138, val loss: 0.10797069221735\n",
      "Epoch 6629: train loss: 0.09323921799659729, val loss: 0.10792362689971924\n",
      "Epoch 6630: train loss: 0.09318280220031738, val loss: 0.10787677764892578\n",
      "Epoch 6631: train loss: 0.09312641620635986, val loss: 0.1078300029039383\n",
      "Epoch 6632: train loss: 0.0930701270699501, val loss: 0.10778322070837021\n",
      "Epoch 6633: train loss: 0.09301387518644333, val loss: 0.10773654282093048\n",
      "Epoch 6634: train loss: 0.09295771270990372, val loss: 0.10768982023000717\n",
      "Epoch 6635: train loss: 0.0929015725851059, val loss: 0.10764320939779282\n",
      "Epoch 6636: train loss: 0.09284549206495285, val loss: 0.10759659111499786\n",
      "Epoch 6637: train loss: 0.09278951585292816, val loss: 0.10755014419555664\n",
      "Epoch 6638: train loss: 0.09273355454206467, val loss: 0.1075037270784378\n",
      "Epoch 6639: train loss: 0.09267768263816833, val loss: 0.10745735466480255\n",
      "Epoch 6640: train loss: 0.09262184798717499, val loss: 0.1074109822511673\n",
      "Epoch 6641: train loss: 0.09256605803966522, val loss: 0.10736481100320816\n",
      "Epoch 6642: train loss: 0.09251036494970322, val loss: 0.10731853544712067\n",
      "Epoch 6643: train loss: 0.0924547016620636, val loss: 0.10727246850728989\n",
      "Epoch 6644: train loss: 0.09239911288022995, val loss: 0.10722636431455612\n",
      "Epoch 6645: train loss: 0.09234358370304108, val loss: 0.10718031972646713\n",
      "Epoch 6646: train loss: 0.0922880694270134, val loss: 0.10713441669940948\n",
      "Epoch 6647: train loss: 0.09223266690969467, val loss: 0.10708846151828766\n",
      "Epoch 6648: train loss: 0.09217730909585953, val loss: 0.10704252868890762\n",
      "Epoch 6649: train loss: 0.09212197363376617, val loss: 0.10699676722288132\n",
      "Epoch 6650: train loss: 0.09206675738096237, val loss: 0.1069510206580162\n",
      "Epoch 6651: train loss: 0.09201154112815857, val loss: 0.10690534114837646\n",
      "Epoch 6652: train loss: 0.09195641428232193, val loss: 0.10685978084802628\n",
      "Epoch 6653: train loss: 0.09190135449171066, val loss: 0.10681416839361191\n",
      "Epoch 6654: train loss: 0.09184630960226059, val loss: 0.10676868259906769\n",
      "Epoch 6655: train loss: 0.09179135411977768, val loss: 0.10672322660684586\n",
      "Epoch 6656: train loss: 0.09173644334077835, val loss: 0.1066778227686882\n",
      "Epoch 6657: train loss: 0.09168160706758499, val loss: 0.1066325232386589\n",
      "Epoch 6658: train loss: 0.091626837849617, val loss: 0.10658719390630722\n",
      "Epoch 6659: train loss: 0.0915720984339714, val loss: 0.10654199123382568\n",
      "Epoch 6660: train loss: 0.09151742607355118, val loss: 0.10649680346250534\n",
      "Epoch 6661: train loss: 0.09146281331777573, val loss: 0.10645172744989395\n",
      "Epoch 6662: train loss: 0.09140828996896744, val loss: 0.10640666633844376\n",
      "Epoch 6663: train loss: 0.09135378152132034, val loss: 0.10636168718338013\n",
      "Epoch 6664: train loss: 0.09129932522773743, val loss: 0.10631676763296127\n",
      "Epoch 6665: train loss: 0.09124495089054108, val loss: 0.10627184063196182\n",
      "Epoch 6666: train loss: 0.0911906287074089, val loss: 0.10622705519199371\n",
      "Epoch 6667: train loss: 0.09113634377717972, val loss: 0.10618231445550919\n",
      "Epoch 6668: train loss: 0.0910821333527565, val loss: 0.10613757371902466\n",
      "Epoch 6669: train loss: 0.09102796018123627, val loss: 0.10609292984008789\n",
      "Epoch 6670: train loss: 0.09097389131784439, val loss: 0.10604832321405411\n",
      "Epoch 6671: train loss: 0.0909198448061943, val loss: 0.10600385814905167\n",
      "Epoch 6672: train loss: 0.0908658355474472, val loss: 0.10595928877592087\n",
      "Epoch 6673: train loss: 0.09081192314624786, val loss: 0.10591491311788559\n",
      "Epoch 6674: train loss: 0.09075804054737091, val loss: 0.10587059706449509\n",
      "Epoch 6675: train loss: 0.09070426225662231, val loss: 0.1058262512087822\n",
      "Epoch 6676: train loss: 0.09065049886703491, val loss: 0.1057819277048111\n",
      "Epoch 6677: train loss: 0.0905967727303505, val loss: 0.10573773831129074\n",
      "Epoch 6678: train loss: 0.09054313600063324, val loss: 0.10569365322589874\n",
      "Epoch 6679: train loss: 0.09048956632614136, val loss: 0.10564952343702316\n",
      "Epoch 6680: train loss: 0.09043605625629425, val loss: 0.10560555756092072\n",
      "Epoch 6681: train loss: 0.09038259834051132, val loss: 0.1055615097284317\n",
      "Epoch 6682: train loss: 0.09032916277647018, val loss: 0.10551758110523224\n",
      "Epoch 6683: train loss: 0.09027580916881561, val loss: 0.10547371953725815\n",
      "Epoch 6684: train loss: 0.09022250771522522, val loss: 0.10543002188205719\n",
      "Epoch 6685: train loss: 0.09016929566860199, val loss: 0.10538625717163086\n",
      "Epoch 6686: train loss: 0.09011611342430115, val loss: 0.10534260421991348\n",
      "Epoch 6687: train loss: 0.09006296843290329, val loss: 0.10529892891645432\n",
      "Epoch 6688: train loss: 0.090009905397892, val loss: 0.10525534301996231\n",
      "Epoch 6689: train loss: 0.08995690196752548, val loss: 0.10521185398101807\n",
      "Epoch 6690: train loss: 0.08990395814180374, val loss: 0.10516836494207382\n",
      "Epoch 6691: train loss: 0.08985105156898499, val loss: 0.10512497276067734\n",
      "Epoch 6692: train loss: 0.08979818969964981, val loss: 0.10508161783218384\n",
      "Epoch 6693: train loss: 0.0897454172372818, val loss: 0.10503838211297989\n",
      "Epoch 6694: train loss: 0.08969268947839737, val loss: 0.10499509423971176\n",
      "Epoch 6695: train loss: 0.08964000642299652, val loss: 0.1049518808722496\n",
      "Epoch 6696: train loss: 0.08958738297224045, val loss: 0.10490868240594864\n",
      "Epoch 6697: train loss: 0.08953480422496796, val loss: 0.10486569255590439\n",
      "Epoch 6698: train loss: 0.08948232978582382, val loss: 0.10482263565063477\n",
      "Epoch 6699: train loss: 0.08942987769842148, val loss: 0.10477963835000992\n",
      "Epoch 6700: train loss: 0.08937745541334152, val loss: 0.10473678261041641\n",
      "Epoch 6701: train loss: 0.08932512253522873, val loss: 0.10469397157430649\n",
      "Epoch 6702: train loss: 0.08927284181118011, val loss: 0.10465114563703537\n",
      "Epoch 6703: train loss: 0.08922063559293747, val loss: 0.104608453810215\n",
      "Epoch 6704: train loss: 0.08916844427585602, val loss: 0.10456571727991104\n",
      "Epoch 6705: train loss: 0.08911633491516113, val loss: 0.1045231744647026\n",
      "Epoch 6706: train loss: 0.08906430006027222, val loss: 0.10448052734136581\n",
      "Epoch 6707: train loss: 0.0890122652053833, val loss: 0.10443798452615738\n",
      "Epoch 6708: train loss: 0.08896034955978394, val loss: 0.1043955609202385\n",
      "Epoch 6709: train loss: 0.08890847116708755, val loss: 0.10435314476490021\n",
      "Epoch 6710: train loss: 0.08885661512613297, val loss: 0.10431075096130371\n",
      "Epoch 6711: train loss: 0.08880484849214554, val loss: 0.10426846891641617\n",
      "Epoch 6712: train loss: 0.0887531191110611, val loss: 0.10422629117965698\n",
      "Epoch 6713: train loss: 0.08870146423578262, val loss: 0.1041841134428978\n",
      "Epoch 6714: train loss: 0.08864986896514893, val loss: 0.1041419506072998\n",
      "Epoch 6715: train loss: 0.08859828114509583, val loss: 0.10409989207983017\n",
      "Epoch 6716: train loss: 0.08854680508375168, val loss: 0.1040579304099083\n",
      "Epoch 6717: train loss: 0.08849536627531052, val loss: 0.10401593893766403\n",
      "Epoch 6718: train loss: 0.08844399452209473, val loss: 0.10397394746541977\n",
      "Epoch 6719: train loss: 0.08839267492294312, val loss: 0.10393213480710983\n",
      "Epoch 6720: train loss: 0.08834138512611389, val loss: 0.10389038175344467\n",
      "Epoch 6721: train loss: 0.08829017728567123, val loss: 0.10384862869977951\n",
      "Epoch 6722: train loss: 0.08823902904987335, val loss: 0.10380697250366211\n",
      "Epoch 6723: train loss: 0.08818791061639786, val loss: 0.1037653312087059\n",
      "Epoch 6724: train loss: 0.08813685923814774, val loss: 0.10372375696897507\n",
      "Epoch 6725: train loss: 0.0880858525633812, val loss: 0.10368227958679199\n",
      "Epoch 6726: train loss: 0.08803492039442062, val loss: 0.10364075005054474\n",
      "Epoch 6727: train loss: 0.08798402547836304, val loss: 0.10359938442707062\n",
      "Epoch 6728: train loss: 0.08793322741985321, val loss: 0.10355804115533829\n",
      "Epoch 6729: train loss: 0.08788244426250458, val loss: 0.10351665318012238\n",
      "Epoch 6730: train loss: 0.08783173561096191, val loss: 0.10347535461187363\n",
      "Epoch 6731: train loss: 0.08778104931116104, val loss: 0.10343419760465622\n",
      "Epoch 6732: train loss: 0.08773044496774673, val loss: 0.10339313745498657\n",
      "Epoch 6733: train loss: 0.0876799076795578, val loss: 0.10335200279951096\n",
      "Epoch 6734: train loss: 0.08762942999601364, val loss: 0.10331099480390549\n",
      "Epoch 6735: train loss: 0.08757895976305008, val loss: 0.1032700315117836\n",
      "Epoch 6736: train loss: 0.08752857893705368, val loss: 0.10322914272546768\n",
      "Epoch 6737: train loss: 0.08747823536396027, val loss: 0.103188157081604\n",
      "Epoch 6738: train loss: 0.08742798864841461, val loss: 0.10314738005399704\n",
      "Epoch 6739: train loss: 0.08737774938344955, val loss: 0.10310661792755127\n",
      "Epoch 6740: train loss: 0.08732759207487106, val loss: 0.10306598991155624\n",
      "Epoch 6741: train loss: 0.08727748692035675, val loss: 0.10302529484033585\n",
      "Epoch 6742: train loss: 0.08722741901874542, val loss: 0.10298478603363037\n",
      "Epoch 6743: train loss: 0.08717744052410126, val loss: 0.10294421017169952\n",
      "Epoch 6744: train loss: 0.08712748438119888, val loss: 0.10290365666151047\n",
      "Epoch 6745: train loss: 0.0870775654911995, val loss: 0.10286330431699753\n",
      "Epoch 6746: train loss: 0.08702775090932846, val loss: 0.10282289236783981\n",
      "Epoch 6747: train loss: 0.08697795867919922, val loss: 0.10278262943029404\n",
      "Epoch 6748: train loss: 0.08692825585603714, val loss: 0.10274229198694229\n",
      "Epoch 6749: train loss: 0.08687856793403625, val loss: 0.10270202159881592\n",
      "Epoch 6750: train loss: 0.08682893216609955, val loss: 0.10266193002462387\n",
      "Epoch 6751: train loss: 0.08677937090396881, val loss: 0.10262178629636765\n",
      "Epoch 6752: train loss: 0.08672987669706345, val loss: 0.10258173942565918\n",
      "Epoch 6753: train loss: 0.08668044209480286, val loss: 0.10254179686307907\n",
      "Epoch 6754: train loss: 0.08663103729486465, val loss: 0.10250180214643478\n",
      "Epoch 6755: train loss: 0.08658167719841003, val loss: 0.10246193408966064\n",
      "Epoch 6756: train loss: 0.08653238415718079, val loss: 0.10242205113172531\n",
      "Epoch 6757: train loss: 0.08648312836885452, val loss: 0.10238226503133774\n",
      "Epoch 6758: train loss: 0.08643396198749542, val loss: 0.10234260559082031\n",
      "Epoch 6759: train loss: 0.0863848403096199, val loss: 0.10230284929275513\n",
      "Epoch 6760: train loss: 0.08633577078580856, val loss: 0.10226324945688248\n",
      "Epoch 6761: train loss: 0.0862867459654808, val loss: 0.1022237166762352\n",
      "Epoch 6762: train loss: 0.08623776584863663, val loss: 0.1021842509508133\n",
      "Epoch 6763: train loss: 0.08618886768817902, val loss: 0.1021447703242302\n",
      "Epoch 6764: train loss: 0.08614002168178558, val loss: 0.10210521519184113\n",
      "Epoch 6765: train loss: 0.08609119057655334, val loss: 0.10206592082977295\n",
      "Epoch 6766: train loss: 0.08604244887828827, val loss: 0.10202660411596298\n",
      "Epoch 6767: train loss: 0.08599373698234558, val loss: 0.10198745876550674\n",
      "Epoch 6768: train loss: 0.08594511449337006, val loss: 0.10194816440343857\n",
      "Epoch 6769: train loss: 0.08589651435613632, val loss: 0.10190912336111069\n",
      "Epoch 6770: train loss: 0.08584799617528915, val loss: 0.10187002271413803\n",
      "Epoch 6771: train loss: 0.08579950779676437, val loss: 0.1018308773636818\n",
      "Epoch 6772: train loss: 0.08575106412172318, val loss: 0.10179194062948227\n",
      "Epoch 6773: train loss: 0.08570270985364914, val loss: 0.10175297409296036\n",
      "Epoch 6774: train loss: 0.08565440773963928, val loss: 0.1017141118645668\n",
      "Epoch 6775: train loss: 0.08560612052679062, val loss: 0.10167520493268967\n",
      "Epoch 6776: train loss: 0.08555790781974792, val loss: 0.10163650661706924\n",
      "Epoch 6777: train loss: 0.08550973981618881, val loss: 0.10159778594970703\n",
      "Epoch 6778: train loss: 0.08546167612075806, val loss: 0.10155916213989258\n",
      "Epoch 6779: train loss: 0.08541359752416611, val loss: 0.1015205904841423\n",
      "Epoch 6780: train loss: 0.08536563813686371, val loss: 0.10148205608129501\n",
      "Epoch 6781: train loss: 0.0853177011013031, val loss: 0.10144344717264175\n",
      "Epoch 6782: train loss: 0.08526979386806488, val loss: 0.10140497982501984\n",
      "Epoch 6783: train loss: 0.08522196114063263, val loss: 0.10136666148900986\n",
      "Epoch 6784: train loss: 0.08517418801784515, val loss: 0.10132820904254913\n",
      "Epoch 6785: train loss: 0.08512647449970245, val loss: 0.10128996521234512\n",
      "Epoch 6786: train loss: 0.08507879823446274, val loss: 0.10125173628330231\n",
      "Epoch 6787: train loss: 0.08503115922212601, val loss: 0.10121357440948486\n",
      "Epoch 6788: train loss: 0.08498362451791763, val loss: 0.10117547959089279\n",
      "Epoch 6789: train loss: 0.08493610471487045, val loss: 0.10113730281591415\n",
      "Epoch 6790: train loss: 0.08488865941762924, val loss: 0.10109926760196686\n",
      "Epoch 6791: train loss: 0.08484125137329102, val loss: 0.10106124728918076\n",
      "Epoch 6792: train loss: 0.08479389548301697, val loss: 0.10102329403162003\n",
      "Epoch 6793: train loss: 0.0847465842962265, val loss: 0.10098548978567123\n",
      "Epoch 6794: train loss: 0.08469933271408081, val loss: 0.10094770044088364\n",
      "Epoch 6795: train loss: 0.08465214818716049, val loss: 0.10090992599725723\n",
      "Epoch 6796: train loss: 0.08460503071546555, val loss: 0.1008722186088562\n",
      "Epoch 6797: train loss: 0.08455790579319, val loss: 0.10083450376987457\n",
      "Epoch 6798: train loss: 0.08451087027788162, val loss: 0.10079687088727951\n",
      "Epoch 6799: train loss: 0.08446388691663742, val loss: 0.1007593423128128\n",
      "Epoch 6800: train loss: 0.08441698551177979, val loss: 0.10072179138660431\n",
      "Epoch 6801: train loss: 0.08437010645866394, val loss: 0.10068430006504059\n",
      "Epoch 6802: train loss: 0.08432324975728989, val loss: 0.10064691305160522\n",
      "Epoch 6803: train loss: 0.08427650481462479, val loss: 0.10060956329107285\n",
      "Epoch 6804: train loss: 0.08422976732254028, val loss: 0.10057232528924942\n",
      "Epoch 6805: train loss: 0.08418312668800354, val loss: 0.10053505748510361\n",
      "Epoch 6806: train loss: 0.08413650095462799, val loss: 0.10049786418676376\n",
      "Epoch 6807: train loss: 0.08408995717763901, val loss: 0.10046068578958511\n",
      "Epoch 6808: train loss: 0.08404345065355301, val loss: 0.10042355209589005\n",
      "Epoch 6809: train loss: 0.0839969739317894, val loss: 0.10038650035858154\n",
      "Epoch 6810: train loss: 0.08395060896873474, val loss: 0.10034950077533722\n",
      "Epoch 6811: train loss: 0.08390425890684128, val loss: 0.10031258314847946\n",
      "Epoch 6812: train loss: 0.083857960999012, val loss: 0.10027575492858887\n",
      "Epoch 6813: train loss: 0.08381173014640808, val loss: 0.10023897886276245\n",
      "Epoch 6814: train loss: 0.08376550674438477, val loss: 0.10020215809345245\n",
      "Epoch 6815: train loss: 0.08371938765048981, val loss: 0.10016540437936783\n",
      "Epoch 6816: train loss: 0.08367330580949783, val loss: 0.10012870281934738\n",
      "Epoch 6817: train loss: 0.08362726867198944, val loss: 0.1000920906662941\n",
      "Epoch 6818: train loss: 0.08358129858970642, val loss: 0.10005549341440201\n",
      "Epoch 6819: train loss: 0.08353536576032639, val loss: 0.10001897811889648\n",
      "Epoch 6820: train loss: 0.08348948508501053, val loss: 0.09998250007629395\n",
      "Epoch 6821: train loss: 0.08344365656375885, val loss: 0.09994611889123917\n",
      "Epoch 6822: train loss: 0.08339790254831314, val loss: 0.0999097153544426\n",
      "Epoch 6823: train loss: 0.08335217833518982, val loss: 0.099873386323452\n",
      "Epoch 6824: train loss: 0.08330649137496948, val loss: 0.09983707964420319\n",
      "Epoch 6825: train loss: 0.08326087892055511, val loss: 0.09980088472366333\n",
      "Epoch 6826: train loss: 0.08321531116962433, val loss: 0.09976466745138168\n",
      "Epoch 6827: train loss: 0.08316978812217712, val loss: 0.0997285470366478\n",
      "Epoch 6828: train loss: 0.08312433958053589, val loss: 0.09969251602888107\n",
      "Epoch 6829: train loss: 0.08307895809412003, val loss: 0.09965649992227554\n",
      "Epoch 6830: train loss: 0.08303359895944595, val loss: 0.09962053596973419\n",
      "Epoch 6831: train loss: 0.08298828452825546, val loss: 0.09958463162183762\n",
      "Epoch 6832: train loss: 0.08294305205345154, val loss: 0.09954869002103806\n",
      "Epoch 6833: train loss: 0.08289782702922821, val loss: 0.09951292723417282\n",
      "Epoch 6834: train loss: 0.08285270631313324, val loss: 0.09947719424962997\n",
      "Epoch 6835: train loss: 0.08280760794878006, val loss: 0.09944146126508713\n",
      "Epoch 6836: train loss: 0.08276255428791046, val loss: 0.09940578788518906\n",
      "Epoch 6837: train loss: 0.08271756023168564, val loss: 0.09937016665935516\n",
      "Epoch 6838: train loss: 0.0826726108789444, val loss: 0.09933467954397202\n",
      "Epoch 6839: train loss: 0.08262773603200912, val loss: 0.09929915517568588\n",
      "Epoch 6840: train loss: 0.08258289098739624, val loss: 0.09926366060972214\n",
      "Epoch 6841: train loss: 0.08253808319568634, val loss: 0.09922830015420914\n",
      "Epoch 6842: train loss: 0.08249334990978241, val loss: 0.09919289499521255\n",
      "Epoch 6843: train loss: 0.08244865387678146, val loss: 0.09915758669376373\n",
      "Epoch 6844: train loss: 0.08240403980016708, val loss: 0.09912224858999252\n",
      "Epoch 6845: train loss: 0.0823594406247139, val loss: 0.09908708184957504\n",
      "Epoch 6846: train loss: 0.08231493830680847, val loss: 0.09905198216438293\n",
      "Epoch 6847: train loss: 0.08227046579122543, val loss: 0.09901692718267441\n",
      "Epoch 6848: train loss: 0.08222602307796478, val loss: 0.09898184984922409\n",
      "Epoch 6849: train loss: 0.0821816548705101, val loss: 0.09894680231809616\n",
      "Epoch 6850: train loss: 0.0821373239159584, val loss: 0.09891187399625778\n",
      "Epoch 6851: train loss: 0.08209305256605148, val loss: 0.09887697547674179\n",
      "Epoch 6852: train loss: 0.08204885572195053, val loss: 0.09884200990200043\n",
      "Epoch 6853: train loss: 0.08200467377901077, val loss: 0.09880725294351578\n",
      "Epoch 6854: train loss: 0.08196055889129639, val loss: 0.09877249598503113\n",
      "Epoch 6855: train loss: 0.0819164589047432, val loss: 0.09873783588409424\n",
      "Epoch 6856: train loss: 0.08187245577573776, val loss: 0.09870320558547974\n",
      "Epoch 6857: train loss: 0.08182848244905472, val loss: 0.09866863489151001\n",
      "Epoch 6858: train loss: 0.08178458362817764, val loss: 0.09863406419754028\n",
      "Epoch 6859: train loss: 0.08174072951078415, val loss: 0.09859956800937653\n",
      "Epoch 6860: train loss: 0.08169691264629364, val loss: 0.09856504946947098\n",
      "Epoch 6861: train loss: 0.0816531553864479, val loss: 0.0985306054353714\n",
      "Epoch 6862: train loss: 0.08160943537950516, val loss: 0.09849627315998077\n",
      "Epoch 6863: train loss: 0.08156580477952957, val loss: 0.09846196323633194\n",
      "Epoch 6864: train loss: 0.08152217417955399, val loss: 0.09842772781848907\n",
      "Epoch 6865: train loss: 0.08147859573364258, val loss: 0.09839355200529099\n",
      "Epoch 6866: train loss: 0.08143510669469833, val loss: 0.09835942089557648\n",
      "Epoch 6867: train loss: 0.08139164000749588, val loss: 0.09832528978586197\n",
      "Epoch 6868: train loss: 0.08134825527667999, val loss: 0.09829121083021164\n",
      "Epoch 6869: train loss: 0.0813048779964447, val loss: 0.09825711697340012\n",
      "Epoch 6870: train loss: 0.08126156032085419, val loss: 0.09822320193052292\n",
      "Epoch 6871: train loss: 0.08121829479932785, val loss: 0.09818928688764572\n",
      "Epoch 6872: train loss: 0.0811750739812851, val loss: 0.09815549105405807\n",
      "Epoch 6873: train loss: 0.0811319500207901, val loss: 0.09812171757221222\n",
      "Epoch 6874: train loss: 0.0810888260602951, val loss: 0.09808792173862457\n",
      "Epoch 6875: train loss: 0.08104579150676727, val loss: 0.09805414825677872\n",
      "Epoch 6876: train loss: 0.08100278675556183, val loss: 0.09802046418190002\n",
      "Epoch 6877: train loss: 0.08095981180667877, val loss: 0.09798683971166611\n",
      "Epoch 6878: train loss: 0.08091691881418228, val loss: 0.09795334935188293\n",
      "Epoch 6879: train loss: 0.08087407052516937, val loss: 0.09791984409093857\n",
      "Epoch 6880: train loss: 0.08083127439022064, val loss: 0.09788639843463898\n",
      "Epoch 6881: train loss: 0.08078853040933609, val loss: 0.09785296767950058\n",
      "Epoch 6882: train loss: 0.08074580132961273, val loss: 0.09781958162784576\n",
      "Epoch 6883: train loss: 0.08070316165685654, val loss: 0.09778620302677155\n",
      "Epoch 6884: train loss: 0.08066052198410034, val loss: 0.09775294363498688\n",
      "Epoch 6885: train loss: 0.0806179940700531, val loss: 0.0977197214961052\n",
      "Epoch 6886: train loss: 0.08057548105716705, val loss: 0.0976865217089653\n",
      "Epoch 6887: train loss: 0.08053305000066757, val loss: 0.09765344858169556\n",
      "Epoch 6888: train loss: 0.08049064129590988, val loss: 0.09762036055326462\n",
      "Epoch 6889: train loss: 0.08044829219579697, val loss: 0.09758735448122025\n",
      "Epoch 6890: train loss: 0.08040597289800644, val loss: 0.09755434840917587\n",
      "Epoch 6891: train loss: 0.08036372810602188, val loss: 0.0975213348865509\n",
      "Epoch 6892: train loss: 0.0803215280175209, val loss: 0.09748851507902145\n",
      "Epoch 6893: train loss: 0.0802793800830841, val loss: 0.09745568037033081\n",
      "Epoch 6894: train loss: 0.08023727685213089, val loss: 0.09742286801338196\n",
      "Epoch 6895: train loss: 0.08019524067640305, val loss: 0.09739022701978683\n",
      "Epoch 6896: train loss: 0.08015322685241699, val loss: 0.09735754877328873\n",
      "Epoch 6897: train loss: 0.08011128008365631, val loss: 0.09732488542795181\n",
      "Epoch 6898: train loss: 0.08006937056779861, val loss: 0.09729234874248505\n",
      "Epoch 6899: train loss: 0.08002752810716629, val loss: 0.0972597748041153\n",
      "Epoch 6900: train loss: 0.07998570799827576, val loss: 0.09722723811864853\n",
      "Epoch 6901: train loss: 0.0799439325928688, val loss: 0.09719479084014893\n",
      "Epoch 6902: train loss: 0.07990223914384842, val loss: 0.0971623882651329\n",
      "Epoch 6903: train loss: 0.07986057549715042, val loss: 0.09713008254766464\n",
      "Epoch 6904: train loss: 0.0798189640045166, val loss: 0.09709777683019638\n",
      "Epoch 6905: train loss: 0.07977741956710815, val loss: 0.09706556051969528\n",
      "Epoch 6906: train loss: 0.07973591983318329, val loss: 0.09703335911035538\n",
      "Epoch 6907: train loss: 0.07969445735216141, val loss: 0.09700118750333786\n",
      "Epoch 6908: train loss: 0.07965303957462311, val loss: 0.09696903079748154\n",
      "Epoch 6909: train loss: 0.07961168140172958, val loss: 0.0969369038939476\n",
      "Epoch 6910: train loss: 0.07957036793231964, val loss: 0.0969049260020256\n",
      "Epoch 6911: train loss: 0.07952910661697388, val loss: 0.09687294811010361\n",
      "Epoch 6912: train loss: 0.0794878900051117, val loss: 0.09684107452630997\n",
      "Epoch 6913: train loss: 0.0794467180967331, val loss: 0.0968092530965805\n",
      "Epoch 6914: train loss: 0.07940561324357986, val loss: 0.09677740186452866\n",
      "Epoch 6915: train loss: 0.07936453819274902, val loss: 0.09674563258886337\n",
      "Epoch 6916: train loss: 0.07932350784540176, val loss: 0.09671390056610107\n",
      "Epoch 6917: train loss: 0.07928255200386047, val loss: 0.09668218344449997\n",
      "Epoch 6918: train loss: 0.07924164086580276, val loss: 0.09665060043334961\n",
      "Epoch 6919: train loss: 0.07920075953006744, val loss: 0.09661903977394104\n",
      "Epoch 6920: train loss: 0.0791599377989769, val loss: 0.09658746421337128\n",
      "Epoch 6921: train loss: 0.07911916822195053, val loss: 0.09655597805976868\n",
      "Epoch 6922: train loss: 0.07907842844724655, val loss: 0.09652452915906906\n",
      "Epoch 6923: train loss: 0.07903775572776794, val loss: 0.09649311751127243\n",
      "Epoch 6924: train loss: 0.07899711281061172, val loss: 0.09646180272102356\n",
      "Epoch 6925: train loss: 0.07895656675100327, val loss: 0.0964304730296135\n",
      "Epoch 6926: train loss: 0.078916035592556, val loss: 0.09639918059110641\n",
      "Epoch 6927: train loss: 0.07887554913759232, val loss: 0.09636807441711426\n",
      "Epoch 6928: train loss: 0.07883509993553162, val loss: 0.09633686393499374\n",
      "Epoch 6929: train loss: 0.07879471778869629, val loss: 0.09630582481622696\n",
      "Epoch 6930: train loss: 0.07875439524650574, val loss: 0.096274733543396\n",
      "Epoch 6931: train loss: 0.07871410995721817, val loss: 0.0962437242269516\n",
      "Epoch 6932: train loss: 0.07867388427257538, val loss: 0.09621274471282959\n",
      "Epoch 6933: train loss: 0.07863368839025497, val loss: 0.09618180245161057\n",
      "Epoch 6934: train loss: 0.07859354466199875, val loss: 0.09615099430084229\n",
      "Epoch 6935: train loss: 0.0785534456372261, val loss: 0.09612011164426804\n",
      "Epoch 6936: train loss: 0.07851341366767883, val loss: 0.09608931839466095\n",
      "Epoch 6937: train loss: 0.07847343385219574, val loss: 0.09605857729911804\n",
      "Epoch 6938: train loss: 0.07843346148729324, val loss: 0.09602784365415573\n",
      "Epoch 6939: train loss: 0.07839354872703552, val loss: 0.09599726647138596\n",
      "Epoch 6940: train loss: 0.07835370302200317, val loss: 0.09596671164035797\n",
      "Epoch 6941: train loss: 0.07831387966871262, val loss: 0.09593610465526581\n",
      "Epoch 6942: train loss: 0.07827414572238922, val loss: 0.09590563923120499\n",
      "Epoch 6943: train loss: 0.07823444157838821, val loss: 0.09587524086236954\n",
      "Epoch 6944: train loss: 0.07819478958845139, val loss: 0.0958448126912117\n",
      "Epoch 6945: train loss: 0.07815516740083694, val loss: 0.09581442177295685\n",
      "Epoch 6946: train loss: 0.07811558246612549, val loss: 0.09578409045934677\n",
      "Epoch 6947: train loss: 0.0780760869383812, val loss: 0.09575387835502625\n",
      "Epoch 6948: train loss: 0.0780365839600563, val loss: 0.09572362154722214\n",
      "Epoch 6949: train loss: 0.07799718528985977, val loss: 0.09569350630044937\n",
      "Epoch 6950: train loss: 0.07795781642198563, val loss: 0.09566336125135422\n",
      "Epoch 6951: train loss: 0.07791850715875626, val loss: 0.09563321620225906\n",
      "Epoch 6952: train loss: 0.07787922024726868, val loss: 0.09560323506593704\n",
      "Epoch 6953: train loss: 0.07783997058868408, val loss: 0.09557322412729263\n",
      "Epoch 6954: train loss: 0.07780079543590546, val loss: 0.0955432876944542\n",
      "Epoch 6955: train loss: 0.07776165753602982, val loss: 0.09551339596509933\n",
      "Epoch 6956: train loss: 0.07772258669137955, val loss: 0.09548359364271164\n",
      "Epoch 6957: train loss: 0.07768353819847107, val loss: 0.09545372426509857\n",
      "Epoch 6958: train loss: 0.07764455676078796, val loss: 0.09542402625083923\n",
      "Epoch 6959: train loss: 0.07760562002658844, val loss: 0.09539423882961273\n",
      "Epoch 6960: train loss: 0.0775667130947113, val loss: 0.09536460787057877\n",
      "Epoch 6961: train loss: 0.07752787321805954, val loss: 0.0953349843621254\n",
      "Epoch 6962: train loss: 0.07748907059431076, val loss: 0.095305435359478\n",
      "Epoch 6963: train loss: 0.07745031267404556, val loss: 0.09527590125799179\n",
      "Epoch 6964: train loss: 0.07741161435842514, val loss: 0.09524647146463394\n",
      "Epoch 6965: train loss: 0.0773729532957077, val loss: 0.09521704167127609\n",
      "Epoch 6966: train loss: 0.07733434438705444, val loss: 0.09518762677907944\n",
      "Epoch 6967: train loss: 0.07729577273130417, val loss: 0.09515827894210815\n",
      "Epoch 6968: train loss: 0.07725725322961807, val loss: 0.09512896835803986\n",
      "Epoch 6969: train loss: 0.07721877843141556, val loss: 0.09509963542222977\n",
      "Epoch 6970: train loss: 0.07718037813901901, val loss: 0.09507047384977341\n",
      "Epoch 6971: train loss: 0.07714198529720306, val loss: 0.095041424036026\n",
      "Epoch 6972: train loss: 0.07710367441177368, val loss: 0.09501229971647263\n",
      "Epoch 6973: train loss: 0.07706539332866669, val loss: 0.0949833020567894\n",
      "Epoch 6974: train loss: 0.07702714949846268, val loss: 0.09495417773723602\n",
      "Epoch 6975: train loss: 0.07698897272348404, val loss: 0.09492512792348862\n",
      "Epoch 6976: train loss: 0.07695081830024719, val loss: 0.09489608556032181\n",
      "Epoch 6977: train loss: 0.07691272348165512, val loss: 0.09486733376979828\n",
      "Epoch 6978: train loss: 0.07687468826770782, val loss: 0.09483849257230759\n",
      "Epoch 6979: train loss: 0.07683669030666351, val loss: 0.09480985254049301\n",
      "Epoch 6980: train loss: 0.07679874449968338, val loss: 0.09478107839822769\n",
      "Epoch 6981: train loss: 0.07676082104444504, val loss: 0.09475238621234894\n",
      "Epoch 6982: train loss: 0.07672296464443207, val loss: 0.09472359716892242\n",
      "Epoch 6983: train loss: 0.07668515294790268, val loss: 0.09469498693943024\n",
      "Epoch 6984: train loss: 0.07664738595485687, val loss: 0.09466637670993805\n",
      "Epoch 6985: train loss: 0.07660967111587524, val loss: 0.09463787078857422\n",
      "Epoch 6986: train loss: 0.0765720084309578, val loss: 0.09460940212011337\n",
      "Epoch 6987: train loss: 0.07653439044952393, val loss: 0.09458097070455551\n",
      "Epoch 6988: train loss: 0.07649677991867065, val loss: 0.09455268830060959\n",
      "Epoch 6989: train loss: 0.07645925879478455, val loss: 0.09452435374259949\n",
      "Epoch 6990: train loss: 0.07642176747322083, val loss: 0.09449601173400879\n",
      "Epoch 6991: train loss: 0.07638432830572128, val loss: 0.09446772187948227\n",
      "Epoch 6992: train loss: 0.07634693384170532, val loss: 0.09443943947553635\n",
      "Epoch 6993: train loss: 0.07630959898233414, val loss: 0.09441127628087997\n",
      "Epoch 6994: train loss: 0.07627230137586594, val loss: 0.09438319504261017\n",
      "Epoch 6995: train loss: 0.07623501867055893, val loss: 0.09435514360666275\n",
      "Epoch 6996: train loss: 0.07619782537221909, val loss: 0.09432714432477951\n",
      "Epoch 6997: train loss: 0.07616064697504044, val loss: 0.09429913759231567\n",
      "Epoch 6998: train loss: 0.07612353563308716, val loss: 0.094271220266819\n",
      "Epoch 6999: train loss: 0.07608646154403687, val loss: 0.09424331039190292\n",
      "Epoch 7000: train loss: 0.07604943960905075, val loss: 0.09421544522047043\n",
      "Epoch 7001: train loss: 0.07601247727870941, val loss: 0.0941876694560051\n",
      "Epoch 7002: train loss: 0.07597552239894867, val loss: 0.09415985643863678\n",
      "Epoch 7003: train loss: 0.07593865692615509, val loss: 0.09413214772939682\n",
      "Epoch 7004: train loss: 0.07590178400278091, val loss: 0.09410443156957626\n",
      "Epoch 7005: train loss: 0.0758650079369545, val loss: 0.09407687932252884\n",
      "Epoch 7006: train loss: 0.07582825422286987, val loss: 0.09404930472373962\n",
      "Epoch 7007: train loss: 0.07579156011343002, val loss: 0.09402178227901459\n",
      "Epoch 7008: train loss: 0.07575489580631256, val loss: 0.09399423003196716\n",
      "Epoch 7009: train loss: 0.07571827620267868, val loss: 0.09396683424711227\n",
      "Epoch 7010: train loss: 0.07568172365427017, val loss: 0.0939393937587738\n",
      "Epoch 7011: train loss: 0.07564518600702286, val loss: 0.09391206502914429\n",
      "Epoch 7012: train loss: 0.07560872286558151, val loss: 0.09388472884893417\n",
      "Epoch 7013: train loss: 0.07557228952646255, val loss: 0.09385737031698227\n",
      "Epoch 7014: train loss: 0.07553590834140778, val loss: 0.0938301607966423\n",
      "Epoch 7015: train loss: 0.07549957931041718, val loss: 0.09380289167165756\n",
      "Epoch 7016: train loss: 0.07546327263116837, val loss: 0.09377582371234894\n",
      "Epoch 7017: train loss: 0.07542702555656433, val loss: 0.09374874830245972\n",
      "Epoch 7018: train loss: 0.07539080828428268, val loss: 0.09372171014547348\n",
      "Epoch 7019: train loss: 0.07535465061664581, val loss: 0.09369472414255142\n",
      "Epoch 7020: train loss: 0.07531855255365372, val loss: 0.09366770088672638\n",
      "Epoch 7021: train loss: 0.0752824917435646, val loss: 0.0936407670378685\n",
      "Epoch 7022: train loss: 0.07524648308753967, val loss: 0.09361390024423599\n",
      "Epoch 7023: train loss: 0.07521048933267593, val loss: 0.0935870036482811\n",
      "Epoch 7024: train loss: 0.07517457008361816, val loss: 0.09356018900871277\n",
      "Epoch 7025: train loss: 0.07513867318630219, val loss: 0.09353351593017578\n",
      "Epoch 7026: train loss: 0.07510282099246979, val loss: 0.0935068130493164\n",
      "Epoch 7027: train loss: 0.07506702095270157, val loss: 0.09348016232252121\n",
      "Epoch 7028: train loss: 0.07503129541873932, val loss: 0.09345350414514542\n",
      "Epoch 7029: train loss: 0.07499556988477707, val loss: 0.09342699497938156\n",
      "Epoch 7030: train loss: 0.07495994120836258, val loss: 0.09340035170316696\n",
      "Epoch 7031: train loss: 0.07492433488368988, val loss: 0.0933738499879837\n",
      "Epoch 7032: train loss: 0.07488874346017838, val loss: 0.09334736317396164\n",
      "Epoch 7033: train loss: 0.07485321909189224, val loss: 0.09332101792097092\n",
      "Epoch 7034: train loss: 0.07481774687767029, val loss: 0.09329473227262497\n",
      "Epoch 7035: train loss: 0.07478231936693192, val loss: 0.09326841682195663\n",
      "Epoch 7036: train loss: 0.07474693655967712, val loss: 0.09324214607477188\n",
      "Epoch 7037: train loss: 0.07471159845590591, val loss: 0.09321587532758713\n",
      "Epoch 7038: train loss: 0.07467629015445709, val loss: 0.09318966418504715\n",
      "Epoch 7039: train loss: 0.07464100420475006, val loss: 0.09316348284482956\n",
      "Epoch 7040: train loss: 0.07460582256317139, val loss: 0.09313740581274033\n",
      "Epoch 7041: train loss: 0.07457064092159271, val loss: 0.09311135113239288\n",
      "Epoch 7042: train loss: 0.0745355412364006, val loss: 0.09308532625436783\n",
      "Epoch 7043: train loss: 0.07450045645236969, val loss: 0.09305945038795471\n",
      "Epoch 7044: train loss: 0.07446544617414474, val loss: 0.09303347021341324\n",
      "Epoch 7045: train loss: 0.0744304433465004, val loss: 0.09300762414932251\n",
      "Epoch 7046: train loss: 0.07439550012350082, val loss: 0.09298175573348999\n",
      "Epoch 7047: train loss: 0.07436061650514603, val loss: 0.0929558053612709\n",
      "Epoch 7048: train loss: 0.07432574033737183, val loss: 0.0929301306605339\n",
      "Epoch 7049: train loss: 0.074290931224823, val loss: 0.09290441125631332\n",
      "Epoch 7050: train loss: 0.07425616681575775, val loss: 0.09287876635789871\n",
      "Epoch 7051: train loss: 0.07422143965959549, val loss: 0.0928531214594841\n",
      "Epoch 7052: train loss: 0.0741867646574974, val loss: 0.09282756596803665\n",
      "Epoch 7053: train loss: 0.0741521567106247, val loss: 0.09280204027891159\n",
      "Epoch 7054: train loss: 0.07411758601665497, val loss: 0.09277651458978653\n",
      "Epoch 7055: train loss: 0.07408300787210464, val loss: 0.09275100380182266\n",
      "Epoch 7056: train loss: 0.0740484967827797, val loss: 0.09272566437721252\n",
      "Epoch 7057: train loss: 0.07401403784751892, val loss: 0.09270027279853821\n",
      "Epoch 7058: train loss: 0.07397965341806412, val loss: 0.09267493337392807\n",
      "Epoch 7059: train loss: 0.07394526153802872, val loss: 0.09264964610338211\n",
      "Epoch 7060: train loss: 0.07391097396612167, val loss: 0.09262438863515854\n",
      "Epoch 7061: train loss: 0.07387668639421463, val loss: 0.09259921312332153\n",
      "Epoch 7062: train loss: 0.07384245097637177, val loss: 0.09257396310567856\n",
      "Epoch 7063: train loss: 0.07380826026201248, val loss: 0.09254887700080872\n",
      "Epoch 7064: train loss: 0.07377408444881439, val loss: 0.09252380579710007\n",
      "Epoch 7065: train loss: 0.07373999804258347, val loss: 0.09249874204397202\n",
      "Epoch 7066: train loss: 0.07370592653751373, val loss: 0.09247378259897232\n",
      "Epoch 7067: train loss: 0.07367192953824997, val loss: 0.09244882315397263\n",
      "Epoch 7068: train loss: 0.073637954890728, val loss: 0.09242389351129532\n",
      "Epoch 7069: train loss: 0.0736040323972702, val loss: 0.0923989862203598\n",
      "Epoch 7070: train loss: 0.07357016950845718, val loss: 0.09237411618232727\n",
      "Epoch 7071: train loss: 0.07353629916906357, val loss: 0.09234940260648727\n",
      "Epoch 7072: train loss: 0.07350251823663712, val loss: 0.0923246368765831\n",
      "Epoch 7073: train loss: 0.07346875965595245, val loss: 0.09230007976293564\n",
      "Epoch 7074: train loss: 0.07343505322933197, val loss: 0.09227529913187027\n",
      "Epoch 7075: train loss: 0.07340139150619507, val loss: 0.09225070476531982\n",
      "Epoch 7076: train loss: 0.07336778193712234, val loss: 0.09222612529993057\n",
      "Epoch 7077: train loss: 0.07333417981863022, val loss: 0.0922015905380249\n",
      "Epoch 7078: train loss: 0.07330068945884705, val loss: 0.09217702597379684\n",
      "Epoch 7079: train loss: 0.07326717674732208, val loss: 0.09215264767408371\n",
      "Epoch 7080: train loss: 0.0732337236404419, val loss: 0.09212823212146759\n",
      "Epoch 7081: train loss: 0.07320031523704529, val loss: 0.09210386127233505\n",
      "Epoch 7082: train loss: 0.07316695153713226, val loss: 0.0920795351266861\n",
      "Epoch 7083: train loss: 0.07313364744186401, val loss: 0.09205519407987595\n",
      "Epoch 7084: train loss: 0.07310036569833755, val loss: 0.09203095734119415\n",
      "Epoch 7085: train loss: 0.07306715101003647, val loss: 0.09200678020715714\n",
      "Epoch 7086: train loss: 0.07303395122289658, val loss: 0.09198262542486191\n",
      "Epoch 7087: train loss: 0.07300080358982086, val loss: 0.09195853769779205\n",
      "Epoch 7088: train loss: 0.07296769320964813, val loss: 0.09193446487188339\n",
      "Epoch 7089: train loss: 0.07293462753295898, val loss: 0.09191037714481354\n",
      "Epoch 7090: train loss: 0.07290160655975342, val loss: 0.09188640117645264\n",
      "Epoch 7091: train loss: 0.07286863774061203, val loss: 0.09186247736215591\n",
      "Epoch 7092: train loss: 0.07283572107553482, val loss: 0.0918385237455368\n",
      "Epoch 7093: train loss: 0.07280283421278, val loss: 0.09181465208530426\n",
      "Epoch 7094: train loss: 0.07277000695466995, val loss: 0.09179076552391052\n",
      "Epoch 7095: train loss: 0.0727372020483017, val loss: 0.09176695346832275\n",
      "Epoch 7096: train loss: 0.07270441204309464, val loss: 0.09174323081970215\n",
      "Epoch 7097: train loss: 0.07267170399427414, val loss: 0.09171953052282333\n",
      "Epoch 7098: train loss: 0.07263902574777603, val loss: 0.09169591963291168\n",
      "Epoch 7099: train loss: 0.0726064071059227, val loss: 0.09167218208312988\n",
      "Epoch 7100: train loss: 0.07257381081581116, val loss: 0.09164860844612122\n",
      "Epoch 7101: train loss: 0.07254127413034439, val loss: 0.09162508696317673\n",
      "Epoch 7102: train loss: 0.07250877469778061, val loss: 0.09160156548023224\n",
      "Epoch 7103: train loss: 0.0724763348698616, val loss: 0.0915781557559967\n",
      "Epoch 7104: train loss: 0.07244391739368439, val loss: 0.09155471622943878\n",
      "Epoch 7105: train loss: 0.07241152971982956, val loss: 0.09153133630752563\n",
      "Epoch 7106: train loss: 0.07237918674945831, val loss: 0.09150800108909607\n",
      "Epoch 7107: train loss: 0.07234690338373184, val loss: 0.0914846733212471\n",
      "Epoch 7108: train loss: 0.07231464236974716, val loss: 0.09146139770746231\n",
      "Epoch 7109: train loss: 0.07228244096040726, val loss: 0.0914381742477417\n",
      "Epoch 7110: train loss: 0.07225029170513153, val loss: 0.09141504019498825\n",
      "Epoch 7111: train loss: 0.07221817970275879, val loss: 0.09139186143875122\n",
      "Epoch 7112: train loss: 0.07218609750270844, val loss: 0.09136881679296494\n",
      "Epoch 7113: train loss: 0.07215406745672226, val loss: 0.09134572744369507\n",
      "Epoch 7114: train loss: 0.07212207466363907, val loss: 0.09132271260023117\n",
      "Epoch 7115: train loss: 0.07209012657403946, val loss: 0.09129972755908966\n",
      "Epoch 7116: train loss: 0.07205820083618164, val loss: 0.09127677232027054\n",
      "Epoch 7117: train loss: 0.0720263347029686, val loss: 0.09125387668609619\n",
      "Epoch 7118: train loss: 0.07199452072381973, val loss: 0.09123106300830841\n",
      "Epoch 7119: train loss: 0.07196275144815445, val loss: 0.0912083089351654\n",
      "Epoch 7120: train loss: 0.07193099707365036, val loss: 0.09118544310331345\n",
      "Epoch 7121: train loss: 0.07189927995204926, val loss: 0.09116273373365402\n",
      "Epoch 7122: train loss: 0.07186762988567352, val loss: 0.09113994985818863\n",
      "Epoch 7123: train loss: 0.07183600217103958, val loss: 0.09111722558736801\n",
      "Epoch 7124: train loss: 0.07180444151163101, val loss: 0.09109460562467575\n",
      "Epoch 7125: train loss: 0.07177288085222244, val loss: 0.09107207506895065\n",
      "Epoch 7126: train loss: 0.07174140959978104, val loss: 0.09104961156845093\n",
      "Epoch 7127: train loss: 0.07170996814966202, val loss: 0.09102713316679001\n",
      "Epoch 7128: train loss: 0.07167857140302658, val loss: 0.09100466221570969\n",
      "Epoch 7129: train loss: 0.07164718955755234, val loss: 0.09098213165998459\n",
      "Epoch 7130: train loss: 0.07161584496498108, val loss: 0.09095972031354904\n",
      "Epoch 7131: train loss: 0.07158457487821579, val loss: 0.09093742072582245\n",
      "Epoch 7132: train loss: 0.0715533196926117, val loss: 0.09091507643461227\n",
      "Epoch 7133: train loss: 0.07152213156223297, val loss: 0.09089290350675583\n",
      "Epoch 7134: train loss: 0.07149095833301544, val loss: 0.09087073057889938\n",
      "Epoch 7135: train loss: 0.07145985215902328, val loss: 0.09084854274988174\n",
      "Epoch 7136: train loss: 0.07142877578735352, val loss: 0.0908263549208641\n",
      "Epoch 7137: train loss: 0.07139775902032852, val loss: 0.09080421179533005\n",
      "Epoch 7138: train loss: 0.07136674970388412, val loss: 0.09078215807676315\n",
      "Epoch 7139: train loss: 0.07133578509092331, val loss: 0.09076013416051865\n",
      "Epoch 7140: train loss: 0.07130489498376846, val loss: 0.09073807299137115\n",
      "Epoch 7141: train loss: 0.07127400487661362, val loss: 0.09071614593267441\n",
      "Epoch 7142: train loss: 0.07124318182468414, val loss: 0.09069432318210602\n",
      "Epoch 7143: train loss: 0.07121239602565765, val loss: 0.09067244827747345\n",
      "Epoch 7144: train loss: 0.07118166238069534, val loss: 0.09065064787864685\n",
      "Epoch 7145: train loss: 0.07115095853805542, val loss: 0.0906287357211113\n",
      "Epoch 7146: train loss: 0.07112029939889908, val loss: 0.09060697257518768\n",
      "Epoch 7147: train loss: 0.07108968496322632, val loss: 0.09058522433042526\n",
      "Epoch 7148: train loss: 0.07105911523103714, val loss: 0.0905635878443718\n",
      "Epoch 7149: train loss: 0.07102858275175095, val loss: 0.09054199606180191\n",
      "Epoch 7150: train loss: 0.07099806517362595, val loss: 0.09052050113677979\n",
      "Epoch 7151: train loss: 0.07096761465072632, val loss: 0.09049887955188751\n",
      "Epoch 7152: train loss: 0.07093719393014908, val loss: 0.09047739952802658\n",
      "Epoch 7153: train loss: 0.07090681791305542, val loss: 0.0904558077454567\n",
      "Epoch 7154: train loss: 0.07087648659944534, val loss: 0.09043437242507935\n",
      "Epoch 7155: train loss: 0.07084620743989944, val loss: 0.09041296690702438\n",
      "Epoch 7156: train loss: 0.07081593573093414, val loss: 0.09039156883955002\n",
      "Epoch 7157: train loss: 0.07078574597835541, val loss: 0.0903702974319458\n",
      "Epoch 7158: train loss: 0.07075557857751846, val loss: 0.09034910053014755\n",
      "Epoch 7159: train loss: 0.07072543352842331, val loss: 0.09032786637544632\n",
      "Epoch 7160: train loss: 0.07069536298513412, val loss: 0.09030652791261673\n",
      "Epoch 7161: train loss: 0.07066529244184494, val loss: 0.09028533846139908\n",
      "Epoch 7162: train loss: 0.07063528150320053, val loss: 0.0902642235159874\n",
      "Epoch 7163: train loss: 0.0706053078174591, val loss: 0.0902431532740593\n",
      "Epoch 7164: train loss: 0.07057538628578186, val loss: 0.09022211283445358\n",
      "Epoch 7165: train loss: 0.0705455020070076, val loss: 0.09020108729600906\n",
      "Epoch 7166: train loss: 0.07051565498113632, val loss: 0.0901801586151123\n",
      "Epoch 7167: train loss: 0.07048586755990982, val loss: 0.09015915542840958\n",
      "Epoch 7168: train loss: 0.07045608758926392, val loss: 0.0901382565498352\n",
      "Epoch 7169: train loss: 0.07042635232210159, val loss: 0.090117447078228\n",
      "Epoch 7170: train loss: 0.07039666175842285, val loss: 0.09009657055139542\n",
      "Epoch 7171: train loss: 0.07036702334880829, val loss: 0.0900757685303688\n",
      "Epoch 7172: train loss: 0.07033741474151611, val loss: 0.09005501121282578\n",
      "Epoch 7173: train loss: 0.07030785828828812, val loss: 0.09003428369760513\n",
      "Epoch 7174: train loss: 0.07027833163738251, val loss: 0.09001364558935165\n",
      "Epoch 7175: train loss: 0.07024884968996048, val loss: 0.08999299257993698\n",
      "Epoch 7176: train loss: 0.07021941244602203, val loss: 0.08997243642807007\n",
      "Epoch 7177: train loss: 0.07019001990556717, val loss: 0.08995189517736435\n",
      "Epoch 7178: train loss: 0.07016065716743469, val loss: 0.08993136882781982\n",
      "Epoch 7179: train loss: 0.0701313242316246, val loss: 0.08991086483001709\n",
      "Epoch 7180: train loss: 0.0701020359992981, val loss: 0.08989043533802032\n",
      "Epoch 7181: train loss: 0.07007277756929398, val loss: 0.08987002074718475\n",
      "Epoch 7182: train loss: 0.07004359364509583, val loss: 0.08984958380460739\n",
      "Epoch 7183: train loss: 0.07001442462205887, val loss: 0.08982930332422256\n",
      "Epoch 7184: train loss: 0.06998530775308609, val loss: 0.08980896323919296\n",
      "Epoch 7185: train loss: 0.0699562206864357, val loss: 0.08978874981403351\n",
      "Epoch 7186: train loss: 0.06992718577384949, val loss: 0.08976853638887405\n",
      "Epoch 7187: train loss: 0.06989818066358566, val loss: 0.08974837511777878\n",
      "Epoch 7188: train loss: 0.06986920535564423, val loss: 0.08972819894552231\n",
      "Epoch 7189: train loss: 0.06984029710292816, val loss: 0.08970806747674942\n",
      "Epoch 7190: train loss: 0.0698113739490509, val loss: 0.08968794345855713\n",
      "Epoch 7191: train loss: 0.0697825476527214, val loss: 0.08966793119907379\n",
      "Epoch 7192: train loss: 0.0697537213563919, val loss: 0.08964796364307404\n",
      "Epoch 7193: train loss: 0.06972497701644897, val loss: 0.08962810784578323\n",
      "Epoch 7194: train loss: 0.06969622522592545, val loss: 0.08960814774036407\n",
      "Epoch 7195: train loss: 0.06966755539178848, val loss: 0.0895882397890091\n",
      "Epoch 7196: train loss: 0.06963890045881271, val loss: 0.0895683765411377\n",
      "Epoch 7197: train loss: 0.06961029767990112, val loss: 0.0895485058426857\n",
      "Epoch 7198: train loss: 0.06958173215389252, val loss: 0.08952874690294266\n",
      "Epoch 7199: train loss: 0.0695532038807869, val loss: 0.08950897306203842\n",
      "Epoch 7200: train loss: 0.06952470541000366, val loss: 0.08948933333158493\n",
      "Epoch 7201: train loss: 0.0694962590932846, val loss: 0.08946975320577621\n",
      "Epoch 7202: train loss: 0.06946782767772675, val loss: 0.08945001661777496\n",
      "Epoch 7203: train loss: 0.06943947076797485, val loss: 0.08943045139312744\n",
      "Epoch 7204: train loss: 0.06941113620996475, val loss: 0.08941087126731873\n",
      "Epoch 7205: train loss: 0.06938282400369644, val loss: 0.08939138799905777\n",
      "Epoch 7206: train loss: 0.0693545863032341, val loss: 0.0893719419836998\n",
      "Epoch 7207: train loss: 0.06932637095451355, val loss: 0.08935252577066422\n",
      "Epoch 7208: train loss: 0.0692981705069542, val loss: 0.08933313935995102\n",
      "Epoch 7209: train loss: 0.06927002966403961, val loss: 0.08931373804807663\n",
      "Epoch 7210: train loss: 0.06924192607402802, val loss: 0.08929441124200821\n",
      "Epoch 7211: train loss: 0.0692138671875, val loss: 0.0892750695347786\n",
      "Epoch 7212: train loss: 0.06918583065271378, val loss: 0.08925580978393555\n",
      "Epoch 7213: train loss: 0.06915784627199173, val loss: 0.08923657238483429\n",
      "Epoch 7214: train loss: 0.06912990659475327, val loss: 0.08921743184328079\n",
      "Epoch 7215: train loss: 0.06910201162099838, val loss: 0.0891982838511467\n",
      "Epoch 7216: train loss: 0.0690741240978241, val loss: 0.0891791358590126\n",
      "Epoch 7217: train loss: 0.06904631853103638, val loss: 0.08916009962558746\n",
      "Epoch 7218: train loss: 0.06901852786540985, val loss: 0.08914109319448471\n",
      "Epoch 7219: train loss: 0.06899075210094452, val loss: 0.08912207931280136\n",
      "Epoch 7220: train loss: 0.06896305084228516, val loss: 0.08910307288169861\n",
      "Epoch 7221: train loss: 0.06893537193536758, val loss: 0.08908416330814362\n",
      "Epoch 7222: train loss: 0.0689077228307724, val loss: 0.08906526863574982\n",
      "Epoch 7223: train loss: 0.0688801258802414, val loss: 0.08904638141393661\n",
      "Epoch 7224: train loss: 0.06885257363319397, val loss: 0.08902763575315475\n",
      "Epoch 7225: train loss: 0.06882503628730774, val loss: 0.08900878578424454\n",
      "Epoch 7226: train loss: 0.06879758834838867, val loss: 0.08899003267288208\n",
      "Epoch 7227: train loss: 0.06877013295888901, val loss: 0.0889713391661644\n",
      "Epoch 7228: train loss: 0.06874272972345352, val loss: 0.0889526829123497\n",
      "Epoch 7229: train loss: 0.06871536374092102, val loss: 0.0889340341091156\n",
      "Epoch 7230: train loss: 0.06868800520896912, val loss: 0.08891532570123672\n",
      "Epoch 7231: train loss: 0.06866073608398438, val loss: 0.08889683336019516\n",
      "Epoch 7232: train loss: 0.06863347440958023, val loss: 0.08887823671102524\n",
      "Epoch 7233: train loss: 0.06860625743865967, val loss: 0.08885975927114487\n",
      "Epoch 7234: train loss: 0.06857907772064209, val loss: 0.08884134143590927\n",
      "Epoch 7235: train loss: 0.0685519427061081, val loss: 0.08882296085357666\n",
      "Epoch 7236: train loss: 0.06852483749389648, val loss: 0.08880458027124405\n",
      "Epoch 7237: train loss: 0.06849776953458786, val loss: 0.08878619223833084\n",
      "Epoch 7238: train loss: 0.06847075372934341, val loss: 0.0887678936123848\n",
      "Epoch 7239: train loss: 0.06844378262758255, val loss: 0.08874955773353577\n",
      "Epoch 7240: train loss: 0.06841683387756348, val loss: 0.08873128890991211\n",
      "Epoch 7241: train loss: 0.0683899000287056, val loss: 0.08871308714151382\n",
      "Epoch 7242: train loss: 0.06836303323507309, val loss: 0.08869490772485733\n",
      "Epoch 7243: train loss: 0.06833618879318237, val loss: 0.0886768028140068\n",
      "Epoch 7244: train loss: 0.06830940395593643, val loss: 0.08865866810083389\n",
      "Epoch 7245: train loss: 0.06828262656927109, val loss: 0.08864059299230576\n",
      "Epoch 7246: train loss: 0.06825591623783112, val loss: 0.0886225476861\n",
      "Epoch 7247: train loss: 0.06822922825813293, val loss: 0.08860462158918381\n",
      "Epoch 7248: train loss: 0.06820258498191833, val loss: 0.08858666568994522\n",
      "Epoch 7249: train loss: 0.06817598640918732, val loss: 0.08856876939535141\n",
      "Epoch 7250: train loss: 0.06814941763877869, val loss: 0.088550865650177\n",
      "Epoch 7251: train loss: 0.06812289357185364, val loss: 0.08853287994861603\n",
      "Epoch 7252: train loss: 0.06809637695550919, val loss: 0.08851506561040878\n",
      "Epoch 7253: train loss: 0.06806991994380951, val loss: 0.08849719911813736\n",
      "Epoch 7254: train loss: 0.06804347783327103, val loss: 0.08847951889038086\n",
      "Epoch 7255: train loss: 0.06801711022853851, val loss: 0.08846185356378555\n",
      "Epoch 7256: train loss: 0.0679907500743866, val loss: 0.08844422549009323\n",
      "Epoch 7257: train loss: 0.06796444952487946, val loss: 0.08842653036117554\n",
      "Epoch 7258: train loss: 0.06793816387653351, val loss: 0.08840888738632202\n",
      "Epoch 7259: train loss: 0.06791195273399353, val loss: 0.08839129656553268\n",
      "Epoch 7260: train loss: 0.06788574159145355, val loss: 0.08837377279996872\n",
      "Epoch 7261: train loss: 0.06785959005355835, val loss: 0.08835624903440475\n",
      "Epoch 7262: train loss: 0.06783347576856613, val loss: 0.08833880722522736\n",
      "Epoch 7263: train loss: 0.0678073987364769, val loss: 0.08832138031721115\n",
      "Epoch 7264: train loss: 0.06778134405612946, val loss: 0.08830400556325912\n",
      "Epoch 7265: train loss: 0.067755326628685, val loss: 0.08828659355640411\n",
      "Epoch 7266: train loss: 0.06772936135530472, val loss: 0.08826928585767746\n",
      "Epoch 7267: train loss: 0.06770338863134384, val loss: 0.08825191855430603\n",
      "Epoch 7268: train loss: 0.06767751276493073, val loss: 0.08823462575674057\n",
      "Epoch 7269: train loss: 0.0676516443490982, val loss: 0.08821740001440048\n",
      "Epoch 7270: train loss: 0.06762582063674927, val loss: 0.08820018917322159\n",
      "Epoch 7271: train loss: 0.06760004162788391, val loss: 0.08818312734365463\n",
      "Epoch 7272: train loss: 0.06757429242134094, val loss: 0.0881660133600235\n",
      "Epoch 7273: train loss: 0.06754857301712036, val loss: 0.08814892917871475\n",
      "Epoch 7274: train loss: 0.06752289086580276, val loss: 0.08813182264566422\n",
      "Epoch 7275: train loss: 0.06749726831912994, val loss: 0.08811477571725845\n",
      "Epoch 7276: train loss: 0.06747164577245712, val loss: 0.08809775114059448\n",
      "Epoch 7277: train loss: 0.06744608283042908, val loss: 0.08808077871799469\n",
      "Epoch 7278: train loss: 0.06742053478956223, val loss: 0.08806385844945908\n",
      "Epoch 7279: train loss: 0.06739506125450134, val loss: 0.08804694563150406\n",
      "Epoch 7280: train loss: 0.06736957281827927, val loss: 0.08803015202283859\n",
      "Epoch 7281: train loss: 0.06734416633844376, val loss: 0.08801332116127014\n",
      "Epoch 7282: train loss: 0.06731876730918884, val loss: 0.08799652010202408\n",
      "Epoch 7283: train loss: 0.0672934278845787, val loss: 0.08797978609800339\n",
      "Epoch 7284: train loss: 0.06726811826229095, val loss: 0.08796309679746628\n",
      "Epoch 7285: train loss: 0.06724284589290619, val loss: 0.0879463329911232\n",
      "Epoch 7286: train loss: 0.06721760332584381, val loss: 0.08792976289987564\n",
      "Epoch 7287: train loss: 0.06719240546226501, val loss: 0.0879131332039833\n",
      "Epoch 7288: train loss: 0.0671672448515892, val loss: 0.08789656311273575\n",
      "Epoch 7289: train loss: 0.06714209914207458, val loss: 0.08788003772497177\n",
      "Epoch 7290: train loss: 0.06711701303720474, val loss: 0.08786346763372421\n",
      "Epoch 7291: train loss: 0.0670919418334961, val loss: 0.08784694969654083\n",
      "Epoch 7292: train loss: 0.06706692278385162, val loss: 0.08783053606748581\n",
      "Epoch 7293: train loss: 0.06704192608594894, val loss: 0.08781413733959198\n",
      "Epoch 7294: train loss: 0.06701698154211044, val loss: 0.08779770880937576\n",
      "Epoch 7295: train loss: 0.06699205189943314, val loss: 0.0877813920378685\n",
      "Epoch 7296: train loss: 0.0669671818614006, val loss: 0.08776509016752243\n",
      "Epoch 7297: train loss: 0.06694235652685165, val loss: 0.08774887025356293\n",
      "Epoch 7298: train loss: 0.0669175460934639, val loss: 0.08773260563611984\n",
      "Epoch 7299: train loss: 0.06689278036355972, val loss: 0.08771638572216034\n",
      "Epoch 7300: train loss: 0.06686802953481674, val loss: 0.08770016580820084\n",
      "Epoch 7301: train loss: 0.06684333831071854, val loss: 0.08768396824598312\n",
      "Epoch 7302: train loss: 0.06681865453720093, val loss: 0.08766788244247437\n",
      "Epoch 7303: train loss: 0.06679404526948929, val loss: 0.08765184134244919\n",
      "Epoch 7304: train loss: 0.06676943600177765, val loss: 0.08763580769300461\n",
      "Epoch 7305: train loss: 0.06674489378929138, val loss: 0.08761976659297943\n",
      "Epoch 7306: train loss: 0.06672035157680511, val loss: 0.08760370314121246\n",
      "Epoch 7307: train loss: 0.06669586896896362, val loss: 0.08758771419525146\n",
      "Epoch 7308: train loss: 0.06667142361402512, val loss: 0.0875718966126442\n",
      "Epoch 7309: train loss: 0.06664703041315079, val loss: 0.08755604177713394\n",
      "Epoch 7310: train loss: 0.06662265211343765, val loss: 0.08754026144742966\n",
      "Epoch 7311: train loss: 0.0665983259677887, val loss: 0.0875244289636612\n",
      "Epoch 7312: train loss: 0.06657402217388153, val loss: 0.08750861138105392\n",
      "Epoch 7313: train loss: 0.06654973328113556, val loss: 0.08749283105134964\n",
      "Epoch 7314: train loss: 0.06652548909187317, val loss: 0.08747705072164536\n",
      "Epoch 7315: train loss: 0.06650128215551376, val loss: 0.08746131509542465\n",
      "Epoch 7316: train loss: 0.06647712737321854, val loss: 0.0874457061290741\n",
      "Epoch 7317: train loss: 0.0664529800415039, val loss: 0.08743015676736832\n",
      "Epoch 7318: train loss: 0.06642889976501465, val loss: 0.08741459250450134\n",
      "Epoch 7319: train loss: 0.06640482693910599, val loss: 0.08739901334047318\n",
      "Epoch 7320: train loss: 0.0663808062672615, val loss: 0.0873834416270256\n",
      "Epoch 7321: train loss: 0.0663568302989006, val loss: 0.08736799657344818\n",
      "Epoch 7322: train loss: 0.0663328692317009, val loss: 0.08735251426696777\n",
      "Epoch 7323: train loss: 0.06630896031856537, val loss: 0.08733709901571274\n",
      "Epoch 7324: train loss: 0.06628509610891342, val loss: 0.08732175827026367\n",
      "Epoch 7325: train loss: 0.06626125425100327, val loss: 0.0873064398765564\n",
      "Epoch 7326: train loss: 0.0662374347448349, val loss: 0.0872911587357521\n",
      "Epoch 7327: train loss: 0.06621365994215012, val loss: 0.08727581799030304\n",
      "Epoch 7328: train loss: 0.06618990749120712, val loss: 0.08726053684949875\n",
      "Epoch 7329: train loss: 0.0661662146449089, val loss: 0.0872451588511467\n",
      "Epoch 7330: train loss: 0.06614252179861069, val loss: 0.08723001927137375\n",
      "Epoch 7331: train loss: 0.06611888855695724, val loss: 0.08721484243869781\n",
      "Epoch 7332: train loss: 0.0660952776670456, val loss: 0.08719981461763382\n",
      "Epoch 7333: train loss: 0.06607173383235931, val loss: 0.08718471229076385\n",
      "Epoch 7334: train loss: 0.06604818254709244, val loss: 0.0871695950627327\n",
      "Epoch 7335: train loss: 0.06602469831705093, val loss: 0.0871545821428299\n",
      "Epoch 7336: train loss: 0.06600122153759003, val loss: 0.0871395394206047\n",
      "Epoch 7337: train loss: 0.0659777969121933, val loss: 0.08712464570999146\n",
      "Epoch 7338: train loss: 0.06595440953969955, val loss: 0.08710968494415283\n",
      "Epoch 7339: train loss: 0.0659310519695282, val loss: 0.08709480613470078\n",
      "Epoch 7340: train loss: 0.06590774655342102, val loss: 0.08707991987466812\n",
      "Epoch 7341: train loss: 0.06588444858789444, val loss: 0.08706503361463547\n",
      "Epoch 7342: train loss: 0.06586118042469025, val loss: 0.08705022186040878\n",
      "Epoch 7343: train loss: 0.06583795696496964, val loss: 0.0870354101061821\n",
      "Epoch 7344: train loss: 0.06581477075815201, val loss: 0.08702059835195541\n",
      "Epoch 7345: train loss: 0.06579160690307617, val loss: 0.08700597286224365\n",
      "Epoch 7346: train loss: 0.06576848030090332, val loss: 0.0869913101196289\n",
      "Epoch 7347: train loss: 0.06574540585279465, val loss: 0.08697668462991714\n",
      "Epoch 7348: train loss: 0.06572236865758896, val loss: 0.08696206659078598\n",
      "Epoch 7349: train loss: 0.06569934636354446, val loss: 0.0869474932551384\n",
      "Epoch 7350: train loss: 0.06567636132240295, val loss: 0.08693291246891022\n",
      "Epoch 7351: train loss: 0.06565342843532562, val loss: 0.08691836148500443\n",
      "Epoch 7352: train loss: 0.06563053280115128, val loss: 0.08690387010574341\n",
      "Epoch 7353: train loss: 0.06560763716697693, val loss: 0.0868893638253212\n",
      "Epoch 7354: train loss: 0.06558478623628616, val loss: 0.08687495440244675\n",
      "Epoch 7355: train loss: 0.06556196510791779, val loss: 0.08686055988073349\n",
      "Epoch 7356: train loss: 0.06553918868303299, val loss: 0.08684618771076202\n",
      "Epoch 7357: train loss: 0.06551643460988998, val loss: 0.08683180063962936\n",
      "Epoch 7358: train loss: 0.06549372524023056, val loss: 0.08681750297546387\n",
      "Epoch 7359: train loss: 0.06547106057405472, val loss: 0.08680323511362076\n",
      "Epoch 7360: train loss: 0.06544841080904007, val loss: 0.08678903430700302\n",
      "Epoch 7361: train loss: 0.0654258131980896, val loss: 0.0867747887969017\n",
      "Epoch 7362: train loss: 0.06540323048830032, val loss: 0.08676068484783173\n",
      "Epoch 7363: train loss: 0.06538070738315582, val loss: 0.08674649894237518\n",
      "Epoch 7364: train loss: 0.06535819172859192, val loss: 0.08673232793807983\n",
      "Epoch 7365: train loss: 0.06533573567867279, val loss: 0.08671827614307404\n",
      "Epoch 7366: train loss: 0.06531330943107605, val loss: 0.08670421689748764\n",
      "Epoch 7367: train loss: 0.0652908906340599, val loss: 0.08669021725654602\n",
      "Epoch 7368: train loss: 0.06526853144168854, val loss: 0.08667612075805664\n",
      "Epoch 7369: train loss: 0.06524619460105896, val loss: 0.08666212856769562\n",
      "Epoch 7370: train loss: 0.06522388756275177, val loss: 0.08664827048778534\n",
      "Epoch 7371: train loss: 0.06520160287618637, val loss: 0.08663436025381088\n",
      "Epoch 7372: train loss: 0.06517937034368515, val loss: 0.08662053197622299\n",
      "Epoch 7373: train loss: 0.06515716016292572, val loss: 0.08660668879747391\n",
      "Epoch 7374: train loss: 0.06513499468564987, val loss: 0.08659283071756363\n",
      "Epoch 7375: train loss: 0.06511286646127701, val loss: 0.08657903969287872\n",
      "Epoch 7376: train loss: 0.06509075313806534, val loss: 0.0865652784705162\n",
      "Epoch 7377: train loss: 0.06506867706775665, val loss: 0.08655156940221786\n",
      "Epoch 7378: train loss: 0.06504666060209274, val loss: 0.08653789758682251\n",
      "Epoch 7379: train loss: 0.06502464413642883, val loss: 0.08652430027723312\n",
      "Epoch 7380: train loss: 0.0650026872754097, val loss: 0.0865105390548706\n",
      "Epoch 7381: train loss: 0.06498076021671295, val loss: 0.0864969864487648\n",
      "Epoch 7382: train loss: 0.06495886296033859, val loss: 0.08648335188627243\n",
      "Epoch 7383: train loss: 0.06493698060512543, val loss: 0.08646990358829498\n",
      "Epoch 7384: train loss: 0.06491515785455704, val loss: 0.08645645529031754\n",
      "Epoch 7385: train loss: 0.06489336490631104, val loss: 0.08644291013479233\n",
      "Epoch 7386: train loss: 0.06487156450748444, val loss: 0.08642943948507309\n",
      "Epoch 7387: train loss: 0.06484983116388321, val loss: 0.08641602843999863\n",
      "Epoch 7388: train loss: 0.06482812017202377, val loss: 0.08640266209840775\n",
      "Epoch 7389: train loss: 0.06480646878480911, val loss: 0.08638926595449448\n",
      "Epoch 7390: train loss: 0.06478480249643326, val loss: 0.08637603372335434\n",
      "Epoch 7391: train loss: 0.06476321816444397, val loss: 0.08636265248060226\n",
      "Epoch 7392: train loss: 0.06474163383245468, val loss: 0.08634933084249496\n",
      "Epoch 7393: train loss: 0.06472011655569077, val loss: 0.08633611351251602\n",
      "Epoch 7394: train loss: 0.06469859182834625, val loss: 0.08632298558950424\n",
      "Epoch 7395: train loss: 0.06467712670564651, val loss: 0.08630983531475067\n",
      "Epoch 7396: train loss: 0.06465568393468857, val loss: 0.0862966924905777\n",
      "Epoch 7397: train loss: 0.0646342858672142, val loss: 0.08628356456756592\n",
      "Epoch 7398: train loss: 0.06461290270090103, val loss: 0.08627047389745712\n",
      "Epoch 7399: train loss: 0.06459156423807144, val loss: 0.08625739067792892\n",
      "Epoch 7400: train loss: 0.06457024067640305, val loss: 0.0862443596124649\n",
      "Epoch 7401: train loss: 0.06454896181821823, val loss: 0.08623132854700089\n",
      "Epoch 7402: train loss: 0.064527727663517, val loss: 0.08621839433908463\n",
      "Epoch 7403: train loss: 0.06450650095939636, val loss: 0.08620549738407135\n",
      "Epoch 7404: train loss: 0.06448531895875931, val loss: 0.0861925259232521\n",
      "Epoch 7405: train loss: 0.06446415930986404, val loss: 0.08617963641881943\n",
      "Epoch 7406: train loss: 0.06444305926561356, val loss: 0.08616676181554794\n",
      "Epoch 7407: train loss: 0.06442198157310486, val loss: 0.08615399152040482\n",
      "Epoch 7408: train loss: 0.06440091878175735, val loss: 0.08614125847816467\n",
      "Epoch 7409: train loss: 0.06437989324331284, val loss: 0.08612851053476334\n",
      "Epoch 7410: train loss: 0.06435892730951309, val loss: 0.08611579239368439\n",
      "Epoch 7411: train loss: 0.06433796882629395, val loss: 0.08610306680202484\n",
      "Epoch 7412: train loss: 0.06431706249713898, val loss: 0.08609037101268768\n",
      "Epoch 7413: train loss: 0.0642961710691452, val loss: 0.08607769012451172\n",
      "Epoch 7414: train loss: 0.06427531689405441, val loss: 0.08606500923633575\n",
      "Epoch 7415: train loss: 0.06425448507070541, val loss: 0.08605247735977173\n",
      "Epoch 7416: train loss: 0.0642336830496788, val loss: 0.08603992313146591\n",
      "Epoch 7417: train loss: 0.06421294063329697, val loss: 0.08602740615606308\n",
      "Epoch 7418: train loss: 0.06419219821691513, val loss: 0.08601492643356323\n",
      "Epoch 7419: train loss: 0.06417149305343628, val loss: 0.08600250631570816\n",
      "Epoch 7420: train loss: 0.06415083259344101, val loss: 0.0859900712966919\n",
      "Epoch 7421: train loss: 0.06413020193576813, val loss: 0.08597760647535324\n",
      "Epoch 7422: train loss: 0.06410957872867584, val loss: 0.08596523851156235\n",
      "Epoch 7423: train loss: 0.06408902257680893, val loss: 0.08595281094312668\n",
      "Epoch 7424: train loss: 0.06406848132610321, val loss: 0.08594053238630295\n",
      "Epoch 7425: train loss: 0.06404798477888107, val loss: 0.08592822402715683\n",
      "Epoch 7426: train loss: 0.06402750313282013, val loss: 0.08591601997613907\n",
      "Epoch 7427: train loss: 0.06400707364082336, val loss: 0.08590376377105713\n",
      "Epoch 7428: train loss: 0.06398666650056839, val loss: 0.08589156717061996\n",
      "Epoch 7429: train loss: 0.0639662966132164, val loss: 0.0858793705701828\n",
      "Epoch 7430: train loss: 0.0639459490776062, val loss: 0.08586719632148743\n",
      "Epoch 7431: train loss: 0.06392564624547958, val loss: 0.08585508912801743\n",
      "Epoch 7432: train loss: 0.06390535831451416, val loss: 0.08584300428628922\n",
      "Epoch 7433: train loss: 0.06388510018587112, val loss: 0.08583099395036697\n",
      "Epoch 7434: train loss: 0.06386488676071167, val loss: 0.08581894636154175\n",
      "Epoch 7435: train loss: 0.06384468078613281, val loss: 0.0858069583773613\n",
      "Epoch 7436: train loss: 0.06382452696561813, val loss: 0.08579494059085846\n",
      "Epoch 7437: train loss: 0.06380438804626465, val loss: 0.08578291535377502\n",
      "Epoch 7438: train loss: 0.06378430128097534, val loss: 0.08577096462249756\n",
      "Epoch 7439: train loss: 0.06376422196626663, val loss: 0.08575910329818726\n",
      "Epoch 7440: train loss: 0.0637442097067833, val loss: 0.08574739098548889\n",
      "Epoch 7441: train loss: 0.06372419744729996, val loss: 0.0857355147600174\n",
      "Epoch 7442: train loss: 0.0637042373418808, val loss: 0.08572369068861008\n",
      "Epoch 7443: train loss: 0.06368430703878403, val loss: 0.08571182191371918\n",
      "Epoch 7444: train loss: 0.06366439908742905, val loss: 0.0857001319527626\n",
      "Epoch 7445: train loss: 0.06364452093839645, val loss: 0.08568833023309708\n",
      "Epoch 7446: train loss: 0.06362468749284744, val loss: 0.08567669242620468\n",
      "Epoch 7447: train loss: 0.06360488384962082, val loss: 0.08566509932279587\n",
      "Epoch 7448: train loss: 0.06358509510755539, val loss: 0.08565344661474228\n",
      "Epoch 7449: train loss: 0.06356534361839294, val loss: 0.08564182370901108\n",
      "Epoch 7450: train loss: 0.06354562193155289, val loss: 0.08563012629747391\n",
      "Epoch 7451: train loss: 0.06352592259645462, val loss: 0.08561861515045166\n",
      "Epoch 7452: train loss: 0.06350624561309814, val loss: 0.08560707420110703\n",
      "Epoch 7453: train loss: 0.06348662078380585, val loss: 0.08559560775756836\n",
      "Epoch 7454: train loss: 0.06346702575683594, val loss: 0.08558414876461029\n",
      "Epoch 7455: train loss: 0.06344746053218842, val loss: 0.08557265996932983\n",
      "Epoch 7456: train loss: 0.06342792510986328, val loss: 0.08556132018566132\n",
      "Epoch 7457: train loss: 0.06340841948986053, val loss: 0.08554986864328384\n",
      "Epoch 7458: train loss: 0.06338894367218018, val loss: 0.08553852885961533\n",
      "Epoch 7459: train loss: 0.0633694976568222, val loss: 0.085527203977108\n",
      "Epoch 7460: train loss: 0.06335008889436722, val loss: 0.08551588654518127\n",
      "Epoch 7461: train loss: 0.06333071738481522, val loss: 0.08550463616847992\n",
      "Epoch 7462: train loss: 0.06331134587526321, val loss: 0.08549337834119797\n",
      "Epoch 7463: train loss: 0.06329204887151718, val loss: 0.0854821503162384\n",
      "Epoch 7464: train loss: 0.06327274441719055, val loss: 0.08547095209360123\n",
      "Epoch 7465: train loss: 0.0632535070180893, val loss: 0.08545976877212524\n",
      "Epoch 7466: train loss: 0.06323426961898804, val loss: 0.08544858545064926\n",
      "Epoch 7467: train loss: 0.06321506947278976, val loss: 0.08543751388788223\n",
      "Epoch 7468: train loss: 0.06319589912891388, val loss: 0.08542647212743759\n",
      "Epoch 7469: train loss: 0.06317675858736038, val loss: 0.08541541546583176\n",
      "Epoch 7470: train loss: 0.06315765529870987, val loss: 0.08540435135364532\n",
      "Epoch 7471: train loss: 0.06313855201005936, val loss: 0.08539342135190964\n",
      "Epoch 7472: train loss: 0.06311951577663422, val loss: 0.08538233488798141\n",
      "Epoch 7473: train loss: 0.06310047954320908, val loss: 0.08537134528160095\n",
      "Epoch 7474: train loss: 0.06308149546384811, val loss: 0.08536042273044586\n",
      "Epoch 7475: train loss: 0.06306254118680954, val loss: 0.08534963428974152\n",
      "Epoch 7476: train loss: 0.06304360926151276, val loss: 0.08533879369497299\n",
      "Epoch 7477: train loss: 0.06302471458911896, val loss: 0.08532799780368805\n",
      "Epoch 7478: train loss: 0.06300584971904755, val loss: 0.08531713485717773\n",
      "Epoch 7479: train loss: 0.06298699975013733, val loss: 0.08530624955892563\n",
      "Epoch 7480: train loss: 0.06296822428703308, val loss: 0.08529540151357651\n",
      "Epoch 7481: train loss: 0.06294943392276764, val loss: 0.08528470993041992\n",
      "Epoch 7482: train loss: 0.06293071061372757, val loss: 0.0852740928530693\n",
      "Epoch 7483: train loss: 0.0629119798541069, val loss: 0.08526351302862167\n",
      "Epoch 7484: train loss: 0.06289331614971161, val loss: 0.0852528065443039\n",
      "Epoch 7485: train loss: 0.06287464499473572, val loss: 0.08524215221405029\n",
      "Epoch 7486: train loss: 0.06285601109266281, val loss: 0.08523150533437729\n",
      "Epoch 7487: train loss: 0.06283742189407349, val loss: 0.08522092550992966\n",
      "Epoch 7488: train loss: 0.06281885504722595, val loss: 0.0852104052901268\n",
      "Epoch 7489: train loss: 0.0628003180027008, val loss: 0.08519988507032394\n",
      "Epoch 7490: train loss: 0.06278178840875626, val loss: 0.08518940210342407\n",
      "Epoch 7491: train loss: 0.06276331841945648, val loss: 0.08517902344465256\n",
      "Epoch 7492: train loss: 0.0627448707818985, val loss: 0.08516855537891388\n",
      "Epoch 7493: train loss: 0.0627264529466629, val loss: 0.08515813946723938\n",
      "Epoch 7494: train loss: 0.0627080649137497, val loss: 0.08514769375324249\n",
      "Epoch 7495: train loss: 0.06268970668315887, val loss: 0.08513728529214859\n",
      "Epoch 7496: train loss: 0.06267137080430984, val loss: 0.08512698858976364\n",
      "Epoch 7497: train loss: 0.0626530721783638, val loss: 0.08511670678853989\n",
      "Epoch 7498: train loss: 0.06263479590415955, val loss: 0.08510643988847733\n",
      "Epoch 7499: train loss: 0.06261656433343887, val loss: 0.08509621769189835\n",
      "Epoch 7500: train loss: 0.0625983476638794, val loss: 0.08508598059415817\n",
      "Epoch 7501: train loss: 0.0625801682472229, val loss: 0.08507569879293442\n",
      "Epoch 7502: train loss: 0.06256202608346939, val loss: 0.08506549894809723\n",
      "Epoch 7503: train loss: 0.06254389882087708, val loss: 0.08505545556545258\n",
      "Epoch 7504: train loss: 0.06252581626176834, val loss: 0.08504530042409897\n",
      "Epoch 7505: train loss: 0.0625077560544014, val loss: 0.08503522723913193\n",
      "Epoch 7506: train loss: 0.062489718198776245, val loss: 0.08502514660358429\n",
      "Epoch 7507: train loss: 0.06247171387076378, val loss: 0.08501514047384262\n",
      "Epoch 7508: train loss: 0.062453728169202805, val loss: 0.08500504493713379\n",
      "Epoch 7509: train loss: 0.06243576481938362, val loss: 0.08499505370855331\n",
      "Epoch 7510: train loss: 0.06241784617304802, val loss: 0.08498503267765045\n",
      "Epoch 7511: train loss: 0.0623999647796154, val loss: 0.08497514575719833\n",
      "Epoch 7512: train loss: 0.06238209083676338, val loss: 0.08496522903442383\n",
      "Epoch 7513: train loss: 0.06236425042152405, val loss: 0.0849553570151329\n",
      "Epoch 7514: train loss: 0.062346458435058594, val loss: 0.08494551479816437\n",
      "Epoch 7515: train loss: 0.062328677624464035, val loss: 0.08493565768003464\n",
      "Epoch 7516: train loss: 0.06231094151735306, val loss: 0.08492588251829147\n",
      "Epoch 7517: train loss: 0.06229321286082268, val loss: 0.08491607755422592\n",
      "Epoch 7518: train loss: 0.06227554753422737, val loss: 0.08490624278783798\n",
      "Epoch 7519: train loss: 0.06225787475705147, val loss: 0.0848964974284172\n",
      "Epoch 7520: train loss: 0.06224025785923004, val loss: 0.08488684147596359\n",
      "Epoch 7521: train loss: 0.062222640961408615, val loss: 0.084877148270607\n",
      "Epoch 7522: train loss: 0.06220507249236107, val loss: 0.08486750721931458\n",
      "Epoch 7523: train loss: 0.062187518924474716, val loss: 0.0848580002784729\n",
      "Epoch 7524: train loss: 0.062170036137104034, val loss: 0.08484837412834167\n",
      "Epoch 7525: train loss: 0.06215254217386246, val loss: 0.08483868092298508\n",
      "Epoch 7526: train loss: 0.062135063111782074, val loss: 0.08482904732227325\n",
      "Epoch 7527: train loss: 0.06211763992905617, val loss: 0.08481953293085098\n",
      "Epoch 7528: train loss: 0.062100231647491455, val loss: 0.08481010049581528\n",
      "Epoch 7529: train loss: 0.06208285316824913, val loss: 0.08480065315961838\n",
      "Epoch 7530: train loss: 0.062065497040748596, val loss: 0.08479122072458267\n",
      "Epoch 7531: train loss: 0.062048181891441345, val loss: 0.08478175848722458\n",
      "Epoch 7532: train loss: 0.062030889093875885, val loss: 0.08477230370044708\n",
      "Epoch 7533: train loss: 0.06201362609863281, val loss: 0.08476283401250839\n",
      "Epoch 7534: train loss: 0.06199640780687332, val loss: 0.08475354313850403\n",
      "Epoch 7535: train loss: 0.06197919324040413, val loss: 0.08474428951740265\n",
      "Epoch 7536: train loss: 0.06196201965212822, val loss: 0.08473493903875351\n",
      "Epoch 7537: train loss: 0.061944860965013504, val loss: 0.0847257599234581\n",
      "Epoch 7538: train loss: 0.06192775070667267, val loss: 0.08471651375293732\n",
      "Epoch 7539: train loss: 0.06191067770123482, val loss: 0.08470725268125534\n",
      "Epoch 7540: train loss: 0.061893612146377563, val loss: 0.08469793200492859\n",
      "Epoch 7541: train loss: 0.0618765726685524, val loss: 0.08468872308731079\n",
      "Epoch 7542: train loss: 0.061859577894210815, val loss: 0.08467955887317657\n",
      "Epoch 7543: train loss: 0.06184261292219162, val loss: 0.08467049151659012\n",
      "Epoch 7544: train loss: 0.061825647950172424, val loss: 0.08466146141290665\n",
      "Epoch 7545: train loss: 0.06180872395634651, val loss: 0.08465240150690079\n",
      "Epoch 7546: train loss: 0.06179184094071388, val loss: 0.08464337140321732\n",
      "Epoch 7547: train loss: 0.061774976551532745, val loss: 0.08463429659605026\n",
      "Epoch 7548: train loss: 0.06175814941525459, val loss: 0.0846252515912056\n",
      "Epoch 7549: train loss: 0.061741337180137634, val loss: 0.08461619168519974\n",
      "Epoch 7550: train loss: 0.06172455474734306, val loss: 0.08460726588964462\n",
      "Epoch 7551: train loss: 0.06170779466629028, val loss: 0.08459834009408951\n",
      "Epoch 7552: train loss: 0.061691053211688995, val loss: 0.08458945900201797\n",
      "Epoch 7553: train loss: 0.06167435273528099, val loss: 0.08458060026168823\n",
      "Epoch 7554: train loss: 0.06165769696235657, val loss: 0.08457163721323013\n",
      "Epoch 7555: train loss: 0.061641037464141846, val loss: 0.08456286042928696\n",
      "Epoch 7556: train loss: 0.061624422669410706, val loss: 0.08455406129360199\n",
      "Epoch 7557: train loss: 0.061607830226421356, val loss: 0.08454526215791702\n",
      "Epoch 7558: train loss: 0.06159127876162529, val loss: 0.08453647792339325\n",
      "Epoch 7559: train loss: 0.06157474219799042, val loss: 0.08452777564525604\n",
      "Epoch 7560: train loss: 0.06155824288725853, val loss: 0.08451906591653824\n",
      "Epoch 7561: train loss: 0.06154175102710724, val loss: 0.0845104232430458\n",
      "Epoch 7562: train loss: 0.061525315046310425, val loss: 0.08450168371200562\n",
      "Epoch 7563: train loss: 0.06150887906551361, val loss: 0.08449303358793259\n",
      "Epoch 7564: train loss: 0.06149248778820038, val loss: 0.08448436111211777\n",
      "Epoch 7565: train loss: 0.061476126313209534, val loss: 0.08447577804327011\n",
      "Epoch 7566: train loss: 0.06145978718996048, val loss: 0.08446728438138962\n",
      "Epoch 7567: train loss: 0.06144348531961441, val loss: 0.0844588503241539\n",
      "Epoch 7568: train loss: 0.06142720207571983, val loss: 0.08445029705762863\n",
      "Epoch 7569: train loss: 0.06141092628240585, val loss: 0.0844416692852974\n",
      "Epoch 7570: train loss: 0.061394691467285156, val loss: 0.0844331830739975\n",
      "Epoch 7571: train loss: 0.06137849763035774, val loss: 0.08442480862140656\n",
      "Epoch 7572: train loss: 0.061362311244010925, val loss: 0.08441635221242905\n",
      "Epoch 7573: train loss: 0.06134616211056709, val loss: 0.0844079777598381\n",
      "Epoch 7574: train loss: 0.06133003532886505, val loss: 0.08439956605434418\n",
      "Epoch 7575: train loss: 0.0613139383494854, val loss: 0.08439121395349503\n",
      "Epoch 7576: train loss: 0.06129785254597664, val loss: 0.08438283205032349\n",
      "Epoch 7577: train loss: 0.06128181889653206, val loss: 0.08437453955411911\n",
      "Epoch 7578: train loss: 0.06126580014824867, val loss: 0.08436628431081772\n",
      "Epoch 7579: train loss: 0.061249807476997375, val loss: 0.08435805886983871\n",
      "Epoch 7580: train loss: 0.06123382970690727, val loss: 0.0843498483300209\n",
      "Epoch 7581: train loss: 0.06121791526675224, val loss: 0.08434166759252548\n",
      "Epoch 7582: train loss: 0.06120198592543602, val loss: 0.0843333899974823\n",
      "Epoch 7583: train loss: 0.061186108738183975, val loss: 0.08432519435882568\n",
      "Epoch 7584: train loss: 0.06117026507854462, val loss: 0.08431708067655563\n",
      "Epoch 7585: train loss: 0.061154428869485855, val loss: 0.08430894464254379\n",
      "Epoch 7586: train loss: 0.061138641089200974, val loss: 0.08430089801549911\n",
      "Epoch 7587: train loss: 0.06112287566065788, val loss: 0.08429279923439026\n",
      "Epoch 7588: train loss: 0.06110712140798569, val loss: 0.08428476005792618\n",
      "Epoch 7589: train loss: 0.061091404408216476, val loss: 0.08427668362855911\n",
      "Epoch 7590: train loss: 0.06107569858431816, val loss: 0.08426865935325623\n",
      "Epoch 7591: train loss: 0.06106004863977432, val loss: 0.08426070213317871\n",
      "Epoch 7592: train loss: 0.06104441359639168, val loss: 0.08425286412239075\n",
      "Epoch 7593: train loss: 0.06102877855300903, val loss: 0.08424490690231323\n",
      "Epoch 7594: train loss: 0.06101319193840027, val loss: 0.0842369869351387\n",
      "Epoch 7595: train loss: 0.06099763140082359, val loss: 0.08422895520925522\n",
      "Epoch 7596: train loss: 0.06098208203911781, val loss: 0.08422113955020905\n",
      "Epoch 7597: train loss: 0.060966577380895615, val loss: 0.0842132717370987\n",
      "Epoch 7598: train loss: 0.06095109507441521, val loss: 0.08420544862747192\n",
      "Epoch 7599: train loss: 0.06093562766909599, val loss: 0.08419759571552277\n",
      "Epoch 7600: train loss: 0.060920193791389465, val loss: 0.08418993651866913\n",
      "Epoch 7601: train loss: 0.06090477854013443, val loss: 0.08418231457471848\n",
      "Epoch 7602: train loss: 0.06088940426707268, val loss: 0.08417453616857529\n",
      "Epoch 7603: train loss: 0.06087404862046242, val loss: 0.08416681736707687\n",
      "Epoch 7604: train loss: 0.060858726501464844, val loss: 0.08415903151035309\n",
      "Epoch 7605: train loss: 0.060843415558338165, val loss: 0.08415137231349945\n",
      "Epoch 7606: train loss: 0.060828156769275665, val loss: 0.08414372056722641\n",
      "Epoch 7607: train loss: 0.06081290543079376, val loss: 0.08413614332675934\n",
      "Epoch 7608: train loss: 0.06079768389463425, val loss: 0.08412858098745346\n",
      "Epoch 7609: train loss: 0.06078248843550682, val loss: 0.08412104099988937\n",
      "Epoch 7610: train loss: 0.060767319053411484, val loss: 0.08411344140768051\n",
      "Epoch 7611: train loss: 0.06075218319892883, val loss: 0.0841059610247612\n",
      "Epoch 7612: train loss: 0.06073708087205887, val loss: 0.08409838378429413\n",
      "Epoch 7613: train loss: 0.0607219822704792, val loss: 0.0840909332036972\n",
      "Epoch 7614: train loss: 0.06070692464709282, val loss: 0.08408349007368088\n",
      "Epoch 7615: train loss: 0.06069187819957733, val loss: 0.08407612890005112\n",
      "Epoch 7616: train loss: 0.06067687273025513, val loss: 0.0840686783194542\n",
      "Epoch 7617: train loss: 0.06066187843680382, val loss: 0.08406127989292145\n",
      "Epoch 7618: train loss: 0.060646917670965195, val loss: 0.08405381441116333\n",
      "Epoch 7619: train loss: 0.06063198298215866, val loss: 0.08404657989740372\n",
      "Epoch 7620: train loss: 0.060617055743932724, val loss: 0.08403921872377396\n",
      "Epoch 7621: train loss: 0.06060216203331947, val loss: 0.08403191715478897\n",
      "Epoch 7622: train loss: 0.06058729067444801, val loss: 0.08402464538812637\n",
      "Epoch 7623: train loss: 0.06057246774435043, val loss: 0.08401736617088318\n",
      "Epoch 7624: train loss: 0.06055765226483345, val loss: 0.08401019871234894\n",
      "Epoch 7625: train loss: 0.06054287031292915, val loss: 0.08400299400091171\n",
      "Epoch 7626: train loss: 0.06052811071276665, val loss: 0.0839957520365715\n",
      "Epoch 7627: train loss: 0.060513369739055634, val loss: 0.08398854732513428\n",
      "Epoch 7628: train loss: 0.06049865856766701, val loss: 0.08398144692182541\n",
      "Epoch 7629: train loss: 0.06048399582505226, val loss: 0.08397432416677475\n",
      "Epoch 7630: train loss: 0.06046932190656662, val loss: 0.08396724611520767\n",
      "Epoch 7631: train loss: 0.06045470014214516, val loss: 0.08396011590957642\n",
      "Epoch 7632: train loss: 0.060440097004175186, val loss: 0.08395308256149292\n",
      "Epoch 7633: train loss: 0.060425519943237305, val loss: 0.08394602686166763\n",
      "Epoch 7634: train loss: 0.06041095778346062, val loss: 0.0839390680193901\n",
      "Epoch 7635: train loss: 0.06039641797542572, val loss: 0.08393213152885437\n",
      "Epoch 7636: train loss: 0.060381922870874405, val loss: 0.08392512053251266\n",
      "Epoch 7637: train loss: 0.06036745756864548, val loss: 0.08391819149255753\n",
      "Epoch 7638: train loss: 0.06035298854112625, val loss: 0.0839112401008606\n",
      "Epoch 7639: train loss: 0.0603385791182518, val loss: 0.08390433341264725\n",
      "Epoch 7640: train loss: 0.06032416224479675, val loss: 0.08389737457036972\n",
      "Epoch 7641: train loss: 0.06030980870127678, val loss: 0.08389055728912354\n",
      "Epoch 7642: train loss: 0.06029543653130531, val loss: 0.08388371765613556\n",
      "Epoch 7643: train loss: 0.060281094163656235, val loss: 0.08387694507837296\n",
      "Epoch 7644: train loss: 0.060266803950071335, val loss: 0.08387016505002975\n",
      "Epoch 7645: train loss: 0.06025252118706703, val loss: 0.08386329561471939\n",
      "Epoch 7646: train loss: 0.06023825705051422, val loss: 0.0838564857840538\n",
      "Epoch 7647: train loss: 0.0602240152657032, val loss: 0.08384978026151657\n",
      "Epoch 7648: train loss: 0.060209810733795166, val loss: 0.08384311944246292\n",
      "Epoch 7649: train loss: 0.06019563972949982, val loss: 0.08383648842573166\n",
      "Epoch 7650: train loss: 0.060181472450494766, val loss: 0.0838298350572586\n",
      "Epoch 7651: train loss: 0.060167346149683, val loss: 0.08382311463356018\n",
      "Epoch 7652: train loss: 0.06015324592590332, val loss: 0.08381638675928116\n",
      "Epoch 7653: train loss: 0.060139138251543045, val loss: 0.08380980789661407\n",
      "Epoch 7654: train loss: 0.06012508645653725, val loss: 0.08380328118801117\n",
      "Epoch 7655: train loss: 0.06011105701327324, val loss: 0.08379676192998886\n",
      "Epoch 7656: train loss: 0.060097042471170425, val loss: 0.08379019796848297\n",
      "Epoch 7657: train loss: 0.06008307263255119, val loss: 0.08378369361162186\n",
      "Epoch 7658: train loss: 0.060069113969802856, val loss: 0.08377712965011597\n",
      "Epoch 7659: train loss: 0.060055166482925415, val loss: 0.08377066254615784\n",
      "Epoch 7660: train loss: 0.06004127115011215, val loss: 0.08376413583755493\n",
      "Epoch 7661: train loss: 0.06002739071846008, val loss: 0.08375769853591919\n",
      "Epoch 7662: train loss: 0.0600135438144207, val loss: 0.08375140279531479\n",
      "Epoch 7663: train loss: 0.059999678283929825, val loss: 0.08374501019716263\n",
      "Epoch 7664: train loss: 0.05998588353395462, val loss: 0.08373861759901047\n",
      "Epoch 7665: train loss: 0.05997209623456001, val loss: 0.08373221009969711\n",
      "Epoch 7666: train loss: 0.05995834991335869, val loss: 0.08372588455677032\n",
      "Epoch 7667: train loss: 0.05994459241628647, val loss: 0.08371952176094055\n",
      "Epoch 7668: train loss: 0.05993088334798813, val loss: 0.08371331542730331\n",
      "Epoch 7669: train loss: 0.059917204082012177, val loss: 0.08370710909366608\n",
      "Epoch 7670: train loss: 0.05990353226661682, val loss: 0.08370079845190048\n",
      "Epoch 7671: train loss: 0.059889886528253555, val loss: 0.08369459956884384\n",
      "Epoch 7672: train loss: 0.059876251965761185, val loss: 0.08368835598230362\n",
      "Epoch 7673: train loss: 0.059862662106752396, val loss: 0.08368207514286041\n",
      "Epoch 7674: train loss: 0.0598490945994854, val loss: 0.08367595821619034\n",
      "Epoch 7675: train loss: 0.05983554571866989, val loss: 0.08366979658603668\n",
      "Epoch 7676: train loss: 0.05982200801372528, val loss: 0.08366371691226959\n",
      "Epoch 7677: train loss: 0.059808529913425446, val loss: 0.08365759998559952\n",
      "Epoch 7678: train loss: 0.059795040637254715, val loss: 0.08365146815776825\n",
      "Epoch 7679: train loss: 0.059781596064567566, val loss: 0.08364539593458176\n",
      "Epoch 7680: train loss: 0.05976816266775131, val loss: 0.08363936096429825\n",
      "Epoch 7681: train loss: 0.059754759073257446, val loss: 0.0836334303021431\n",
      "Epoch 7682: train loss: 0.05974138155579567, val loss: 0.0836273804306984\n",
      "Epoch 7683: train loss: 0.05972801893949509, val loss: 0.08362135291099548\n",
      "Epoch 7684: train loss: 0.059714701026678085, val loss: 0.08361538499593735\n",
      "Epoch 7685: train loss: 0.05970139056444168, val loss: 0.08360940963029861\n",
      "Epoch 7686: train loss: 0.05968811362981796, val loss: 0.08360351622104645\n",
      "Epoch 7687: train loss: 0.059674859046936035, val loss: 0.08359762281179428\n",
      "Epoch 7688: train loss: 0.0596616230905056, val loss: 0.08359171450138092\n",
      "Epoch 7689: train loss: 0.05964842066168785, val loss: 0.08358579128980637\n",
      "Epoch 7690: train loss: 0.0596352256834507, val loss: 0.08358000218868256\n",
      "Epoch 7691: train loss: 0.059622060507535934, val loss: 0.08357414603233337\n",
      "Epoch 7692: train loss: 0.05960892140865326, val loss: 0.08356834203004837\n",
      "Epoch 7693: train loss: 0.05959579721093178, val loss: 0.08356261253356934\n",
      "Epoch 7694: train loss: 0.05958272144198418, val loss: 0.0835568904876709\n",
      "Epoch 7695: train loss: 0.05956965312361717, val loss: 0.08355116844177246\n",
      "Epoch 7696: train loss: 0.059556592255830765, val loss: 0.08354534953832626\n",
      "Epoch 7697: train loss: 0.05954354628920555, val loss: 0.08353965729475021\n",
      "Epoch 7698: train loss: 0.059530556201934814, val loss: 0.08353396505117416\n",
      "Epoch 7699: train loss: 0.059517569839954376, val loss: 0.0835283175110817\n",
      "Epoch 7700: train loss: 0.05950460955500603, val loss: 0.08352264016866684\n",
      "Epoch 7701: train loss: 0.059491679072380066, val loss: 0.08351705223321915\n",
      "Epoch 7702: train loss: 0.0594787672162056, val loss: 0.08351143449544907\n",
      "Epoch 7703: train loss: 0.05946588143706322, val loss: 0.08350584656000137\n",
      "Epoch 7704: train loss: 0.05945302173495293, val loss: 0.08350025862455368\n",
      "Epoch 7705: train loss: 0.05944018065929413, val loss: 0.08349474519491196\n",
      "Epoch 7706: train loss: 0.05942736193537712, val loss: 0.08348926901817322\n",
      "Epoch 7707: train loss: 0.0594145692884922, val loss: 0.08348370343446732\n",
      "Epoch 7708: train loss: 0.05940179526805878, val loss: 0.0834781676530838\n",
      "Epoch 7709: train loss: 0.059389036148786545, val loss: 0.08347266912460327\n",
      "Epoch 7710: train loss: 0.05937632545828819, val loss: 0.0834672674536705\n",
      "Epoch 7711: train loss: 0.05936362221837044, val loss: 0.08346187323331833\n",
      "Epoch 7712: train loss: 0.05935094878077507, val loss: 0.08345641195774078\n",
      "Epoch 7713: train loss: 0.059338293969631195, val loss: 0.08345097303390503\n",
      "Epoch 7714: train loss: 0.05932564288377762, val loss: 0.08344557136297226\n",
      "Epoch 7715: train loss: 0.05931304395198822, val loss: 0.08344025909900665\n",
      "Epoch 7716: train loss: 0.059300459921360016, val loss: 0.08343499898910522\n",
      "Epoch 7717: train loss: 0.0592879056930542, val loss: 0.08342965692281723\n",
      "Epoch 7718: train loss: 0.05927537381649017, val loss: 0.08342436701059341\n",
      "Epoch 7719: train loss: 0.05926283821463585, val loss: 0.08341903239488602\n",
      "Epoch 7720: train loss: 0.0592503659427166, val loss: 0.08341378718614578\n",
      "Epoch 7721: train loss: 0.05923788249492645, val loss: 0.08340855687856674\n",
      "Epoch 7722: train loss: 0.05922544375061989, val loss: 0.0834033414721489\n",
      "Epoch 7723: train loss: 0.05921301618218422, val loss: 0.08339812606573105\n",
      "Epoch 7724: train loss: 0.05920059606432915, val loss: 0.08339294046163559\n",
      "Epoch 7725: train loss: 0.05918822064995766, val loss: 0.08338772505521774\n",
      "Epoch 7726: train loss: 0.059175848960876465, val loss: 0.08338262140750885\n",
      "Epoch 7727: train loss: 0.05916352570056915, val loss: 0.08337745815515518\n",
      "Epoch 7728: train loss: 0.059151194989681244, val loss: 0.08337239176034927\n",
      "Epoch 7729: train loss: 0.05913889780640602, val loss: 0.08336734026670456\n",
      "Epoch 7730: train loss: 0.05912662297487259, val loss: 0.08336228877305984\n",
      "Epoch 7731: train loss: 0.059114374220371246, val loss: 0.08335711807012558\n",
      "Epoch 7732: train loss: 0.0591021329164505, val loss: 0.08335214853286743\n",
      "Epoch 7733: train loss: 0.05908994749188423, val loss: 0.08334711939096451\n",
      "Epoch 7734: train loss: 0.05907776206731796, val loss: 0.08334210515022278\n",
      "Epoch 7735: train loss: 0.059065598994493484, val loss: 0.08333704620599747\n",
      "Epoch 7736: train loss: 0.0590534545481205, val loss: 0.08333217352628708\n",
      "Epoch 7737: train loss: 0.0590413436293602, val loss: 0.08332724869251251\n",
      "Epoch 7738: train loss: 0.05902924761176109, val loss: 0.08332239091396332\n",
      "Epoch 7739: train loss: 0.059017203748226166, val loss: 0.08331742882728577\n",
      "Epoch 7740: train loss: 0.059005144983530045, val loss: 0.08331258594989777\n",
      "Epoch 7741: train loss: 0.058993127197027206, val loss: 0.08330760151147842\n",
      "Epoch 7742: train loss: 0.05898111313581467, val loss: 0.08330285549163818\n",
      "Epoch 7743: train loss: 0.05896911770105362, val loss: 0.0832979679107666\n",
      "Epoch 7744: train loss: 0.058957163244485855, val loss: 0.08329320698976517\n",
      "Epoch 7745: train loss: 0.05894524231553078, val loss: 0.08328840136528015\n",
      "Epoch 7746: train loss: 0.058933328837156296, val loss: 0.08328362554311752\n",
      "Epoch 7747: train loss: 0.05892143398523331, val loss: 0.08327879756689072\n",
      "Epoch 7748: train loss: 0.05890956521034241, val loss: 0.08327405154705048\n",
      "Epoch 7749: train loss: 0.058897729963064194, val loss: 0.083269402384758\n",
      "Epoch 7750: train loss: 0.058885905891656876, val loss: 0.08326468616724014\n",
      "Epoch 7751: train loss: 0.058874089270830154, val loss: 0.08326005190610886\n",
      "Epoch 7752: train loss: 0.05886230990290642, val loss: 0.08325538784265518\n",
      "Epoch 7753: train loss: 0.058850545436143875, val loss: 0.08325070142745972\n",
      "Epoch 7754: train loss: 0.05883880332112312, val loss: 0.08324607461690903\n",
      "Epoch 7755: train loss: 0.05882706865668297, val loss: 0.08324140310287476\n",
      "Epoch 7756: train loss: 0.0588153712451458, val loss: 0.08323680609464645\n",
      "Epoch 7757: train loss: 0.05880369618535042, val loss: 0.08323221653699875\n",
      "Epoch 7758: train loss: 0.05879202485084534, val loss: 0.08322775363922119\n",
      "Epoch 7759: train loss: 0.05878039449453354, val loss: 0.08322323113679886\n",
      "Epoch 7760: train loss: 0.05876878276467323, val loss: 0.08321864902973175\n",
      "Epoch 7761: train loss: 0.058757197111845016, val loss: 0.08321411907672882\n",
      "Epoch 7762: train loss: 0.05874563008546829, val loss: 0.08320971578359604\n",
      "Epoch 7763: train loss: 0.05873408913612366, val loss: 0.08320524543523788\n",
      "Epoch 7764: train loss: 0.05872255563735962, val loss: 0.08320070803165436\n",
      "Epoch 7765: train loss: 0.05871104449033737, val loss: 0.0831962525844574\n",
      "Epoch 7766: train loss: 0.058699578046798706, val loss: 0.0831918865442276\n",
      "Epoch 7767: train loss: 0.058688100427389145, val loss: 0.08318747580051422\n",
      "Epoch 7768: train loss: 0.058676667511463165, val loss: 0.08318310230970383\n",
      "Epoch 7769: train loss: 0.058665234595537186, val loss: 0.08317872881889343\n",
      "Epoch 7770: train loss: 0.05865386500954628, val loss: 0.08317433297634125\n",
      "Epoch 7771: train loss: 0.05864247307181358, val loss: 0.08316998928785324\n",
      "Epoch 7772: train loss: 0.058631137013435364, val loss: 0.08316569030284882\n",
      "Epoch 7773: train loss: 0.05861978605389595, val loss: 0.08316146582365036\n",
      "Epoch 7774: train loss: 0.05860847979784012, val loss: 0.0831572413444519\n",
      "Epoch 7775: train loss: 0.05859719589352608, val loss: 0.08315295726060867\n",
      "Epoch 7776: train loss: 0.058585941791534424, val loss: 0.08314865827560425\n",
      "Epoch 7777: train loss: 0.05857468396425247, val loss: 0.083144411444664\n",
      "Epoch 7778: train loss: 0.05856345593929291, val loss: 0.08314020931720734\n",
      "Epoch 7779: train loss: 0.05855226516723633, val loss: 0.08313607424497604\n",
      "Epoch 7780: train loss: 0.05854107812047005, val loss: 0.08313190191984177\n",
      "Epoch 7781: train loss: 0.05852991342544556, val loss: 0.0831276997923851\n",
      "Epoch 7782: train loss: 0.05851876735687256, val loss: 0.08312356472015381\n",
      "Epoch 7783: train loss: 0.05850762501358986, val loss: 0.08311942964792252\n",
      "Epoch 7784: train loss: 0.05849653109908104, val loss: 0.08311537653207779\n",
      "Epoch 7785: train loss: 0.05848544463515282, val loss: 0.08311130851507187\n",
      "Epoch 7786: train loss: 0.05847437307238579, val loss: 0.08310722559690475\n",
      "Epoch 7787: train loss: 0.05846333131194115, val loss: 0.08310317248106003\n",
      "Epoch 7788: train loss: 0.0584523007273674, val loss: 0.0830991193652153\n",
      "Epoch 7789: train loss: 0.058441322296857834, val loss: 0.08309503644704819\n",
      "Epoch 7790: train loss: 0.05843034386634827, val loss: 0.08309101313352585\n",
      "Epoch 7791: train loss: 0.058419376611709595, val loss: 0.08308706432580948\n",
      "Epoch 7792: train loss: 0.05840844288468361, val loss: 0.0830831453204155\n",
      "Epoch 7793: train loss: 0.058397527784109116, val loss: 0.08307922631502151\n",
      "Epoch 7794: train loss: 0.05838664621114731, val loss: 0.08307520300149918\n",
      "Epoch 7795: train loss: 0.0583757609128952, val loss: 0.0830712616443634\n",
      "Epoch 7796: train loss: 0.058364901691675186, val loss: 0.08306736499071121\n",
      "Epoch 7797: train loss: 0.05835407227277756, val loss: 0.08306355774402618\n",
      "Epoch 7798: train loss: 0.058343254029750824, val loss: 0.08305973559617996\n",
      "Epoch 7799: train loss: 0.05833246558904648, val loss: 0.08305583894252777\n",
      "Epoch 7800: train loss: 0.05832168459892273, val loss: 0.08305201679468155\n",
      "Epoch 7801: train loss: 0.05831095576286316, val loss: 0.08304816484451294\n",
      "Epoch 7802: train loss: 0.058300212025642395, val loss: 0.0830443724989891\n",
      "Epoch 7803: train loss: 0.05828952044248581, val loss: 0.08304058760404587\n",
      "Epoch 7804: train loss: 0.05827881768345833, val loss: 0.08303683996200562\n",
      "Epoch 7805: train loss: 0.05826815590262413, val loss: 0.08303308486938477\n",
      "Epoch 7806: train loss: 0.058257512748241425, val loss: 0.08302926272153854\n",
      "Epoch 7807: train loss: 0.05824688449501991, val loss: 0.08302559703588486\n",
      "Epoch 7808: train loss: 0.058236271142959595, val loss: 0.08302193880081177\n",
      "Epoch 7809: train loss: 0.058225687593221664, val loss: 0.0830182433128357\n",
      "Epoch 7810: train loss: 0.05821511149406433, val loss: 0.0830145850777626\n",
      "Epoch 7811: train loss: 0.05820457637310028, val loss: 0.08301087468862534\n",
      "Epoch 7812: train loss: 0.05819404497742653, val loss: 0.08300722390413284\n",
      "Epoch 7813: train loss: 0.05818351358175278, val loss: 0.08300359547138214\n",
      "Epoch 7814: train loss: 0.058173034340143204, val loss: 0.08299997448921204\n",
      "Epoch 7815: train loss: 0.05816254764795303, val loss: 0.0829964354634285\n",
      "Epoch 7816: train loss: 0.05815210938453674, val loss: 0.08299287408590317\n",
      "Epoch 7817: train loss: 0.058141667395830154, val loss: 0.08298938721418381\n",
      "Epoch 7818: train loss: 0.05813126638531685, val loss: 0.08298579603433609\n",
      "Epoch 7819: train loss: 0.05812086537480354, val loss: 0.08298224955797195\n",
      "Epoch 7820: train loss: 0.05811050161719322, val loss: 0.08297866582870483\n",
      "Epoch 7821: train loss: 0.058100149035453796, val loss: 0.08297523111104965\n",
      "Epoch 7822: train loss: 0.058089833706617355, val loss: 0.08297181129455566\n",
      "Epoch 7823: train loss: 0.058079514652490616, val loss: 0.0829683393239975\n",
      "Epoch 7824: train loss: 0.05806924030184746, val loss: 0.08296488970518112\n",
      "Epoch 7825: train loss: 0.058058962225914, val loss: 0.08296146243810654\n",
      "Epoch 7826: train loss: 0.058048706501722336, val loss: 0.08295800536870956\n",
      "Epoch 7827: train loss: 0.05803848057985306, val loss: 0.08295466005802155\n",
      "Epoch 7828: train loss: 0.05802825465798378, val loss: 0.08295126259326935\n",
      "Epoch 7829: train loss: 0.05801808089017868, val loss: 0.08294789493083954\n",
      "Epoch 7830: train loss: 0.05800791084766388, val loss: 0.08294456452131271\n",
      "Epoch 7831: train loss: 0.05799775943160057, val loss: 0.08294118940830231\n",
      "Epoch 7832: train loss: 0.05798761546611786, val loss: 0.0829378291964531\n",
      "Epoch 7833: train loss: 0.057977523654699326, val loss: 0.08293461799621582\n",
      "Epoch 7834: train loss: 0.0579674206674099, val loss: 0.08293135464191437\n",
      "Epoch 7835: train loss: 0.05795735865831375, val loss: 0.08292807638645172\n",
      "Epoch 7836: train loss: 0.0579473078250885, val loss: 0.08292478322982788\n",
      "Epoch 7837: train loss: 0.05793728306889534, val loss: 0.08292155712842941\n",
      "Epoch 7838: train loss: 0.05792725458741188, val loss: 0.08291828632354736\n",
      "Epoch 7839: train loss: 0.05791725590825081, val loss: 0.08291511982679367\n",
      "Epoch 7840: train loss: 0.05790727585554123, val loss: 0.082911916077137\n",
      "Epoch 7841: train loss: 0.05789731442928314, val loss: 0.08290871232748032\n",
      "Epoch 7842: train loss: 0.057887375354766846, val loss: 0.08290555328130722\n",
      "Epoch 7843: train loss: 0.05787746608257294, val loss: 0.08290234208106995\n",
      "Epoch 7844: train loss: 0.05786754563450813, val loss: 0.08289922773838043\n",
      "Epoch 7845: train loss: 0.057857662439346313, val loss: 0.0828961730003357\n",
      "Epoch 7846: train loss: 0.05784781277179718, val loss: 0.08289308845996857\n",
      "Epoch 7847: train loss: 0.057837970554828644, val loss: 0.08289004862308502\n",
      "Epoch 7848: train loss: 0.057828132063150406, val loss: 0.08288698643445969\n",
      "Epoch 7849: train loss: 0.05781833082437515, val loss: 0.08288385719060898\n",
      "Epoch 7850: train loss: 0.057808537036180496, val loss: 0.08288078010082245\n",
      "Epoch 7851: train loss: 0.05779878422617912, val loss: 0.0828777551651001\n",
      "Epoch 7852: train loss: 0.057789042592048645, val loss: 0.08287476748228073\n",
      "Epoch 7853: train loss: 0.05777931213378906, val loss: 0.08287183940410614\n",
      "Epoch 7854: train loss: 0.057769596576690674, val loss: 0.08286882191896439\n",
      "Epoch 7855: train loss: 0.05775991082191467, val loss: 0.08286582678556442\n",
      "Epoch 7856: train loss: 0.05775024741888046, val loss: 0.08286284655332565\n",
      "Epoch 7857: train loss: 0.05774059519171715, val loss: 0.08285994082689285\n",
      "Epoch 7858: train loss: 0.05773095786571503, val loss: 0.08285704255104065\n",
      "Epoch 7859: train loss: 0.0577213391661644, val loss: 0.08285414427518845\n",
      "Epoch 7860: train loss: 0.057711757719516754, val loss: 0.08285130560398102\n",
      "Epoch 7861: train loss: 0.05770218372344971, val loss: 0.08284838497638702\n",
      "Epoch 7862: train loss: 0.05769260227680206, val loss: 0.0828455314040184\n",
      "Epoch 7863: train loss: 0.05768308788537979, val loss: 0.0828426405787468\n",
      "Epoch 7864: train loss: 0.057673562318086624, val loss: 0.08283986151218414\n",
      "Epoch 7865: train loss: 0.057664066553115845, val loss: 0.08283701539039612\n",
      "Epoch 7866: train loss: 0.057654574513435364, val loss: 0.08283424377441406\n",
      "Epoch 7867: train loss: 0.057645101100206375, val loss: 0.0828314945101738\n",
      "Epoch 7868: train loss: 0.057635657489299774, val loss: 0.08282861858606339\n",
      "Epoch 7869: train loss: 0.057626232504844666, val loss: 0.08282583206892014\n",
      "Epoch 7870: train loss: 0.05761681869626045, val loss: 0.08282321691513062\n",
      "Epoch 7871: train loss: 0.05760741978883743, val loss: 0.08282041549682617\n",
      "Epoch 7872: train loss: 0.0575980618596077, val loss: 0.08281775563955307\n",
      "Epoch 7873: train loss: 0.05758870765566826, val loss: 0.08281510323286057\n",
      "Epoch 7874: train loss: 0.057579342275857925, val loss: 0.08281242102384567\n",
      "Epoch 7875: train loss: 0.057570043951272964, val loss: 0.0828096941113472\n",
      "Epoch 7876: train loss: 0.057560741901397705, val loss: 0.08280699700117111\n",
      "Epoch 7877: train loss: 0.05755145475268364, val loss: 0.08280440419912338\n",
      "Epoch 7878: train loss: 0.05754220485687256, val loss: 0.08280178159475327\n",
      "Epoch 7879: train loss: 0.05753296986222267, val loss: 0.08279915153980255\n",
      "Epoch 7880: train loss: 0.05752372741699219, val loss: 0.08279650658369064\n",
      "Epoch 7881: train loss: 0.057514529675245285, val loss: 0.0827939435839653\n",
      "Epoch 7882: train loss: 0.05750533193349838, val loss: 0.08279144018888474\n",
      "Epoch 7883: train loss: 0.05749617889523506, val loss: 0.0827888771891594\n",
      "Epoch 7884: train loss: 0.057487037032842636, val loss: 0.08278631418943405\n",
      "Epoch 7885: train loss: 0.057477887719869614, val loss: 0.0827837586402893\n",
      "Epoch 7886: train loss: 0.05746876448392868, val loss: 0.08278125524520874\n",
      "Epoch 7887: train loss: 0.05745967850089073, val loss: 0.08277877420186996\n",
      "Epoch 7888: train loss: 0.05745060369372368, val loss: 0.08277629315853119\n",
      "Epoch 7889: train loss: 0.057441532611846924, val loss: 0.08277379721403122\n",
      "Epoch 7890: train loss: 0.057432498782873154, val loss: 0.0827714130282402\n",
      "Epoch 7891: train loss: 0.05742347240447998, val loss: 0.08276893198490143\n",
      "Epoch 7892: train loss: 0.057414453476667404, val loss: 0.08276644349098206\n",
      "Epoch 7893: train loss: 0.05740547180175781, val loss: 0.0827641412615776\n",
      "Epoch 7894: train loss: 0.05739649757742882, val loss: 0.08276169747114182\n",
      "Epoch 7895: train loss: 0.057387545704841614, val loss: 0.0827593058347702\n",
      "Epoch 7896: train loss: 0.0573786124587059, val loss: 0.08275693655014038\n",
      "Epoch 7897: train loss: 0.05736968293786049, val loss: 0.08275451511144638\n",
      "Epoch 7898: train loss: 0.057360779494047165, val loss: 0.08275224268436432\n",
      "Epoch 7899: train loss: 0.05735189840197563, val loss: 0.08274996280670166\n",
      "Epoch 7900: train loss: 0.057343024760484695, val loss: 0.08274760842323303\n",
      "Epoch 7901: train loss: 0.05733418092131615, val loss: 0.08274529874324799\n",
      "Epoch 7902: train loss: 0.057325344532728195, val loss: 0.08274301886558533\n",
      "Epoch 7903: train loss: 0.05731653794646263, val loss: 0.0827406644821167\n",
      "Epoch 7904: train loss: 0.05730773136019707, val loss: 0.0827384814620018\n",
      "Epoch 7905: train loss: 0.057298969477415085, val loss: 0.08273616433143616\n",
      "Epoch 7906: train loss: 0.0572902075946331, val loss: 0.08273404836654663\n",
      "Epoch 7907: train loss: 0.05728146433830261, val loss: 0.08273180574178696\n",
      "Epoch 7908: train loss: 0.05727274343371391, val loss: 0.08272957056760788\n",
      "Epoch 7909: train loss: 0.0572640523314476, val loss: 0.08272738754749298\n",
      "Epoch 7910: train loss: 0.05725535377860069, val loss: 0.08272518962621689\n",
      "Epoch 7911: train loss: 0.05724669620394707, val loss: 0.08272301405668259\n",
      "Epoch 7912: train loss: 0.057238027453422546, val loss: 0.08272089809179306\n",
      "Epoch 7913: train loss: 0.05722939223051071, val loss: 0.08271877467632294\n",
      "Epoch 7914: train loss: 0.05722076818346977, val loss: 0.08271665126085281\n",
      "Epoch 7915: train loss: 0.05721217766404152, val loss: 0.0827145203948021\n",
      "Epoch 7916: train loss: 0.05720360204577446, val loss: 0.08271228522062302\n",
      "Epoch 7917: train loss: 0.057195018976926804, val loss: 0.08271023631095886\n",
      "Epoch 7918: train loss: 0.05718647688627243, val loss: 0.08270823210477829\n",
      "Epoch 7919: train loss: 0.05717792734503746, val loss: 0.08270629495382309\n",
      "Epoch 7920: train loss: 0.057169437408447266, val loss: 0.08270418643951416\n",
      "Epoch 7921: train loss: 0.05716091766953468, val loss: 0.08270210772752762\n",
      "Epoch 7922: train loss: 0.05715244263410568, val loss: 0.0826999843120575\n",
      "Epoch 7923: train loss: 0.05714396387338638, val loss: 0.0826980397105217\n",
      "Epoch 7924: train loss: 0.05713552609086037, val loss: 0.08269612491130829\n",
      "Epoch 7925: train loss: 0.05712708458304405, val loss: 0.0826941579580307\n",
      "Epoch 7926: train loss: 0.05711868032813072, val loss: 0.0826922208070755\n",
      "Epoch 7927: train loss: 0.05711027979850769, val loss: 0.08269026130437851\n",
      "Epoch 7928: train loss: 0.05710190162062645, val loss: 0.08268821984529495\n",
      "Epoch 7929: train loss: 0.057093556970357895, val loss: 0.08268626779317856\n",
      "Epoch 7930: train loss: 0.05708521977066994, val loss: 0.08268441259860992\n",
      "Epoch 7931: train loss: 0.057076867669820786, val loss: 0.08268248289823532\n",
      "Epoch 7932: train loss: 0.05706856772303581, val loss: 0.08268062770366669\n",
      "Epoch 7933: train loss: 0.057060275226831436, val loss: 0.08267880231142044\n",
      "Epoch 7934: train loss: 0.05705200880765915, val loss: 0.08267688751220703\n",
      "Epoch 7935: train loss: 0.05704372376203537, val loss: 0.08267504721879959\n",
      "Epoch 7936: train loss: 0.057035502046346664, val loss: 0.08267321437597275\n",
      "Epoch 7937: train loss: 0.05702727660536766, val loss: 0.0826713815331459\n",
      "Epoch 7938: train loss: 0.05701906606554985, val loss: 0.08266959339380264\n",
      "Epoch 7939: train loss: 0.05701088160276413, val loss: 0.08266781270503998\n",
      "Epoch 7940: train loss: 0.05700270086526871, val loss: 0.08266597986221313\n",
      "Epoch 7941: train loss: 0.05699456110596657, val loss: 0.08266416937112808\n",
      "Epoch 7942: train loss: 0.05698638781905174, val loss: 0.08266245573759079\n",
      "Epoch 7943: train loss: 0.056978270411491394, val loss: 0.08266061544418335\n",
      "Epoch 7944: train loss: 0.05697016417980194, val loss: 0.08265898376703262\n",
      "Epoch 7945: train loss: 0.056962061673402786, val loss: 0.08265724033117294\n",
      "Epoch 7946: train loss: 0.05695397034287453, val loss: 0.08265551179647446\n",
      "Epoch 7947: train loss: 0.05694591999053955, val loss: 0.08265387266874313\n",
      "Epoch 7948: train loss: 0.05693788826465607, val loss: 0.08265207707881927\n",
      "Epoch 7949: train loss: 0.056929852813482285, val loss: 0.08265049755573273\n",
      "Epoch 7950: train loss: 0.056921832263469696, val loss: 0.08264882862567902\n",
      "Epoch 7951: train loss: 0.05691385269165039, val loss: 0.08264724165201187\n",
      "Epoch 7952: train loss: 0.05690585821866989, val loss: 0.08264554291963577\n",
      "Epoch 7953: train loss: 0.056897904723882675, val loss: 0.08264393359422684\n",
      "Epoch 7954: train loss: 0.05688996985554695, val loss: 0.08264230936765671\n",
      "Epoch 7955: train loss: 0.056882042437791824, val loss: 0.08264067769050598\n",
      "Epoch 7956: train loss: 0.05687412992119789, val loss: 0.08263909071683884\n",
      "Epoch 7957: train loss: 0.05686623230576515, val loss: 0.08263760060071945\n",
      "Epoch 7958: train loss: 0.05685834586620331, val loss: 0.08263597637414932\n",
      "Epoch 7959: train loss: 0.05685049295425415, val loss: 0.08263447135686874\n",
      "Epoch 7960: train loss: 0.05684265121817589, val loss: 0.08263285458087921\n",
      "Epoch 7961: train loss: 0.05683482810854912, val loss: 0.08263140171766281\n",
      "Epoch 7962: train loss: 0.05682700127363205, val loss: 0.08262985199689865\n",
      "Epoch 7963: train loss: 0.05681922659277916, val loss: 0.08262839168310165\n",
      "Epoch 7964: train loss: 0.05681142956018448, val loss: 0.08262688666582108\n",
      "Epoch 7965: train loss: 0.056803662329912186, val loss: 0.08262541145086288\n",
      "Epoch 7966: train loss: 0.056795910000801086, val loss: 0.08262389898300171\n",
      "Epoch 7967: train loss: 0.05678816884756088, val loss: 0.08262249827384949\n",
      "Epoch 7968: train loss: 0.056780457496643066, val loss: 0.0826210305094719\n",
      "Epoch 7969: train loss: 0.05677275359630585, val loss: 0.08261962234973907\n",
      "Epoch 7970: train loss: 0.05676507204771042, val loss: 0.08261828124523163\n",
      "Epoch 7971: train loss: 0.05675739794969559, val loss: 0.08261671662330627\n",
      "Epoch 7972: train loss: 0.05674973875284195, val loss: 0.08261535316705704\n",
      "Epoch 7973: train loss: 0.0567421019077301, val loss: 0.08261401951313019\n",
      "Epoch 7974: train loss: 0.05673447996377945, val loss: 0.08261268585920334\n",
      "Epoch 7975: train loss: 0.05672686919569969, val loss: 0.0826113224029541\n",
      "Epoch 7976: train loss: 0.05671929940581322, val loss: 0.08261000365018845\n",
      "Epoch 7977: train loss: 0.05671170726418495, val loss: 0.082608662545681\n",
      "Epoch 7978: train loss: 0.056704163551330566, val loss: 0.08260737359523773\n",
      "Epoch 7979: train loss: 0.05669660121202469, val loss: 0.08260603994131088\n",
      "Epoch 7980: train loss: 0.05668908357620239, val loss: 0.08260469883680344\n",
      "Epoch 07981: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 7981: train loss: 0.05668158084154129, val loss: 0.08260341733694077\n",
      "Epoch 7982: train loss: 0.05667407438158989, val loss: 0.08260305970907211\n",
      "Epoch 7983: train loss: 0.05667184293270111, val loss: 0.08260257542133331\n",
      "Epoch 7984: train loss: 0.056669607758522034, val loss: 0.08260225504636765\n",
      "Epoch 7985: train loss: 0.05666737258434296, val loss: 0.08260186016559601\n",
      "Epoch 7986: train loss: 0.056665126234292984, val loss: 0.08260153234004974\n",
      "Epoch 7987: train loss: 0.05666289106011391, val loss: 0.08260121196508408\n",
      "Epoch 07988: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 7988: train loss: 0.056660644710063934, val loss: 0.08260078728199005\n",
      "Epoch 7989: train loss: 0.056658416986465454, val loss: 0.08260064572095871\n",
      "Epoch 7990: train loss: 0.05665775015950203, val loss: 0.08260055631399155\n",
      "Epoch 7991: train loss: 0.05665707588195801, val loss: 0.08260047435760498\n",
      "Epoch 7992: train loss: 0.05665640905499458, val loss: 0.08260033279657364\n",
      "Epoch 7993: train loss: 0.056655727326869965, val loss: 0.08260022848844528\n",
      "Epoch 07994: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 7994: train loss: 0.05665506795048714, val loss: 0.08260013163089752\n",
      "Epoch 7995: train loss: 0.05665441229939461, val loss: 0.08260005712509155\n",
      "Epoch 7996: train loss: 0.0566541887819767, val loss: 0.08260004967451096\n",
      "Epoch 7997: train loss: 0.05665400251746178, val loss: 0.0825999304652214\n",
      "Epoch 7998: train loss: 0.05665378272533417, val loss: 0.08259990066289902\n",
      "Epoch 7999: train loss: 0.056653596460819244, val loss: 0.08259979635477066\n",
      "Epoch 08000: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 8000: train loss: 0.05665337294340134, val loss: 0.08259978145360947\n",
      "Epoch 8001: train loss: 0.05665316805243492, val loss: 0.08259979635477066\n",
      "Epoch 8002: train loss: 0.056653108447790146, val loss: 0.08259975165128708\n",
      "Epoch 8003: train loss: 0.05665303394198418, val loss: 0.08259973675012589\n",
      "Epoch 8004: train loss: 0.05665298178792, val loss: 0.08259975910186768\n",
      "Epoch 8005: train loss: 0.05665292218327522, val loss: 0.08259973675012589\n",
      "Epoch 08006: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 8006: train loss: 0.05665285512804985, val loss: 0.0825997143983841\n",
      "Early stop at epoch 8006\n",
      "Predictions saved to results-2-10----.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train).detach().cpu().numpy()\n",
    "scaler = StandardScaler()\n",
    "x_train_featured = scaler.fit_transform(x_train_featured)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy()).detach().cpu().numpy()\n",
    "x_test_featured = scaler.transform(x_test_featured)\n",
    "x_test_featured = torch.tensor(x_test_featured, dtype=torch.float).to(device)\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
