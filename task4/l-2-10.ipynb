{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "        # and then used to extract features from the training and test data.\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        nn.init.xavier_normal_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "        # defined in the constructor.\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    def make_feature(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, batch_size=256, eval_size=1000):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # model declaration\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement the training loop. The model should be trained on the pretraining data. Use validation set \n",
    "    # to monitor the loss.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 100\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    def make_features(x):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        x = x.to(device)\n",
    "        x = model.make_feature(x)\n",
    "        return x\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretraining_class(feature_extractors):\n",
    "    \"\"\"\n",
    "    The wrapper function which makes pretraining API compatible with sklearn pipeline\n",
    "    \n",
    "    input: feature_extractors: dict, a dictionary of feature extractors\n",
    "\n",
    "    output: PretrainedFeatures: class, a class which implements sklearn API\n",
    "    \"\"\"\n",
    "\n",
    "    class PretrainedFeatures(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"\n",
    "        The wrapper class for Pretraining pipeline.\n",
    "        \"\"\"\n",
    "        def __init__(self, *, feature_extractor=None, mode=None):\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.mode = mode\n",
    "\n",
    "        def fit(self, X=None, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            assert self.feature_extractor is not None\n",
    "            X_new = feature_extractors[self.feature_extractor](X)\n",
    "            return X_new\n",
    "        \n",
    "    return PretrainedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        \"\"\"\n",
    "        The model class, which defines our feature extractor used in pretraining.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            The constructor of the model.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "            # and then used to extract features from the training and test data.\n",
    "            self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "            # nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            The forward pass of the model.\n",
    "\n",
    "            input: x: torch.Tensor, the input to the model\n",
    "\n",
    "            output: x: torch.Tensor, the output of the model\n",
    "            \"\"\"\n",
    "            # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "            # defined in the constructor.\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # x = torch.tensor(X, dtype=torch.float)\n",
    "    x = X.clone().detach()\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(-1)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-7):\n",
    "            print(f\"Early stop at epoch {epoch+1}, loss: {loss}\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-2-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3738a716d3754886bdd1e27379efbe22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.5404117482000468, val loss: 0.2649177303314209\n",
      "Epoch 2: train loss: 0.21991576582801586, val loss: 0.1986224663257599\n",
      "Epoch 3: train loss: 0.17227975731966447, val loss: 0.1582759804725647\n",
      "Epoch 4: train loss: 0.1372458987929383, val loss: 0.11772543197870254\n",
      "Epoch 5: train loss: 0.10516012777965895, val loss: 0.09415727597475052\n",
      "Epoch 6: train loss: 0.08862929803862864, val loss: 0.0720019166469574\n",
      "Epoch 7: train loss: 0.06920652346586695, val loss: 0.06730549186468124\n",
      "Epoch 8: train loss: 0.05858527991783862, val loss: 0.0562630832195282\n",
      "Epoch 9: train loss: 0.048918799425266224, val loss: 0.05585264706611633\n",
      "Epoch 10: train loss: 0.04146150252405478, val loss: 0.03653193905949593\n",
      "Epoch 11: train loss: 0.034155345472146055, val loss: 0.03411168812215328\n",
      "Epoch 12: train loss: 0.028636854153810714, val loss: 0.030073319494724273\n",
      "Epoch 13: train loss: 0.024463322613008168, val loss: 0.021941289022564887\n",
      "Epoch 14: train loss: 0.019909592431418748, val loss: 0.01903874057531357\n",
      "Epoch 15: train loss: 0.016935190583826327, val loss: 0.015977563992142677\n",
      "Epoch 16: train loss: 0.014970024104781297, val loss: 0.015203515104949474\n",
      "Epoch 17: train loss: 0.011866824347297756, val loss: 0.013656316757202148\n",
      "Epoch 18: train loss: 0.010298199683275758, val loss: 0.011638071663677692\n",
      "Epoch 19: train loss: 0.008972630450007867, val loss: 0.01060271530598402\n",
      "Epoch 20: train loss: 0.008632697201809104, val loss: 0.009630531549453735\n",
      "Epoch 21: train loss: 0.007188329767359763, val loss: 0.007958998918533326\n",
      "Epoch 22: train loss: 0.006625748500836139, val loss: 0.006846026930958033\n",
      "Epoch 23: train loss: 0.006246850475973013, val loss: 0.007511856567114591\n",
      "Epoch 24: train loss: 0.006465016807752604, val loss: 0.00665004051849246\n",
      "Epoch 25: train loss: 0.005963498428463936, val loss: 0.006999165818095207\n",
      "Epoch 26: train loss: 0.0061355865088348486, val loss: 0.00651934314519167\n",
      "Epoch 27: train loss: 0.005666284107926245, val loss: 0.006141832675784826\n",
      "Epoch 28: train loss: 0.0056677632642035585, val loss: 0.005420313503593207\n",
      "Epoch 29: train loss: 0.005500698427536658, val loss: 0.006092088691890239\n",
      "Epoch 30: train loss: 0.0055491229752648846, val loss: 0.005812415760010481\n",
      "Epoch 31: train loss: 0.0053510046518426765, val loss: 0.006366267226636409\n",
      "Epoch 32: train loss: 0.005395652436100099, val loss: 0.006227757249027491\n",
      "Epoch 33: train loss: 0.005359053924375651, val loss: 0.005606973458081484\n",
      "Epoch 00034: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 34: train loss: 0.0053773542803008945, val loss: 0.005859622243791818\n",
      "Epoch 35: train loss: 0.0037535512083191045, val loss: 0.004056793235242367\n",
      "Epoch 36: train loss: 0.003029258307335632, val loss: 0.004095154132694006\n",
      "Epoch 37: train loss: 0.0029218687586942496, val loss: 0.004007034435868263\n",
      "Epoch 38: train loss: 0.002890228554910543, val loss: 0.004157598830759526\n",
      "Epoch 39: train loss: 0.00300860668840457, val loss: 0.004486195158213377\n",
      "Epoch 40: train loss: 0.0030108168711695743, val loss: 0.0037748331539332867\n",
      "Epoch 41: train loss: 0.0031192643671315544, val loss: 0.003733452657237649\n",
      "Epoch 42: train loss: 0.0032104152016417713, val loss: 0.004422285839915276\n",
      "Epoch 43: train loss: 0.0031845219546586885, val loss: 0.004740724410861731\n",
      "Epoch 44: train loss: 0.003182639691712601, val loss: 0.003904996398836374\n",
      "Epoch 45: train loss: 0.0031448106398539884, val loss: 0.003921300560235977\n",
      "Epoch 46: train loss: 0.0031645405369677714, val loss: 0.0038908007964491846\n",
      "Epoch 00047: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 47: train loss: 0.0031085430596555982, val loss: 0.004247463777661323\n",
      "Epoch 48: train loss: 0.002335307064941343, val loss: 0.0031471294388175012\n",
      "Epoch 49: train loss: 0.002029359547801468, val loss: 0.003188472531735897\n",
      "Epoch 50: train loss: 0.0019117439457842587, val loss: 0.00280872936360538\n",
      "Epoch 51: train loss: 0.0018761834192032717, val loss: 0.0031105235274881122\n",
      "Epoch 52: train loss: 0.001877664883982162, val loss: 0.003192294031381607\n",
      "Epoch 53: train loss: 0.0018620654887766862, val loss: 0.003504993846639991\n",
      "Epoch 54: train loss: 0.0018612680392986049, val loss: 0.0030532826241105797\n",
      "Epoch 55: train loss: 0.0018326639394294852, val loss: 0.0031731740757822992\n",
      "Epoch 00056: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 56: train loss: 0.0018287781836959173, val loss: 0.0029720755089074373\n",
      "Epoch 57: train loss: 0.001637862053300653, val loss: 0.0027781297750771048\n",
      "Epoch 58: train loss: 0.0014969774265868627, val loss: 0.0027064864505082368\n",
      "Epoch 59: train loss: 0.001473609425317572, val loss: 0.0028937446549534796\n",
      "Epoch 60: train loss: 0.0014607218616280932, val loss: 0.0027502398230135443\n",
      "Epoch 61: train loss: 0.0014396772506475753, val loss: 0.0028715058360248803\n",
      "Epoch 62: train loss: 0.001426357481229518, val loss: 0.002745256697759032\n",
      "Epoch 63: train loss: 0.0014372188339618092, val loss: 0.002780452888458967\n",
      "Epoch 00064: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 64: train loss: 0.0014148254152957578, val loss: 0.002836563276126981\n",
      "Epoch 65: train loss: 0.001339913266064713, val loss: 0.0025147990602999927\n",
      "Epoch 66: train loss: 0.0012927253393798458, val loss: 0.002629075150936842\n",
      "Epoch 67: train loss: 0.00128644154557236, val loss: 0.0028287027552723882\n",
      "Epoch 68: train loss: 0.0012942355748798166, val loss: 0.0027130338940769432\n",
      "Epoch 69: train loss: 0.001307781127737645, val loss: 0.0024812794867902993\n",
      "Epoch 70: train loss: 0.0012795925820436405, val loss: 0.0028424122463911773\n",
      "Epoch 71: train loss: 0.0012754752089136413, val loss: 0.0026265936829149724\n",
      "Epoch 72: train loss: 0.0012539287894888192, val loss: 0.0024835294578224422\n",
      "Epoch 73: train loss: 0.001265177926979959, val loss: 0.0026705932598561047\n",
      "Epoch 74: train loss: 0.0012455341761399594, val loss: 0.002519740676507354\n",
      "Epoch 00075: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 75: train loss: 0.001241218070043441, val loss: 0.0027355386428534983\n",
      "Epoch 76: train loss: 0.0012553881386827145, val loss: 0.0026749265100806953\n",
      "Epoch 77: train loss: 0.0012368807958210914, val loss: 0.0025845271069556476\n",
      "Epoch 78: train loss: 0.0012207725218934368, val loss: 0.0025907188542187215\n",
      "Epoch 79: train loss: 0.0012181641415559819, val loss: 0.0026355909425765274\n",
      "Epoch 80: train loss: 0.001217696249523029, val loss: 0.0026338803097605704\n",
      "Epoch 00081: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 81: train loss: 0.0012051962332381885, val loss: 0.002567801754921675\n",
      "Early stop at epoch 81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35d935be5ca417daafc2f5c18ac31d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 9.106086730957031\n",
      "Epoch 20: train loss: 8.697389602661133\n",
      "Epoch 30: train loss: 8.30089282989502\n",
      "Epoch 40: train loss: 7.9172892570495605\n",
      "Epoch 50: train loss: 7.546857833862305\n",
      "Epoch 60: train loss: 7.189591407775879\n",
      "Epoch 70: train loss: 6.845335483551025\n",
      "Epoch 80: train loss: 6.513853549957275\n",
      "Epoch 90: train loss: 6.194875240325928\n",
      "Epoch 100: train loss: 5.888116359710693\n",
      "Epoch 110: train loss: 5.593286991119385\n",
      "Epoch 120: train loss: 5.310095310211182\n",
      "Epoch 130: train loss: 5.0382490158081055\n",
      "Epoch 140: train loss: 4.777456283569336\n",
      "Epoch 150: train loss: 4.52742862701416\n",
      "Epoch 160: train loss: 4.287875652313232\n",
      "Epoch 170: train loss: 4.058510780334473\n",
      "Epoch 180: train loss: 3.839048385620117\n",
      "Epoch 190: train loss: 3.6292033195495605\n",
      "Epoch 200: train loss: 3.4286937713623047\n",
      "Epoch 210: train loss: 3.237239360809326\n",
      "Epoch 220: train loss: 3.0545623302459717\n",
      "Epoch 230: train loss: 2.880387544631958\n",
      "Epoch 240: train loss: 2.7144415378570557\n",
      "Epoch 250: train loss: 2.55645489692688\n",
      "Epoch 260: train loss: 2.406160593032837\n",
      "Epoch 270: train loss: 2.2632946968078613\n",
      "Epoch 280: train loss: 2.1275970935821533\n",
      "Epoch 290: train loss: 1.998810052871704\n",
      "Epoch 300: train loss: 1.8766814470291138\n",
      "Epoch 310: train loss: 1.7609612941741943\n",
      "Epoch 320: train loss: 1.6514043807983398\n",
      "Epoch 330: train loss: 1.5477697849273682\n",
      "Epoch 340: train loss: 1.4498205184936523\n",
      "Epoch 350: train loss: 1.3573250770568848\n",
      "Epoch 360: train loss: 1.2700550556182861\n",
      "Epoch 370: train loss: 1.187788724899292\n",
      "Epoch 380: train loss: 1.11030912399292\n",
      "Epoch 390: train loss: 1.0374020338058472\n",
      "Epoch 400: train loss: 0.968860924243927\n",
      "Epoch 410: train loss: 0.9044836163520813\n",
      "Epoch 420: train loss: 0.844073474407196\n",
      "Epoch 430: train loss: 0.7874391078948975\n",
      "Epoch 440: train loss: 0.7343947291374207\n",
      "Epoch 450: train loss: 0.6847608685493469\n",
      "Epoch 460: train loss: 0.6383621096611023\n",
      "Epoch 470: train loss: 0.5950301885604858\n",
      "Epoch 480: train loss: 0.5546017289161682\n",
      "Epoch 490: train loss: 0.5169194936752319\n",
      "Epoch 500: train loss: 0.48183149099349976\n",
      "Epoch 510: train loss: 0.44919171929359436\n",
      "Epoch 520: train loss: 0.41885992884635925\n",
      "Epoch 530: train loss: 0.3907010853290558\n",
      "Epoch 540: train loss: 0.3645860552787781\n",
      "Epoch 550: train loss: 0.34039103984832764\n",
      "Epoch 560: train loss: 0.31799763441085815\n",
      "Epoch 570: train loss: 0.29729288816452026\n",
      "Epoch 580: train loss: 0.27816909551620483\n",
      "Epoch 590: train loss: 0.2605236768722534\n",
      "Epoch 600: train loss: 0.2442588359117508\n",
      "Epoch 610: train loss: 0.2292821854352951\n",
      "Epoch 620: train loss: 0.21550554037094116\n",
      "Epoch 630: train loss: 0.20284610986709595\n",
      "Epoch 640: train loss: 0.19122470915317535\n",
      "Epoch 650: train loss: 0.18056735396385193\n",
      "Epoch 660: train loss: 0.17080368101596832\n",
      "Epoch 670: train loss: 0.1618678867816925\n",
      "Epoch 680: train loss: 0.15369783341884613\n",
      "Epoch 690: train loss: 0.14623519778251648\n",
      "Epoch 700: train loss: 0.13942547142505646\n",
      "Epoch 710: train loss: 0.1332174688577652\n",
      "Epoch 720: train loss: 0.1275632232427597\n",
      "Epoch 730: train loss: 0.12241818010807037\n",
      "Epoch 740: train loss: 0.1177407056093216\n",
      "Epoch 750: train loss: 0.11349204927682877\n",
      "Epoch 760: train loss: 0.10963618755340576\n",
      "Epoch 770: train loss: 0.10613952577114105\n",
      "Epoch 780: train loss: 0.102971151471138\n",
      "Epoch 790: train loss: 0.10010233521461487\n",
      "Epoch 800: train loss: 0.0975065529346466\n",
      "Epoch 810: train loss: 0.09515926241874695\n",
      "Epoch 820: train loss: 0.0930379256606102\n",
      "Epoch 830: train loss: 0.09112174063920975\n",
      "Epoch 840: train loss: 0.08939157426357269\n",
      "Epoch 850: train loss: 0.08782989531755447\n",
      "Epoch 860: train loss: 0.08642064034938812\n",
      "Epoch 870: train loss: 0.08514909446239471\n",
      "Epoch 880: train loss: 0.08400176465511322\n",
      "Epoch 890: train loss: 0.0829664096236229\n",
      "Epoch 900: train loss: 0.08203182369470596\n",
      "Epoch 910: train loss: 0.08118782192468643\n",
      "Epoch 920: train loss: 0.08042514324188232\n",
      "Epoch 930: train loss: 0.07973537594079971\n",
      "Epoch 940: train loss: 0.07911091297864914\n",
      "Epoch 950: train loss: 0.07854484021663666\n",
      "Epoch 960: train loss: 0.07803094387054443\n",
      "Epoch 970: train loss: 0.07756362110376358\n",
      "Epoch 980: train loss: 0.077137790620327\n",
      "Epoch 990: train loss: 0.07674887031316757\n",
      "Epoch 1000: train loss: 0.0763927698135376\n",
      "Epoch 1010: train loss: 0.0760657787322998\n",
      "Epoch 1020: train loss: 0.07576465606689453\n",
      "Epoch 1030: train loss: 0.07548639923334122\n",
      "Epoch 1040: train loss: 0.07522838562726974\n",
      "Epoch 1050: train loss: 0.07498826831579208\n",
      "Epoch 1060: train loss: 0.07476391643285751\n",
      "Epoch 1070: train loss: 0.07455351203680038\n",
      "Epoch 1080: train loss: 0.0743553414940834\n",
      "Epoch 1090: train loss: 0.07416795194149017\n",
      "Epoch 1100: train loss: 0.07399006187915802\n",
      "Epoch 1110: train loss: 0.07382049411535263\n",
      "Epoch 1120: train loss: 0.07365822792053223\n",
      "Epoch 1130: train loss: 0.07350236922502518\n",
      "Epoch 1140: train loss: 0.07335212826728821\n",
      "Epoch 1150: train loss: 0.07320679724216461\n",
      "Epoch 1160: train loss: 0.07306576520204544\n",
      "Epoch 1170: train loss: 0.07292848825454712\n",
      "Epoch 1180: train loss: 0.07279454171657562\n",
      "Epoch 1190: train loss: 0.07266347855329514\n",
      "Epoch 1200: train loss: 0.072534941136837\n",
      "Epoch 1210: train loss: 0.07240863144397736\n",
      "Epoch 1220: train loss: 0.0722842887043953\n",
      "Epoch 1230: train loss: 0.07216164469718933\n",
      "Epoch 1240: train loss: 0.07204053550958633\n",
      "Epoch 1250: train loss: 0.07192075252532959\n",
      "Epoch 1260: train loss: 0.07180218398571014\n",
      "Epoch 1270: train loss: 0.07168467342853546\n",
      "Epoch 1280: train loss: 0.07156810909509659\n",
      "Epoch 1290: train loss: 0.07145240902900696\n",
      "Epoch 1300: train loss: 0.07133749127388\n",
      "Epoch 1310: train loss: 0.07122328877449036\n",
      "Epoch 1320: train loss: 0.07110973447561264\n",
      "Epoch 1330: train loss: 0.07099678367376328\n",
      "Epoch 1340: train loss: 0.07088440656661987\n",
      "Epoch 1350: train loss: 0.07077258080244064\n",
      "Epoch 1360: train loss: 0.07066123932600021\n",
      "Epoch 1370: train loss: 0.07055038958787918\n",
      "Epoch 1380: train loss: 0.07044003158807755\n",
      "Epoch 1390: train loss: 0.07033009827136993\n",
      "Epoch 1400: train loss: 0.07022063434123993\n",
      "Epoch 1410: train loss: 0.07011159509420395\n",
      "Epoch 1420: train loss: 0.07000298798084259\n",
      "Epoch 1430: train loss: 0.06989482045173645\n",
      "Epoch 1440: train loss: 0.06978707760572433\n",
      "Epoch 1450: train loss: 0.06967974454164505\n",
      "Epoch 1460: train loss: 0.06957283616065979\n",
      "Epoch 1470: train loss: 0.06946637481451035\n",
      "Epoch 1480: train loss: 0.06936033815145493\n",
      "Epoch 1490: train loss: 0.06925473362207413\n",
      "Epoch 1500: train loss: 0.06914956122636795\n",
      "Epoch 1510: train loss: 0.06904482841491699\n",
      "Epoch 1520: train loss: 0.06894055008888245\n",
      "Epoch 1530: train loss: 0.0688367411494255\n",
      "Epoch 1540: train loss: 0.06873335689306259\n",
      "Epoch 1550: train loss: 0.06863045692443848\n",
      "Epoch 1560: train loss: 0.06852801144123077\n",
      "Epoch 1570: train loss: 0.06842604279518127\n",
      "Epoch 1580: train loss: 0.06832455843687057\n",
      "Epoch 1590: train loss: 0.06822355836629868\n",
      "Epoch 1600: train loss: 0.06812302768230438\n",
      "Epoch 1610: train loss: 0.06802301108837128\n",
      "Epoch 1620: train loss: 0.06792349368333817\n",
      "Epoch 1630: train loss: 0.06782449781894684\n",
      "Epoch 1640: train loss: 0.06772597879171371\n",
      "Epoch 1650: train loss: 0.06762800365686417\n",
      "Epoch 1660: train loss: 0.06753053516149521\n",
      "Epoch 1670: train loss: 0.06743360310792923\n",
      "Epoch 1680: train loss: 0.06733720004558563\n",
      "Epoch 1690: train loss: 0.06724132597446442\n",
      "Epoch 1700: train loss: 0.06714600324630737\n",
      "Epoch 1710: train loss: 0.0670512244105339\n",
      "Epoch 1720: train loss: 0.06695699691772461\n",
      "Epoch 1730: train loss: 0.06686333566904068\n",
      "Epoch 1740: train loss: 0.06677022576332092\n",
      "Epoch 1750: train loss: 0.06667768955230713\n",
      "Epoch 1760: train loss: 0.0665857121348381\n",
      "Epoch 1770: train loss: 0.06649430841207504\n",
      "Epoch 1780: train loss: 0.06640350073575974\n",
      "Epoch 1790: train loss: 0.0663132593035698\n",
      "Epoch 1800: train loss: 0.06622360646724701\n",
      "Epoch 1810: train loss: 0.06613453477621078\n",
      "Epoch 1820: train loss: 0.06604604423046112\n",
      "Epoch 1830: train loss: 0.06595815718173981\n",
      "Epoch 1840: train loss: 0.06587088108062744\n",
      "Epoch 1850: train loss: 0.06578420847654343\n",
      "Epoch 1860: train loss: 0.06569810956716537\n",
      "Epoch 1870: train loss: 0.06561264395713806\n",
      "Epoch 1880: train loss: 0.0655277818441391\n",
      "Epoch 1890: train loss: 0.06544353067874908\n",
      "Epoch 1900: train loss: 0.06535991281270981\n",
      "Epoch 1910: train loss: 0.06527689844369888\n",
      "Epoch 1920: train loss: 0.0651945173740387\n",
      "Epoch 1930: train loss: 0.06511275470256805\n",
      "Epoch 1940: train loss: 0.06503160297870636\n",
      "Epoch 1950: train loss: 0.0649510994553566\n",
      "Epoch 1960: train loss: 0.06487122178077698\n",
      "Epoch 1970: train loss: 0.0647919625043869\n",
      "Epoch 1980: train loss: 0.06471333652734756\n",
      "Epoch 1990: train loss: 0.06463534384965897\n",
      "Epoch 2000: train loss: 0.06455796957015991\n",
      "Epoch 2010: train loss: 0.06448125839233398\n",
      "Epoch 2020: train loss: 0.0644051805138588\n",
      "Epoch 2030: train loss: 0.06432974338531494\n",
      "Epoch 2040: train loss: 0.06425496190786362\n",
      "Epoch 2050: train loss: 0.06418079882860184\n",
      "Epoch 2060: train loss: 0.06410729140043259\n",
      "Epoch 2070: train loss: 0.06403440982103348\n",
      "Epoch 2080: train loss: 0.0639621838927269\n",
      "Epoch 2090: train loss: 0.06389059871435165\n",
      "Epoch 2100: train loss: 0.06381965428590775\n",
      "Epoch 2110: train loss: 0.06374935060739517\n",
      "Epoch 2120: train loss: 0.06367969512939453\n",
      "Epoch 2130: train loss: 0.06361068040132523\n",
      "Epoch 2140: train loss: 0.06354231387376785\n",
      "Epoch 2150: train loss: 0.06347458809614182\n",
      "Epoch 2160: train loss: 0.06340750306844711\n",
      "Epoch 2170: train loss: 0.06334106624126434\n",
      "Epoch 2180: train loss: 0.06327527016401291\n",
      "Epoch 2190: train loss: 0.06321012228727341\n",
      "Epoch 2200: train loss: 0.06314560025930405\n",
      "Epoch 2210: train loss: 0.06308173388242722\n",
      "Epoch 2220: train loss: 0.06301849335432053\n",
      "Epoch 2230: train loss: 0.06295590102672577\n",
      "Epoch 2240: train loss: 0.06289394199848175\n",
      "Epoch 2250: train loss: 0.06283263117074966\n",
      "Epoch 2260: train loss: 0.06277193874120712\n",
      "Epoch 2270: train loss: 0.06271188706159592\n",
      "Epoch 2280: train loss: 0.06265246868133545\n",
      "Epoch 2290: train loss: 0.06259368360042572\n",
      "Epoch 2300: train loss: 0.06253551691770554\n",
      "Epoch 2310: train loss: 0.062477998435497284\n",
      "Epoch 2320: train loss: 0.06242108345031738\n",
      "Epoch 2330: train loss: 0.06236480176448822\n",
      "Epoch 2340: train loss: 0.0623091459274292\n",
      "Epoch 2350: train loss: 0.06225409731268883\n",
      "Epoch 2360: train loss: 0.062199678272008896\n",
      "Epoch 2370: train loss: 0.06214587017893791\n",
      "Epoch 2380: train loss: 0.062092673033475876\n",
      "Epoch 2390: train loss: 0.06204007938504219\n",
      "Epoch 2400: train loss: 0.06198810413479805\n",
      "Epoch 2410: train loss: 0.06193672865629196\n",
      "Epoch 2420: train loss: 0.06188596040010452\n",
      "Epoch 2430: train loss: 0.06183578446507454\n",
      "Epoch 2440: train loss: 0.061786215752363205\n",
      "Epoch 2450: train loss: 0.061737239360809326\n",
      "Epoch 2460: train loss: 0.06168884038925171\n",
      "Epoch 2470: train loss: 0.06164104491472244\n",
      "Epoch 2480: train loss: 0.06159383803606033\n",
      "Epoch 2490: train loss: 0.06154719740152359\n",
      "Epoch 2500: train loss: 0.0615011528134346\n",
      "Epoch 2510: train loss: 0.06145566701889038\n",
      "Epoch 2520: train loss: 0.06141076982021332\n",
      "Epoch 2530: train loss: 0.061366427689790726\n",
      "Epoch 2540: train loss: 0.061322662979364395\n",
      "Epoch 2550: train loss: 0.06127944961190224\n",
      "Epoch 2560: train loss: 0.06123679503798485\n",
      "Epoch 2570: train loss: 0.061194710433483124\n",
      "Epoch 2580: train loss: 0.061153165996074677\n",
      "Epoch 2590: train loss: 0.061112165451049805\n",
      "Epoch 2600: train loss: 0.06107170879840851\n",
      "Epoch 2610: train loss: 0.061031799763441086\n",
      "Epoch 2620: train loss: 0.060992415994405746\n",
      "Epoch 2630: train loss: 0.060953572392463684\n",
      "Epoch 2640: train loss: 0.060915250331163406\n",
      "Epoch 2650: train loss: 0.06087745353579521\n",
      "Epoch 2660: train loss: 0.0608401820063591\n",
      "Epoch 2670: train loss: 0.06080343574285507\n",
      "Epoch 2680: train loss: 0.06076718121767044\n",
      "Epoch 2690: train loss: 0.060731444507837296\n",
      "Epoch 2700: train loss: 0.06069619953632355\n",
      "Epoch 2710: train loss: 0.06066146865487099\n",
      "Epoch 2720: train loss: 0.060627225786447525\n",
      "Epoch 2730: train loss: 0.06059347093105316\n",
      "Epoch 2740: train loss: 0.060560207813978195\n",
      "Epoch 2750: train loss: 0.06052741780877113\n",
      "Epoch 2760: train loss: 0.06049511209130287\n",
      "Epoch 2770: train loss: 0.060463275760412216\n",
      "Epoch 2780: train loss: 0.06043190509080887\n",
      "Epoch 2790: train loss: 0.06040100380778313\n",
      "Epoch 2800: train loss: 0.0603705570101738\n",
      "Epoch 2810: train loss: 0.06034056469798088\n",
      "Epoch 2820: train loss: 0.06031101942062378\n",
      "Epoch 2830: train loss: 0.06028193235397339\n",
      "Epoch 2840: train loss: 0.06025327369570732\n",
      "Epoch 2850: train loss: 0.06022505462169647\n",
      "Epoch 2860: train loss: 0.060197267681360245\n",
      "Epoch 2870: train loss: 0.06016990914940834\n",
      "Epoch 2880: train loss: 0.060142967849969864\n",
      "Epoch 2890: train loss: 0.060116443783044815\n",
      "Epoch 2900: train loss: 0.06009034067392349\n",
      "Epoch 2910: train loss: 0.06006462872028351\n",
      "Epoch 2920: train loss: 0.06003933772444725\n",
      "Epoch 2930: train loss: 0.06001443788409233\n",
      "Epoch 2940: train loss: 0.05998992919921875\n",
      "Epoch 2950: train loss: 0.05996580794453621\n",
      "Epoch 2960: train loss: 0.05994207412004471\n",
      "Epoch 2970: train loss: 0.05991872772574425\n",
      "Epoch 2980: train loss: 0.059895746409893036\n",
      "Epoch 2990: train loss: 0.05987314134836197\n",
      "Epoch 3000: train loss: 0.059850890189409256\n",
      "Epoch 3010: train loss: 0.05982900410890579\n",
      "Epoch 3020: train loss: 0.059807486832141876\n",
      "Epoch 3030: train loss: 0.05978631228208542\n",
      "Epoch 3040: train loss: 0.05976548045873642\n",
      "Epoch 3050: train loss: 0.05974498763680458\n",
      "Epoch 3060: train loss: 0.059724852442741394\n",
      "Epoch 3070: train loss: 0.059705037623643875\n",
      "Epoch 3080: train loss: 0.059685543179512024\n",
      "Epoch 3090: train loss: 0.05966638773679733\n",
      "Epoch 3100: train loss: 0.05964753031730652\n",
      "Epoch 3110: train loss: 0.05962900072336197\n",
      "Epoch 3120: train loss: 0.05961077660322189\n",
      "Epoch 3130: train loss: 0.05959286168217659\n",
      "Epoch 3140: train loss: 0.059575241059064865\n",
      "Epoch 3150: train loss: 0.059557922184467316\n",
      "Epoch 3160: train loss: 0.059540893882513046\n",
      "Epoch 3170: train loss: 0.059524137526750565\n",
      "Epoch 3180: train loss: 0.05950767546892166\n",
      "Epoch 3190: train loss: 0.05949150025844574\n",
      "Epoch 3200: train loss: 0.05947558581829071\n",
      "Epoch 3210: train loss: 0.05945994332432747\n",
      "Epoch 3220: train loss: 0.05944455787539482\n",
      "Epoch 3230: train loss: 0.05942944437265396\n",
      "Epoch 3240: train loss: 0.0594145841896534\n",
      "Epoch 3250: train loss: 0.05939997360110283\n",
      "Epoch 3260: train loss: 0.05938561260700226\n",
      "Epoch 3270: train loss: 0.05937149003148079\n",
      "Epoch 3280: train loss: 0.05935761332511902\n",
      "Epoch 3290: train loss: 0.05934397131204605\n",
      "Epoch 3300: train loss: 0.059330541640520096\n",
      "Epoch 3310: train loss: 0.05931735783815384\n",
      "Epoch 3320: train loss: 0.05930439755320549\n",
      "Epoch 3330: train loss: 0.059291645884513855\n",
      "Epoch 3340: train loss: 0.05927911773324013\n",
      "Epoch 3350: train loss: 0.059266794472932816\n",
      "Epoch 3360: train loss: 0.059254687279462814\n",
      "Epoch 3370: train loss: 0.059242770075798035\n",
      "Epoch 3380: train loss: 0.059231050312519073\n",
      "Epoch 3390: train loss: 0.05921953544020653\n",
      "Epoch 3400: train loss: 0.05920819938182831\n",
      "Epoch 3410: train loss: 0.059197068214416504\n",
      "Epoch 3420: train loss: 0.05918610468506813\n",
      "Epoch 3430: train loss: 0.05917532742023468\n",
      "Epoch 3440: train loss: 0.059164728969335556\n",
      "Epoch 3450: train loss: 0.05915430560708046\n",
      "Epoch 3460: train loss: 0.059144046157598495\n",
      "Epoch 3470: train loss: 0.059133946895599365\n",
      "Epoch 3480: train loss: 0.05912401154637337\n",
      "Epoch 03489: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 3490: train loss: 0.0591142512857914\n",
      "Epoch 03496: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 3500: train loss: 0.059111956506967545\n",
      "Epoch 03502: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 03508: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 3510: train loss: 0.05911153182387352\n",
      "Epoch 03514: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 03520: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 3520: train loss: 0.05911148339509964\n",
      "Epoch 03526: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 3530: train loss: 0.05911147966980934\n",
      "Epoch 03532: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Early stop at epoch 3532, loss: 0.05911147966980934\n",
      "Predictions saved to results-2-10.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "print(\"Data loaded!\")\n",
    "# Utilize pretraining data by creating feature extractor which extracts lumo energy \n",
    "# features from available initial features\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "PretrainedFeatureClass = make_pretraining_class({\"pretrain\": feature_extractor})\n",
    "pretrainedfeatures = PretrainedFeatureClass(feature_extractor=\"pretrain\")\n",
    "\n",
    "x_train_featured = pretrainedfeatures.transform(x_train)\n",
    "x_test_featured = pretrainedfeatures.transform(x_test.to_numpy())\n",
    "# regression model\n",
    "regression_model = get_regression_model(x_train_featured, y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "# TODO: Implement the pipeline. It should contain feature extraction and regression. You can optionally\n",
    "# use other sklearn tools, such as StandardScaler, FunctionTransformer, etc.\n",
    "y_pred = regression_model(x_test_featured).squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
