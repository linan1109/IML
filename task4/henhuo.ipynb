{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_features = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 1000,\n",
    "    \"eval_size\": 4*256,\n",
    "    \"momentum\": 0.005,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"./pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"./pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"./train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"./train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"./test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    x_pretrain = scaler.fit_transform(x_pretrain)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test_transed = scaler.transform(x_test)\n",
    "    x_test[x_test.columns] = x_test_transed\n",
    "\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linan\\.conda\\envs\\DL\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our feature extractor used in pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(1000, 1000)\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(1000, 1000)\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):    \n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba3109fd8a54aedad99e768ed4aa4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 1.0943057450232494, val loss: 0.9604995846748352\n",
      "Epoch 11: train loss: 0.7546058118012324, val loss: 0.8425064980983734\n",
      "Epoch 00014: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 21: train loss: 0.6990577956306822, val loss: 0.7889628261327744\n",
      "Epoch 31: train loss: 0.687005219779658, val loss: 0.7803559750318527\n",
      "Epoch 00036: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 41: train loss: 0.6488821160477853, val loss: 0.7431006133556366\n",
      "Epoch 00046: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 51: train loss: 0.6228825188041396, val loss: 0.7169996052980423\n",
      "Epoch 00061: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 61: train loss: 0.6196563172402705, val loss: 0.7176687717437744\n",
      "Epoch 71: train loss: 0.6101881380916148, val loss: 0.7063692212104797\n",
      "Epoch 00075: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 81: train loss: 0.6071158064147765, val loss: 0.704039916396141\n",
      "Epoch 00090: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 91: train loss: 0.6051038548822537, val loss: 0.7112768292427063\n",
      "Epoch 00098: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 101: train loss: 0.6046934849450262, val loss: 0.7002174258232117\n",
      "Epoch 00104: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Early stop at epoch 104\n"
     ]
    }
   ],
   "source": [
    "eval_size = pretrain_features[\"eval_size\"]\n",
    "batch_size = pretrain_features[\"batch_size\"]\n",
    "ae_model = AE()\n",
    "ae_model.train()\n",
    "ae_model.to(device)\n",
    "\n",
    "def train_autoencoder():\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_pretrain, y_pretrain, test_size=eval_size, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(ae_model.parameters(), lr=pretrain_features['learning_rate'], weight_decay=pretrain_features['weight_decay'])\n",
    "    # optimizer = torch.optim.SGD(ae_model.parameters(), lr=pretrain_features['learning_rate'], momentum=pretrain_features['momentum'], weight_decay=pretrain_features['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = pretrain_features['epochs']\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, _] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr += loss.item() * len(x)\n",
    "        loss_tr /= len(train_loader.dataset)\n",
    "        for [x, _] in val_loader:\n",
    "            x = x.to(device)\n",
    "            predictions = ae_model(x)\n",
    "            loss = criterion(predictions, x)\n",
    "            loss_val += loss.item() * len(x)\n",
    "        loss_val /= len(val_loader.dataset)\n",
    "        scheduler.step(loss_val)\n",
    "        \n",
    "        if(epoch % 10 == 0):\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-6):\n",
    "            print(f\"Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f1f4240aec494a9e34a16bc01b9e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 12.371824264526367, val loss: 9.59903621673584\n",
      "Epoch 00017: reducing learning rate of group 0 to 3.0000e-03.\n",
      "Epoch 00026: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 00032: reducing learning rate of group 0 to 2.7000e-04.\n",
      "Epoch 00042: reducing learning rate of group 0 to 8.1000e-05.\n",
      "Epoch 00048: reducing learning rate of group 0 to 2.4300e-05.\n",
      "Epoch 00054: reducing learning rate of group 0 to 7.2900e-06.\n",
      "Epoch 00060: reducing learning rate of group 0 to 2.1870e-06.\n",
      "Epoch 00066: reducing learning rate of group 0 to 6.5610e-07.\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.9683e-07.\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.9049e-08.\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.7715e-08.\n",
      "Epoch 00093: reducing learning rate of group 0 to 5.3144e-09.\n",
      "Early stop at epoch 93\n"
     ]
    }
   ],
   "source": [
    "def get_regression_model(X, y):\n",
    "\n",
    "    class Model(nn.Module):\n",
    "            \"\"\"\n",
    "            The model class, which defines our feature extractor used in pretraining.\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                \"\"\"\n",
    "                The constructor of the model.\n",
    "                \"\"\"\n",
    "                super().__init__()\n",
    "                # TODO: Define the architecture of the model. It should be able to be trained on pretraing data \n",
    "                # and then used to extract features from the training and test data.\n",
    "                self.seq = nn.Sequential(\n",
    "                    nn.Linear(1000, 40),\n",
    "                    nn.BatchNorm1d(40),\n",
    "                    nn.LeakyReLU(0.01),\n",
    "                    nn.Dropout(0.6),\n",
    "                    nn.Linear(40, 1)\n",
    "                )\n",
    "\n",
    "                for m in self.modules():\n",
    "                    if isinstance(m, nn.Linear):    \n",
    "                        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "            def forward(self, x):\n",
    "                \"\"\"\n",
    "                The forward pass of the model.\n",
    "\n",
    "                input: x: torch.Tensor, the input to the model\n",
    "\n",
    "                output: x: torch.Tensor, the output of the model\n",
    "                \"\"\"\n",
    "                # TODO: Implement the forward pass of the model, in accordance with the architecture \n",
    "                # defined in the constructor.\n",
    "                x = self.seq(x)\n",
    "                return x\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the regression model used in the pipeline.\n",
    "\n",
    "    input: None\n",
    "\n",
    "    output: model: sklearn compatible model, the regression model\n",
    "    \"\"\"\n",
    "    # TODO: Implement the regression model. It should be able to be trained on the features extracted\n",
    "    # by the feature extractor.\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=10, random_state=0, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_tr, y_tr)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.4, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=5, verbose=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    epochs = 10000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_tr = 0\n",
    "        loss_val = 0\n",
    "        for [x, y] in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_tr = loss.item()\n",
    "        for [x, y] in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x).squeeze(-1)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss_val = loss.item()\n",
    "        scheduler.step(loss_val)\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f\"Epoch {epoch+1}: train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "        if(optimizer.param_groups[0]['lr'] < 1e-8):\n",
    "            print(f\"Early stop at epoch {epoch+1}, train loss: {loss_tr}, val loss: {loss_val}\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "featured_x_pretrain = ae_model.encode(torch.tensor(x_pretrain, dtype=torch.float).to(device)).detach().cpu().numpy()\n",
    "one_model = get_regression_model(featured_x_pretrain, y_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"results-henhuo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import huberregression\n",
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to results-henhuo.csv, all done!\n"
     ]
    }
   ],
   "source": [
    "huber = HuberRegressor()\n",
    "featured_x_train = one_model(ae_model.encoder(torch.tensor(x_train, dtype=torch.float).to(device))).squeeze(-1).detach().cpu().numpy()\n",
    "huber.fit(featured_x_train.reshape(-1, 1), y_train)\n",
    "\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "featured_x_test = one_model(ae_model.encoder(torch.tensor(x_test.to_numpy(), dtype=torch.float).to(device))).squeeze(-1).detach().cpu().numpy()\n",
    "y_pred = huber.predict(featured_x_test.reshape(-1, 1))\n",
    "\n",
    "assert y_pred.shape == (x_test.shape[0],)\n",
    "y_pred = pd.DataFrame({\"y\": y_pred}, index=x_test.index)\n",
    "y_pred.to_csv(result_file, index_label=\"Id\")\n",
    "print(f\"Predictions saved to {result_file}, all done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
