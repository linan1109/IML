{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading():\n",
    "    \"\"\"\n",
    "    This function loads the training and test data, preprocesses it, removes the NaN values and interpolates the missing \n",
    "    data using imputation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train: matrix of floats, training input with features\n",
    "    y_train: array of floats, training output with labels\n",
    "    X_test: matrix of floats: dim = (100, ?), test input with features\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    print(\"Training data:\")\n",
    "    print(\"Shape:\", train_df.shape)\n",
    "\n",
    "    train_df = train_df.dropna(subset=['price_CHF'])\n",
    "    print(\"Shape after dropping price_CHF missing:\", train_df.shape)\n",
    "    \n",
    "    print(train_df.head(2))\n",
    "    print('\\n')\n",
    "\n",
    "    train_df = pd.get_dummies(train_df, columns=['season'])\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    print(\"Test data:\")\n",
    "    print(test_df.shape)\n",
    "    test_df = pd.get_dummies(test_df, columns=['season'])\n",
    "    print(test_df.head(2))\n",
    "\n",
    "    # Dummy initialization of the X_train, X_test and y_train   \n",
    "    X_train = train_df.drop(['price_CHF'],axis=1)\n",
    "    y_train = train_df['price_CHF']\n",
    "    X_test = test_df[X_train.columns]\n",
    "\n",
    "    X = pd.concat([X_train, X_test], axis=0)\n",
    "    X_incomplete_normalized = BiScaler().fit_transform(np.array(X))\n",
    "    X_filled_softimpute = SoftImpute().fit_transform(X_incomplete_normalized)\n",
    "    X = pd.DataFrame(X_filled_softimpute, columns=X.columns)\n",
    "    X_train = X.iloc[:X_train.shape[0], :]\n",
    "    X_test = X.iloc[X_train.shape[0]:, :]\n",
    "    \n",
    "    assert (X_train.shape[1] == X_test.shape[1]) and (X_train.shape[0] == y_train.shape[0]) and (X_test.shape[0] == 100), \"Invalid data shape\"\n",
    "    return X_train, y_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_and_prediction(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    This function defines the model, fits training data and then does the prediction with the test data \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: matrix of floats, training input with 10 features\n",
    "    y_train: array of floats, training output\n",
    "    X_test: matrix of floats: dim = (100, ?), test input with 10 features\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y_test: array of floats: dim = (100,), predictions on test set\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred=np.zeros(X_test.shape[0])\n",
    "    \n",
    "    # find the alpha for huber\n",
    "    alpha_list = np.logspace(-3, 3, 1000)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = []\n",
    "    for alpha in alpha_list:\n",
    "        score = []\n",
    "        for train_idx, val_idx in cv.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            model = HuberRegressor(alpha=alpha, max_iter=100000000)\n",
    "            model.fit(X_tr, y_tr)\n",
    "            score.append(model.score(X_val, y_val))\n",
    "        scores.append(np.mean(score))\n",
    "    alpha = alpha_list[np.argmax(scores)]\n",
    "    print(\"alpha:\", alpha)\n",
    "    model = HuberRegressor(alpha=alpha, max_iter=100000000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred[y_pred<0] = 0\n",
    "\n",
    "    assert y_pred.shape == (100,), \"Invalid data shape\"\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Shape: (900, 11)\n",
      "Shape after dropping price_CHF missing: (631, 11)\n",
      "   season  price_AUS  price_CHF  price_CZE  price_GER  price_ESP  price_FRA  \\\n",
      "0  spring        NaN   9.644028  -1.686248  -1.748076  -3.666005        NaN   \n",
      "1  summer        NaN   7.246061  -2.132377  -2.054363  -3.295697  -4.104759   \n",
      "\n",
      "   price_UK  price_ITA  price_POL  price_SVK  \n",
      "0 -1.822720  -3.931031        NaN  -3.238197  \n",
      "1 -1.826021        NaN        NaN  -3.212894  \n",
      "\n",
      "\n",
      "Test data:\n",
      "(100, 10)\n",
      "   price_AUS  price_CZE  price_GER  price_ESP  price_FRA  price_UK  price_ITA  \\\n",
      "0        NaN   0.472985   0.707957        NaN  -1.136441 -0.596703        NaN   \n",
      "1  -1.184837   0.358019        NaN  -3.199028  -1.069695       NaN  -1.420091   \n",
      "\n",
      "   price_POL  price_SVK  season_autumn  season_spring  season_summer  \\\n",
      "0   3.298693   1.921886              0              1              0   \n",
      "1   3.238307        NaN              0              0              1   \n",
      "\n",
      "   season_winter  \n",
      "0              0  \n",
      "1              0  \n",
      "[BiScaler] Initial log residual value = 7.503613\n",
      "[BiScaler] Iter 1: log residual = 3.427117, log improvement ratio=4.076497\n",
      "[BiScaler] Iter 2: log residual = 2.275026, log improvement ratio=1.152090\n",
      "[BiScaler] Iter 3: log residual = 1.292902, log improvement ratio=0.982125\n",
      "[BiScaler] Iter 4: log residual = 0.366224, log improvement ratio=0.926678\n",
      "[BiScaler] Iter 5: log residual = -0.545035, log improvement ratio=0.911259\n",
      "[BiScaler] Iter 6: log residual = -1.453457, log improvement ratio=0.908422\n",
      "[BiScaler] Iter 7: log residual = -2.361955, log improvement ratio=0.908498\n",
      "[BiScaler] Iter 8: log residual = -3.270196, log improvement ratio=0.908240\n",
      "[BiScaler] Iter 9: log residual = -4.177142, log improvement ratio=0.906946\n",
      "[BiScaler] Iter 10: log residual = -5.081933, log improvement ratio=0.904792\n",
      "[BiScaler] Iter 11: log residual = -5.984084, log improvement ratio=0.902151\n",
      "[BiScaler] Iter 12: log residual = -6.883441, log improvement ratio=0.899357\n",
      "[BiScaler] Iter 13: log residual = -7.780085, log improvement ratio=0.896643\n",
      "[BiScaler] Iter 14: log residual = -8.674231, log improvement ratio=0.894146\n",
      "[BiScaler] Iter 15: log residual = -9.566160, log improvement ratio=0.891930\n",
      "[BiScaler] Iter 16: log residual = -10.456173, log improvement ratio=0.890013\n",
      "[BiScaler] Iter 17: log residual = -11.344557, log improvement ratio=0.888385\n",
      "[BiScaler] Iter 18: log residual = -12.231577, log improvement ratio=0.887020\n",
      "[BiScaler] Iter 19: log residual = -13.117465, log improvement ratio=0.885888\n",
      "[BiScaler] Iter 20: log residual = -14.002421, log improvement ratio=0.884956\n",
      "[BiScaler] Iter 21: log residual = -14.886613, log improvement ratio=0.884192\n",
      "[BiScaler] Iter 22: log residual = -15.770182, log improvement ratio=0.883569\n",
      "[BiScaler] Iter 23: log residual = -16.653246, log improvement ratio=0.883063\n",
      "[BiScaler] Iter 24: log residual = -17.535899, log improvement ratio=0.882653\n",
      "[BiScaler] Iter 25: log residual = -18.418220, log improvement ratio=0.882322\n",
      "[BiScaler] Iter 26: log residual = -19.300274, log improvement ratio=0.882054\n",
      "[BiScaler] Iter 27: log residual = -20.182111, log improvement ratio=0.881837\n",
      "[BiScaler] Iter 28: log residual = -21.063775, log improvement ratio=0.881663\n",
      "[BiScaler] Iter 29: log residual = -21.945298, log improvement ratio=0.881523\n",
      "[BiScaler] Iter 30: log residual = -22.826708, log improvement ratio=0.881410\n",
      "[BiScaler] Iter 31: log residual = -23.708027, log improvement ratio=0.881319\n",
      "[BiScaler] Iter 32: log residual = -24.589274, log improvement ratio=0.881246\n",
      "[BiScaler] Iter 33: log residual = -25.470461, log improvement ratio=0.881188\n",
      "[BiScaler] Iter 34: log residual = -26.351602, log improvement ratio=0.881141\n",
      "[BiScaler] Iter 35: log residual = -27.232705, log improvement ratio=0.881103\n",
      "[BiScaler] Iter 36: log residual = -28.113777, log improvement ratio=0.881072\n",
      "[BiScaler] Iter 37: log residual = -28.994825, log improvement ratio=0.881048\n",
      "[BiScaler] Iter 38: log residual = -29.875853, log improvement ratio=0.881028\n",
      "[BiScaler] Iter 39: log residual = -30.756865, log improvement ratio=0.881012\n",
      "[BiScaler] Iter 40: log residual = -31.637865, log improvement ratio=0.881000\n",
      "[BiScaler] Iter 41: log residual = -32.518855, log improvement ratio=0.880990\n",
      "[BiScaler] Iter 42: log residual = -33.399836, log improvement ratio=0.880981\n",
      "[BiScaler] Iter 43: log residual = -34.280811, log improvement ratio=0.880975\n",
      "[BiScaler] Iter 44: log residual = -35.161781, log improvement ratio=0.880970\n",
      "[BiScaler] Iter 45: log residual = -36.042746, log improvement ratio=0.880964\n",
      "[BiScaler] Iter 46: log residual = -36.923708, log improvement ratio=0.880963\n",
      "[BiScaler] Iter 47: log residual = -37.804666, log improvement ratio=0.880958\n",
      "[BiScaler] Iter 48: log residual = -38.685624, log improvement ratio=0.880958\n",
      "[BiScaler] Iter 49: log residual = -39.566580, log improvement ratio=0.880957\n",
      "[BiScaler] Iter 50: log residual = -40.447537, log improvement ratio=0.880956\n",
      "[BiScaler] Iter 51: log residual = -41.328481, log improvement ratio=0.880944\n",
      "[BiScaler] Iter 52: log residual = -42.209435, log improvement ratio=0.880954\n",
      "[BiScaler] Iter 53: log residual = -43.090415, log improvement ratio=0.880980\n",
      "[BiScaler] Iter 54: log residual = -43.971349, log improvement ratio=0.880934\n",
      "[BiScaler] Iter 55: log residual = -44.852247, log improvement ratio=0.880897\n",
      "[BiScaler] Iter 56: log residual = -45.733267, log improvement ratio=0.881021\n",
      "[BiScaler] Iter 57: log residual = -46.614212, log improvement ratio=0.880944\n",
      "[BiScaler] Iter 58: log residual = -47.495126, log improvement ratio=0.880914\n",
      "[BiScaler] Iter 59: log residual = -48.376076, log improvement ratio=0.880950\n",
      "[BiScaler] Iter 60: log residual = -49.257361, log improvement ratio=0.881285\n",
      "[BiScaler] Iter 61: log residual = -50.137655, log improvement ratio=0.880294\n",
      "[BiScaler] Iter 62: log residual = -51.018620, log improvement ratio=0.880965\n",
      "[BiScaler] Iter 63: log residual = -51.899921, log improvement ratio=0.881301\n",
      "[BiScaler] Iter 64: log residual = -52.777303, log improvement ratio=0.877382\n",
      "[BiScaler] Iter 65: log residual = -53.664621, log improvement ratio=0.887318\n",
      "[BiScaler] Iter 66: log residual = -54.543967, log improvement ratio=0.879345\n",
      "[BiScaler] Iter 67: log residual = -55.414843, log improvement ratio=0.870876\n",
      "[BiScaler] Iter 68: log residual = -56.314010, log improvement ratio=0.899167\n",
      "[BiScaler] Iter 69: log residual = -57.201232, log improvement ratio=0.887222\n",
      "[BiScaler] Iter 70: log residual = -58.068429, log improvement ratio=0.867197\n",
      "[BiScaler] Iter 71: log residual = -58.947557, log improvement ratio=0.879128\n",
      "[BiScaler] Iter 72: log residual = -59.779846, log improvement ratio=0.832289\n",
      "[BiScaler] Iter 73: log residual = -60.708385, log improvement ratio=0.928539\n",
      "[BiScaler] Iter 74: log residual = -61.591510, log improvement ratio=0.883125\n",
      "[BiScaler] Iter 75: log residual = -62.142494, log improvement ratio=0.550984\n",
      "[BiScaler] Iter 76: log residual = -62.886523, log improvement ratio=0.744029\n",
      "[BiScaler] Iter 77: log residual = -63.667993, log improvement ratio=0.781470\n",
      "[BiScaler] Iter 78: log residual = -63.731485, log improvement ratio=0.063492\n",
      "[BiScaler] Iter 79: log residual = -64.176820, log improvement ratio=0.445335\n",
      "[BiScaler] Iter 80: log residual = -64.430892, log improvement ratio=0.254072\n",
      "[BiScaler] Iter 81: log residual = -64.039475, log improvement ratio=-0.391417\n",
      "[SoftImpute] Max Singular Value of X_init = 56.893788\n",
      "[SoftImpute] Iter 1: observed MAE=0.035502 rank=12\n",
      "[SoftImpute] Iter 2: observed MAE=0.035661 rank=12\n",
      "[SoftImpute] Iter 3: observed MAE=0.035820 rank=12\n",
      "[SoftImpute] Iter 4: observed MAE=0.035973 rank=12\n",
      "[SoftImpute] Iter 5: observed MAE=0.036114 rank=12\n",
      "[SoftImpute] Iter 6: observed MAE=0.036240 rank=12\n",
      "[SoftImpute] Iter 7: observed MAE=0.036353 rank=12\n",
      "[SoftImpute] Iter 8: observed MAE=0.036456 rank=12\n",
      "[SoftImpute] Iter 9: observed MAE=0.036551 rank=12\n",
      "[SoftImpute] Iter 10: observed MAE=0.036638 rank=12\n",
      "[SoftImpute] Iter 11: observed MAE=0.036718 rank=12\n",
      "[SoftImpute] Iter 12: observed MAE=0.036792 rank=12\n",
      "[SoftImpute] Iter 13: observed MAE=0.036860 rank=12\n",
      "[SoftImpute] Iter 14: observed MAE=0.036924 rank=12\n",
      "[SoftImpute] Iter 15: observed MAE=0.036985 rank=12\n",
      "[SoftImpute] Iter 16: observed MAE=0.037043 rank=12\n",
      "[SoftImpute] Iter 17: observed MAE=0.037099 rank=12\n",
      "[SoftImpute] Iter 18: observed MAE=0.037150 rank=12\n",
      "[SoftImpute] Iter 19: observed MAE=0.037200 rank=12\n",
      "[SoftImpute] Iter 20: observed MAE=0.037246 rank=12\n",
      "[SoftImpute] Iter 21: observed MAE=0.037290 rank=12\n",
      "[SoftImpute] Iter 22: observed MAE=0.037331 rank=12\n",
      "[SoftImpute] Iter 23: observed MAE=0.037370 rank=12\n",
      "[SoftImpute] Iter 24: observed MAE=0.037406 rank=12\n",
      "[SoftImpute] Iter 25: observed MAE=0.037441 rank=12\n",
      "[SoftImpute] Iter 26: observed MAE=0.037474 rank=12\n",
      "[SoftImpute] Iter 27: observed MAE=0.037505 rank=12\n",
      "[SoftImpute] Iter 28: observed MAE=0.037534 rank=12\n",
      "[SoftImpute] Iter 29: observed MAE=0.037562 rank=12\n",
      "[SoftImpute] Iter 30: observed MAE=0.037589 rank=12\n",
      "[SoftImpute] Iter 31: observed MAE=0.037615 rank=12\n",
      "[SoftImpute] Iter 32: observed MAE=0.037638 rank=12\n",
      "[SoftImpute] Iter 33: observed MAE=0.037660 rank=12\n",
      "[SoftImpute] Iter 34: observed MAE=0.037681 rank=12\n",
      "[SoftImpute] Iter 35: observed MAE=0.037687 rank=13\n",
      "[SoftImpute] Iter 36: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 37: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 38: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 39: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 40: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 41: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 42: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 43: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 44: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 45: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 46: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 47: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 48: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 49: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 50: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 51: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 52: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 53: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 54: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 55: observed MAE=0.037686 rank=13\n",
      "[SoftImpute] Iter 56: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 57: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 58: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 59: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 60: observed MAE=0.037685 rank=13\n",
      "[SoftImpute] Iter 61: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 62: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 63: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 64: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 65: observed MAE=0.037684 rank=13\n",
      "[SoftImpute] Iter 66: observed MAE=0.037683 rank=13\n",
      "[SoftImpute] Iter 67: observed MAE=0.037683 rank=13\n",
      "[SoftImpute] Iter 68: observed MAE=0.037683 rank=13\n",
      "[SoftImpute] Iter 69: observed MAE=0.037683 rank=13\n",
      "[SoftImpute] Iter 70: observed MAE=0.037682 rank=13\n",
      "[SoftImpute] Iter 71: observed MAE=0.037682 rank=13\n",
      "[SoftImpute] Iter 72: observed MAE=0.037682 rank=13\n",
      "[SoftImpute] Iter 73: observed MAE=0.037681 rank=13\n",
      "[SoftImpute] Iter 74: observed MAE=0.037681 rank=13\n",
      "[SoftImpute] Iter 75: observed MAE=0.037681 rank=13\n",
      "[SoftImpute] Iter 76: observed MAE=0.037681 rank=13\n",
      "[SoftImpute] Iter 77: observed MAE=0.037680 rank=13\n",
      "[SoftImpute] Iter 78: observed MAE=0.037680 rank=13\n",
      "[SoftImpute] Iter 79: observed MAE=0.037680 rank=13\n",
      "[SoftImpute] Iter 80: observed MAE=0.037680 rank=13\n",
      "[SoftImpute] Iter 81: observed MAE=0.037679 rank=13\n",
      "[SoftImpute] Iter 82: observed MAE=0.037679 rank=13\n",
      "[SoftImpute] Iter 83: observed MAE=0.037679 rank=13\n",
      "[SoftImpute] Iter 84: observed MAE=0.037679 rank=13\n",
      "[SoftImpute] Iter 85: observed MAE=0.037678 rank=13\n",
      "[SoftImpute] Iter 86: observed MAE=0.037678 rank=13\n",
      "[SoftImpute] Iter 87: observed MAE=0.037678 rank=13\n",
      "[SoftImpute] Iter 88: observed MAE=0.037677 rank=13\n",
      "[SoftImpute] Iter 89: observed MAE=0.037677 rank=13\n",
      "[SoftImpute] Iter 90: observed MAE=0.037677 rank=13\n",
      "[SoftImpute] Iter 91: observed MAE=0.037677 rank=13\n",
      "[SoftImpute] Iter 92: observed MAE=0.037676 rank=13\n",
      "[SoftImpute] Stopped after iteration 92 for lambda=1.137876\n",
      "alpha: 0.0031513634848664793\n",
      "\n",
      "Results file successfully generated!\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "X_train, y_train, X_test = data_loading()\n",
    "# The function retrieving optimal LR parameters\n",
    "y_pred=modeling_and_prediction(X_train, y_train, X_test)\n",
    "# Save results in the required format\n",
    "dt = pd.DataFrame(y_pred) \n",
    "dt.columns = ['price_CHF']\n",
    "dt.to_csv('results-fancySoft.csv', index=False)\n",
    "print(\"\\nResults file successfully generated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
